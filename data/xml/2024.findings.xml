<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.findings">
  <volume id="eacl" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: EACL 2024</booktitle>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="f35e73f5">2024.findings-eacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="24d75a3c">2024.findings-eacl.0</url>
      <bibkey>findings-2024-findings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Chem-<fixed-case>FINESE</fixed-case>: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction</title>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Hongxiang</first><last>Li</last><affiliation>UIUC</affiliation></author>
      <author><first>Xuan</first><last>Liu</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Huimin</first><last>Zhao</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>1-16</pages>
      <abstract>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</abstract>
      <url hash="d115d641">2024.findings-eacl.1</url>
      <attachment type="software" hash="4203c223">2024.findings-eacl.1.software.zip</attachment>
      <attachment type="note" hash="c244f46e">2024.findings-eacl.1.note.zip</attachment>
      <bibkey>wang-etal-2024-chem</bibkey>
      <video href="2024.findings-eacl.1.mp4"/>
      <revision id="1" href="2024.findings-eacl.1v1" hash="b832457e"/>
      <revision id="2" href="2024.findings-eacl.1v2" hash="d115d641" date="2024-05-30">Fix two typos in equation 2 and 4.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>GPT</fixed-case>s Are Multilingual Annotators for Sequence Generation Tasks</title>
      <author><first>Juhwan</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Eunju</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyohoon</first><last>Jin</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>ChungAng University</affiliation></author>
      <pages>17-40</pages>
      <abstract>Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.</abstract>
      <url hash="d0582eb1">2024.findings-eacl.2</url>
      <attachment type="software" hash="fff5e2cf">2024.findings-eacl.2.software.zip</attachment>
      <attachment type="note" hash="cce955c1">2024.findings-eacl.2.note.zip</attachment>
      <bibkey>choi-etal-2024-gpts</bibkey>
      <video href="2024.findings-eacl.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive <fixed-case>EHR</fixed-case> Modelling with Hierarchical Regularisation</title>
      <author><first>Heejoon</first><last>Koo</last></author>
      <pages>41-55</pages>
      <abstract>Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical codes representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.</abstract>
      <url hash="27b828b0">2024.findings-eacl.3</url>
      <bibkey>koo-2024-next</bibkey>
      <video href="2024.findings-eacl.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>F</fixed-case>lexi<fixed-case>QA</fixed-case>: Leveraging <fixed-case>LLM</fixed-case>’s Evaluation Capabilities for Flexible Knowledge Selection in Open-domain Question Answering</title>
      <author><first>Yuhan</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuqi</first><last>Li</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>56-66</pages>
      <abstract>Nowadays, large language models (LLMs) have demonstrated their ability to be a powerful knowledge generator of generate-then-read paradigm for open-domain question answering (ODQA). However this new paradigm mainly suffers from the “hallucination” and struggles to handle time-sensitive issue because of its expensive knowledge update costs. On the other hand, retrieve-then-read, as a traditional paradigm, is more limited by the relevance of acquired knowledge to the given question. In order to combine the strengths of both paradigms, and overcome their respective shortcomings, we design a new pipeline called “FlexiQA”, in which we utilize the diverse evaluation capabilities of LLMs to select knowledge effectively and flexibly. First, given a question, we prompt a LLM as a discriminator to identify whether it is time-sensitive. For time-sensitive questions, we follow the retrieve-then-read paradigm to obtain the answer. For the non time-sensitive questions, we further prompt the LLM as an evaluator to select a better document from two perspectives: factuality and relevance. Based on the selected document, we leverage a reader to get the final answer. We conduct extensive experiments on three widely-used ODQA benchmarks, the experimental results fully confirm the effectiveness of our approach.</abstract>
      <url hash="cbe0ec51">2024.findings-eacl.4</url>
      <bibkey>chen-etal-2024-flexiqa</bibkey>
      <video href="2024.findings-eacl.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Hyper-<fixed-case>BTS</fixed-case> Dataset: Scalability and Enhanced Analysis of Back <fixed-case>T</fixed-case>ran<fixed-case>S</fixed-case>cription (<fixed-case>BTS</fixed-case>) for <fixed-case>ASR</fixed-case> Post-Processing</title>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seolhwa</first><last>Lee</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Junyoung</first><last>Son</last></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanhee</first><last>Lee</last><affiliation>NAVER</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>67-78</pages>
      <abstract>The recent advancements in the realm of Automatic Speech Recognition (ASR) post-processing have been primarily driven by sequence-to-sequence paradigms. Despite their effectiveness, these methods often demand substantial amounts of data, necessitating the expensive recruitment of phonetic transcription experts to rectify the erroneous outputs of ASR systems, thereby creating the desired training data. Back TranScription (BTS) alleviates this issue by generating ASR inputs from clean text via a Text-to-Speech (TTS) system. While initial studies on BTS exhibited promise, they were constrained by a limited dataset of just 200,000 sentence pairs, leaving the scalability of this method in question. In this study, we delve into the potential scalability of BTS. We introduce the “Hyper-BTS” dataset, a corpus approximately five times larger than that utilized in prior research. Additionally, we present innovative criteria for categorizing error types within ASR post-processing. This not only facilitates a more comprehensive qualitative analysis, which was absent in preceding studies, but also enhances the understanding of ASR error patterns. Our empirical results, both quantitative and qualitative, suggest that the enlarged scale of the Hyper-BTS dataset sufficiently addresses a vast majority of the ASR error categories. We make the Hyper-BTS dataset publicly available.</abstract>
      <url hash="822de166">2024.findings-eacl.5</url>
      <bibkey>park-etal-2024-hyper</bibkey>
      <video href="2024.findings-eacl.5.mp4"/>
    </paper>
    <paper id="6">
      <title><fixed-case>P</fixed-case>arrot<fixed-case>TTS</fixed-case>: Text-to-speech synthesis exploiting disentangled self-supervised representations</title>
      <author><first>Neil</first><last>Shah</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Saiteja</first><last>Kosgi</last></author>
      <author><first>Vishal</first><last>Tambrahalli</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Neha</first><last>S</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Anil</first><last>Nelakanti</last></author>
      <author><first>Vineet</first><last>Gandhi</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>79-91</pages>
      <abstract>We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker’s voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual text-to-speech (TTS) models using only a fraction of paired data as latter. Speech samples from ParrotTTS and code can be found at https://parrot-tts.github.io/tts/</abstract>
      <url hash="1104692c">2024.findings-eacl.6</url>
      <bibkey>shah-etal-2024-parrottts</bibkey>
      <video href="2024.findings-eacl.6.mp4"/>
    </paper>
    <paper id="7">
      <title><fixed-case>N</fixed-case>av<fixed-case>H</fixed-case>int: Vision and Language Navigation Agent with a Hint Generator</title>
      <author><first>Yue</first><last>Zhang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Quan</first><last>Guo</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>92-103</pages>
      <abstract>The existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment.In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions.The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent’s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment.We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the agent’s interpretability.</abstract>
      <url hash="63815ab4">2024.findings-eacl.7</url>
      <bibkey>zhang-etal-2024-navhint</bibkey>
      <video href="2024.findings-eacl.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?</title>
      <author><first>Piush</first><last>Aggarwal</last><affiliation>Fernuniversität Gesamthochschule Hagen and Fernuniversität Gesamthochschule Hagen</affiliation></author>
      <author><first>Jawar</first><last>Mehrabanian</last></author>
      <author><first>Weigang</first><last>Huang</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Özge</first><last>Alacam</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Fernuniversität in Hagen</affiliation></author>
      <pages>104-117</pages>
      <abstract>This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide evidence supporting the hypothesis that only the textual component of hateful memes enables the multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme’s image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with average ∆F1 of 0.18.</abstract>
      <url hash="38939fde">2024.findings-eacl.8</url>
      <bibkey>aggarwal-etal-2024-text</bibkey>
    </paper>
    <paper id="9">
      <title>Where are we Still Split on Tokenization?</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>118-137</pages>
      <abstract>Many Natural Language Processing (NLP) tasks are labeled on the token level, forthese tasks, the first step is to identify the tokens (tokenization). Becausethis step is often considered to be a solved problem, gold tokenization iscommonly assumed. In this paper, we propose an efficient method fortokenization with subword-based language models, and reflect on the status ofperformance on the tokenization task by evaluating on 122 languages in 20different scripts. We show that our proposed model performs on par with thestate-of-the-art, and that tokenization performance is mainly dependent on theamount and consistency of annotated data. We conclude that besidesinconsistencies in the data and exceptional cases the task can be consideredsolved for Latin languages for in-dataset settings (&gt;99.5 F1). However,performance is 0.75 F1 point lower on average for datasets in other scripts andperformance deteriorates in cross-dataset setups.</abstract>
      <url hash="e9d06d61">2024.findings-eacl.9</url>
      <attachment type="software" hash="4898d0e8">2024.findings-eacl.9.software.tgz</attachment>
      <bibkey>goot-2024-still</bibkey>
      <video href="2024.findings-eacl.9.mp4"/>
    </paper>
    <paper id="10">
      <title>A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages</title>
      <author><first>Nikita</first><last>Martynov</last><affiliation>New Economic School</affiliation></author>
      <author><first>Mark</first><last>Baushenko</last></author>
      <author><first>Anastasia</first><last>Kozlova</last></author>
      <author><first>Katerina</first><last>Kolomeytseva</last></author>
      <author><first>Aleksandr</first><last>Abramov</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <pages>138-155</pages>
      <abstract>Large language models excel in text generation and generalization, however they face challenges in text editing tasks, especially in correcting spelling errors and mistyping.In this paper, we present a methodology for generative spelling correction (SC), tested on English and Russian languages and potentially can be extended to any language with minor changes. Our research mainly focuses on exploring natural spelling errors and mistyping in texts and studying how those errors can be emulated in correct sentences to enrich generative models’ pre-train procedure effectively. We investigate the effects of emulations in various text domains and examine two spelling corruption techniques: 1) first one mimics human behavior when making a mistake through leveraging statistics of errors from a particular dataset, and 2) second adds the most common spelling errors, keyboard miss clicks, and some heuristics within the texts.We conducted experiments employing various corruption strategies, models’ architectures, and sizes in the pre-training and fine-tuning stages and evaluated the models using single-domain and multi-domain test sets. As a practical outcome of our work, we introduce SAGE (Spell checking via Augmentation and Generative distribution Emulation).</abstract>
      <url hash="86d5f92e">2024.findings-eacl.10</url>
      <attachment type="software" hash="41b27f50">2024.findings-eacl.10.software.zip</attachment>
      <attachment type="note" hash="72e71561">2024.findings-eacl.10.note.zip</attachment>
      <bibkey>martynov-etal-2024-methodology</bibkey>
    </paper>
    <paper id="11">
      <title>How Does In-Context Learning Help Prompt Tuning?</title>
      <author><first>Simeng</first><last>Sun</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Dan</first><last>Iter</last></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>156-165</pages>
      <abstract>Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale. This motivates the use of parameter-efficient adaptation methods such as prompt tuning (PT), which adds a small number of tunable embeddings to an otherwise frozen model, and in-context learning (ICL), in which demonstrations of the task are provided to the model in natural language without any additional training. Recently, (CITATION) propose “instruction prompt tuning” (IPT), which combines PT with ICL by concatenating a natural language demonstration with learned prompt embeddings. While all of these methods have proven effective on different tasks, how they interact with each other remains unexplored. In this paper, we empirically study when and how in-context examples improve prompt tuning by measuring the effectiveness of ICL, PT, and IPT on five text generation tasks with multiple base language models. We observe that (1) IPT does <i>not</i> always outperform PT, and in fact requires the in-context demonstration to be semantically similar to the test input to yield improvements; (2) PT is unstable and exhibits high variance, but combining PT and ICL (into IPT) consistently reduces variance across all five tasks; and(3) prompts learned for a specific source task via PT exhibit positive transfer when paired with in-context examples of a different target task. Our results offer actionable insights on choosing a suitable parameter-efficient adaptation method for a given task.</abstract>
      <url hash="a17235c9">2024.findings-eacl.11</url>
      <bibkey>sun-etal-2024-context</bibkey>
      <video href="2024.findings-eacl.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Large Language Models for Psycholinguistic Plausibility Pretesting</title>
      <author><first>Samuel</first><last>Amouyal</last></author>
      <author><first>Aya</first><last>Meltzer-Asscher</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Google and Tel Aviv University</affiliation></author>
      <pages>166-181</pages>
      <abstract>In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.</abstract>
      <url hash="08a290d1">2024.findings-eacl.12</url>
      <attachment type="software" hash="d5b10b6a">2024.findings-eacl.12.software.zip</attachment>
      <attachment type="note" hash="6832e347">2024.findings-eacl.12.note.zip</attachment>
      <bibkey>amouyal-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Modeling Aspect Sentiment Coherency via Local Sentiment Aggregation</title>
      <author><first>Heng</first><last>Yang</last></author>
      <author><first>Ke</first><last>Li</last><affiliation>University of Exeter</affiliation></author>
      <pages>182-195</pages>
      <abstract>Aspect sentiment coherency is an intriguing yet underexplored topic in the field of aspect-based sentiment classification. This concept reflects the common pattern where adjacent aspects often share similar sentiments. Despite its prevalence, current studies have not fully recognized the potential of modeling aspect sentiment coherency, including its implications in adversarial defense. To model aspect sentiment coherency, we propose a novel local sentiment aggregation (LSA) paradigm based on constructing a differential-weighted sentiment aggregation window. We have rigorously evaluated our model through experiments, and the results affirm the proficiency of LSA in terms of aspect coherency prediction and aspect sentiment classification. For instance, it outperforms existing models and achieves state-of-the-art sentiment classification performance across five public datasets. Furthermore, we demonstrate the promising ability of LSA in ABSC adversarial defense, thanks to its sentiment coherency modeling. To encourage further exploration and application of this concept, we have made our code publicly accessible. This will provide researchers with a valuable tool to delve into sentiment coherency modeling in future research.</abstract>
      <url hash="e0ee34c5">2024.findings-eacl.13</url>
      <attachment type="software" hash="7abddaeb">2024.findings-eacl.13.software.zip</attachment>
      <attachment type="note" hash="7abddaeb">2024.findings-eacl.13.note.zip</attachment>
      <bibkey>yang-li-2024-modeling</bibkey>
      <video href="2024.findings-eacl.13.mp4"/>
    </paper>
    <paper id="14">
      <title>An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics</title>
      <author><first>Saba</first><last>Ahmadi</last></author>
      <author><first>Aishwarya</first><last>Agrawal</last><affiliation>Université de Montréal, Mila – Quebec AI Institute and Google DeepMind</affiliation></author>
      <pages>196-208</pages>
      <abstract>Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negation, and CLIPScore and PAC-S are insensitive to the structure of the caption to a great extent. We hope our findings will guide further improvements in reference-free evaluation of image captioning.</abstract>
      <url hash="98d0754c">2024.findings-eacl.14</url>
      <bibkey>ahmadi-agrawal-2024-examination</bibkey>
      <video href="2024.findings-eacl.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Barriers to Effective Evaluation of Simultaneous Interpretation</title>
      <author><first>Shira</first><last>Wein</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Te</first><last>I</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <author><first>Juraj</first><last>Juraska</last><affiliation>Google</affiliation></author>
      <author><first>Dirk</first><last>Padfield</last><affiliation>Google</affiliation></author>
      <author><first>Wolfgang</first><last>Macherey</last><affiliation>Google</affiliation></author>
      <pages>209-219</pages>
      <abstract>Simultaneous interpretation is an especially challenging form of translation because it requires converting speech from one language to another in real-time. Though prior work has relied on out-of-the-box machine translation metrics to evaluate interpretation data, we hypothesize that strategies common in high-quality human interpretations, such as summarization, may not be handled well by standard machine translation metrics. In this work, we examine both qualitatively and quantitatively four potential barriers to evaluation of interpretation: disfluency, summarization, paraphrasing, and segmentation. Our experiments reveal that, while some machine translation metrics correlate fairly well with human judgments of interpretation quality, much work is still needed to account for strategies of interpretation during evaluation. As a first step to address this, we develop a fine-tuned model for interpretation evaluation, and achieve better correlation with human judgments than the state-of-the-art machine translation metrics.</abstract>
      <url hash="2ec8040a">2024.findings-eacl.15</url>
      <bibkey>wein-etal-2024-barriers</bibkey>
      <video href="2024.findings-eacl.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Inconsistent dialogue responses and how to recover from them</title>
      <author><first>Mian</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Jin</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>220-230</pages>
      <abstract>One critical issue for chat systems is to stay consistent about preferences, opinions, beliefs and facts of itself, which has been shown a difficult problem. In this work, we study methods to assess and bolster utterance consistency of chat systems. A dataset is first developed for studying the inconsistencies, where inconsistent dialogue responses, explanations of the inconsistencies, and recovery utterances are authored by annotators. This covers the life span of inconsistencies, namely introduction, understanding, and resolution. Building on this, we introduce a set of tasks centered on dialogue consistency, specifically focused on its detection and resolution. Our experimental findings indicate that our dataset significantly helps the progress in identifying and resolving conversational inconsistencies, and current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.</abstract>
      <url hash="def387c7">2024.findings-eacl.16</url>
      <bibkey>zhang-etal-2024-inconsistent</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>MUG</fixed-case>: Interactive Multimodal Grounding on User Interfaces</title>
      <author><first>Tao</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Gang</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Jingjie</first><last>Zheng</last></author>
      <author><first>Purple</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Yang</first><last>Li</last><affiliation>Google</affiliation></author>
      <pages>231-251</pages>
      <abstract>We present MUG, a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen. Prior works modeled multimodal UI grounding in one round: the user gives a command and the agent responds to the command. Yet, in a realistic scenario, a user command can be ambiguous when the target action is inherently difficult to articulate in natural language. MUG allows multiple rounds of interactions such that upon seeing the agent responses, the user can give further commands for the agent to refine or even correct its actions. Such interaction is critical for improving grounding performances in real-world use cases. To investigate the problem, we create a new dataset that consists of 77,820 sequences of human user-agent interaction on mobile interfaces in which 20% involves multiple rounds of interactions. To establish benchmark, we experiment with a range of modeling variants and evaluation strategies, including both offline and online evaluation—the online strategy consists of both human evaluation and automatic with simulators. Our experiments show that iterative interaction significantly improves the absolute task completion by 18% over the entire test set and 31% over the challenging split. Our results lay the foundation for further investigation of the problem.</abstract>
      <url hash="ba7f75c0">2024.findings-eacl.17</url>
      <bibkey>li-etal-2024-mug</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>PRIL</fixed-case>o<fixed-case>RA</fixed-case>: Pruned and Rank-Increasing Low-Rank Adaptation</title>
      <author><first>Nadav</first><last>Benedek</last></author>
      <author><first>Lior</first><last>Wolf</last><affiliation>Tel Aviv University, Tel Aviv University and Tel Aviv University</affiliation></author>
      <pages>252-263</pages>
      <abstract>With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.</abstract>
      <url hash="b7def7f3">2024.findings-eacl.18</url>
      <attachment type="software" hash="300fc956">2024.findings-eacl.18.software.zip</attachment>
      <bibkey>benedek-wolf-2024-prilora</bibkey>
      <video href="2024.findings-eacl.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Revamping Multilingual Agreement Bidirectionally via Switched Back-translation for Multilingual Neural Machine Translation</title>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Haoyang</first><last>Huang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>264-275</pages>
      <abstract>Despite the fact that multilingual agreement (MA) has shown its importance for multilingual neural machine translation (MNMT), current methodologies in the field have two shortages: (i) require parallel data between multiple language pairs, which is not always realistic and (ii) optimize the agreement in an ambiguous direction, which hampers the translation performance. We present <b>B</b>idirectional <b>M</b>ultilingual <b>A</b>greement via <b>S</b>witched <b>B</b>ack-<b>t</b>ranslation (<b>BMA-SBT</b>), a novel and universal multilingual agreement framework for fine-tuning pre-trained MNMT models, which (i) exempts the need for aforementioned parallel data by using a novel method called switched BT that creates synthetic text written in another source language using the translation target and (ii) optimizes the agreement bidirectionally with the Kullback-Leibler Divergence loss. Experiments indicate that BMA-SBT clearly improves the strong baselines on the task of MNMT with three benchmarks: TED Talks, News, and Europarl. In-depth analyzes indicate that BMA-SBT brings additive improvements to the conventional BT method.</abstract>
      <url hash="05159d1d">2024.findings-eacl.19</url>
      <bibkey>lu-etal-2024-revamping</bibkey>
    </paper>
    <paper id="20">
      <title>m<fixed-case>PLM</fixed-case>-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Chengzhi</first><last>Hu</last></author>
      <author><first>Zheyu</first><last>Zhang</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>276-310</pages>
      <abstract>Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whether mPLM-Sim is effective for zero-shot cross-lingual transfer by conducting experiments on both low-level syntactic tasks and high-level semantic tasks. The experimental results demonstrate that mPLM-Sim is capable of selecting better source languages than linguistic measures, resulting in a 1%-2% improvement in zero-shot cross-lingual transfer performance.</abstract>
      <url hash="accddcb1">2024.findings-eacl.20</url>
      <bibkey>lin-etal-2024-mplm</bibkey>
      <video href="2024.findings-eacl.20.mp4"/>
    </paper>
    <paper id="21">
      <title><fixed-case>OYXOY</fixed-case>: A <fixed-case>M</fixed-case>odern <fixed-case>NLP</fixed-case> Test Suite for <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Eirini</first><last>Giannikouri</last><affiliation>University of Crete</affiliation></author>
      <author><first>Vasiliki</first><last>Katsouli</last><affiliation>University of Crete</affiliation></author>
      <author><first>Christina</first><last>Klironomou</last></author>
      <author><first>Christina</first><last>Koula</last></author>
      <author><first>Dimitris</first><last>Papadakis</last></author>
      <author><first>Thelka</first><last>Pasparaki</last></author>
      <author><first>Erofili</first><last>Psaltaki</last><affiliation>University of Crete</affiliation></author>
      <author><first>Efthymia</first><last>Sakellariou</last><affiliation>University of Crete</affiliation></author>
      <author><first>Charikleia</first><last>Soupiona</last></author>
      <pages>311-322</pages>
      <abstract>This paper serves as a foundational step towards the development of a linguistically motivated and technically relevant evaluation suite for Greek NLP. We initiate this endeavor by introducing four expert-verified evaluation tasks, specifically targeted at natural language inference, word sense disambiguation (through example comparison or sense selection) and metaphor detection. More than language-adapted replicas of existing tasks, we contribute two innovations which will resonate with the broader resource and evaluation community. Firstly, our inference dataset is the first of its kind, marking not just one, but rather all possible inference labels, accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we demonstrate a cost-efficient method to obtain datasets for under-resourced languages. Using ChatGPT as a language-neutral parser, we transform the Dictionary of Standard Modern Greek into a structured format, from which we derive the other three tasks through simple projections. Alongside each task, we conduct experiments using currently available state of the art machinery. Our experimental baselines affirm the challenging nature of our tasks and highlight the need for expedited progress in order for the Greek NLP ecosystem to keep pace with contemporary mainstream research.</abstract>
      <url hash="e3c361c4">2024.findings-eacl.21</url>
      <attachment type="software" hash="5bafd75d">2024.findings-eacl.21.software.zip</attachment>
      <attachment type="note" hash="eb900342">2024.findings-eacl.21.note.zip</attachment>
      <bibkey>kogkalidis-etal-2024-oyxoy</bibkey>
    </paper>
    <paper id="22">
      <title>A Comprehensive Evaluation of Inductive Reasoning Capabilities and Problem Solving in Large Language Models</title>
      <author><first>Chen</first><last>Bowen</last></author>
      <author><first>Rune</first><last>Sætre</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>323-339</pages>
      <abstract>Inductive reasoning is fundamental to both human and artificial intelligence. The inductive reasoning abilities of current Large Language Models (LLMs) are evaluated in this research.We argue that only considering induction of rules is too narrow and unrealistic, since inductive reasoning is usually mixed with other abilities, like rules application, results/rules validation, and updated information integration.We probed the LLMs with a set of designed symbolic tasks and found that even state-of-the-art (SotA) LLMs fail significantly, showing the inability of LLMs to perform these intuitively simple tasks.Furthermore, we found that perfect accuracy in a small-size problem does not guarantee the same accuracy in a larger-size version of the same problem, provoking the question of how we can assess the LLMs’ actual problem-solving capabilities.We also argue that Chain-of-Thought prompts help the LLMs by decomposing the problem-solving process, but the LLMs still learn limitedly.Furthermore, we reveal that few-shot examples assist LLM generalization in out-of-domain (OOD) cases, albeit limited. The LLM starts to fail when the problem deviates from the provided few-shot examples.</abstract>
      <url hash="85cb68ea">2024.findings-eacl.22</url>
      <attachment type="software" hash="2f3e1abc">2024.findings-eacl.22.software.zip</attachment>
      <bibkey>bowen-etal-2024-comprehensive</bibkey>
      <video href="2024.findings-eacl.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Towards efficient self-supervised representation learning in speech processing</title>
      <author><first>Luis</first><last>Lugo</last></author>
      <author><first>Valentin</first><last>Vielzeuf</last><affiliation>Orange-labs</affiliation></author>
      <pages>340-346</pages>
      <abstract>Self-supervised learning has achieved impressive results in speech processing, but current models are computationally expensive, generating environmental concerns because of their high energy consumption. Therefore, we propose an efficient self-supervised approach to address high computational costs, using a single GPU during 24 to 48 hours of pretraining. The proposed approach combines linear, convolutional, and self-attention layers with several optimizations, including dynamic batching, flash attention, mixed-precision training, gradient accumulation, and acoustic feature extraction with input preprocessing. Computational cost estimations for our proposed model represent up to two orders of magnitude improvements in computational efficiency against existing speech models.</abstract>
      <url hash="7d046408">2024.findings-eacl.23</url>
      <bibkey>lugo-vielzeuf-2024-towards</bibkey>
    </paper>
    <paper id="24">
      <title>Improving Cross-Domain Low-Resource Text Generation through <fixed-case>LLM</fixed-case> Post-Editing: A Programmer-Interpreter Approach</title>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Levon</first><last>Haroutunian</last><affiliation>Openstream, Inc.</affiliation></author>
      <author><first>Raj</first><last>Tumuluri</last><affiliation>Openstream Inc</affiliation></author>
      <author><first>Philip</first><last>Cohen</last></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>347-354</pages>
      <abstract>Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs while editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5’s performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.</abstract>
      <url hash="f39ab06e">2024.findings-eacl.24</url>
      <bibkey>li-etal-2024-improving-cross</bibkey>
    </paper>
    <paper id="25">
      <title>Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition</title>
      <author><first>Tu</first><last>Nguyen</last><affiliation>Huawei R&amp;D Munich</affiliation></author>
      <author><first>Nedim</first><last>Šrndić</last><affiliation>Huawei Technologies Duesseldorf GmbH</affiliation></author>
      <author><first>Alexander</first><last>Neth</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>355-373</pages>
      <abstract>Techniques, Tactics and Procedures (TTP) mapping is an important and difficult task in the application of cyber threat intelligence (CTI) extraction for threat reports. TTPs are typically expressed in semantic forms within security knowledge bases like MITRE ATT&amp;CK, serving as textual high-level descriptions for sophisticated attack patterns. Conversely, attacks in CTI threat reports are detailed in a combination of natural and technical language forms, presenting a significant challenge even for security experts to establish correlations or mappings with the corresponding TTPs.Conventional learning approaches often target the TTP mapping problem in the classical multiclass/label classification setting. This setting hinders the learning capabilities of the model, due to the large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. In this work, we approach the problem in a different learning paradigm, such that the assignment of a text to a TTP label is essentially decided by the direct semantic similarity between the two, thus, reducing the complexity of competing solely over the large labeling space. In order that, we propose a neural matching architecture that incorporates a sampling based learn-to-compare mechanism, facilitating the learning process of the matching model despite constrained resources.</abstract>
      <url hash="5bf9f38e">2024.findings-eacl.25</url>
      <attachment type="note" hash="c0b2ed72">2024.findings-eacl.25.note.tgz</attachment>
      <bibkey>nguyen-etal-2024-noise</bibkey>
      <video href="2024.findings-eacl.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Large Language Models for Scientific Information Extraction: An Empirical Study for Virology</title>
      <author><first>Mahsa</first><last>Shamsabadi</last><affiliation>TIB Hannover</affiliation></author>
      <author><first>Jennifer</first><last>D’Souza</last><affiliation>TIB Hannover</affiliation></author>
      <author><first>Sören</first><last>Auer</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>374-392</pages>
      <abstract>In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs’ emergent abilities.For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.</abstract>
      <url hash="41888965">2024.findings-eacl.26</url>
      <attachment type="software" hash="bce15d0c">2024.findings-eacl.26.software.zip</attachment>
      <attachment type="note" hash="27bd030b">2024.findings-eacl.26.note.zip</attachment>
      <bibkey>shamsabadi-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Re3val: Reinforced and Reranked Generative Retrieval</title>
      <author><first>EuiYul</first><last>Song</last><affiliation>Samsung Electronics</affiliation></author>
      <author><first>Sangryul</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Haeju</first><last>Lee</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Joonkee</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>393-409</pages>
      <abstract>Generative retrieval models encode pointers to information in a corpus as an index within the model’s parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can’t be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from the KILT database using the rerank page titles. Upon grounding the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other generative retrieval models across five KILT datasets.</abstract>
      <url hash="9e63f5ec">2024.findings-eacl.27</url>
      <bibkey>song-etal-2024-re3val</bibkey>
      <video href="2024.findings-eacl.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Entity Linking in the Job Market Domain</title>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>410-419</pages>
      <abstract>In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention–skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@k).</abstract>
      <url hash="21d3a5dd">2024.findings-eacl.28</url>
      <bibkey>zhang-etal-2024-entity</bibkey>
      <video href="2024.findings-eacl.28.mp4"/>
    </paper>
    <paper id="29">
      <title>(Chat)<fixed-case>GPT</fixed-case> v <fixed-case>BERT</fixed-case> Dawn of Justice for Semantic Change Detection</title>
      <author><first>Francesco</first><last>Periti</last><affiliation>University of Milan</affiliation></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>Göteborg University</affiliation></author>
      <pages>420-436</pages>
      <abstract>In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.</abstract>
      <url hash="248fa74e">2024.findings-eacl.29</url>
      <attachment type="software" hash="58309920">2024.findings-eacl.29.software.zip</attachment>
      <attachment type="note" hash="6ee98e73">2024.findings-eacl.29.note.zip</attachment>
      <bibkey>periti-etal-2024-chat</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Unified Uni- and Multi-modal News Headline Generation</title>
      <author><first>Mateusz</first><last>Krubiński</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>437-450</pages>
      <abstract>Thanks to the recent progress in vision-language modeling and the evolving nature of news consumption, the tasks of automatic summarization and headline generation based on multimodal news articles have been gaining popularity. One of the limitations of the current approaches is caused by the commonly used sophisticated modular architectures built upon hierarchical cross-modal encoders and modality-specific decoders, which restrict the model’s applicability to specific data modalities – once trained on, e.g., text+video pairs there is no straightforward way to apply the model to text+image or text-only data. In this work, we propose a unified task formulation that utilizes a simple encoder-decoder model to generate headlines from uni- and multi-modal news articles. This model is trained jointly on data of several modalities and extends the textual decoder to handle the multimodal output.</abstract>
      <url hash="3aed0449">2024.findings-eacl.30</url>
      <bibkey>krubinski-pecina-2024-towards</bibkey>
      <video href="2024.findings-eacl.30.mp4"/>
    </paper>
    <paper id="31">
      <title>On the Relationship between Sentence Analogy Identification and Sentence Structure Encoding in Large Language Models</title>
      <author><first>Thilini</first><last>Wijesiriwardene</last></author>
      <author><first>Ruwan</first><last>Wickramarachchi</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Aishwarya Naresh</first><last>Reganti</last><affiliation>Amazon</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Stanford University</affiliation></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <author><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>451-457</pages>
      <abstract>The ability of Large Language Models (LLMs) to encode syntactic and semantic structures of language is well examined in NLP. Additionally, analogy identification, in the form of word analogies are extensively studied in the last decade of language modeling literature. In this work we specifically look at how LLMs’ abilities to capture sentence analogies (sentences that convey analogous meaning to each other) vary with LLMs’ abilities to encode syntactic and semantic structures of sentences. Through our analysis, we find that LLMs’ ability to identify sentence analogies is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.</abstract>
      <url hash="6fe2b6ac">2024.findings-eacl.31</url>
      <bibkey>wijesiriwardene-etal-2024-relationship</bibkey>
      <video href="2024.findings-eacl.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Contextualization Distillation from Large Language Model for Knowledge Graph Completion</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Tianlong</first><last>Chen</last></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>458-477</pages>
      <abstract>While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the <i>Contextualization Distillation</i> strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks—reconstruction and contextualization—allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into how to generate high-quality corpora for KGC, as well as the selection of suitable distillation tasks.</abstract>
      <url hash="4b81611d">2024.findings-eacl.32</url>
      <bibkey>li-etal-2024-contextualization</bibkey>
      <revision id="1" href="2024.findings-eacl.32v1" hash="7f273319"/>
      <revision id="2" href="2024.findings-eacl.32v2" hash="4b81611d" date="2024-03-30">This revision corrects the citation display problem in the Appendix.</revision>
      <video href="2024.findings-eacl.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Differentially Private Natural Language Models: Recent Advances and Future Directions</title>
      <author><first>Lijie</first><last>Hu</last><affiliation>KAUST</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Universität Paderborn</affiliation></author>
      <author><first>Lei</first><last>Shen</last><affiliation>JD AI Research, Beijing, China</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>478-499</pages>
      <abstract>Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP andpresent its recent developments from three aspects: gradient perturbation based methods, embedding vector perturbation based methods, and ensemble model based methods. We also discuss some challenges and future directions.</abstract>
      <url hash="25234848">2024.findings-eacl.33</url>
      <bibkey>hu-etal-2024-differentially</bibkey>
      <video href="2024.findings-eacl.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Learning to Compare Financial Reports for Financial Forecasting</title>
      <author><first>Ross</first><last>Koval</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Nicholas</first><last>Andrews</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>500-512</pages>
      <abstract>Public companies in the US are required to publish annual reports that detail their recent financial performance, present the current state of ongoing business operations, and discuss future prospects. However, they typically contain over 25,000 words across all sections, large amounts of industry and legal jargon, and a high percentage of boilerplate content that does not change much year-to-year. These unique characteristics present challenges for many generic pretrained language models because it is likely that only a small percentage of the long report that reflects salient information contains meaningful signal about the future prospects of the company. In this work, we curate a large-scale dataset of paired financial reports and introduce two novel, challenging tasks of predicting long-horizon company risk and correlation that evaluate the ability of the model to recognize cross-document relationships with complex, nuanced signals. We explore and present a comprehensive set of methods and experiments, and establish strong baselines designed to learn to identify subtle similarities and differences between long documents. Furthermore, we demonstrate that it is possible to predict company risk and correlation solely from the text of their financial reports and further that modeling the cross-document interactions at a fine-grained level provides significant benefit. Finally, we probe the best performing model through quantitative and qualitative interpretability methods to reveal some insight into the underlying task signal.</abstract>
      <url hash="d6898770">2024.findings-eacl.34</url>
      <bibkey>koval-etal-2024-learning</bibkey>
    </paper>
    <paper id="35">
      <title>Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation</title>
      <author><first>Shohei</first><last>Higashiyama</last><affiliation>Nara Institute of Science and Technology, Japan and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Hiroki</first><last>Teranishi</last><affiliation>RIKEN</affiliation></author>
      <author><first>Hiroyuki</first><last>Otomo</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Yusuke</first><last>Ide</last></author>
      <author><first>Aitaro</first><last>Yamamoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroyuki</first><last>Shindo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yuki</first><last>Matsuda</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Ikuya</first><last>Yamada</last><affiliation>RIKEN and Studio Ousia</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>513-532</pages>
      <abstract>Geoparsing is a fundamental technique for analyzing geo-entity information in text, which is useful for geographic applications, e.g., tourist spot recommendation. We focus on document-level geoparsing that considers geographic relatedness among geo-entity mentions and present a Japanese travelogue dataset designed for training and evaluating document-level geoparsing systems. Our dataset comprises 200 travelogue documents with rich geo-entity information: 12,171 mentions, 6,339 coreference clusters, and 2,551 geo-entities linked to geo-database entries.</abstract>
      <url hash="feb0bd10">2024.findings-eacl.35</url>
      <bibkey>higashiyama-etal-2024-arukikata</bibkey>
    </paper>
    <paper id="36">
      <title>Knowledge Generation for Zero-shot Knowledge-based <fixed-case>VQA</fixed-case></title>
      <author><first>Rui</first><last>Cao</last></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>533-549</pages>
      <abstract>Previous solutions to knowledge-based visual question answering (K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model.Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results.However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.</abstract>
      <url hash="fbd74890">2024.findings-eacl.36</url>
      <attachment type="software" hash="6164d06c">2024.findings-eacl.36.software.zip</attachment>
      <bibkey>cao-jiang-2024-knowledge</bibkey>
      <video href="2024.findings-eacl.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Simple Temperature Cool-down in Contrastive Framework for Unsupervised Sentence Representation Learning</title>
      <author><first>Yoo Hyun</first><last>Jeong</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Myeong Soo</first><last>Han</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>550-559</pages>
      <abstract>In this paper, we proposes a simple, tricky method to improve sentence representation of unsupervised contrastive learning. Even though contrastive learning has achieved great performances in both visual representation learning (VRL) and sentence representation learning (SRL) fields, we focus on the fact that there is a gap between characteristics and training dynamics of VRL and SRL. We first examine the role of temperature to bridge the gap between VRL and SRL, and find some temperature-dependent elements in SRL; <i>i.e.</i>, a higher temperature causes overfitting of the uniformity while improving the alignment in earlier phase of training. Then, we design a <i>temperature cool-down</i> technique based on this observation, which helps PLMs to be more suitable for contrastive learning via preparation of uniform representation space. Our experimental results on widely-utilized benchmarks demonstrate the effectiveness and extensiblity of our method.</abstract>
      <url hash="13d445d4">2024.findings-eacl.37</url>
      <bibkey>jeong-etal-2024-simple</bibkey>
      <video href="2024.findings-eacl.37.mp4"/>
    </paper>
    <paper id="38">
      <title>Bootstrap Your Own <fixed-case>PLM</fixed-case>: Boosting Semantic Features of <fixed-case>PLM</fixed-case>s for Unsuperivsed Contrastive Learning</title>
      <author><first>Yoo Hyun</first><last>Jeong</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Myeong Soo</first><last>Han</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>560-569</pages>
      <abstract>This paper aims to investigate the possibility of exploiting original semantic features of PLMs (pre-trained language models) during contrastive learning in the context of SRL (sentence representation learning). In the context of feature modification, we identified a method called IFM (implicit feature modification), which reduces the tendency of contrastive models for VRL (visual representation learning) to rely on feature-suppressing shortcut solutions. We observed that IFM did not work well for SRL, which may be due to differences between the nature of VRL and SRL. We propose BYOP, which boosts well-represented features, taking the opposite idea of IFM, under the assumption that SimCSE’s dropout-noise-based augmentation may be too simple to modify high-level semantic features, and that the features learned by PLMs are semantically meaningful and should be boosted, rather than removed. Extensive experiments lend credence to the logic of BYOP, which considers the nature of SRL.</abstract>
      <url hash="10ca921c">2024.findings-eacl.38</url>
      <bibkey>jeong-etal-2024-bootstrap</bibkey>
      <video href="2024.findings-eacl.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Personalized Abstractive Summarization by Tri-agent Generation Pipeline</title>
      <author><first>Wen</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yujia</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>, University of British Columbia</affiliation></author>
      <author><first>Pengcheng</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>570-581</pages>
      <abstract>Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities. In this paper, we propose a tri-agent generation pipeline comprising a generator, an instructor, and an editor to enhance output personalization. The generator produces an initial output, the instructor automatically generates editing instructions based on user preferences, and the editor refines the output to align with those preferences. The inference-only large language model (ChatGPT) serves as both the generator and editor, with a smaller model acting as the instructor to guide output generation. We train the instructor using editor-steered reinforcement learning, leveraging feedback from a large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better meet user expectations.</abstract>
      <url hash="17757746">2024.findings-eacl.39</url>
      <bibkey>xiao-etal-2024-personalized</bibkey>
      <video href="2024.findings-eacl.39.mp4"/>
    </paper>
    <paper id="40">
      <title>Revisiting the <fixed-case>M</fixed-case>arkov Property for Machine Translation</title>
      <author><first>Cunxiao</first><last>Du</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>582-588</pages>
      <abstract>In this paper, we re-examine the Markov property in the context of neural machine translation. We design a Markov Autoregressive Transformer (MAT) and undertake a comprehensive assessment of its performance across four WMT benchmarks. Our findings indicate that MAT with an order larger than 4 can generate translations with quality on par with that of conventional autoregressive transformers. In addition, counter-intuitively, we also find that the advantages of utilizing a higher-order MAT do not specifically contribute to the translation of longer sentences.</abstract>
      <url hash="df419ec5">2024.findings-eacl.40</url>
      <bibkey>du-etal-2024-revisiting</bibkey>
      <video href="2024.findings-eacl.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Reward Engineering for Generating Semi-structured Explanation</title>
      <author><first>Jiuzhou</first><last>Han</last></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <pages>589-602</pages>
      <abstract>Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is utilised and supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model’s true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research. Our proposed method on two semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new state-of-the-art results.</abstract>
      <url hash="b3d3ccc3">2024.findings-eacl.41</url>
      <bibkey>han-etal-2024-reward</bibkey>
      <video href="2024.findings-eacl.41.mp4"/>
    </paper>
    <paper id="42">
      <title>Towards Context-Based Violence Detection: A <fixed-case>K</fixed-case>orean Crime Dialogue Dataset</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Heuiyeen</first><last>Yeen</last></author>
      <author><first>Myoung-Wan</first><last>Koo</last><affiliation>Sogang University</affiliation></author>
      <pages>603-623</pages>
      <abstract>In order to enhance the security of society, there is rising interest in artificial intelligence (AI) to help detect and classify in advanced violence in daily life. The field of violence detection has introduced various datasets, yet context-based violence detection predominantly focuses on vision data, with a notable lack of NLP datasets. To overcome this, this paper presents the first Korean dialogue dataset for classifying violence that occurs in online settings: the Korean Crime Dialogue Dataset (KCDD). KCDD contains 22,249 dialogues created by crowd workers assuming offline scenarios. It has four criminal classes that meet international legal standards and one clean class (Serious Threats, Extortion or Blackmail, Harassment in the Workplace, Other Harassment, and Clean Dialogue). Plus, we propose a strong baseline for the proposed dataset, Relationship-Aware BERT. The model shows that understanding varying relationships among interlocutors improves the performance of crime dialogue classification. We hope that the proposed dataset will be used to detect cases of violence and aid people in danger. The KCDD dataset and corresponding baseline implementations can be found at the following link: <url>https://sites.google.com/view/kcdd</url>.</abstract>
      <url hash="f6301672">2024.findings-eacl.42</url>
      <attachment type="software" hash="e13f50ad">2024.findings-eacl.42.software.zip</attachment>
      <attachment type="note" hash="0af93109">2024.findings-eacl.42.note.zip</attachment>
      <bibkey>kim-etal-2024-towards</bibkey>
    </paper>
    <paper id="43">
      <title>Capturing the Relationship Between Sentence Triplets for <fixed-case>LLM</fixed-case> and Human-Generated Texts to Enhance Sentence Embeddings</title>
      <author><first>Na Min</first><last>An</last><affiliation>KAIST</affiliation></author>
      <author><first>Sania</first><last>Waheed</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>624-638</pages>
      <abstract>Deriving meaningful sentence embeddings is crucial in capturing the semantic relationship between texts. Recent advances in building sentence embedding models have centered on replacing traditional human-generated text datasets with those generated by LLMs. However, the properties of these widely used LLM-generated texts remain largely unexplored. Here, we evaluate the quality of the LLM-generated texts from four perspectives (Positive Text Repetition, Length Difference Penalty, Positive Score Compactness, and Negative Text Implausibility) and find that there exists an inherent difference between human and LLM-generated datasets. To further enhance sentence embeddings using both human and LLM-generated datasets, we propose a novel loss function that incorporates Positive-Negative sample Augmentation (PNA) within the contrastive learning objective. Our results demonstrate that PNA effectively mitigates the sentence anisotropy problem in Wikipedia corpus (-7% compared to CLHAIF) and simultaneously improves the Spearman’s correlation in standard Semantic Textual Similarity (STS) tasks (+1.47% compared to CLHAIF).</abstract>
      <url hash="5023b599">2024.findings-eacl.43</url>
      <bibkey>an-etal-2024-capturing</bibkey>
      <video href="2024.findings-eacl.43.mp4"/>
    </paper>
    <paper id="44">
      <title>Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed Response Generation in Dialogues</title>
      <author><first>Shivani</first><last>Kumar</last><affiliation>Indraprastha Institute of Information Technology, Delhi, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>639-653</pages>
      <abstract>Code-mixing, the blending of multiple languages within a single conversation, introduces a distinctive challenge, particularly in the context of response generation. Capturing the intricacies of code-mixing proves to be a formidable task, given the wide-ranging variations influenced by individual speaking styles and cultural backgrounds. In this study, we explore response generation within code-mixed conversations. We introduce a novel approach centered on harnessing the Big Five personality traits acquired in an unsupervised manner from the conversations to bolster the performance of response generation. These inferred personality attributes are seamlessly woven into the fabric of the dialogue context, using a novel fusion mechanism, . It uses an effective two-step attention formulation to fuse the dialogue and personality information. This fusion not only enhances the contextual relevance of generated responses but also elevates the overall performance of the model. Our experimental results, grounded in a dataset comprising of multi-party Hindi-English code-mix conversations, highlight the substantial advantages offered by personality-infused models over their conventional counterparts. This is evident in the increase observed in ROUGE and BLUE scores for the response generation task when the identified personality is seamlessly integrated into the dialogue context. Qualitative assessment for personality identification and response generation aligns well with our quantitative results.</abstract>
      <url hash="6210c1f5">2024.findings-eacl.44</url>
      <bibkey>kumar-chakraborty-2024-harmonizing</bibkey>
      <video href="2024.findings-eacl.44.mp4"/>
    </paper>
    <paper id="45">
      <title>Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning</title>
      <author><first>Jeongwoo</first><last>Park</last></author>
      <author><first>Enrico</first><last>Liscio</last></author>
      <author><first>Pradeep</first><last>Murukannaiah</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>654-673</pages>
      <abstract>Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.</abstract>
      <url hash="7a5007bf">2024.findings-eacl.45</url>
      <attachment type="software" hash="935b473d">2024.findings-eacl.45.software.zip</attachment>
      <bibkey>park-etal-2024-morality</bibkey>
      <video href="2024.findings-eacl.45.mp4"/>
    </paper>
    <paper id="46">
      <title>Prosody in Cascade and Direct Speech-to-Text Translation: a case study on <fixed-case>K</fixed-case>orean Wh-Phrases</title>
      <author><first>Giulio</first><last>Zhou</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <pages>674-683</pages>
      <abstract>Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it’s a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories. To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody. The code for our evaluation is openly accessible and freely available for review and utilisation.</abstract>
      <url hash="5135cf37">2024.findings-eacl.46</url>
      <bibkey>zhou-etal-2024-prosody</bibkey>
      <video href="2024.findings-eacl.46.mp4"/>
    </paper>
    <paper id="47">
      <title>Exploring the Potential of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations</title>
      <author><first>Chunkit</first><last>Chan</last></author>
      <author><first>Cheng</first><last>Jiayang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>684-721</pages>
      <abstract>This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT’s promising performance across various tasks, we proceed to carry out thorough evaluations on the whole test sets of 11 datasets, including temporal and causal relations, PDTB2.0-based, and dialogue-based discourse relations. To ensure the reliability of our findings, we employ three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. Through our study, we discover that ChatGPT exhibits exceptional proficiency in detecting and reasoning about causal relations, albeit it may not possess the same level of expertise in identifying the temporal order between two events. While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.</abstract>
      <url hash="375a167d">2024.findings-eacl.47</url>
      <bibkey>chan-etal-2024-exploring</bibkey>
      <video href="2024.findings-eacl.47.mp4"/>
    </paper>
    <paper id="48">
      <title>Backtracing: Retrieving the Cause of the Query</title>
      <author><first>Rose</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Pawan</first><last>Wirawarn</last></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Noah</first><last>Goodman</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <pages>722-735</pages>
      <abstract>Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators—such as lecturers who want to improve their content—identify segments that caused a user to ask those questions.We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query.We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain.We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT.While traditional IR systems retrieve semantically relevant information (e.g., details on “projection matrices” for a query “does projecting multiple times still lead to the same point?”), they often miss the causally relevant context (e.g., the lecturer states “projecting twice gets me the same answer as one projection”). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches.We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries.</abstract>
      <url hash="b176d238">2024.findings-eacl.48</url>
      <bibkey>wang-etal-2024-backtracing</bibkey>
      <video href="2024.findings-eacl.48.mp4"/>
    </paper>
    <paper id="49">
      <title>Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling</title>
      <author><first>Chao-Wei</first><last>Huang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chen-An</first><last>Li</last><affiliation>Department of computer science and informational engineering, National Taiwan University</affiliation></author>
      <author><first>Tsu-Yuan</first><last>Hsu</last></author>
      <author><first>Chen-Yu</first><last>Hsu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>Department of Computer Science and Informational Engineering, National Taiwan University</affiliation></author>
      <pages>736-746</pages>
      <abstract>Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces <tex-math>\textbf{UMR}</tex-math>, an <tex-math>\underline{U}</tex-math>nsupervised <tex-math>\underline{M}</tex-math>ultilingual dense <tex-math>\underline{R}</tex-math>etriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. All of our source code, data, and models are available: https://github.com/MiuLab/UMR</abstract>
      <url hash="9afec971">2024.findings-eacl.49</url>
      <attachment type="software" hash="a6b3b82e">2024.findings-eacl.49.software.zip</attachment>
      <bibkey>huang-etal-2024-unsupervised</bibkey>
      <video href="2024.findings-eacl.49.mp4"/>
    </paper>
    <paper id="50">
      <title>Investigating grammatical abstraction in language models using few-shot learning of novel noun gender</title>
      <author><first>Priyanka</first><last>Sukumaran</last></author>
      <author><first>Conor</first><last>Houghton</last><affiliation>University of Bristol</affiliation></author>
      <author><first>Nina</first><last>Kazanina</last><affiliation>University of Bristol</affiliation></author>
      <pages>747-765</pages>
      <abstract>Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word-embedding space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.</abstract>
      <url hash="8b217871">2024.findings-eacl.50</url>
      <bibkey>sukumaran-etal-2024-investigating</bibkey>
    </paper>
    <paper id="51">
      <title>On-the-fly Denoising for Data Augmentation in Natural Language Understanding</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Zoom</affiliation></author>
      <author><first>Fangyu</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>766-781</pages>
      <abstract>Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically.However, data augmentation may introduce noisy data that impairs training.To guarantee the quality of augmented data,existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out “noisy” data.However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals.In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data.To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consistent across two distinct dropouts.Our method can be applied to general augmentation techniques and consistently improve the performance on both text classification and question-answering tasks.</abstract>
      <url hash="def6cbc2">2024.findings-eacl.51</url>
      <attachment type="software" hash="f0ea4f26">2024.findings-eacl.51.software.zip</attachment>
      <attachment type="note" hash="7205c223">2024.findings-eacl.51.note.zip</attachment>
      <bibkey>fang-etal-2024-fly</bibkey>
      <video href="2024.findings-eacl.51.mp4"/>
    </paper>
    <paper id="52">
      <title>Style Vectors for Steering Generative Large Language Models</title>
      <author><first>Kai</first><last>Konen</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sophie</first><last>Jentzsch</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Diaoulé</first><last>Diallo</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Peer</first><last>Schütt</last><affiliation>German Aerospace Center</affiliation></author>
      <author><first>Oliver</first><last>Bensch</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Roxanne</first><last>El Baff</last><affiliation>German Aerospace Center and Bauhaus-University Weimar</affiliation></author>
      <author><first>Dominik</first><last>Opitz</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Tobias</first><last>Hecking</last><affiliation>German Aerospace Center</affiliation></author>
      <pages>782-802</pages>
      <abstract>This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.</abstract>
      <url hash="a02d4eb5">2024.findings-eacl.52</url>
      <attachment type="software" hash="29f876dd">2024.findings-eacl.52.software.zip</attachment>
      <bibkey>konen-etal-2024-style</bibkey>
      <video href="2024.findings-eacl.52.mp4"/>
    </paper>
    <paper id="53">
      <title>Consistent Joint Decision-Making with Heterogeneous Learning Models</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>803-813</pages>
      <abstract>This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming(ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions’ prior probability, confidence (uncertainty), and the models’ expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.</abstract>
      <url hash="4dc934db">2024.findings-eacl.53</url>
      <bibkey>rajaby-faghihi-kordjamshidi-2024-consistent</bibkey>
      <video href="2024.findings-eacl.53.mp4"/>
    </paper>
    <paper id="54">
      <title>Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage</title>
      <author><first>Hanyin</first><last>Shao</last></author>
      <author><first>Jie</first><last>Huang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Shen</first><last>Zheng</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>814-825</pages>
      <abstract>The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.</abstract>
      <url hash="0e1d4951">2024.findings-eacl.54</url>
      <bibkey>shao-etal-2024-quantifying</bibkey>
    </paper>
    <paper id="55">
      <title>Probing Critical Learning Dynamics of <fixed-case>PLM</fixed-case>s for Hate Speech Detection</title>
      <author><first>Sarah</first><last>Masud</last><affiliation>Indraprastha Institute of Information Technology Delhi (IIIT-Delhi)</affiliation></author>
      <author><first>Mohammad Aflah</first><last>Khan</last></author>
      <author><first>Vikram</first><last>Goyal</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>826-845</pages>
      <abstract>Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs’ use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.</abstract>
      <url hash="9b9e33fb">2024.findings-eacl.55</url>
      <attachment type="software" hash="5e4d61a2">2024.findings-eacl.55.software.zip</attachment>
      <bibkey>masud-etal-2024-probing</bibkey>
      <video href="2024.findings-eacl.55.mp4"/>
    </paper>
    <paper id="56">
      <title>Embible: Reconstruction of <fixed-case>A</fixed-case>ncient <fixed-case>H</fixed-case>ebrew and <fixed-case>A</fixed-case>ramaic Texts Using Transformers</title>
      <author><first>Niv</first><last>Fono</last></author>
      <author><first>Harel</first><last>Moshayof</last></author>
      <author><first>Eldar</first><last>Karol</last></author>
      <author><first>Itai</first><last>Assraf</last></author>
      <author><first>Mark</first><last>Last</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>846-852</pages>
      <abstract>Hebrew and Aramaic inscriptions serve as an essential source of information on the ancient history of the Near East. Unfortunately, some parts of the inscribed texts become illegible over time. Special experts, called epigraphists, use time-consuming manual procedures to estimate the missing content. This problem can be considered an extended masked language modeling task, where the damaged content can comprise single characters, character n-grams (partial words), single complete words, and multi-word n-grams.This study is the first attempt to apply the masked language modeling approach to corrupted inscriptions in Hebrew and Aramaic languages, both using the Hebrew alphabet consisting mostly of consonant symbols. In our experiments, we evaluate several transformer-based models, which are fine-tuned on the Biblical texts and tested on three different percentages of randomly masked parts in the testing corpus. For any masking percentage, the highest text completion accuracy is obtained with a novel ensemble of word and character prediction models.</abstract>
      <url hash="a4fcfdf5">2024.findings-eacl.56</url>
      <bibkey>fono-etal-2024-embible</bibkey>
      <video href="2024.findings-eacl.56.mp4"/>
    </paper>
    <paper id="57">
      <title>Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling</title>
      <author><first>Qingyang</first><last>Wu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>853-867</pages>
      <abstract>Transformer models have achieved great performance in dialogue generation tasks. However, their inability to process long dialogue history often leads to truncation of the context. To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. The new model incorporates a separate memory module alongside the pre-trained transformer, which can effectively interchange information between the memory states and the current input context. We evaluate the efficiency of our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.</abstract>
      <url hash="98a3579c">2024.findings-eacl.57</url>
      <attachment type="software" hash="83a1986c">2024.findings-eacl.57.software.zip</attachment>
      <bibkey>wu-yu-2024-stateful</bibkey>
    </paper>
    <paper id="58">
      <title>The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models</title>
      <author><first>Anton</first><last>Razzhigaev</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Matvey</first><last>Mikhalchuk</last><affiliation>Artificial Intelligence Research Institute (AIRI) and Lomonosov Moscow State University</affiliation></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute and Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>Samara National Research University</affiliation></author>
      <pages>868-874</pages>
      <abstract>In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. This fact is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.</abstract>
      <url hash="3436b782">2024.findings-eacl.58</url>
      <bibkey>razzhigaev-etal-2024-shape</bibkey>
      <video href="2024.findings-eacl.58.mp4"/>
    </paper>
    <paper id="59">
      <title><fixed-case>MED</fixed-case>s for <fixed-case>PET</fixed-case>s: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms</title>
      <author><first>Patrick</first><last>Lee</last></author>
      <author><first>Alain</first><last>Chirino Trujillo</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Diana</first><last>Cuevas Plancarte</last></author>
      <author><first>Olumide</first><last>Ojo</last></author>
      <author><first>Xinyi</first><last>Liu</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Iyanuoluwa</first><last>Shode</last></author>
      <author><first>Yuan</first><last>Zhao</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Anna</first><last>Feldman</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Jing</first><last>Peng</last><affiliation>Montclair State University</affiliation></author>
      <pages>875-881</pages>
      <abstract>Euphemisms are found across the world’s languages, making them a universal linguistic phenomenon. As such, euphemistic data may have useful properties for computational tasks across languages. In this study, we explore this premise by training a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic “categories” such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.</abstract>
      <url hash="23037a84">2024.findings-eacl.59</url>
      <bibkey>lee-etal-2024-meds</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>E</fixed-case>xplainer: Explaining Language Models through Prompt-based Learning</title>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>882-895</pages>
      <abstract>Pretrained language models have become workhorses for various natural language processing (NLP) tasks, sparking a growing demand for enhanced interpretability and transparency. However, prevailing explanation methods, such as attention-based and gradient-based strategies, largely rely on linear approximations, potentially causing inaccuracies such as accentuating irrelevant input tokens. To mitigate the issue, we develop PromptExplainer, a novel method for explaining language models through prompt-based learning. PromptExplainer aligns the explanation process with the masked language modeling (MLM) task of pretrained language models and leverages the prompt-based learning framework for explanation generation. It disentangles token representations into the explainable embedding space using the MLM head and extracts discriminative features with a verbalizer to generate class-dependent explanations. Extensive experiments demonstrate that PromptExplainer significantly outperforms state-of-the-art explanation methods.</abstract>
      <url hash="fd7afc75">2024.findings-eacl.60</url>
      <attachment type="software" hash="e7e35714">2024.findings-eacl.60.software.zip</attachment>
      <bibkey>feng-etal-2024-promptexplainer</bibkey>
      <video href="2024.findings-eacl.60.mp4"/>
    </paper>
    <paper id="61">
      <title>Do-Not-Answer: Evaluating Safeguards in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>896-911</pages>
      <abstract>With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of “dangerous capabilities” in order to responsibly deploy LLMs. Here we aim to facilitate this process. In particular, we collect an open-source dataset to evaluate the safeguards in LLMs, to facilitate the deployment of safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We assess the responses of six popular LLMs to these instructions, and we find that simple BERT-style classifiers can achieve results that are comparable to GPT-4 on automatic safety evaluation. Our data and code are available at https://github.com/Libr-AI/do-not-answer</abstract>
      <url hash="3329c299">2024.findings-eacl.61</url>
      <bibkey>wang-etal-2024-answer</bibkey>
      <video href="2024.findings-eacl.61.mp4"/>
    </paper>
    <paper id="62">
      <title>Do Language Models Know When They’re Hallucinating References?</title>
      <author><first>Ayush</first><last>Agrawal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mirac</first><last>Suzgun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Lester</first><last>Mackey</last><affiliation>Microsoft Research New England</affiliation></author>
      <author><first>Adam</first><last>Kalai</last></author>
      <pages>912-928</pages>
      <abstract>State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the “model organism” of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as “consistency checks.” Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to “know” when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature.</abstract>
      <url hash="1d49754a">2024.findings-eacl.62</url>
      <attachment type="software" hash="2c355a56">2024.findings-eacl.62.software.zip</attachment>
      <attachment type="note" hash="35db54a9">2024.findings-eacl.62.note.zip</attachment>
      <bibkey>agrawal-etal-2024-language</bibkey>
      <video href="2024.findings-eacl.62.mp4"/>
    </paper>
    <paper id="63">
      <title>Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Min</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>929-945</pages>
      <abstract>The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It’s clear that various sociocultural aspects—from communication styles and beliefs to shared metaphors and knowledge—profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further exploration in this exciting domain, we publish our benchmark publicly accessible at https://github.com/yongcaoplus/cuDialog.</abstract>
      <url hash="37d09e4b">2024.findings-eacl.63</url>
      <bibkey>cao-etal-2024-bridging</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>CEO</fixed-case>: Corpus-based Open-Domain Event Ontology Induction</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Jianshu</first><last>Chen</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>946-964</pages>
      <abstract>Existing event-centric NLP models often only apply to the pre-defined ontology, which significantly restricts their generalization capabilities.This paper presents CEO, a novel Corpus-based Event Ontology induction model to relax the restriction imposed by pre-defined event ontologies. Without direct supervision, CEO leverages distant supervision from available summary datasets to detect corpus-wise salient events and exploits external event knowledge to force events within a short distance to have close embeddings. Experiments on three popular event datasets show that the schema induced by CEO has better coverage and higher accuracy than previous methods. Moreover, CEO is the first event ontology induction model that can induce a hierarchical event ontology with meaningful names on eleven open-domain corpora, making the induced schema more trustworthy and easier to be further curated. We anonymously release our dataset, codes, and induced ontology.</abstract>
      <url hash="6a6fd189">2024.findings-eacl.64</url>
      <bibkey>xu-etal-2024-ceo</bibkey>
    </paper>
    <paper id="65">
      <title>Rethinking <fixed-case>STS</fixed-case> and <fixed-case>NLI</fixed-case> in Large Language Models</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Monash University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>965-982</pages>
      <abstract>Recent years, have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs’ predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.</abstract>
      <url hash="4cbaa3d3">2024.findings-eacl.65</url>
      <bibkey>wang-etal-2024-rethinking</bibkey>
      <video href="2024.findings-eacl.65.mp4"/>
    </paper>
    <paper id="66">
      <title>Learning High-Quality and General-Purpose Phrase Representations</title>
      <author><first>Lihu</first><last>Chen</last></author>
      <author><first>Gael</first><last>Varoquaux</last><affiliation>INRIA</affiliation></author>
      <author><first>Fabian</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>983-994</pages>
      <abstract>Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible.We propose an improved framework to learn phrase representations in a context-free fashion.The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation.Furthermore, we design three granularities of data augmentation to increase the diversity of training samples.Our experiments across a wide range of tasks reveal that our approach generates superior phrase embeddings compared to previous methods while requiring a smaller model size.</abstract>
      <url hash="da034f83">2024.findings-eacl.66</url>
      <attachment type="software" hash="55439295">2024.findings-eacl.66.software.zip</attachment>
      <bibkey>chen-etal-2024-learning</bibkey>
      <video href="2024.findings-eacl.66.mp4"/>
    </paper>
    <paper id="67">
      <title>Explaining Language Model Predictions with High-Impact Concepts</title>
      <author><first>Ruochen</first><last>Zhao</last></author>
      <author><first>Tan</first><last>Wang</last></author>
      <author><first>Yongjie</first><last>Wang</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>995-1012</pages>
      <abstract>To encourage fairness and transparency, there exists an urgent demand for deriving reliable explanations for large language models (LLMs). One promising solution is concept-based explanations, i.e., human-understandable concepts from internal representations. However, due to the compositional nature of languages, current methods mostly discover correlational explanations instead of causal features. Therefore, we propose a novel framework to provide impact-aware explanations for users to understand the LLM’s behavior, which are robust to feature changes and influential to the model’s predictions. Specifically, we extract predictive high-level features (concepts) from the model’s hidden layer activations. Then, we innovatively optimize for features whose existence causes the output predictions to change substantially. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on predictive impact, explainability, and faithfulness compared to the baselines, especially for LLMs.</abstract>
      <url hash="5970a8fc">2024.findings-eacl.67</url>
      <bibkey>zhao-etal-2024-explaining</bibkey>
    </paper>
    <paper id="68">
      <title>Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis</title>
      <author><first>Oscar</first><last>Chew</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>1013-1025</pages>
      <abstract>Recent research has revealed that machine learning models have a tendency to leverage spurious correlations that exist in the training set but may not hold true in general circumstances. For instance, a sentiment classifier may erroneously learn that the token “performances” is commonly associated with positive movie reviews.Relying on these spurious correlations degrades the classifier’s performance when it deploys on out-of-distribution data.In this paper, we examine the implications of spurious correlations through a novel perspective called neighborhood analysis. The analysis uncovers how spurious correlations lead unrelated words to erroneously cluster together in the embedding space. Driven by the analysis, we design a metric to detect spurious tokens and also propose a family of regularization methods, NFL (doN’t Forget your Language) to mitigate spurious correlations in text classification.Experiments show that NFL can effectively prevent erroneous clusters and significantly improve the robustness of classifiers without auxiliary data. The code is publicly available at https://github.com/oscarchew/doNt-Forget-your-Language.</abstract>
      <url hash="ac912e09">2024.findings-eacl.68</url>
      <bibkey>chew-etal-2024-understanding</bibkey>
      <video href="2024.findings-eacl.68.mp4"/>
    </paper>
    <paper id="69">
      <title>On the Intractability to Synthesize Factual Inconsistencies in Summarization</title>
      <author><first>Ge</first><last>Luo</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Weisi</first><last>Fan</last></author>
      <author><first>Miaoran</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Youbiao</first><last>He</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Yinfei</first><last>Yang</last><affiliation>Apple</affiliation></author>
      <author><first>Forrest</first><last>Bao</last><affiliation>Iowa State University, Iowa State University and Iowa State University</affiliation></author>
      <pages>1026-1037</pages>
      <abstract>Factual consistency detection has gotten raised attention in the task of abstractive summarization. Many existing works rely on synthetic training data, which may not accurately reflect or match the inconsistencies produced by summarization models. In this paper, we first systematically analyze the shortcomings of the current methods in synthesizing inconsistent summaries. Current synthesis methods may fail to produce inconsistencies of coreference errors and discourse errors, per our quantitative and qualitative study. Then, employing the parameter-efficient finetuning (PEFT) technique, we discover that a competitive factual consistency detector can be achieved using thousands of real model-generated summaries with human annotations. Our study demonstrates the importance of real machine-generated texts with human annotation in NLG evaluation as our model outperforms the SOTA on the CoGenSumm, FactCC, Frank, and SummEval datasets.</abstract>
      <url hash="814be2d6">2024.findings-eacl.69</url>
      <bibkey>luo-etal-2024-intractability</bibkey>
    </paper>
    <paper id="70">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>V</fixed-case>ec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators</title>
      <author><first>Luyang</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lingzhi</first><last>Wang</last></author>
      <author><first>Xiaoyan</first><last>Zhao</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>1038-1050</pages>
      <abstract>This study focuses on media bias detection, crucial in today’s era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input’s bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec’s significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework’s effectiveness.</abstract>
      <url hash="05911d09">2024.findings-eacl.70</url>
      <bibkey>lin-etal-2024-indivec</bibkey>
      <video href="2024.findings-eacl.70.mp4"/>
    </paper>
    <paper id="71">
      <title>Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?</title>
      <author><first>Rishav</first><last>Hada</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Adrian</first><last>Wynter</last></author>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>1051-1070</pages>
      <abstract>Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models’ outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.</abstract>
      <url hash="24bd4a12">2024.findings-eacl.71</url>
      <bibkey>hada-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.71.mp4"/>
    </paper>
    <paper id="72">
      <title>Computational Morphology and Lexicography Modeling of <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Nominals</title>
      <author><first>Christian</first><last>Khairallah</last><affiliation>New York University</affiliation></author>
      <author><first>Reham</first><last>Marzouk</last><affiliation>Alexandria University and New York University, Abu Dhabi</affiliation></author>
      <author><first>Salam</first><last>Khalifa</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Mayar</first><last>Nassar</last><affiliation>Ain Shams University</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>1071-1084</pages>
      <abstract>Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals’ intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.</abstract>
      <url hash="4372130f">2024.findings-eacl.72</url>
      <bibkey>khairallah-etal-2024-computational</bibkey>
      <video href="2024.findings-eacl.72.mp4"/>
    </paper>
    <paper id="73">
      <title>Relabeling Minimal Training Subset to Flip a Prediction</title>
      <author><first>Jinghan</first><last>Yang</last></author>
      <author><first>Linjie</first><last>Xu</last></author>
      <author><first>Lequan</first><last>Yu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>1085-1098</pages>
      <abstract>When facing an unsatisfactory prediction from a machine learning model, users can be interested in investigating the underlying reasons and exploring the potential for reversing the outcome. We ask: To flip the prediction on a test point <tex-math>x_t</tex-math>, how to identify the smallest training subset <tex-math>\mathcal{S}_t</tex-math> that we need to relabel?We propose an efficient algorithm to identify and relabel such a subset via an extended influence function for binary classification models with convex loss.We find that relabeling fewer than 2% of the training points can always flip a prediction.This mechanism can serve multiple purposes: (1) providing an approach to challenge a model prediction by altering training points; (2) evaluating model robustness with the cardinality of the subset (i.e., <tex-math>|\mathcal{S}_t|</tex-math>); we show that <tex-math>|\mathcal{S}_t|</tex-math> is highly related to the noise ratio in the training set and <tex-math>|\mathcal{S}_t|</tex-math> is correlated with but complementary to predicted probabilities; and (3) revealing training points lead to group attribution bias. To the best of our knowledge, we are the first to investigate identifying and relabeling the minimal training subset required to flip a given prediction.</abstract>
      <url hash="57da3035">2024.findings-eacl.73</url>
      <bibkey>yang-etal-2024-relabeling</bibkey>
      <video href="2024.findings-eacl.73.mp4"/>
    </paper>
    <paper id="74">
      <title>Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models</title>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Nitin</first><last>Ramrakhiyani</last><affiliation>International Institute of Information Technology, Hyderabad and Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Anubhav</first><last>Sinha</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manoj</first><last>Apte</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <pages>1099-1114</pages>
      <abstract>In this paper, we propose a novel two-step technique for text classification using autoregressive Language Models (LM). In the first step, a set of perplexity and log-likelihood based numeric features are elicited from an LM for a text instance to be classified. Then, in the second step, a classifier based on these features is trained to predict the final label. The classifier used is usually a simple machine learning classifier like Support Vector Machine (SVM) or Logistic Regression (LR) and it is trained using a small set of training examples. We believe, our technique presents a whole new way of exploiting the available training instances, in addition to the existing ways like fine-tuning LMs or in-context learning. Our approach stands out by eliminating the need for parameter updates in LMs, as required in fine-tuning, and does not impose limitations on the number of training examples faced while building prompts for in-context learning. We evaluate our technique across 5 different datasets and compare with multiple competent baselines.</abstract>
      <url hash="c83c1003">2024.findings-eacl.74</url>
      <bibkey>pawar-etal-2024-generate</bibkey>
      <video href="2024.findings-eacl.74.mp4"/>
    </paper>
    <paper id="75">
      <title>Autism Detection in Speech – A Survey</title>
      <author><first>Nadine</first><last>Probol</last></author>
      <author><first>Margot</first><last>Mieskes</last><affiliation>University of Applied Sciences Darmstadt</affiliation></author>
      <pages>1115-1125</pages>
      <abstract>There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers. Additionally, we were unable to find research combining both features from audio and transcripts.</abstract>
      <url hash="51af6a1c">2024.findings-eacl.75</url>
      <bibkey>probol-mieskes-2024-autism</bibkey>
      <video href="2024.findings-eacl.75.mp4"/>
    </paper>
    <paper id="76">
      <title>Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary Tasks</title>
      <author><first>Danae</first><last>Sanchez Villegas</last></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>1126-1137</pages>
      <abstract>Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection or hate speech classification. Jointly modeling text and images is challenging because cross-modal semantics might be hidden or the relation between image and text is weak. However, prior work on multimodal classification of social media posts has not yet addressed these challenges. In this work, we present an extensive study on the effectiveness of using two auxiliary losses jointly with the main task during fine-tuning multimodal models. First, Image-Text Contrastive (ITC) is designed to minimize the distance between image-text representations within a post, thereby effectively bridging the gap between posts where the image plays an important role in conveying the post’s meaning. Second, Image-Text Matching (ITM) enhances the model’s ability to understand the semantic relationship between images and text, thus improving its capacity to handle ambiguous or loosely related posts. We combine these objectives with five multimodal models, demonstrating consistent improvements of up to 2.6 F1 score across five diverse social media datasets. Our comprehensive analysis shows the specific scenarios where each auxiliary task is most effective.</abstract>
      <url hash="43ecdf60">2024.findings-eacl.76</url>
      <bibkey>sanchez-villegas-etal-2024-improving</bibkey>
    </paper>
    <paper id="77">
      <title>What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition</title>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Markus</first><last>Frohmann</last></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>1138-1157</pages>
      <abstract>The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge composition strategies. In particular, we test two module combination methods and five selection and weighting strategies for their effectiveness and efficiency in an extensive experimental setup. Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods. Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted.</abstract>
      <url hash="1265d531">2024.findings-eacl.77</url>
      <attachment type="software" hash="a178fa2c">2024.findings-eacl.77.software.zip</attachment>
      <bibkey>holtermann-etal-2024-weight</bibkey>
      <video href="2024.findings-eacl.77.mp4"/>
    </paper>
    <paper id="78">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>F</fixed-case>ood<fixed-case>VQA</fixed-case>: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline</title>
      <author><first>Pulkit</first><last>Agarwal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Settaluri</first><last>Sravanthi</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>1158-1176</pages>
      <abstract>Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA.</abstract>
      <url hash="f3aa97e4">2024.findings-eacl.78</url>
      <attachment type="note" hash="c8051c81">2024.findings-eacl.78.note.zip</attachment>
      <bibkey>agarwal-etal-2024-indifoodvqa</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>MAPLE</fixed-case>: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification</title>
      <author><first>Xia</first><last>Zeng</last><affiliation>Queen Mary University London</affiliation></author>
      <author><first>Arkaitz</first><last>Zubiaga</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>1177-1196</pages>
      <abstract>Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available.</abstract>
      <url hash="07de59b9">2024.findings-eacl.79</url>
      <attachment type="software" hash="19d09dc0">2024.findings-eacl.79.software.zip</attachment>
      <attachment type="note" hash="60b3c1f3">2024.findings-eacl.79.note.zip</attachment>
      <bibkey>zeng-zubiaga-2024-maple</bibkey>
      <video href="2024.findings-eacl.79.mp4"/>
    </paper>
    <paper id="80">
      <title>Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection</title>
      <author><first>David</first><last>Dukić</last><affiliation>Faculty of Electrical Engineering and Computing, University of Zagreb</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe, St.Cyril and Methodius University and NEC Laboratories Europe</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Jan</first><last>Snajder</last><affiliation>UniZg-FER, University of Zagreb</affiliation></author>
      <pages>1197-1213</pages>
      <abstract>Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) – identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the target domain, observing further TD transfer gains. Finally, we demonstrate that the gains are robust to the choice of the OIE system.</abstract>
      <url hash="32442344">2024.findings-eacl.80</url>
      <bibkey>dukic-etal-2024-leveraging</bibkey>
      <video href="2024.findings-eacl.80.mp4"/>
    </paper>
    <paper id="81">
      <title>Exploring efficient zero-shot synthetic dataset generation for Information Retrieval</title>
      <author><first>Tiago</first><last>Almeida</last></author>
      <author><first>Sérgio</first><last>Matos</last><affiliation>Universidade de Aveiro</affiliation></author>
      <pages>1214-1231</pages>
      <abstract>The broad integration of neural retrieval models into Information Retrieval (IR) systems is significantly impeded by the high cost and laborious process associated with the manual labelling of training data. Similarly, synthetic training data generation, a potential workaround, often requires expensive computational resources due to the reliance on large language models. This work explored the potential of small language models for efficiently creating high-quality synthetic datasets to train neural retrieval models. We aim to identify an optimal method to generate synthetic datasets, enabling training neural reranking models in document collections where annotated data is unavailable. We introduce a novel methodology, grounded in the principles of information theory, to select the most appropriate documents to be used as context for question generation. Then, we employ a small language model for zero-shot conditional question generation, supplemented by a filtering mechanism to ensure the quality of generated questions. Extensive evaluation on five datasets unveils the potential of our approach, outperforming unsupervised retrieval methods such as BM25 and pretrained monoT5. Our findings indicate that an efficiently generated “silver-standard” dataset allows effective training of neural rerankers in unlabeled scenarios. To ensure reproducibility and facilitate wider application, we will release a code repository featuring an accessible API for zero-shot synthetic question generation.</abstract>
      <url hash="a7485aef">2024.findings-eacl.81</url>
      <bibkey>almeida-matos-2024-exploring</bibkey>
    </paper>
    <paper id="82">
      <title>Clustering-based Sampling for Few-Shot Cross-Domain Keyphrase Extraction</title>
      <author><first>Prakamya</first><last>Mishra</last><affiliation>AMD AI</affiliation></author>
      <author><first>Lincy</first><last>Pattanaik</last></author>
      <author><first>Arunima</first><last>Sundar</last></author>
      <author><first>Nishant</first><last>Yadav</last><affiliation>Department of Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <author><first>Mayank</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <pages>1232-1250</pages>
      <abstract>Keyphrase extraction is the task of identifying a set of keyphrases present in a document that captures its most salient topics. Scientific domain-specific pre-training has led to achieving state-of-the-art keyphrase extraction performance with a majority of benchmarks being within the domain. In this work, we explore how to effectively enable the cross-domain generalization capabilities of such models without requiring the same scale of data. We primarily focus on the few-shot setting in non-scientific domain datasets such as OpenKP from the Web domain &amp; StackEx from the StackExchange forum. We propose to leverage topic information intrinsically available in the data, to build a novel clustering-based sampling approach that facilitates selecting a few samples to label from the target domain facilitating building robust and performant models. This approach leads to large gains in performance of up to 26.35 points in F1 when compared to selecting few-shot samples uniformly at random. We also explore the setting where we have access to labeled data from the model’s pretraining domain corpora and perform gradual training which involves slowly folding in target domain data to the source domain data. Here we demonstrate further improvements in the model performance by up to 12.76 F1 points.</abstract>
      <url hash="91306589">2024.findings-eacl.82</url>
      <bibkey>mishra-etal-2024-clustering</bibkey>
      <video href="2024.findings-eacl.82.mp4"/>
    </paper>
    <paper id="83">
      <title>Random Smooth-based Certified Defense against Text Adversarial Attack</title>
      <author><first>Zeliang</first><last>Zhang</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Wei</first><last>Yao</last></author>
      <author><first>Susan</first><last>Liang</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Chenliang</first><last>Xu</last><affiliation>University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>1251-1265</pages>
      <abstract>Certified defense methods have identified their effectiveness against textual adversarial examples, which train models on the worst-case text generated by substituting words in original texts with synonyms. However, due to the discrete word embedding representations, the large search space hinders the robust training efficiency, resulting in significant time consumption. To overcome this challenge, motivated by the observation that synonym embedding has a small distance, we propose to treat the word substitution as a continuous perturbation on the word embedding representation. The proposed method Text-RS applies random smooth techniques to approximate the word substitution operation, offering a computationally efficient solution that outperforms conventional discrete methods and improves the robustness in training. The evaluation results demonstrate its effectiveness in defending against multiple textual adversarial attacks.</abstract>
      <url hash="563c62a2">2024.findings-eacl.83</url>
      <bibkey>zhang-etal-2024-random</bibkey>
      <video href="2024.findings-eacl.83.mp4"/>
    </paper>
    <paper id="84">
      <title>Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness</title>
      <author><first>Hossein A.</first><last>Rahmani</last></author>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mohammadmehdi</first><last>Naghiaei</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>1266-1277</pages>
      <abstract>Clarifying questions are an integral component of modern information retrieval systems, directly impacting user satisfaction and overall system performance. Poorly formulated questions can lead to user frustration and confusion, negatively affecting the system’s performance. This research addresses the urgent need to identify and leverage key features that contribute to the classification of clarifying questions, enhancing user satisfaction. To gain deeper insights into how different features influence user satisfaction, we conduct a comprehensive analysis, considering a broad spectrum of lexical, semantic, and statistical features, such as question length and sentiment polarity. Our empirical results provide three main insights into the qualities of effective query clarification: (1) specific questions are more effective than generic ones; (2) the subjectivity and emotional tone of a question play a role; and (3) shorter and more ambiguous queries benefit significantly from clarification. Based on these insights, we implement feature-integrated user satisfaction prediction using various classifiers, both traditional and neural-based, including random forest, BERT, and large language models. Our experiments show a consistent and significant improvement, particularly in traditional classifiers, with a minimum performance boost of 45%. This study presents invaluable guidelines for refining the formulation of clarifying questions and enhancing both user satisfaction and system performance.</abstract>
      <url hash="25b6e579">2024.findings-eacl.84</url>
      <bibkey>rahmani-etal-2024-clarifying</bibkey>
    </paper>
    <paper id="85">
      <title>Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning</title>
      <author><first>Lifu</first><last>Tu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Jin</first><last>Qu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Wenhao</first><last>Liu</last><affiliation>Faire</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <pages>1278-1294</pages>
      <abstract>Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model’s cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings. We also conduct studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.</abstract>
      <url hash="26a32271">2024.findings-eacl.85</url>
      <bibkey>tu-etal-2024-efficiently</bibkey>
      <video href="2024.findings-eacl.85.mp4"/>
    </paper>
    <paper id="86">
      <title>Correcting Language Model Outputs by Editing Salient Layers</title>
      <author><first>Kshitij</first><last>Mishra</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Tamer</first><last>Soliman</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Anoop</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <pages>1295-1305</pages>
      <abstract>Large language models can accumulate incorrect or outdated knowledge as the real world evolves. Compared to typical solutions such as retraining, retrieval augmented generation, model editing offers an effective yet low cost solution to address this issue. However, existing model editing algorithms employ manual selection of edit layers, which requires prior domain knowledge or expensive architecture-specific empirical layer selection methods, such as causal tracing. In this work, we propose SaLEM (Salient Layers Editing Model), an efficient solution for data driven layer selection for the model editing task. Our solution utilizes layer-wise saliency maps for layer selection, and matches the accuracy of prior approaches but with only 1/3 of their edits, enabling efficient updates to the parametric knowledge in large language models.</abstract>
      <url hash="d27cffde">2024.findings-eacl.86</url>
      <bibkey>mishra-etal-2024-correcting</bibkey>
    </paper>
    <paper id="87">
      <title>Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback</title>
      <author><first>Nikhil</first><last>Mehta</last></author>
      <author><first>Milagro</first><last>Teruel</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xin</first><last>Deng</last></author>
      <author><first>Sergio</first><last>Figueroa Sanz</last></author>
      <author><first>Ahmed</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Julia</first><last>Kiseleva</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>1306-1321</pages>
      <abstract>Many approaches to Natural Language Processing tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaboration.In this paper, we investigate these directions using the challenging task established by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We delve into multiple types of help players can give to the AI to guide it and analyze the impact of this help on behavior, resulting in performance improvements and an end-to-end interactive system.</abstract>
      <url hash="49597c49">2024.findings-eacl.87</url>
      <bibkey>mehta-etal-2024-improving</bibkey>
      <video href="2024.findings-eacl.87.mp4"/>
    </paper>
    <paper id="88">
      <title>Goodhart’s Law Applies to <fixed-case>NLP</fixed-case>’s Explanation Benchmarks</title>
      <author><first>Jennifer</first><last>Hsia</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Aarti</first><last>Singh</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zachary</first><last>Lipton</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1322-1335</pages>
      <abstract>Despite the rising popularity of saliency-based explanations, the research community remains at an impasse, facing doubts concerning their purpose, efficacy, and tendency to contradict each other. Seeking to unite the community’s efforts around common goals, several recent works have proposed evaluation metrics. In this paper, we critically examine two sets of metrics: the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics, focusing our inquiry on natural language processing. First, we show that we can inflate a model’s comprehensiveness and sufficiency scores dramatically without altering its predictions or explanations on in-distribution test inputs. Our strategy exploits the tendency for extracted explanations and their complements to be “out-of-support” relative to each other and in-distribution inputs. Next, we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple method that encodes the label, even though EVAL-X is precisely motivated to address such exploits. Our results raise doubts about the ability of current metrics to guide explainability research, underscoring the need for a broader reassessment of what precisely these metrics are intended to capture.</abstract>
      <url hash="e6cc8838">2024.findings-eacl.88</url>
      <bibkey>hsia-etal-2024-goodharts</bibkey>
      <video href="2024.findings-eacl.88.mp4"/>
    </paper>
    <paper id="89">
      <title>Syllable-level lyrics generation from melody exploiting character-level language model</title>
      <author><first>Zhe</first><last>Zhang</last><affiliation>NII, SOKENDAI</affiliation></author>
      <author><first>Karol</first><last>Lasocki</last></author>
      <author><first>Yi</first><last>Yu</last><affiliation>NII, National Institute of Informatics</affiliation></author>
      <author><first>Atsuhiro</first><last>Takasu</last><affiliation>SOKENDAI</affiliation></author>
      <pages>1336-1346</pages>
      <abstract>The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method aims to fine-tune a character-level pre-trained language model, allowing to incorporation of linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Besides, by exploring ChatGPT-based evaluation of generated lyrics in addition to human subjective evaluation, we prove that our approach improves the coherence and correctness of generated lyrics, without the need to train expensive new language models.</abstract>
      <url hash="5a393d93">2024.findings-eacl.89</url>
      <bibkey>zhang-etal-2024-syllable</bibkey>
      <video href="2024.findings-eacl.89.mp4"/>
    </paper>
    <paper id="90">
      <title>Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca</title>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh and University of Edinburgh</affiliation></author>
      <author><first>Shaoxiong</first><last>Ji</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Nikolay</first><last>Bogoychev</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>1347-1356</pages>
      <abstract>Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.</abstract>
      <url hash="f30017a2">2024.findings-eacl.90</url>
      <bibkey>chen-etal-2024-monolingual</bibkey>
      <video href="2024.findings-eacl.90.mp4"/>
    </paper>
    <paper id="91">
      <title>Prompt Perturbation Consistency Learning for Robust Language Models</title>
      <author><first>Yao</first><last>Qiang</last><affiliation>Wayne State University</affiliation></author>
      <author><first>Subhrangshu</first><last>Nandi</last><affiliation>Amazon</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Greg</first><last>Ver Steeg</last><affiliation>University of California, Riverside, Amazon and USC/ISI</affiliation></author>
      <author><first>Anoop</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>University of Massachusetts, Lowell, University of Massachusetts at Lowell and Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <pages>1357-1370</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.</abstract>
      <url hash="90de46fe">2024.findings-eacl.91</url>
      <bibkey>qiang-etal-2024-prompt</bibkey>
      <video href="2024.findings-eacl.91.mp4"/>
    </paper>
    <paper id="92">
      <title>Enhancing Society-Undermining Disinformation Detection through Fine-Grained Sentiment Analysis Pre-Finetuning</title>
      <author><first>Tsung-Hsuan</first><last>Pan</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>1371-1377</pages>
      <abstract>In the era of the digital world, while freedom of speech has been flourishing, it has also paved the way for disinformation, causing detrimental effects on society. Legal and ethical criteria are insufficient to address this concern, thus necessitating technological intervention. This paper presents a novel method leveraging pre-finetuning concept for efficient detection and removal of disinformation that may undermine society, as deemed by judicial entities. We argue the importance of detecting this type of disinformation and validate our approach with real-world data derived from court orders. Following a study that highlighted four areas of interest for rumor analysis, our research proposes the integration of a fine-grained sentiment analysis task in the pre-finetuning phase of language models, using the GoEmotions dataset. Our experiments validate the effectiveness of our approach in enhancing performance significantly. Furthermore, we explore the application of our approach across different languages using multilingual language models, showing promising results. To our knowledge, this is the first study that investigates the role of sentiment analysis pre-finetuning in disinformation detection.</abstract>
      <url hash="64499bf9">2024.findings-eacl.92</url>
      <bibkey>pan-etal-2024-enhancing</bibkey>
      <video href="2024.findings-eacl.92.mp4"/>
    </paper>
    <paper id="93">
      <title>Minimal Distillation Schedule for Extreme Language Model Compression</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jiahao</first><last>Liu</last></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <author><first>Dawei</first><last>Song</last><affiliation>Beijing Institute of Technology and Open University</affiliation></author>
      <pages>1378-1394</pages>
      <abstract>Recent studies have revealed that language model distillation can become less effective when there is a significant capacity gap between the teacher and the student models. In order to bridge the gap, teacher assistant-based distillation has been introduced, in which the selection of the teacher assistant plays a crucial role in transferring knowledge from the teacher to the student. However, existing approaches for teacher assistant-based distillation require numerous trials to find the optimal teacher assistant.In this paper, we propose a novel approach called Minimal Distillation Schedule (MiniDisc), which enables the scheduling of an optimal teacher assistant in just one trial for extreme model compression (e.g, to 5% scale). In particular, we empirically show that the performance of the student is positively correlated with the scale-performance tradeoff of the teacher assistant. We then introduce a new <tex-math>\lambda</tex-math>-tradeoff metric that quantifies the optimality of the teacher assistant without the need for trial distillation to the student. By employing a sandwich framework, MiniDisc can select the optimal teacher assistant with the best <tex-math>\lambda</tex-math>-tradeoff.We extensively evaluate MiniDisc through a series of experiments on the GLUE benchmark. The results demonstrate that our approach achieved an improved efficiency compared to various state-of-the-art baselines. Furthermore, we showcase the scalability of MiniDisc by applying it to a language model with billions of parameters.</abstract>
      <url hash="036b40bb">2024.findings-eacl.93</url>
      <bibkey>zhang-etal-2024-minimal</bibkey>
    </paper>
    <paper id="94">
      <title>Event Semantic Classification in Context</title>
      <author><first>Haoyu</first><last>Wang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>1395-1407</pages>
      <abstract>In this work, we focus on a fundamental yet underexplored problem, event semantic classification in context, to help machines gain a deeper understanding of events. We classify events from six perspectives: modality, affirmation, specificity, telicity, durativity, and kinesis. These properties provide essential cues regarding the occurrence and grounding of events, changes of status that events can bring about, and the connection between events and time. To this end, this paper introduces a novel dataset collected for the semantic classification tasks and several effective models. By incorporating these event properties into downstream tasks, we demonstrate that understanding the fine-grained event semantics benefits downstream event understanding and reasoning via experiments on event extraction, temporal relation extraction, and subevent relation extraction.</abstract>
      <url hash="a9fb504c">2024.findings-eacl.94</url>
      <bibkey>wang-etal-2024-event</bibkey>
    </paper>
    <paper id="95">
      <title>Local and Global Contexts for Conversation</title>
      <author><first>Zuoquan</first><last>Lin</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyi</first><last>Shen</last><affiliation>Peking University</affiliation></author>
      <pages>1408-1418</pages>
      <abstract>The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally contextualized encodings ensures a comprehensive comprehension of the conversation. Experiments on popular datasets show that LGCM outperforms the existing conversation models on the performance of automatic metrics with significant margins.</abstract>
      <url hash="070044b6">2024.findings-eacl.95</url>
      <bibkey>lin-shen-2024-local</bibkey>
    </paper>
    <paper id="96">
      <title>Aspect-based Key Point Analysis for Quantitative Summarization of Reviews</title>
      <author><first>An</first><last>Tang</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Minh</first><last>Dinh</last></author>
      <pages>1419-1433</pages>
      <abstract>Key Point Analysis (KPA) is originally for summarizing arguments, where short sentences containing salient viewpoints are extracted as key points (KPs) and quantified for their prevalence as salience scores. Recently, KPA was applied to summarize reviews, but the study still relies on sentence-based KP extraction and matching, which leads to two issues: sentence-based extraction can result in KPs of overlapping opinions on the same aspects, and sentence-based matching of KP to review comment can be inaccurate, resulting in inaccurate salience scores. To address the above issues, in this paper, we propose Aspect-based Key Point Analysis (ABKPA), a novel framework for quantitative review summarization. Leveraging the readily available aspect-based sentiment analysis (ABSA) resources of reviews to automatically annotate silver labels for matching aspect-sentiment pairs, we propose a contrastive learning model to effectively match KPs to reviews and quantify KPs at the aspect level. Especially, the framework ensures extracting KP of distinct aspects and opinions, leading to more accurate opinion quantification. Experiments on five business categories of the popular Yelp review dataset show that ABKPA outperforms state-of-the-art baselines. Source code and data are available at: https://github.com/antangrocket1312/ABKPA</abstract>
      <url hash="a8a88d1d">2024.findings-eacl.96</url>
      <bibkey>tang-etal-2024-aspect</bibkey>
    </paper>
    <paper id="97">
      <title>Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Manchester, University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>1434-1450</pages>
      <abstract>Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoder (VQVAE) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAE to guide the self-attention mechanism in T5, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of control and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved reasoning capabilities, suggesting potential applications for downstream natural language and symbolic inference tasks.</abstract>
      <url hash="170b94f0">2024.findings-eacl.97</url>
      <bibkey>zhang-etal-2024-improving</bibkey>
    </paper>
    <paper id="98">
      <title>High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models</title>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>1451-1461</pages>
      <abstract>The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric’s suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.</abstract>
      <url hash="6a16680d">2024.findings-eacl.98</url>
      <bibkey>lorandi-belz-2024-high</bibkey>
      <video href="2024.findings-eacl.98.mp4"/>
    </paper>
    <paper id="99">
      <title>Antonym vs Synonym Distinction using <fixed-case>I</fixed-case>nterla<fixed-case>C</fixed-case>ed Encoder <fixed-case>NET</fixed-case>works (<fixed-case>ICE</fixed-case>-<fixed-case>NET</fixed-case>)</title>
      <author><first>Muhammad</first><last>Ali</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Yan</first><last>Hu</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Jianbin</first><last>Qin</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>1462-1473</pages>
      <abstract>Antonyms vs synonyms distinction is a core challenge in lexico-semantic analysis and automated lexical resource construction. These pairs share a similar distributional context which makes it harder to distinguish them. Leading research in this regard attempts to capture the properties of the relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However, the inability of existing research to appropriately model the relation-specific properties limits their end performance. In this paper, we propose InterlaCed Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim to capture and model the relation-specific properties of the antonyms and synonyms pairs in order to perform the classification task in a performance-enhanced manner. Experimental evaluation using the benchmark datasets shows that ICE-NET outperforms the existing research by a relative score of upto 1.8% in F1-measure.</abstract>
      <url hash="a7dca423">2024.findings-eacl.99</url>
      <bibkey>ali-etal-2024-antonym</bibkey>
    </paper>
    <paper id="100">
      <title>Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity</title>
      <author><first>Eric</first><last>Khiu</last></author>
      <author><first>Hasti</first><last>Toossi</last></author>
      <author><first>Jinyu</first><last>Liu</last></author>
      <author><first>Jiaxu</first><last>Li</last></author>
      <author><first>David</first><last>Anugraha</last></author>
      <author><first>Juan</first><last>Flores</last><affiliation>Universidad de Guanajuato</affiliation></author>
      <author><first>Leandro</first><last>Roman</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Ghent University</affiliation></author>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <pages>1474-1486</pages>
      <abstract>Fine-tuning and testing a multilingual large language model is a challenge for low-resource languages (LRLs) since it is an expensive process. While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors (the size of the fine-tuning corpus, domain similarity between fine-tuning and testing corpora, and language similarity between source and target languages), which can potentially impact the model performance by using classical regression models. Our results indicate that domain similarity has the most important impact on predicting the performance of Machine Translation models.</abstract>
      <url hash="144d271f">2024.findings-eacl.100</url>
      <attachment type="software" hash="6c430bf7">2024.findings-eacl.100.software.zip</attachment>
      <attachment type="note" hash="6b714bab">2024.findings-eacl.100.note.zip</attachment>
      <bibkey>khiu-etal-2024-predicting</bibkey>
      <revision id="1" href="2024.findings-eacl.100v1" hash="02411aaa"/>
      <revision id="2" href="2024.findings-eacl.100v2" hash="144d271f" date="2024-03-25">Include authors email address.</revision>
      <video href="2024.findings-eacl.100.mp4"/>
    </paper>
    <paper id="101">
      <title>Does <fixed-case>CLIP</fixed-case> Bind Concepts? Probing Compositionality in Large Image Models</title>
      <author><first>Martha</first><last>Lewis</last><affiliation>University of Bristol and University of Bristol</affiliation></author>
      <author><first>Nihal</first><last>Nayak</last><affiliation>Brown University</affiliation></author>
      <author><first>Peilin</first><last>Yu</last><affiliation>Brown University</affiliation></author>
      <author><first>Jack</first><last>Merullo</last><affiliation>Brown University</affiliation></author>
      <author><first>Qinan</first><last>Yu</last></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Computer Science Department, Brown University and Snorkel AI</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University</affiliation></author>
      <pages>1487-1500</pages>
      <abstract>Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.</abstract>
      <url hash="88b32c5a">2024.findings-eacl.101</url>
      <attachment type="software" hash="88b0c508">2024.findings-eacl.101.software.zip</attachment>
      <bibkey>lewis-etal-2024-clip</bibkey>
      <video href="2024.findings-eacl.101.mp4"/>
    </paper>
    <paper id="102">
      <title>Code-Switching and Back-Transliteration Using a Bilingual Model</title>
      <author><first>Daniel</first><last>Weisberg Mitelman</last><affiliation>Reichman University</affiliation></author>
      <author><first>Nachum</first><last>Dershowitz</last><affiliation>Tel Aviv University, Technion</affiliation></author>
      <author><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <pages>1501-1511</pages>
      <abstract>The challenges of automated transliteration and code-switching–detection in Judeo-Arabic texts are addressed. We introduce two novel machine-learning models, one focused on transliterating Judeo-Arabic into Arabic, and another aimed at identifying non-Arabic words, predominantly Hebrew and Aramaic. Unlike prior work, our models are based on a bilingual Arabic-Hebrew language model, providing a unique advantage in capturing shared linguistic nuances. Evaluation results show that our models outperform prior solutions for the same tasks. As a practical contribution, we present a comprehensive pipeline capable of taking Judeo-Arabic text, identifying non-Arabic words, and then transliterating the Arabic portions into Arabic script. This work not only advances the state of the art but also offers a valuable toolset for making Judeo-Arabic texts more accessible to a broader Arabic-speaking audience.</abstract>
      <url hash="d7cc1bdf">2024.findings-eacl.102</url>
      <bibkey>weisberg-mitelman-etal-2024-code</bibkey>
    </paper>
    <paper id="103">
      <title>Tsetlin Machine Embedding: Representing Words Using Logical Expressions</title>
      <author><first>Bimal</first><last>Bhattarai</last></author>
      <author><first>Ole-Christoffer</first><last>Granmo</last></author>
      <author><first>Lei</first><last>Jiao</last><affiliation>University of Agder</affiliation></author>
      <author><first>Rohan</first><last>Yadav</last></author>
      <author><first>Jivitesh</first><last>Sharma</last><affiliation>Norwegian Institute for Air Research</affiliation></author>
      <pages>1512-1522</pages>
      <abstract>Embedding words in vector space is a fundamental first step in state-of-the-art natural language processing (NLP). Typical NLP solutions employ pre-defined vector representations to improve generalization by co-locating similar words in vector space. For instance, Word2Vec is a self-supervised predictive model that captures the context of words using a neural network. Similarly, GLoVe is a popular unsupervised model incorporating corpus-wide word co-occurrence statistics. Such word embedding has significantly boosted important NLP tasks, including sentiment analysis, document classification, and machine translation. However, the embeddings are dense floating-point vectors, making them expensive to compute and difficult to interpret. In this paper, we instead propose to represent the semantics of words with a few defining words that are related using propositional logic. To produce such logical embeddings, we introduce a Tsetlin Machine-based autoencoder that learns logical clauses self-supervised. The clauses consist of contextual words like <tex-math>\textit{black}</tex-math>, <tex-math>\textit{cup}</tex-math>, and <tex-math>\textit{hot}</tex-math> to define other words like <tex-math>\textit{coffee}</tex-math>, thus being human-understandable. We evaluate our embedding approach on several intrinsic and extrinsic benchmarks, outperforming GLoVe on six classification tasks. Furthermore, we investigate the interpretability of our embedding using the logical representations acquired during training. We also visualize word clusters in vector space, demonstrating how our logical embedding co-locate similar words.</abstract>
      <url hash="e635de69">2024.findings-eacl.103</url>
      <attachment type="software" hash="fe2bb58c">2024.findings-eacl.103.software.zip</attachment>
      <bibkey>bhattarai-etal-2024-tsetlin</bibkey>
    </paper>
    <paper id="104">
      <title>Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities</title>
      <author><first>Zihao</first><last>He</last></author>
      <author><first>Ashwin</first><last>Rao</last></author>
      <author><first>Siyi</first><last>Guo</last></author>
      <author><first>Negar</first><last>Mokhberian</last></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <pages>1523-1536</pages>
      <abstract>Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.</abstract>
      <url hash="f8386119">2024.findings-eacl.104</url>
      <attachment type="software" hash="9c7db982">2024.findings-eacl.104.software.zip</attachment>
      <attachment type="note" hash="f185d42a">2024.findings-eacl.104.note.zip</attachment>
      <bibkey>he-etal-2024-reading</bibkey>
      <video href="2024.findings-eacl.104.mp4"/>
    </paper>
    <paper id="105">
      <title>Unified Embeddings for Multimodal Retrieval via Frozen <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ziyang</first><last>Wang</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Heba</first><last>Elfardy</last><affiliation>Amazon</affiliation></author>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon</affiliation></author>
      <author><first>Kevin</first><last>Small</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>1537-1547</pages>
      <abstract>In this work, We present Unified Embeddings for Multimodal Retrieval (UniMuR), a simple but effective approach that embeds multimodal inputs and retrieves visual and textual outputs via frozen Large Language Models (LLMs). Specifically, UniMuR jointly retrieves multimodal outputs via a unified multimodal embedding and applies dual alignment training to account for both visual and textual semantics. Thus, unlike previous approaches, UniMuR significantly reduces LLM’s modality bias towards generating text-only outputs. Meanwhile, the proposed unified multimodal embedding mitigates the inconsistency between visual and textual outputs and provides coherent multimodal outputs. Furthermore, benefiting from the joint training of visual and textual semantics, UniMuR also achieves strong image/text retrieval ability. Compared to existing approaches, UniMuR achieves better zero-shot multimodal response retrieval performance on MMDialog, improving the overall R@1 by 6.5% while boosting the image retrieval rate and having better cross-modal consistency on multimodal outputs. UniMuR also achieves 2.4% and 3.9% improvement on context-based image retrieval tasks on MMDialog and VisDial respectively when compared to previous approaches, validating its generalization ability across multiple tasks.</abstract>
      <url hash="7ee77965">2024.findings-eacl.105</url>
      <bibkey>wang-etal-2024-unified</bibkey>
      <video href="2024.findings-eacl.105.mp4"/>
    </paper>
    <paper id="106">
      <title>Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods</title>
      <author><first>Mohammed</first><last>Mohammed</last></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>1548-1556</pages>
      <abstract>As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning.In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module.We find that the ported modules far outperform the two alternatives tested, but that there are interesting differences between the four PEFT techniques tested.We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.</abstract>
      <url hash="4e5f93e8">2024.findings-eacl.106</url>
      <bibkey>mohammed-belz-2024-assessing</bibkey>
    </paper>
    <paper id="107">
      <title>Exploiting Class Probabilities for Black-box Sentence-level Attacks</title>
      <author><first>Raha</first><last>Moraffah</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>1557-1568</pages>
      <abstract>Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack’s success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.</abstract>
      <url hash="973ff83c">2024.findings-eacl.107</url>
      <bibkey>moraffah-liu-2024-exploiting</bibkey>
      <video href="2024.findings-eacl.107.mp4"/>
    </paper>
    <paper id="108">
      <title>Learning Label Hierarchy with Supervised Contrastive Learning</title>
      <author><first>Ruixue</first><last>Lian</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>William</first><last>Sethares</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <pages>1569-1581</pages>
      <abstract>Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LA-SCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. In this way, a better feature representation is generated with improvements of intra-cluster compactness and inter-cluster separation. Experiments on three datasets show that the proposed LA-SCL works well on text classification of distinguishing a single label among multi-labels, outperforming the baseline supervised approaches. Our code is publicly available <tex-math>^1</tex-math>.</abstract>
      <url hash="2d0f13d7">2024.findings-eacl.108</url>
      <bibkey>lian-etal-2024-learning</bibkey>
      <video href="2024.findings-eacl.108.mp4"/>
    </paper>
    <paper id="109">
      <title><fixed-case>G</fixed-case>roun<fixed-case>D</fixed-case>ial: Human-norm Grounded Safe Dialog Response Generation</title>
      <author><first>Siwon</first><last>Kim</last></author>
      <author><first>Shuyang</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon</affiliation></author>
      <author><first>Shayan</first><last>Ray</last></author>
      <author><first>Tara</first><last>Taghavi</last></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>1582-1588</pages>
      <abstract>Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.</abstract>
      <url hash="fd648be5">2024.findings-eacl.109</url>
      <bibkey>kim-etal-2024-groundial</bibkey>
    </paper>
    <paper id="110">
      <title>Trainable Hard Negative Examples in Contrastive Learning for Unsupervised Abstractive Summarization</title>
      <author><first>Haojie</first><last>Zhuang</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Wei Emma</first><last>Zhang</last><affiliation>The University of Adelaide</affiliation></author>
      <author><first>Chang</first><last>Dong</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Quan</first><last>Sheng</last><affiliation>Macquarie University</affiliation></author>
      <pages>1589-1600</pages>
      <abstract>Contrastive learning has demonstrated promising results in unsupervised abstractive summarization. However, existing methods rely on manually crafted negative examples, demanding substantial human effort and domain knowledge. Moreover, these human-generated negative examples may be poor in quality and lack adaptability during model training. To address these issues, we propose a novel approach that learns trainable negative examples for contrastive learning in unsupervised abstractive summarization, which eliminates the need for manual negative example design. Our framework introduces an adversarial optimization process between a negative example network and a representation network (including the summarizer and encoders). The negative example network is trained to synthesize hard negative examples that are close to the positive examples, driving the representation network to improve the quality of the generated summaries. We evaluate our method on two benchmark datasets for unsupervised abstractive summarization and observe significant performance improvements compared to strong baseline models.</abstract>
      <url hash="fc891a5c">2024.findings-eacl.110</url>
      <bibkey>zhuang-etal-2024-trainable</bibkey>
    </paper>
    <paper id="111">
      <title>Low-Resource Counterspeech Generation for <fixed-case>I</fixed-case>ndic Languages: The Case of <fixed-case>B</fixed-case>engali and <fixed-case>H</fixed-case>indi</title>
      <author><first>Mithun</first><last>Das</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Saurabh</first><last>Pandey</last></author>
      <author><first>Shivansh</first><last>Sethi</last></author>
      <author><first>Punyajoy</first><last>Saha</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>1601-1614</pages>
      <abstract>With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can “counter” the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali, and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.</abstract>
      <url hash="419ea91d">2024.findings-eacl.111</url>
      <attachment type="note" hash="8bdbf9b3">2024.findings-eacl.111.note.zip</attachment>
      <bibkey>das-etal-2024-low</bibkey>
      <video href="2024.findings-eacl.111.mp4"/>
    </paper>
    <paper id="112">
      <title>Teaching Probabilistic Logical Reasoning to Transformers</title>
      <author><first>Aliakbar</first><last>Nafar</last></author>
      <author><first>K. Brent</first><last>Venable</last><affiliation>University of West Florida and Florida Institute of Human and Machine Cognition</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>1615-1632</pages>
      <abstract>In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model’s intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT equips these models to effectively handle novel situations, including higher reasoning depth, new domains, and complex probabilistic structures.</abstract>
      <url hash="e83b6dd5">2024.findings-eacl.112</url>
      <attachment type="software" hash="ea1e1ac9">2024.findings-eacl.112.software.zip</attachment>
      <attachment type="note" hash="ea47095c">2024.findings-eacl.112.note.zip</attachment>
      <bibkey>nafar-etal-2024-teaching</bibkey>
      <video href="2024.findings-eacl.112.mp4"/>
    </paper>
    <paper id="113">
      <title>On Measuring Context Utilization in Document-Level <fixed-case>MT</fixed-case> Systems</title>
      <author><first>Wafaa</first><last>Mohammed</last></author>
      <author><first>Vlad</first><last>Niculae</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>1633-1643</pages>
      <abstract>Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models’ performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annotations are not available. Finally, we highlight the importance of using discourse-rich datasets when assessing context utilization.</abstract>
      <url hash="d45369b7">2024.findings-eacl.113</url>
      <bibkey>mohammed-niculae-2024-measuring</bibkey>
      <video href="2024.findings-eacl.113.mp4"/>
    </paper>
    <paper id="114">
      <title>Solving <fixed-case>NLP</fixed-case> Problems through Human-System Collaboration: A Discussion-based Approach</title>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>1644-1658</pages>
      <abstract>Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other.Similarly, if a system can have discussions with human partners when solving tasks, it has the potential to improve the system’s performance and reliability.In previous research on explainability, it has only been possible for systems to make predictions and for humans to ask questions about them, rather than having a mutual exchange of opinions.This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans, improving the accuracy by up to 25 points on a natural language inference task.</abstract>
      <url hash="e5d679ba">2024.findings-eacl.114</url>
      <attachment type="note" hash="e94e1232">2024.findings-eacl.114.note.zip</attachment>
      <bibkey>kaneko-etal-2024-solving</bibkey>
    </paper>
    <paper id="115">
      <title>Autoregressive Score Generation for Multi-trait Essay Scoring</title>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>1659-1666</pages>
      <abstract>Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of *encoder*, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a *decoding* process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits.</abstract>
      <url hash="ae5946d1">2024.findings-eacl.115</url>
      <bibkey>do-etal-2024-autoregressive</bibkey>
      <video href="2024.findings-eacl.115.mp4"/>
    </paper>
    <paper id="116">
      <title><fixed-case>CMA</fixed-case>-<fixed-case>R</fixed-case>: Causal Mediation Analysis for Explaining Rumour Detection</title>
      <author><first>Lin</first><last>Tian</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>1667-1675</pages>
      <abstract>We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter.Interventions at the input and network level reveal the causal impacts of tweets and words in the model output.We find that our approach CMA-R – Causal Mediation Analysis for Rumour detection – identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories.CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.</abstract>
      <url hash="c2bab0c1">2024.findings-eacl.116</url>
      <bibkey>tian-etal-2024-cma</bibkey>
    </paper>
    <paper id="117">
      <title>Morphology Aware Source Term Masking for Terminology-Constrained <fixed-case>NMT</fixed-case></title>
      <author><first>Ander</first><last>Corral</last><affiliation>Orai NLP Technologies</affiliation></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <pages>1676-1688</pages>
      <abstract>Terminology-constrained NMT systems facilitate the forced translation of domain-specific vocabulary. A notable method in this context is the “copy-and-inflect” approach, which appends the target term lemmas of constraints to their corresponding source terms in the input sentence. In this work, we propose a novel adaptation of the “copy-and-inflect” method, referred to as “morph-masking”. Our method involves masking the source terms of the constraints from the input sentence while retaining essential grammatical information. Our approach is based on the hypothesis that “copy-and-inflect” systems have access to both source and target terms, allowing them to generate the correct surface form of the constraint by either translating the source term itself or properly inflecting the target term lemma. Through extensive validation of our method in two translation directions with different levels of source morphological complexity, Basque to Spanish and English to German, we have demonstrated that “morph-masking” is capable of providing a harder constraint signal, resulting in a notable improvement over the “copy-and-inflect” method (up to 38% in term accuracy), especially in challenging constraint scenarios.</abstract>
      <url hash="f037957d">2024.findings-eacl.117</url>
      <bibkey>corral-saralegi-2024-morphology</bibkey>
      <video href="2024.findings-eacl.117.mp4"/>
    </paper>
    <paper id="118">
      <title>Improving Backchannel Prediction Leveraging Sequential and Attentive Context Awareness</title>
      <author><first>Yo-Han</first><last>Park</last><affiliation>Chungnam National University and Chungnam National University</affiliation></author>
      <author><first>Wencke</first><last>Liermann</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Yong-Seok</first><last>Choi</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Kong Joo</first><last>Lee</last><affiliation>Chungnam National University</affiliation></author>
      <pages>1689-1694</pages>
      <abstract>Backchannels, which refer to short and often affirmative or empathetic responses from a listener during a conversation, play a crucial role in effective communication. In this paper, we introduce CABP(Context-Aware Backchannel Prediction), a sequential and attentive context approach aimed at enhancing backchannel prediction performance. Additionally, CABP leverages the pretrained wav2vec model for encoding audio signal. Experimental results show that CABP performs better than context-free models, with performance improvements of 1.3% and 1.8% in Korean and English datasets, respectively. Furthermore, when utilizing the pretrained wav2vec model, CABP consistently demonstrates the best performance, achieving performance improvements of 4.4% and 3.1% in Korean and English datasets.</abstract>
      <url hash="0120613a">2024.findings-eacl.118</url>
      <attachment type="software" hash="747f9bcc">2024.findings-eacl.118.software.zip</attachment>
      <bibkey>park-etal-2024-improving</bibkey>
      <video href="2024.findings-eacl.118.mp4"/>
    </paper>
    <paper id="119">
      <title><fixed-case>SENSE</fixed-case>-<fixed-case>LM</fixed-case> : A Synergy between a Language Model and Sensorimotor Representations for Auditory and Olfactory Information Extraction</title>
      <author><first>Cédric</first><last>Boscher</last></author>
      <author><first>Christine</first><last>Largeron</last><affiliation>Université Jean Monnet</affiliation></author>
      <author><first>Véronique</first><last>Eglin</last><affiliation>Institut National des Sciences Appliquées de Lyon</affiliation></author>
      <author><first>Elöd</first><last>Egyed-Zsigmond</last><affiliation>Institut National des Sciences Appliquées de Lyon</affiliation></author>
      <pages>1695-1711</pages>
      <abstract>The five human senses – vision, taste, smell, hearing, and touch – are key concepts that shape human perception of the world. The extraction of sensory references (i.e., expressions that evoke the presence of a sensory experience) in textual corpus is a challenge of high interest, with many applications in various areas. In this paper, we propose SENSE-LM, an information extraction system tailored for the discovery of sensory references in large collections of textual documents. Based on the novel idea of combining the strength of large language models and linguistic resources such as sensorimotor norms, it addresses the task of sensory information extraction at a coarse-grained (sentence binary classification) and fine-grained (sensory term extraction) level.Our evaluation of SENSE-LM for two sensory functions, Olfaction and Audition, and comparison with state-of-the-art methods emphasize a significant leap forward in automating these complex tasks.</abstract>
      <url hash="1b5a5d95">2024.findings-eacl.119</url>
      <bibkey>boscher-etal-2024-sense</bibkey>
      <video href="2024.findings-eacl.119.mp4"/>
    </paper>
    <paper id="120">
      <title>Analyzing the Role of Part-of-Speech in Code-Switching: A Corpus-Based Study</title>
      <author><first>Jie</first><last>Chi</last></author>
      <author><first>Peter</first><last>Bell</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>1712-1721</pages>
      <abstract>Code-switching (CS) is a common linguistic phenomenon wherein speakers fluidly transition between languages in conversation. While the cognitive processes driving CS remain a complex domain, earlier investigations have shed light on its multifaceted triggers. This study delves into the influence of Part-of-Speech (POS) on the propensity of bilinguals to engage in CS, employing a comprehensive analysis of Spanish-English and Mandarin-English corpora. Compared with prior research, our findings not only affirm the existence of a statistically significant connection between POS and the likelihood of CS across language pairs, but notably find this relationship exhibits its maximum strength in proximity to CS instances, progressively diminishing as tokens distance themselves from these CS points.</abstract>
      <url hash="cdc4e20a">2024.findings-eacl.120</url>
      <bibkey>chi-bell-2024-analyzing</bibkey>
      <video href="2024.findings-eacl.120.mp4"/>
    </paper>
    <paper id="121">
      <title>In-Contextual Gender Bias Suppression for Large Language Models</title>
      <author><first>Daisuke</first><last>Oba</last><affiliation>Institute of Industrial Science, The University of Tokyo</affiliation></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <pages>1722-1742</pages>
      <abstract>Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.</abstract>
      <url hash="5dd0284b">2024.findings-eacl.121</url>
      <bibkey>oba-etal-2024-contextual</bibkey>
      <video href="2024.findings-eacl.121.mp4"/>
    </paper>
    <paper id="122">
      <title>Parameter-Efficient Fine-Tuning: Is There An Optimal Subset of Parameters to Tune?</title>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>1743-1759</pages>
      <abstract>The ever-growing size of pretrained language models (PLM) presents a significant challenge for efficiently fine-tuning and deploying these models for diverse sets of tasks within memory-constrained environments.In light of this, recent research has illuminated the possibility of selectively updating only a small subset of a model’s parameters during the fine-tuning process.Since no new parameters or modules are added, these methods retain the inference speed of the original model and come at no additional computational cost. However, an open question pertains to which subset of parameters should best be tuned to maximize task performance and generalizability. To investigate, this paper presents comprehensive experiments covering a large spectrum of subset selection strategies. We comparatively evaluate their impact on model performance as well as the resulting model’s capability to generalize to different tasks.Surprisingly, we find that the gains achieved in performance by elaborate selection strategies are, at best, marginal when compared to the outcomes obtained by tuning a random selection of parameter subsets. Our experiments also indicate that selection-based tuning impairs generalizability to new tasks.</abstract>
      <url hash="da50d04e">2024.findings-eacl.122</url>
      <bibkey>ploner-akbik-2024-parameter</bibkey>
      <video href="2024.findings-eacl.122.mp4"/>
    </paper>
    <paper id="123">
      <title>Contextualized Topic Coherence Metrics</title>
      <author><first>Hamed</first><last>Rahimi</last></author>
      <author><first>David</first><last>Mimno</last><affiliation>Cornell University and Cornell University</affiliation></author>
      <author><first>Jacob</first><last>Hoover</last><affiliation>McGill University</affiliation></author>
      <author><first>Hubert</first><last>Naacke</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Camelia</first><last>Constantin</last></author>
      <author><first>Bernd</first><last>Amann</last><affiliation>Sorbonne Université</affiliation></author>
      <pages>1760-1773</pages>
      <abstract>This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.</abstract>
      <url hash="f7168365">2024.findings-eacl.123</url>
      <attachment type="software" hash="3868254e">2024.findings-eacl.123.software.zip</attachment>
      <bibkey>rahimi-etal-2024-contextualized</bibkey>
      <video href="2024.findings-eacl.123.mp4"/>
    </paper>
    <paper id="124">
      <title><fixed-case>P</fixed-case>ro<fixed-case>MIS</fixed-case>e: A Proactive Multi-turn Dialogue Dataset for Information-seeking Intent Resolution</title>
      <author><first>Yash</first><last>Butala</last></author>
      <author><first>Siddhant</first><last>Garg</last><affiliation>Meta</affiliation></author>
      <author><first>Pratyay</first><last>Banerjee</last><affiliation>Amazon</affiliation></author>
      <author><first>Amita</first><last>Misra</last><affiliation>Amazon</affiliation></author>
      <pages>1774-1789</pages>
      <abstract>Users of AI-based virtual assistants and search systems encounter challenges in articulating their intents while seeking information on unfamiliar topics, possibly due to complexity of the user’s intent or the lack of meta-information on the topic. We posit that an iterative suggested question-answering (SQA) conversation can improve the trade-off between the satisfaction of the user’s intent while keeping the information exchange natural and cognitive load of the interaction minimal on the users. In this paper, we evaluate a novel setting ProMISe by means of a sequence of interactions between a user, having a predefined information-seeking intent, and an agent that generates a set of SQA pairs at each step to aid the user to get closer to their intent. We simulate this two-player setting to create a multi-turn conversational dataset of SQAs and user choices (1025 dialogues comprising 4453 turns and 17812 SQAs) using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models. We evaluate the quality of the SQs in the dataset on attributes such as diversity, specificity, grounding, etc, and benchmark the performance of different language models for the task of replicating user behavior.</abstract>
      <url hash="f2aec2e2">2024.findings-eacl.124</url>
      <attachment type="note" hash="551af5d6">2024.findings-eacl.124.note.zip</attachment>
      <bibkey>butala-etal-2024-promise</bibkey>
    </paper>
    <paper id="125">
      <title><fixed-case>CODET</fixed-case>: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>1790-1859</pages>
      <abstract>Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release CODET, a contrastive dialectal benchmark encompassing 891 different variations from twelve different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. All the data and code have been released.</abstract>
      <url hash="9e3000f0">2024.findings-eacl.125</url>
      <bibkey>alam-etal-2024-codet</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>QAEVENT</fixed-case>: Event Extraction as Question-Answer Pairs Generation</title>
      <author><first>Milind</first><last>Choudhary</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Xinya</first><last>Du</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>1860-1873</pages>
      <abstract>We propose a novel representation of document-level events as question and answer pairs (QAEVENT). Under this paradigm: (1) questions themselves can define argument roles without the need for predefined schemas, which will cover a comprehensive list of event arguments from the document; (2) it allows for more scalable and faster annotations from crowdworkers without linguistic expertise. Based on our new paradigm, we collect a novel and wide-coverage dataset. Our examinations show that annotations with the QA representations produce high-quality data for document-level event extraction, both in terms of human agreement level and high coverage of roles comparing to the pre-defined schema. We present and compare representative approaches for generating event question answer pairs on our benchmark.</abstract>
      <url hash="43e68930">2024.findings-eacl.126</url>
      <bibkey>choudhary-du-2024-qaevent</bibkey>
    </paper>
    <paper id="127">
      <title>Sequence Shortening for Context-Aware Machine Translation</title>
      <author><first>Paweł</first><last>Maka</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Yusuf</first><last>Semerci</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Jan</first><last>Scholtes</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>1874-1894</pages>
      <abstract>Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size.</abstract>
      <url hash="774a45f4">2024.findings-eacl.127</url>
      <bibkey>maka-etal-2024-sequence</bibkey>
      <video href="2024.findings-eacl.127.mp4"/>
    </paper>
    <paper id="128">
      <title>Jigsaw Pieces of Meaning: Modeling Discourse Coherence with Informed Negative Sample Synthesis</title>
      <author><first>Shubhankar</first><last>Singh</last></author>
      <pages>1895-1908</pages>
      <abstract>Coherence in discourse is fundamental for comprehension and perception. Much research on coherence modeling has focused on better model architectures and training setups optimizing on the permuted document task, where random permutations of a coherent document are considered incoherent. However, there’s very limited work on creating “informed” synthetic incoherent samples that better represent or mimic incoherence. We source a diverse positive corpus for local coherence and propose six rule-based methods leveraging information from Constituency trees, Part-of-speech, semantic overlap and more, for “informed” negative sample synthesis for better representation of incoherence. We keep a straightforward training setup for local coherence modeling by fine-tuning popular transformer models, and aggregate local scores for global coherence. We evaluate on a battery of independent downstream tasks to assess the impact of improved negative sample quality. We assert that a step towards optimality for coherence modeling requires better negative sample synthesis in tandem with model improvements.</abstract>
      <url hash="7d14f3f1">2024.findings-eacl.128</url>
      <bibkey>singh-2024-jigsaw</bibkey>
    </paper>
    <paper id="129">
      <title>Non-Exchangeable Conformal Language Generation with Nearest Neighbors</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <pages>1909-1929</pages>
      <abstract>Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on *non-exchangeable* conformal prediction, which still ensures bounds on coverage. The result, *non-exchangeable conformal nucleus sampling*, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.</abstract>
      <url hash="74ae0a2d">2024.findings-eacl.129</url>
      <bibkey>ulmer-etal-2024-non</bibkey>
      <video href="2024.findings-eacl.129.mp4"/>
    </paper>
    <paper id="130">
      <title>Evidentiality-aware Retrieval for Overcoming Abstractiveness in Open-Domain Question Answering</title>
      <author><first>Yongho</first><last>Song</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dahyun</first><last>Lee</last></author>
      <author><first>Myungha</first><last>Jang</last><affiliation>Pinterest Inc.</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>1930-1943</pages>
      <abstract>The long-standing goal of dense retrievers in abtractive open-domain question answering (ODQA) tasks is to learn to capture evidence passages among relevant passages for any given query, such that the reader produce factually correct outputs from evidence passages. One of the key challenge is the insufficient amount of training data with the supervision of the answerability of the passages. Recent studies rely on iterative pipelines to annotate answerability using signals from the reader, but their high computational costs hamper practical applications. In this paper, we instead focus on a data-driven approach and propose Evidentiality-Aware Dense Passage Retrieval (EADPR), which leverages synthetic distractor samples to learn to discriminate evidence passages from distractors. We conduct extensive experiments to validate the effectiveness of our proposed method on multiple abstractive ODQA tasks.</abstract>
      <url hash="021372bf">2024.findings-eacl.130</url>
      <bibkey>song-etal-2024-evidentiality</bibkey>
      <video href="2024.findings-eacl.130.mp4"/>
    </paper>
    <paper id="131">
      <title>Self-training Strategies for Sentiment Analysis: An Empirical Study</title>
      <author><first>Haochen</first><last>Liu</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Sai</first><last>Rallabandi</last></author>
      <author><first>Yijing</first><last>Wu</last></author>
      <author><first>Parag</first><last>Dakle</last></author>
      <author><first>Preethi</first><last>Raghavan</last><affiliation>Fidelity</affiliation></author>
      <pages>1944-1954</pages>
      <abstract>Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a large amount of unlabeled data. However, given a set of training data, how to utilize them to conduct self-training makes a significant difference in the final performance of the model. We refer to this methodology as the self-training strategy. In this paper, we present an empirical study of various self-training strategies for sentiment analysis. First, we investigate the influence of the self-training strategy and hyper-parameters on the performance of traditional small language models (SLMs) in various few-shot settings. Second, we also explore the feasibility of leveraging large language models (LLMs) to help self-training. We propose and empirically compare several self-training strategies with the intervention of LLMs. Extensive experiments are conducted on three real-world sentiment analysis datasets.</abstract>
      <url hash="10c2026e">2024.findings-eacl.131</url>
      <bibkey>liu-etal-2024-self</bibkey>
      <video href="2024.findings-eacl.131.mp4"/>
    </paper>
    <paper id="132">
      <title>Language is All a Graph Needs</title>
      <author><first>Ruosong</first><last>Ye</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Runhui</first><last>Wang</last></author>
      <author><first>Shuyuan</first><last>Xu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>1955-1973</pages>
      <abstract>The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.</abstract>
      <url hash="8ec9c9ff">2024.findings-eacl.132</url>
      <bibkey>ye-etal-2024-language</bibkey>
    </paper>
    <paper id="133">
      <title>Unraveling the Dynamics of Semi-Supervised Hate Speech Detection: The Impact of Unlabeled Data Characteristics and Pseudo-Labeling Strategies</title>
      <author><first>Florian</first><last>Ludwig</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Klara</first><last>Dolos</last><affiliation>ZITiS</affiliation></author>
      <author><first>Ana</first><last>Alves-Pinto</last><affiliation>Central Office for Information Technology in the Security Sector</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Fernuniversität in Hagen</affiliation></author>
      <pages>1974-1986</pages>
      <abstract>Despite advances in machine learning based hate speech detection, the need for larges amounts of labeled training data for state-of-the-art approaches remains a challenge for their application. Semi-supervised learning addresses this problem by leveraging unlabeled data and thus reducing the amount of annotated data required. Underlying this approach is the assumption that labeled and unlabeled data follow similar distributions. This assumption however may not always hold, with consequences for real world applications. We address this problem by investigating the dynamics of pseudo-labeling, a commonly employed form of semi-supervised learning, in the context of hate speech detection. Concretely we analysed the influence of data characteristics and of two strategies for selecting pseudo-labeled samples: threshold- and ratio-based. The results show that the influence of data characteristics on the pseudo-labeling performances depends on other factors, such as pseudo-label selection strategies or model biases. Furthermore, the effectiveness of pseudo-labeling in classification performance is determined by the interaction between the number, hate ratio and accuracy of the selected pseudo-labels. Analysis of the results suggests an advantage of the threshold-based approach when labeled and unlabeled data arise from the same domain, whilst the ratio-based approach may be recommended in the opposite situation.</abstract>
      <url hash="e98ff5b9">2024.findings-eacl.133</url>
      <attachment type="software" hash="faa4add9">2024.findings-eacl.133.software.zip</attachment>
      <bibkey>ludwig-etal-2024-unraveling</bibkey>
    </paper>
    <paper id="134">
      <title>When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>David</first><last>Wadden</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>1987-2003</pages>
      <abstract>Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear.</abstract>
      <url hash="74f3c4d7">2024.findings-eacl.134</url>
      <bibkey>weller-etal-2024-generative</bibkey>
      <video href="2024.findings-eacl.134.mp4"/>
    </paper>
    <paper id="135">
      <title>Can Large Language Models Understand Context?</title>
      <author><first>Yilun</first><last>Zhu</last></author>
      <author><first>Joel</first><last>Moniz</last><affiliation>Apple</affiliation></author>
      <author><first>Shruti</first><last>Bhargava</last><affiliation>Apple</affiliation></author>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Dhivya</first><last>Piraviperumal</last></author>
      <author><first>Site</first><last>Li</last></author>
      <author><first>Yuan</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Apple</affiliation></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <pages>2004-2018</pages>
      <abstract>Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models’ ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.</abstract>
      <url hash="400d0d77">2024.findings-eacl.135</url>
      <bibkey>zhu-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.135.mp4"/>
    </paper>
    <paper id="136">
      <title>Let’s Negotiate! A Survey of Negotiation Dialogue Systems</title>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Tao</first><last>Feng</last><affiliation>Monash University</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Suraj</first><last>Sharma</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhaleh</first><last>Semnani Azad</last><affiliation>California State University, Northridge</affiliation></author>
      <author><first>Ingrid</first><last>Zukerman</last><affiliation>Monash University</affiliation></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>2019-2031</pages>
      <abstract>Negotiation is a crucial ability in human communication. Recently, there has been a resurgent research interest in negotiation dialogue systems, whose goal is to create intelligent agents that can assist people in resolving conflicts or reaching agreements. Although there have been many explorations into negotiation dialogue systems, a systematic review of this task has not been performed to date. We aim to fill this gap by investigating recent studies in the field of negotiation dialogue systems, and covering benchmarks, evaluations and methodologies within the literature. We also discuss potential future directions, including multi-modal, multi-party and cross-cultural negotiation scenarios. Our goal is to provide the community with a systematic overview of negotiation dialogue systems and to inspire future research.</abstract>
      <url hash="5722240a">2024.findings-eacl.136</url>
      <bibkey>zhan-etal-2024-lets</bibkey>
    </paper>
    <paper id="137">
      <title>Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models</title>
      <author><first>Younghun</first><last>Lee</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University</affiliation></author>
      <author><first>Laura Schwab</first><last>Reese</last><affiliation>Purdue University</affiliation></author>
      <pages>2032-2047</pages>
      <abstract>Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to conversations.</abstract>
      <url hash="000711a7">2024.findings-eacl.137</url>
      <attachment type="software" hash="62e740c5">2024.findings-eacl.137.software.zip</attachment>
      <bibkey>lee-etal-2024-towards</bibkey>
    </paper>
    <paper id="138">
      <title>Better Explain Transformers by Illuminating Important Information</title>
      <author><first>Linxin</first><last>Song</last></author>
      <author><first>Yan</first><last>Cui</last></author>
      <author><first>Ao</first><last>Luo</last></author>
      <author><first>Freddy</first><last>Lecue</last><affiliation>INRIA</affiliation></author>
      <author><first>Irene</first><last>Li</last></author>
      <pages>2048-2062</pages>
      <abstract>Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperforms with over 3% to 33% improvement on explanation metrics, providing superior explanation performance. Our anonymous code repository is available at: https://anonymous.4open.science/r/MLRP-E676/</abstract>
      <url hash="ffb0def1">2024.findings-eacl.138</url>
      <bibkey>song-etal-2024-better</bibkey>
    </paper>
    <paper id="139">
      <title>Testing the Depth of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>’s Comprehension via Cross-Modal Tasks Based on <fixed-case>ASCII</fixed-case>-Art: <fixed-case>GPT</fixed-case>3.5’s Abilities in Regard to Recognizing and Generating <fixed-case>ASCII</fixed-case>-Art Are Not Totally Lacking</title>
      <author><first>David</first><last>Bayani</last><affiliation>Inpleo, Inc.</affiliation></author>
      <pages>2063-2077</pages>
      <abstract>In the months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche industry of papers have emerged examining the scope of capabilities these models possess, language — whether natural or stylized like code — has been the vehicle to exchange information with the network. Drawing inspiration from the multi-modal knowledge we’d expect an agent with true understanding to possess, we examine GPT3.5’s aptitude for visual tasks, where the inputs feature ASCII-art without overt distillation into a lingual summary. In particular, we scrutinize its performance on carefully designed image recognition and generation tasks. An extended version of this write-up is available at: https://arxiv.org/abs/2307.16806 .</abstract>
      <url hash="f92e7ca9">2024.findings-eacl.139</url>
      <bibkey>bayani-2024-testing</bibkey>
      <video href="2024.findings-eacl.139.mp4"/>
    </paper>
    <paper id="140">
      <title>Cross-lingual Editing in Multilingual Language Models</title>
      <author><first>Himanshu</first><last>Beniwal</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <author><first>Kowsik</first><last>D</last></author>
      <author><first>Mayank</first><last>Singh</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <pages>2078-2128</pages>
      <abstract>The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (XME) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: Latin (English, French, and Spanish) and Indic (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families. These findings highlight the need for further research and development of XME techniques to address these challenges. For more comprehensive information, the dataset used in this research and the associated code are publicly available at the following [URL](https://github.com/lingo-iitgn/XME).</abstract>
      <url hash="4343bb32">2024.findings-eacl.140</url>
      <bibkey>beniwal-etal-2024-cross</bibkey>
      <video href="2024.findings-eacl.140.mp4"/>
    </paper>
    <paper id="141">
      <title>Sorted <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference</title>
      <author><first>Parsa</first><last>Kavehzadeh</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mojtaba</first><last>Valipour</last></author>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>2129-2145</pages>
      <abstract>Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the potential of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The efficacy of our proposed method was demonstrated by applying it to tune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and TriviaQA for closed-book question answering. Our results show the superior performance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT (Early-Exit), all achieved with very efficient tuning and without additional memory usage during inference.</abstract>
      <url hash="7cadd98a">2024.findings-eacl.141</url>
      <bibkey>kavehzadeh-etal-2024-sorted</bibkey>
      <video href="2024.findings-eacl.141.mp4"/>
    </paper>
    <paper id="142">
      <title><fixed-case>A</fixed-case>ccent<fixed-case>F</fixed-case>old: A Journey through <fixed-case>A</fixed-case>frican Accents for Zero-Shot <fixed-case>ASR</fixed-case> Adaptation to Target Accents</title>
      <author><first>Abraham</first><last>Owodunni</last><affiliation>Masakhane</affiliation></author>
      <author><first>Aditya</first><last>Yadavalli</last><affiliation>Karya Inc</affiliation></author>
      <author><first>Chris</first><last>Emezue</last></author>
      <author><first>Tobi</first><last>Olatunji</last></author>
      <author><first>Clinton</first><last>Mbataku</last></author>
      <pages>2146-2161</pages>
      <abstract>Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose AccentFold, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness of AccentFold by showing that, for out-of-distribution (OOD) accents, sampling accent subsets for training based on AccentFold information outperforms strong baselines a relative WER improvement of 4.6%. AccentFold presents a promising approach for improving ASR performance on accented speech, particularly in the context of African accents, where data scarcity and budget constraints pose significant challenges. Our findings emphasize the potential of leveraging linguistic relationships to improve zero-shot ASR adaptation to target accents.</abstract>
      <url hash="057ef8c0">2024.findings-eacl.142</url>
      <bibkey>owodunni-etal-2024-accentfold</bibkey>
    </paper>
    <paper id="143">
      <title>Hierarchical and Dynamic Prompt Compression for Efficient Zero-shot <fixed-case>API</fixed-case> Usage</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Marco</first><last>Vecchio</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Anders</first><last>Johannsen</last></author>
      <pages>2162-2174</pages>
      <abstract>Long prompts present a significant challenge for practical LLM-based systems that need to operate with low latency and limited resources. We investigate prompt compression for zero-shot dialogue systems that learn to use unseen APIs directly in-context from their documentation, which may take up hundreds of prompt tokens per API. We start from a recently introduced approach (Mu et al., 2023) that learns to compress the prompt into a few “gist token” activations during finetuning. However, this simple idea is ineffective in compressing API documentation, resulting in low accuracy compared to the baseline using an uncompressed prompt. In this work, we introduce two major improvements. First, we specialize gist tokens for different hierarchies within an API: we use one <tex-math>\mathrm{Gist}_{\mathrm{arg}}</tex-math> token for compressing an argument and one <tex-math>\mathrm{Gist}_{\mathrm{value}}</tex-math> token for compressing an acceptable value of a categorical argument. We then dynamically reveal <tex-math>\mathrm{Gist}_{\mathrm{value}}</tex-math> tokens only when they are needed. Second, we add a reconstruction loss to predict the API documentation from the gist tokens. On multiple API-calling tasks, our proposed system keeps the simplicity, efficiency, and large compression factor (20x on SGD) of the gist token approach while achieving significantly better accuracy.</abstract>
      <url hash="8c1adec3">2024.findings-eacl.143</url>
      <bibkey>jiang-etal-2024-hierarchical</bibkey>
      <video href="2024.findings-eacl.143.mp4"/>
    </paper>
    <paper id="144">
      <title>Fine-tuning <fixed-case>CLIP</fixed-case> Text Encoders with Two-step Paraphrasing</title>
      <author><first>Hyunjae</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Quan</first><last>Tran</last><affiliation>servicenow</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>2175-2184</pages>
      <abstract>Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 7.6% and 9.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.</abstract>
      <url hash="25c91d2e">2024.findings-eacl.144</url>
      <bibkey>kim-etal-2024-fine</bibkey>
    </paper>
    <paper id="145">
      <title>Generative Interpretation: Toward Human-Like Evaluation for Educational Question-Answer Pair Generation</title>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>2185-2196</pages>
      <abstract>Educational question-answer generation has been extensively researched owing to its practical applicability. However, we have identified a persistent challenge concerning the evaluation of such systems. Existing evaluation methods often fail to produce objective results and instead exhibit a bias towards favoring high similarity to the ground-truth question-answer pairs. In this study, we demonstrate that these evaluation methods yield low human alignment and propose an alternative approach called Generative Interpretation (GI) to achieve more objective evaluations. Through experimental analysis, we reveal that GI outperforms existing evaluation methods in terms of human alignment, and even shows comparable performance with GPT3.5, only with BART-large.</abstract>
      <url hash="7a40f6e9">2024.findings-eacl.145</url>
      <attachment type="software" hash="0e3e8735">2024.findings-eacl.145.software.zip</attachment>
      <bibkey>moon-etal-2024-generative</bibkey>
      <video href="2024.findings-eacl.145.mp4"/>
    </paper>
    <paper id="146">
      <title>Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization</title>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Technische Universität Darmstadt and Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>2197-2214</pages>
      <abstract>Pre-trained language models (PLMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics. This paper analyzes various PLMs with three probing-based experiments to better understand the reasons behind such generalization gaps. For the first time, we demonstrate that the extent of these generalization gaps and the sensitivity to token-level interventions vary significantly across PLMs. By evaluating large language models (LLMs), we show the usefulness of our analysis for these recent models. Overall, we observe diverse pre-training objectives and architectural regularization contribute to more robust PLMs and mitigate generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.</abstract>
      <url hash="a4646e90">2024.findings-eacl.146</url>
      <bibkey>waldis-etal-2024-dive</bibkey>
      <video href="2024.findings-eacl.146.mp4"/>
    </paper>
    <paper id="147">
      <title><fixed-case>LLM</fixed-case>-<fixed-case>GE</fixed-case>m: Large Language Model-Guided Prediction of People’s Empathy Levels towards Newspaper Article</title>
      <author><first>Md Rakibul</first><last>Hasan</last><affiliation>Curtin University of Technology and BRAC University, Bangladesh</affiliation></author>
      <author><first>Md Zakir</first><last>Hossain</last><affiliation>CSIRO and Australian National University</affiliation></author>
      <author><first>Tom</first><last>Gedeon</last></author>
      <author><first>Shafin</first><last>Rahman</last><affiliation>North South University</affiliation></author>
      <pages>2215-2231</pages>
      <abstract>Empathy – encompassing the understanding and supporting others’ emotions and perspectives – strengthens various social interactions, including written communication in healthcare, education and journalism. Detecting empathy using AI models by relying on self-assessed ground truth through crowdsourcing is challenging due to the inherent noise in such annotations. To this end, we propose a novel system, named Large Language Model-Guided Empathy _(LLM-GEm)_ prediction system. It rectifies annotation errors based on our defined annotation selection threshold and makes the annotations reliable for conventional empathy prediction models, e.g., BERT-based pre-trained language models (PLMs). Previously, demographic information was often integrated numerically into empathy detection models. In contrast, our _LLM-GEm_ leverages GPT-3.5 LLM to convert numerical data into semantically meaningful textual sequences, enabling seamless integration into PLMs. We experiment with three _NewsEmpathy_ datasets involving people’s empathy levels towards newspaper articles and achieve state-of-the-art test performance using a RoBERTa-based PLM. Code and evaluations are publicly available at [https://github.com/hasan-rakibul/LLM-GEm](https://github.com/hasan-rakibul/LLM-GEm).</abstract>
      <url hash="ac47d4fc">2024.findings-eacl.147</url>
      <attachment type="software" hash="b41c778e">2024.findings-eacl.147.software.zip</attachment>
      <attachment type="note" hash="291dcb66">2024.findings-eacl.147.note.zip</attachment>
      <bibkey>hasan-etal-2024-llm</bibkey>
      <revision id="1" href="2024.findings-eacl.147v1" hash="d405c649"/>
      <revision id="2" href="2024.findings-eacl.147v2" hash="ac47d4fc" date="2024-05-02">Minor updates.</revision>
      <video href="2024.findings-eacl.147.mp4"/>
    </paper>
    <paper id="148">
      <title><fixed-case>ICE</fixed-case>-Score: Instructing Large Language Models to Evaluate Code</title>
      <author><first>Terry Yue</first><last>Zhuo</last></author>
      <pages>2232-2242</pages>
      <abstract>Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (<i>human preference</i> and <i>execution success</i>) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.</abstract>
      <url hash="f121a28a">2024.findings-eacl.148</url>
      <bibkey>zhuo-2024-ice</bibkey>
    </paper>
    <paper id="149">
      <title><fixed-case>CR</fixed-case>e<fixed-case>SE</fixed-case>: Benchmark Data and Automatic Evaluation Framework for Recommending Eligibility Criteria from Clinical Trial Information</title>
      <author><first>Siun</first><last>Kim</last></author>
      <author><first>Jung-Hyun</first><last>Won</last><affiliation>Seoul National University</affiliation></author>
      <author><first>David</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Renqian</first><last>Luo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Howard</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>2243-2273</pages>
      <abstract>Eligibility criteria (EC) refer to a set of conditions an individual must meet to participate in a clinical trial, defining the study population and minimizing potential risks to patients. Previous research in clinical trial design has been primarily focused on searching for similar trials and generating EC within manual instructions, employing similarity-based performance metrics, which may not fully reflect human judgment. In this study, we propose a novel task of recommending EC based on clinical trial information, including trial titles, and introduce an automatic evaluation framework to assess the clinical validity of the EC recommendation model. Our new approach, known as CReSE (Contrastive learning and Rephrasing-based and Clinical Relevance-preserving Sentence Embedding), represents EC through contrastive learning and rephrasing via large language models (LLMs). The CReSE model outperforms existing language models pre-trained on the biomedical domain in EC clustering. Additionally, we have curated a benchmark dataset comprising 3.2M high-quality EC-title pairs extracted from 270K clinical trials available on ClinicalTrials.gov. The EC recommendation models achieve commendable performance metrics, with 49.0% precision@1 and 44.2% MAP@5 on our evaluation framework. We expect that our evaluation framework built on the CReSE model will contribute significantly to the development and assessment of the EC recommendation models in terms of clinical validity.</abstract>
      <url hash="9b808224">2024.findings-eacl.149</url>
      <attachment type="note" hash="a01f614e">2024.findings-eacl.149.note.zip</attachment>
      <bibkey>kim-etal-2024-crese</bibkey>
      <video href="2024.findings-eacl.149.mp4"/>
    </paper>
    <paper id="150">
      <title><fixed-case>BMX</fixed-case>: Boosting Natural Language Generation Metrics with Explainability</title>
      <author><first>Christoph</first><last>Leiter</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Hoa</first><last>Nguyen</last></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>2274-2288</pages>
      <abstract>State-of-the-art natural language generation evaluation metrics are based on black-box language models. Hence, recent works consider their explainability with the goals of better understandability for humans and better metric analysis, including failure cases. In contrast, we explicitly leverage explanations to boost the metrics’ performance. In particular, we perceive feature importance explanations as word-level scores, which we convert, via power means, into a segment-level score. We then combine this segment-level score with the original metric to obtain a better metric. Our tests show improvements for multiple metrics across MT and summarization datasets. While improvements on machine translation are small, they are strong for summarization. Notably, BMX with the LIME explainer and preselected parameters achieves an average improvement of 0.087 points in Spearman correlation on the system-level evaluation of SummEval.</abstract>
      <url hash="98e48521">2024.findings-eacl.150</url>
      <attachment type="software" hash="bb92b528">2024.findings-eacl.150.software.zip</attachment>
      <attachment type="note" hash="8c28b8db">2024.findings-eacl.150.note.zip</attachment>
      <bibkey>leiter-etal-2024-bmx</bibkey>
      <video href="2024.findings-eacl.150.mp4"/>
    </paper>
    <paper id="151">
      <title>Joint Inference of Retrieval and Generation for Passage Re-ranking</title>
      <author><first>Wei</first><last>Fang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>2289-2298</pages>
      <abstract>Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. In this study we focus on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.</abstract>
      <url hash="b861c24e">2024.findings-eacl.151</url>
      <attachment type="software" hash="45613d80">2024.findings-eacl.151.software.zip</attachment>
      <bibkey>fang-etal-2024-joint</bibkey>
      <video href="2024.findings-eacl.151.mp4"/>
    </paper>
    <paper id="152">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>S</fixed-case>tudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Zhiwei</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Rui</first><last>Meng</last><affiliation>SalesForce Research</affiliation></author>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Huan</first><last>Wang</last></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>2299-2315</pages>
      <abstract>Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.To further enhance the utility of DialogStudio, we identify the licenses for each dataset, design external knowledge and domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio will be made publicly accessible.</abstract>
      <url hash="152de025">2024.findings-eacl.152</url>
      <attachment type="note" hash="81f074a6">2024.findings-eacl.152.note.zip</attachment>
      <bibkey>zhang-etal-2024-dialogstudio</bibkey>
    </paper>
    <paper id="153">
      <title>Exploring hybrid approaches to readability: experiments on the complementarity between linguistic features and transformers</title>
      <author><first>Rodrigo</first><last>Wilkens</last><affiliation>UCL</affiliation></author>
      <author><first>Patrick</first><last>Watrin</last><affiliation>UCL</affiliation></author>
      <author><first>Rémi</first><last>Cardon</last><affiliation>Cental, ILC - UCLouvain</affiliation></author>
      <author><first>Alice</first><last>Pintard</last></author>
      <author><first>Isabelle</first><last>Gribomont</last><affiliation>UCLouvain</affiliation></author>
      <author><first>Thomas</first><last>François</last><affiliation>UCL</affiliation></author>
      <pages>2316-2331</pages>
      <abstract>Linguistic features have a strong contribution in the context of the automatic assessment of text readability (ARA). They have been one of the anchors between the computational and theoretical models. With the development in the ARA field, the research moved to Deep Learning (DL). In an attempt to reconcile the mixed results reported in this context, we present a systematic comparison of 6 hybrid approaches along with standard Machine Learning and DL approaches, on 4 corpora (different languages and target audiences). The various experiments clearly highlighted two rather simple hybridization methods (soft label and simple concatenation). They also appear to be the most robust on smaller datasets and across various tasks and languages. This study stands out as the first to systematically compare different architectures and approaches to feature hybridization in DL, as well as comparing performance in terms of two languages and two target audiences of the text, which leads to a clearer pattern of results.</abstract>
      <url hash="4f4857c4">2024.findings-eacl.153</url>
      <bibkey>wilkens-etal-2024-exploring</bibkey>
      <video href="2024.findings-eacl.153.mp4"/>
    </paper>
    <paper id="154">
      <title>Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models</title>
      <author><first>Maxime</first><last>Fily</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last><affiliation>LLF / Université Paris Cité</affiliation></author>
      <author><first>Severine</first><last>Guillaume</last><affiliation>CNRS</affiliation></author>
      <author><first>Gilles</first><last>Adda</last><affiliation>CNRS</affiliation></author>
      <author><first>Alexis</first><last>Michaud</last><affiliation>CNRS</affiliation></author>
      <pages>2332-2341</pages>
      <abstract>In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially opening new research avenues for comparative work on under-documented languages.</abstract>
      <url hash="be4bc756">2024.findings-eacl.154</url>
      <bibkey>fily-etal-2024-establishing</bibkey>
      <video href="2024.findings-eacl.154.mp4"/>
    </paper>
    <paper id="155">
      <title>The Queen of <fixed-case>E</fixed-case>ngland is not <fixed-case>E</fixed-case>ngland’s Queen: On the Lack of Factual Coherency in <fixed-case>PLM</fixed-case>s</title>
      <author><first>Paul</first><last>Youssef</last><affiliation>Phillips-Universität Marburg</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <pages>2342-2354</pages>
      <abstract>Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an _object_ entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the _subject_ entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.</abstract>
      <url hash="5a44cdb4">2024.findings-eacl.155</url>
      <bibkey>youssef-etal-2024-queen</bibkey>
      <video href="2024.findings-eacl.155.mp4"/>
    </paper>
    <paper id="156">
      <title><fixed-case>H</fixed-case>ierarchy<fixed-case>N</fixed-case>et: Learning to Summarize Source Code with Heterogeneous Representations</title>
      <author><first>Minh</first><last>Nguyen</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Nghi</first><last>Bui</last></author>
      <author><first>Truong Son</first><last>Hy</last><affiliation>Indiana State University</affiliation></author>
      <author><first>Long</first><last>Tran-Thanh</last><affiliation>The university of Warwick</affiliation></author>
      <author><first>Tien</first><last>Nguyen</last><affiliation>university of texas at dallas</affiliation></author>
      <pages>2355-2367</pages>
      <abstract>Code representation is important to machine learning models in the code-related applications. Existing code summarization approaches primarily leverage Abstract Syntax Trees (ASTs) and sequential information from source code to generate code summaries while often overlooking the critical consideration of the interplay of dependencies among code elements and code hierarchy. However, effective summarization necessitates a holistic analysis of code snippets from three distinct aspects: lexical, syntactic, and semantic information. In this paper, we propose a novel code summarization approach utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs adeptly capture essential code features at lexical, syntactic, and semantic levels within a hierarchical structure. HierarchyNet processes each layer of the HCR separately, employing a Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. In addition, HierarchyNet demonstrates superior performance compared to fine-tuned pre-trained models, including CodeT5, and CodeBERT, as well as large language models that employ zero/few-shot settings, such as CodeLlama, StarCoder, and CodeGen. Implementation details can be found at https://github.com/FSoft-AI4Code/HierarchyNet.</abstract>
      <url hash="3eef3fbe">2024.findings-eacl.156</url>
      <bibkey>nguyen-etal-2024-hierarchynet</bibkey>
      <video href="2024.findings-eacl.156.mp4"/>
    </paper>
    <paper id="157">
      <title>Understanding the effects of language-specific class imbalance in multilingual fine-tuning</title>
      <author><first>Vincent</first><last>Jung</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Lonneke</first><last>Plas</last><affiliation>Idiap Research Institute</affiliation></author>
      <pages>2368-2376</pages>
      <abstract>We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.</abstract>
      <url hash="a17b7cc2">2024.findings-eacl.157</url>
      <bibkey>jung-plas-2024-understanding</bibkey>
      <video href="2024.findings-eacl.157.mp4"/>
    </paper>
    <paper id="158">
      <title><fixed-case>NL</fixed-case>2<fixed-case>F</fixed-case>ormula: Generating Spreadsheet Formulas from Natural Language Queries</title>
      <author><first>Wei</first><last>Zhao</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zhitao</first><last>Hou</last></author>
      <author><first>Siyuan</first><last>Wu</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>University of Newcastle, Australia</affiliation></author>
      <author><first>Yulei</first><last>Sui</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Haidong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>2377-2388</pages>
      <abstract>Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.</abstract>
      <url hash="310ef33c">2024.findings-eacl.158</url>
      <bibkey>zhao-etal-2024-nl2formula</bibkey>
    </paper>
  </volume>
  <volume id="naacl" ingest-date="2024-06-10" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: NAACL 2024</booktitle>
      <editor><first>Kevin</first><last>Duh</last></editor>
      <editor><first>Helena</first><last>Gomez</last></editor>
      <editor><first>Steve</first><last>Bethard</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="7b3934ed">2024.findings-naacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="7be083cc">2024.findings-naacl.0</url>
    </frontmatter>
    <paper id="1">
      <title>Structured Pruning for Large Language Models Using Coupled Components Elimination and Minor Fine-tuning</title>
      <author><first>Honghe</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>XiaolongShi</first><last>XiaolongShi</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jingwei</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Guangzhong</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>1-12</pages>
      <abstract>Large language models (LLMs) have demonstrated powerful capabilities in natural language processing, yet their vast number of parameters poses challenges for deployment and inference efficiency. Structured model pruning emerges as a viable approach to reduce model size and accelerate inference, without requiring specialized operators and libraries for deployment. However, structured pruning often severely weakens the model’s capability.Despite repetitive fine-tuning can restore the capability to a certain extent, it impairs LLMs’ utility as versatile problem solvers.To address this issue, we propose a novel structured pruning algorithm tailored for LLMs. It derives the importance of different components, namely rows and columns in parameter matrices, based on intermediate data dependencies. Then it removes coupled components across different layers simultaneously and preserves dependency relationships within remaining parameters, avoiding significant performance degradation. The pruned model requires only few epochs of fine-tuning to restore its performance, ensuring the model’s ability to generalize.Empirical evaluations on LLaMA, Vicuna, and ChatGLM3 demonstrate our algorithm’s efficacy, yielding 20% parameter reduction while retaining at least 94.4% of original performance metrics.</abstract>
      <url hash="08576512">2024.findings-naacl.1</url>
    </paper>
    <paper id="2">
      <title>Weight-Inherited Distillation for Task-Agnostic <fixed-case>BERT</fixed-case> Compression</title>
      <author><first>Taiqiang</first><last>Wu</last></author>
      <author><first>Cheng</first><last>Hou</last></author>
      <author><first>Shanshan</first><last>Lao</last></author>
      <author><first>Jiayi</first><last>Li</last></author>
      <author><first>Ngai</first><last>Wong</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <pages>13-28</pages>
      <abstract>Knowledge Distillation (KD) is a predominant approach for BERT compression.Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model.These methods transfer the knowledge in an indirect way.In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher.WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation.Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization.Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines.Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.The code is available at https://github.com/wutaiqiang/WID-NAACL2024.</abstract>
      <url hash="fc6d5b73">2024.findings-naacl.2</url>
    </paper>
    <paper id="3">
      <title>Ignore Me But Don’t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain</title>
      <author><first>Eugene</first><last>Jang</last><affiliation>S2W Inc.</affiliation></author>
      <author><first>Jian</first><last>Cui</last></author>
      <author><first>Dayeon</first><last>Yim</last><affiliation>S2W Inc.</affiliation></author>
      <author><first>Youngjin</first><last>Jin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jin-Woo</first><last>Chung</last></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yongjae</first><last>Lee</last><affiliation>S2W Inc.</affiliation></author>
      <pages>29-42</pages>
      <abstract>Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We experiment with different pretraining methodologies to account for non-linguistic elements (NLEs) and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy, a combination of selective MLM and jointly training NLE token classification, outperforms the commonly taken approach of replacing NLEs. We use our domain-customized methodology to train CyBERTuned, a cybersecurity domain language model that outperforms other cybersecurity PLMs on most tasks.</abstract>
      <url hash="e254e578">2024.findings-naacl.3</url>
    </paper>
    <paper id="4">
      <title>Extremely efficient online query encoding for dense retrieval</title>
      <author><first>Nachshon</first><last>Cohen</last><affiliation>Amazon</affiliation></author>
      <author><first>Yaron</first><last>Fairstein</last><affiliation>Amazon</affiliation></author>
      <author><first>Guy</first><last>Kushilevitz</last><affiliation>Amazon</affiliation></author>
      <pages>43-50</pages>
      <abstract>Existing dense retrieval systems utilize the same model architecture for encoding both the passages and the queries, even though queries are much shorter and simpler than passages. This leads to high latency of the query encoding, which is performed online and therefore might impact user experience. We show that combining a standard large passage encoder with a small efficient query encoder can provide significant latency drops with only a small decrease in quality. We offer a pretraining and training solution for multiple small query encoder architectures. Using a small transformer architecture we are able to decrease latency by up to <tex-math>\sim12\times</tex-math>, while <tex-math>MRR@10</tex-math> on the MS MARCO dev set only decreases from 38.2 to 36.2. If this solution does not reach the desired latency requirements, we propose an efficient RNN as the query encoder, which processes the query prefix incrementally and only infers the last word after the query is issued. This shortens latency by <tex-math>\sim38\times</tex-math> with only a minor drop in quality, reaching 35.5 <tex-math>MRR@10</tex-math> score.</abstract>
      <url hash="bf378671">2024.findings-naacl.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>DIVKNOWQA</fixed-case>: Assessing the Reasoning Ability of <fixed-case>LLM</fixed-case>s via Open-Domain Question Answering over Knowledge Base and Text</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Tong</first><last>Niu</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <pages>51-68</pages>
      <abstract>Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrievalaugmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) Generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.</abstract>
      <url hash="eb80471c">2024.findings-naacl.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>peed<fixed-case>E</fixed-case>: <fixed-case>E</fixed-case>uclidean Geometric Knowledge Graph Embedding Strikes Back</title>
      <author><first>Aleksandar</first><last>Pavlović</last></author>
      <author><first>Emanuel</first><last>Sallinger</last><affiliation>TU Wien (Vienna University of Technology)</affiliation></author>
      <pages>69-92</pages>
      <abstract>Geometric knowledge graph embedding models (gKGEs) have shown great potential for knowledge graph completion (KGC), i.e., automatically predicting missing triples. However, contemporary gKGEs require high embedding dimensionalities or complex embedding spaces for good KGC performance, drastically limiting their space and time efficiency. Facing these challenges, we propose SpeedE, a lightweight Euclidean gKGE that (1) provides strong inference capabilities, (2) is competitive with state-of-the-art gKGEs, even significantly outperforming them on YAGO3-10 and WN18RR, and (3) dramatically increases their efficiency, in particular, needing solely a fifth of the training time and a fourth of the parameters of the state-of-the-art ExpressivE model on WN18RR to reach the same KGC performance.</abstract>
      <url hash="653eddea">2024.findings-naacl.6</url>
    </paper>
    <paper id="7">
      <title>Language Guided Exploration for <fixed-case>RL</fixed-case> Agents in Text Environments</title>
      <author><first>Hitesh</first><last>Golchha</last></author>
      <author><first>Sahil</first><last>Yerawar</last></author>
      <author><first>Dhruvesh</first><last>Patel</last><affiliation>College of Information and Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <pages>93-102</pages>
      <abstract>Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like <tex-math>\textit{tabula rasa}</tex-math> reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER ). We observe that on ScienceWorld (Wang et al., 2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.</abstract>
      <url hash="8cc2e9f4">2024.findings-naacl.7</url>
    </paper>
    <paper id="8">
      <title><fixed-case>GPT</fixed-case>-who: An Information Density-based Machine-Generated Text Detector</title>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Adaku</first><last>Uchendu</last><affiliation>MIT Lincoln Laboratory, Massachusetts Institute of Technology</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>103-115</pages>
      <abstract>The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)-generated and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based featuresto model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp; non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over 20% across domains.In addition to better performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible.UID-based measures for all datasets and code are available at https://github.com/saranya-venkatraman/gpt-who.</abstract>
      <url hash="7bd26e5e">2024.findings-naacl.8</url>
    </paper>
    <paper id="9">
      <title><fixed-case>DEED</fixed-case>: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</title>
      <author><first>Peng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Pengkai</first><last>Zhu</last><affiliation>Boston University</affiliation></author>
      <author><first>Tian</first><last>Li</last></author>
      <author><first>Srikar</first><last>Appalaraju</last><affiliation>Amazon</affiliation></author>
      <author><first>Vijay</first><last>Mahadevan</last><affiliation>Amazon</affiliation></author>
      <author><first>R.</first><last>Manmatha</last><affiliation>Amazon</affiliation></author>
      <pages>116-131</pages>
      <abstract>Encoder-decoder transformer models have achieved great success on various vision-language (VL) and language tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with three state-of-the-art encoder-decoder transformer models on various VL and language tasks. We show our approach can reduce overall inference latency by 20%-74% with comparable or even higher accuracy compared to baselines.</abstract>
      <url hash="918b7d9f">2024.findings-naacl.9</url>
    </paper>
    <paper id="10">
      <title>Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation</title>
      <author><first>Ta-Chung</first><last>Chi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ting-Han</first><last>Fan</last></author>
      <author><first>Alexander</first><last>Rudnicky</last><affiliation>Carnegie Mellon University and Carnegie Mellon University</affiliation></author>
      <pages>132-148</pages>
      <abstract>An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any fine-tuning. Such long-context utilization capability relies heavily on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings show improvement on the long-context utilization capability of T5 on language modeling, retrieval, multi-document question answering, and code completion tasks without any fine-tuning. This suggests that a flexible positional embedding design and attention alignment can go a long way toward Transformer length extrapolation. The code is released at: <url>https://github.com/chijames/T5-Attention-Alignment</url></abstract>
      <url hash="8ed04a10">2024.findings-naacl.10</url>
    </paper>
    <paper id="11">
      <title>Automatic Pair Construction for Contrastive Post-training</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Corby</first><last>Rosset</last></author>
      <author><first>Ethan</first><last>Chau</last><affiliation>Microsoft</affiliation></author>
      <author><first>Luciano</first><last>Corro</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Shweti</first><last>Mahajan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Jennifer</first><last>Neville</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Ahmed</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nikhil</first><last>Rao</last><affiliation>Microsoft</affiliation></author>
      <pages>149-162</pages>
      <abstract>Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from “easier” pairs and transitioning to “harder” ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.</abstract>
      <url hash="50682bfd">2024.findings-naacl.11</url>
    </paper>
    <paper id="12">
      <title>Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models</title>
      <author><first>Miaoran</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Michel</first><last>Galley</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhu</first><last>Zhang</last></author>
      <pages>163-181</pages>
      <abstract>Fact-checking is an essential task in NLP that is commonly utilized to validate the factual accuracy of a piece of text. Previous approaches mainly involve the resource-intensive process of fine-tuning pre-trained language models on specific datasets. In addition, there is a notable gap in datasets that focus on fact-checking texts generated by large language models (LLMs). In this paper, we introduce Self-Checker, a plug-and-play framework that harnesses LLMs for efficient and rapid fact-checking in a few-shot manner. We also present the BingCheck dataset, specifically designed for fact-checking texts generated by LLMs. Empirical results demonstrate the potential of Self-Checker in the use of LLMs for fact-checking. Compared to state-of-the-art fine-tuned models, there is still significant room for improvement, indicating that adopting LLMs could be a promising direction for future fact-checking research.</abstract>
      <url hash="0b3f79fb">2024.findings-naacl.12</url>
    </paper>
    <paper id="13">
      <title>Low-resource neural machine translation with morphological modeling</title>
      <author><first>Antoine</first><last>Nzeyimana</last></author>
      <pages>182-195</pages>
      <abstract>Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated and shown to increase translation performance in low-resource settings. We evaluate our proposed solution on Kinyarwanda <tex-math>\leftrightarrow</tex-math> English translation using public-domain parallel text. Our final models achieve competitive performance in relation to large multi-lingual models. We hope that our results will motivate more use of explicit morphological information and the proposed model and data augmentations in low-resource NMT.</abstract>
      <url hash="43cf480e">2024.findings-naacl.13</url>
    </paper>
    <paper id="14">
      <title>Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances</title>
      <author><first>Zhendong</first><last>Chu</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <author><first>Vlad</first><last>Morariu</last><affiliation>Adobe</affiliation></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>196-210</pages>
      <abstract>To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.</abstract>
      <url hash="5910a6ac">2024.findings-naacl.14</url>
    </paper>
    <paper id="15">
      <title><fixed-case>VLUE</fixed-case>: A New Benchmark and Multi-task Knowledge Transfer Learning for <fixed-case>V</fixed-case>ietnamese Natural Language Understanding</title>
      <author><first>Phong</first><last>Do</last><affiliation>The UIT NLP Group and Zalo</affiliation></author>
      <author><first>Son</first><last>Tran</last></author>
      <author><first>Phu</first><last>Hoang</last></author>
      <author><first>Kiet</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <author><first>Ngan</first><last>Nguyen</last></author>
      <pages>211-222</pages>
      <abstract>The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.</abstract>
      <url hash="bd106f14">2024.findings-naacl.15</url>
    </paper>
    <paper id="16">
      <title><fixed-case>LETI</fixed-case>: Learning to Generate from Textual Interactions</title>
      <author><first>Xingyao</first><last>Wang</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Reyhaneh</first><last>Jabbarvand</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>223-239</pages>
      <abstract>Fine-tuning pre-trained language models (LMs) is essential for enhancing their capabilities.Existing techniques commonly fine-tune on input-output pairs (e.g., instruction tuning) or with numerical rewards that gauge the output quality (e.g., RLHF). We explore LMs’ potential to **le**arn from **t**extual **i**nteractions (**LETI**) that not only check their correctness with *binary labels* but also pinpoint and explain errors in their outputs through *textual feedback*.Our focus is the code generation task, where the model produces code based on natural language instructions. This setting invites a natural and scalable way to acquire textual feedback: the error messages and stack traces from code execution using a Python interpreter. LETI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions.LETI requires *no* ground-truth outputs for training and even outperforms a fine-tuned baseline that does. LETI not only improves the performance of LMs on a code generation dataset MBPP, but also generalizes to other datasets. Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps.LETI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.</abstract>
      <url hash="edab0c21">2024.findings-naacl.16</url>
    </paper>
    <paper id="17">
      <title>Bilateral Masking with prompt for Knowledge Graph Completion</title>
      <author><first>Yonghui</first><last>Kong</last></author>
      <author><first>Cunhang</first><last>Fan</last><affiliation>School of Computer Science and Technology, Anhui University, Hefei 230601, China</affiliation></author>
      <author><first>Yujie</first><last>Chen</last></author>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Zhao</first><last>Lv</last><affiliation>School of Computer Science and Technology, Anhui University, Hefei 230601, China</affiliation></author>
      <author><first>Jianhua</first><last>Tao</last><affiliation>Tsinghua University</affiliation></author>
      <pages>240-249</pages>
      <abstract>The pre-trained language model (PLM) has achieved significant success in the field of knowledge graph completion (KGC) by effectively modeling entity and relation descriptions. In recent studies, the research in this field has been categorized into methods based on word matching and sentence matching, with the former significantly lags behind. However, there is a critical issue in word matching methods, which is that these methods fail to obtain satisfactory single embedding representations for entities.To address this issue and enhance entity representation, we propose the Bilateral Masking with prompt for Knowledge Graph Completion (BMKGC) approach.Our methodology employs prompts to narrow the distance between the predicted entity and the known entity. Additionally, the BMKGC model incorporates a bi-encoder architecture, enabling simultaneous predictions at both the head and tail. Furthermore, we propose a straightforward technique to augment positive samples, mitigating the problem of degree bias present in knowledge graphs and thereby improving the model’s robustness. Experimental results conclusively demonstrate that BMKGC achieves state-of-the-art performance on the WN18RR dataset.</abstract>
      <url hash="7220b607">2024.findings-naacl.17</url>
    </paper>
    <paper id="18">
      <title><fixed-case>M</fixed-case>i<fixed-case>L</fixed-case>e Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models</title>
      <author><first>Zhenpeng</first><last>Su</last></author>
      <author><first>Zijia</first><last>Lin</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Baixue</first><last>Baixue</last></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Guiguang</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xing</first><last>W</last></author>
      <pages>250-262</pages>
      <abstract>Generative language models are usually pre-trained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose a **MiLe Loss** function for **mi**tigating the bias of **le**arning difficulties with tokens. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that models incorporating the proposed MiLe Loss can gain consistent performance improvement on downstream benchmarks.</abstract>
      <url hash="a60fe50c">2024.findings-naacl.18</url>
    </paper>
    <paper id="19">
      <title><fixed-case>GOLD</fixed-case>: Geometry Problem Solver with Natural Language Description</title>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>University of Strathclyde</affiliation></author>
      <author><first>Yashar</first><last>Moshfeghi</last><affiliation>University of Strathclyde</affiliation></author>
      <pages>263-278</pages>
      <abstract>Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics. blackCurrent methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving. To tackle this issue, we present the <b>G</b>eometry problem s<b>O</b>lver with natural <b>L</b>anguage <b>D</b>escription (GOLD) model. GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram. Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems. Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets. Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively.</abstract>
      <url hash="e6718134">2024.findings-naacl.19</url>
    </paper>
    <paper id="20">
      <title><fixed-case>R</fixed-case>o<fixed-case>D</fixed-case>ia: A New Dataset for <fixed-case>R</fixed-case>omanian Dialect Identification from Speech</title>
      <author><first>Rotaru</first><last>Codruț</last></author>
      <author><first>Nicolae</first><last>Ristea</last></author>
      <author><first>Radu</first><last>Ionescu</last><affiliation>Universitatea Bucuresti</affiliation></author>
      <pages>279-286</pages>
      <abstract>We introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource that will stimulate research aiming to address the challenges of Romanian dialect identification. We release our dataset at https://github.com/codrut2/RoDia.</abstract>
      <url hash="ff42ecca">2024.findings-naacl.20</url>
    </paper>
    <paper id="21">
      <title>Examining Modularity in Multilingual <fixed-case>LM</fixed-case>s via Language-Specialized Subnetworks</title>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Dan</first><last>Garrette</last><affiliation>Google Research</affiliation></author>
      <pages>287-301</pages>
      <abstract>Recent work has proposed explicitly inducing language-wise modularity in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a means of better guiding cross-lingual sharing. In this paper, we investigate (1) the degree to which language-wise modularity *naturally* arises within models with no special modularity interventions, and (2) how cross-lingual sharing and interference differ between such models and those with explicit SFT-guided subnetwork modularity. In order to do so, we use XLM-R as our multilingual LM. Moreover, to quantify language specialization and cross-lingual interaction, we use a Training Data Attribution method that estimates the degree to which a model’s predictions are influenced by in-language or cross-language training examples. Our results show that language-specialized subnetworks do naturally arise, and that SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing.</abstract>
      <url hash="16d37d5a">2024.findings-naacl.21</url>
    </paper>
    <paper id="22">
      <title>Reverse Chain: A Generic-Rule for <fixed-case>LLM</fixed-case>s to Master Multi-<fixed-case>API</fixed-case> Planning</title>
      <author><first>Yinger</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Cai</last></author>
      <author><first>Xierui</first><last>Song</last></author>
      <author><first>Yicheng</first><last>Chen</last></author>
      <author><first>Rui</first><last>Sun</last></author>
      <author><first>Jing</first><last>Zheng</last><affiliation>Ant Group</affiliation></author>
      <pages>302-325</pages>
      <abstract>While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of Large Language Models (LLMs), function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper introduces “Reverse Chain”, a controllable, target-driven approach designed to empower LLMs with the capability to operate external APIs only via prompts. Recognizing that most LLMs have limited tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument Completion. Furthermore, to manage a controllable multi-function calling, Reverse Chain adopts a generic rule-based on a backward reasoning process. This rule determines when to do API selection or Argument completion. To evaluate the multi-tool-use capability of LLMs, we have released a compositional multi-tool task dataset, available at https://github.com/zhangyingerjelly/reverse-chain. Extensive numerical experiments validate the remarkable proficiency of Reverse Chain in managing multiple API calls.</abstract>
      <url hash="109c94ed">2024.findings-naacl.22</url>
    </paper>
    <paper id="23">
      <title>Incorporating Exponential Smoothing into <fixed-case>MLP</fixed-case>: a Simple but Effective Sequence Model</title>
      <author><first>JiqunChu</first><last>JiqunChu</last></author>
      <author><first>Zuoquan</first><last>Lin</last><affiliation>Peking University</affiliation></author>
      <pages>326-337</pages>
      <abstract>Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.</abstract>
      <url hash="b9b797d8">2024.findings-naacl.23</url>
    </paper>
    <paper id="24">
      <title><fixed-case>O</fixed-case>pen<fixed-case>FMN</fixed-case>av: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models</title>
      <author><first>Yuxuan</first><last>Kuang</last></author>
      <author><first>Hai</first><last>Lin</last><affiliation>University of Notre Dame and University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>338-351</pages>
      <abstract>Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose **OpenFMNav**, an **Open**-set **F**oundation **M**odel based framework for zero-shot object **Nav**igation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user’s demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a *Versatile Semantic Score Map (VSSM)*. Then, by conducting common sense reasoning on *VSSM*, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method’s effectiveness. Furthermore, we perform real robot demonstrations to validate our method’s open-set-ness and generalizability to real-world environments.</abstract>
      <url hash="bd82aee4">2024.findings-naacl.24</url>
    </paper>
    <paper id="25">
      <title>Comparing Two Model Designs for Clinical Note Generation; Is an <fixed-case>LLM</fixed-case> a Useful Evaluator of Consistency?</title>
      <author><first>Nathan</first><last>Brake</last><affiliation>3M</affiliation></author>
      <author><first>Thomas</first><last>Schaaf</last></author>
      <pages>352-363</pages>
      <abstract>Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.</abstract>
      <url hash="342f49a0">2024.findings-naacl.25</url>
    </paper>
    <paper id="26">
      <title><fixed-case>VOLTA</fixed-case>: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder</title>
      <author><first>Yueen</first><last>Ma</last></author>
      <author><first>DaFeng</first><last>Chi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jingjing</first><last>Li</last></author>
      <author><first>Kai</first><last>Song</last></author>
      <author><first>Yuzheng</first><last>Zhuang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>364-378</pages>
      <abstract>The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support for continuous inputs. We perform comprehensive experiments with two types of Transformers on six datasets from three different NLG tasks to show that our approach can significantly improve generative diversity while maintaining generative quality.</abstract>
      <url hash="28417884">2024.findings-naacl.26</url>
    </paper>
    <paper id="27">
      <title><fixed-case>E</fixed-case>co<fixed-case>S</fixed-case>peak: Cost-Efficient Bias Mitigation for Partially Cross-Lingual Speaker Verification</title>
      <author><first>Divya</first><last>Sharma</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>379-394</pages>
      <abstract>Linguistic bias is a critical problem concerning the diversity, equity, and inclusiveness of Natural Language Processing tools. The severity of this problem intensifies in security systems, such as speaker verification, where fairness is paramount. Speaker verification systems are biometric systems that determine whether two speech recordings are of the same speaker. Such user-centric systems should be inclusive to bilingual speakers. However, Deep neural network models are linguistically biased. Linguistic bias can be full or partial. Partially cross-lingual bias occurs when one test trial pair recording is in the training set’s language, and the other is in an unseen target language. Such linguistic mismatch influences the speaker verification model’s decision, dissuading bilingual speakers from using the system. Domain adaptation can mitigate this problem. However, adapting to each existing language is expensive. This paper explores cost-efficient bias mitigation techniques for partially cross-lingual speaker verification. We study the behavior of five baselines in five partially cross-lingual scenarios. Using our baseline behavioral insights, we propose EcoSpeak, a low-cost solution to partially cross-lingual speaker verification. EcoSpeak incorporates contrastive linguistic (CL) attention. CL attention utilizes linguistic differences in trial pairs to emphasize relevant speaker verification embedding parts. Experimental results demonstrate EcoSpeak’s robustness to partially cross-lingual testing.</abstract>
      <url hash="e4b2fc1e">2024.findings-naacl.27</url>
    </paper>
    <paper id="28">
      <title>Leveraging Contextual Information for Effective Entity Salience Detection</title>
      <author><first>Rajarshi</first><last>Bhowmik</last><affiliation>Bloomberg L.P.</affiliation></author>
      <author><first>Marco</first><last>Ponza</last></author>
      <author><first>Atharva</first><last>Tendle</last></author>
      <author><first>Anant</first><last>Gupta</last></author>
      <author><first>Rebecca</first><last>Jiang</last></author>
      <author><first>Xingyu</first><last>Lu</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Qian</first><last>Zhao</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last><affiliation>Bloomberg</affiliation></author>
      <pages>395-408</pages>
      <abstract>In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task’s uniqueness and complexity.</abstract>
      <url hash="296c7ab9">2024.findings-naacl.28</url>
    </paper>
    <paper id="29">
      <title><fixed-case>LLM</fixed-case>-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?</title>
      <author><first>Qihui</first><last>Zhang</last></author>
      <author><first>Chujie</first><last>Gao</last></author>
      <author><first>Dongping</first><last>Chen</last></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Yixin</first><last>Huang</last></author>
      <author><first>Zhenyang</first><last>Sun</last></author>
      <author><first>Shilin</first><last>Zhang</last></author>
      <author><first>Weiye</first><last>Li</last></author>
      <author><first>Zhengyan</first><last>Fu</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <pages>409-436</pages>
      <abstract>With the rapid development and widespread application of Large Language Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly common, bringing with it potential risks, especially in terms of quality and integrity in fields like news, education, and science. Current research mainly focuses on purely MGT detection, without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge, we define mixtext, a form of mixed text involving both AI and human-generated content. Then we introduce MixSet, the first dataset dedicated to studying these mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to assess the efficacy of prevalent MGT detectors in handling mixtext situations, evaluating their performance in terms of effectiveness, robustness, and generalization. Our findings reveal that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixtext, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet.</abstract>
      <url hash="c7ea8e78">2024.findings-naacl.29</url>
    </paper>
    <paper id="30">
      <title>A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection</title>
      <author><first>Ivo</first><last>Verhoeven</last></author>
      <author><first>Pushkar</first><last>Mishra</last><affiliation>Meta AI</affiliation></author>
      <author><first>Rahel</first><last>Beloch</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last><affiliation>Computer Laboratory, University of Cambridge and King’s College London</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>437-463</pages>
      <abstract>Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup.</abstract>
      <url hash="1cae9f0d">2024.findings-naacl.30</url>
    </paper>
    <paper id="31">
      <title>Citation: A Key to Building Responsible and Accountable Large Language Models</title>
      <author><first>Jie</first><last>Huang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>464-473</pages>
      <abstract>Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify “citation”—the acknowledgement or reference to a source or evidence—as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.</abstract>
      <url hash="4163e006">2024.findings-naacl.31</url>
    </paper>
    <paper id="32">
      <title>Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational <fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>ncoders</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Opole and University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>474-489</pages>
      <abstract>The injection of syntactic information in Variational AutoEncoders (VAEs) can result in an overall improvement of performances and generalisation. An effective strategy to achieve such a goal is to separate the encoding of distributional semantic features and syntactic structures into heterogeneous latent spaces via multi-task learning or dual encoder architectures. However, existing works employing such techniques are limited to LSTM-based VAEs. This work investigates latent space separation methods for structural syntactic injection in Transformer-based VAE architectures (i.e., Optimus) through the integration of graph-based models. Our empirical evaluation reveals that the proposed end-to-end VAE architecture can improve theoverall organisation of the latent space, alleviating the information loss occurring in standard VAE setups, and resulting in enhanced performances on language modelling and downstream generation tasks.</abstract>
      <url hash="73dfa0d8">2024.findings-naacl.32</url>
    </paper>
    <paper id="33">
      <title>Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles</title>
      <author><first>Weiting</first><last>Tan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Haoran</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Lingfeng</first><last>Shen</last></author>
      <author><first>Shuyue Stella</first><last>Li</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Yunmo</first><last>Chen</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>490-502</pages>
      <abstract>Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.</abstract>
      <url hash="7b22ed95">2024.findings-naacl.33</url>
    </paper>
    <paper id="34">
      <title>Which Modality should <fixed-case>I</fixed-case> use - Text, Motif, or Image? : Understanding Graphs with Large Language Models</title>
      <author><first>Debarati</first><last>Das</last></author>
      <author><first>Ishaan</first><last>Gupta</last></author>
      <author><first>Jaideep</first><last>Srivastava</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>503-519</pages>
      <abstract>Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph’s global connectivity, thereby enhancing LLMs’ efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and comes close to prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the performance of each encoding modality and outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks. Our code and data are publicly available on our project page - https://minnesotanlp.github.io/GraphLLM/</abstract>
      <url hash="cc82bb83">2024.findings-naacl.34</url>
    </paper>
    <paper id="35">
      <title>On-the-Fly Fusion of Large Language Models and Machine Translation</title>
      <author><first>Hieu</first><last>Hoang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huda</first><last>Khayrallah</last><affiliation>Microsoft</affiliation></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last><affiliation>Microsoft</affiliation></author>
      <pages>520-532</pages>
      <abstract>We propose on-the-fly ensembling of a neural machine translation (NMT) model with a large language model (LLM), prompted on the same task and input. Through experiments on 4 language directions with varying data amounts, we find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and such an ensemble can produce better translations than ensembling two stronger NMT models.We demonstrate that our ensemble method can be combined with various techniques from LLM prompting, such as in context learning and translation context.</abstract>
      <url hash="d2f8e37a">2024.findings-naacl.35</url>
    </paper>
    <paper id="36">
      <title><fixed-case>READ</fixed-case>: Improving Relation Extraction from an <fixed-case>AD</fixed-case>versarial Perspective</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>William</first><last>Hogan</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>533-548</pages>
      <abstract>Recent works in relation extraction (RE) have achieved promising benchmark accuracy; however, our adversarial attack experiments show that these works excessively rely on entities, making their generalization capability questionable. To address this issue, we propose an adversarial training method specifically designed for RE. Our approach introduces both sequence- and token-level perturbations to the sample and uses a separate perturbation vocabulary to improve the search for entity and context perturbations.Furthermore, we introduce a probabilistic strategy for leaving clean tokens in the context during adversarial training. This strategy enables a larger attack budget for entities and coaxes the model to leverage relational patterns embedded in the context. Extensive experiments show that compared to various adversarial training methods, our method significantly improves both the accuracy and robustness of the model. Additionally, experiments on different data availability settings highlight the effectiveness of our method in low-resource scenarios.We also perform in-depth analyses of our proposed method and provide further hints.We will release our code at https://github.com/David-Li0406/READ.</abstract>
      <url hash="ce89dc86">2024.findings-naacl.36</url>
    </paper>
    <paper id="37">
      <title><fixed-case>REQUAL</fixed-case>-<fixed-case>LM</fixed-case>: Reliability and Equity through Aggregation in Large Language Models</title>
      <author><first>Sana</first><last>Ebrahimi</last></author>
      <author><first>Nima</first><last>Shahbazi</last></author>
      <author><first>Abolfazl</first><last>Asudeh</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>549-560</pages>
      <abstract>The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing. In particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data, raises critical concerns regarding reliability and equity. Addressing these challenges are necessary before using LLMs for applications with societal impact. Towards addressing this gap, we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM outputs through aggregation. Specifically, we develop a Montecarlo method based on repeated sampling to find a reliable output close to the mean of the underlying distribution of possible outputs. We formally define the terms such as reliability and bias, and design an equity-aware aggregation to minimize harmful bias while finding a highly reliable output. REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox. This design choice enables seamless scalability alongside the rapid advancement of LLM technologies. Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt. Our comprehensive experiments using various tasks and datasets demonstrate that REQUAL-LM effectively mitigates bias and selects a more equitable response, specifically the outputs that properly represents minority groups.</abstract>
      <url hash="2a5106ac">2024.findings-naacl.37</url>
    </paper>
    <paper id="38">
      <title>Addressing Both Statistical and Causal Gender Fairness in <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Hannah</first><last>Chen</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <author><first>David</first><last>Evans</last><affiliation>University of Virginia</affiliation></author>
      <pages>561-582</pages>
      <abstract>Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</abstract>
      <url hash="895dab56">2024.findings-naacl.38</url>
    </paper>
    <paper id="39">
      <title><fixed-case>LLM</fixed-case>-Rec: Personalized Recommendation via Prompting Large Language Models</title>
      <author><first>Hanjia</first><last>Lyu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Song</first><last>Jiang</last></author>
      <author><first>Hanqing</first><last>Zeng</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yinglong</first><last>Xia</last><affiliation>Meta</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Si</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Ren</first><last>Chen</last></author>
      <author><first>Chris</first><last>Leung</last><affiliation>Meta AI and College of Computing, Georgia Institute of Technology</affiliation></author>
      <author><first>Jiajie</first><last>Tang</last></author>
      <author><first>Jiebo</first><last>Luo</last><affiliation>University of Rochester, University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>583-612</pages>
      <abstract>Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model’s comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.</abstract>
      <url hash="e11fbd97">2024.findings-naacl.39</url>
    </paper>
    <paper id="40">
      <title>A Robust Semantics-based Watermark for Large Language Model against Paraphrasing</title>
      <author><first>Jie</first><last>Ren</last><affiliation>Baidu and Michigan State University</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yiding</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Yingqian</first><last>Cui</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>613-625</pages>
      <abstract>Large language models (LLMs) have show their remarkable ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermarks can be easily eliminated by paraphrase and, correspondingly, the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework, SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the semantic meaning of the sentences will be likely preserved under paraphrase and the watermark can remain robust. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.</abstract>
      <url hash="9c915b44">2024.findings-naacl.40</url>
    </paper>
    <paper id="41">
      <title>Solving Data-centric Tasks using Large Language Models</title>
      <author><first>Shraddha</first><last>Barke</last></author>
      <author><first>Christian</first><last>Poelitz</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Carina</first><last>Negreanu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Benjamin</first><last>Zorn</last></author>
      <author><first>José</first><last>Cambronero</last><affiliation>Microsoft</affiliation></author>
      <author><first>Andrew</first><last>Gordon</last><affiliation>University of Edinburgh and Microsoft Research</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Elnaz</first><last>Nouri</last></author>
      <author><first>Nadia</first><last>Polikarpova</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Advait</first><last>Sarkar</last><affiliation>Microsoft</affiliation></author>
      <author><first>Brian</first><last>Slininger</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Neil</first><last>Toronto</last></author>
      <author><first>Jack</first><last>Williams</last><affiliation>Microsoft</affiliation></author>
      <pages>626-638</pages>
      <abstract>Large language models are rapidly replacing help forums like StackOverflow, and are especially helpful to non-professional programmers and end users. These users are often interested in <i>data-centric tasks</i>, like spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including data. But how do we decide how much data and which data to include in the prompt?This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a novel <i>cluster-then-select</i> prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table,our cluster-then-select technique outperforms a random selection baseline.</abstract>
      <url hash="e0bee7a5">2024.findings-naacl.41</url>
    </paper>
    <paper id="42">
      <title>A Novel Paradigm Boosting Translation Capabilities of Large Language Models</title>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>639-649</pages>
      <abstract>This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs’ cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2(CITATION)model, particularly on Chinese-Llama2(CITATION) after monolingual augmentation, demonstrate the improved translation capabilities of LLMs. A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B(CITATION) and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of machine translation.</abstract>
      <url hash="732a65e7">2024.findings-naacl.42</url>
    </paper>
    <paper id="43">
      <title>Measuring Social Norms of Large Language Models</title>
      <author><first>Ye</first><last>Yuan</last></author>
      <author><first>Kexin</first><last>Tang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jianhao</first><last>Shen</last></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <pages>650-699</pages>
      <abstract>We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models’ ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.</abstract>
      <url hash="98426576">2024.findings-naacl.43</url>
    </paper>
    <paper id="44">
      <title>Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted Self-learning</title>
      <author><first>Maxwell</first><last>Yin</last></author>
      <author><first>Boyu</first><last>Wang</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Charles</first><last>Ling</last><affiliation>Western University</affiliation></author>
      <pages>700-713</pages>
      <abstract>This work addresses source-free domain adaptation (SFDA) for Question Answering (QA), wherein a model trained on a source domain is adapted to unlabeled target domains without additional source data. Existing SFDA methods only focus on the adaptation phase, overlooking the impact of source domain training on model generalizability. In this paper, we argue that source model training itself is also critical for improving the adaptation performance and stability. To this end, we investigate the role of prompt learning as an effective method to internalize domain-agnostic QA knowledge, which can be integrated into source training. After source training, an interactive self-learning strategy is proposed to further fine tune both model and prompt in the model adaptation phase. This leads to the Prompt-Assisted Self-Adaptive Learning (PASAL), an innovative SFDA approach for QA. Empirical evaluation on four benchmark datasets shows that PASAL surpasses existing methods in managing domain gaps and demonstrates greater stability across various target domains, validating the significance of source domain training for effective domain adaptation.</abstract>
      <url hash="e40b2ae9">2024.findings-naacl.44</url>
    </paper>
    <paper id="45">
      <title>Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level</title>
      <author><first>Chenlong</first><last>Zhao</last></author>
      <author><first>Xiwen</first><last>Zhou</last></author>
      <author><first>Xiaopeng</first><last>Xie</last></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>714-726</pages>
      <abstract>Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents.</abstract>
      <url hash="3afa3d6e">2024.findings-naacl.45</url>
    </paper>
    <paper id="46">
      <title><fixed-case>LEEET</fixed-case>s-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</title>
      <author><first>Nalin</first><last>Kumar</last></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>727-735</pages>
      <abstract>Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While entrainment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue entrainment in a GPT-2-based end-to-end task-oriented dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, entrainment-specific loss, and additional conditioning to generate responses that align with the user. We demonstrate that all three approaches produce significantly better entrainment than the base, non-entrainment-optimized model, as confirmed by both automated and manual evaluation metrics.</abstract>
      <url hash="e849c64d">2024.findings-naacl.46</url>
    </paper>
    <paper id="47">
      <title>Efficient Dependency Tree Sampling Without Replacement</title>
      <author><first>Bogdan</first><last>Dobre</last><affiliation>University of Bucharest</affiliation></author>
      <pages>736-741</pages>
      <abstract>In the context of computational models of dependency syntax, most dependency treebanks have the restriction that any valid dependency tree must have exactly one edge coming out of the root node in addition to respecting the spanning tree constraints. Many algorithms for dependency tree sampling were recently proposed, both for sampling with and without replacement.In this paper we propose a new algorithm called Wilson Reject SWOR for the case of sampling without replacement by adapting the Wilson Reject algorithm originally created for sampling with replacement and combining it with a Trie data structure. Experimental results indicate the efficiency of our approach in the scenario of sampling without replacement from dependency graphs with random weights.</abstract>
      <url hash="27fc8c51">2024.findings-naacl.47</url>
    </paper>
    <paper id="48">
      <title>Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</title>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Revanth</first><last>Gangi Reddy</last></author>
      <author><first>Kevin</first><last>Small</last><affiliation>Amazon</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>742-753</pages>
      <abstract>Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader’s over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model’s performance in its original corpus and domain.</abstract>
      <url hash="29b59459">2024.findings-naacl.48</url>
    </paper>
    <paper id="49">
      <title><fixed-case>GEE</fixed-case>! Grammar Error Explanation with Large Language Models</title>
      <author><first>Yixiao</first><last>Song</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Kalpesh</first><last>Krishna</last><affiliation>Google</affiliation></author>
      <author><first>Rajesh</first><last>Bhatt</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Kevin</first><last>Gimpel</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>754-781</pages>
      <abstract>Existing grammatical error correction tools do not provide natural language explanations of the errors that they correct in user-written text. However, such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006).To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. The task is not easily solved by prompting LLMs: we find that, using one-shot prompting, GPT-4 only explains 40.6% of the errors and does not even attempt to explain 39.8% of the errors.Since LLMs struggle to identify grammar errors, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to explain each edit. We evaluate our pipeline on German, Chinese, and English grammar error correction data. Our atomic edit extraction achieves an F1 of 0.93 on German, 0.91 on Chinese, and 0.891 on English. Human evaluation of generated explanations reveals that 93.9% of German errors, 96.4% of Chinese errors, and 92.20% of English errors are correctly detected and explained. To encourage further research, we open-source our data and code.</abstract>
      <url hash="ef0d1625">2024.findings-naacl.49</url>
    </paper>
    <paper id="50">
      <title><fixed-case>A</fixed-case>da<fixed-case>R</fixed-case>efiner: Refining Decisions of Language Models with Adaptive Feedback</title>
      <author><first>Wanpeng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>782-799</pages>
      <abstract>Large Language Models (LLMs) have demonstrated significant success across various domains. However, their application in complex decision-making tasks frequently necessitates intricate prompt engineering or fine-tuning, leading to challenges in unseen downstream tasks and heavy demands on computational resources. Meanwhile, Reinforcement Learning (RL) has been recognized as effective in decision-making problems but struggles in environments with sparse rewards, such as open-world games. To overcome these challenges, we introduce AdaRefiner, a novel framework designed to enhance the synergy between LLMs and RL feedback. The key component of AdaRefiner is a lightweight Adapter Language Model (LM), which automatically refines task comprehension based on feedback from RL agents. This method mitigates the need for intricate prompt engineering and intensive LLM fine-tuning while maintaining the LLMs’ generalization abilities and enhancing their decision-making capabilities in downstream tasks. Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world game <i>Crafter</i> have demonstrated its superior effectiveness, especially in guiding agents towards higher-level and common-sense skills. Our work makes contributions to the automatic self-refinement of LLMs with RL feedback, offering a more adaptable and efficient solution for complex decision-making problems. The code is available at https://github.com/PKU-RL/AdaRefiner.</abstract>
      <url hash="e71ae406">2024.findings-naacl.50</url>
    </paper>
    <paper id="51">
      <title><fixed-case>D</fixed-case>iv<fixed-case>TOD</fixed-case>: Unleashing the Power of <fixed-case>LLM</fixed-case>s for Diversifying Task-Oriented Dialogue Representations</title>
      <author><first>Weihao</first><last>Zeng</last></author>
      <author><first>Dayuan</first><last>Fu</last></author>
      <author><first>Keqing</first><last>He</last><affiliation>Meituan Group</affiliation></author>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Yukai</first><last>Xu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>800-813</pages>
      <abstract>Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context.In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.</abstract>
      <url hash="e75d4e33">2024.findings-naacl.51</url>
    </paper>
    <paper id="52">
      <title>Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training</title>
      <author><first>Pavel</first><last>Denisov</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <author><first>Thang</first><last>Vu</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <pages>814-834</pages>
      <abstract>Recent advancements in language modeling have led to the emergenceof Large Language Models (LLMs) capable ofvarious natural language processing tasks.Despite their success in text-based tasks, applying LLMs to the speech domainremains limited and challenging. This paper presents BLOOMZMMS, a novel modelthat integrates a multilingual LLM with a multilingual speech encoder,aiming to harness the capabilities of LLMs for speech recognition and beyond.Utilizing a multi-instructional training approach, we demonstrate the transferabilityof linguistic knowledge from the text to the speech modality.Our experiments, conducted on 1900 hours of transcribed data from 139 languages,establish that a multilingual speech representation can be effectivelylearned and aligned with a multilingual LLM. While this learned representationinitially shows limitations in task generalization, we address this issue bygenerating synthetic targets in a multi-instructional style.Our zero-shot evaluation results confirm the robustness of our approach acrossmultiple tasks, including speech translation and multilingual spoken languageunderstanding, thereby opening new avenues for applying LLMs in the speech domain.</abstract>
      <url hash="6895c343">2024.findings-naacl.52</url>
    </paper>
    <paper id="53">
      <title><fixed-case>CLEAN</fixed-case>–<fixed-case>EVAL</fixed-case>: Clean Evaluation on Contaminated Large Language Models</title>
      <author><first>Wenhong</first><last>Zhu</last></author>
      <author><first>Hongkun</first><last>Hao</last></author>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yun-Ze</first><last>Song</last></author>
      <author><first>Jiao</first><last>Yueyang</last></author>
      <author><first>Yumeng</first><last>Zhang</last></author>
      <author><first>Hanxu</first><last>Hu</last></author>
      <author><first>Yiran</first><last>Wei</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <pages>835-847</pages>
      <abstract>We are currently in an era of fierce competition among various large language models (LLMs), continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination. In this paper, we propose a novel and valuable method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs more cleanly. Clean-Eval employs a neural-based model to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter those generated low-quality samples to narrow down this candidate set. Candidates with moderate BLEURT scores against the original samples are selected as the final evaluation set. According to human assessment, this set is almost semantically equivalent to the original contamination set but expressed differently. We conduct experiments on 20 existing benchmarks across diverse tasks, and results demonstrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.</abstract>
      <url hash="6f8249e0">2024.findings-naacl.53</url>
    </paper>
    <paper id="54">
      <title><fixed-case>R</fixed-case>-<fixed-case>BASS</fixed-case> : Relevance-aided Block-wise Adaptation for Speech Summarization</title>
      <author><first>Roshan</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Ruchira</first><last>Sharma</last></author>
      <author><first>Hira</first><last>Dhamyal</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Rita</first><last>Singh</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University, Carnegie Mellon University and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>848-857</pages>
      <abstract>End-to-end speech summarization on long recordings is challenging because of the high computational cost. Block-wise Adaptation for Speech Summarization (BASS) summarizes arbitrarily long sequences by sequentially processing abutting chunks of audio. Despite the benefits of BASS, it has higher compute time due to sequential processing of all blocks, regardless of whether they are relevant to the final summary. In this paper, we propose R-BASS, a new relevance-aware block-wise adaptation method. First, we introduce two approaches to automatically estimate block relevance based on lexical and semantic similarity between the block-level transcript and the summary. Experiments on the How2 dataset show that using ground truth relevance during inference improves efficiency by 63.9 % by dropping irrelevant blocks. Finally, we incorporate relevance scores into training using a novel relevance loss and relevance predictor, and the proposed R-BASS model makes it possible to drop 86.3 % of the blocks while retaining comparable performance, resulting in a 2.2x speedup over BASS.</abstract>
      <url hash="ba342e02">2024.findings-naacl.54</url>
    </paper>
    <paper id="55">
      <title><fixed-case>OVM</fixed-case>, Outcome-supervised Value Models for Planning in Mathematical Reasoning</title>
      <author><first>Fei</first><last>Yu</last></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ShenZhen research institute of big data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>858-875</pages>
      <abstract>Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer.To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a <tex-math>\textit{value estimation}</tex-math> problem in planning.Inspired by the findings that <tex-math>\textit{outcome supervision for guided decoding essentially acts as a value model}</tex-math>, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our <tex-math>\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}</tex-math>; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.</abstract>
      <url hash="58705d5a">2024.findings-naacl.55</url>
    </paper>
    <paper id="56">
      <title>The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation</title>
      <author><first>Lei</first><last>Wang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Ee-Peng</first><last>Lim</last><affiliation>Singapore Management University</affiliation></author>
      <pages>876-895</pages>
      <abstract>Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.</abstract>
      <url hash="56e01f99">2024.findings-naacl.56</url>
    </paper>
    <paper id="57">
      <title>Bring Your Own <fixed-case>KG</fixed-case>: Self-Supervised Program Synthesis for Zero-Shot <fixed-case>KGQA</fixed-case></title>
      <author><first>Dhruv</first><last>Agarwal</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Rajarshi</first><last>Das</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Sopan</first><last>Khosla</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last><affiliation>Amazon</affiliation></author>
      <pages>896-919</pages>
      <abstract>We present BYOKG, a universal question-answering (QA) system that can operate on any knowledge graph (KG), requires no human-annotated training data, and can be ready to use within a day—attributes that are out-of-scope for current KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration—starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge. Exploration in BYOKG leverages an LLM-backed symbolic agent that generates a diverse set of query-program exemplars, which are then used to ground a retrieval-augmented reasoning procedure to synthesize programs for arbitrary questions. BYOKG is effective over both small- and large-scale graphs, showing dramatic gains in zero-shot QA accuracy of 27.89 and 59.88 F1 on GrailQA and MetaQA, respectively. We further find that performance of BYOKG reliably improves with continued exploration as well as improvements in the base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a sub-sampled zero-shot split of GrailQA. Lastly, we verify our universality claim by evaluating BYOKG on a domain-specific materials science KG and show that it improves zero-shot performance by 46.33 F1.</abstract>
      <url hash="f3826e89">2024.findings-naacl.57</url>
    </paper>
    <paper id="58">
      <title><fixed-case>G</fixed-case>ra<fixed-case>SAME</fixed-case>: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism</title>
      <author><first>Shuzhou</first><last>Yuan</last></author>
      <author><first>Michael</first><last>Färber</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <pages>920-933</pages>
      <abstract>Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.</abstract>
      <url hash="41315557">2024.findings-naacl.58</url>
    </paper>
    <paper id="59">
      <title>Can Public Large Language Models Help Private Cross-device Federated Learning?</title>
      <author><first>Boxin</first><last>Wang</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yibo</first><last>Zhang</last><affiliation>Stanford University and University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yuan</first><last>Cao</last><affiliation>Google DeepMind</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign and University of California Berkeley</affiliation></author>
      <author><first>Hugh</first><last>McMahan</last><affiliation>Google</affiliation></author>
      <author><first>Sewoong</first><last>Oh</last><affiliation>University of Washington, University of Illinois at Urbana-Champaign and University of Washington, Seattle</affiliation></author>
      <author><first>Zheng</first><last>Xu</last><affiliation>Google</affiliation></author>
      <author><first>Manzil</first><last>Zaheer</last><affiliation>Zaheer and DeepMind</affiliation></author>
      <pages>934-949</pages>
      <abstract>We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private models by taking advantage of public data, especially for customized on-device architectures that do not have ready-touse pre-trained models.</abstract>
      <url hash="81fdfe84">2024.findings-naacl.59</url>
    </paper>
    <paper id="60">
      <title><fixed-case>L</fixed-case>ang<fixed-case>N</fixed-case>av: Language as a Perceptual Representation for Navigation</title>
      <author><first>Bowen</first><last>Pan</last></author>
      <author><first>Rameswar</first><last>Panda</last><affiliation>MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>SouYoung</first><last>Jin</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Rogerio</first><last>Feris</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Aude</first><last>Oliva</last><affiliation>Massachusetts Institute of Technology, Massachusetts Institute of Technology and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Phillip</first><last>Isola</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>950-974</pages>
      <abstract>We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent’s egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision- and language-based representations for VLN. Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.</abstract>
      <url hash="1b0d01a3">2024.findings-naacl.60</url>
    </paper>
    <paper id="61">
      <title>Planning and Editing What You Retrieve for Enhanced Tool Learning</title>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Dongwon</first><last>Jung</last></author>
      <author><first>Vaibhav</first><last>Kumar</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Puyang</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>975-988</pages>
      <abstract>Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel PLUTO (Planning, Learning, and Understanding for TOols) approach, encompassing “Plan-and-Retrieve (P&amp;R)” and “Edit-and-Ground (E&amp;G)” paradigms. The P&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.</abstract>
      <url hash="5553eac7">2024.findings-naacl.61</url>
    </paper>
    <paper id="62">
      <title>Chart-based Reasoning: Transferring Capabilities from <fixed-case>LLM</fixed-case>s to <fixed-case>VLM</fixed-case>s</title>
      <author><first>Victor</first><last>Carbune</last><affiliation>Google</affiliation></author>
      <author><first>Hassan</first><last>Mansoor</last><affiliation>Google</affiliation></author>
      <author><first>Fangyu</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Rahul</first><last>Aralikatte</last><affiliation>Mila, McGill University</affiliation></author>
      <author><first>Gilles</first><last>Baechler</last><affiliation>Research, Google</affiliation></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Abhanshu</first><last>Sharma</last><affiliation>Research, Google</affiliation></author>
      <pages>989-1004</pages>
      <abstract>Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We pro-pose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-artperformance when applied on the PaLI3-5B VLM by Chen et al. (2023c), while also enabling much better performance on PlotQA and FigureQA.We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by Liu et al. (2023a). We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by Hsieh et al. (2023).Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt (Chen et al., 2023a), our model outperforms the recently introduced Gemini Ultra and GPT-4V.</abstract>
      <url hash="fee8b4f4">2024.findings-naacl.62</url>
    </paper>
    <paper id="63">
      <title><fixed-case>SL</fixed-case>i<fixed-case>M</fixed-case>: Speculative Decoding with Hypothesis Reduction</title>
      <author><first>Chi-Heng</first><last>Lin</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Shikhar</first><last>Tuli</last></author>
      <author><first>James</first><last>Smith</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yilin</first><last>Shen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>1005-1017</pages>
      <abstract>Speculative decoding has emerged as a prominent alternative to autoregressive decoding for expediting inference in large language models (LLMs). However, prevailing assumptions often focus solely on latency reduction, neglecting the computational expenses. In this paper, we present <b>S</b>peculate <b>L</b>ess, val<b>i</b>date <b>M</b>ore (SLiM), a speculative decoding enhancement to reduce the speculation set while validating more effective tokens. SLiM is designed to mitigate LLMs’ computation costs associated with the token verification by introducing hypothesis reduction based on a fast posterior estimation. It consistently surpasses counterparts lacking cost reduction across a spectrum from CPU to GPU. Our evaluation with diverse conversational datasets shows that SLiM can achieve a substantial 70% reduction in FLOPs while generating more effective predictions on top of prior arts.</abstract>
      <url hash="37b6c5b8">2024.findings-naacl.63</url>
    </paper>
    <paper id="64">
      <title><fixed-case>REMATCH</fixed-case>: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity</title>
      <author><first>Zoher</first><last>Kachwala</last><affiliation>Indiana University</affiliation></author>
      <author><first>Jisun</first><last>An</last><affiliation>Indiana University</affiliation></author>
      <author><first>Haewoon</first><last>Kwak</last><affiliation>Indiana University</affiliation></author>
      <author><first>Filippo</first><last>Menczer</last><affiliation>Indiana University</affiliation></author>
      <pages>1018-1028</pages>
      <abstract>Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1–5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.</abstract>
      <url hash="d9c566d4">2024.findings-naacl.64</url>
    </paper>
    <paper id="65">
      <title>Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing</title>
      <author><first>Ben</first><last>Hutchinson</last><affiliation>Google</affiliation></author>
      <pages>1029-1043</pages>
      <abstract>This position paper concerns the use of religious texts in Natural Language Processing (NLP), which is of special interest to the Ethics of NLP. Religious texts are expressions of culturally important values, and machine learned models have a propensity to reproduce cultural values encoded in their training data. Furthermore, translations of religious texts are frequently used by NLP researchers when language data is scarce. This repurposes the translations from their original uses and motivations, which often involve attracting new followers. This paper argues that NLP’s use of such texts raises considerations that go beyond model biases, including data provenance, cultural contexts, and their use in proselytism. We argue for more consideration of researcher positionality, and of the perspectives of marginalized linguistic and religious communities.</abstract>
      <url hash="aff3cc80">2024.findings-naacl.65</url>
    </paper>
    <paper id="66">
      <title>Testing the Effect of Code Documentation on Large Language Model Code Understanding</title>
      <author><first>William</first><last>Macke</last><affiliation>MITRE</affiliation></author>
      <author><first>Michael</first><last>Doyle</last><affiliation>MITRE</affiliation></author>
      <pages>1044-1050</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM’s ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM’s capabilities. We show that providing an LLM with “incorrect” documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM’s ability to understand code.</abstract>
      <url hash="60139f30">2024.findings-naacl.66</url>
    </paper>
    <paper id="67">
      <title>Aligning Large Language Models with Recommendation Knowledge</title>
      <author><first>Yuwei</first><last>Cao</last></author>
      <author><first>Nikhil</first><last>Mehta</last><affiliation>Research, Google</affiliation></author>
      <author><first>Xinyang</first><last>Yi</last><affiliation>Google</affiliation></author>
      <author><first>Raghunandan</first><last>Hulikal Keshavan</last></author>
      <author><first>Lukasz</first><last>Heldt</last></author>
      <author><first>Lichan</first><last>Hong</last><affiliation>Google</affiliation></author>
      <author><first>Ed</first><last>Chi</last><affiliation>Google</affiliation></author>
      <author><first>Maheswaran</first><last>Sathiamoorthy</last></author>
      <pages>1051-1066</pages>
      <abstract>Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs’ knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommendation-task data samples facilitates the injection of recommendation-specific knowledge into LLMs. Extensive experiments across retrieval, ranking, and rating prediction tasks on LLMs such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique in domains such as Amazon Toys &amp; Games, Beauty, and Sports &amp; Outdoors. Notably, our method outperforms conventional and LLM-based baselines, including the current SOTA, by significant margins in retrieval, showcasing its potential for enhancing recommendation quality.</abstract>
      <url hash="174b3285">2024.findings-naacl.67</url>
    </paper>
    <paper id="68">
      <title><fixed-case>OFA</fixed-case>: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</title>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1067-1097</pages>
      <abstract>Instead of pretraining multilingual language models from scratch, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the model, thus weakening the efficiency. To address these issues, we propose a novel framework: <tex-math>\textbf{O}</tex-math>ne <tex-math>\textbf{F}</tex-math>or <tex-math>\textbf{A}</tex-math>ll (<tex-math>\textbf{OFA}</tex-math>), which wisely initializes the embeddings of unseen subwords and thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the subword embeddings. In addition, OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which largely reduces the number of parameters. We show OFA accelerates the convergence of continued pretraining, which is environmentally friendly as much fewer carbon footprints are generated. Through extensive experiments, we demonstrate OFA can achieve competitive or better performance than default continued pretraining baselines on a wide range of crosslingual downstream tasks. We make our code and models publicly available.</abstract>
      <url hash="de674fdf">2024.findings-naacl.68</url>
    </paper>
    <paper id="69">
      <title><fixed-case>SELF</fixed-case>-<fixed-case>EXPERTISE</fixed-case>: Knowledge-based Instruction Dataset Augmentation for a Legal Expert Language Model</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Haein</first><last>Jung</last></author>
      <author><first>Myoung-Wan</first><last>Koo</last><affiliation>Sogang University</affiliation></author>
      <pages>1098-1112</pages>
      <abstract>The advent of instruction-tuned large language models (LLMs) has significantly advanced the field of automatic instruction dataset augmentation. However, the method of generating instructions and outputs from inherent knowledge of LLM can unintentionally produce hallucinations — instances of generating factually incorrect or misleading information. To overcome this, we propose SELF-EXPERTISE, automatically generating instruction dataset in the legal domain from a seed dataset. SELF-EXPERTISE extracts knowledge from the outputs of the seed dataset, and generates new instructions, inputs, and outputs. In this way, the proposed method reduces hallucination in automatic instruction augmentation. We trained an SELF-EXPERTISE augmented instruction dataset on the LLaMA-2 7B model to construct Korean legal specialized model, called LxPERT. LxPERT has demonstrated performance surpassing GPT-3.5-turbo in both in-domain and out-of-domain datasets. The SELF-EXPERTISE augmentation pipeline is not only applicable to the legal field but is also expected to be extendable to various domains, potentially advancing domain-specialized LLMs.</abstract>
      <url hash="aaf8e20d">2024.findings-naacl.69</url>
    </paper>
    <paper id="70">
      <title>Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction</title>
      <author><first>Boyi</first><last>Li</last><affiliation>NVIDIA Research and University of California, Berkeley</affiliation></author>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Karttikeya</first><last>Mangalam</last></author>
      <author id="catherine-chen-bu"><first>Catherine</first><last>Chen</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Daniel</first><last>Flaherty</last></author>
      <author><first>Serge</first><last>Belongie</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Kilian</first><last>Weinberger</last><affiliation>Cornell University, Cornell University and Cornell University</affiliation></author>
      <author><first>Jitendra</first><last>Malik</last><affiliation>Facebook and University of California Berkeley</affiliation></author>
      <author><first>Trevor</first><last>Darrell</last><affiliation>Electrical Engineering &amp; Computer Science Department</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>1113-1123</pages>
      <abstract>Are multimodal inputs necessary for grammar induction? Recent work has shown that multimodal training inputs can improve grammar induction. However, these improvements are based on comparisons to weak text-only baselines that were trained on relatively little textual data. To determine whether multimodal inputs are needed in regimes with large amounts of textual training data, we design a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is a C-PFCG that incorporates embeddings from text-only large language models (LLMs). We use a fixed grammar family to directly compare LC-PCFG to various multimodal grammar induction methods. We compare performance on four benchmark datasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1 compared to state-of-the-art multimodal grammar induction methods. LC-PCFG is also more computationally efficient, providing an up to 85% reduction in parameter count and <tex-math>8.8\times</tex-math> reduction in training time compared to multimodal approaches. These results suggest that multimodal inputs may not be necessary for grammar induction, and emphasize the importance of strong vision-free baselines for evaluating the benefit of multimodal approaches.</abstract>
      <url hash="7dcec7e8">2024.findings-naacl.70</url>
    </paper>
    <paper id="71">
      <title><fixed-case>EDE</fixed-case>ntail: An Entailment-based Few-shot Text Classification with Extensional Definition</title>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Junlang</first><last>Qian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>1124-1137</pages>
      <abstract>Few-shot text classification has seen significant advancements, particularly with entailment-based methods, which typically use either class labels or intensional definitions of class labels in hypotheses for label semantics expression. In this paper, we propose EDEntail, a method that employs extensional definition (EDef) of class labels in hypotheses, aiming to express the semantics of class labels more explicitly. To achieve the above goal, we develop an algorithm to gather and select extensional descriptive words of class labels and then order and format them into a sequence to form hypotheses. Our method has been evaluated and compared with state-of-the-art models on five classification datasets. The results demonstrate that our approach surpasses the supervised-learning methods and prompt-based methods under the few-shot setting, which underlines the potential of using an extensional definition of class labels for entailment-based few-shot text classification. Our code is available at https://github.com/MidiyaZhu/EDEntail.</abstract>
      <url hash="c569707d">2024.findings-naacl.71</url>
    </paper>
    <paper id="72">
      <title>What Makes Math Word Problems Challenging for <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Kv Aditya</first><last>Srivatsa</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1138-1148</pages>
      <abstract>This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.</abstract>
      <url hash="b0220150">2024.findings-naacl.72</url>
    </paper>
    <paper id="73">
      <title><fixed-case>SMILE</fixed-case>: Multimodal Dataset for Understanding Laughter in Video with Language Models</title>
      <author><first>Lee</first><last>Hyun</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Kim</first><last>Sung-Bin</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Allen Institute for Artificial Intelligence and Seoul National University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Tae-Hyun</first><last>Oh</last><affiliation>POSTECH</affiliation></author>
      <pages>1149-1167</pages>
      <abstract>Despite the recent advances in artificial intelligence, building social intelligence remains a challenge.Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans.In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning.We introduce this new task to explain why people laugh in a particular video and a dataset for this task.Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model checkpoints on https://github.com/postech-ami/SMILE-Dataset.</abstract>
      <url hash="9ecc171b">2024.findings-naacl.73</url>
    </paper>
    <paper id="74">
      <title><fixed-case>T</fixed-case>3<fixed-case>M</fixed-case>: Text Guided 3<fixed-case>D</fixed-case> Human Motion Synthesis from Speech</title>
      <author><first>Gloria</first><last>Gloria</last></author>
      <author><first>Kaipeng</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Sai Qian</first><last>Zhang</last><affiliation>Harvard University, Harvard University, University of Toronto and Facebook</affiliation></author>
      <pages>1168-1177</pages>
      <abstract>Speech-driven 3D motion synthesis seeks to create lifelike animations based on human speech, with potential uses in virtual reality, gaming, and the film production. Existing approaches reply solely on speech audio for motion generation, leading to inaccurate and inflexible synthesis results. To mitigate this problem, we introduce a novel text-guided 3D human motion synthesis method, termed T3M. Unlike traditional approaches, T3M allows precise control over motion synthesis via textual input, enhancing the degree of diversity and user customization. The experiment results demonstrate that T3M can greatly outperform the state-of-the-art methods in both quantitative metrics and qualitative evaluations. We have publicly released our code at https://github.com/Gloria2tt/naacl2024.git</abstract>
      <url hash="e6a7d07f">2024.findings-naacl.74</url>
    </paper>
    <paper id="75">
      <title>Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning</title>
      <author><first>Miao</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ben</first><last>Liu</last></author>
      <author><first>Wenjie</first><last>Xu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zihao</first><last>Jiang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jiahui</first><last>Zhu</last></author>
      <author><first>Min</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <pages>1178-1191</pages>
      <abstract>Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.</abstract>
      <url hash="831923d2">2024.findings-naacl.75</url>
    </paper>
    <paper id="76">
      <title>Explanation Extraction from Hierarchical Classification Frameworks for Long Legal Documents</title>
      <author><first>Nishchal</first><last>Prasad</last></author>
      <author><first>Taoufiq</first><last>Dkaki</last><affiliation>Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Mohand</first><last>Boughanem</last><affiliation>Université de Toulouse</affiliation></author>
      <pages>1192-1201</pages>
      <abstract>Hierarchical classification frameworks have been widely used to process long sequences, especially in the legal domain for predictions from long legal documents. But being black-box models they are unable to explain their predictions making them less reliable for practical applications, more so in the legal domain. In this work, we develop an extractive explanation algorithm for hierarchical frameworks for long sequences based on the sensitivity of the trained model to its input perturbations. We perturb using occlusion and develop Ob-HEx; an Occlusion-based Hierarchical Explanation-extractor. We adapt Ob-HEx to Hierarchical Transformer models trained on long Indian legal texts. And use Ob-HEx to analyze them and extract their explanations for the ILDC-Expert dataset, achieving a minimum gain of 1 point over the previous benchmark on most of our performance evaluation metrics.</abstract>
      <url hash="a38eba58">2024.findings-naacl.76</url>
    </paper>
    <paper id="77">
      <title>Low-Rank Adaptation for Multilingual Summarization: An Empirical Study</title>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Fantine</first><last>Huot</last><affiliation>Google</affiliation></author>
      <author><first>Jasmijn</first><last>Bastings</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Mostafa</first><last>Dehghani</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Chu-Cheng</first><last>Lin</last><affiliation>Google</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>1202-1228</pages>
      <abstract>Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning outperforms full fine-tuning and the dynamic composition of language-specific LoRA modules.</abstract>
      <url hash="17b7c749">2024.findings-naacl.77</url>
    </paper>
    <paper id="78">
      <title>A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages</title>
      <author><first>Leonardo</first><last>Ranaldi</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <author><first>Federico</first><last>Ranaldi</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last><affiliation>Università degli Studi di Roma Tor Vergata</affiliation></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <pages>1229-1241</pages>
      <abstract>Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT), empower the reasoning abilities of Large Language Models (LLMs) by eliciting them to solve complex tasks in a step-by-step manner. Although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to English because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. In this paper, we propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning across languages. The proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. Experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.</abstract>
      <url hash="e6b875b5">2024.findings-naacl.78</url>
    </paper>
    <paper id="79">
      <title>Emergent Abilities in Reduced-Scale Generative Language Models</title>
      <author><first>Sherin</first><last>Muckatira</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Vijeta</first><last>Deshpande</last></author>
      <author><first>Vladislav</first><last>Lialin</last><affiliation>University of Massachusetts, Lowell</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>University of Massachusetts, Lowell, University of Massachusetts at Lowell and Amazon</affiliation></author>
      <pages>1242-1257</pages>
      <abstract>Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size.Additionally, we find that these smaller models pre-trained on simplified data demonstrate a power law relationship between the evaluation loss and the three scaling factors: compute, dataset size, and model size.</abstract>
      <url hash="a85c3b20">2024.findings-naacl.79</url>
    </paper>
    <paper id="80">
      <title>Context Does Matter: Implications for Crowdsourced Evaluation Labels in Task-Oriented Dialogue Systems</title>
      <author><first>Clemencia</first><last>Siro</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Maarten</first><last>Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>1258-1273</pages>
      <abstract>Crowdsourced labels play a crucial role in evaluating task-oriented dialogue systems (TDSs). Obtaining high-quality and consistent ground-truth labels from annotators presents challenges. When evaluating a TDS, annotators must fully comprehend the dialogue before providing judgments. Previous studies suggest using only a portion of the dialogue context in the annotation process. However, the impact of this limitation on label quality remains unexplored. This study investigates the influence of dialogue context on annotation quality, considering the truncated context for relevance and usefulness labeling. We further propose to use large language models ( LLMs) to summarize the dialogue context to provide a rich and short description of the dialogue context and study the impact of doing so on the annotator’s performance. Reducing context leads to more positive ratings. Conversely, providing the entire dialogue context yields higher-quality relevance ratings but introduces ambiguity in usefulness ratings. Using the first user utterance as context leads to consistent ratings, akin to those obtained using the entire dialogue, with significantly reduced annotation effort. Our findings show how task design, particularly the availability of dialogue context, affects the quality and consistency of crowdsourced evaluation labels.</abstract>
      <url hash="802d6d07">2024.findings-naacl.80</url>
    </paper>
    <paper id="81">
      <title>Matching Varying-Length Texts via Topic-Informed and Decoupled Sentence Embeddings</title>
      <author><first>Xixi</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chunbin</first><last>Gu</last></author>
      <author><first>Xin</first><last>Jie</last></author>
      <author><first>Jiajun</first><last>Bu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haishuai</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>1274-1280</pages>
      <abstract>Measuring semantic similarity between texts is a crucial task in natural language processing. While existing semantic text matching focuses on pairs of similar-length sequences, matching texts with non-comparable lengths has broader applications in specific domains, such as comparing professional document summaries and content. Current approaches struggle with text pairs of non-comparable lengths due to truncation issues. To address this, we split texts into natural sentences and decouple sentence representations using supervised contrastive learning (SCL). Meanwhile, we adopt the embedded topic model (ETM) for specific domain data. Our experiments demonstrate the effectiveness of our model, based on decoupled and topic-informed sentence embeddings, in matching texts of significantly different lengths across three well-studied datasets.</abstract>
      <url hash="0e770747">2024.findings-naacl.81</url>
    </paper>
    <paper id="82">
      <title>Instruction Tuning with Human Curriculum</title>
      <author><first>Bruce W</first><last>Lee</last></author>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Ewha Women’s University</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <pages>1281-1309</pages>
      <abstract>In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the potential advantages of employing diverse curriculum strategies, and (3) delineate a synthetic instruction-response generation framework that complements our theoretical approach. Distinct from the existing instruction tuning dataset, our generation pipeline is systematically structured to emulate the sequential and orderly characteristic of human learning. Additionally, we describe a methodology for generating instruction-response datasets that extensively span the various stages of human education, from middle school through the graduate level, utilizing educational subject catalogs.Before training, we meticulously organize the instruction data to ensure that questions escalate in difficulty regarding (A) the subject matter and (B) the intricacy of the instructions. The findings of our study reveal that substantial improvements in performance can be achieved through the mere application of curriculum ordering to instruction data—achieving gains of +4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard—compared to random shuffling. This enhancement is achieved without incurring additional computational expenses. Through comprehensive experimentation, we observe that the advantages of our proposed method are consistently evident across nine benchmarks.</abstract>
      <url hash="1e099e77">2024.findings-naacl.82</url>
    </paper>
    <paper id="83">
      <title>Natural Language-based State Representation in Deep Reinforcement Learning</title>
      <author><first>Md Masudur</first><last>Rahman</last><affiliation>Purdue University</affiliation></author>
      <author><first>Yexiang</first><last>Xue</last><affiliation>Purdue University, Purdue University and Purdue University</affiliation></author>
      <pages>1310-1319</pages>
      <abstract>This paper investigates the potential of using natural language descriptions as an alternative to direct image-based observations for learning policies in reinforcement learning. Due to the inherent challenges in managing image-based observations, which include abundant information and irrelevant features, we propose a method that compresses images into a natural language form for state representation. This approach allows better interpretability and leverages the processing capabilities of large-language models. We conducted several experiments involving tasks that required image-based observation. The results demonstrated that policies trained using natural language descriptions of images yield better generalization than those trained directly from images, emphasizing the potential of this approach in practical settings.</abstract>
      <url hash="33d37f92">2024.findings-naacl.83</url>
    </paper>
    <paper id="84">
      <title>Learning Cross-Architecture Instruction Embeddings for Binary Code Analysis in Low-Resource Architectures</title>
      <author><first>Junzhe</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Zeng</last><affiliation>George Mason University</affiliation></author>
      <author><first>Lannan</first><last>Luo</last><affiliation>George Mason University</affiliation></author>
      <pages>1320-1332</pages>
      <abstract>Binary code analysis is indispensable for a variety of software security tasks. Applying deep learning to binary code analysis has drawn great attention because of its notable performance. Today, source code is frequently compiled for various Instruction Set Architectures (ISAs). It is thus critical to expand binary analysis capabilities to multiple ISAs. Given a binary analysis task, the scale of available data on different ISAs varies. As a result, the rich datasets (e.g., malware) for certain ISAs, such as x86, lead to a disproportionate focus on these ISAs and a negligence of other ISAs, such as PowerPC, which suffer from the “data scarcity” problem. To address the problem, we propose to learn cross-architecture instruction embeddings (CAIE), where semantically-similar instructions, regardless of their ISAs, have close embeddings in a shared space. Consequently, we can transfer a model trained on a data-rich ISA to another ISA with less available data. We consider four ISAs (x86, ARM, MIPS, and PowerPC) and conduct both intrinsic and extrinsic evaluations (including malware detection and function similarity comparison). The results demonstrate the effectiveness of our approach to generate high-quality CAIE with good transferability.</abstract>
      <url hash="1560880c">2024.findings-naacl.84</url>
    </paper>
    <paper id="85">
      <title><fixed-case>R</fixed-case>e<fixed-case>E</fixed-case>val: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks</title>
      <author><first>Xiaodong</first><last>Yu</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1333-1351</pages>
      <abstract>Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs’ reliability in using new evidence for answering.We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection ofLLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.</abstract>
      <url hash="a23a862a">2024.findings-naacl.85</url>
    </paper>
    <paper id="86">
      <title>An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution</title>
      <author><first>Tien-Hong</first><last>Lo</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Fu-An</first><last>Chao</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Tzu-i</first><last>Wu</last></author>
      <author><first>Yao-Ting</first><last>Sung</last></author>
      <author><first>Berlin</first><last>Chen</last><affiliation>National Taiwan Normal University</affiliation></author>
      <pages>1352-1362</pages>
      <abstract>Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner’s speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss re-weighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.</abstract>
      <url hash="1a3b5edc">2024.findings-naacl.86</url>
    </paper>
    <paper id="87">
      <title><fixed-case>GPT</fixed-case>-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards <fixed-case>GPT</fixed-case>-4 and Beyond</title>
      <author><first>Shen</first><last>Zheng</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yuyu</first><last>Zhang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yijie</first><last>Zhu</last></author>
      <author><first>Chenguang</first><last>Xi</last></author>
      <author><first>Pengyang</first><last>Gao</last></author>
      <author><first>Zhou</first><last>Xun</last></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>1363-1382</pages>
      <abstract>With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI’s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI’s earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM’s reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.</abstract>
      <url hash="e5577fe8">2024.findings-naacl.87</url>
    </paper>
    <paper id="88">
      <title>Subword Attention and Post-Processing for Rare and Unknown Contextualized Embeddings</title>
      <author><first>Raj</first><last>Patel</last><affiliation>George Mason University</affiliation></author>
      <author><first>Carlotta</first><last>Domeniconi</last><affiliation>George Mason University and George Mason University</affiliation></author>
      <pages>1383-1389</pages>
      <abstract>Word representations are an important aspect of Natural Language Processing (NLP). Representations are trained using large corpora, either as independent static embeddings or as part of a deep contextualized model. While word embeddings are useful, they struggle on rare and unknown words. As such, a large body of work has been done on estimating rare and unknown words. However, most of the methods focus on static embeddings, with few models focused on contextualized representations. In this work, we propose SPRUCE, a rare/unknown embedding architecture that focuses on contextualized representations. This architecture uses subword attention and embedding post-processing combined with the contextualized model to produce high quality embeddings. We then demonstrate these techniques lead to improved performance in most intrinsic and downstream tasks.</abstract>
      <url hash="970fc054">2024.findings-naacl.88</url>
    </paper>
    <paper id="89">
      <title><fixed-case>UGIF</fixed-case>-<fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>et: A New Dataset for Cross-lingual, Cross-modal Sequential actions on the <fixed-case>UI</fixed-case></title>
      <author><first>Sagar</first><last>Gubbi Venkatesh</last></author>
      <author><first>Partha</first><last>Talukdar</last><affiliation>Google Research and Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Srini</first><last>Narayanan</last><affiliation>Google research</affiliation></author>
      <pages>1390-1399</pages>
      <abstract>Help documents are supposed to aid smartphone users in resolving queries such as “How to block calls from unknown numbers?”. However, given a query, identifying the right help document, understanding instructions from the document, and using them to resolve the issue at hand is challenging. The user experience may be enhanced by converting the instructions in the help document to a step-by-step tutorial overlaid on the phone UI. Successful execution of this task requires overcoming research challenges in retrieval, parsing, and grounding in the multilingual-multimodal setting. For example, user queries in one language may have to be matched against instructions in another language, which in turn needs to be grounded in a multimodal UI in yet another language. Moreover, there isn’t any relevant dataset for such a task. In order to bridge this gap, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone, containing 4,184 tasks across 8 languages. The instruction steps in UGIF-DataSet are available only in English, so the challenge involves operations in the cross-modal, cross-lingual setting. We compare the performance of different large language models for this task and find that the end-to-end task completion rate drops from 48% in English to 32% for other languages, demonstrating significant overall headroom for improvement. We are hopeful that UGIF-DataSet and our analysis will aid further research on the important problem of sequential task completion in the multilingual and multimodal setting.</abstract>
      <url hash="3d9faca9">2024.findings-naacl.89</url>
    </paper>
    <paper id="90">
      <title><fixed-case>S</fixed-case>im<fixed-case>SCOOD</fixed-case>: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models</title>
      <author><first>Hossein</first><last>Hajipour</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Ning</first><last>Yu</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Cristian-Alexandru</first><last>Staicu</last></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>1400-1416</pages>
      <abstract>Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet.In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues.</abstract>
      <url hash="f128ece0">2024.findings-naacl.90</url>
    </paper>
    <paper id="91">
      <title>Pruning as a Domain-specific <fixed-case>LLM</fixed-case> Extractor</title>
      <author><first>Nan</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Xujiang</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Runxue</first><last>Bao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last><affiliation>NEC-Labs</affiliation></author>
      <pages>1417-1428</pages>
      <abstract>Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task- agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.</abstract>
      <url hash="347ccb4e">2024.findings-naacl.91</url>
    </paper>
    <paper id="92">
      <title><fixed-case>LLMR</fixed-case>efine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback</title>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>Mara</first><last>Finkelstein</last><affiliation>Google</affiliation></author>
      <author><first>Juraj</first><last>Juraska</last><affiliation>Google</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>Google Research</affiliation></author>
      <author><first>Zhongtao</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <pages>1429-1445</pages>
      <abstract>Recent large language models (LLM) areleveraging human feedback to improve theirgeneration quality. However, human feedbackis costly to obtain, especially during inference.In this work, we propose LLMRefine, aninference time optimization method to refineLLM’s output. The core idea is to usea learned fine-grained feedback model topinpoint defects and guide LLM to refinethem iteratively. Using original LLM as aproposal of edits, LLMRefine searches fordefect-less text via simulated annealing, tradingoff the exploration and exploitation. Weconduct experiments on three text generationtasks, including machine translation, long-form question answering (QA), and topicalsummarization. LLMRefine consistentlyoutperforms all baseline approaches, achievingimprovements up to 1.7 MetricX points ontranslation tasks, 8.1 ROUGE-L on ASQA, 2.2ROUGE-L on topical summarization.</abstract>
      <url hash="183f0a5d">2024.findings-naacl.92</url>
    </paper>
    <paper id="93">
      <title>Noisy Multi-Label Text Classification via Instance-Label Pair Correction</title>
      <author><first>Pengyu</first><last>Xu</last></author>
      <author><first>Mingyang</first><last>Song</last><affiliation>Tencent MLPD</affiliation></author>
      <author><first>Linkaida</first><last>Liu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Hongjian</first><last>Sun</last></author>
      <author><first>Liping</first><last>Jing</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Yu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>1446-1458</pages>
      <abstract>In noisy label learning, instance selection based on small-loss criteria has been proven to be highly effective. However, in the case of noisy multi-label text classification (NMLTC), the presence of noise is not limited to the instance-level but extends to the (instance-label) pair-level.This gives rise to two main challenges.(1) The loss information at the pair-level fails to capture the variations between instances. (2) There are two types of noise at the pair-level: false positives and false negatives. Identifying false negatives from a large pool of negative pairs presents an exceedingly difficult task. To tackle these issues, we propose a novel approach called instance-label pair correction (iLaCo), which aims to address the problem of noisy pair selection and correction in NMLTC tasks.Specifically, we first introduce a holistic selection metric that identifies noisy pairs by simultaneously considering global loss information and instance-specific ranking information.Secondly, we employ a filter guided by label correlation to focus exclusively on negative pairs with label relevance. This filter significantly reduces the difficulty of identifying false negatives.Experimental analysis indicates that our framework effectively corrects noisy pairs in NMLTC datasets, leading to a significant improvement in model performance.</abstract>
      <url hash="99be6cc9">2024.findings-naacl.93</url>
    </paper>
    <paper id="94">
      <title>Composite Backdoor Attacks Against Large Language Models</title>
      <author><first>Hai</first><last>Huang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Zhengyu</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Michael</first><last>Backes</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yun</first><last>Shen</last><affiliation>NetApp</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <pages>1459-1472</pages>
      <abstract>Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with 3% poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100% Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06% and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.</abstract>
      <url hash="e805056d">2024.findings-naacl.94</url>
    </paper>
    <paper id="95">
      <title>Adapting Fake News Detection to the Era of Large Language Models</title>
      <author><first>Jinyan</first><last>Su</last><affiliation>Cornell University</affiliation></author>
      <author><first>Claire</first><last>Cardie</last><affiliation>Cornell University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1473-1490</pages>
      <abstract>In the age of large language models (LLMs) and the widespread adoption of AI-driven content creation, the landscape of information dissemination has witnessed a paradigm shift. With the proliferation of both human-written and machine-generated real and fake news, robustly and effectively discerning the veracity of news articles has become an intricate challenge. While substantial research has been dedicated to fake news detection, it has either assumed that all news articles are human-written or has abruptly assumed that all machine-generated news was fake. Thus, a significant gap exists in understanding the interplay between machine-paraphrased real news, machine-generated fake news, human-written fake news, and human-written real news. In this paper, we study this gap by conducting a comprehensive evaluation of fake news detectors trained in various scenarios. Our primary objectives revolve around the following pivotal question: How can we adapt fake news detectors to the era of LLMs?Our experiments reveal an interesting pattern that detectors trained exclusively on human-written articles can indeed perform well at detecting machine-generated fake news, but not vice versa. Moreover, due to the bias of detectors against machine-generated texts (CITATION), they should be trained on datasets with a lower machine-generated news ratio than the test set. Building on our findings, we provide a practical strategy for the development of robust fake news detectors.</abstract>
      <url hash="f50626fd">2024.findings-naacl.95</url>
    </paper>
    <paper id="96">
      <title><fixed-case>MCAD</fixed-case>: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval</title>
      <author><first>Youbo</first><last>Lei</last></author>
      <author><first>Feifei</first><last>He</last><affiliation>Researcher at OPPO Research Institute</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Yingbin</first><last>Mo</last></author>
      <author><first>Sijia</first><last>Li</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Defeng</first><last>Xie</last></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <pages>1491-1503</pages>
      <abstract>Due to the success of large-scale visual-language pretraining (VLP) models and the widespread use of image-text retrieval in industry areas, it is now critically necessary to reduce the model size and streamline their mobile-device deployment. Single- and dual-stream model structures are commonly used in image-text retrieval with the goal of closing the semantic gap between textual and visual modalities. While single-stream models use deep feature fusion to achieve more accurate cross-model alignment, dual-stream models are better at offline indexing and fast inference. We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique to integrate the advantages of single- and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher similarity distributions and features. Then, we conduct both distribution and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a lightweight CLIP model on Snapdragon/Dimensity chips with only ~100M running memory and ~8.0ms search latency, achieving the mobile-device application of VLP models.</abstract>
      <url hash="5cdd6c49">2024.findings-naacl.96</url>
    </paper>
    <paper id="97">
      <title>Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting</title>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Rolf</first><last>Jagerman</last><affiliation>Google</affiliation></author>
      <author><first>Kai</first><last>Hui</last><affiliation>Google</affiliation></author>
      <author><first>Honglei</first><last>Zhuang</last><affiliation>Google Research</affiliation></author>
      <author><first>Junru</first><last>Wu</last><affiliation>Google Research</affiliation></author>
      <author><first>Le</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google Research</affiliation></author>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Donald</first><last>Metzler</last><affiliation>Google</affiliation></author>
      <author><first>Xuanhui</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>1504-1518</pages>
      <abstract>Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets.We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP).Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&amp;2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10.Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.</abstract>
      <url hash="2887586a">2024.findings-naacl.97</url>
    </paper>
    <paper id="98">
      <title><fixed-case>F</fixed-case>ed<fixed-case>LFC</fixed-case>: Towards Efficient Federated Multilingual Modeling with <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-based Language Family Clustering</title>
      <author><first>Zhihan</first><last>Guo</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1519-1528</pages>
      <abstract>Federated Multilingual Modeling (FMM) plays a crucial role in the applications of natural language processing due to the increasing diversity of languages and the growing demand for data privacy. However, FMM faces limitations stemming from (1) the substantial communication costs in networking and (2) the conflicts arising from parameter interference between different languages. To address these challenges, we introduce a communication-efficient federated learning framework with low-rank adaptation and language family clustering for Multilingual Modeling (MM). In this framework, we maintain the weights of the base model, exclusively updating the lightweight Low-rank adaptation (LoRA) parameters to minimize communication costs. Additionally, we mitigate parameter conflicts by grouping languages based on their language family affiliations, as opposed to aggregating all LoRA parameters. Experiments demonstrate that our proposed model not only surpasses the baseline models in performance but also reduces the communication overhead. Our code is available at https://github.com/zhihan-guo/FedLFC.</abstract>
      <url hash="5e60027f">2024.findings-naacl.98</url>
    </paper>
    <paper id="99">
      <title><fixed-case>G</fixed-case>aussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models</title>
      <author><first>Mohammad Mahdi</first><last>Abdollah Pour</last></author>
      <author><first>Ali</first><last>Pesaranghader</last><affiliation>LG Electronics</affiliation></author>
      <author><first>Eldan</first><last>Cohen</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Scott</first><last>Sanner</last><affiliation>Department of Mechanical and Industrial Engineering, University of Toronto and Department of Computer Science</affiliation></author>
      <pages>1529-1536</pages>
      <abstract>In multi-objective text generation, we aim to optimize over multiple weighted aspects (e.g., toxicity, semantic preservation, fluency) of the generated text. However, multi-objective weighting schemes may change dynamically in practice according to deployment requirements, evolving business needs, personalization requirements on edge devices, or the availability of new language models and/or objective requirements. Ideally, we need an efficient method to adapt to the dynamic requirements of the overall objective. To address these requirements, we propose a linear combination of objective-specific language models to <b>efficiently</b> adapt the decoding process and optimize for the desired objective <b>without</b> the significant computational overhead of retraining one or more language models. We show empirically that we can leverage Gaussian Process black box optimization to adapt the language model decoder weights to outperform other fixed weighting schemes and standard baselines of the task in only a few iterations of decoding. Overall this approach enables highly efficient adaptation of controllable language models via multi-objective weighting schemes that may evolve dynamically in practical deployment situations.</abstract>
      <url hash="37319ecc">2024.findings-naacl.99</url>
    </paper>
    <paper id="100">
      <title>Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</title>
      <author><first>Alessandro</first><last>Stolfo</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>1537-1552</pages>
      <abstract>We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs).In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model’s pre-training data.Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers.Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.</abstract>
      <url hash="f7ed1c43">2024.findings-naacl.100</url>
    </paper>
    <paper id="101">
      <title><fixed-case>T</fixed-case>ag<fixed-case>D</fixed-case>ebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained Language Models</title>
      <author><first>Mehrnaz</first><last>Moslemi</last></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>1553-1567</pages>
      <abstract>Pre-trained language models (PLMs) play a crucial role in various applications, including sensitive domains such as the hiring process. However, extensive research has unveiled that these models tend to replicate social biases present in their pre-training data, raising ethical concerns. In this study, we propose the TagDebias method, which proposes debiasing a dataset using type tags. It then proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.</abstract>
      <url hash="78fcdccb">2024.findings-naacl.101</url>
    </paper>
    <paper id="102">
      <title>Improving Absent Keyphrase Generation with Diversity Heads</title>
      <author><first>Edwin</first><last>Thomas</last></author>
      <author><first>Sowmya</first><last>Vajjala</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>1568-1584</pages>
      <abstract>Keyphrase Generation (KPG) is the task of automatically generating appropriate keyphrases for a given text, with a wide range of real-world applications such as document indexing and tagging, information retrieval, and text summarization. NLP research makes a distinction between present and absent keyphrases based on whether a keyphrase is directly present as a sequence of words in the document during evaluation. However, present and absent keyphrases are treated together in a text-to-text generation framework during training. We treat present keyphrase extraction as a sequence labeling problem and propose a new absent keyphrase generation model that uses a modified cross-attention layer with additional heads to capture diverse views for the same context encoding in this paper. Our experiments show improvements over the state-of-the-art for four datasets for present keyphrase extraction and five datasets for absent keyphrase generation among the six English datasets we explored, covering long and short documents.</abstract>
      <url hash="a76b118a">2024.findings-naacl.102</url>
    </paper>
    <paper id="103">
      <title>m<fixed-case>O</fixed-case>thello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</title>
      <author><first>Tianze</first><last>Hua</last><affiliation>Brown University</affiliation></author>
      <author><first>Tian</first><last>Yun</last><affiliation>Brown University</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University and Brown University</affiliation></author>
      <pages>1585-1598</pages>
      <abstract>Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of “anchor tokens” (i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach – multilingual pretraining with unified output space – that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.</abstract>
      <url hash="9d4d40a2">2024.findings-naacl.103</url>
    </paper>
    <paper id="104">
      <title>Discovering and Mitigating Indirect Bias in Attention-Based Model Explanations</title>
      <author><first>Farsheed</first><last>Haque</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Depeng</first><last>Xu</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Shuhan</first><last>Yuan</last><affiliation>Utah State University</affiliation></author>
      <pages>1599-1614</pages>
      <abstract>As the field of Natural Language Processing (NLP) increasingly adopts transformer-based models, the issue of bias becomes more pronounced. Such bias, manifesting through stereotypes and discriminatory practices, can disadvantage certain groups. Our study focuses on direct and indirect bias in the model explanations, where the model makes predictions relying heavily on identity tokens or associated contexts. We present a novel analysis of bias in model explanation, especially the subtle indirect bias, underlining the limitations of traditional fairness metrics. We first define direct and indirect bias in model explanations, which is complementary to fairness in predictions. We then develop an indirect bias discovery algorithm for quantitatively evaluating indirect bias in transformer models using their in-built self-attention matrix. We also propose an indirect bias mitigation algorithm to ensure fairness in transformer models by leveraging attention explanations. Our evaluation shows the significance of indirect bias and the effectiveness of our indirect bias discovery and mitigation.</abstract>
      <url hash="619145a8">2024.findings-naacl.104</url>
    </paper>
    <paper id="105">
      <title>i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data</title>
      <author><first>Ziyi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mahmoud</first><last>Khademi</last><affiliation>Microsoft and University of British Columbia</affiliation></author>
      <author><first>Yichong</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Reid</first><last>Pryzant</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Yuwei</first><last>Fang</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Dongdong</first><last>Chen</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yao</first><last>Qian</last></author>
      <author><first>Xuemei</first><last>Gao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yi-Ling</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Robert</first><last>Gmyr</last><affiliation>Paderborn University and Microsoft</affiliation></author>
      <author><first>Naoyuki</first><last>Kanda</last><affiliation>Microsoft</affiliation></author>
      <author><first>Noel</first><last>Codella</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bin</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yu</first><last>Shi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lu</first><last>Yuan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Takuya</first><last>Yoshioka</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Zeng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xuedong</first><last>Huang</last></author>
      <pages>1615-1627</pages>
      <abstract>The convergence of text, visual, and audio data is crucial towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models that lack generative abilities. We propose closing this gap with i-Code V2, one of the first models capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder to project combinations of modalities into a shared representational space. Language tokens are generated from these representations via an autoregressive decoder. i-Code V2 is pretrained end-to-end on a large collection of dual- and single-modality datasets with a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.</abstract>
      <url hash="ae3111b3">2024.findings-naacl.105</url>
    </paper>
    <paper id="106">
      <title>Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Varun</first><last>Embar</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Benjamin</first><last>Han</last><affiliation>Apple</affiliation></author>
      <pages>1628-1644</pages>
      <abstract>Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs input facts with their original and perturbed descriptions. We test TWEAK with two generators, and the best TWEAK variants improve on average for the two models by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution evaluations, respectively, and with only a 0.14/0.32-point decline in quality (BERTScore).</abstract>
      <url hash="063bdc72">2024.findings-naacl.106</url>
    </paper>
    <paper id="107">
      <title>It’s All Relative! – A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction</title>
      <author><first>Aditi</first><last>Chaudhary</last><affiliation>Google</affiliation></author>
      <author><first>Karthik</first><last>Raman</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>1645-1664</pages>
      <abstract>Large language models (LLMs) have shown promising ability to generate synthetic query-document pairs by prompting with as few as 8 demonstrations. This has enabled building better IR models, especially for tasks with no training data. Typically, such synthetic query generation (QGen) approaches condition on an input context (e.g. a text document) and generate a query relevant to that context, or condition the QGen additionally on the relevance label (e.g. relevant vs irrelevant) to generate queries across relevance buckets. However, we find that such QGen approaches are sub-optimal as they require the model to reason about the desired label and the input from a handful of examples. In this work, we propose to reduce this burden of LLMs by generating queries simultaneously for different labels. We hypothesize that instead of asking the model to generate, say, an irrelevant query given an input context, asking the model to generate an irrelevant query relative to a relevant query is a much simpler task. Extensive experimentation across nine IR datasets shows that synthetic queries generated in such a fashion translates to better downstream performance.</abstract>
      <url hash="7d04868b">2024.findings-naacl.107</url>
    </paper>
    <paper id="108">
      <title><fixed-case>RS</fixed-case>-<fixed-case>DPO</fixed-case>: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models</title>
      <author><first>Saeed</first><last>Khaki</last><affiliation>Amazon</affiliation></author>
      <author><first>JinJin</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Lan</first><last>Ma</last><affiliation>Amazon</affiliation></author>
      <author><first>Liu</first><last>Yang</last></author>
      <author><first>Prathap</first><last>Ramachandra</last><affiliation>Amazon</affiliation></author>
      <pages>1665-1680</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO often relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</abstract>
      <url hash="7045d22e">2024.findings-naacl.108</url>
    </paper>
    <paper id="109">
      <title>Hypernetwork-Assisted Parameter-Efficient Fine-Tuning with Meta-Knowledge Distillation for Domain Knowledge Disentanglement</title>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Shizhou</first><last>Huang</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>1681-1695</pages>
      <abstract>Domain adaptation from labeled source domains to the target domain is important in practical summarization scenarios. However, the key challenge is domain knowledge disentanglement. In this work, we explore how to disentangle domain-invariant knowledge from source domains while learning specific knowledge of the target domain. Specifically, we propose a hypernetwork-assisted encoder-decoder architecture with parameter-efficient fine-tuning. It leverages a hypernetwork instruction learning module to generate domain-specific parameters from the encoded inputs accompanied by task-related instruction. Further, to better disentangle and transfer knowledge from source domains to the target domain, we introduce a meta-knowledge distillation strategy to build a meta-teacher model that captures domain-invariant knowledge across multiple domains and use it to transfer knowledge to students. Experiments on three dialogue summarization datasets show the effectiveness of the proposed model. Human evaluations also show the superiority of our model with regard to the summary generation quality.</abstract>
      <url hash="259ba3b1">2024.findings-naacl.109</url>
    </paper>
    <paper id="110">
      <title><fixed-case>MIC</fixed-case>o: Preventative Detoxification of Large Language Models through Inhibition Control</title>
      <author><first>Roy</first><last>Siegelmann</last><affiliation>Whiting School of Engineering</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Prasoon</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Lisa</first><last>Bauer</last><affiliation>Amazon</affiliation></author>
      <author><first>Jwala</first><last>Dhamala</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Reza</first><last>Ghanadan</last><affiliation>Amazon</affiliation></author>
      <pages>1696-1703</pages>
      <abstract>Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60% reduced, with over 75% reduction in severe toxicity.</abstract>
      <url hash="c92aa678">2024.findings-naacl.110</url>
    </paper>
    <paper id="111">
      <title>Reinforcement Learning with Token-level Feedback for Controllable Text Generation</title>
      <author><first>Wendi</first><last>Li</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Kaihe</first><last>Xu</last></author>
      <author><first>Wenfeng</first><last>Xie</last></author>
      <author><first>Dangyang</first><last>Chen</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1704-1719</pages>
      <abstract>To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a “first-quantize-then-noise” paradigm to enhance the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG.</abstract>
      <url hash="d0a25219">2024.findings-naacl.111</url>
    </paper>
    <paper id="112">
      <title><fixed-case>C</fixed-case>o<fixed-case>MM</fixed-case>: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving</title>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Boran</first><last>Han</last></author>
      <pages>1720-1738</pages>
      <abstract>Large Language Models (LLMs) have shown great ability in solving traditional natural language tasks and elementary reasoning tasks with appropriate prompting techniques. However, their ability is still limited in solving complicated science problems. In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.</abstract>
      <url hash="2cd09a43">2024.findings-naacl.112</url>
    </paper>
    <paper id="113">
      <title>Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies</title>
      <author><first>Anaelia</first><last>Ovalle</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Jwala</first><last>Dhamala</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Richard</first><last>Zemel</last><affiliation>Department of Computer Science, Columbia University and Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>1739-1756</pages>
      <abstract>Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data scarcity is a known culprit, the precise mechanisms through which scarcity affects this behavior remain underexplored. We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training. This disparate tokenization mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking new misgendering mitigation strategies. We propose two techniques: (1) pronoun tokenization parity, a method to enforce consistent tokenization across gendered pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency. Our proposed methods outperform finetuning with standard BPE, improving neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering to tokenization and deficient neopronoun grammar, indicating that LLMs unable to correctly treat neopronouns as pronouns are more prone to misgender.</abstract>
      <url hash="842cac25">2024.findings-naacl.113</url>
    </paper>
    <paper id="114">
      <title><fixed-case>A</fixed-case>da<fixed-case>PT</fixed-case>: A Set of Guidelines for Hyperbolic Multimodal Multilingual <fixed-case>NLP</fixed-case></title>
      <author><first>Ramit</first><last>Sawhney</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Shrey</first><last>Pandit</last></author>
      <author><first>Vishwa</first><last>Shah</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>1757-1771</pages>
      <abstract>The Euclidean space is the familiar space for training neural models and performing arithmetic operations.However, many data types inherently possess complex geometries, and model training methods involve operating over their latent representations, which cannot be effectively captured in the Euclidean space.The hyperbolic space provides a more generalized representative geometry to model the hierarchical complexities of the tree-like structure of natural language.We propose AdaPT a set of guidelines for initialization, parametrization, and training of neural networks, which adapts to the dataset and can be used with different manifolds. AdaPT can be generalized over any existing neural network training methodology and leads to more stable training without a substantial increase in training time.We apply AdaPT guidelines over two state-of-the-art deep learning approaches and empirically demonstrate its effectiveness through experiments on three tasks over 12 languages across speech and text.Through extensive qualitative analysis, we put forward the applicability of AdaPT as a set of guidelines optimally utilizing the manifold geometry, which can be extended to various downstream tasks across languages and modalities.</abstract>
      <url hash="ae2405f1">2024.findings-naacl.114</url>
    </paper>
    <paper id="115">
      <title>More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning for <fixed-case>LLM</fixed-case>s with In-Context Sampling</title>
      <author><first>Bingsheng</first><last>Yao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Guiming</first><last>Chen</last></author>
      <author><first>Ruishi</first><last>Zou</last></author>
      <author><first>Yuxuan</first><last>Lu</last></author>
      <author><first>Jiachen</first><last>Li</last></author>
      <author><first>Shao</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yisi</first><last>Sang</last><affiliation>Apple</affiliation></author>
      <author><first>Sijia</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>James</first><last>Hendler</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Dakuo</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <pages>1772-1790</pages>
      <abstract>While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM’s performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs’ performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM’s performance, which sheds light on a new yet promising future research direction.</abstract>
      <url hash="a07d24c8">2024.findings-naacl.115</url>
    </paper>
    <paper id="116">
      <title><fixed-case>ZSEE</fixed-case>: A Dataset based on Zeolite Synthesis Event Extraction for Automated Synthesis Platform</title>
      <author><first>Song</first><last>He</last></author>
      <author><first>Xin</first><last>Peng</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Yihan</first><last>Cai</last><affiliation>Shanghai Research Institute of Petrochemical Technology, Sinopec Corporation</affiliation></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Yuan</last></author>
      <author><first>WenLi</first><last>Du</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Weimin</first><last>Yang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <pages>1791-1808</pages>
      <abstract>Automated synthesis of zeolite, one of the most important catalysts in chemical industries, holds great significance for attaining economic and environmental benefits. Structural synthesis data extracted through NLP technologies from zeolite experimental procedures can significantly expedite automated synthesis owing to its machine readability. However, the utilization of NLP technologies in information extraction of zeolite synthesis remains restricted due to the lack of annotated datasets. In this paper, we formulate an event extraction task to mine structural synthesis actions from experimental narratives for modular automated synthesis. Furthermore, we introduce ZSEE, a novel dataset containing fine-grained event annotations of zeolite synthesis actions. Our dataset features 16 event types and 13 argument roles which cover all the experimental operational steps of zeolite synthesis. We explore current state-of-the-art event extraction methods on ZSEE, perform error analysis based on the experimental results, and summarize the challenges and corresponding research directions to further facilitate the automated synthesis of zeolites. The code is publicly available at https://github.com/Hi-0317/ZSEE.</abstract>
      <url hash="07ecf9ca">2024.findings-naacl.116</url>
    </paper>
    <paper id="117">
      <title>Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information</title>
      <author><first>Kyubyung</first><last>Chae</last></author>
      <author><first>Jaepill</first><last>Choi</last></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taesup</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>1809-1820</pages>
      <abstract>A primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text. We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text. To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information. This strategy adjusts the generation probability of each token by comparing it with the token’s marginal probability within the domain of the source text. According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.</abstract>
      <url hash="7cbaab57">2024.findings-naacl.117</url>
    </paper>
    <paper id="118">
      <title>Adversarial <fixed-case>DPO</fixed-case>: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</title>
      <author><first>San</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>1821-1835</pages>
      <abstract>Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model’s resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.</abstract>
      <url hash="8c5bd224">2024.findings-naacl.118</url>
    </paper>
    <paper id="119">
      <title>Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models</title>
      <author><first>Fobo</first><last>Shi</last></author>
      <author><first>Peijun</first><last>Qing</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Dong</first><last>Yang</last></author>
      <author><first>Nan</first><last>Wang</last></author>
      <author><first>Youbo</first><last>Lei</last></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <author><first>Xiaodong</first><last>Lin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Duantengchuan</first><last>Li</last></author>
      <pages>1836-1862</pages>
      <abstract>Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt “Let’s think step by step”, Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at https://github.com/YouBLEI/Prompt-Space</abstract>
      <url hash="6f3152a4">2024.findings-naacl.119</url>
    </paper>
    <paper id="120">
      <title><fixed-case>DAGCN</fixed-case>: Distance-based and Aspect-oriented Graph Convolutional Network for Aspect-based Sentiment Analysis</title>
      <author><first>Zhihao</first><last>Wang</last></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Ru</first><last>Yang</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Chang</first><last>Guo</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Maozhen</first><last>Li</last><affiliation>Brunel University Uxbridge</affiliation></author>
      <pages>1863-1876</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a task that aims to determine the sentiment polarity of aspects by identifying opinion words. Recent advancements have predominantly been rooted either in semantic or syntactic methods. However, both of them tend to interference from local factors such as irrelevant words and edges, hindering the precise identification of opinion words. In this paper, we present Distance-based and Aspect-oriented Graph Convolutional Network (DAGCN) to address the aforementioned issue. Firstly, we introduce the Distance-based Syntactic Weight (DSW). It focuses on the local scope of aspects in the pruned dependency trees, thereby reducing the candidate pool of opinion words. Additionally, we propose Aspect-Fusion Attention (AF) to further filter opinion words within the local context and consider cases where opinion words are distant from the aspect. With the combination of DSW and AF, we achieve precise identification of corresponding opinion words. Extensive experiments on three public datasets demonstrate that the proposed model outperforms state-of-the-art models and verify the effectiveness of the proposed architecture.</abstract>
      <url hash="92e2da76">2024.findings-naacl.120</url>
    </paper>
    <paper id="121">
      <title>Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs</title>
      <author><first>Zhuoyi</first><last>Peng</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>1877-1890</pages>
      <abstract>We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases. For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph. We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end-to-end manner. Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a self-supervised fashion. Substantial improvements are also observed in the supervised setting, underscoring the potential benefits of leveraging retrieved phrase graph augmentation.</abstract>
      <url hash="96ebed37">2024.findings-naacl.121</url>
    </paper>
    <paper id="122">
      <title>Self-Regulated Sample Diversity in Large Language Models</title>
      <author><first>Mingyue</first><last>Liu</last></author>
      <author><first>Jonathan</first><last>Frawley</last><affiliation>Durham University</affiliation></author>
      <author><first>Sarah</first><last>Wyer</last></author>
      <author><first>Hubert P. H.</first><last>Shum</last><affiliation>Durham University</affiliation></author>
      <author><first>Sara</first><last>Uckelman</last><affiliation>Durham University</affiliation></author>
      <author><first>Sue</first><last>Black</last><affiliation>Durham University</affiliation></author>
      <author><first>Chris</first><last>Willcocks</last><affiliation>Durham University and Durham University</affiliation></author>
      <pages>1891-1899</pages>
      <abstract>Sample diversity depends on the task; within mathematics, precision and determinism are paramount, while storytelling thrives on creativity and surprise. This paper presents a simple self-regulating approach where we adjust sample diversity inference parameters dynamically based on the input prompt—in contrast to existing methods that require expensive and inflexible setups, or maintain static values during inference. Capturing a broad spectrum of sample diversities can be formulated as a straightforward self-supervised inference task, which we find significantly improves the quality of responses generically without model retraining or fine-tuning. In particular, our method demonstrates significant improvement in all supercategories of the MMLU multitask benchmark (GPT-3.5: <tex-math>+4.4\%</tex-math>, GPT-4: <tex-math>+1.5\%</tex-math>), which captures a large variety of difficult tasks covering STEM, the humanities and social sciences.</abstract>
      <url hash="5c543fa8">2024.findings-naacl.122</url>
    </paper>
    <paper id="123">
      <title>Methods, Applications, and Directions of Learning-to-Rank in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Justin</first><last>Lee</last></author>
      <author><first>Gabriel</first><last>Bernier-Colborne</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Tegan</first><last>Maharaj</last><affiliation>Toronto University and Ecole Polytechnique de Montreal</affiliation></author>
      <author><first>Sowmya</first><last>Vajjala</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>1900-1917</pages>
      <abstract>Learning-to-rank (LTR) algorithms aim to order a set of items according to some criteria. They are at the core of applications such as web search and social media recommendations, and are an area of rapidly increasing interest, with the rise of large language models (LLMs) and the widespread impact of these technologies on society. In this paper, we survey the diverse use cases of LTR methods in natural language processing (NLP) research, looking at previously under-studied aspects such as multilingualism in LTR applications and statistical significance testing for LTR problems. We also consider how large language models are changing the LTR landscape. This survey is aimed at NLP researchers and practitioners interested in understanding the formalisms and best practices regarding the application of LTR approaches in their research.</abstract>
      <url hash="22b02e46">2024.findings-naacl.123</url>
    </paper>
    <paper id="124">
      <title>When Quantization Affects Confidence of Large Language Models?</title>
      <author><first>Irina</first><last>Proskurina</last></author>
      <author><first>Luc</first><last>Brun</last><affiliation>Laboratoire ERIC, Université Lumiére (Lyon II)</affiliation></author>
      <author><first>Guillaume</first><last>Metzler</last><affiliation>Université Lumiére (Lyon II)</affiliation></author>
      <author><first>Julien</first><last>Velcin</last><affiliation>ERIC</affiliation></author>
      <pages>1918-1928</pages>
      <abstract>Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs.This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss.Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.We make our code and quantized models publicly available.</abstract>
      <url hash="13453099">2024.findings-naacl.124</url>
    </paper>
    <paper id="125">
      <title><fixed-case>M</fixed-case>ed<fixed-case>C</fixed-case>ycle: Unpaired Medical Report Generation via Cycle-Consistency</title>
      <author><first>Elad</first><last>Hirsch</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Gefen</first><last>Dawidowicz</last></author>
      <author><first>Ayellet</first><last>Tal</last><affiliation>Technion and Technion</affiliation></author>
      <pages>1929-1944</pages>
      <abstract>Generating medical reports for X-ray images presents a significant challenge, particularly in unpaired scenarios where access to paired image-report data for training is unavailable. Previous works have typically learned a joint embedding space for images and reports, necessitating a specific labeling schema for both. We introduce an innovative approach that eliminates the need for consistent labeling schemas, thereby enhancing data accessibility and enabling the use of incompatible datasets. This approach is based on cycle-consistent mapping functions that transform image embeddings into report embeddings, coupled with report auto encoding for medical report generation. Our model and objectives consider intricate local details and the overarching semantic context within images and reports. This approach facilitates the learning of effective mapping functions, resulting in the generation of coherent reports. It outperforms state-of-the-art results in unpaired chest X-ray report generation, demonstrating improvements in both language and clinical metrics.</abstract>
      <url hash="78420f05">2024.findings-naacl.125</url>
    </paper>
    <paper id="126">
      <title>Beta-<fixed-case>LR</fixed-case>: Interpretable Logical Reasoning based on Beta Distribution</title>
      <author><first>Yizhuo</first><last>Ma</last></author>
      <author><first>Ke</first><last>Qin</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Shuang</first><last>Liang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <pages>1945-1955</pages>
      <abstract>The logical information contained in text isof significant importance for logical reasoning.Previous approaches have relied on embeddingtext into a low-dimensional vector to capturelogical information and perform reasoning inEuclidean space. These methods involve constructing special graph architectures that matchlogical relations or designing data augmentation frameworks by extending texts based onsymbolic logic. However, it presents two obvious problems. 1) The logical informationreflected in the text exhibits uncertainty that isdifficult to represent using a vector. 2) Integrating logical information requires modeling logical operations (such as ∪, ∩, and ¬), while onlysimple arithmetic operations can be performedin Euclidean space. To address both the problems, we propose Beta-LR, a probabilistic embedding method to capture logical information.Specifically, we embed texts into beta distribution on each dimension to eliminate logical uncertainty. We also define neural operators thatenable interpretability and perform logical operations based on the characteristics of the betadistribution. We conduct experiments on twodatasets, ReClor and LogiQA, and our Beta-LRachieves competitive results. The experimentsdemonstrate that our method effectively captures the logical information in text for reasoning purposes. The source code is available athttps://github.com/myz12138/Beta-LR.</abstract>
      <url hash="5865e473">2024.findings-naacl.126</url>
    </paper>
    <paper id="127">
      <title>Applications of <fixed-case>BERT</fixed-case> Models Towards Automation of Clinical Coding in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Haraldur</first><last>Hauksson</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Hafsteinn</first><last>Einarsson</last><affiliation>deCODE genetics and University of Iceland</affiliation></author>
      <pages>1956-1967</pages>
      <abstract>This study explores the potential of automating clinical coding in Icelandic, a language with limited digital resources, by leveraging over 25 years of electronic health records (EHR) from the Landspitali University Hospital. Traditionally a manual and error-prone task, clinical coding is essential for patient care, billing, and research. Our research delves into the effectiveness of Transformer-based models in automating this process. We investigate various model training strategies, including continued pretraining and model adaptation, under a constrained computational budget. Our findings reveal that the best-performing model achieves competitive results in both micro and macro F1 scores, with label attention contributing significantly to its success. The study also explores the possibility of training on unlabeled data. Our research provides valuable insights into the possibilities of using NLP for clinical coding in low-resource languages, demonstrating that small countries with unique languages and well-segmented healthcare records can achieve results comparable to those in higher-resourced languages.</abstract>
      <url hash="3140157d">2024.findings-naacl.127</url>
    </paper>
    <paper id="128">
      <title>“Tell me who you are and <fixed-case>I</fixed-case> tell you how you argue”: Predicting Stances and Arguments for Stakeholder Groups</title>
      <author><first>Philipp</first><last>Heinisch</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Lorik</first><last>Dumani</last><affiliation>Trier University</affiliation></author>
      <author><first>Philipp</first><last>Cimiano</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Ralf</first><last>Schenkel</last><affiliation>Trier University</affiliation></author>
      <pages>1968-1982</pages>
      <abstract>Argument mining has focused so far mainly on the identification, extraction, and formalization of arguments. An important yet unaddressedtask consists in the prediction of the argumentative behavior of stakeholders in a debate. Predicting the argumentative behavior in advance can support foreseeing issues in public policy making or help recognize potential disagreements early on and help to resolve them. In this paper, we consider the novel task of predicting the argumentative behavior of individual stakeholders. We present ARGENST, a framework that relies on a recommender-based architecture to predict the stance and the argumentative main point on a specific controversial topic for a given stakeholder, which is described in terms of a profile including properties related to demographic attributes, religious and political orientation, socio-economic background, etc. We evaluate our approach on the well-known debate.org dataset in terms of accuracy for predicting stance as well as in terms of similarity of the generated arguments to the ground truth arguments using BERTScore. As part of a case study, we show how juries of members representing different stakeholder groups and perspectives can be assembled to simulate the public opinion on a given topic.</abstract>
      <url hash="f7b77a22">2024.findings-naacl.128</url>
    </paper>
    <paper id="129">
      <title>Psychometric Predictive Power of Large Language Models</title>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>1983-2005</pages>
      <abstract>Instruction tuning aligns the response of large language models (LLMs) with human preferences.Despite such efforts in human–LLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs.In addition, we explore prompting methodologies for simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve psychometric predictive power, but are still inferior to small base models.These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, pure next-word probability remains a strong predictor for human reading behavior, even in the age of LLMs.</abstract>
      <url hash="9b906779">2024.findings-naacl.129</url>
    </paper>
    <paper id="130">
      <title>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</title>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>2006-2017</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions—commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 85% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model’s bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs’ predictions, leading to up to 8 percentage points improvement across different models and benchmarks.</abstract>
      <url hash="caa19846">2024.findings-naacl.130</url>
    </paper>
    <paper id="131">
      <title><fixed-case>PEEB</fixed-case>: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck</title>
      <author><first>Thang</first><last>Pham</last></author>
      <author><first>Peijie</first><last>Chen</last><affiliation>Auburn University</affiliation></author>
      <author><first>Tin</first><last>Nguyen</last></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Anh</first><last>Nguyen</last><affiliation>Auburn University</affiliation></author>
      <pages>2018-2053</pages>
      <abstract>CLIP-based classifiers rely on the prompt containing a class name that is known to the text encoder. Therefore, they perform poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB – an explainable and editable classifier to (1) express the class name into a set of text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a huge margin (∼10× in top-1 accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20% accuracy on CUB-200 and Stanford Dogs-120, respectively) but also the first to enable users to edit the text descriptors to form a new classifier without any re-training. Compared to concept bottleneck models, PEEB is also the SOTA in both zero-shot and supervised-learning settings.</abstract>
      <url hash="8a6403c4">2024.findings-naacl.131</url>
    </paper>
    <paper id="132">
      <title>Ethos: Rectifying Language Models in Orthogonal Parameter Space</title>
      <author><first>Lei</first><last>Gao</last></author>
      <author><first>Yue</first><last>Niu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tingting</first><last>Tang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California, University of Southern California and University of Southern California</affiliation></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>2054-2068</pages>
      <abstract>Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: bias, toxicity, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge while maintaining the overall model performance compared to current task arithmetic methods.</abstract>
      <url hash="ce7d27ed">2024.findings-naacl.132</url>
    </paper>
    <paper id="133">
      <title>Crafting In-context Examples according to <fixed-case>LM</fixed-case>s’ Parametric Knowledge</title>
      <author><first>Yoonsang</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Pranav</first><last>Atreya</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Xi</first><last>Ye</last></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>2069-2085</pages>
      <abstract>In-context learning can improve the performances of knowledge-rich tasks such as question answering. In such scenarios, in-context examples trigger a language model (LM) to surface information stored in its parametric knowledge. We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples. We identify ‘known’ examples, where models can correctly answer from their parametric knowledge, and ‘unknown’ ones. Our experiments show that prompting with ‘unknown’ examples decreases the performance, potentially as it encourages hallucination rather than searching for its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM’s knowledge of each answer. Together, our study sheds light on how to best construct in-context example sets for knowledge-rich tasks.</abstract>
      <url hash="916b4b4f">2024.findings-naacl.133</url>
    </paper>
    <paper id="134">
      <title><fixed-case>ICXML</fixed-case>: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</title>
      <author><first>Yaxin</first><last>Zhu</last></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>Google and University of Massachusetts, Amherst</affiliation></author>
      <pages>2086-2098</pages>
      <abstract>This paper focuses on the task of Extreme Multi-Label Classification (XMC) whose goal is to predict multiple labels for each instance from an extremely large label space. While existing research has primarily focused on fully supervised XMC, real-world scenarios often lack supervision signals, highlighting the importance of zero-shot settings. Given the large label space, utilizing in-context learning approaches is not trivial. We address this issue by introducing In-Context Extreme Multi-label Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through in-context learning and then reranks them. Extensive experiments suggest that ICXML advances the state of the art on two diverse public benchmarks.</abstract>
      <url hash="9002a706">2024.findings-naacl.134</url>
    </paper>
    <paper id="135">
      <title><fixed-case>CLGSI</fixed-case>: A Multimodal Sentiment Analysis Framework based on Contrastive Learning Guided by Sentiment Intensity</title>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Xunde</first><last>Dong</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Yupeng</first><last>Qiang</last></author>
      <pages>2099-2110</pages>
      <abstract>Recently, contrastive learning has begun to gain popularity in multimodal sentiment analysis (MSA). However, most of existing MSA methods based on contrastive learning lacks more detailed learning of the distribution of sample pairs with different sentiment intensity differences in the contrastive learning representation space. In addition, limited research has been conducted on the fusion of each modality representation obtained by contrastive learning training.In this paper, we propose a novel framework for multimodal sentiment analysis based on Contrastive Learning Guided by Sentiment Intensity (CLGSI). Firstly, the proposed contrastive learning guided by sentiment intensity selects positive and negative sample pairs based on the difference in sentiment intensity and assigns corresponding weights accordingly.Subsequently, we propose a new multimodal representation fusion mechanism, called Global-Local-Fine-Knowledge (GLFK), which extracts common features between different modalities’ representations. At the same time, each unimodal encoder output is separately processed by a Multilayer Perceptron (MLP) to extract specific features of each modality. Finally, joint learning of the common and specific features is used to predict sentiment intensity. The effectiveness of CLGSI is assessed on two English datasets, MOSI and MOSEI, as well as one Chinese dataset, SIMS. We achieve competitive experimental results, which attest to the strong generalization performance of our approach. The code for our approach will be released in https://github.com/AZYoung233/CLGSI</abstract>
      <url hash="942b7c17">2024.findings-naacl.135</url>
    </paper>
    <paper id="136">
      <title>Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains</title>
      <author><first>Zijie</first><last>Wang</last></author>
      <author><first>Farzana</first><last>Rashid</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <pages>2111-2128</pages>
      <abstract>People often answer yes-no questions without explicitly saying yes, no, or similar polar key-words. Figuring out the meaning of indirectanswers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.</abstract>
      <url hash="7f22a2c6">2024.findings-naacl.136</url>
    </paper>
    <paper id="137">
      <title>Enhancing Perception: Refining Explanations of News Claims with <fixed-case>LLM</fixed-case> Conversations</title>
      <author><first>Yi-Li</first><last>Hsu</last><affiliation>National Tsinghua University and Academia Sinica</affiliation></author>
      <author><first>Jui-Ning</first><last>Chen</last><affiliation>National Taiwan University and , Academia Sinica</affiliation></author>
      <author><first>Yang</first><last>Fan Chiang</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Shang-Chien</first><last>Liu</last></author>
      <author><first>Aiping</first><last>Xiong</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></author>
      <pages>2129-2147</pages>
      <abstract>We introduce Enhancing Perception, a framework for Large Language Models (LLMs) designed to streamline the time-intensive task typically undertaken by professional fact-checkers of crafting explanations for fake news. This study investigates the effectiveness of enhancing LLM explanations through conversational refinement. We compare various questioner agents, including state-of-the-art LLMs like GPT-4, Claude 2, PaLM 2, and 193 American participants acting as human questioners. Based on the histories of these refinement conversations, we further generate comprehensive summary explanations. We evaluated the effectiveness of these initial, refined, and summary explanations across 40 news claims by involving 2,797 American participants, measuring their self-reported belief change regarding both real and fake claims after receiving the explanations. Our findings reveal that, in the context of fake news, explanations that have undergone conversational refinement—whether by GPT-4 or human questioners, who ask more diverse and detail-oriented questions—were significantly more effective than both the initial unrefined explanations and the summary explanations. Moreover, these refined explanations achieved a level of effectiveness comparable to that of expert-written explanations. The results highlight the potential of automatic explanation refinement by LLMs in debunking fake news claims.</abstract>
      <url hash="43921586">2024.findings-naacl.137</url>
    </paper>
    <paper id="138">
      <title>How Interpretable are Reasoning Explanations from Prompting Large Language Models?</title>
      <author><first>Yeo</first><last>Wei Jie</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Ranjan</first><last>Satapathy</last></author>
      <author><first>Rick</first><last>Goh</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>2148-2164</pages>
      <abstract>Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability</abstract>
      <url hash="3009ac12">2024.findings-naacl.138</url>
    </paper>
    <paper id="139">
      <title>Plug-in Language Model: Controlling Text Generation with a Simple Regression Model</title>
      <author><first>Nai-Chi</first><last>Yang</last></author>
      <author><first>Wei-Yun</first><last>Ma</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Pu-Jen</first><last>Cheng</last><affiliation>National Taiwan University</affiliation></author>
      <pages>2165-2181</pages>
      <abstract>Large-scale pre-trained language models have displayed unrivaled capacity in generating text that closely resembles human-written text. Nevertheless, generating texts adhering to specific conditions without fine-tuning or adding new parameters can be challenging. Contemporary approaches commonly rely on either prompts or auxiliary models to avoid modifying the language models. These auxiliary models are designed to assess whether a generated token contributes to meeting the desired requirements. These approaches adjust the distribution of the next token during the inference phase by leveraging the prediction score of the desired attribute to calculate gradients. However, these auxiliary models typically require the language model’s latent states. This prerequisite challenges integrating various existing black box attribute models or tools. We present the Plug-in Language Model (PiLM) as a solution to address the limitations. PiLM leverages reinforcement learning to utilize black box tools directly, adjusting the latent state to control text generation. However, performing backpropagation during the inference phase is time-consuming for PiLM. By replacing backpropagation with a simple regression model, PiLM can achieve an inference time comparable to that of the original LLM. Experiment results show that our approaches in this paper outperform existing state-of-the-art methods that rely on gradient-based, weighted decoding, or prompt-based methodologies.</abstract>
      <url hash="477552b6">2024.findings-naacl.139</url>
    </paper>
    <paper id="140">
      <title>Signer Diversity-driven Data Augmentation for Signer-Independent Sign Language Translation</title>
      <author><first>Honghaofu</first><last>Honghaofu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Rui</first><last>Zhao</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>2182-2193</pages>
      <abstract>The primary objective of sign language translation (SLT) is to transform sign language videos into natural sentences.A crucial challenge in this field is developing signer-independent SLT systems which requires models to generalize effectively to signers not encountered during training.This challenge is exacerbated by the limited diversity of signers in existing SLT datasets, which often results in suboptimal generalization capabilities of current models.Achieving robustness to unseen signers is essential for signer-independent SLT.However, most existing method relies on signer identity labels, which is often impractical and costly in real-world applications.To address this issue, we propose the Signer Diversity-driven Data Augmentation (SDDA) method that can achieve good generalization without relying on signer identity labels. SDDA comprises two data augmentation schemes. The first is data augmentation based on adversarial training, which aims to utilize the gradients of the model to generate adversarial examples. The second is data augmentation based on diffusion model, which focuses on using the advanced diffusion-based text guided image editing method to modify the appearances of the signer in images. The combination of the two strategies significantly enriches the diversity of signers in the training process.Moreover, we introduce a consistency loss and a discrimination loss to enhance the learning of signer-independent features.Our experimental results demonstrate our model significantly enhances the performance of SLT in the signer-independent setting, achieving state-of-the-art results without relying on signer identity labels.</abstract>
      <url hash="65bd9383">2024.findings-naacl.140</url>
    </paper>
    <paper id="141">
      <title>A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation</title>
      <author><first>Francois</first><last>Meyer</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <pages>2194-2200</pages>
      <abstract>Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.</abstract>
      <url hash="1f5fe628">2024.findings-naacl.141</url>
    </paper>
    <paper id="142">
      <title>Multi-Granularity Guided Fusion-in-Decoder</title>
      <author><first>Eunseong</first><last>Choi</last></author>
      <author><first>Hyeri</first><last>Lee</last></author>
      <author><first>Jongwuk</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>2201-2212</pages>
      <abstract>In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, *i.e.*, Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the ***M**ulti-**G**ranularity guided **F**usion-**i**n-**D**ecoder (**MGFiD**)*, discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an *anchor vector* that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for *passage pruning*. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution.</abstract>
      <url hash="584ede0d">2024.findings-naacl.142</url>
    </paper>
    <paper id="143">
      <title>Group Fairness in Multilingual Speech Recognition Models</title>
      <author><first>Anna</first><last>Zee</last></author>
      <author><first>Marc</first><last>Zee</last><affiliation>Research, Google</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>2213-2226</pages>
      <abstract>We evaluate the performance disparity of the Whisper and MMS families of ASR models across the VoxPopuli and Common Voice multilingual datasets, with an eye toward intersectionality. Our two most important findings are that model size, surprisingly, correlates logarithmically with worst-case performance disparities, meaning that larger (and better) models are less fair. We also observe the importance of intersectionality. In particular, models often exhibit significant performance disparity across binary gender for adolescents.</abstract>
      <url hash="30fec689">2024.findings-naacl.143</url>
    </paper>
    <paper id="144">
      <title>Rethinking Machine Ethics – Can <fixed-case>LLM</fixed-case>s Perform Moral Reasoning through the Lens of Moral Theories?</title>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Minda</first><last>Hu</last></author>
      <author><first>Junan</first><last>Li</last></author>
      <author><first>Xiaoying</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2227-2242</pages>
      <abstract>Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators’ moral stances and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.</abstract>
      <url hash="c0042da4">2024.findings-naacl.144</url>
    </paper>
    <paper id="145">
      <title>Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yi</first><last>Chen</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Qi</first><last>Zhu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2243-2255</pages>
      <abstract>The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains.Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.</abstract>
      <url hash="e8001e6e">2024.findings-naacl.145</url>
    </paper>
    <paper id="146">
      <title><fixed-case>BERT</fixed-case>weet’s <fixed-case>TACO</fixed-case> Fiesta: Contrasting Flavors On The Path Of Inference And Information-Driven Argument Mining On <fixed-case>T</fixed-case>witter</title>
      <author><first>Marc</first><last>Feger</last></author>
      <author><first>Stefan</first><last>Dietze</last><affiliation>GESIS and Heinrich-Heine-University Düsseldorf</affiliation></author>
      <pages>2256-2266</pages>
      <abstract>Argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of Twitter (now <tex-math>\mathbb{X}</tex-math>), a key platform for online discourse and exchange. Thereby, Twitter offers a diverse repository of short messages bearing on both of these elements. For text classification, transformer approaches, particularly BERT, offer state-of-the-art solutions. Our study delves into optimizing the embeddings of the understudied BERTweet transformer for argument mining on Twitter and broader generalization across topics.We explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. Using the TACO dataset, our approach augments tweets for optimizing BERTweet in a Siamese network, strongly improving classification and cross-topic generalization compared to standard methods.Overall, we contribute the transformer WRAPresentations and classifier WRAP, scoring 86.62% F1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on Twitter.</abstract>
      <url hash="8295166d">2024.findings-naacl.146</url>
    </paper>
    <paper id="147">
      <title>Testing the limits of logical reasoning in neural and hybrid models</title>
      <author><first>Manuel</first><last>Guzman</last></author>
      <author><first>Jakub</first><last>Szymanik</last><affiliation>University of Trento</affiliation></author>
      <author><first>Maciej</first><last>Malicki</last></author>
      <pages>2267-2279</pages>
      <abstract>We study the ability of neural and hybrid models to generalize logical reasoning patterns. We created a series of tests for analyzing various aspects of generalization in the context of language and reasoning, focusing on compositionality and recursiveness. We used them to study the syllogistic logic in hybrid models, where the network assists in premise selection. We analyzed feed-forward, recurrent, convolutional, and transformer architectures. Our experiments demonstrate that even though the models can capture elementary aspects of the meaning of logical terms, they learn to generalize logical reasoning only to a limited degree.</abstract>
      <url hash="90a37e6c">2024.findings-naacl.147</url>
    </paper>
    <paper id="148">
      <title><fixed-case>METAL</fixed-case>: Towards Multilingual Meta-Evaluation</title>
      <author><first>Rishav</first><last>Hada</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>2280-2298</pages>
      <abstract>With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.</abstract>
      <url hash="2e187bda">2024.findings-naacl.148</url>
    </paper>
    <paper id="149">
      <title><fixed-case>AGIE</fixed-case>val: A Human-Centric Benchmark for Evaluating Foundation Models</title>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Yiduo</first><last>Guo</last></author>
      <author><first>Yaobo</first><last>Liang</last></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yanlin</first><last>Wang</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Amin</first><last>Saied</last></author>
      <author><first>Weizhu</first><last>Chen</last><affiliation>Microsoft GenAI</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>2299-2314</pages>
      <abstract>Assessing foundation models’ abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95% accuracy on SAT Math and 92.5% on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models’ performance in real-world scenarios.</abstract>
      <url hash="ab73271f">2024.findings-naacl.149</url>
    </paper>
    <paper id="150">
      <title>Product Description and <fixed-case>QA</fixed-case> Assisted Self-Supervised Opinion Summarization</title>
      <author><first>Tejpalsingh</first><last>Siledar</last></author>
      <author><first>Rupasai</first><last>Rangaraju</last></author>
      <author><first>Sankara</first><last>Muddu</last></author>
      <author><first>Suman</first><last>Banerjee</last><affiliation>Flipkart</affiliation></author>
      <author><first>Amey</first><last>Patil</last></author>
      <author><first>Sudhanshu</first><last>Singh</last></author>
      <author><first>Muthusamy</first><last>Chelliah</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last></author>
      <author><first>Swaprava</first><last>Nath</last><affiliation>IIT Kanpur and Computer Science and Engineering, Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>2315-2332</pages>
      <abstract>In e-commerce, opinion summarization is the process of summarizing the consensus opinions found in product reviews. However, the potential of additional sources such as product description and question-answers (QA) has been considered less often. Moreover, the absence of any supervised training data makes this task challenging. To address this, we propose a novel synthetic dataset creation (SDC) strategy that leverages information from reviews as well as additional sources for selecting one of the reviews as a pseudo-summary to enable supervised training. Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS) employs a separate encoder for each source, enabling effective selection of information while generating the summary. For evaluation, due to the unavailability of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to annotate summaries. Experiments across nine test sets demonstrate that the combination of our SDC approach and MEDOS model achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis underlines the significance of incorporating additional sources for generating more informative summaries. Human evaluations further indicate that MEDOS scores relatively higher in coherence and fluency with 0.41 and 0.5 (−1 to 1) respectively, compared to existing models. To the best of our knowledge, we are the first to generate opinion summaries leveraging additional sources in a self-supervised setting.</abstract>
      <url hash="bc59ec7b">2024.findings-naacl.150</url>
    </paper>
    <paper id="151">
      <title><fixed-case>COMEM</fixed-case>: In-Context Retrieval-Augmented Mass-Editing Memory in Large Language Models</title>
      <author><first>Shanbao</first><last>Qiao</last></author>
      <author><first>Xuebing</first><last>Liu</last></author>
      <author><first>Seung-Hoon</first><last>Na</last><affiliation>Chonbuk National University</affiliation></author>
      <pages>2333-2347</pages>
      <abstract>Noting that world knowledge continuously evolves over time, large language models (LLMs) need to be properly adjusted by performing the “knowledge editing”, which involves updating outdated information or correcting false information. To achieve reliable and “massive” editing capabilities in terms of <tex-math>\textit{generalization}</tex-math> and <tex-math>\textit{specificity}</tex-math>, this paper proposes a unified knowledge editing method called in-<tex-math>\textbf{CO}</tex-math>ntext retrieval-augmented <tex-math>\textbf{M}</tex-math>ass-<tex-math>\textbf{E}</tex-math>diting <tex-math>\textbf{M}</tex-math>emory (COMEM), which combines two types of editing approaches: parameter updating and in-context knowledge editing (IKE). In particular, COMEM incorporates <tex-math>\textit{retrieval-augmented IKE}</tex-math>, a novel extension of IKE designed for massive editing tasks, based on an <tex-math>\textit{updating}</tex-math>-aware demonstration construction.Experimental results on the zsRE and CounterFact datasets demonstrate that COMEM outperforms all existing methods, achieving state-of-the-art performance. Our code is available at https://github.com/JoveReCode/COMEM.git.</abstract>
      <url hash="7a9912fd">2024.findings-naacl.151</url>
    </paper>
    <paper id="152">
      <title>Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought</title>
      <author><first>Kohtaro</first><last>Tanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Kohei</first><last>Uehara</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Lin</first><last>Gu</last><affiliation>RIKEN</affiliation></author>
      <author><first>Yusuke</first><last>Mukuta</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Tatsuya</first><last>Harada</last><affiliation>RIKEN and The University of Tokyo</affiliation></author>
      <pages>2348-2367</pages>
      <abstract>Although automated image captioning methods have benefited considerably from the development of large language models (LLMs), generating humorous captions is still a challenging task. Humorous captions generated by humans are unique to the image and reflect the content of the image. However, captions generated using previous captioning models tend to be generic. Therefore, we propose incongruity-resolution chain-of-thought (IRCoT) as a novel prompting framework that creates content-specific resolutions from fine details extracted from an image. Furthermore, we integrate logit bias and negative sampling to suppress the output of generic resolutions. The results of experiments with GPT4-V demonstrate that our proposed framework effectively generated humorous captions tailored to the content of specific input images.</abstract>
      <url hash="5ad156a5">2024.findings-naacl.152</url>
    </paper>
    <paper id="153">
      <title>Denoising Attention for Query-aware User Modeling</title>
      <author><first>Elias</first><last>Bassani</last></author>
      <author><first>Pranav</first><last>Kasela</last><affiliation>University of Milan - Bicocca</affiliation></author>
      <author><first>Gabriella</first><last>Pasi</last><affiliation>University of Milan - Bicocca</affiliation></author>
      <pages>2368-2380</pages>
      <abstract>Personalization of search results has gained increasing attention in the past few years, also thanks to the development of Neural Networks-based approaches for Information Retrieval. Recent works have proposed to build user models at query time by leveraging the Attention mechanism, which allows weighing the contribution of the user-related information w.r.t. the current query.This approach allows giving more importance to the user’s interests related to the current search performed by the user.In this paper, we discuss some shortcomings of the Attention mechanism when employed for personalization and introduce a novel Attention variant, the Denoising Attention, to solve them.Denoising Attention adopts a robust normalization scheme and introduces a filtering mechanism to better discern among the user-related data those helpful for personalization.Experimental evaluation shows improvements in MAP, MRR, and NDCG above 15% w.r.t. other Attention variants at the state-of-the-art.</abstract>
      <url hash="99826170">2024.findings-naacl.153</url>
    </paper>
    <paper id="154">
      <title>A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy</title>
      <author><first>Fan</first><last>Zhang</last><affiliation>Communication University of China and Samsung</affiliation></author>
      <author><first>Mei</first><last>Tu</last></author>
      <author><first>Song</first><last>Liu</last></author>
      <author><first>Jinyao</first><last>Yan</last><affiliation>Communication University of China</affiliation></author>
      <pages>2381-2392</pages>
      <abstract>Dealing with language heterogeneity has always been one of the challenges in neural machine translation (NMT).The idea of using mixture-of-experts (MoE) naturally excels in addressing this issue by employing different experts to take responsibility for different problems.However, the parameter-inefficiency problem in MoE results in less performance improvement when boosting the number of parameters.Moreover, most of the MoE models are suffering from the training instability problem.This paper proposes MoA (Mixture-of-Adapters), a lightweight MoE-based NMT model that is trained via an elaborately designed stage-wise training strategy.With the standard Transformer as the backbone model, we introduce lightweight adapters as experts for easy expansion.To improve the parameter efficiency, we explicitly model and distill the language heterogeneity into the gating network with clustering.After freezing the gating network, we adopt the Gumbel-Max sampling as the routing scheme when training experts to balance the knowledge of generalization and specialization while preventing expert over-fitting.Empirical results show that MoA achieves stable improvements in different translation tasks by introducing much fewer extra parameters compared to other MoE baselines.Additionally, the performance evaluations on a multi-domain translation task illustrate the effectiveness of our training strategy.</abstract>
      <url hash="7a64dbd8">2024.findings-naacl.154</url>
    </paper>
    <paper id="155">
      <title><fixed-case>BEAR</fixed-case>: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</title>
      <author><first>Jacek</first><last>Wiland</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>2393-2411</pages>
      <abstract>Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to masked or causal LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM’s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.</abstract>
      <url hash="6fef6fbd">2024.findings-naacl.155</url>
    </paper>
    <paper id="156">
      <title>Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition</title>
      <author><first>Floris</first><last>Hengst</last></author>
      <author><first>Ralf</first><last>Wolter</last><affiliation>ING Bank</affiliation></author>
      <author><first>Patrick</first><last>Altmeyer</last></author>
      <author><first>Arda</first><last>Kaygan</last></author>
      <pages>2412-2432</pages>
      <abstract>We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems. The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level.By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection.In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection.CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions.</abstract>
      <url hash="f90c4950">2024.findings-naacl.156</url>
    </paper>
    <paper id="157">
      <title>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions</title>
      <author><first>Alex</first><last>Nyffenegger</last></author>
      <author><first>Matthias</first><last>Stürmer</last><affiliation>BFH - Bern University of Applied Sciences and Universität Bern</affiliation></author>
      <author><first>Joel</first><last>Niklaus</last><affiliation>University of Bern, Universität Bern</affiliation></author>
      <pages>2433-2462</pages>
      <abstract>Anonymity in court rulings is a critical aspect of privacy protection in the European Union and Switzerland but with the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland (FSCS), we study re-identification risks using actual legal data. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. In addition to the datasets, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. We demonstrate that for now, the risk of re-identifications using LLMs is minimal in the vast majority of cases. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading the courts to publish more decisions.</abstract>
      <url hash="131d71a7">2024.findings-naacl.157</url>
    </paper>
    <paper id="158">
      <title><fixed-case>X</fixed-case>-<fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>: Optimizing Bilingual Large Vision-Language Alignment</title>
      <author><first>DongJae</first><last>Shin</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>HyeonSeok</first><last>Lim</last><affiliation>Hanbat National University</affiliation></author>
      <author><first>Inho</first><last>Won</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>ChangSu</first><last>Choi</last></author>
      <author><first>Minjun</first><last>Kim</last></author>
      <author><first>SeungWoo</first><last>Song</last><affiliation>Hanbat National University</affiliation></author>
      <author><first>HanGyeol</first><last>Yoo</last></author>
      <author><first>SangMin</first><last>Kim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>KyungTae</first><last>Lim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <pages>2463-2473</pages>
      <abstract>The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.</abstract>
      <url hash="e182449a">2024.findings-naacl.158</url>
    </paper>
    <paper id="159">
      <title>Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise</title>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <author><first>Joyce</first><last>Whang</last><affiliation>KAIST</affiliation></author>
      <pages>2474-2495</pages>
      <abstract>Most existing retrieval-augmented language models (LMs) assume a naive dichotomy within a retrieved document set: query-relevance and irrelevance. Our work investigates a more challenging scenario in which even the “relevant” documents may contain misleading or incorrect information, causing conflict among the retrieved documents and thereby negatively influencing model decisions as noise. We observe that existing LMs are highly brittle to the presence of conflicting information in both the fine-tuning and in-context few-shot learning scenarios. We propose approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability. Our empirical results on open-domain QA show that these approaches significantly enhance model robustness. We also provide our findings on incorporating the fine-tuned discriminator’s decision into the in-context learning process, proposing a way to exploit the benefits of two disparate learning schemes. Alongside our findings, we provide MacNoise, a machine-generated, conflict-induced dataset to further encourage research in this direction.</abstract>
      <url hash="cc8b8227">2024.findings-naacl.159</url>
    </paper>
    <paper id="160">
      <title>Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake</title>
      <author><first>Orchid</first><last>Chetia Phukan</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Gautam</first><last>Kashyap</last></author>
      <author><first>Arun Balaji</first><last>Buduru</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Rajesh</first><last>Sharma</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <pages>2496-2506</pages>
      <abstract>In this work, we investigate multilingual speech Pre-Trained models (PTMs) for Audio deepfake detection (ADD). We hypothesize thatmultilingual PTMs trained on large-scale diverse multilingual data gain knowledge about diverse pitches, accents, and tones, during theirpre-training phase and making them more robust to variations. As a result, they will be more effective for detecting audio deepfakes. To validate our hypothesis, we extract representations from state-of-the-art (SOTA) PTMs including monolingual, multilingual as well as PTMs trained for speaker and emotion recognition, and evaluated them on ASVSpoof 2019 (ASV), In-the-Wild (ITW), and DECRO benchmark databases. We show that representations from multilingual PTMs, with simple downstream networks, attain the best performance for ADD compared to other PTM representations, which validates our hypothesis. We also explore the possibility of fusion of selected PTM representations for further improvements in ADD, and we propose a framework, MiO (Merge into One) for this purpose. With MiO, we achieve SOTA performance on ASV and ITW and comparable performance on DECRO with current SOTA works.</abstract>
      <url hash="8dc3a7b0">2024.findings-naacl.160</url>
    </paper>
    <paper id="161">
      <title>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</title>
      <author><first>Chenghao</first><last>Yang</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Karli</first><last>Hochstatter</last></author>
      <author><first>Melissa</first><last>Slavin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Nabila</first><last>El-Bassel</last><affiliation>Columbia University and Columbia University</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <pages>2507-2521</pages>
      <abstract>In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum.</abstract>
      <url hash="f6f2db23">2024.findings-naacl.161</url>
    </paper>
    <paper id="162">
      <title>Self-Adaptive Sampling for Accurate Video Question Answering on Image Text Models</title>
      <author><first>Wei</first><last>Han</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>2522-2534</pages>
      <abstract>Image–text models (ITMs) is the prevalent architecture to solve video question–answering tasks, which requires only a few input frames to save huge computational cost compared to video–language models.However, we find existent ITM video question–answering solutions either 1) adopt simplistic and unintentional sampling strategies, which may miss key frames to offer the answer clues; or 2) sample a large number of frames into divided groups, which the computational sources can not accommodate. In this work, we aim at an efficient sampling method towards the few-frame situations.We first summarize a family of prior sampling methods based on question–frame correlation into a unified one, dubbed *Most Implied Frames* (MIF). Through some primary results and analysis, Through analysis, we form a hypothesis that question-aware sampling is not necessary, from which we further propose the other method *Most Dominant Frames* (MDF).Experimental results on four public datasets and three advanced ITMs demonstrate that our proposed strategies can boost the performance for image–text pretrained models, and have a wide application scenario in terms of model architectures and dataset types. Our code is available at https://github.com/declare-lab/Sealing<url>https://github.com/declare-lab/Sealing</url>.</abstract>
      <url hash="346ad62e">2024.findings-naacl.162</url>
    </paper>
    <paper id="163">
      <title>Towards an On-device Agent for Text Rewriting</title>
      <author><first>Yun</first><last>Zhu</last></author>
      <author><first>Yinxiao</first><last>Liu</last></author>
      <author><first>Felix</first><last>Stahlberg</last><affiliation>Google</affiliation></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <author><first>Yu-Hui</first><last>Chen</last></author>
      <author><first>Liangchen</first><last>Luo</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Shu</last><affiliation>Google</affiliation></author>
      <author><first>Renjie</first><last>Liu</last></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Meng</last></author>
      <pages>2535-2552</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. However creating a smaller yet potent language model for text rewriting presents two formidable challenges: costly data collection and absence of emergent capabilities.In this paper we present solutions to address the above challenges.We propose an new instruction tuning method to develop a mo-bile text rewriting model that leverages LLM-generated data and heuristic reinforcement learning, eliminating the need for human data collection. Moreover, to bridge the performance gap from the constraint size, we pro-pose a cascading approach based on the confidence levels which are distilled from the large server model’s critiques. To evaluate the text rewriting tasks for mobile scenarios, we introduce MessageRewriteEval, a human-labeled benchmark that focuses on text rewriting of messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size using public benchmark EditEval and our new benchmark. We also demonstrate that our proposed cascading approach improves model performance further.</abstract>
      <url hash="a2b0df4f">2024.findings-naacl.163</url>
    </paper>
    <paper id="164">
      <title>Tailoring Vaccine Messaging with Common-Ground Opinions</title>
      <author><first>Rickard</first><last>Stureborg</last><affiliation>Duke University</affiliation></author>
      <author><first>Sanxing</first><last>Chen</last><affiliation>Duke University</affiliation></author>
      <author><first>Roy</first><last>Xie</last></author>
      <author><first>Aayushi</first><last>Patel</last></author>
      <author><first>Christopher</first><last>Li</last></author>
      <author><first>Chloe</first><last>Zhu</last></author>
      <author><first>Tingnan</first><last>Hu</last></author>
      <author><first>Jun</first><last>Yang</last><affiliation>Department of Computer Science, Duke University</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>2553-2575</pages>
      <abstract>One way to personalize chatbot interactions is by establishing common ground with the intended reader. A domain where establishing mutual understanding could be particularly impactful is vaccine concerns and misinformation. Vaccine interventions are forms of messaging which aim to answer concerns expressed about vaccination. Tailoring responses in this domain is difficult, since opinions often have seemingly little ideological overlap. We define the task of tailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring responses to a CGO involves meaningfully improving the answer by relating it to an opinion or belief the reader holds. In this paper we introduce Tailor-CGO, a dataset for evaluating how well responses are tailored to provided CGOs. We benchmark several major LLMs on this task; finding GPT-4-Turbo performs significantly better than others. We also build automatic evaluation metrics, including an efficient and accurate BERT model that outperforms finetuned LLMs, investigate how to successfully tailor vaccine messaging to CGOs, and provide actionable recommendations from this investigation.Tailor-CGO dataset and code available at: https://github.com/rickardstureborg/tailor-cgo</abstract>
      <url hash="2652e2dc">2024.findings-naacl.164</url>
    </paper>
    <paper id="165">
      <title>Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification</title>
      <author><first>Robert</first><last>Vacareanu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Fahmida</first><last>Alam</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Md Asiful</first><last>Islam</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Haris</first><last>Riaz</last></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>2576-2594</pages>
      <abstract>This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching.Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data.Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher.In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data.Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation org:parents boost the performance on that relation by as much as 26% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.</abstract>
      <url hash="629f42d4">2024.findings-naacl.165</url>
    </paper>
    <paper id="166">
      <title><fixed-case>Q</fixed-case>-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning</title>
      <author><first>Yanhui</first><last>Guo</last></author>
      <author><first>Shaoyuan</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinmiao</first><last>Fu</last></author>
      <author><first>Jia</first><last>Liu</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Chaosheng</first><last>Dong</last></author>
      <author><first>Bryan</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <pages>2595-2622</pages>
      <abstract>This paper introduces Q-tuning, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model. When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks. To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix. Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue’s size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks. In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks. Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference.</abstract>
      <url hash="2c6bc010">2024.findings-naacl.166</url>
    </paper>
    <paper id="167">
      <title>In-Context Example Ordering Guided by Label Distributions</title>
      <author><first>Zhichao</first><last>Xu</last><affiliation>University of Utah</affiliation></author>
      <author><first>Daniel</first><last>Cohen</last><affiliation>Brown University</affiliation></author>
      <author><first>Bei</first><last>Wang</last><affiliation>University of Utah</affiliation></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <pages>2623-2640</pages>
      <abstract>By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary from near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model’s probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples.</abstract>
      <url hash="98c894be">2024.findings-naacl.167</url>
    </paper>
    <paper id="168">
      <title>Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives</title>
      <author><first>Jiaxin</first><last>Liu</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kar Yan</first><last>Tam</last></author>
      <pages>2641-2652</pages>
      <abstract>In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company’s financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.</abstract>
      <url hash="90dd624b">2024.findings-naacl.168</url>
    </paper>
    <paper id="169">
      <title>Laying Anchors: Semantically Priming Numerals in Language Modeling</title>
      <author><first>Mandar</first><last>Sharma</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Rutuja</first><last>Taware</last></author>
      <author><first>Pravesh</first><last>Koirala</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Nikhil</first><last>Muralidhar</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Naren</first><last>Ramakrishnan</last><affiliation>Virginia Tech</affiliation></author>
      <pages>2653-2660</pages>
      <abstract>Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.</abstract>
      <url hash="f9ce253e">2024.findings-naacl.169</url>
    </paper>
    <paper id="170">
      <title><fixed-case>UEGP</fixed-case>: Unified Expert-Guided Pre-training for Knowledge Rekindle</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Kexiang</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jianhe</first><last>Lin</last></author>
      <author><first>Dehong</first><last>Ma</last></author>
      <author><first>Jun</first><last>Fan</last></author>
      <author><first>Daiting</first><last>Shi</last></author>
      <author><first>Zhicong</first><last>Cheng</last></author>
      <author><first>Gu</first><last>Simiu</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>2661-2673</pages>
      <abstract>Pre-training and fine-tuning framework has become the standard training paradigm for NLP tasks and is also widely used in industrial-level applications. However, there are still a limitation with this paradigm: simply fine-tuning with task-specific objectives tends to converge to local minima, resulting in a sub-optimal performance. In this paper, we first propose a new paradigm: knowledge rekindle, which aims to re-incorporate the fine-tuned expert model into the training cycle and break through the performance upper bounds of experts without introducing additional annotated data. Then we further propose a unified expert-guided pre-training (UEGP) framework for knowledge rekindle. Specifically, we reuse fine-tuned expert models for various downstream tasks as knowledge sources and inject task-specific prior knowledge to pre-trained language models (PLMs) by means of knowledge distillation. In this process, we perform multi-task learning with knowledge distillation and masked language modeling (MLM) objectives. We also further explored whether mixture-of-expert guided pre-training (MoEGP) can further enhance the effect of knowledge rekindle. Experiments and analysis on eight datasets in GLUE benchmark and a industrial-level search re-ranking dataset show the effectiveness of our method.</abstract>
      <url hash="1b4cc956">2024.findings-naacl.170</url>
    </paper>
    <paper id="171">
      <title><fixed-case>L</fixed-case>attice<fixed-case>G</fixed-case>en: Hiding Generated Text in a Lattice for Privacy-Aware Large Language Model Generation on Cloud</title>
      <author><first>Mengke</first><last>Zhang</last></author>
      <author><first>Tianxing</first><last>He</last></author>
      <author><first>Tianle</first><last>Wang</last></author>
      <author><first>Lu</first><last>Mi</last><affiliation>University of Washington and Allen Institute</affiliation></author>
      <author><first>Niloofar</first><last>Mireshghallah</last><affiliation>University of Washington</affiliation></author>
      <author><first>Binyi</first><last>Chen</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>2674-2690</pages>
      <abstract>In the current user-server interaction paradigm of prompted generation with large language models (LLMs) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text private to themselves. For privacy-aware text generation on cloud, we propose LatticeGen, a cooperative protocol in which the server still handles most of the computation while the client controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the client and hidden in a noised lattice. Only the client knows which tokens are the true ones. Considering potential attacks from a hypothetically malicious server and how the client can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by BERTScore).</abstract>
      <url hash="26d5ce6c">2024.findings-naacl.171</url>
    </paper>
    <paper id="172">
      <title><fixed-case>H</fixed-case>ate<fixed-case>M</fixed-case>oderate: Testing Hate Speech Detectors against Content Moderation Policies</title>
      <author><first>Jiangrui</first><last>Zheng</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Xueqing</first><last>Liu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Mirazul</first><last>Haque</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Xing</first><last>Qian</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Guanqun</first><last>Yang</last></author>
      <author><first>Wei</first><last>Yang</last><affiliation>University of Texas, Dallas</affiliation></author>
      <pages>2691-2710</pages>
      <abstract>To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: Do automated hate speech detectors conform to social media content policies? A platform’s content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question. This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook’s 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these models have in their conformity to the policies. Third, using HateModerate, we augment the training data of a top-downloaded hate detector on HuggingFace. We observe significant improvement in the models’ conformity to content policies while having comparable scores on the original test data. Our dataset and code can be found on https://github.com/stevens-textmining/HateModerate.</abstract>
      <url hash="c8b3ba8f">2024.findings-naacl.172</url>
    </paper>
    <paper id="173">
      <title>Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other</title>
      <author><first>Yifei</first><last>Gao</last></author>
      <author><first>Jie</first><last>Ou</last></author>
      <author><first>Lei</first><last>Wang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuting</first><last>Xiao</last></author>
      <author><first>Xiangzhiyuan</first><last>Xiangzhiyuan</last></author>
      <author><first>Ruiting</first><last>Dai</last></author>
      <author><first>Jun</first><last>Cheng</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>2711-2722</pages>
      <abstract>Emergent Large Language Models (LLMs) use their extraordinary performance and powerful deduction capacity to discern from traditional language models. However, the expenses of computational resources and storage for these LLMs are stunning, quantization then arises as a trending conversation. To address accuracy decay caused by quantization, two streams of works in post-training quantization methods stand out. One uses other weights to compensate existing quantization error, while the other transfers the quantization difficulty to other parts in the model. Combining both merits, we introduce Learnable Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value Decomposition to extract singular values of the weights and make them learnable to help weights compensate each other conditioned on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art performance in diverse quantization settings, no matter in weight-only, weight-activation or extremely low bit scenarios. By unleashing the potential of LSI, efficient finetuning on quantized model is no longer a prohibitive problem.</abstract>
      <url hash="aacf397c">2024.findings-naacl.173</url>
    </paper>
    <paper id="174">
      <title>Contrastive Preference Learning for Neural Machine Translation</title>
      <author><first>Jianfei</first><last>He</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Shichao</first><last>Sun</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Sen</first><last>Peng</last></author>
      <author><first>Jie</first><last>Xu</last></author>
      <author><first>Xiaohua</first><last>Jia</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>2723-2734</pages>
      <abstract>There exists a discrepancy between the token-level objective during training and the overall sequence-level quality that is expected from the model. This discrepancy leads to issues like exposure bias.To align the model with human expectations, sequence-level objectives are often used to fine-tune pre-trained models.In this paper, we introduce a contrastive preference model that enhances the traditional Plackett-Luce model by incorporating an indicator function. Building upon this novel preference model, we propose Contrastive Preference Learning (CPL), which uses offline samples with list-wise preferences to fine-tune a pre-trained model in Neural Machine Translation. Our experiments, conducted on three language pairs, demonstrate that CPL outperforms not only the vanilla Transformer model but also other token-level and sequence-level baselines. Furthermore, the ablation study highlights the essential role of the proposed indicator function in achieving this improvement.</abstract>
      <url hash="2edae52f">2024.findings-naacl.174</url>
    </paper>
    <paper id="175">
      <title><fixed-case>S</fixed-case>oc<fixed-case>RE</fixed-case>val: Large Language Models with the Socratic Method for Reference-free Reasoning Evaluation</title>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>2735-2763</pages>
      <abstract>To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such “gold-standard” human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval (**Soc**ratic Method-Inspired **R**easoning **Eval**uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4’s performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.</abstract>
      <url hash="662b61e1">2024.findings-naacl.175</url>
    </paper>
    <paper id="176">
      <title>Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis</title>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hongyi</first><last>Liu</last></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>2764-2780</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs’ performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.</abstract>
      <url hash="2acfc824">2024.findings-naacl.176</url>
    </paper>
    <paper id="177">
      <title>Unleashing the Power of <fixed-case>LLM</fixed-case>s in Court View Generation by Stimulating Internal Knowledge and Incorporating External Knowledge</title>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Yating</first><last>Zhang</last></author>
      <author><first>Changlong</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>2781-2791</pages>
      <abstract>Court View Generation (CVG) plays a vital role in the realm of legal artificial intelligence, which aims to support judges in crafting legal judgment documents. The court view consists of three essential judgment parts: the charge-related, law article-related, and prison term-related parts, each requiring specialized legal knowledge, rendering CVG a challenging task.Although Large Language Models (LLMs) have made remarkable strides in language generation, they encounter difficulties in the knowledge-intensive legal domain.Actually, there can be two types of knowledge: internal knowledge stored within LLMs’ parameters and external knowledge sourced from legal documents outside the models.In this paper, we decompose court views into different parts, stimulate internal knowledge, and incorporate external information to unleash the power of LLMs in the CVG task.To validate our method, we conduct a series of experiment results on two real-world datasets LAIC2021 and CJO2022. The experiments demonstrate that our method is capable of generating more accurate and reliable court views.</abstract>
      <url hash="906c44b4">2024.findings-naacl.177</url>
    </paper>
    <paper id="178">
      <title>Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions</title>
      <author><first>Danfeng</first><last>Guo</last></author>
      <author><first>Sanchit</first><last>Agarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Arpit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiun-Yu</first><last>Kao</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Emre</first><last>Barut</last><affiliation>Amazon</affiliation></author>
      <author><first>Tagyoung</first><last>Chung</last><affiliation>Amazon</affiliation></author>
      <author><first>Jing</first><last>Huang</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>2792-2806</pages>
      <abstract>Referring Expression Generation (REG) is the task of generating a description that unambiguously identifies a given target in the scene. Different from Image Captioning (IC), REG requires learning fine-grained characteristics of not only the scene objects but also their surrounding context. Referring expressions are usually not singular; an object can often be uniquely referenced in numerous ways, for instance, by color, by location, or by relationship with other objects. Most prior works, however, have not explored this ‘aspect-based multiplicity’ of referring expressions. Hence, in this work, we focus on the Aspect-Controlled REG task, which requires generating a referring expression conditioned on the input aspect(s), where an aspect captures a style of reference. By changing the input aspect such as color, location, action etc., one can generate multiple distinct expressions per target region. To solve this new task, we first modify BLIP for aligning image-regions and text-expressions. We achieve this through a novel approach for feeding the input by drawing a bounding box around the target image-region and prompting the model to generate the referring expression. Our base REG model already beats all prior works in CIDEr score. To tackle Aspect-Controlled REG, we append ‘aspect tokens’ to the prompt and show that distinct expressions can be generated by just changing the prompt. Finally, to prove the high-quality and diversity of the data generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based evaluation on the downstream Referring Expression Comprehension (REC) task. With just half of the real data augmented with the generated synthetic data, we achieve performance comparable to training with 100% of real data, using a SOTA REC model.</abstract>
      <url hash="dc91b4c1">2024.findings-naacl.178</url>
    </paper>
    <paper id="179">
      <title>Task-Agnostic Detector for Insertion-Based Backdoor Attacks</title>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Xiao</first><last>Lin</last><affiliation>SRI International</affiliation></author>
      <author><first>Songzhu</first><last>Zheng</last><affiliation>Morgan Stanley</affiliation></author>
      <author><first>Lu</first><last>Pang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Haibin</first><last>Ling</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Susmit</first><last>Jha</last><affiliation>SRI International</affiliation></author>
      <author><first>Chao</first><last>Chen</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>2807-2821</pages>
      <abstract>Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like question answering and named entity recognition. We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.</abstract>
      <url hash="21f74631">2024.findings-naacl.179</url>
    </paper>
    <paper id="180">
      <title>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</title>
      <author><first>Jianfeng</first><last>He</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Linlin</first><last>Yu</last></author>
      <author><first>Shuo</first><last>Lei</last><affiliation>Sony Coporation of America</affiliation></author>
      <author><first>Chang-Tien</first><last>Lu</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Feng</first><last>Chen</last><affiliation>University of Texas, Dallas</affiliation></author>
      <pages>2822-2834</pages>
      <abstract>Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on three datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset. Our code is available at <url>https://github.com/he159ok/UncSeqLabeling_SLPN</url>.</abstract>
      <url hash="0a1a820e">2024.findings-naacl.180</url>
    </paper>
    <paper id="181">
      <title>Exploring Language Model’s Code Generation Ability with Auxiliary Functions</title>
      <author><first>Seonghyeon</first><last>Lee</last></author>
      <author><first>Sanghwan</first><last>Jang</last><affiliation>POSTECH</affiliation></author>
      <author><first>Seongbo</first><last>Jang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hwanjo</first><last>Yu</last><affiliation>POSTECH</affiliation></author>
      <pages>2835-2847</pages>
      <abstract>Auxiliary function is a helpful component to improve language model’s code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other.With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models’ various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models’ promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model’s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.</abstract>
      <url hash="6ffcc6a6">2024.findings-naacl.181</url>
    </paper>
    <paper id="182">
      <title>Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of <fixed-case>V</fixed-case>ietnamese Large Language Models</title>
      <author><first>Sang</first><last>Truong</last></author>
      <author><first>Duc</first><last>Nguyen</last></author>
      <author><first>Toan</first><last>Nguyen</last></author>
      <author><first>Dong</first><last>Le</last></author>
      <author><first>Nhi</first><last>Truong</last></author>
      <author><first>Tho</first><last>Quan</last></author>
      <author><first>Sanmi</first><last>Koyejo</last><affiliation>Stanford University and Google</affiliation></author>
      <pages>2848-2899</pages>
      <abstract>Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 tasks and 31 metrics. We observe that finetuning can help LLMs transfer knowledge across languages, serving as an efficient way to bolster their capabilities in non-English languages. Moreover, our analysis indicates that larger models can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or finetuning datasets. These insights underscore the significance of meticulous finetuning with high-quality datasets in enhancing LLM performance.</abstract>
      <url hash="f756745d">2024.findings-naacl.182</url>
    </paper>
    <paper id="183">
      <title><fixed-case>G</fixed-case>o<fixed-case>T</fixed-case>: Effective Graph-of-Thought Reasoning in Language Models</title>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2900-2920</pages>
      <abstract>With the widespread use of language models (LMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. GoT adopts a two-stage framework with an additional GoT encoder for thought graph representation and fuses the graph representation with the original input representation through a gated fusion mechanism. We evaluate GoT’s performance on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59% using the T5-base model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Our code is publicly available at https://github.com/Zoeyyao27/Graph-of-Thought</abstract>
      <url hash="e0d89775">2024.findings-naacl.183</url>
    </paper>
    <paper id="184">
      <title>Enhancing the General Agent Capabilities of Low-Paramter <fixed-case>LLM</fixed-case>s through Tuning and Multi-Branch Reasoning</title>
      <author><first>Qinhao</first><last>Zhou</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Xiang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>2921-2930</pages>
      <abstract>Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.</abstract>
      <url hash="35fda340">2024.findings-naacl.184</url>
    </paper>
    <paper id="185">
      <title><fixed-case>M</fixed-case>u<fixed-case>M</fixed-case>ath: Multi-perspective Data Augmentation for Mathematical Reasoning in Large Language Models</title>
      <author><first>Weihao</first><last>You</last></author>
      <author><first>Shuo</first><last>Yin</last></author>
      <author><first>Xudong</first><last>Zhao</last></author>
      <author><first>Zhilong</first><last>Ji</last><affiliation>Tomorrow Advancing Life</affiliation></author>
      <author><first>Guoqiang</first><last>Zhong</last><affiliation>Ocean University of China</affiliation></author>
      <author><first>Jinfeng</first><last>Bai</last></author>
      <pages>2931-2957</pages>
      <abstract>Recently, the tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs. However, these models fall short in demonstrating the calculation process, which compromises user-friendliness and understanding of problem-solving steps. Conversely, while tool-free methods offer a clear display of the problem-solving process, their accuracy leaves room for improvement.These tool-free methods typically employ a somewhat narrow range of augmentation techniques such as rephrasing and difficulty enhancement to boost performance. In response to this issue, we have amalgamated and further refined these strengths while broadening the scope of augmentation methods to construct a **mu**lti-perspective augmentation dataset for **math**ematics—termed **MuMath** (<tex-math>\mu</tex-math>-Math) Dataset.Subsequently, we finetune LLaMA-2 on the MuMath dataset to derive the MuMath model. Our experiments indicate that our MuMath-70B model achieves new state-of-the-art performance among tool-free methods—achieving 88.3% on GSM8K and 34.5% on MATH .We release the MuMath dataset along with its corresponding models and code for public use.</abstract>
      <url hash="56b69572">2024.findings-naacl.185</url>
    </paper>
    <paper id="186">
      <title>Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization</title>
      <author><first>Tong</first><last>Ye</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lingfei</first><last>Wu</last><affiliation>Anytime AI and Pinterest</affiliation></author>
      <author><first>Tengfei</first><last>Ma</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yangkai</first><last>Du</last></author>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Shouling</first><last>Ji</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wenhai</first><last>Wang</last></author>
      <pages>2958-2970</pages>
      <abstract>Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although neural language models achieve significant performance in this field, they are limited by their inability to access external knowledge. To address this limitation, an emerging trend is combining neural models with external knowledge through retrieval methods. Previous methods have relied on the sentence-level retrieval paradigm on the encoder side. However, this paradigm is coarse-grained, noise-filled and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we propose a fine-grained Token-level retrieval-augmented mechanism (Tram) on the decoder side rather than the encoder side to enhance the performance of neural models and produce more low-frequency tokens in generating summaries. Furthermore, to overcome the challenge of token-level retrieval in capturing contextual code semantics, we also propose integrating code semantics into individual summary tokens. The results of extensive experiments and human evaluation show that our token-level retrieval-augmented approach significantly improves performance and is more interpretable.</abstract>
      <url hash="17724228">2024.findings-naacl.186</url>
    </paper>
    <paper id="187">
      <title><fixed-case>UNO</fixed-case>-<fixed-case>DST</fixed-case>: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</title>
      <author><first>Chuang</first><last>Li</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>2971-2982</pages>
      <abstract>Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain.We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method’s effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.</abstract>
      <url hash="66f20a98">2024.findings-naacl.187</url>
    </paper>
    <paper id="188">
      <title>Evaluating Step-by-Step Reasoning through Symbolic Verification</title>
      <author><first>YiFan</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Hanlin</first><last>Zhang</last><affiliation>Harvard University</affiliation></author>
      <author><first>Li</first><last>Li</last><affiliation>Amazon and Columbia University</affiliation></author>
      <author><first>Eric</first><last>Xing</last><affiliation>Mohamed bin Zayed Univeristy of AI and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>2983-3001</pages>
      <abstract>Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog’s backward chaining algorithm and supporting automated verification of LMs’ outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than 25% higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.</abstract>
      <url hash="9167e296">2024.findings-naacl.188</url>
    </paper>
    <paper id="189">
      <title>Multi-Review Fusion-in-Context</title>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Ori</first><last>Shapira</last><affiliation>Amazon</affiliation></author>
      <author><first>Ran</first><last>Levy</last><affiliation>Amazon</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>3002-3020</pages>
      <abstract>Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness.Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize Fusion-in-Context (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information.Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses.This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content. Our benchmark, FuseReviews, including the dataset, evaluation framework, and designated leaderboard, can be found at https://fusereviews.github.io/.</abstract>
      <url hash="e41162ce">2024.findings-naacl.189</url>
    </paper>
    <paper id="190">
      <title>Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison</title>
      <author><first>Maxime</first><last>Bouthors</last></author>
      <author><first>Josep</first><last>Crego</last><affiliation>SYSTRAN</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)</affiliation></author>
      <pages>3021-3038</pages>
      <abstract>Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures to better understand the interplay between these two processes.We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.</abstract>
      <url hash="6a6f7f71">2024.findings-naacl.190</url>
    </paper>
    <paper id="191">
      <title>Extending Input Contexts of Language Models through Training on Segmented Sequences</title>
      <author><first>Petros</first><last>Karypis</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>3039-3051</pages>
      <abstract>Effectively training language models on longinputs poses many technical challenges. As acost consideration, languages models are pre-trained on a fixed sequence length before beingadapted to longer sequences. We explore var-ious methods for adapting models to longerinputs by training on segmented sequences andan interpolation-based method for extendingabsolute positional embeddings. We developa training procedure to extend the input con-text size of pretrained models with no architec-tural changes and no additional memory coststhan training on the original input lengths. Bysub-sampling segments from long inputs whilemaintaining their original position the model isable to learn new positional interactions. Ourmethod benefits both models trained with abso-lute positional embeddings, by extending theirinput contexts, as well as popular relative posi-tional embedding methods showing a reducedperplexity on sequences longer than they weretrained on. We demonstrate our method canextend input contexts by a factor of 4× whileimproving perplexity.</abstract>
      <url hash="764fed88">2024.findings-naacl.191</url>
    </paper>
    <paper id="192">
      <title>Reason from Fallacy: Enhancing Large Language Models’ Logical Reasoning through Logical Fallacy Understanding</title>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Dixuan</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Guochao</first><last>Jiang</last></author>
      <author><first>Qianyu</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>3052-3065</pages>
      <abstract>Large Language Models (LLMs) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for LLMs’ suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate LLMs’ capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we have successfully constructed a new dataset LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments justify that our LFUD can be used not only to evaluate LLMs’ LFU capability, but also to fine-tune LLMs to obtain significantly enhanced performance on logical reasoning.</abstract>
      <url hash="a856e936">2024.findings-naacl.192</url>
    </paper>
    <paper id="193">
      <title>Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models</title>
      <author><first>Wanyong</first><last>Feng</last></author>
      <author><first>Jaewook</first><last>Lee</last></author>
      <author><first>Hunter</first><last>McNichols</last></author>
      <author><first>Alexander</first><last>Scarlatos</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Digory</first><last>Smith</last><affiliation>Eedi</affiliation></author>
      <author><first>Simon</first><last>Woodhead</last><affiliation>Eedi</affiliation></author>
      <author><first>Nancy</first><last>Ornelas</last></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>3066-3081</pages>
      <abstract>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.</abstract>
      <url hash="7d53214a">2024.findings-naacl.193</url>
    </paper>
    <paper id="194">
      <title>Aspect-based Sentiment Analysis with Context Denoising</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yongdong</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3082-3094</pages>
      <abstract>Given a sentence and a particular aspect term, aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity towards this aspect term, which provides fine-grained analysis on sentiment understanding and it has attracted much attention in recent years. In order to achieve a good performance on ABSA, it is important for a model to appropriately encode contextual information, especially identifying salient features and eliminating noise in the context. To make incorrect predictions, most existing approaches employ powerful text encoders to locate important context features, as well as noises that mislead ABSA models. These approaches determine the noise in the text for ABSA by assigning low weights to context features or directly removing them from model input, which runs the risk of computing wrong weights or eliminating important context information. In this paper, we propose to improve ABSA with context denoising, where three types of word-level information are regarded as noise, namely, lexicographic noise, bag-of-words noise, and syntax noise. We utilize diffusion networks to perform the denoising process to gradually eliminate them so as to better predict sentiment polarities for given aspect terms. Our approach uses task-specific noise rather than the standard stochastic Gaussian noise in the diffusion networks. The experimental results on five widely used ABSA datasets demonstrate the validity and effectiveness of our approach.</abstract>
      <url hash="fb6696d1">2024.findings-naacl.194</url>
    </paper>
    <paper id="195">
      <title><fixed-case>I</fixed-case>ru<fixed-case>M</fixed-case>ozhi: Automatically classifying diglossia in <fixed-case>T</fixed-case>amil</title>
      <author><first>Kabilan</first><last>Prasanna</last></author>
      <author><first>Aryaman</first><last>Arora</last></author>
      <pages>3095-3102</pages>
      <abstract>Tamil, a Dravidian language of South Asia, is a highly diglossic language with two very different registers in everyday use: Literary Tamil (preferred in writing and formal communication) and Spoken Tamil (confined to speech and informal media). Spoken Tamil is under-studied in modern NLP systems compared to Literary Tamil written in the Tamil script, as evidenced by a lack of datasets explicitly targetting the Spoken variety. In this paper, we release IruMozhi, a human-translated dataset of parallel text in Literary and Spoken Tamil. Using IruMozhi, we train classifiers on the task of identifying which Tamil variety a text belongs to. We use these models to gauge the availability of pretraining data in Spoken Tamil, to audit the composition of existing labelled datasets for Tamil, and to encourage future work on the variety.</abstract>
      <url hash="1ae3849d">2024.findings-naacl.195</url>
    </paper>
    <paper id="196">
      <title><fixed-case>RENOVI</fixed-case>: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations</title>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Xiaoxi</first><last>Kang</last></author>
      <author><first>Tao</first><last>Feng</last><affiliation>Monash University</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Yi Ying</first><last/><affiliation>Binus University</affiliation></author>
      <author><first>Mei Rianto</first><last>Chandra</last><affiliation>Binus University</affiliation></author>
      <author><first>Kelly</first><last>Rosalin</last></author>
      <author><first>Jureynolds</first><last>Jureynolds</last><affiliation>Binus University</affiliation></author>
      <author><first>Suraj</first><last>Sharma</last></author>
      <author><first>Shilin</first><last>Qu</last></author>
      <author><first>Linhao</first><last>Luo</last></author>
      <author><first>Ingrid</first><last>Zukerman</last><affiliation>Monash University</affiliation></author>
      <author><first>Lay-Ki</first><last>Soon</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhaleh</first><last>Semnani Azad</last><affiliation>California State University, Northridge</affiliation></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>3103-3116</pages>
      <abstract>Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi — a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT through prompt learning. While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between LLMs and humans in the awareness of social norms. We thus harness the power of ChatGPT to generate synthetic training data for our task. To ensure the quality of both human-authored and synthetic data, we follow a quality control protocol during data collection. Our experimental results demonstrate the importance of remediating norm violations in socio-cultural conversations, as well as the improvement in performance obtained from synthetic data.</abstract>
      <url hash="7152d537">2024.findings-naacl.196</url>
    </paper>
    <paper id="197">
      <title>Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking</title>
      <author><first>Hong Jin</first><last>Kang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Muhammad Ali</first><last>Gulzar</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Miryung</first><last>Kim</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>3117-3128</pages>
      <abstract>Data augmentation techniques apply transformations to existing texts to generate additional data. The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension. Analyzing the synthetically generated texts and their corresponding labels is slow and demanding. To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling. INSPECTOR allows users to group related texts by their <tex-math>\textit{transformation provenance}</tex-math>, i.e., the transformations applied to the original text, or <tex-math>\textit{feature provenance}</tex-math>, the linguistic features of the original text. For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model. In a user study, INSPECTOR increases the number of texts with correct labels identified by <tex-math>3\times</tex-math> on a sentiment analysis task and by <tex-math>4\times</tex-math> on a hate speech detection task. The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique. Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful. Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort. This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort.</abstract>
      <url hash="88548a77">2024.findings-naacl.197</url>
    </paper>
    <paper id="198">
      <title><fixed-case>COMMIT</fixed-case>: Code-Mixing <fixed-case>E</fixed-case>nglish-Centric Large Language Model for Multilingual Instruction Tuning</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>YeonJoon</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>3129-3136</pages>
      <abstract>Recently, instruction-tuned large language models (LLMs) are showing prominent performance on various tasks, such as question answering. However, the majority of instruction-tuned LLMs are English-centric, which hinders their application to low-resource language QA. In this paper, we propose COde-Mixed Multilingual Instruction Tuning (COMMIT) to adapt English-centric LLM to low-resource language QA. We point out two main causes of English-centricness: imbalance of unlabeled data, and English-centric instruction tuning datasets. To deviate from English-centric instruction tuning, we propose to specialize code-mixing for instruction tuning, which blocks code-mixing in English templates, to leverage the potential of its superiority. To overcome data imbalance, we perform cross-lingual alignment. The majority of cross-lingual alignment works focused on making representations similar, which is not desirable to decoder-based LLMs, such as LLaMA. Therefore, we propose code-mixed continual causal language modeling to align the decoder. COMMIT improves the exact match score of low-resourced language QA by up to 32x. Code is publicly available.</abstract>
      <url hash="8b3eeec8">2024.findings-naacl.198</url>
    </paper>
    <paper id="199">
      <title><fixed-case>D</fixed-case>i<fixed-case>LM</fixed-case>: Distilling Dataset into Language Model for Text-level Dataset Distillation</title>
      <author><first>Aru</first><last>Maekawa</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Satoshi</first><last>Kosugi</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Institute of Innovative Research, Tokyo Institute of Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>3137-3152</pages>
      <abstract>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.</abstract>
      <url hash="9c4c0073">2024.findings-naacl.199</url>
    </paper>
    <paper id="200">
      <title><fixed-case>M</fixed-case>ind<fixed-case>A</fixed-case>gent: Emergent Gaming Interaction</title>
      <author><first>Ran</first><last>Gong</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Qiuyuan</first><last>Huang</last><affiliation>Microsoft Research, Redmond</affiliation></author>
      <author><first>Xiaojian</first><last>Ma</last></author>
      <author><first>Yusuke</first><last>Noda</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zane</first><last>Durante</last></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Demetri</first><last>Terzopoulos</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Li</first><last>Fei-Fei</last><affiliation>Stanford University and Stanford University</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Hoi</first><last>Vo</last></author>
      <pages>3153-3182</pages>
      <abstract>Large Foundation Models (LFMs) can perform complex scheduling in a multi-agent system and can coordinate agents to complete sophisticated tasks that require extensive collaboration.However, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing collaboration between LFMs and human-NPCs. We propose a novel infrastructure—Mindagent—for evaluating planning and coordination capabilities in the context of gaming interaction. In particular, our infrastructure leverages an existing gaming framework to (i) act as the coordinator for a multi-agent system, (ii) collaborate with human players via instructions, and (iii) enable in-context learning based on few-shot prompting with feedback.Furthermore, we introduce “Cuisineworld”, a new gaming scenario and its related benchmark that supervises multiple agents playing the game simultaneously and measures multi-agent collaboration efficiency. We have conducted comprehensive evaluations with a new auto-metric Collaboration Score: CoS for assessing the collaboration efficiency. Finally, Mindagent can be deployed in real-world gaming scenarios in a customized VR version of Cuisineworld and adapted in the “Minecraft” domain. Our work involving LFMs within our new infrastructure for general-purpose scheduling and coordination can elucidate how such skills may be obtained by learning from large language corpora.</abstract>
      <url hash="2bb2c7f0">2024.findings-naacl.200</url>
    </paper>
    <paper id="201">
      <title><fixed-case>B</fixed-case>ot<fixed-case>C</fixed-case>hat: Evaluating <fixed-case>LLM</fixed-case>s’ Capabilities of Having Multi-Turn Dialogues</title>
      <author><first>Haodong</first><last>Duan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jueqi</first><last>Wei</last></author>
      <author><first>Chonghua</first><last>Wang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hongwei</first><last>Liu</last></author>
      <author><first>Yixiao</first><last>Fang</last><affiliation>Tencent</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>3183-3199</pages>
      <abstract>In the realm of modern Large Language Models (LLMs), facilitating high-quality, multi-turn dialogues with humans represents a cornerstone feature. However, human-based evaluation of such a capability involves substantial manual effort. This study offers a formative assessment of current LLMs’ proficiency in emulating human-like, multi-turn conversations using an LLM-centric approach. The evaluation encompasses three key elements in the evaluation pipeline: utterance generation, evaluation protocol, and judgement, and we delve deeply into each aspect. GPT-4, both as an utterance generator and as a judge, exhibits exceptional performance. As a generator, GPT-4 crafts dialogues indistinguishable from human interactions in terms of style and flow. When judging, it shows a heightened alignment with human evaluative standards and consistency. Conversely, other LLMs face challenges in producing quality multi-turn dialogues, hindered by inadequate instruction-following abilities, a propensity for prolix utterances, and overall limited capabilities. Notably, generating extensive dialogues (e.g., spanning tens of turns) remains a formidable task for most LLMs, particularly in Chinese contexts. We hope that our work can serve as a valuable resource for evaluating the multi-turn chatting capabilities of LLMs. Related resources are available at https://github.com/open-compass/BotChat.</abstract>
      <url hash="2cf1b4e3">2024.findings-naacl.201</url>
    </paper>
    <paper id="202">
      <title>Learning Mutually Informed Representations for Characters and Subwords</title>
      <author><first>Yilin</first><last>Wang</last><affiliation>School of Engineering and Applied Sciences, Harvard University and Carnegie Mellon University</affiliation></author>
      <author><first>Xinyi</first><last>Hu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Matthew</first><last>Gormley</last><affiliation>School of Computer Science, Carnegie Mellon University and 3M</affiliation></author>
      <pages>3200-3212</pages>
      <abstract>Most pretrained language models rely on subword tokenization, which processes text as a sequence of subword tokens. However, different granularities of text, such as characters, subwords, and words, can contain different kinds of information. Previous studies have shown that incorporating multiple input granularities improves model generalization, yet very few of them outputs useful representations for each granularity. In this paper, we introduce the entanglement model, aiming to combine character and subword language models. Inspired by vision-language models, our model treats characters and subwords as separate modalities, and it generates mutually informed representations for both granularities as output. We evaluate our model on text classification, named entity recognition, POS-tagging, and character-level sequence labeling (intraword code-switching). Notably, the entanglement model outperforms its backbone language models, particularly in the presence of noisy texts and low-resource languages. Furthermore, the entanglement model even outperforms larger pre-trained models on all English sequence labeling tasks and classification tasks. We make our code publically available.</abstract>
      <url hash="527d3b14">2024.findings-naacl.202</url>
    </paper>
    <paper id="203">
      <title>A Novel Two-step Fine-tuning Framework for Transfer Learning in Low-Resource Neural Machine Translation</title>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Feng</first><last>Hou</last><affiliation>Massey University</affiliation></author>
      <author><first>Ruili</first><last>Wang</last><affiliation>Massey University</affiliation></author>
      <pages>3213-3223</pages>
      <abstract>Existing transfer learning methods for neural machine translation typically use a well-trained translation model (i.e., a parent model) of a high-resource language pair to directly initialize a translation model (i.e., a child model) of a low-resource language pair, and the child model is then fine-tuned with corresponding datasets. In this paper, we propose a novel two-step fine-tuning (TSFT) framework for transfer learning in low-resource neural machine translation. In the first step, we adjust the parameters of the parent model to fit the child language by using the child source data. In the second step, we transfer the adjusted parameters to the child model and fine-tune it with a proposed distillation loss for efficient optimization. Our experimental results on five low-resource translations demonstrate that our framework yields significant improvements over various strong transfer learning baselines. Further analysis demonstrated the effectiveness of different components in our framework.</abstract>
      <url hash="7f0a49f3">2024.findings-naacl.203</url>
    </paper>
    <paper id="204">
      <title>Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment</title>
      <author><first>Zhongtao</first><last>Miao</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Qiyu</first><last>Wu</last><affiliation>The University of Tokyo, Tokyo Institute of Technology and Peking University</affiliation></author>
      <author><first>Kaiyan</first><last>Zhao</last></author>
      <author><first>Zilong</first><last>Wu</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>3224-3235</pages>
      <abstract>The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.</abstract>
      <url hash="1ee5f324">2024.findings-naacl.204</url>
    </paper>
    <paper id="205">
      <title>C<tex-math>^{3}</tex-math><fixed-case>LPGCN</fixed-case>:Integrating Contrastive Learning and Cooperative Learning with Prompt into Graph Convolutional Network for Aspect-based Sentiment Analysis</title>
      <author><first>Ye</first><last>He</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>Shihao</first><last>Zou</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>YuzheChen</first><last>YuzheChen</last></author>
      <author><first>Xianying</first><last>Huang</last><affiliation>Chongqing University of Technology</affiliation></author>
      <pages>3236-3246</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA) is a fine-grained task. Recently, using graph convolutional networks (GCNs) to model syntactic information has become a popular topic. In addition, a growing consensus exists to enhance sentence representation using contrastive learning. However, when modeling syntactic information, incorrect syntactic structure may introduce additional noise. Meanwhile, we believe that contrastive learning implicitly introduce label information as priori. Therefore, we propose C<tex-math>^{3}</tex-math>LPGCN, which integrates Contrastive Learning and Cooperative Learning with Prompt into GCN. Specifically, to alleviate the noise when modeling syntactic information, we propose mask-aware aspect information filter, which combines prompt information of template with aspect information to filter the syntactic information. Besides, we propose prompt-based contrastive learning and cooperative learning to utilise the label information further. On the one hand, we construct prompts containing labels for contrastive learning, by which the model can focus more on task-relevant features. On the other hand, cooperative learning further extracts label information by aligning input samples’ representation and output distribution with label samples. Extensive experiments on three datasets demonstrate that our method significantly improves the model’s performance compared to traditional contrastive learning methods. Moreover, our C<tex-math>^{3}</tex-math>LPGCN outperforms state-of-the-art methods. Our source code and final models are publicly available at github</abstract>
      <url hash="72b8e4c2">2024.findings-naacl.205</url>
    </paper>
    <paper id="206">
      <title>Visual Enhanced Entity-Level Interaction Network for Multimodal Summarization</title>
      <author><first>Haolong</first><last>Yan</last></author>
      <author><first>Binghao</first><last>Tang</last></author>
      <author><first>Boda</first><last>Lin</last></author>
      <author><first>Gang</first><last>Zhao</last></author>
      <author><first>Si</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>3247-3259</pages>
      <abstract>MultiModal Summarization (MMS) aims to generate a concise summary based on multimodal data like texts and images and has wide application in multimodal fields.Previous works mainly focus on the coarse-level textual and visual features in which the overall features of the image interact with the whole sentence.However, the entities of the input text and the objects of the image may be underutilized, limiting the performance of current MMS models.In this paper, we propose a novel Visual Enhanced Entity-Level Interaction Network (VE-ELIN) to address the problem of underutilization of multimodal inputs at a fine-grained level in two ways.We first design a cross-modal entity interaction module to better fuse the entity information in text and the object information in vision.Then, we design an object-guided visual enhancement module to fully extract the visual features and enhance the focus of the image on the object area.We evaluate VE-ELIN on two MMS datasets and propose new metrics to measure the factual consistency of entities in the output.Finally, experimental results demonstrate that VE-ELIN is effective and outperforms previous methods under both traditional metrics and ours.The source code is available at https://github.com/summoneryhl/VE-ELIN.</abstract>
      <url hash="b242f3d8">2024.findings-naacl.206</url>
    </paper>
    <paper id="207">
      <title>Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chuanqi</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>3260-3279</pages>
      <abstract>Large language models (LLMs) enable in-context learning (ICL) by conditioning on a few labeled training examples as a text-based prompt, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets: the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL:1) injecting knowledge into LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples for ICL with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge.We evaluate the proposed approaches on autoregressive models (e.g., GPT-style LLMs) over multiple text classification and question-answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines and improves by more than 13% and 7% on text classification and question-answering tasks, respectively.</abstract>
      <url hash="99ded53d">2024.findings-naacl.207</url>
    </paper>
    <paper id="208">
      <title>Time Machine <fixed-case>GPT</fixed-case></title>
      <author><first>Felix</first><last>Drinkall</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Eghbal</first><last>Rahimikia</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Janet</first><last>Pierrehumbert</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Stefan</first><last>Zohren</last><affiliation>University of Oxford</affiliation></author>
      <pages>3280-3291</pages>
      <abstract>Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called TimeMachineGPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.</abstract>
      <url hash="6dd2c813">2024.findings-naacl.208</url>
    </paper>
    <paper id="209">
      <title>An End-to-End Submodular Framework for Data-Efficient In-Context Learning</title>
      <author><first>Lilly</first><last>Kumari</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Shengjie</first><last>Wang</last><affiliation>University of Washington, University of Illinois, Urbana Champaign and New York University, Shanghai</affiliation></author>
      <author><first>Arnav</first><last>Das</last><affiliation>University of Washington</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jeff</first><last>Bilmes</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>3292-3307</pages>
      <abstract>Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework <tex-math>\textit{Div-S3}</tex-math> for exemplar selection for ICL. The first stage focuses on data annotation and employs a pool-based active learning approach to select a set of <tex-math>\textit{Div}</tex-math>erse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (<tex-math>\textit{S3}</tex-math>) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show <tex-math>\textit{Div-S3}</tex-math> outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.</abstract>
      <url hash="46b89e54">2024.findings-naacl.209</url>
    </paper>
    <paper id="210">
      <title>Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer</title>
      <author><first>Hele-Andra</first><last>Kuulmets</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Taido</first><last>Purason</last></author>
      <author><first>Agnes</first><last>Luhtaru</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>3308-3324</pages>
      <abstract>This paper explores cost-efficient methods to adapt pretrained Large Language Models (LLMs) to new lower-resource languages, with a specific focus on Estonian. Leveraging the Llama 2 model, we investigate the impact of combining cross-lingual instruction-tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual instruction-tuning significantly enhances results on Estonian. Furthermore, we showcase cross-lingual knowledge transfer from high-quality English instructions to Estonian, resulting in improvements in commonsense reasoning and multi-turn conversation capabilities. Our best model, named Llammas, represents the first open-source instruction-following LLM for Estonian. Additionally, we publish Alpaca-est, the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open-source LLMs for Estonian.</abstract>
      <url hash="3a664f6c">2024.findings-naacl.210</url>
    </paper>
    <paper id="211">
      <title>Simulating Opinion Dynamics with Networks of <fixed-case>LLM</fixed-case>-based Agents</title>
      <author><first>Yun-Shiuan</first><last>Chuang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Agam</first><last>Goyal</last></author>
      <author><first>Nikunj</first><last>Harlalka</last></author>
      <author><first>Siddharth</first><last>Suresh</last></author>
      <author><first>Robert</first><last>Hawkins</last><affiliation>Princeton University</affiliation></author>
      <author><first>Sijia</first><last>Yang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dhavan</first><last>Shah</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Timothy</first><last>Rogers</last></author>
      <pages>3325-3345</pages>
      <abstract>Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.</abstract>
      <url hash="8812a6df">2024.findings-naacl.211</url>
    </paper>
    <paper id="212">
      <title>Probing the Category of Verbal Aspect in Transformer Language Models</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <pages>3346-3365</pages>
      <abstract>We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by ”alternative contexts”: where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models’ performance on aspect prediction, via behavioral probing. Next, we examine the models’ performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the “boundedness” feature—a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect—mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.</abstract>
      <url hash="2c918ac9">2024.findings-naacl.212</url>
    </paper>
    <paper id="213">
      <title>A Measure for Transparent Comparison of Linguistic Diversity in Multilingual <fixed-case>NLP</fixed-case> Data Sets</title>
      <author><first>Tanja</first><last>Samardzic</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Ximena</first><last>Gutierrez</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Christian</first><last>Bentz</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Steven</first><last>Moran</last><affiliation>University of Miami</affiliation></author>
      <author><first>Olga</first><last>Pelloni</last></author>
      <pages>3366-3381</pages>
      <abstract>Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.</abstract>
      <url hash="046c6f18">2024.findings-naacl.213</url>
    </paper>
    <paper id="214">
      <title>Beyond Read-Only: Crafting a Comprehensive <fixed-case>C</fixed-case>hinese Text-to-<fixed-case>SQL</fixed-case> Dataset for Database Manipulation and Query</title>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Jinguo</first><last>You</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Likun</first><last>Likun</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <pages>3382-3392</pages>
      <abstract>Text-to-SQL aims to convert natural language into structured query language, which is a challenging task. Current research focuses mainly on read operations and ignores other aspects of database operations such as create, update, and delete operations. The benchmark datasets as well as models that have been proposed also fail to cover these operations, limiting the development and practical applications in the field. To bridge this gap, we propose CRUDSQL, a large-scale cross-domain single-table CRUD operations Chinese Text-to-SQL dataset. The dataset contains 10,000 question/SQL pairs involving 625 tables from different domains. To support further research on this dataset, we also propose a baseline method, CRUDParser, which employs a two-phase approach based on BERT and T5 for SQL generation and incorporates two strategies, value matching, and value prompting, for interacting with databases to further improve the performance. The experimental results show that the new operation types bring different challenges for future research, and our approach achieves 67.08% and 83.8% exact set matching accuracy under both read and delete operations in the test set, but only 49.6% and 61.8% under create and update operations. We believe that the proposal of CRUDSQL as well as CRUDParser can provide new directions and possibilities for research and practical applications in the field of Text-to-SQL. The dataset is published at https://github.com/bizard-lab/CRUDSQL.</abstract>
      <url hash="a3f6f547">2024.findings-naacl.214</url>
    </paper>
    <paper id="215">
      <title>Normalizing without Modernizing: Keeping Historical Wordforms of <fixed-case>M</fixed-case>iddle <fixed-case>F</fixed-case>rench while Reducing Spelling Variants</title>
      <author><first>Raphael</first><last>Rubino</last><affiliation>University of Geneva</affiliation></author>
      <author><first>Johanna</first><last>Gerlach</last><affiliation>University of Geneva</affiliation></author>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Pierrette</first><last>Bouillon</last><affiliation>University of Geneva</affiliation></author>
      <pages>3393-3401</pages>
      <abstract>Conservation of historical documents benefits from computational methods by alleviating the manual labor related to digitization and modernization of textual content. Languages usually evolve over time and keeping historical wordforms is crucial for diachronic studies and digital humanities. However, spelling conventions did not necessarily exist when texts were originally written and orthographic variations are commonly observed depending on scribes and time periods. In this study, we propose to automatically normalize orthographic wordforms found in historical archives written in Middle French during the 16th century without fully modernizing textual content. We leverage pre-trained models in a low resource setting based on a manually curated parallel corpus and produce additional resources with artificial data generation approaches. Results show that causal language models and knowledge distillation improve over a strong baseline, thus validating the proposed methods.</abstract>
      <url hash="7e81b2e6">2024.findings-naacl.215</url>
    </paper>
    <paper id="216">
      <title>Anti-<fixed-case>LM</fixed-case> Decoding for Zero-shot In-context Machine Translation</title>
      <author><first>Suzanna</first><last>Sia</last></author>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3402-3419</pages>
      <abstract>Zero-shot In-context learning is the phenomenon where models can perform a task given only the instructions. However, pre-trained large language models are known to be poorly calibrated for zero-shot tasks. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on a context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search. The proposed method outperforms other state-of-the-art decoding objectives, with up to 20 BLEU point improvement from the default objective in some settings.</abstract>
      <url hash="44f79688">2024.findings-naacl.216</url>
    </paper>
    <paper id="217">
      <title>Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning</title>
      <author><first>Shuai</first><last>Zhao</last><affiliation>Jinan University</affiliation></author>
      <author><first>Leilei</first><last>Gan</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lingjuan</first><last>Lyu</last><affiliation>Sony Research</affiliation></author>
      <author><first>Meihuizi</first><last>Jia</last></author>
      <author><first>Jinming</first><last>Wen</last></author>
      <pages>3420-3437</pages>
      <abstract>Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.</abstract>
      <url hash="4cb48b80">2024.findings-naacl.217</url>
    </paper>
    <paper id="218">
      <title>Select and Summarize: Scene Saliency for Movie Script Summarization</title>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Frank</first><last>Keller</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>3438-3454</pages>
      <abstract>Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.</abstract>
      <url hash="e3ad2d70">2024.findings-naacl.218</url>
    </paper>
    <paper id="219">
      <title>Don’t be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks</title>
      <author><first>Seunguk</first><last>Yu</last></author>
      <author><first>Juhwan</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>3455-3466</pages>
      <abstract>Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.</abstract>
      <url hash="0d8a6791">2024.findings-naacl.219</url>
    </paper>
    <paper id="220">
      <title><fixed-case>Z</fixed-case>-<fixed-case>GMOT</fixed-case>: Zero-shot Generic Multiple Object Tracking</title>
      <author><first>Kim</first><last>Tran</last><affiliation>FPT Software</affiliation></author>
      <author><first>Anh Duy</first><last>Le Dinh</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Tien-Phat</first><last>Nguyen</last><affiliation>John von Neumann</affiliation></author>
      <author><first>Thinh</first><last>Phan</last></author>
      <author><first>Pha</first><last>Nguyen</last><affiliation>University of Arkansas - Fayetteville</affiliation></author>
      <author><first>Khoa</first><last>Luu</last><affiliation>University of Arkansas, Fayetteville</affiliation></author>
      <author><first>Donald</first><last>Adjeroh</last><affiliation>West Virginia University</affiliation></author>
      <author><first>Gianfranco</first><last>Doretto</last><affiliation>West Virginia University</affiliation></author>
      <author><first>Ngan</first><last>Le</last><affiliation>University of Arkansas, Fayetteville</affiliation></author>
      <pages>3467-3478</pages>
      <abstract>Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories and struggles with unseen objects. To address these issues, Generic Multiple Object Tracking (GMOT) has emerged as an alternative approach, requiring less prior information. However, current GMOT methods often rely on initial bounding boxes and struggle to handle variations in factors such as viewpoint, lighting, occlusion, and scale, among others. Our contributions commence with the introduction of the Referring GMOT dataset a collection of videos, each accompanied by detailed textual descriptions of their attributes. Subsequently, we propose Z-GMOT, a cutting-edge tracking solution capable of tracking objects from never-seen categories without the need of initial bounding boxes or predefined categories. Within our Z-GMOT framework, we introduce two novel components: (i) iGLIP, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. (ii) MA-SORT, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking objects with high similarity. Our contributions are benchmarked through extensive experiments conducted on the Referring GMOT dataset for GMOT task. Additionally, to assess the generalizability of the proposed Z-GMOT, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT</abstract>
      <url hash="0637c834">2024.findings-naacl.220</url>
    </paper>
    <paper id="221">
      <title><fixed-case>NLP</fixed-case> for Counterspeech against Hate: A Survey and How-To Guide</title>
      <author><first>Helena</first><last>Bonaldi</last><affiliation>Fondazione Bruno Kessler and University of Trento</affiliation></author>
      <author><first>Yi-Ling</first><last>Chung</last><affiliation>Alan Turing Institute</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3479-3498</pages>
      <abstract>In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate. These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence. Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it. In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources. In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic. Finally, we discuss open challenges and future directions of counterspeech research in NLP.</abstract>
      <url hash="39601ed6">2024.findings-naacl.221</url>
    </paper>
    <paper id="222">
      <title><fixed-case>PRODIG</fixed-case>y: a <fixed-case>PRO</fixed-case>file-based <fixed-case>DI</fixed-case>alogue Generation dataset</title>
      <author><first>Daniela</first><last>Occhipinti</last></author>
      <author><first>Serra</first><last>Tekiroglu</last></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3499-3513</pages>
      <abstract>Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we introduce the PRODIGy (PROfile-based DIalogue Generation) dataset, which brings diverse representations together, providing a more comprehensive profile dimension set for each speaker. This resource comprises more than 20k dialogues, sourced from movie scripts, aligned with speaker representations such as communication style, biography, personality and gender. Initial experiments with diverse baselines show that providing generative language models with these aspects of a profile, both separately and jointly, enhances models’ performance. This improvement holds true in both in-domain and cross-domain settings, for both fine-tuned and instruction-based LLMs.</abstract>
      <url hash="8f4586b9">2024.findings-naacl.222</url>
    </paper>
    <paper id="223">
      <title><fixed-case>W</fixed-case>ater<fixed-case>J</fixed-case>udge: Quality-Detection Trade-off when Watermarking Large Language Models</title>
      <author><first>Piotr</first><last>Molenda</last></author>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>3514-3524</pages>
      <abstract>Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.</abstract>
      <url hash="d623f912">2024.findings-naacl.223</url>
    </paper>
    <paper id="224">
      <title>Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Bangzheng</first><last>Li</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3525-3547</pages>
      <abstract>While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their vulnerabilities. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of 1) multilingual cognitive overload, 2) veiled expression, and 3) effect-to- cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.</abstract>
      <url hash="77b7a659">2024.findings-naacl.224</url>
    </paper>
    <paper id="225">
      <title><fixed-case>PAELLA</fixed-case>: Parameter-Efficient Lightweight Language-Agnostic Captioning Model</title>
      <author><first>Rita</first><last>Ramos</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Emanuele</first><last>Bugliarello</last><affiliation>Google</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>and University of Copenhagen</affiliation></author>
      <pages>3548-3563</pages>
      <abstract>We introduce PAELLA, a Parameter-Efficient Lightweight Language-Agnostic image captioning model designed to be both parameter and data-efficient using retrieval augmentation. The model is trained by learning a small mapping network with 34M parameters between a pre-trained visual model and a multilingual language model that is conditioned on two types of input: (i) the image itself, and (ii) a set of retrieved captions in the target language. The retrieved examples play a key role in guiding the model to generate captions across languages. Through retrieval, the model can be lightweight in terms of the number of trainable parameters, which only exist in its mapping network, and also in the amount of multilingual training data that is required. Experiments on the XM3600 dataset, featuring 36 languages, show that PAELLA can outperform or compete against some models with 3–77<tex-math>\times</tex-math> more learned parameters and 35–863<tex-math>\times</tex-math> more data, particularly in low-resource languages. We also find that PAELLA can be trained on only monolingual data and still show strong zero-shot abilities in other languages.</abstract>
      <url hash="25609661">2024.findings-naacl.225</url>
    </paper>
    <paper id="226">
      <title><fixed-case>OSC</fixed-case>a<fixed-case>R</fixed-case>: Object State Captioning and State Change Representation</title>
      <author><first>Nguyen</first><last>Nguyen</last></author>
      <author><first>Jing</first><last>Bi</last></author>
      <author><first>Ali</first><last>Vosoughi</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Yapeng</first><last>Tian</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Pooyan</first><last>Fazli</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chenliang</first><last>Xu</last><affiliation>University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>3564-3575</pages>
      <abstract>The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating Multimodal Large Language Models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.</abstract>
      <url hash="4868a460">2024.findings-naacl.226</url>
    </paper>
    <paper id="227">
      <title><fixed-case>S</fixed-case>um<fixed-case>CSE</fixed-case>: Summary as a transformation for Contrastive Learning</title>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Xiaolan</first><last>Wang</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Jun</first><last>Chen</last><affiliation>Meta Platform</affiliation></author>
      <author><first>Shuyang</first><last>Li</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Rong</first><last>Jin</last><affiliation>Twitter</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>3576-3587</pages>
      <abstract>Sentence embedding models are typically trained using contrastive learning (CL), either using human annotations directly or by repurposing other annotated datasets. In this work, we explore the recently introduced paradigm of generating CL data using generative language models (LM). In CL for computer vision (CV), compositional transformations (series of operations applied over an image. e.g. cropping + color distortion) which modify the input/image to retain minimal information were shown to be very effective. We show that composition of a ‘Summary’ transformation with diverse paraphrasing/contradicting transformations accomplishes the same and works very well in CL for sentence embeddings. Our final generated dataset (using Vicuna-13B) significantly outperforms the previous best unsupervised method (using ChatGPT) by 1.8 points, and SimCSE, a strong supervised baseline by 0.3 points on the semantic text similarity (STS) benchmark.</abstract>
      <url hash="1b44d160">2024.findings-naacl.227</url>
    </paper>
    <paper id="228">
      <title>The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Guokan</first><last>Shang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique, France</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA and Télécom Paris</affiliation></author>
      <pages>3588-3602</pages>
      <abstract>This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models.</abstract>
      <url hash="fbdd954a">2024.findings-naacl.228</url>
    </paper>
    <paper id="229">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>LLM</fixed-case>: Investigating the Ability of Large Language Models to Express Personality Traits</title>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Xiajie</first><last>Zhang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Xubo</first><last>Cao</last></author>
      <author><first>Cynthia</first><last>Breazeal</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>3603-3625</pages>
      <abstract>Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas’ self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas’ writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.</abstract>
      <url hash="6ca63b3f">2024.findings-naacl.229</url>
    </paper>
    <paper id="230">
      <title><fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>-<fixed-case>SLU</fixed-case>: Towards <fixed-case>ASR</fixed-case>-Robust Spoken Language Understanding via Mixture-of-Experts</title>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Zhanpeng</first><last>Chen</last></author>
      <author><first>Zhiqi</first><last>Huang</last><affiliation>Tencent Game</affiliation></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>3626-3636</pages>
      <abstract>As a crucial task in the task-oriented dialogue systems, spoken language understanding (SLU) has garnered increasing attention. However, errors from automatic speech recognition (ASR) often hinder the performance of understanding. To tackle this problem, we propose MoE-SLU, an ASR-Robust SLU framework based on the mixture-of-experts technique. Specifically, we first introduce three strategies to generate additional transcripts from clean transcripts. Then, we apply the mixture-of-experts technique to weight the representations of the generated transcripts, ASR transcripts, and the corresponding clean manual transcripts. Additionally, we regularize the weighted average of predictions and the predictions of ASR transcripts by minimizing the Jensen-Shannon Divergence (JSD) between the two output distributions. Experiment results on three benchmark SLU datasets demonstrate that our MoE-SLU achieves state-of-the-art performance. Further analysis also verifies the superiority of our method.</abstract>
      <url hash="8029c05b">2024.findings-naacl.230</url>
    </paper>
    <paper id="231">
      <title><fixed-case>FIRE</fixed-case>: A Dataset for Financial Relation Extraction</title>
      <author><first>Hassan</first><last>Hamad</last></author>
      <author><first>Abhinav Kumar</first><last>Thakur</last></author>
      <author><first>Nijil</first><last>Kolleri</last></author>
      <author><first>Sujith</first><last>Pulikodan</last></author>
      <author><first>Keith</first><last>Chugg</last><affiliation>University of Southern California</affiliation></author>
      <pages>3637-3651</pages>
      <abstract>This paper introduces FIRE (**FI**nancial **R**elation **E**xtraction), a sentence-level dataset of named entities and relations within the financial sector. Comprising 3,025 instances, the dataset encapsulates 13 named entity types along with 18 relation types. Sourced from public financial reports and financial news articles, FIRE captures a wide array of financial information about a business including, but not limited to, corporate structure, business model, revenue streams, and market activities such as acquisitions. The full dataset was labeled by a single annotator to minimize labeling noise. The labeling time for each sentence was recorded during the labeling process. We show how this feature, along with curriculum learning techniques, can be used to improved a model’s performance. The FIRE dataset is designed to serve as a valuable resource for training and evaluating machine learning algorithms in the domain of financial information extraction. The dataset and the code to reproduce our experimental results are available at https://github.com/hmhamad/FIRE. The repository for the labeling tool can be found at https://github.com/abhinav-kumar-thakur/relation-extraction-annotator.</abstract>
      <url hash="ea1b3641">2024.findings-naacl.231</url>
    </paper>
    <paper id="232">
      <title><fixed-case>M</fixed-case>usi<fixed-case>L</fixed-case>ingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</title>
      <author><first>Zihao</first><last>Deng</last></author>
      <author><first>Yinghao</first><last>Ma</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Yudong</first><last>Liu</last></author>
      <author><first>Rongchen</first><last>Guo</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Emmanouil</first><last>Benetos</last><affiliation>Queen Mary, University of London</affiliation></author>
      <pages>3652-3664</pages>
      <abstract>Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains not well-explored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT (CITATION) with a frozen LLM, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from captions in the MusicCaps datasets, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.</abstract>
      <url hash="eb8e10e5">2024.findings-naacl.232</url>
    </paper>
    <paper id="233">
      <title>Investigating Acceleration of <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with ‘<fixed-case>LITE</fixed-case>’</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Agneet</first><last>Chatterjee</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University, Arizona State University and Arizona State University</affiliation></author>
      <pages>3665-3686</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable performance across a wide variety of tasks; however, their large size makes their inference slow and computationally expensive. Focusing on this problem, we study instruction tuning LLMs with additional explicit Losses from the Intermediate layers (LITE) and show that it enables these layers to acquire ‘good’ generation ability without affecting the generation ability of the final layer. We then perform ‘dynamic confidence-based early exiting’ at token level from the intermediate layers which improves the computational efficiency of text generation without sacrificing the quality of the generation. We conduct comprehensive experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and evaluate on four different instruction test sets. We show that dynamic early exiting achieves consistent and considerable inference cost improvements (37.86% for 7B and 46.35% for 13B model) while maintaining the generation quality. We further conduct a thorough analysis of the results and dissect the efficiency improvements which reveals several important findings.</abstract>
      <url hash="3d1ca028">2024.findings-naacl.233</url>
    </paper>
    <paper id="234">
      <title>Instruction-following Evaluation through Verbalizer Manipulation</title>
      <author><first>Shiyang</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Hai</first><last>Wang</last><affiliation>Samsung</affiliation></author>
      <author><first>Zheng</first><last>Tang</last><affiliation>Samsung</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California, University of Southern California and University of Southern California</affiliation></author>
      <author><first>Vijay</first><last>Srinivasan</last></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>3687-3701</pages>
      <abstract>While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting “positive” for positive sentiment), to minimally aligned (e.g., outputting “negative” for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model’s reliance on priors and its ability to override them to accurately follow the instructions. We conduct a comprehensive evaluation of four major model families across nine datasets, employing twelve sets of verbalizers for each of them. We observe that the instruction-following abilities of models, across different families and scales, are significantly distinguished by their performance on less natural verbalizers. Even the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer, emphasizing the need for continued advancements to improve their instruction-following abilities.</abstract>
      <url hash="653b376f">2024.findings-naacl.234</url>
    </paper>
    <paper id="235">
      <title><fixed-case>W</fixed-case>eb<fixed-case>WISE</fixed-case>: Unlocking Web Interface Control for <fixed-case>LLM</fixed-case>s via Sequential Exploration</title>
      <author><first>Heyi</first><last>Tao</last></author>
      <author><first>Sethuraman</first><last>T V</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Michal</first><last>Shlapentokh-Rothman</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Tanmay</first><last>Gupta</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Derek</first><last>Hoiem</last><affiliation>Department of Computer Science, Reconstruct and University of Illinois, Urbana Champaign</affiliation></author>
      <pages>3702-3720</pages>
      <abstract>This paper investigates using Large Language Models (LLMs) to automatically perform web software tasks using click, scroll, and text in- put operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate our proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method using gpt-3.5-turbo achieves similar or better performance than other methods that require many demonstrations or trials.</abstract>
      <url hash="defdeac3">2024.findings-naacl.235</url>
    </paper>
    <paper id="236">
      <title><fixed-case>C</fixed-case>odec<fixed-case>LM</fixed-case>: Aligning Language Models with Tailored Synthetic Data</title>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Chun-Liang</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Jin</first><last>Miao</last><affiliation>Google</affiliation></author>
      <author><first>Zizhao</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>3721-3738</pages>
      <abstract>Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users’ actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.</abstract>
      <url hash="375678c6">2024.findings-naacl.236</url>
    </paper>
    <paper id="237">
      <title>Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware Semantics</title>
      <author><first>Zefeng</first><last>Lin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weidong</first><last>Chen</last></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yongdong</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3739-3749</pages>
      <abstract>Given several documents, multi-hop question generation (MQG) is a task aims to generate complicated questions that require reasoning over multiple pieces of these documents to find the answer. To perform this task, existing studies focus on designing advanced architectures to locate essential keywords or sentences in multiple documents and then generate questions accordingly, where they normally do not note that question types could provide crucial hints for extracting key information from the documents for MQG. In general, supervised approaches are used that rely on large annotated data, which is not available in many low-resource scenarios and thus makes MQG hard in these domains. Consider the recent success of large language models (LLMs) on natural language processing tasks using limited labeled data under few-shot settings, in this paper, we propose an approach named type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot MQG. Specifically, our approach firstly extracts question types and essential semantic phrases from the given documents and the answer. Then, we design a three-step CoT template to leverage the extracted question type and semantic phrases to predict multi-hop questions. Extensive experiments and the results demonstrate the effectiveness of our approach and the proposed modules.</abstract>
      <url hash="fbfe25b7">2024.findings-naacl.237</url>
    </paper>
    <paper id="238">
      <title>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</title>
      <author><first>Yanhong</first><last>Li</last></author>
      <author><first>Chenghao</first><last>Yang</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Allyson</first><last>Ettinger</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>3750-3762</pages>
      <abstract>Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs’ ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA.We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models’ initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.</abstract>
      <url hash="0f437491">2024.findings-naacl.238</url>
    </paper>
    <paper id="239">
      <title><fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>a: Constrained Generation based Data Augmentation for Low-Resource <fixed-case>NLP</fixed-case></title>
      <author><first>Chandra Kiran</first><last>Evuru</last></author>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Ramaneswaran</first><last>S</last></author>
      <author><first>Utkarsh</first><last>Tyagi</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3763-3778</pages>
      <abstract>We present CoDa (**Co**nstrained Generation based **Da**ta Augmentation), a controllable, effective, and *training-free* data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available.</abstract>
      <url hash="8ee1bf57">2024.findings-naacl.239</url>
    </paper>
    <paper id="240">
      <title>Synonym relations affect object detection learned on vision-language data</title>
      <author><first>Giacomo</first><last>Nebbia</last></author>
      <author><first>Adriana</first><last>Kovashka</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>3779-3785</pages>
      <abstract>We analyze whether object detectors trained on vision-language data learn effective visual representations for synonyms. Since many current vision-language models accept user-provided textual input, we highlight the need for such models to learn feature representations that are robust to changes in how such input is provided. Specifically, we analyze changes in synonyms used to refer to objects. Here, we study object detectors trained on vision-language data and investigate how to make their performance less dependent on whether synonyms are used to refer to an object. We propose two approaches to achieve this goal: data augmentation by back-translation and class embedding enrichment. We show the promise of such approaches, reporting improved performance on synonyms from mAP@0.5=33.87% to 37.93%.</abstract>
      <url hash="e5aa70d0">2024.findings-naacl.240</url>
    </paper>
    <paper id="241">
      <title><fixed-case>CM</fixed-case>-<fixed-case>TTS</fixed-case>: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models</title>
      <author><first>Xiang</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>FanBu</first><last>FanBu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Ambuj</first><last>Mehrish</last></author>
      <author><first>Yingting</first><last>Li</last></author>
      <author><first>Jiale</first><last>Han</last></author>
      <author><first>Bo</first><last>Cheng</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>3786-3803</pages>
      <abstract>Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbiased learning throughout the entire training process. We present a real-time mel-spectrogram generation consistency model, validated through comprehensive evaluations. Experimental results underscore CM-TTS’s superiority over existing single-step speech synthesis systems, representing a significant advancement in the field.</abstract>
      <url hash="8e572a6f">2024.findings-naacl.241</url>
    </paper>
    <paper id="242">
      <title><fixed-case>R</fixed-case>obust<fixed-case>S</fixed-case>ent<fixed-case>E</fixed-case>mbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning</title>
      <author><first>Javad</first><last>Rafiei Asl</last></author>
      <author><first>Prajwal</first><last>Panzade</last><affiliation>Georgia State University</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Daniel</first><last>Takabi</last><affiliation>Old Dominion University</affiliation></author>
      <author><first>Zhipeng</first><last>Cai</last><affiliation>Georgia State University</affiliation></author>
      <pages>3804-3818</pages>
      <abstract>Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing the BERTAttack success rate by almost half (from 75.51% to 38.81%). The framework also yields improvements of 1.59% and 0.23% in semantic textual similarity tasks and various transfer tasks, respectively.</abstract>
      <url hash="b2805734">2024.findings-naacl.242</url>
    </paper>
    <paper id="243">
      <title>Characterizing Human and Zero-Shot <fixed-case>GPT</fixed-case>-3.5 Object-Similarity Judgments</title>
      <author><first>D</first><last>McKnight</last></author>
      <author><first>Alona</first><last>Fyshe</last><affiliation>University of Alberta</affiliation></author>
      <pages>3819-3837</pages>
      <abstract>Recent advancements in large language models’ (LLMs) capabilities have yielded few-shot, human-comparable performance on a range of tasks. At the same time, researchers expend significant effort and resources gathering human annotations. At some point, LLMs may be able to perform some simple annotation tasks, but studies of LLM annotation accuracy and behavior are sparse. In this paper, we characterize OpenAI’s GPT-3.5’s judgment on a behavioral task for implicit object categorization. We characterize the embedding spaces of models trained on human vs. GPT responses and give similarities and differences between them, finding many similar dimensions. We also find that despite these similar dimensions, augmenting humans’ responses with GPT ones drives model divergence across the sizes of datasets tested.</abstract>
      <url hash="8f07d0d1">2024.findings-naacl.243</url>
    </paper>
    <paper id="244">
      <title>Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models</title>
      <author><first>Wei</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shichun</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yiwen</first><last>Ding</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>3838-3854</pages>
      <abstract>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos’s generalization and provide more insights.</abstract>
      <url hash="ee8556e7">2024.findings-naacl.244</url>
    </paper>
    <paper id="245">
      <title>Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Zoom</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3855-3877</pages>
      <abstract>Event temporal reasoning aims at identifying the temporal relations between two or more events from narratives. However, knowledge conflicts arise when there is a mismatch between the actual temporal relations of events in the context and the prior knowledge or biases learned by the model. In this paper, we propose to detect knowledge-conflict examples in event temporal reasoning using bias indicators, which include event relation prior bias, tense bias, narrative bias, and dependency bias. We define conflict examples as those where event relations are opposite to biased or prior relations. To mitigate event-related knowledge conflicts, we introduce a Counterfactual Data Augmentation (CDA) based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In- Context Learning. Experiments suggest both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning, and CDA has the potential for reducing hallucination and improving model performance.</abstract>
      <url hash="c0b406f6">2024.findings-naacl.245</url>
    </paper>
    <paper id="246">
      <title><fixed-case>MCECR</fixed-case>: A Novel Dataset for Multilingual Cross-Document Event Coreference Resolution</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Viet</first><last>Lai</last><affiliation>Kensho Technologies</affiliation></author>
      <author><first>Chien</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>, University of Oregon</affiliation></author>
      <pages>3878-3889</pages>
      <abstract>Event coreference resolution (ECR) is a critical task in information extraction of natural language processing, aiming to identify and link event mentions across multiple documents. Despite recent progress, existing datasets for ECR primarily focus on within-document event coreference and English text, lacking cross-document ECR datasets for multiple languages beyond English. To address this issue, this work presents the first multiligual dataset for cross-document ECR, called MCECR (Multilingual Cross-Document Event Coreference Resolution), that manually annotates a diverse collection of documents for event mentions and coreference in five languages, i.e., English, Spanish, Hindi, Turkish, and Ukrainian. Using sampled articles from Wikinews over various topics as the seeds, our dataset fetches related news articles from the Google search engine to increase the number of non-singleton event clusters. In total, we annotate 5,802 news articles, providing a substantial and varied dataset for multilingual ECR in both within-document and cross-document scenarios. Extensive analysis of the proposed dataset reveals the challenging nature of multilingual event coreference resolution tasks, promoting MCECR as a strong benchmark dataset for future research in this area.</abstract>
      <url hash="66699cc4">2024.findings-naacl.246</url>
    </paper>
    <paper id="247">
      <title>Sentiment Analysis in the Era of Large Language Models: A Reality Check</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Deng</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Bing</first><last>Liu</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Sinno</first><last>Pan</last><affiliation>Nanyang Technological University and The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3890-3915</pages>
      <abstract>Sentiment analysis (SA) has been a long-standing research area in natural language processing. With the recent advent of large language models (LLMs), there is great potential for their employment on SA problems. However, the extent to which current LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs’ SA abilities and propose a novel benchmark, SentiEval, for a more comprehensive and realistic evaluation. Data and code are available at <url>https://github.com/DAMO-NLP-SG/LLM-Sentiment</url>.</abstract>
      <url hash="ae5c065d">2024.findings-naacl.247</url>
    </paper>
    <paper id="248">
      <title>Tokenizer Choice For <fixed-case>LLM</fixed-case> Training: Negligible or Crucial?</title>
      <author><first>Mehdi</first><last>Ali</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Michael</first><last>Fromm</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Klaudia</first><last>Thellmann</last><affiliation>TU Dresden</affiliation></author>
      <author><first>Richard</first><last>Rutmann</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Max</first><last>Lübbering</last><affiliation>Fraunhofer IAIS</affiliation></author>
      <author><first>Johannes</first><last>Leveling</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Katrin</first><last>Klug</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Jan</first><last>Ebert</last><affiliation>Forschungszentrum Jülich GmbH</affiliation></author>
      <author><first>Niclas</first><last>Doll</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Jasper</first><last>Buschhoff</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Charvi</first><last>Jain</last></author>
      <author><first>Alexander</first><last>Weber</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Lena</first><last>Jurkschat</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Hammam</first><last>Abdelwahab</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Chelsea</first><last>John</last><affiliation>Forschungszentrum Juelich GmbH</affiliation></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last><affiliation>Common Crawl Foundation</affiliation></author>
      <author><first>Malte</first><last>Ostendorff</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Samuel</first><last>Weinbach</last><affiliation>Aleph Alpha GmbH</affiliation></author>
      <author><first>Rafet</first><last>Sifa</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Stefan</first><last>Kesselheim</last><affiliation>Forschungszentrum Jülich</affiliation></author>
      <author><first>Nicolas</first><last>Flores-Herr</last><affiliation>Max-Planck Institute and Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <pages>3916-3933</pages>
      <abstract>The recent success of large language models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot.Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model’s downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model’s downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.</abstract>
      <url hash="22d310ee">2024.findings-naacl.248</url>
    </paper>
    <paper id="249">
      <title>Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue</title>
      <author><first>Junkai</first><last>Zhou</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>3934-3960</pages>
      <abstract>The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inner monologues. To evaluate various communication skills, we construct a benchmark named Cskills, which can also more comprehensively evaluate the dialogue generation ability of the model. Experimental results show that the proposed CSIM strategy improves the backbone models and outperforms the baselines.</abstract>
      <url hash="32946540">2024.findings-naacl.249</url>
    </paper>
    <paper id="250">
      <title>The Impact of Differential Privacy on Group Disparity Mitigation</title>
      <author><first>Victor</first><last>Hansen</last></author>
      <author><first>Atula</first><last>Neerkaje</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Ramit</first><last>Sawhney</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>3961-3974</pages>
      <abstract>The performance cost of differential privacy has, for some applications, been shown to be higher for minority groups; fairness, conversely, has been shown to disproportionally compromise the privacy of members of such groups. Most work in this area has been restricted to computer vision and risk assessment. In response, we evaluate the impact of differential privacy on fairness across four diverse tasks, focusing on how attempts to mitigate privacy violations and between-group performance differences interact: Does privacy inhibit attempts to ensure fairness? To this end, we train <tex-math>(\varepsilon,\delta)</tex-math>-differentially private models with empirical risk minimization and group distributionally robust training objectives. Consistent with previous findings, we find that differential privacy increases between-group performance differences in the baseline setting; more interestingly, differential privacy <i>reduces</i> between-group performance differences in the robust setting. We explain this by interpreting differential privacy as regularization.</abstract>
      <url hash="b1f42fbe">2024.findings-naacl.250</url>
    </paper>
    <paper id="251">
      <title>Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning</title>
      <author><first>Shivam</first><last>Mhaskar</last><affiliation>Rakuten Mobile, Inc.</affiliation></author>
      <author><first>Nirmesh</first><last>Shah</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Mohammadi</first><last>Zaki</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Ashishkumar</first><last>Gudmalwar</last></author>
      <author><first>Pankaj</first><last>Wasnik</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Rajiv</first><last>Shah</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>3975-3985</pages>
      <abstract>Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the Phoneme Count Compliance (PCC) score, which is a measure of length compliance. Our approach demonstrates a substantial improvement of approximately 36% in the PCC score compared to the state-of-the-art models when applied to English-Hindi language pairs. Moreover, we propose a student-teacher architecture within the framework of our RL approach to maintain a trade-off between the phoneme count and translation quality.</abstract>
      <url hash="1dd8fc36">2024.findings-naacl.251</url>
    </paper>
    <paper id="252">
      <title>Read between the lines - Functionality Extraction From <fixed-case>README</fixed-case>s</title>
      <author><first>Prince</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Srikanth</first><last>Tamilselvam</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Garg</last></author>
      <pages>3986-3999</pages>
      <abstract>While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.</abstract>
      <url hash="58508536">2024.findings-naacl.252</url>
    </paper>
    <paper id="253">
      <title><fixed-case>A</fixed-case>bs<fixed-case>P</fixed-case>yramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</title>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Sehyun</first><last>Choi</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>4000-4019</pages>
      <abstract>Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.</abstract>
      <url hash="de6dbb0d">2024.findings-naacl.253</url>
    </paper>
    <paper id="254">
      <title>Few-<fixed-case>TK</fixed-case>: A Dataset for Few-shot Scientific Typed Keyphrase Recognition</title>
      <author><first>Avishek</first><last>Lahiri</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Pratyay</first><last>Sarkar</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Medha</first><last>Sen</last></author>
      <author><first>Debarshi</first><last>Sanyal</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Imon</first><last>Mukherjee</last></author>
      <pages>4020-4034</pages>
      <abstract>Scientific texts are distinctive from ordinary texts in quite a few aspects like their vocabulary and discourse structure. Consequently, Information Extraction (IE) tasks for scientific texts come with their own set of challenges. The classical definition of Named Entities restricts the inclusion of all scientific terms under its hood, which is why previous works have used the terms Named Entities and Keyphrases interchangeably. We suggest the rechristening of Named Entities for the scientific domain as Typed Keyphrases (TK), broadening their scope. We advocate for exploring this task in the few-shot domain due to the scarcity of labeled scientific IE data. Currently, no dataset exists for few-shot scientific Typed Keyphrase Recognition. To address this gap, we develop an annotation schema and present Few-TK, a dataset in the AI/ML field that includes scientific Typed Keyphrase annotations on abstracts of 500 research papers. To the best of our knowledge, this is the introductory few-shot Typed Keyphrase recognition dataset and only the second dataset structured specifically for few-shot NER, after Few-NERD. We report the results of several few-shot sequence-labelling models applied to our dataset. The data and code are available at https://github.com/AvishekLahiri/Few_TK.git</abstract>
      <url hash="2bbf8f9a">2024.findings-naacl.254</url>
    </paper>
    <paper id="255">
      <title>Language Models can be Deductive Solvers</title>
      <author><first>Jiazhan</first><last>Feng</last></author>
      <author><first>Ruochen</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Junheng</first><last>Hao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hiteshi</first><last>Sharma</last></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Weizhu</first><last>Chen</last><affiliation>Microsoft GenAI</affiliation></author>
      <pages>4035-4051</pages>
      <abstract>Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of external logical solvers and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly internalizes and emulates the reasoning processes of logical solvers and avoids parsing errors by learning strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning benchmarks show that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like GPT-4. This project is available in https://github.com/Cyril-JZ/LoGiPT.</abstract>
      <url hash="33f26321">2024.findings-naacl.255</url>
    </paper>
    <paper id="256">
      <title>Interpreting User Requests in the Context of Natural Language Standing Instructions</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Patrick</first><last>Xia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Harsh</first><last>Jhamtani</last><affiliation>Microsoft</affiliation></author>
      <pages>4052-4069</pages>
      <abstract>Users of natural language interfaces, frequently powered by Large Language Models (LLMs), must often repeat their full set of preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences – collectively termed standing instructions – are provided as additional context for such interfaces. For example, when a user states “I’m hungry”, a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants.We develop NLSI, a language-to-program dataset consisting of over 2.4K English dialogues spanning 17 domains, in which each dialogue is paired with a user profile (a set of user-specific standing instructions) and corresponding structured representations (a sequence of API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 46% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls</abstract>
      <url hash="55ef7e20">2024.findings-naacl.256</url>
    </paper>
    <paper id="257">
      <title>Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models</title>
      <author><first>Ruixiang</first><last>Tang</last></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Xuanting</first><last>Cai</last></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>4070-4082</pages>
      <abstract>Large language models (LLMs) have notably revolutionized many domains within natural language processing due to their exceptional performance. Their security has become increasingly vital. This study is centered on protecting LLMs against unauthorized access and potential theft. We propose a simple yet effective protective measure wherein a unique key prompt is embedded within the LLM. This mechanism enables the model to respond only when presented with the correct key prompt; otherwise, LLMs will refuse to react to any input instructions. This key prompt protection offers a robust solution to prevent the unauthorized use of LLMs, as the model becomes unusable without the correct key. We evaluated the proposed protection on multiple LLMs and NLP tasks. Results demonstrate that our method can successfully protect the LLM without significantly impacting the model’s original function. Moreover, we demonstrate potential attacks that attempt to bypass the protection mechanism will adversely affect the model’s performance, further emphasizing the effectiveness of the proposed protection method.</abstract>
      <url hash="57abc029">2024.findings-naacl.257</url>
    </paper>
    <paper id="258">
      <title>Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models</title>
      <author><first>Jiashuo</first><last>Sun</last></author>
      <author><first>Yi</first><last>Luo</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Chen</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>4083-4110</pages>
      <abstract>Large language models (LLMs) can achieve impressive performance on various reasoning tasks by incorporating chain-of-thought (CoT) prompting, where step-by-step reasoning is provided to guide LLMs to generate answers to questions, and the question-rationale-answer triplets are utilized as demonstration exemplars. However, the reasoning chains of demonstrations generated by LLMs are observed to be prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars, e.g., overly simplistic or complex exemplars depending on the question’s difficulty level, can affect the LLM’s performance. To address these issues, we introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts prompting). Iter-CoT has two advantages: (1) it adopts iterative bootstrapping that enables LLMs to rectify errors autonomously, resulting in more precise and comprehensive reasoning chains. (2) it selects exemplars of challenging yet answerable (i.e., the LLM has the potential to answer correctly) questions, enhancing the LLMs’ generalizability to answer questions with varying difficulty levels. Experimental results exhibit Iter-CoT superior performance on three distinct reasoning tasks on ten datasets.</abstract>
      <url hash="9bdb27a4">2024.findings-naacl.258</url>
    </paper>
    <paper id="259">
      <title>Do Prompt Positions Really Matter?</title>
      <author><first>Junyu</first><last>Mao</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Stuart</first><last>Middleton</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Mahesan</first><last>Niranjan</last><affiliation>University of Southampton</affiliation></author>
      <pages>4111-4139</pages>
      <abstract>Prompt-based models have gathered a lot of attention from researchers due to their remarkable advancements in the fields of zero-shot and few-shot learning. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary searching or embedding initialization within a predefined template with the prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position for diverse Natural Language Processing (NLP) tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt positions used in prior studies are often sub-optimal, and this observation is consistent even in widely used instruction-tuned models. These findings suggest prompt position optimisation as a valuable research direction to augment prompt engineering methodologies and prompt position-aware instruction tuning as a potential way to build more robust models in the future.</abstract>
      <url hash="3503a6ce">2024.findings-naacl.259</url>
    </paper>
    <paper id="260">
      <title>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</title>
      <author><first>Tianhua</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Ge</last></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Mingye</first><last>Gao</last></author>
      <author><first>Yuan</first><last>Gong</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>4140-4164</pages>
      <abstract>How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We found that the generated programs are interpretable since they outline the exact reasoning process followed by the program interpreter.</abstract>
      <url hash="8c507857">2024.findings-naacl.260</url>
    </paper>
    <paper id="261">
      <title>A Study on Scaling Up Multilingual News Framing Analysis</title>
      <author><first>Syeda Sabrina</first><last>Akter</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>4165-4182</pages>
      <abstract>Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains.Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (LLMs) for this task, finding that task-specific fine-tuning is a better approach than employing bigger non-specialized models.</abstract>
      <url hash="ab9fb36e">2024.findings-naacl.261</url>
    </paper>
    <paper id="262">
      <title><fixed-case>V</fixed-case>i<fixed-case>GLUE</fixed-case>: A <fixed-case>V</fixed-case>ietnamese General Language Understanding Benchmark and Analysis of <fixed-case>V</fixed-case>ietnamese Language Models</title>
      <author><first>Minh-Nam</first><last>Tran</last></author>
      <author><first>Phu-Vinh</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Nguyen</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Dien</first><last>Dinh</last></author>
      <pages>4183-4198</pages>
      <abstract>As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to the difficulty in accessing natural language processing datasets or the scarcity of task-specific datasets. **ViGLUE**, the proposed dataset collection, is a **Vi**etnamese **G**eneral **L**anguage **U**nderstanding **E**valuation benchmark developed using three methods: translating an existing benchmark, generating new corpora, and collecting available datasets. ViGLUE contains twelve tasks and encompasses over ten areas and subjects, enabling it to evaluate models comprehensively over a broad spectrum of aspects. Baseline models utilizing multilingual language models are also provided for all tasks in the proposed benchmarks. In addition, the study of the available Vietnamese large language models is conducted to explore the language models’ ability in the few-shot learning framework, leading to the exploration of the relationship between specific tasks and the number of shots.</abstract>
      <url hash="08781eac">2024.findings-naacl.262</url>
    </paper>
    <paper id="263">
      <title>Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales</title>
      <author><first>Lucas</first><last>Resck</last><affiliation>Fundação Getulio Vargas</affiliation></author>
      <author><first>Marcos</first><last>M. Raimundo</last><affiliation>Universidade Estadual de Campinas</affiliation></author>
      <author><first>Jorge</first><last>Poco</last><affiliation>Fundação Getulio Vargas</affiliation></author>
      <pages>4199-4225</pages>
      <abstract>Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model’s reasoning, they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning. By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model’s performance.</abstract>
      <url hash="7b3323d9">2024.findings-naacl.263</url>
    </paper>
    <paper id="264">
      <title>Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation</title>
      <author><first>Tong</first><last>Su</last></author>
      <author><first>Xin</first><last>Peng</last></author>
      <author><first>Sarubi</first><last>Thillainathan</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>David</first><last>Guzmán</last></author>
      <author><first>Surangika</first><last>Ranathunga</last><affiliation>Massey University</affiliation></author>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <pages>4226-4234</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in adapting large-scale pre-trained language models for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods.</abstract>
      <url hash="a98e405d">2024.findings-naacl.264</url>
    </paper>
    <paper id="265">
      <title><fixed-case>AD</fixed-case>a<fixed-case>PT</fixed-case>: As-Needed Decomposition and Planning with Language Models</title>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ashish</first><last>Sabharwal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tushar</first><last>Khot</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4235-4261</pages>
      <abstract>Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft – a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.</abstract>
      <url hash="48232e1f">2024.findings-naacl.265</url>
    </paper>
    <paper id="266">
      <title>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</title>
      <author><first>Dayeon</first><last>Ki</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>4262-4282</pages>
      <abstract>Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.</abstract>
      <url hash="d5e1d9c1">2024.findings-naacl.266</url>
    </paper>
    <paper id="267">
      <title>Non-contrastive sentence representations via self-supervision</title>
      <author><first>Duccio</first><last>Pappadopulo</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Marco</first><last>Farina</last><affiliation>Bloomberg</affiliation></author>
      <pages>4283-4293</pages>
      <abstract>Sample contrastive methods, typically referred to simply as contrastive are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised non-contrastive loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.</abstract>
      <url hash="ad6bdb14">2024.findings-naacl.267</url>
    </paper>
    <paper id="268">
      <title>Semantically-Prompted Language Models Improve Visual Descriptions</title>
      <author><first>Michael</first><last>Ogezi</last></author>
      <author><first>Bradley</first><last>Hauer</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Grzegorz</first><last>Kondrak</last><affiliation>University of Alberta</affiliation></author>
      <pages>4294-4311</pages>
      <abstract>Language-vision models like CLIP have made significant strides in vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive visual descriptions remains challenging; descriptions produced by current methods are often ambiguous and lacking in granularity. To tackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built upon two key ideas. The first is Semantic Prompting, which conditions a language model on structured semantic knowledge. The second is a new contrastive algorithm that elicits fine-grained distinctions between similar concepts. With both ideas, we demonstrate that V-GLOSS improves visual descriptions and achieves strong results in the zero-shot setting on general and fine-grained image-classification datasets, including ImageNet, STL-10, FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities contribute to enhancing image-generation performance. Finally, we introduce a quality-tested silver dataset with descriptions generated with V-GLOSS for all ImageNet classes.</abstract>
      <url hash="08691d1f">2024.findings-naacl.268</url>
    </paper>
    <paper id="269">
      <title><fixed-case>G</fixed-case>en<fixed-case>TKG</fixed-case>: Generative Forecasting on Temporal Knowledge Graph with Large Language Models</title>
      <author><first>Ruotong</first><last>Liao</last></author>
      <author><first>Xu</first><last>Jia</last></author>
      <author><first>Yangzhe</first><last>Li</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Yunpu</first><last>Ma</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <pages>4312-4326</pages>
      <abstract>The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. The code and data are released here: <url>https://github.com/mayhugotong/GenTKG</url>.</abstract>
      <url hash="349595b3">2024.findings-naacl.269</url>
    </paper>
    <paper id="270">
      <title>A Transformer Language Model with Stack Attention</title>
      <author><first>Jiaoda</first><last>Li</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Jennifer</first><last>White</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>4327-4339</pages>
      <abstract/>
      <url hash="7d9f1b7a">2024.findings-naacl.270</url>
    </paper>
    <paper id="271">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>E</fixed-case>val: Systematic Evaluation of Instruction Selection Methods</title>
      <author><first>Anirudh</first><last>Ajith</last></author>
      <author><first>Chris</first><last>Pan</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Ameet</first><last>Deshpande</last></author>
      <author><first>Karthik</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <pages>4340-4354</pages>
      <abstract>In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite (at https://github.com/princeton-nlp/InstructEval) for benchmarking instruction selection approaches and enabling more generalizable methods in this space.</abstract>
      <url hash="c630780a">2024.findings-naacl.271</url>
    </paper>
    <paper id="272">
      <title><fixed-case>R</fixed-case>ec<fixed-case>M</fixed-case>ind: Large Language Model Powered Agent For Recommendation</title>
      <author><first>Yancheng</first><last>Wang</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ziyan</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yingxue</first><last>Zhou</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Yanbin</first><last>Lu</last></author>
      <author><first>Xiaojiang</first><last>Huang</last></author>
      <author><first>Yingzhen</first><last>Yang</last><affiliation>Arizona State University</affiliation></author>
      <pages>4355-4368</pages>
      <abstract>While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM “self-inspires” to consider all previously explored states to plan for the next step. This mechanism greatly improves the model’s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind’s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.</abstract>
      <url hash="8e550d84">2024.findings-naacl.272</url>
    </paper>
    <paper id="273">
      <title><fixed-case>GOLD</fixed-case>: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation</title>
      <author><first>Mohsen</first><last>Gholami</last></author>
      <author><first>Mohammad</first><last>Akbari</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Tianxi</first><last>Hu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Vaden</first><last>Masrani</last></author>
      <author><first>Z.</first><last>Wang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>4369-4384</pages>
      <abstract>Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior arts and the LLM with an average improvement of 5% and 14%. We will also show that the proposed method is applicable to less explored and novel tasks. Code is available in the Appendix.</abstract>
      <url hash="db69d6e5">2024.findings-naacl.273</url>
    </paper>
    <paper id="274">
      <title>How Lexical is Bilingual Lexicon Induction?</title>
      <author><first>Harsh</first><last>Kohli</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Helian</first><last>Feng</last><affiliation>Amazon</affiliation></author>
      <author><first>Nicholas</first><last>Dronen</last><affiliation>Lightmatter</affiliation></author>
      <author><first>Calvin</first><last>McCarter</last></author>
      <author><first>Sina</first><last>Moeini</last></author>
      <author><first>Ali</first><last>Kebarighotbi</last></author>
      <pages>4385-4390</pages>
      <abstract>In contemporary machine learning approaches to bilingual lexicon induction (BLI), a model learns a mapping between the embedding spaces of a language pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art results on the task. However, the problem remains challenging in low-resource settings, due to the paucity of data. The task is complicated by factors such as lexical variation across languages. We argue that the incorporation of additional lexical information into the recent retrieve-and-rank approach should improve lexicon induction. We demonstrate the efficacy of our proposed approach on XLING, improving over the previous state of the art by an average of 2% across all language pairs.</abstract>
      <url hash="27d27451">2024.findings-naacl.274</url>
    </paper>
    <paper id="275">
      <title>Fumbling in Babel: An Investigation into <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>’s Language Identification Ability</title>
      <author><first>Wei-Rui</first><last>Chen</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Ife</first><last>Adebara</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Khai</first><last>Doan</last></author>
      <author><first>Qisheng</first><last>Liao</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4391-4417</pages>
      <abstract>ChatGPT has recently emerged as a powerful NLP tool that can carry out a variety of tasks. However, the range of languages ChatGPT can handle remains largely a mystery. To uncover which languages ChatGPT ‘knows’, we investigate its language identification (LID) abilities. For this purpose, we compile Babel-670, a benchmark comprising 670 languages representing 23 language families spoken in five continents. Languages in Babel-670 run the gamut from the very high-resource to the very low-resource. We then study ChatGPT’s (both GPT-3.5 and GPT-4) ability to (i) identify language names and language codes (ii) under zero- and few-shot conditions (iii) with and without provision of a label set. When compared to smaller finetuned LID tools, we find that ChatGPT lags behind. For example, it has poor performance on African languages. We conclude that current large language models would benefit from further development before they can sufficiently serve diverse communities.</abstract>
      <url hash="0bc6bbb1">2024.findings-naacl.275</url>
    </paper>
    <paper id="276">
      <title>Targeted Augmentation for Low-Resource Event Extraction</title>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>4418-4432</pages>
      <abstract>Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.</abstract>
      <url hash="7be71b97">2024.findings-naacl.276</url>
    </paper>
    <paper id="277">
      <title>Asking More Informative Questions for Grounded Retrieval</title>
      <author><first>Sedrick</first><last>Keh</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Justin</first><last>Chiu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>4433-4446</pages>
      <abstract>When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions (White et al., 2021), limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.</abstract>
      <url hash="4f7aab3f">2024.findings-naacl.277</url>
    </paper>
    <paper id="278">
      <title>Efficient Citer: Tuning Large Language Models for Enhanced Answer Quality and Verification</title>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Aref</first><last>Jafari</last><affiliation>University of Waterloo and Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ahmad</first><last>Rashid</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>David</first><last>Alfonso-Hermelo</last></author>
      <author><first>Khalil</first><last>Bibi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yimeng</first><last>Wu</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>4447-4454</pages>
      <abstract>In recent years, there has been a growing interest in utilizing external knowledge to reduce hallucinations in large language models (LLMs) and provide them with updated information. Despite this improvement, a major challenge lies in the lack of explicit citations, which hampers the ability to verify the information generated by these models.This paper focuses on providing models with citation capabilities efficiently. By constructing a dataset of citations, we train two model architectures: an FID-style FLAN-T5 model for efficient answer composition and a 13B model known for its success in instruction following after tuning. Evaluation on fluency, correctness, and citation quality is conducted through human assessment and the newly introduced Automatic LLMs’ Citation Evaluation (ALCE) benchmark.Results demonstrate significant improvements in answer quality and efficiency, surpassing the performance of the popular ChatGPT on some of the metrics. The models exhibit exceptional out-of-domain generalization in both human and automatic evaluation. Notably, the FID-style FLAN-T5 model with only 3B parameters performs impressively compared to the 13B model.</abstract>
      <url hash="4d07b00d">2024.findings-naacl.278</url>
    </paper>
    <paper id="279">
      <title>Addressing Healthcare-related Racial and <fixed-case>LGBTQ</fixed-case>+ Biases in Pretrained Language Models</title>
      <author><first>Sean</first><last>Xie</last></author>
      <author><first>Saeed</first><last>Hassanpour</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>4455-4468</pages>
      <abstract>Recent studies have highlighted the issue of Pretrained Language Models (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern given their widespread use. This is particularly problematic in sensitive areas like healthcare, where such biases could lead to detrimental outcomes. Our research addresses this by adapting two intrinsic bias benchmarks to quantify racial and LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness of various debiasing methods in mitigating these biases. Furthermore, we assess the impact of debiasing on both Natural Language Understanding and specific biomedical applications. Our findings reveal that while PLMs commonly exhibit healthcare-related racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce these biases without compromising the models’ performance in downstream tasks.</abstract>
      <url hash="c37dc25d">2024.findings-naacl.279</url>
    </paper>
    <paper id="280">
      <title><fixed-case>ATG</fixed-case>: Benchmarking Automated Theorem Generation for Generative Language Models</title>
      <author><first>Xiaohan</first><last>Lin</last></author>
      <author><first>Qingxing</first><last>Cao</last><affiliation>SUN YAT-SEN UNIVERSITY, Tsinghua University</affiliation></author>
      <author><first>Yinya</first><last>Huang</last></author>
      <author><first>Zhicheng</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Zhengying</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>4469-4484</pages>
      <abstract>Humans can develop new theorems to explore broader and more complex mathematical results.While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to generate new or reusable theorems is still under-explored. Without the new theorems, current LMs struggle to prove harder theorems that are distant from the given hypotheses with the exponentially growing search space.More advanced theorem proving is if an agent (for instance, a generative LM) can leverage its creativity to generate new but also reasonable theorems that properly substitute part of a proof and also be saved as reusable knowledge for future theorem proving.Therefore, this paper proposes an Automated Theorem Generation (ATG) benchmark that evaluates whether an agent can automatically generate valuable (and possibly brand new) theorems that are applicable for downstream theorem proving as reusable knowledge. Specifically, we construct the ATG benchmark by splitting the Metamath library into three sets: axioms, library, and problem based on their proving depth.We conduct extensive experiments to investigate whether current LMs can generate theorems in the library and benefit the problem theorems proving. The results demonstrate that high-quality ATG data facilitates models’ performances on downstream ATP. However, there is still room for current LMs to develop better ATG and generate more advanced and human-like theorems. We hope the new ATG challenge can shed some light on advanced complex theorem proving.</abstract>
      <url hash="bae9408f">2024.findings-naacl.280</url>
    </paper>
    <paper id="281">
      <title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jiawen</first><last>Chen</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Simeng</first><last>Han</last><affiliation>Yale University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4485-4505</pages>
      <abstract>While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction.</abstract>
      <url hash="78a18f78">2024.findings-naacl.281</url>
    </paper>
    <paper id="282">
      <title><fixed-case>N</fixed-case>euro<fixed-case>C</fixed-case>omparatives: Neuro-Symbolic Distillation of Comparative Knowledge</title>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <author><first>Junlin</first><last>Wang</last></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel</affiliation></author>
      <author><first>Gadi</first><last>Singer</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Swabha</first><last>Swayamdipta</last><affiliation>University of Southern California</affiliation></author>
      <pages>4506-4524</pages>
      <abstract>Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we harvest the dramatic improvements in knowledge capabilities of language models into a large-scale comparative knowledge base. While the ease of acquisition of such comparative knowledge is much higher from extreme-scale models like GPT-4, compared to their considerably smaller and weaker counterparts such as GPT-2, not even the most powerful models are exempt from making errors. We thus ask: to what extent are models at different scales able to generate valid and diverse comparative knowledge?We introduce NeuroComparatives, a novel framework for comparative knowledge distillation overgenerated from language models such as GPT-variants and LLaMA, followed by stringent filtering of the generated knowledge. Our framework acquires comparative knowledge between everyday objects, producing a corpus of up to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources. Moreover, human evaluations show that NeuroComparatives outperform existing resources in terms of validity (up to 32% absolute improvement). Our acquired NeuroComparatives leads to performance improvements on five downstream tasks.We find that neuro-symbolic manipulation of smaller models offers complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation.</abstract>
      <url hash="3534af2d">2024.findings-naacl.282</url>
    </paper>
    <paper id="283">
      <title>Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation</title>
      <author><first>Fangxu</first><last>Yu</last></author>
      <author><first>Junjie</first><last>Guo</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xinyu</first><last>Dai</last><affiliation>Nanjing University</affiliation></author>
      <pages>4525-4538</pages>
      <abstract>Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.</abstract>
      <url hash="eb86c79f">2024.findings-naacl.283</url>
    </paper>
    <paper id="284">
      <title><fixed-case>SUQL</fixed-case>: Conversational Search over Structured and Unstructured Data with Large Language Models</title>
      <author><first>Shicheng</first><last>Liu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jialiang</first><last>Xu</last></author>
      <author><first>Wesley</first><last>Tjangnaka</last></author>
      <author><first>Sina</first><last>Semnani</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chen</first><last>Yu</last></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>4539-4559</pages>
      <abstract>While most conversational agents are grounded on either free-text or structured knowledge, many knowledge corpora consist of hybrid sources.This paper presents the first conversational agent that supports the full generality of hybrid data access for large knowledge corpora, through a language we developed called SUQL (<tex-math>\textbf{S}</tex-math>tructured and <tex-math>\textbf{U}</tex-math>nstructured <tex-math>\textbf{Q}</tex-math>uery <tex-math>\textbf{L}</tex-math>anguage). Specifically, SUQL extends SQL with free-text primitives (<tex-math>{\small \text{SUMMARY}}</tex-math> and <tex-math>{\small \text{ANSWER}}</tex-math>), so information retrieval can be composed with structured data accesses arbitrarily in a formal, succinct, precise, and interpretable notation. With SUQL, we propose the first semantic parser, an LLM with in-context learning, that can handle hybrid data sources.Our in-context learning-based approach, when applied to the HybridQA dataset, comes within 8.9% Exact Match and 7.1% F1 of the SOTA, which was trained on 62K data samples. More significantly, unlike previous approaches, our technique is applicable to large databases and free-text corpora. We introduce a dataset consisting of crowdsourced questions and conversations on Yelp, a large, real restaurant knowledge base with structured and unstructured data. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 90.3% of the time, compared to 63.4% for a baseline based on linearization.</abstract>
      <url hash="753087c0">2024.findings-naacl.284</url>
    </paper>
    <paper id="285">
      <title>On Evaluating the Integration of Reasoning and Action in <fixed-case>LLM</fixed-case> Agents with Database Question Answering</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Ellen</first><last>Zhang</last></author>
      <author><first>Weijin</first><last>Zou</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Wenfei</first><last>Zhou</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4560-4583</pages>
      <abstract>This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art **GPT-4** model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.</abstract>
      <url hash="b80ddfa5">2024.findings-naacl.285</url>
    </paper>
    <paper id="286">
      <title><fixed-case>CARE</fixed-case>: Extracting Experimental Findings From Clinical Literature</title>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for Artificial Intelligence and Northwestern University</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for Artificial Intelligence and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <pages>4584-4600</pages>
      <abstract>Extracting fine-grained experimental findings from literature can provide dramatic utility for scientific applications. Prior work has developed annotation schemas and datasets for limited aspects of this problem, failing to capture the real-world complexity and nuance required. Focusing on biomedicine, this work presents CARE—a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which unifies phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, variable arity n-ary relations and numeric results in a single schema. We collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also demonstrate the generalizability of our schema to the computer science and materials science domains. We benchmark state-of-the-art IE systems on CARE, showing that even models such as GPT4 struggle. We release our resources to advance research on extracting and aggregating literature findings.</abstract>
      <url hash="5302a63e">2024.findings-naacl.286</url>
    </paper>
    <paper id="287">
      <title>Personalized Federated Learning for Text Classification with Gradient-Free Prompt Tuning</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ryan</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Subrata</first><last>Mitra</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Lina</first><last>Yao</last><affiliation>University of New South Wales and CSIRO’s Data61</affiliation></author>
      <author><first>Ricardo</first><last>Henao</last><affiliation>Duke University and King Abdullah University of Science and Technology</affiliation></author>
      <pages>4601-4616</pages>
      <abstract>In this paper, we study personalized federated learning for text classification with Pretrained Language Models (PLMs). We identify two challenges in efficiently leveraging PLMs for personalized federated learning: 1) Communication. PLMs are usually large in size, e.g., with hundreds of millions of parameters, inducing huge communication cost in a federated setting. 2) Local Training. Training with PLMs generally requires back-propagation, during which memory consumption can be several times that of the forward-propagation. This may not be affordable when the PLMs are trained locally on the clients that are resource constrained, e.g., mobile devices with limited access to memory resources. Additionally, the proprietary PLMs can be provided as concealed APIs, for which the back-propagation operations may not be available. In solving these, we propose a training framework that includes an approach of discrete local search for gradient-free local training, along with a compression mechanism inspired from the linear word analogy that allows communicating with discretely indexed tokens, thus significantly reducing the communication cost. Experiments show that our gradient-free framework achieves superior performance compared with baselines.</abstract>
      <url hash="c7c06b47">2024.findings-naacl.287</url>
    </paper>
    <paper id="288">
      <title><fixed-case>SGSH</fixed-case>: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation</title>
      <author><first>Shasha</first><last>Guo</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Yanling</first><last>Wang</last><affiliation>Zhongguancun Laboratory</affiliation></author>
      <author><first>Cuiping</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>4617-4629</pages>
      <abstract>Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH — a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates “skeleton heuristics”, which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input.Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.</abstract>
      <url hash="b39a1180">2024.findings-naacl.288</url>
    </paper>
    <paper id="289">
      <title>Biomedical Entity Representation with Graph-Augmented Multi-Objective Transformer</title>
      <author><first>Andrey</first><last>Sakhovskiy</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Natalia</first><last>Semenova</last></author>
      <author><first>Artur</first><last>Kadurin</last><affiliation>Artificial Intelligence Research Institute and Kuban State University</affiliation></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <pages>4630-4647</pages>
      <abstract>Modern biomedical concept representations are mostly trained on synonymous concept names from a biomedical knowledge base, ignoring the inter-concept interactions and a concept’s local neighborhood in a knowledge base graph. In this paper, we introduce Biomedical Entity Representation with a Graph-Augmented Multi-Objective Transformer (BERGAMOT), which adopts the power of pre-trained language models (LMs) and graph neural networks to capture both inter-concept and intra-concept interactions from the multilingual UMLS graph. To obtain fine-grained graph representations, we introduce two additional graph-based objectives: (i) a node-level contrastive objective and (ii) the Deep Graph Infomax (DGI) loss, which maximizes the mutual information between a local subgraph and a high-level graph summary. We apply contrastive loss on textual and graph representations to make them less sensitive to surface forms and enable intermodal knowledge exchange. BERGAMOT achieves state-of-the-art results in zero-shot entity linking without task-specific supervision on 4 of 5 languages of the Mantra corpus and on 8 of 10 languages of the XL-BEL benchmark.</abstract>
      <url hash="b2a779ae">2024.findings-naacl.289</url>
    </paper>
    <paper id="290">
      <title>Cross-Lingual Summarization with Pseudo-Label Regularization</title>
      <author><first>Thang</first><last>Le</last><affiliation>VinAI Research</affiliation></author>
      <pages>4648-4681</pages>
      <abstract>Cross-Lingual Summarization (XLS) aims to summarize a document in the source language into a condensed version in the target language, effectively removing language barriers for non-native readers. Previous approaches, however, have the same limitation that only a single reference (gold summary) is exploited during model training, making the base model exposed to an underrepresented hypothesis space since the actual number of possible hypotheses is exponentially large. To alleviate this problem, we present a study adopting pseudo-labels in regularizing standard cross-lingual summarization training. We investigate several components leading to the gains in regularization training with verified experiments involving 8 diverse languages from different families. Conclusively, we show that pseudo-labeling is a simple and effective approach that significantly improves over standard gold reference training in XLS.</abstract>
      <url hash="e9538b27">2024.findings-naacl.290</url>
    </paper>
    <paper id="291">
      <title>On the Way to Gentle <fixed-case>AI</fixed-case> Counselor: Politeness Cause Elicitation and Intensity Tagging in Code-mixed <fixed-case>H</fixed-case>inglish Conversations for Social Good</title>
      <author><first>Priyanshu</first><last>Priya</last></author>
      <author><first>Gopendra</first><last>Singh</last></author>
      <author><first>Mauajama</first><last>Firdaus</last></author>
      <author><first>Jyotsna</first><last>Agrawal</last></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>IIT Patna</affiliation></author>
      <pages>4682-4700</pages>
      <abstract>Politeness is a multifaceted concept influenced by individual perceptions of what is considered polite or impolite. With this objective, we introduce a novel task - Politeness Cause Elicitation and Intensity Tagging (PCEIT). This task focuses on conversations and aims to identify the underlying reasons behind the use of politeness and gauge the degree of politeness conveyed. To address this objective, we create HING-POEM, a new conversational dataset in Hinglish (a blend of Hindi and English) for mental health and legal counseling of crime victims. The rationale for the domain selection lies in the paramount importance of politeness in mental health and legal counseling of crime victims to ensure a compassionate and cordial atmosphere for them. We enrich the HING-POEM dataset by annotating it with politeness labels, politeness causal spans, and intensity values at the level of individual utterances. In the context of the introduced PCEIT task, we present PAANTH (Politeness CAuse ElicitAion and INtensity Tagging in Hinglish), a comprehensive framework based on Contextual Enhanced Attentive Convolution Transformer. We conduct extensive quantitative and qualitative evaluations to establish the effectiveness of our proposed approach using the newly constructed dataset. Our approach is compared against state-of-the-art baselines, and these analyses help demonstrate the superiority of our method.</abstract>
      <url hash="7ac74af3">2024.findings-naacl.291</url>
    </paper>
    <paper id="292">
      <title>Leveraging Summarization for Unsupervised Dialogue Topic Segmentation</title>
      <author><first>Aleksei</first><last>Artemiev</last></author>
      <author><first>Daniil</first><last>Parinov</last><affiliation>Yandex</affiliation></author>
      <author><first>Alexey</first><last>Grishanov</last></author>
      <author><first>Ivan</first><last>Borisov</last></author>
      <author><first>Alexey</first><last>Vasilev</last><affiliation>Sber, AI Lab</affiliation></author>
      <author><first>Daniil</first><last>Muravetskii</last></author>
      <author><first>Aleksey</first><last>Rezvykh</last></author>
      <author><first>Aleksei</first><last>Goncharov</last><affiliation>MIL Team</affiliation></author>
      <author><first>Andrey</first><last>Savchenko</last><affiliation>Sber AI Lab and HSE University</affiliation></author>
      <pages>4701-4708</pages>
      <abstract>Traditional approaches to dialogue segmentation perform reasonably well on synthetic or written dialogues but suffer when dealing with spoken, noisy dialogs. In addition, such methods require careful tuning of hyperparameters. We propose to leverage a novel approach that is based on dialogue summaries. Experiments on different datasets showed that the new approach outperforms popular state-of-the-art algorithms in unsupervised topic segmentation and requires less setup.</abstract>
      <url hash="c0d0abdd">2024.findings-naacl.292</url>
    </paper>
    <paper id="293">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-Rider: Spurring Large Language Models to Explore the Open World</title>
      <author><first>Yicheng</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Jiazheng</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Sipeng</first><last>Zheng</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>4709-4728</pages>
      <abstract>Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments and try to align the LLMs’ knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model’s performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences. By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM’s ability to accomplish more tasks through fine-tuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning. The code is available at https://github.com/PKU-RL/LLaMA-Rider.</abstract>
      <url hash="ac80aa97">2024.findings-naacl.293</url>
    </paper>
    <paper id="294">
      <title>Contrastive Learning as a Polarizer: Mitigating Gender Bias by Fair and Biased sentences</title>
      <author><first>Kyungmin</first><last>Park</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <author><first>Sihyun</first><last>Oh</last></author>
      <author><first>Daehyun</first><last>Kim</last></author>
      <author><first>Juae</first><last>Kim</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <pages>4729-4740</pages>
      <abstract>Recently, language models have accelerated the improvement in natural language processing. However, recent studies have highlighted a significant issue: social biases inherent in training data can lead models to learn and propagate these biases. In this study, we propose a contrastive learning method for bias mitigation, utilizing anchor points to push further negatives and pull closer positives within the representation space. This approach employs stereotypical data as negatives and stereotype-free data as positives, enhancing debiasing performance. Our model attained state-of-the-art performance in the ICAT score on the StereoSet, a benchmark for measuring bias in models. In addition, we observed that effective debiasing is achieved through an awareness of biases, as evidenced by improved hate speech detection scores. The implementation code and trained models are available at https://github.com/HUFS-NLP/CL_Polarizer.git.</abstract>
      <url hash="23285944">2024.findings-naacl.294</url>
    </paper>
    <paper id="295">
      <title><fixed-case>P</fixed-case>o<fixed-case>LLM</fixed-case>graph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics</title>
      <author><first>Derui</first><last>Zhu</last></author>
      <author><first>Dingfan</first><last>Chen</last><affiliation>CISPA, saarland university, saarland informatics campus</affiliation></author>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zongxiong</first><last>Chen</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <author><first>Jens</first><last>Grossklags</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>4741-4755</pages>
      <abstract>Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of "<tex-math>\textit{hallucination}</tex-math>”, where the model fabricates facts and produces non-factual statements. In response, we propose <tex-math>\texttt{PoLLMgraph}</tex-math>—a Polygraph for LLMs—as an effective model-based white-box detection and forecasting approach. <tex-math>\texttt{PoLLMgraph}</tex-math> distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM’s internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of <tex-math>\texttt{PoLLMgraph}</tex-math>, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.</abstract>
      <url hash="5b2623ba">2024.findings-naacl.295</url>
    </paper>
    <paper id="296">
      <title>Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>4756-4767</pages>
      <abstract>In today’s digital world, seeking answers to health questions on the Internet is a common practice. However, existing question answering (QA) systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel questions. Our study focuses on the open-domain QA setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed as a trustworthy collection of medical research documents, we answer health questions from three diverse datasets. We modify different retrieval settings to observe their influence on the QA pipeline’s performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.</abstract>
      <url hash="2e07febe">2024.findings-naacl.296</url>
    </paper>
    <paper id="297">
      <title><fixed-case>D</fixed-case>ecoder<fixed-case>L</fixed-case>ens: Layerwise Interpretation of Encoder-Decoder Transformers</title>
      <author><first>Anna</first><last>Langedijk</last><affiliation>Utrecht University (ICS), Utrecht University and University of Amsterdam</affiliation></author>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Gabriele</first><last>Sarti</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Willem</first><last>Zuidema</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <pages>4768-4784</pages>
      <abstract>In recent years, several interpretability methods have been proposed to interpret the inner workings of Transformer models at different levels of precision and complexity.In this work, we propose a simple but effective technique to analyze encoder-decoder Transformers. Our method, which we name DecoderLens, allows the decoder to cross-attend representations of intermediate encoder activations instead of using the default final encoder output.The method thus maps uninterpretable intermediate vector representations to human-interpretable sequences of words or symbols, shedding new light on the information flow in this popular but understudied class of models.We apply DecoderLens to question answering, logical reasoning, speech recognition and machine translation models, finding that simpler subtasks are solved with high precision by low and intermediate encoder layers.</abstract>
      <url hash="2ba55275">2024.findings-naacl.297</url>
    </paper>
  </volume>
</collection>
