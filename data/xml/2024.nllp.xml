<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nllp">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Natural Legal Language Processing Workshop 2024</booktitle>
      <editor><first>Nikolaos</first><last>Aletras</last></editor>
      <editor><first>Ilias</first><last>Chalkidis</last></editor>
      <editor><first>Leslie</first><last>Barrett</last></editor>
      <editor><first>Cătălina</first><last>Goanță</last></editor>
      <editor><first>Daniel</first><last>Preoțiuc-Pietro</last></editor>
      <editor><first>Gerasimos</first><last>Spanakis</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, FL, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="7bfed25e">2024.nllp-1</url>
      <venue>nllp</venue>
      <doi>10.18653/v1/2024.nllp-1</doi>
    </meta>
    <frontmatter>
      <url hash="7177692e">2024.nllp-1.0</url>
      <bibkey>nllp-2024-1</bibkey>
      <doi>10.18653/v1/2024.nllp-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>L</fixed-case>e<fixed-case>G</fixed-case>en: Complex Information Extraction from Legal sentences using Generative Models</title>
      <author><first>Chaitra</first><last>C R</last><affiliation>BITS Pilani, Hyderabad Campus</affiliation></author>
      <author><first>Sankalp</first><last>Kulkarni</last><affiliation>BITS Hyderabad</affiliation></author>
      <author><first>Sai Rama Akash Varma</first><last>Sagi</last><affiliation>Birla Institute of Technology and Science, Pilani - Hyderabad Campus</affiliation></author>
      <author><first>Shashank</first><last>Pandey</last><affiliation>BITS Pilani, Hyderabad</affiliation></author>
      <author><first>Rohit</first><last>Yalavarthy</last><affiliation>BITS Pilani, Hyderabad Campus</affiliation></author>
      <author><first>Dipanjan</first><last>Chakraborty</last><affiliation>BITS Pilani, Hyderabad</affiliation></author>
      <author><first>Prajna Devi</first><last>Upadhyay</last><affiliation>BITS Pilani Hyderabad</affiliation></author>
      <pages>1-17</pages>
      <abstract>Constructing legal knowledge graphs from unstructured legal texts is a complex challenge due to the intricate nature of legal language. While open information extraction (OIE) techniques can convert text into triples of the form subject, relation, object, they often fall short of capturing the nuanced relationships within lengthy legal sentences, necessitating more sophisticated approaches known as complex information extraction. This paper proposes <tex-math>LeGen</tex-math> – an end-to-end approach leveraging pre-trained large language models (GPT-4o, T5, BART) to perform complex information extraction from legal sentences. <tex-math>LeGen</tex-math> learns and represents the discourse structure of legal sentences, capturing both their complexity and semantics. It minimizes error propagation typical in multi-step pipelines and achieves up to a 32.2% gain on the Indian Legal benchmark. Additionally, it demonstrates competitive performance on open information extraction benchmarks. A promising application of the resulting legal knowledge graphs is in developing question-answering systems for government schemes, tailored to the Next Billion Users who struggle with the complexity of legal language. Our code and data are available at https://github.com/prajnaupadhyay/LegalIE</abstract>
      <url hash="90d7a6cc">2024.nllp-1.1</url>
      <bibkey>c-r-etal-2024-legen</bibkey>
      <doi>10.18653/v1/2024.nllp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Summarizing Long Regulatory Documents with a Multi-Step Pipeline</title>
      <author><first>Mika</first><last>Sie</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Ruby</first><last>Beek</last><affiliation>Power2X</affiliation></author>
      <author><first>Michiel</first><last>Bots</last><affiliation>Power2X</affiliation></author>
      <author><first>Sjaak</first><last>Brinkkemper</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <pages>18-32</pages>
      <abstract>Due to their length and complexity, long regulatory texts are challenging to summarize. To address this, a multi-step extractive-abstractive architecture is proposed to handle lengthy regulatory documents more effectively. In this paper, we show that the effectiveness of a two-step architecture for summarizing long regulatory texts varies significantly depending on the model used. Specifically, the two-step architecture improves the performance of decoder-only models. For abstractive encoder-decoder models with short context lengths, the effectiveness of an extractive step varies, whereas for long-context encoder-decoder models, the extractive step worsens their performance. This research also highlights the challenges of evaluating generated texts, as evidenced by the differing results from human and automated evaluations. Most notably, human evaluations favoured language models pretrained on legal text, while automated metrics rank general-purpose language models higher. The results underscore the importance of selecting the appropriate summarization strategy based on model architecture and context length.</abstract>
      <url hash="c9065a3f">2024.nllp-1.2</url>
      <bibkey>sie-etal-2024-summarizing</bibkey>
      <doi>10.18653/v1/2024.nllp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Enhancing Legal Expertise in Large Language Models through Composite Model Integration: The Development and Evaluation of Law-Neo</title>
      <author><first>Zhihao</first><last>Liu</last><affiliation>Shandong University of Finance and Economics</affiliation></author>
      <author><first>Yanzhen</first><last>Zhu</last><affiliation>Shandong University of Finance and Economics</affiliation></author>
      <author><first>Mengyuan</first><last>Lu</last><affiliation>Shandong University of Finance and Economics</affiliation></author>
      <pages>33-41</pages>
      <abstract>Although large language models (LLMs) like ChatGPT have demonstrated considerable capabilities in general domains, they often lack proficiency in specialized fields. Enhancing a model’s performance in a specific domain, such as law, while maintaining low costs, has been a significant challenge. Existing methods, such as fine-tuning or building mixture of experts (MoE) models, often struggle to balance model parameters, training costs, and domain-specific performance. Inspired by composition to augment language models, we have developed Law-Neo, a novel model designed to enhance legal LLMs. This model significantly improves the model’s legal domain expertise at minimal training costs, while retaining the logical capabilities of a large-scale anchor model. Our Law-Neo model outperformed other models in comprehensive experiments on multiple legal task benchmarks, demonstrating the effectiveness of this approach.</abstract>
      <url hash="ffbb43a1">2024.nllp-1.3</url>
      <bibkey>liu-etal-2024-enhancing-legal</bibkey>
      <doi>10.18653/v1/2024.nllp-1.3</doi>
    </paper>
    <paper id="4">
      <title>u<fixed-case>O</fixed-case>ttawa at <fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens-2024: Transformer-based Classification Experiments</title>
      <author><first>Nima</first><last>Meghdadi</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <pages>42-47</pages>
      <abstract>This paper presents the methods used for LegalLens-2024, which focused on detecting legal violations within unstructured textual data and associating these violations with potentially affected individuals. The shared task included two subtasks: A) Legal Named Entity Recognition (L-NER) and B) Legal Natural Language Inference (L-NLI). For subtask A, we utilized the spaCy library, while for subtask B, we employed a combined model incorporating RoBERTa and CNN. Our results were 86.3% in the L-NER subtask and 88.25% in the L-NLI subtask. Overall, our paper demonstrates the effectiveness of transformer models in addressing complex tasks in the legal domain.</abstract>
      <url hash="c6ee1e20">2024.nllp-1.4</url>
      <bibkey>meghdadi-inkpen-2024-uottawa</bibkey>
      <doi>10.18653/v1/2024.nllp-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>Q</fixed-case>uebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation</title>
      <author><first>David</first><last>Beauchemin</last><affiliation>Universite Laval</affiliation></author>
      <author><first>Richard</first><last>Khoury</last><affiliation>Université Laval</affiliation></author>
      <author><first>Zachary</first><last>Gagnon</last><affiliation>Université Laval</affiliation></author>
      <pages>48-60</pages>
      <abstract>Large Language Models (LLMs) perform outstandingly in various downstream tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has been shown to improve performance for legal question answering (Nuruzzaman and Hussain, 2020; Louis et al., 2024). However, there are limited applications in insurance questions-answering, a specific type of legal document. This paper introduces two corpora: the Quebec Automobile Insurance Expertise Reference Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance Questions. Our study leverages both corpora to automatically and manually assess a GPT4-o, a state-of-the-art (SOTA) LLM, to answer Quebec automobile insurance questions. Our results demonstrate that, on average, using our expertise reference corpus generates better responses on both automatic and manual evaluation metrics. However, they also highlight that LLM QA is unreliable enough for mass utilization in critical areas. Indeed, our results show that between 5% to 13% of answered questions include a false statement that could lead to customer misunderstanding.</abstract>
      <url hash="3315a982">2024.nllp-1.5</url>
      <bibkey>beauchemin-etal-2024-quebec</bibkey>
      <doi>10.18653/v1/2024.nllp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era of Large Language Models</title>
      <author><first>Shubham Kumar</first><last>Nigam</last><affiliation>Indian Institute of Technology</affiliation></author>
      <author><first>Aniket</first><last>Deroy</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Subhankar</first><last>Maity</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>Dept. of Computer Science and Engineering, IIT Kanpur</affiliation></author>
      <pages>61-80</pages>
      <abstract>This study investigates judgment prediction in a realistic scenario within the context of Indian judgments, utilizing a range of transformer-based models, including InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and GPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are predicted at the point when a case is presented for a decision in court, using only the information available at that time, such as the facts of the case, statutes, precedents, and arguments. This approach mimics real-world conditions, where decisions must be made without the benefit of hindsight, unlike retrospective analyses often found in previous studies. For transformer models, we experiment with hierarchical transformers and the summarization of judgment facts to optimize input for these models. Our experiments with LLMs reveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust performance in judgment prediction. Furthermore, incorporating additional legal information, such as statutes and precedents, significantly improves the outcome of the prediction task. The LLMs also provide explanations for their predictions. To evaluate the quality of these predictions and explanations, we introduce two human evaluation metrics: Clarity and Linking. Our findings from both automatic and human evaluations indicate that, despite advancements in LLMs, they are yet to achieve expert-level performance in judgment prediction and explanation tasks.</abstract>
      <url hash="14ca6b39">2024.nllp-1.6</url>
      <bibkey>nigam-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.nllp-1.6</doi>
    </paper>
    <paper id="7">
      <title>The <fixed-case>CLC</fixed-case>-<fixed-case>UKET</fixed-case> Dataset: Benchmarking Case Outcome Prediction for the <fixed-case>UK</fixed-case> Employment Tribunal</title>
      <author><first>Huiyuan</first><last>Xie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Felix</first><last>Steffek</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Joana</first><last>De Faria</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Christine</first><last>Carter</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Jonathan</first><last>Rutherford</last><affiliation>University of Cambridge</affiliation></author>
      <pages>81-96</pages>
      <abstract>This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.</abstract>
      <url hash="bbb2457e">2024.nllp-1.7</url>
      <bibkey>xie-etal-2024-clc</bibkey>
      <doi>10.18653/v1/2024.nllp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Information Extraction for Planning Court Cases</title>
      <author><first>Drish</first><last>Mali</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Rubash</first><last>Mali</last><affiliation>Himalaya College Of Engineering</affiliation></author>
      <author><first>Claire</first><last>Barale</last><affiliation>School of Informatics, University of Edinburgh</affiliation></author>
      <pages>97-114</pages>
      <abstract>Legal documents are often long and unstructured, making them challenging and time-consuming to apprehend. An automatic system that can identify relevant entities and labels within legal documents, would significantly reduce the legal research time. We developed a system to streamline legal case analysis from planning courts by extracting key information from XML files using Named Entity Recognition (NER) and multi-label classification models to convert them into structured form. This research contributes three novel datasets for the Planning Court cases: a NER dataset, a multi-label dataset fully annotated by humans, and newly re-annotated multi-label datasets partially annotated using LLMs. We experimented with various general-purpose and legal domain-specific models with different maximum sequence lengths. It was noted that incorporating paragraph position information improved the performance of models for the multi-label classification task. Our research highlighted the importance of domain-specific models, with LegalRoBERTa and LexLM demonstrating the best performance.</abstract>
      <url hash="2462d79d">2024.nllp-1.8</url>
      <bibkey>mali-etal-2024-information</bibkey>
      <doi>10.18653/v1/2024.nllp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Automated Anonymization of Parole Hearing Transcripts</title>
      <author><first>Abed</first><last>Itani</last><affiliation>University of Passau</affiliation></author>
      <author><first>Wassiliki</first><last>Siskou</last><affiliation>University of Konstanz</affiliation></author>
      <author><first>Annette</first><last>Hautli-Janisz</last><affiliation>University of Passau</affiliation></author>
      <pages>115-128</pages>
      <abstract>Responsible natural language processing is more and more concerned with preventing the violation of personal rights that language technology can entail (CITATION). In this paper we illustrate the case of parole hearings in California, the verbatim transcripts of which are made available to the general public upon a request sent to the California Board of Parole Hearings. The parole hearing setting is highly sensitive: inmates face a board of legal representatives who discuss highly personal matters not only about the inmates themselves but also about victims and their relatives, such as spouses and children. Participants have no choice in contributing to the data collection process, since the disclosure of the transcripts is mandated by law. As researchers who are interested in understanding and modeling the communication in these hierarchy-driven settings, we face an ethical dilemma: publishing raw data as is for the community would compromise the privacy of all individuals affected, but manually cleaning the data requires a substantive effort. In this paper we present an automated anonymization process which reliably removes and pseudonymizes sensitive data in verbatim transcripts, while at the same time preserving the structure and content of the data. Our results show that the process exhibits little to no leakage of sensitive information when applied to more than 300 hearing transcripts.</abstract>
      <url hash="72218f74">2024.nllp-1.9</url>
      <bibkey>itani-etal-2024-automated</bibkey>
      <doi>10.18653/v1/2024.nllp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Towards an Automated Pointwise Evaluation Metric for Generated Long-Form Legal Summaries</title>
      <author><first>Shao Min</first><last>Tan</last><affiliation>Thomson Reuters Labs</affiliation></author>
      <author><first>Quentin</first><last>Grail</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Lee</first><last>Quartey</last><affiliation>Thomson Reuters</affiliation></author>
      <pages>129-142</pages>
      <abstract>Long-form abstractive summarization is a task that has particular importance in the legal domain. Automated evaluation metrics are important for the development of text generation models, but existing research on the evaluation of generated summaries has focused mainly on short summaries. We introduce an automated evaluation methodology for generated long-form legal summaries, which involves breaking each summary into individual points, comparing the points in a human-written and machine-generated summary, and calculating a recall and precision score for the latter. The method is designed to be particularly suited for the complexities of legal text, and is also fully interpretable. We also create and release a small meta-dataset for the benchmarking of evaluation methods, focusing on long-form legal summarization. Our evaluation metric corresponds better with human evaluation compared to existing metrics which were not developed for legal data.</abstract>
      <url hash="c073e32a">2024.nllp-1.10</url>
      <bibkey>tan-etal-2024-towards-automated</bibkey>
      <doi>10.18653/v1/2024.nllp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Enhancing Contract Negotiations with <fixed-case>LLM</fixed-case>-Based Legal Document Comparison</title>
      <author><first>Savinay</first><last>Narendra</last><affiliation>JP Morgan Chase &amp; Co.</affiliation></author>
      <author><first>Kaushal</first><last>Shetty</last><affiliation>JP Morgan Chase</affiliation></author>
      <author><first>Adwait</first><last>Ratnaparkhi</last><affiliation>JPMorganChase</affiliation></author>
      <pages>143-153</pages>
      <abstract>We present a large language model (LLM) based approach for comparing legal contracts with their corresponding template documents. Legal professionals use commonly observed deviations between templates and contracts to help with contract negotiations, and also to refine the template documents. Our comparison approach, based on the well-studied natural language inference (NLI) task, first splits a template into key concepts and then uses LLMs to decide if the concepts are entailed by the contract document. We also repeat this procedure in the opposite direction - contract clauses are tested for entailment against the template clause to see if they contain additional information. The non-entailed concepts are labelled, organized and filtered by frequency, and placed into a clause library, which is used to suggest changes to the template documents. We first show that our LLM-based approach outperforms all previous work on a publicly available dataset designed for NLI in the legal domain. We then apply it to a private real-world legal dataset, achieve an accuracy of 96.46%. Our approach is the first in the literature to produce a natural language comparison between legal contracts and their template documents.</abstract>
      <url hash="37c3f127">2024.nllp-1.11</url>
      <bibkey>narendra-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.nllp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Attributed Question Answering for Preconditions in the <fixed-case>D</fixed-case>utch Law</title>
      <author><first>Felicia</first><last>Redelaar</last><affiliation>Leiden University/TNO</affiliation></author>
      <author><first>Romy</first><last>Van Drie</last><affiliation>TNO</affiliation></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>LIACS, Leiden University</affiliation></author>
      <author><first>Maaike</first><last>De Boer</last><affiliation>TNO</affiliation></author>
      <pages>154-165</pages>
      <abstract>In this paper, we address the problem of answering questions about preconditions in the law, e.g. “When can the court terminate the guardianship of a natural person?”. When answering legal questions, it is important to attribute the relevant part of the law; we therefore not only generate answers but also references to law articles. We implement a retrieval augmented generation (RAG) pipeline for long-form answers based on the Dutch law, using several state-of-the-art retrievers and generators. For evaluating our pipeline, we create a dataset containing legal QA pairs with attributions. Our experiments show promising results on our extended version for the automatic evaluation metrics from the Automatic LLMs’ Citation Evaluation (ALCE) Framework and the G-EVAL Framework. Our findings indicate that RAG has significant potential in complex, citation-heavy domains like law, as it helps laymen understand legal preconditions and rights by generating high-quality answers with accurate attributions.</abstract>
      <url hash="7f8382ee">2024.nllp-1.12</url>
      <bibkey>redelaar-etal-2024-attributed</bibkey>
      <doi>10.18653/v1/2024.nllp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Algorithm for Automatic Legislative Text Consolidation</title>
      <author><first>Matias</first><last>Etcheverry</last><affiliation>Doctrine</affiliation></author>
      <author><first>Thibaud</first><last>Real-del-Sarte</last><affiliation>Doctrine</affiliation></author>
      <author><first>Pauline</first><last>Chavallard</last><affiliation>Doctrine</affiliation></author>
      <pages>166-175</pages>
      <abstract>This study introduces a method for automating the consolidation process in a legal context, a time-consuming task traditionally performed by legal professionals. We present a generative approach that processes legislative texts to automatically apply amendments. Our method employs light quantized generative model, finetuned with LoRA, to generate accurate and reliable amended texts. To the authors knowledge, this is the first time generative models are used on legislative text consolidation. Our dataset is publicly available on HuggingFace. Experimental results demonstrate a significant improvement in efficiency, offering faster updates to legal documents. A full automated pipeline of legislative text consolidation can be done in a few hours, with a success rate of more than 63% on a difficult bill.</abstract>
      <url hash="3b6a2e85">2024.nllp-1.13</url>
      <bibkey>etcheverry-etal-2024-algorithm</bibkey>
      <doi>10.18653/v1/2024.nllp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Measuring the Groundedness of Legal Question-Answering Systems</title>
      <author><first>Dietrich</first><last>Trautmann</last><affiliation>Center for Information and Language Processing, Ludwig Maximilian University of Munich</affiliation></author>
      <author><first>Natalia</first><last>Ostapuk</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Quentin</first><last>Grail</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Adrian</first><last>Pol</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Guglielmo</first><last>Bonifazi</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Shang</first><last>Gao</last><affiliation>Thomson Reuters</affiliation></author>
      <author><first>Martin</first><last>Gajek</last><affiliation>Thomson Reuters</affiliation></author>
      <pages>176-186</pages>
      <abstract>In high-stakes domains like legal question-answering, the accuracy and trustworthiness of generative AI systems are of paramount importance. This work presents a comprehensive benchmark of various methods to assess the groundedness of AI-generated responses, aiming to significantly enhance their reliability. Our experiments include similarity-based metrics and natural language inference models to evaluate whether responses are well-founded in the given contexts. We also explore different prompting strategies for large language models to improve the detection of ungrounded responses. We validated the effectiveness of these methods using a newly created grounding classification corpus, designed specifically for legal queries and corresponding responses from retrieval-augmented prompting, focusing on their alignment with source material. Our results indicate potential in groundedness classification of generated responses, with the best method achieving a macro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their latency to determine their suitability for real-world applications, as this step typically follows the generation process. This capability is essential for processes that may trigger additional manual verification or automated response regeneration. In summary, this study demonstrates the potential of various detection methods to improve the trustworthiness of generative AI in legal settings.</abstract>
      <url hash="5ba2f79b">2024.nllp-1.14</url>
      <bibkey>trautmann-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.nllp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Transductive Legal Judgment Prediction Combining <fixed-case>BERT</fixed-case> Embeddings with Delaunay-Based <fixed-case>GNN</fixed-case>s</title>
      <author><first>Hugo</first><last>Attali</last><affiliation>LIPN, Universite Sorbonne Nord</affiliation></author>
      <author><first>Nadi</first><last>Tomeh</last><affiliation>LIPN-CNRS, Université Sorbonne Paris Nord</affiliation></author>
      <pages>187-193</pages>
      <abstract>This paper presents a novel approach to legal judgment prediction by combining BERT embeddings with a Delaunay-based Graph Neural Network (GNN). Unlike inductive methods that classify legal documents independently, our transductive approach models the entire document set as a graph, capturing both contextual and relational information. This method significantly improves classification accuracy by enabling effective label propagation across connected documents. Evaluated on the Swiss-Judgment-Prediction (SJP) dataset, our model outperforms established baselines, including larger models with cross-lingual training and data augmentation techniques, while maintaining efficiency with minimal computational overhead.</abstract>
      <url hash="7441e233">2024.nllp-1.15</url>
      <bibkey>attali-tomeh-2024-transductive</bibkey>
      <doi>10.18653/v1/2024.nllp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Cross Examine: An Ensemble-based approach to leverage Large Language Models for Legal Text Analytics</title>
      <author><first>Saurav</first><last>Chowdhury</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author><first>Lipika</first><last>Dey</last><affiliation>Ashoka University</affiliation></author>
      <author><first>Suyog</first><last>Joshi</last><affiliation>Ashoka University</affiliation></author>
      <pages>194-204</pages>
      <abstract>Legal documents are complex in nature, describing a course of argumentative reasoning that is followed to settle a case. Churning through large volumes of legal documents is a daily requirement for a large number of professionals who need access to the information embedded in them. Natural language processing methods that help in document summarization with key information components, insight extraction and question answering play a crucial role in legal text processing. Most of the existing document analysis systems use supervised machine learning, which require large volumes of annotated training data for every different application and are expensive to build. In this paper we propose a legal text analytics pipeline using Large Language Models (LLM), which can work with little or no training data. For document summarization, we propose an iterative pipeline using retrieval augmented generation to ensure that the generated text remains contextually relevant. For question answering, we propose a novel ontology-driven ensemble approach similar to cross-examination that exploits questioning and verification principles. A knowledge graph, created with the extracted information, stores the key entities and relationships reflecting the repository content structure. A new dataset is created with Indian court documents related to bail applications for cases filed under Protection of Children from Sexual Offences (POCSO) Act, 2012 an Indian law to protect children from sexual abuse and offences. Analysis of insights extracted from the answers reveal patterns of crime and social conditions leading to those crimes, which are important inputs for social scientists as well as legal system.</abstract>
      <url hash="1b73d074">2024.nllp-1.16</url>
      <bibkey>chowdhury-etal-2024-cross</bibkey>
      <doi>10.18653/v1/2024.nllp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>LLM</fixed-case>s to the Rescue: Explaining <fixed-case>DSA</fixed-case> Statements of Reason with Platform’s Terms of Services</title>
      <author><first>Marco</first><last>Aspromonte</last><affiliation>Alma AI, Alma Mater Studiorum, University of Bologna</affiliation></author>
      <author><first>Andrea</first><last>Ferraris</last><affiliation>Unibo</affiliation></author>
      <author><first>Federico</first><last>Galli</last><affiliation>Alma AI, Alma Mater Studiorum, University of Bologna</affiliation></author>
      <author><first>Giuseppe</first><last>Contissa</last><affiliation>Alma AI, Alma Mater Studiorum, University of Bologna</affiliation></author>
      <pages>205-215</pages>
      <abstract>The Digital Services Act (DSA) requires online platforms in the EU to provide “statements of reason” (SoRs) when restricting user content, but their effectiveness in ensuring transparency is still debated due to vague and complex terms of service (ToS). This paper explores the use of NLP techniques, specifically multi-agent systems based on large language models (LLMs), to clarify SoRs by linking them to relevant ToS sections. Analysing SoRs from platforms like Booking.com, Reddit, and LinkedIn, our findings show that LLMs can enhance the interpretability of content moderation decisions, improving user understanding and engagement with DSA requirements.</abstract>
      <url hash="28ba476b">2024.nllp-1.17</url>
      <bibkey>aspromonte-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.nllp-1.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>BLT</fixed-case>: Can Large Language Models Handle Basic Legal Text?</title>
      <author><first>Andrew</first><last>Blair-Stanek</last><affiliation>University of Maryland Law School</affiliation></author>
      <author><first>Nils</first><last>Holzenberger</last><affiliation>Télécom Paris</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University / Microsoft</affiliation></author>
      <pages>216-232</pages>
      <abstract>We find that the best publicly available LLMs like GPT-4 and Claude currently perform poorly on basic legal text handling. This motivates the creation of a benchmark consisting of examples that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs’ poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning on our training set brings even a small model to near-perfect performance. This benchmark will be useful for fine-tuning LLMs for downstream legal tasks, as well as for tracking LLMs’ reliability as-is for basic legal tasks.</abstract>
      <url hash="ad13e97b">2024.nllp-1.18</url>
      <bibkey>blair-stanek-etal-2024-blt</bibkey>
      <doi>10.18653/v1/2024.nllp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Multi-Property Multi-Label Documents Metadata Recommendation based on Encoder Embeddings</title>
      <author><first>Nasredine</first><last>Cheniki</last><affiliation>Publications Office of the European Union</affiliation></author>
      <author><first>Vidas</first><last>Daudaravicius</last><affiliation>European Commission Joint Research Centre</affiliation></author>
      <author><first>Abdelfettah</first><last>Feliachi</last><affiliation>Publications Office of the European Union</affiliation></author>
      <author><first>Didier</first><last>Hardy</last><affiliation>Publications Office of the European Union</affiliation></author>
      <author><first>Marc Wilhelm</first><last>Küster</last><affiliation>Publications Office of the European Union</affiliation></author>
      <pages>233-242</pages>
      <abstract>The task of document classification, particularly multi-label classification, presents a significant challenge due to the complexity of assigning multiple relevant labels to each document. This complexity is further amplified in multi-property multi-label classification tasks, where documents must be categorized across various sets of labels. In this research, we introduce an innovative encoder embedding-driven approach to multi-property multi-label document classification that leverages semantic-text similarity and the reuse of pre-existing annotated data to enhance the efficiency and accuracy of the document annotation process. Our method requires only a single model for text similarity, eliminating the need for multiple property-specific classifiers and thereby reducing computational demands and simplifying deployment. We evaluate our approach through a prototype deployed for daily operations, which demonstrates superior performance over existing classification systems. Our contributions include improved accuracy without additional training, increased efficiency, and demonstrated effectiveness in practical applications. The results of our study indicate the potential of our approach to be applied across various domains requiring multi-property multi-label document classification, offering a scalable and adaptable solution for metadata annotation tasks.</abstract>
      <url hash="164fbc64">2024.nllp-1.19</url>
      <bibkey>cheniki-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.nllp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Comparative Study of Explainability Methods for Legal Outcome Prediction</title>
      <author><first>Ieva</first><last>Staliunaite</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Josef</first><last>Valvoda</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ken</first><last>Satoh</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>243-258</pages>
      <abstract>This paper investigates explainability in Natural Legal Language Processing (NLLP). We study the task of legal outcome prediction of the European Court of Human Rights cases in a ternary classification setup, where a language model is fine-tuned to predict whether an article has been claimed and violated (positive outcome), claimed but not violated (negative outcome) or not claimed at all (null outcome). Specifically, we experiment with three popular NLP explainability methods. Correlating the attribution scores of input-level methods (Integrated Gradients and Contrastive Explanations) with rationales from court rulings, we show that the correlations are very weak, with absolute values of Spearman and Kendall correlation coefficients ranging between 0.003 and 0.094. Furthermore, we use a concept-level interpretability method (Concept Erasure) with human expert annotations of legal reasoning, to show that obscuring legal concepts from the model representation has an insignificant effect on model performance (at most a decline of 0.26 F1). Therefore, our results indicate that automated legal outcome prediction models are not reliably grounded in legal reasoning.</abstract>
      <url hash="3e22f441">2024.nllp-1.20</url>
      <bibkey>staliunaite-etal-2024-comparative</bibkey>
      <doi>10.18653/v1/2024.nllp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Bonafide at <fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens 2024 Shared Task: Using Lightweight <fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a Based Encoder For Legal Violation Detection and Resolution</title>
      <author><first>Shikha</first><last>Bordia</last><affiliation>Individual Contributor</affiliation></author>
      <pages>259-266</pages>
      <abstract>In this work, we present two systems—Named Entity Resolution (NER) and Natural Language Inference (NLI)—for detecting legal violations within unstructured textual data and for associating these violations with potentially affected individuals, respectively. Both these systems are lightweight DeBERTa based encoders that outperform the LLM baselines. The proposed NER system achieved an F1 score of 60.01% on Subtask A of the LegalLens challenge, which focuses on identifying violations. The proposed NLI system achieved an F1 score of 84.73% on Subtask B of the LegalLens challenge, which focuses on resolving these violations by matching them with pre-existing legal complaints of class action cases. Our NER system ranked sixth and NLI system ranked fifth on the LegalLens leaderboard. We release the trained models and inference scripts.</abstract>
      <url hash="234b9b81">2024.nllp-1.21</url>
      <bibkey>bordia-2024-bonafide</bibkey>
      <doi>10.18653/v1/2024.nllp-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>LAR</fixed-case>-<fixed-case>ECHR</fixed-case>: A New Legal Argument Reasoning Task and Dataset for Cases of the <fixed-case>E</fixed-case>uropean Court of Human Rights</title>
      <author><first>Odysseas</first><last>Chlapanis</last><affiliation>Department of Informatics, Athens University of Economics and Business &amp; Archimedes Unit, Athena Research Center</affiliation></author>
      <author><first>Dimitris</first><last>Galanis</last><affiliation>Institute for Language and Speech Processing, Athena Research Center</affiliation></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <pages>267-279</pages>
      <abstract>We present Legal Argument Reasoning (LAR), a novel task designed to evaluate the legal reasoning capabilities of Large Language Models (LLMs). The task requires selecting the correct next statement (from multiple choice options) in a chain of legal arguments from court proceedings, given the facts of the case. We constructed a dataset (LAR-ECHR) for this task using cases from the European Court of Human Rights (ECHR). We evaluated seven general-purpose LLMs on LAR-ECHR and found that (a) the ranking of the models is aligned with that of LegalBench, an established US-based legal reasoning benchmark, even though LAR-ECHR is based on EU law, (b) LAR-ECHR distinguishes top models more clearly, compared to LegalBench, (c) even the best model (GPT-4o) obtains 75.8% accuracy on LAR-ECHR, indicating significant potential for further model improvement. The process followed to construct LAR-ECHR can be replicated with cases from other legal systems.</abstract>
      <url hash="5e225dc3">2024.nllp-1.22</url>
      <bibkey>chlapanis-etal-2024-lar</bibkey>
      <doi>10.18653/v1/2024.nllp-1.22</doi>
    </paper>
    <paper id="24">
      <title>Gaps or Hallucinations? Scrutinizing Machine-Generated Legal Analysis for Fine-grained Text Evaluations</title>
      <author><first>Abe</first><last>Hou</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>William</first><last>Jurayj</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Nils</first><last>Holzenberger</last><affiliation>Télécom Paris, Institut Polytechnique de Paris</affiliation></author>
      <author><first>Andrew</first><last>Blair-Stanek</last><affiliation>University of Maryland Law School / Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University / Microsoft / HLTCOE</affiliation></author>
      <pages>280-302</pages>
      <abstract>Large Language Models (LLMs) show promise as a writing aid for professionals performing legal analyses. However, LLMs can often hallucinate in this setting, in ways difficult to recognize by non-professionals and existing text evaluation metrics. In this work, we pose the question: when can machine-generated legal analysis be evaluated as acceptable? We introduce the neutral notion of gaps – as opposed to hallucinations in a strict erroneous sense – to refer to the difference between human-written and machine-generated legal analysis. Gaps do not always equate to invalid generation. Working with legal experts, we consider the CLERC generation task proposed in Hou et al. (2024b), leading to a taxonomy, a fine-grained detector for predicting gap categories, and an annotated dataset for automatic evaluation. Our best detector achieves 67% F1 score and 80% precision on the test set. Employing this detector as an automated metric on legal analysis generated by SOTA LLMs, we find around 80% contain hallucinations of different kinds.</abstract>
      <url hash="c4159aad">2024.nllp-1.24</url>
      <bibkey>hou-etal-2024-gaps</bibkey>
      <doi>10.18653/v1/2024.nllp-1.24</doi>
    </paper>
    <paper id="25">
      <title>Classify First, and Then Extract: Prompt Chaining Technique for Information Extraction</title>
      <author><first>Alice</first><last>Kwak</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Clayton</first><last>Morrison</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Derek</first><last>Bambauer</last><affiliation>University of Florida</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>303-317</pages>
      <abstract>This work presents a new task-aware prompt design and example retrieval approach for information extraction (IE) using a prompt chaining technique. Our approach divides IE tasks into two steps: (1) text classification to understand what information (e.g., entity or event types) is contained in the underlying text and (2) information extraction for the identified types. Initially, we use a large language model (LLM) in a few-shot setting to classify the contained information. The classification output is used to select the relevant prompt and retrieve the examples relevant to the input text. Finally, we ask a LLM to do the information extraction with the generated prompt. By evaluating our approach on legal IE tasks with two different LLMs, we demonstrate that the prompt chaining technique improves the LLM’s overall performance in a few-shot setting when compared to the baseline in which examples from all possible classes are included in the prompt. Our approach can be used in a low-resource setting as it does not require a large amount of training data. Also, it can be easily adapted to many different IE tasks by simply adjusting the prompts. Lastly, it provides a cost benefit by reducing the number of tokens in the prompt.</abstract>
      <url hash="ad969fa6">2024.nllp-1.25</url>
      <bibkey>kwak-etal-2024-classify</bibkey>
      <doi>10.18653/v1/2024.nllp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Augmenting Legal Decision Support Systems with <fixed-case>LLM</fixed-case>-based <fixed-case>NLI</fixed-case> for Analyzing Social Media Evidence</title>
      <author><first>Ram Mohan Rao</first><last>Kadiyala</last><affiliation>N/A</affiliation></author>
      <author><first>Siddartha</first><last>Pullakhandam</last><affiliation>University of Wisconsin, Milwaukee</affiliation></author>
      <author><first>Kanwal</first><last>Mehreen</last><affiliation>Traversaal.ai</affiliation></author>
      <author><first>Subhasya</first><last>Tippareddy</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Ashay</first><last>Srivastava</last><affiliation>University of Maryland</affiliation></author>
      <pages>318-325</pages>
      <abstract>This paper presents our system description and error analysis of our entry for NLLP 2024 shared task on Legal Natural Language Inference (L-NLI). The task required classifying these relationships as entailed, contradicted, or neutral, indicating any association between the review and the complaint. Our system emerged as the winning submission, significantly outperforming other entries with a substantial margin and demonstrating the effectiveness of our approach in legal text analysis. We provide a detailed analysis of the strengths and limitations of each model and approach tested, along with a thorough error analysis and suggestions for future improvements. This paper aims to contribute to the growing field of legal NLP by offering insights into advanced techniques for natural language inference in legal contexts, making it accessible to both experts and newcomers in the field.</abstract>
      <url hash="b86eeee8">2024.nllp-1.26</url>
      <bibkey>kadiyala-etal-2024-augmenting</bibkey>
      <doi>10.18653/v1/2024.nllp-1.26</doi>
    </paper>
    <paper id="27">
      <title>Empowering Air Travelers: A Chatbot for <fixed-case>C</fixed-case>anadian Air Passenger Rights</title>
      <author><first>Maksym</first><last>Taranukhin</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Sahithya</first><last>Ravi</last><affiliation>The University of British Columbia, Vancouver</affiliation></author>
      <author><first>Gabor</first><last>Lukacs</last><affiliation>Air Passenger Rights</affiliation></author>
      <author><first>Evangelos</first><last>Milios</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last><affiliation>University of British Columbia</affiliation></author>
      <pages>326-335</pages>
      <abstract>The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot’s usefulness and ease of use. Beyond the primary goal of providing accurate and timely information to air passengers regarding their rights, we hope that this system will also enable further research exploring the tradeoff between the user-friendly conversational interface of chatbots and the accuracy of retrieval systems.</abstract>
      <url hash="f0ee01fa">2024.nllp-1.27</url>
      <bibkey>taranukhin-etal-2024-empowering</bibkey>
      <doi>10.18653/v1/2024.nllp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Enhancing Legal Violation Identification with <fixed-case>LLM</fixed-case>s and Deep Learning Techniques: Achievements in the <fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens 2024 Competition</title>
      <author><first>Nguyen</first><last>Tan Minh</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <author><first>Duy</first><last>Ngoc Mai</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <author><first>Le</first><last>Xuan Bach</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <author><first>Nguyen</first><last>Huu Dung</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <author><first>Pham</first><last>Cong Minh</last><affiliation>VNU University of Engineering and Technology</affiliation></author>
      <author><first>Ha Thanh</first><last>Nguyen</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Thi Hai Yen</first><last>Vuong</last><affiliation>University of Engineering and Technology, Vietnam national university Hanoi</affiliation></author>
      <pages>336-345</pages>
      <abstract>LegalLens is a competition organized to encourage advancements in automatically detecting legal violations. This paper presents our solutions for two tasks Legal Named Entity Recognition (L-NER) and Legal Natural Language Inference (L-NLI). Our approach involves fine-tuning BERT-based models, designing methods based on data characteristics, and a novel prompting template for data augmentation using LLMs. As a result, we secured first place in L-NER and third place in L-NLI among thirty-six participants. We also perform error analysis to provide valuable insights and pave the way for future enhancements in legal NLP. Our implementation is available at https://github.com/lxbach10012004/legal-lens/tree/main</abstract>
      <url hash="28e7eef8">2024.nllp-1.28</url>
      <bibkey>tan-minh-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.nllp-1.28</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens 2024 Shared Task: Masala-chai Submission</title>
      <author><first>Khalid</first><last>Rajan</last><affiliation>Georgian</affiliation></author>
      <author><first>Royal</first><last>Sequiera</last><affiliation>Georgian</affiliation></author>
      <pages>346-354</pages>
      <abstract>In this paper, we present the masala-chai team’s participation in the LegalLens 2024 shared task and detail our approach to predicting legal entities and performing natural language inference (NLI) in the legal domain. We experimented with various transformer-based models, including BERT, RoBERTa, Llama 3.1, and GPT-4o. Our results show that state-of-the-art models like GPT-4o underperformed in NER and NLI tasks, even when using advanced techniques such as bootstrapping and prompt optimization. The best performance in NER (accuracy: 0.806, F1 macro: 0.701) was achieved with a fine-tuned RoBERTa model, while the highest NLI results (accuracy: 0.825, F1 macro: 0.833) came from a fine-tuned Llama 3.1 8B model. Notably, RoBERTa, despite having significantly fewer parameters than Llama 3.1 8B, delivered comparable results. We discuss key findings and insights from our experiments and provide our results and code for reproducibility and further analysis at https://github.com/rosequ/masala-chai</abstract>
      <url hash="57a6fb6f">2024.nllp-1.30</url>
      <bibkey>rajan-sequiera-2024-legallens</bibkey>
      <doi>10.18653/v1/2024.nllp-1.30</doi>
    </paper>
    <paper id="31">
      <title>Semantists at <fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens-2024: Data-efficient Training of <fixed-case>LLM</fixed-case>’s for Legal Violation Identification</title>
      <author><first>Kanagasabai</first><last>Rajaraman</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Hariram</first><last>Veeramani</last><affiliation>UCLA</affiliation></author>
      <pages>355-360</pages>
      <abstract>In this paper, we describe our system for LegalLens-2024 Shared Task on automatically identifying legal violations from unstructured text sources. We participate in Subtask B, called Legal Natural Language Inference (L-NLI), that aims to predict the relationship between a given premise summarizing a class action complaint and a hypothesis from an online media text, indicating any association between the review and the complaint. This task is challenging as it provides only limited labelled data. In our work, we adopt LLM based methods and explore various data-efficient learning approaches for maximizing performance. In the end, our best model employed an ensemble of LLM’s fine-tuned on the task-specific data, and achieved a Macro F1 score of 78.5% on test data, and ranked 2nd among all teams submissions.</abstract>
      <url hash="a07cb011">2024.nllp-1.31</url>
      <bibkey>rajaraman-veeramani-2024-semantists</bibkey>
      <doi>10.18653/v1/2024.nllp-1.31</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens Shared Task 2024: Legal Violation Identification in Unstructured Text</title>
      <author><first>Ben</first><last>Hagag</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Gil</first><last>Gil Semo</last><affiliation>TAU</affiliation></author>
      <author><first>Dor</first><last>Bernsohn</last><affiliation>HUJI</affiliation></author>
      <author><first>Liav</first><last>Harpaz</last><affiliation>Darrow</affiliation></author>
      <author><first>Pashootan</first><last>Vaezipoor</last><affiliation>Georgian</affiliation></author>
      <author><first>Rohit</first><last>Saha</last><affiliation>Georgian</affiliation></author>
      <author><first>Kyryl</first><last>Truskovskyi</last><affiliation>Scoreinforce Inc.</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>361-370</pages>
      <abstract>This paper presents the results of the LegalLens Shared Task, focusing on detecting legal violations within text in the wild across two sub-tasks: LegalLens-NER for identifying legal violation entities and LegalLens-NLI for associating these violations with relevant legal contexts and affected individuals. Using an enhanced LegalLens dataset covering labor, privacy, and consumer protection domains, 38 teams participated in the task. Our analysis reveals that while a mix of approaches was used, the top-performing teams in both tasks consistently relied on fine-tuning pre-trained language models, outperforming legal-specific models and few-shot methods. The top-performing team achieved a 7.11% improvement in NER over the baseline, while NLI saw a more marginal improvement of 5.7%. Despite these gains, the complexity of legal texts leaves room for further advancements.</abstract>
      <url hash="13b89526">2024.nllp-1.33</url>
      <bibkey>hagag-etal-2024-legallens</bibkey>
      <doi>10.18653/v1/2024.nllp-1.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a Beats Behemoths: A Comparative Analysis of Fine-Tuning, Prompting, and <fixed-case>PEFT</fixed-case> Approaches on <fixed-case>L</fixed-case>egal<fixed-case>L</fixed-case>ens<fixed-case>NER</fixed-case></title>
      <author><first>Hanh Thi Hong</first><last>Tran</last><affiliation>La Rochelle University</affiliation></author>
      <author><first>Nishan</first><last>Chatterjee</last><affiliation>Institut Jožef Stefan, University of La Rochelle</affiliation></author>
      <author><first>Senja</first><last>Pollak</last><affiliation>Jožef Stefan Institute</affiliation></author>
      <author><first>Antoine</first><last>Doucet</last><affiliation>University of La Rochelle</affiliation></author>
      <pages>371-380</pages>
      <abstract>This paper summarizes the participation of our team (Flawless Lawgic) in the legal named entity recognition (L-NER) task at LegalLens 2024: Detecting Legal Violations. Given possible unstructured texts (e.g., online media texts), we aim to identify legal violations by extracting legal entities such as “violation”, “violation by”, “violation on”, and “law”. This system-description paper discusses our approaches to address the task, empirically highlighting the performances of fine-tuning models from the Transformers family (e.g., RoBERTa and DeBERTa) against open-sourced LLMs (e.g., Llama, Mistral) with different tuning settings (e.g., LoRA, Supervised Fine-Tuning (SFT) and prompting strategies). Our best results, with a weighted F1 of 0.705 on the test set, show a 30 percentage points increase in F1 compared to the baseline and rank 2 on the leaderboard, leaving a marginal gap of only 0.4 percentage points lower than the top solution. Our solutions are available at github.com/honghanhh/lner.</abstract>
      <url hash="f902c5af">2024.nllp-1.34</url>
      <bibkey>tran-etal-2024-deberta</bibkey>
      <doi>10.18653/v1/2024.nllp-1.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>L</fixed-case>ex<fixed-case>S</fixed-case>umm and <fixed-case>L</fixed-case>ex<fixed-case>T</fixed-case>5: Benchmarking and Modeling Legal Summarization Tasks in <fixed-case>E</fixed-case>nglish</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Cornelius</first><last>Weiss</last><affiliation>Technical University Munich</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>381-403</pages>
      <abstract>In the evolving NLP landscape, benchmarks serve as yardsticks for gauging progress. However, existing Legal NLP benchmarks only focus on predictive tasks, overlooking generative tasks. This work curates LexSumm, a benchmark designed for evaluating legal summarization tasks in English. It comprises eight English legal summarization datasets, from diverse jurisdictions, such as the US, UK, EU and India. Additionally, we release LexT5, legal oriented sequence-to-sequence model, addressing the limitation of the existing BERT-style encoder-only models in the legal domain. We assess its capabilities through zero-shot probing on LegalLAMA and fine-tuning on LexSumm. Our analysis reveals abstraction and faithfulness errors even in summaries generated by zero-shot LLMs, indicating opportunities for further improvements. LexSumm benchmark and LexT5 model are available at https://github.com/TUMLegalTech/LexSumm-LexT5.</abstract>
      <url hash="3582ffae">2024.nllp-1.35</url>
      <bibkey>t-y-s-s-etal-2024-lexsumm</bibkey>
      <doi>10.18653/v1/2024.nllp-1.35</doi>
    </paper>
    <paper id="36">
      <title>Towards Supporting Legal Argumentation with <fixed-case>NLP</fixed-case>: Is More Data Really All You Need?</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Kevin</first><last>Ashley</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Katie</first><last>Atkinson</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>404-421</pages>
      <abstract>Modeling legal reasoning and argumentation justifying decisions in cases has always been central to AI &amp; Law, yet contemporary developments in legal NLP have increasingly focused on statistically classifying legal conclusions from text. While conceptually “simpler’, these approaches often fall short in providing usable justifications connecting to appropriate legal concepts. This paper reviews both traditional symbolic works in AI &amp; Law and recent advances in legal NLP, and distills possibilities of integrating expert-informed knowledge to strike a balance between scalability and explanation in symbolic vs. data-driven approaches. We identify open challenges and discuss the potential of modern NLP models and methods that integrate conceptual legal knowledge.</abstract>
      <url hash="32d447f0">2024.nllp-1.36</url>
      <bibkey>t-y-s-s-etal-2024-towards-supporting</bibkey>
      <doi>10.18653/v1/2024.nllp-1.36</doi>
    </paper>
  </volume>
</collection>
