<?xml version='1.0' encoding='UTF-8'?>
<collection id="2004.jeptalnrecital">
  <volume id="long" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Philippe</first><last>Blache</last></editor>
      <editor><first>Noël</first><last>Nguyen</last></editor>
      <editor><first>Nouredine</first><last>Chenfour</last></editor>
      <editor><first>Abdenbi</first><last>Rajouani</last></editor>
      <publisher>ATALA</publisher>
      <address>Fès, Maroc</address>
      <month>April</month>
      <year>2004</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="be674e6a">2004.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2004-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluation de méthodes de segmentation thématique linéaire non supervisées après adaptation au français</title>
      <author><first>Laurianne</first><last>Sitbon</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>1–10</pages>
      <abstract>Nous proposons une évaluation de différentes méthodes et outils de segmentation thématique de textes. Nous présentons les outils de segmentation linéaire et non supervisée DotPlotting, Segmenter, C99, TextTiling, ainsi qu’une manière de les adapter et de les tester sur des documents français. Les résultats des tests montrent des différences en performance notables selon les sujets abordés dans les documents, et selon que le nombre de segments à trouver est fixé au préalable par l’utilisateur. Ces travaux font partie du projet Technolangue AGILE-OURAL.</abstract>
      <url hash="32945c5b">2004.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>sitbon-bellot-2004-evaluation</bibkey>
    </paper>
    <paper id="2">
      <title>Ambiguïté de rattachement prépositionnel : introduction de ressources exogènes de sous-catégorisation dans un analyseur syntaxique de corpus endogène</title>
      <author><first>Didier</first><last>Bourigault</last></author>
      <author><first>Cécile</first><last>Frérot</last></author>
      <pages>11–20</pages>
      <abstract>Nous présentons les résultats d’expérimentations visant à introduire des ressources lexicosyntaxiques génériques dans un analyseur syntaxique de corpus à base endogène (SYNTEX) pour la résolution d’ambiguïtés de rattachement prépositionnel. Les données de souscatégorisation verbale sont élaborées à partir du lexique-grammaire et d’une acquisition en corpus (journal Le Monde). Nous présentons la stratégie endogène de désambiguïsation, avant d’y intégrer les ressources construites. Ces stratégies sont évaluées sur trois corpus (scientifique, juridique et journalistique). La stratégie mixte augmente le taux de rappel (+15% sur les trois corpus cumulés) sans toutefois modifier le taux de précision (~ 85%). Nous discutons ces performances, notamment à la lumière des résultats obtenus par ailleurs sur la préposition de.</abstract>
      <url hash="b289a6f3">2004.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>bourigault-frerot-2004-ambiguite</bibkey>
    </paper>
    <paper id="3">
      <title>Vers un statut de l’arbre de dérivation : exemples de construction de representations sémantiques pour les Grammaires d’Arbres Adjoints</title>
      <author><first>Sylvain</first><last>Pogodalla</last></author>
      <pages>21–30</pages>
      <abstract>Cet article propose une définition des arbres de dérivation pour les Grammaires d’Arbres Adjoints, étendant la notion habituelle. Elle est construite sur l’utilisation des Grammaires Catégorielles Abstraites et permet de manière symétrique le calcul de la représentation syntaxique (arbre dérivé) et le calcul de la représentation sémantique.</abstract>
      <url hash="cab824a3">2004.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>pogodalla-2004-vers</bibkey>
    </paper>
    <paper id="4">
      <title>Extension de requêtes par lien sémantique nom-verbe acquis sur corpus</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>31–40</pages>
      <abstract>En recherche d’information, savoir reformuler une idée par des termes différents est une des clefs pour l’amélioration des performances des systèmes de recherche d’information (SRI) existants. L’un des moyens pour résoudre ce problème est d’utiliser des ressources sémantiques spécialisées et adaptées à la base documentaire sur laquelle les recherches sont faites. Nous proposons dans cet article de montrer que les liens sémantiques entre noms et verbes appelés liens qualia, définis dans le modèle du Lexique génératif (Pustejovsky, 1995), peuvent effectivement améliorer les résultats des SRI. Pour cela, nous extrayons automatiquement des couples nom-verbe en relation qualia de la base documentaire à l’aide du système d’acquisition ASARES (Claveau, 2003a). Ces couples sont ensuite utilisés pour étendre les requêtes d’un système de recherche. Nous montrons, à l’aide des données de la campagne d’évaluation Amaryllis, que cette extension permet effectivement d’obtenir des réponses plus pertinentes, et plus particulièrement pour les premiers documents retournés à l’utilisateur.</abstract>
      <url hash="e0790521">2004.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>claveau-sebillot-2004-extension</bibkey>
    </paper>
    <paper id="5">
      <title>Découvrir des sens de mots à partir d’un réseau de cooccurrences lexicales</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>41–50</pages>
      <abstract>Les réseaux lexico-sémantiques de type WordNet ont fait l’objet de nombreuses critiques concernant la nature des sens qu’ils distinguent ainsi que la façon dont ils caractérisent ces distinctions de sens. Cet article présente une solution possible à ces limites, solution consistant à définir les sens des mots à partir de leur usage. Plus précisément, il propose de différencier les sens d’un mot à partir d’un réseau de cooccurrences lexicales construit sur la base d’un large corpus. Cette méthode a été testée à la fois pour le français et pour l’anglais et a fait l’objet dans ce dernier cas d’une première évaluation par comparaison avec WordNet.</abstract>
      <url hash="5427aaf6">2004.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>ferret-2004-decouvrir</bibkey>
    </paper>
    <paper id="6">
      <title>Désambiguïsation par proximité structurelle</title>
      <author><first>Bruno</first><last>Gaume</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <pages>51–59</pages>
      <abstract>L’article présente une méthode de désambiguïsation dans laquelle le sens est déterminé en utilisant un dictionnaire. La méthode est basée sur un algorithme qui calcule une distance « sémantique » entre les mots du dictionnaire en prenant en compte la topologie complète du dictionnaire, vu comme un graphe sur ses entrées. Nous l’avons testée sur la désambiguïsation des définitions du dictionnaire elles-mêmes. L’article présente des résultats préliminaires, qui sont très encourageants pour une méthode ne nécessitant pas de corpus annoté.</abstract>
      <url hash="812f092d">2004.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>gaume-etal-2004-desambiguisation</bibkey>
    </paper>
    <paper id="7">
      <title>Fusionner pour mieux analyser : Conception et évaluation de la plate-forme de combinaison</title>
      <author><first>Francis</first><last>Brunet-Manquat</last></author>
      <pages>60–69</pages>
      <abstract>L’objectif de cet article est de présenter nos travaux concernant la combinaison d’analyseurs syntaxiques pour produire un analyseur plus robuste. Nous avons créé une plate-forme nous permettant de comparer des analyseurs syntaxiques pour une langue donnée en découpant leurs résultats en informations élémentaires, en les normalisant, et en les comparant aux résultats de référence. Cette même plate-forme est utilisée pour combiner plusieurs analyseurs pour produire un analyseur de dépendance plus couvrant et plus robuste. À long terme, il sera possible de “compiler” les connaissances extraites de plusieurs analyseurs dans un analyseur de dépendance autonome.</abstract>
      <url hash="b33b6abf">2004.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>brunet-manquat-2004-fusionner</bibkey>
    </paper>
    <paper id="8">
      <title>Un modèle d’acquisition de la syntaxe à l’aide d’informations sémantiques</title>
      <author><first>Daniela</first><last>Dudau Sofronie</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <pages>70–79</pages>
      <abstract>Nous présentons dans cet article un algorithme d’apprentissage syntaxico-sémantique du langage naturel. Les données de départ sont des phrases correctes d’une langue donnée, enrichies d’informations sémantiques. Le résultat est l’ensemble des grammaires formelles satisfaisant certaines conditions et compatibles avec ces données. La stratégie employée, validée d’un point de vue théorique, est testée sur un corpus de textes français constitué pour l’occasion.</abstract>
      <url hash="a144ad78">2004.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>dudau-sofronie-tellier-2004-un</bibkey>
    </paper>
    <paper id="9">
      <title>Catégorisation de patrons syntaxiques par Self Organizing Maps</title>
      <author><first>Jean-Jacques</first><last>Mariage</last></author>
      <author><first>Gilles</first><last>Bernard</last></author>
      <pages>80–89</pages>
      <abstract>Dans cet article, nous présentons quelques résultats en catégorisation automatique de données du langage naturel sans recours à des connaissances préalables. Le système part d’une liste de formes grammaticales françaises et en construit un graphe qui représente les chaînes rencontrées dans un corpus de textes de taille raisonnable ; les liens sont pondérés à partir de données statistiques extraites du corpus. Pour chaque chaîne de formes grammaticales significative, un vecteur reflétant sa distribution est extrait et passé à un réseau de neurones de type carte topologique auto-organisatrice. Une fois le processus d’apprentissage terminé, la carte résultante est convertie en un graphe d’étiquettes générées automatiquement, utilisé dans un tagger ou un analyseur de bas niveau. L’algorithme est aisément adaptable à toute langue dans la mesure où il ne nécessite qu’une liste de marques grammaticales et un corpus important (plus il est gros, mieux c’est). Il présente en outre un intérêt supplémentaire qui est son caractère dynamique : il est extrêmement aisé de recalculer les données à mesure que le corpus augmente.</abstract>
      <url hash="de9075df">2004.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>mariage-bernard-2004-categorisation</bibkey>
    </paper>
    <paper id="10">
      <title>Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel représentés sous la forme d’automates finis pondérés</title>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Alexandra</first><last>Volanschi</last></author>
      <pages>90–99</pages>
      <abstract>Cet article présente une manière d’intégrer un étiqueteur morpho-syntaxique et un analyseur partiel. Cette integration permet de corriger des erreurs effectuées par l’étiqueteur seul. L’étiqueteur et l’analyseur ont été réalisés sous la forme d’automates pondérés. Des résultats sur un corpus du français ont montré une dimintion du taux d’erreur de l’ordre de 12%.</abstract>
      <url hash="d5f08854">2004.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>nasr-volanschi-2004-couplage</bibkey>
    </paper>
    <paper id="11">
      <title>Deux premières étapes vers les documents auto-explicatifs</title>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Christian</first><last>Boitet</last></author>
      <pages>100–109</pages>
      <abstract>Dans le cadre du projet LIDIA, nous avons montré que dans de nombreuses situations, la TA Fondée sur le Dialogue (TAFD) pour auteur monolingue peut offrir une meilleure solution en traduction multicible que les aides aux traducteurs, ou la traduction avec révision, même si des langages contrôlés sont utilisés. Nos premières expériences ont mis en évidence le besoin de conserver les « intentions de l’auteur » au moyen « d’annotations de désambiguïsation ». Ces annotations permettent de transformer le document source en un Document Auto-Explicatif (DAE). Nous présentons ici une solution pour intégrer ces annotations dans un document XML et les rendre visibles et utilisables par un lecteur pour une meilleure compréhension du « vrai contenu » du document. Le concept de Document Auto-Explicatif pourrait changer profondément notre façon de comprendre des documents importants ou écrits dans un style complexe. Nous montrerons aussi qu’un DAE, traduit dans une langue cible L, pourrait aussi être transformé, sans interaction humaine, en un DAE en langue L si un analyseur et un désambiguïseur sont disponibles pour cette langue L. Ainsi, un DAE pourrait être utilisé dans un contexte monolingue, mais aussi dans un contexte multilingue sans travail humain additionnel.</abstract>
      <url hash="d2e2e600">2004.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>blanchon-boitet-2004-deux</bibkey>
    </paper>
    <paper id="12">
      <title>Interprétariat à distance et collecte de dialogues spontanés bilingues, sur une plate-forme générique multifonctionnelle</title>
      <author><first>Georges</first><last>Fafiotte</last></author>
      <pages>110–119</pages>
      <abstract>Parallèlement à l’intégration du français en TA de Parole multilingue (projets C-STAR, NESPOLE!), nous avons développé plusieurs plates-formes, dans le cadre des projets ERIM (Environnement Réseau pour l’Interprétariat Multimodal) et ChinFaDial (collecte de dialogues parlés spontanés français-chinois), pour traiter différents aspects de la communication orale spontanée bilingue non finalisée sur le web : interprétariat humain à distance, collecte de données, intégration d’aides automatiques (serveur de TA de Parole utilisant des composants du marché, interaction multimodale entre interlocuteurs, et prochainement aides en ligne aux intervenants, locuteurs ou interprètes). Les corpus collectés devraient être disponibles sur un site DistribDial au printemps 2004. Ces plates-formes sont en cours d’intégration, en un système générique multifonctionnel unique ERIMM d’aide à la communication multilingue multimodale, dont une variante s’étendra également à la formation à distance (e-training) à l’interprétariat.</abstract>
      <url hash="018bdfc0">2004.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>fafiotte-2004-interpretariat</bibkey>
    </paper>
    <paper id="13">
      <title>Extraction de terminologies bilingues à partir de corpus comparables</title>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Samuel</first><last>Dufour-Kowalski</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>120–129</pages>
      <abstract>Cet article présente une méthode pour extraire, à partir de corpus comparables d’un domaine de spécialité, un lexique bilingue comportant des termes simples et complexes. Cette méthode extrait d’abord les termes complexes dans chaque langue, puis les aligne à l’aide de méthodes statistiques exploitant le contexte des termes. Après avoir rappelé les difficultés que pose l’alignement des termes complexes et précisé notre approche, nous présentons le processus d’extraction de terminologies bilingues adopté et les ressources utilisées pour nos expérimentations. Enfin, nous évaluons notre approche et démontrons son intérêt en particulier pour l’alignement de termes complexes non compositionnels.</abstract>
      <url hash="38e5d4b9">2004.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>morin-etal-2004-extraction</bibkey>
    </paper>
    <paper id="14">
      <title>Traduction, traduction de mots, traduction de phrases</title>
      <author><first>Éric</first><last>Wehrli</last></author>
      <pages>130–138</pages>
      <abstract>Une des conséquences du développement d’Internet et de la globalisation des échanges est le nombre considérable d’individus amenés à consulter des documents en ligne dans une langue autre que la leur. Après avoir montré que ni la traduction automatique, ni les aides terminologiques en ligne ne constituent une réponse pleinement adéquate à ce nouveau besoin, cet article présente un système d’aide à la lecture en langue étrangère basé sur un analyseur syntaxique puissant. Pour un mot sélectionné par l’usager, ce système analyse la phrase entière, de manière (i) à choisir la lecture du mot sélectionné la mieux adaptée au contexte morphosyntaxique et (ii) à identifier une éventuelle expression idiomatique ou une collocation dont le mot serait un élément. Une démonstration de ce système, baptisé TWiC (Translation of words in context “Traduction de mots en contexte”), pourra être présentée.</abstract>
      <url hash="c47fcd19">2004.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>wehrli-2004-traduction</bibkey>
    </paper>
    <paper id="15">
      <title>Extraction d’information en domaine restreint pour la génération multilingue de résumés ciblés</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Caroline</first><last>Hagège</last></author>
      <pages>139–148</pages>
      <abstract>Dans cet article nous présentons une application de génération de résumés multilingues ciblés à partir de textes d’un domaine restreint. Ces résumés sont dits ciblés car ils sont produits d’après les spécifications d’un utilisateur qui doit décider a priori du type de l’information qu’il souhaite voir apparaître dans le résumé final. Pour mener à bien cette tâche, nous effectuons dans un premier temps l’extraction de l’information spécifiée par l’utilisateur. Cette information constitue l’entrée d’un système de génération multilingue qui produira des résumés normalisés en trois langues (anglais, français et espagnol) à partir d’un texte en anglais.</abstract>
      <url hash="ae059afc">2004.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>brun-hagege-2004-extraction</bibkey>
    </paper>
    <paper id="16">
      <title>Repérage et exploitation d’énoncés définitoires en corpus pour l’aide à la construction d’ontologie</title>
      <author><first>Véronique</first><last>Malaisé</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Bruno</first><last>Bachimont</last></author>
      <pages>149–158</pages>
      <abstract>Pour construire une ontologie, un modéliseur a besoin d’objecter des informations sémantiques sur les termes principaux de son domaine d’étude. Les outils d’exploration de corpus peuvent aider à repérer ces types d’information, et l’identification de couples d’hyperonymes a fait l’objet de plusieurs travaux. Nous proposons d’exploiter des énoncés définitoires pour extraire d’un corpus des informations concernant les trois axes de l’ossature ontologique : l’axe vertical, lié à l’hyperonymie, l’axe horizontal, lié à la co-hyponymie et l’axe transversal, lié aux relations du domaine. Après un rappel des travaux existants en repérage d’énoncés définitoires en TAL, nous développons la méthode que nous avons mise en place, puis nous présentons son évaluation et les premiers résultats obtenus. Leur repérage atteint de 10% à 69% de précision suivant les patrons, celui des unités lexicales varie de 31% à 56%, suivant le référentiel adopté.</abstract>
      <url hash="a7c328d6">2004.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>malaise-etal-2004-reperage</bibkey>
    </paper>
    <paper id="17">
      <title>Anonymisation de décisions de justice</title>
      <author><first>Luc</first><last>Plamondon</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <author><first>Frédéric</first><last>Pelletier</last></author>
      <pages>159–168</pages>
      <abstract>La publication de décisions de justice sur le Web permet de rendre la jurisprudence accessible au grand public, mais il existe des domaines du droit pour lesquels la Loi prévoit que l’identité de certaines personnes doit demeurer confidentielle. Nous développons actuellement un système d’anonymisation automatique à l’aide de l’environnement de développement GATE. Le système doit reconnaître certaines entités nommées comme les noms de personne, les lieux et les noms d’entreprise, puis déterminer automatiquement celles qui sont de nature à permettre l’identification des personnes visées par les restrictions légales à la publication.</abstract>
      <url hash="00a78f29">2004.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>plamondon-etal-2004-anonymisation</bibkey>
    </paper>
    <paper id="18">
      <title>Système d’aide à l’accès lexical : trouver le mot qu’on a sur le bout de la langue</title>
      <author><first>Gaëlle</first><last>Lortal</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Michael</first><last>Zock</last></author>
      <pages>169–178</pages>
      <abstract>Le Mot sur le Bout de la Langue (Tip Of the Tongue en anglais), phénomène très étudié par les psycholinguistes, nous a amené nombre d’informations concernant l’organisation du lexique mental. Un locuteur en état de TOT reconnaît instantanément le mot recherché présenté dans une liste. Il en connaît le sens, la forme, les liens avec d’autres mots... Nous présentons ici une étude de développement d’outil qui prend en compte ces spécificités, pour assister un locuteur/rédacteur à trouver le mot qu’il a sur le bout de la langue. Elle consiste à recréer le phénomène du TOT, où, dans un contexte de production un mot, connu par le système, est momentanément inaccessible. L’accès au mot se fait progressivement grâce aux informations provenant de bases de données linguistiques. Ces dernières sont essentiellement des relations de type paradigmatique et syntagmatique. Il s’avère qu’un outil, tel que SVETLAN, capable de structurer automatiquement un dictionnaire par domaine, peut être avantageusement combiné à une base de données riche en liens paradigmatiques comme EuroWordNet, augmentant considérablement les chances de trouver le mot auquel on ne peut accéder.</abstract>
      <url hash="bbf056ec">2004.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>lortal-etal-2004-systeme</bibkey>
    </paper>
    <paper id="19">
      <title>De l’écrit à l’oral : analyses et générations</title>
      <author><first>Fabrice</first><last>Maurel</last></author>
      <pages>179–188</pages>
      <abstract>Longtemps considérée comme ornementale, la structure informationnelle des documents écrits prise en charge par la morpho-disposition devient un objet d’étude à part entière dans diverses disciplines telles que la linguistique, la psycholinguistique ou l’informatique. En particulier, nous nous intéressons à l’utilité de cette dimension et, le cas échéant, son utilisabilité, dans le cadre de la transposition automatique à l’oral des textes. Dans l’objectif de fournir des solutions qui permettent de réagir efficacement à cette « inscription morphologique », nous proposons la synoptique d’un système d’oralisation. Nous avons modélisé et partiellement réalisé le module spécifique aux stratégies d’oralisation, afin de rendre « articulables » certaines parties signifiantes des textes souvent « oubliées » par les systèmes de synthèse. Les premiers résultats de cette étude ont conduit à des spécifications en cours d’intégration par un partenaire industriel. Les perspectives de ce travail peuvent intéresser la communauté TAL en reconnaissance de la parole, en génération/résumé de texte ou en multimodalité.</abstract>
      <url hash="d94ad148">2004.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>maurel-2004-de</bibkey>
    </paper>
    <paper id="20">
      <title>Désambiguïsation de corpus monolingues par des approches de type <fixed-case>L</fixed-case>esk</title>
      <author><first>Florentina</first><last>Vasilescu</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>189–198</pages>
      <abstract>Cet article présente une analyse détaillée des facteurs qui déterminent les performances des approches de désambiguïsation dérivées de la méthode de Lesk (1986). Notre étude porte sur une série d’expériences concernant la méthode originelle de Lesk et des variantes que nous avons adaptées aux caractéristiques de WORDNET. Les variantes implémentées ont été évaluées sur le corpus de test de SENSEVAL2, English All Words, ainsi que sur des extraits du corpus SEMCOR. Notre évaluation se base d’un côté, sur le calcul de la précision et du rappel, selon le modèle de SENSEVAL, et d’un autre côté, sur une taxonomie des réponses qui permet de mesurer la prise de risque d’un décideur par rapport à un système de référence.</abstract>
      <url hash="344cc098">2004.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>vasilescu-langlais-2004-desambiguisation</bibkey>
    </paper>
    <paper id="21">
      <title>Densité d’information syntaxique et gradient de grammaticalité</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>199–208</pages>
      <abstract>Cet article propose l’introduction d’une notion de densité syntaxique permettant de caractériser la complexité d’un énoncé et au-delà d’introduire la spécification d’un gradient de grammaticalité. Un tel gradient s’avère utile dans plusieurs cas : quantification de la difficulté d’interprétation d’une phrase, gradation de la quantité d’information syntaxique contenue dans un énoncé, explication de la variabilité et la dépendances entre les domaines linguistiques, etc. Cette notion exploite la possibilité de caractérisation fine de l’information syntaxique en termes de contraintes : la densité est fonction des contraintes satisfaites par une réalisation pour une grammaire donnée. Les résultats de l’application de cette notion à quelques corpus sont analysés.</abstract>
      <url hash="0b43d283">2004.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>blache-2004-densite</bibkey>
    </paper>
    <paper id="22">
      <title>Application des programmes de contraintes orientés objet à l’analyse du langage naturel</title>
      <author><first>Mathieu</first><last>Estratat</last></author>
      <author><first>Laurent</first><last>Henocque</last></author>
      <pages>209–218</pages>
      <abstract>Les évolutions récentes des formalismes et théories linguistiques font largement appel au concept de contrainte. De plus, les caractéristiques générales des grammaires de traits ont conduit plusieurs auteurs à pointer la ressemblance existant entre ces notions et les objets ou frames. Une évolution récente de la programmation par contraintes vers les programmes de contraintes orientés objet (OOCP) possède une application possible au traitement des langages naturels. Nous proposons une traduction systématique des concepts et contraintes décrits par les grammaires de propriétés sous forme d’un OOCP. Nous détaillons l’application de cette traduction au langage “context free” archétypal anbn, en montrant que cette approche permet aussi bien l’analyse que la génération de phrases, de prendre en compte la sémantique au sein du même modèle et ne requiert pas l’utilisation d’algorithmes ad hoc pour le parsage.</abstract>
      <url hash="e25fd504">2004.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>estratat-henocque-2004-application</bibkey>
    </paper>
    <paper id="23">
      <title>Grammaires d’unification polarisées</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>219–228</pages>
      <abstract>Cet article propose un formalisme mathématique générique pour la combinaison de structures. Le contrôle de la saturation des structures finales est réalisé par une polarisation des objets des structures élémentaires. Ce formalisme permet de mettre en évidence et de formaliser les mécanismes procéduraux masqués de nombreux formalismes, dont les grammaires de réécriture, les grammaires de dépendance, TAG, HPSG et LFG.</abstract>
      <url hash="49488dd0">2004.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>kahane-2004-grammaires</bibkey>
    </paper>
    <paper id="24">
      <title>Tree-local <fixed-case>MCTAG</fixed-case> with Shared Nodes: An Analysis of<fixed-case>W</fixed-case>ord Order Variation in <fixed-case>G</fixed-case>erman and <fixed-case>K</fixed-case>orean</title>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>SinWon</first><last>Yoon</last></author>
      <pages>229–238</pages>
      <abstract>Tree Adjoining Grammars (TAG) are known not to be powerful enough to deal with scrambling in free word order languages. The TAG-variants proposed so far in order to account for scrambling are not entirely satisfying. Therefore, an alternative extension of TAG is introduced based on the notion of node sharing. Considering data from German and Korean, it is shown that this TAG-extension can adequately analyse scrambling data, also in combination with extraposition and topicalization.</abstract>
      <url hash="1412fc6e">2004.jeptalnrecital-long.24</url>
      <bibkey>kallmeyer-yoon-2004-tree</bibkey>
    </paper>
    <paper id="25">
      <title>Une mesure de pertinence pour le tri de l’information dans un index de “fin de livre”</title>
      <author><first>Touria</first><last>Ait El Mekki</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>239–248</pages>
      <abstract>Nous nous intéressons à la construction des index de fin de livres. Nous avons développé le système IndDoc qui aide la construction de tels index. L’un des enjeux de la construction d’index est la sélection des informations : sélection des entrées les plus pertinentes et des renvois au texte les plus intéressants. Cette sélection est évidemment utile pour le lecteur qui doit trouver suffisamment d’information mais sans en être submergé. Elle est également précieuse pour l’auteur de l’index qui doit valider et corriger une ébauche d’index produite automatiquement par IndDoc. Nous montrons comment cette sélection de l’information est réalisée par IndDoc. Nous proposons une mesure qui permet de trier les entrées par ordre de pertinence décroissante et une méthode pour calculer les renvois au texte à associer à chaque entrée de l’index.</abstract>
      <url hash="fe5aedd0">2004.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>ait-el-mekki-nazarenko-2004-une</bibkey>
    </paper>
    <paper id="26">
      <title>Approche statistique pour le repérage de mots informatifs dans les textes oraux</title>
      <author><first>Narjès</first><last>Boufaden</last></author>
      <author><first>Yoshua</first><last>Bengio</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>249–258</pages>
      <abstract>Nous présentons les résultats de l’approche statistique que nous avons développée pour le repérage de mots informatifs à partir de textes oraux. Ce travail fait partie d’un projet lancé par le département de la défense canadienne pour le développement d’un système d’extraction d’information dans le domaine de la Recherche et Sauvetage maritime (SAR). Il s’agit de trouver et annoter les mots pertinents avec des étiquettes sémantiques qui sont les concepts d’une ontologie du domaine (SAR). Notre méthode combine deux types d’information : les vecteurs de similarité générés grâce à l’ontologie du domaine et le dictionnaire-thésaurus Wordsmyth ; le contexte d’énonciation représenté par le thème. L’évaluation est effectuée en comparant la sortie du système avec les réponses de formulaires d’extraction d’information prédéfinis. Les résultats obtenus sur les textes oraux sont comparables à ceux obtenus dans le cadre de MUC7 pour des textes écrits.</abstract>
      <url hash="900b3e5d">2004.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>boufaden-etal-2004-approche</bibkey>
    </paper>
    <paper id="27">
      <title>Fiabilité de la référence humaine dans la détection de thème</title>
      <author><first>Armelle</first><last>Brun</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <pages>259–268</pages>
      <abstract>Dans cet article, nous nous intéressons à la tâche de détection de thème dans le cadre de la reconnaissance automatique de la parole. La combinaison de plusieurs méthodes de détection montre ses limites, avec des performances de 93.1 %. Ces performances nous mènent à remetttre en cause le thème de référence des paragraphes de notre corpus. Nous avons ainsi effectué une étude sur la fiabilité de ces références, en utilisant notamment les mesures Kappa et erreur de Bayes. Nous avons ainsi pu montrer que les étiquettes thématiques des paragraphes du corpus de test comportaient vraisemblablement des erreurs, les performances de détection de thème obtenues doivent donc êtres exploitées prudemment.</abstract>
      <url hash="95534ccb">2004.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>brun-smaili-2004-fiabilite</bibkey>
    </paper>
    <paper id="28">
      <title>Résolution des anaphores pronominales : quelques postulats du <fixed-case>TALN</fixed-case> mis à l’épreuve du dialogue oral finalisé</title>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <pages>269–278</pages>
      <abstract>Cet article étudie l’adaptation au dialogue oral homme-machine des techniques de résolution des anaphores pronominales qui ont été développées par le TALN pour les documents écrits. A partir d’une étude de corpus de dialogue oral, il étudie la faisabilité de ce portage de l’écrit vers l’oral. Les résultats de cette étude montrent que certains indices utilisés à l’écrit (accord en nombre, distance entre le pronom est son antécédent) sont plus friables en dialogue oral finalisé. Les techniques développées pour l’écrit ne peuvent donc pas être réutilisées directement à l’oral.</abstract>
      <url hash="57923c42">2004.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>antoine-2004-resolution</bibkey>
    </paper>
    <paper id="29">
      <title>Une méthode pour l’annotation de relations temporelles dans des textes et son évaluation</title>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>279–288</pages>
      <abstract>Cet article traite de l’annotation automatique d’informations temporelles dans des textes et vise plus particulièrement les relations entre événements introduits par les verbes dans chaque clause. Si ce problème a mobilisé beaucoup de chercheurs sur le plan théorique, il reste en friche pour ce qui est de l’annotation automatique systématique (et son évaluation), même s’il existe des débuts de méthodologie pour faire réaliser la tâche par des humains. Nous proposons ici à la fois une méthode pour réaliser la tâche automatiquement et une manière de mesurer à quel degré l’objectif est atteint. Nous avons testé la faisabilité de ceci sur des dépêches d’agence avec des premiers résultats encourageants.</abstract>
      <url hash="a67fca0b">2004.jeptalnrecital-long.29</url>
      <language>fra</language>
      <bibkey>muller-tannier-2004-une</bibkey>
    </paper>
    <paper id="30">
      <title>Annoter les documents <fixed-case>XML</fixed-case> avec un outil d’analyse syntaxique</title>
      <author><first>Claude</first><last>Roux</last></author>
      <pages>289–298</pages>
      <abstract>Cet article présente l’intégration au sein d’un analyseur syntaxique (Xerox Incremental Parser) de règles spécifiques qui permettent de lier l’analyse grammaticale à la sémantique des balises XML spécifiques à un document donné. Ces règles sont basées sur la norme XPath qui offre une très grande finesse de description et permet de guider très précisément l’application de l’analyseur sur une famille de documents partageant une même DTD. Le résultat est alors être intégré directement comme annotation dans le document traité.</abstract>
      <url hash="711abc4a">2004.jeptalnrecital-long.30</url>
      <language>fra</language>
      <bibkey>roux-2004-annoter</bibkey>
    </paper>
    <paper id="31">
      <title>La <fixed-case>FREEBANK</fixed-case> : vers une base libre de corpus annotés</title>
      <author><first>Susanne</first><last>Salmon-Alt</last></author>
      <author><first>Eckhard</first><last>Bick</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <pages>299–308</pages>
      <abstract>Les corpus français librement accessibles annotés à d’autres niveaux linguistiques que morpho-syntaxique sont insuffisants à la fois quantitativement et qualitativement. Partant de ce constat, la FREEBANK – construite sur la base d’outils d’analyse automatique dont la sortie est révisée manuellement – se veut une base de corpus du français annotés à plusieurs niveaux (structurel, morphologique, syntaxique, coréférentiel) et à différents degrés de finesse linguistique qui soit libre d’accès, codée selon des schémas normalisés, intégrant des ressources existantes et ouverte à l’enrichissement progressif.</abstract>
      <url hash="ebbf4714">2004.jeptalnrecital-long.31</url>
      <language>fra</language>
      <bibkey>salmon-alt-etal-2004-la</bibkey>
    </paper>
    <paper id="32">
      <title>Annoter en constituants pour évaluer des analyseurs syntaxiques</title>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Véronique</first><last>Gendner</last></author>
      <author><first>Gabriel</first><last>Illouz</last></author>
      <author><first>Michèle</first><last>Jardino</last></author>
      <pages>309–318</pages>
      <abstract>Cet article présente l’annotation en constituants menée dans le cadre d’un protocole d’évaluation des analyseurs syntaxiques (mis au point dans le pré-projet PEAS, puis dans le projet EASY). Le choix des constituants est décrit en détail et une première évaluation effectuée à partir des résultats de deux analyseurs est donnée.</abstract>
      <url hash="1a1943c6">2004.jeptalnrecital-long.32</url>
      <language>fra</language>
      <bibkey>vilnat-etal-2004-annoter</bibkey>
    </paper>
    <paper id="33">
      <title>Détermination de contenu dans <fixed-case>GEPHOX</fixed-case></title>
      <author><first>Adil</first><last>El Ghali</last></author>
      <pages>319–328</pages>
      <abstract>Le générateur GEPHOX que nous réalisons a pour ambition de produire des textes pour des définition ou preuves mathématiques écrites à l’aide de l’assistant de preuve PHOX. Dans cet article nous nous concentrons sur le module de détermination de contenu ContDet de GEPHOX. Après un aperçu sur l’entrée du générateur, i.e. la preuve formelle et l’ensemble des règles ayant permis de l’obtenir, nous décrivons les base de connaissances du générateur et le fonctionnement de l’algorithme de détermination de contenu.</abstract>
      <url hash="f5da7d36">2004.jeptalnrecital-long.33</url>
      <language>fra</language>
      <bibkey>el-ghali-2004-determination</bibkey>
    </paper>
    <paper id="34">
      <title>Apprentissage partiel de grammaires catégorielles</title>
      <author><first>Erwan</first><last>Moreau</last></author>
      <pages>329–338</pages>
      <abstract>Cet article traite de l’apprentissage symbolique de règles syntaxiques dans le modèle de Gold. Kanazawa a montré que certaines classes de grammaires catégorielles sont apprenables dans ce modèle. L’algorithme qu’il propose nécessite une grande quantité d’information en entrée pour être efficace. En changeant la nature des informations en entrée, nous proposons un algorithme d’apprentissage de grammaires catégorielles plus réaliste dans la perspective d’applications au langage naturel.</abstract>
      <url hash="449ccf51">2004.jeptalnrecital-long.34</url>
      <language>fra</language>
      <bibkey>moreau-2004-apprentissage</bibkey>
    </paper>
    <paper id="35">
      <title>La sémantique dans les grammaires d’interaction</title>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>339–348</pages>
      <abstract>Nous proposons d’intégrer la sémantique dans les grammaires d’interaction, formalisme qui a été conçu pour représenter la syntaxe des langues. Pour cela, nous ajoutons au formalisme un niveau supplémentaire qui s’appuie sur les mêmes principes fondamentaux que le niveau syntaxique : contrôle de la composition par un système de polarités et utilisation de la notion de description de structure pour exprimer la sous-spécification. A la différence du niveau syntaxique, les structures sont des graphes acycliques orientés et non des arbres localement ordonnés. L’interface entre les deux niveaux est assurée de façon souple par une fonction de liage qui associe à tout noeud syntaxique au plus un noeud sémantique.</abstract>
      <url hash="1fae620c">2004.jeptalnrecital-long.35</url>
      <language>fra</language>
      <bibkey>perrier-2004-la</bibkey>
    </paper>
    <paper id="36">
      <title>Les Grammaires à Concaténation d’Intervalles (<fixed-case>RCG</fixed-case>) comme formalisme grammatical pour la linguistique</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Pierre</first><last>Boullier</last></author>
      <pages>349–358</pages>
      <abstract>Le but de cet article est de montrer pourquoi les Grammaires à Concaténation d’Intervalles (Range Concatenation Grammars, ou RCG) sont un formalisme particulièrement bien adapté à la description du langage naturel. Nous expliquons d’abord que la puissance nécessaire pour décrire le langage naturel est celle de PTIME. Ensuite, parmi les formalismes grammaticaux ayant cette puissance d’expression, nous justifions le choix des RCG. Enfin, après un aperçu de leur définition et de leurs propriétés, nous montrons comment leur utilisation comme grammaires linguistiques permet de traiter des phénomènes syntagmatiques complexes, de réaliser simultanément l’analyse syntaxique et la vérification des diverses contraintes (morphosyntaxiques, sémantique lexicale), et de construire dynamiquement des grammaires linguistiques modulaires.</abstract>
      <url hash="d3c2a5cc">2004.jeptalnrecital-long.36</url>
      <language>fra</language>
      <bibkey>sagot-boullier-2004-les</bibkey>
    </paper>
  </volume>
  <volume id="poster" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles. Posters</booktitle>
      <editor><first>Philippe</first><last>Blache</last></editor>
      <editor><first>Noël</first><last>Nguyen</last></editor>
      <editor><first>Nouredine</first><last>Chenfour</last></editor>
      <editor><first>Abdenbi</first><last>Rajouani</last></editor>
      <publisher>ATALA</publisher>
      <address>Fès, Maroc</address>
      <month>April</month>
      <year>2004</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="c837e7d2">2004.jeptalnrecital-poster.0</url>
      <bibkey>jep-taln-recital-2004-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Mots composés dans les modèles de langue pour la recherche d’information</title>
      <author><first>Carmen</first><last>Alvarez</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <pages>1–6</pages>
      <abstract>Une approche classique en recherche d’information (RI) consiste à bâtir une représentation des documents et des requêtes basée sur les mots simples les constituant. L’utilisation de modèles bigrammes a été étudiée, mais les contraintes sur l’ordre et l’adjacence des mots dans ces travaux ne sont pas toujours justifiées pour la recherche d’information. Nous proposons une nouvelle approche basée sur les modèles de langue qui incorporent des affinités lexicales (ALs), c’est à dire des paires non ordonnées de mots qui se trouvent proches dans un texte. Nous décrivons ce modèle et le comparons aux plus traditionnels modèles unigrammes et bigrammes ainsi qu’au modèle vectoriel.</abstract>
      <url hash="6793bdc5">2004.jeptalnrecital-poster.1</url>
      <language>fra</language>
      <bibkey>alvarez-etal-2004-mots</bibkey>
    </paper>
    <paper id="2">
      <title>Le Regroupement de Types de Mots et l’Unification d’Occurrences de Mots dans des Catégories grammaticales de mots (Clustering of Word Types and Unification of Word Tokens into Grammatical Word-Classes)</title>
      <author><first>Eric</first><last>Atwell</last></author>
      <pages>7–12</pages>
      <abstract>Ce papier discute la Néoposie: l’inférence auto-adaptive de catégories grammaticales de mots de la langue naturelle. L’inférence grammaticale peut être divisée en deux parties : l’inférence de catégories grammaticales de mots et l’inférence de la structure. Nous examinons les éléments de base de l’apprentissage auto-adaptif du marquage des catégories grammaticales, et discutons l’adaptation des trois types principaux de marqueurs des catégories grammaticales à l’inférence auto-adaptive de catégories grammaticales de mots. Des marqueurs statistiques de n-grammes suggèrent une approche de regroupement statistique, mais le regroupement n’aide ni avec les types de mots peu fréquents, ni avec les types de mots nombreux qui peuvent se présenter dans plus d’une catégorie grammaticale. Le marqueur alternatif d’apprentissage basé sur la transformation suggère une approche basée sur la contrainte de l’unification de contextes d’occurrences de mots. Celle-ci présente un moyen de regrouper des mots peu fréquents, et permet aux occurrences différentes d’un seul type de mot d’appartenir à des catégories différentes selon les contextes grammaticaux où ils se présentent. Cependant, la simple unification de contextes d’occurrences de mots produit un nombre incroyablement grand de catégories grammaticales de mots. Nous avons essayé d’unifier plus de catégories en modérant le contexte de la correspondance pour permettre l’unification des catégories de mots aussi bien que des occurrences de mots, mais cela entraîne des unifications fausses. Nous concluons que l’avenir peut être un hybride qui comprend le regroupement de types de mots peu fréquents, l’unification de contextes d’occurrences de mots, et le ‘seeding’ avec une connaissance linguistique limitée. Nous demandons un programme de nouvelles recherches pour développer une valise pour la découverte de la langue naturelle.</abstract>
      <url hash="3f167e83">2004.jeptalnrecital-poster.2</url>
      <language>fra</language>
      <bibkey>atwell-2004-le</bibkey>
    </paper>
    <paper id="3">
      <title>Temporalité linguistique et <fixed-case>S</fixed-case>-Langages</title>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <author><first>Etienne</first><last>Picard</last></author>
      <author><first>Sylviane</first><last>R. Schwer</last></author>
      <pages>13–18</pages>
      <abstract>Après un rappel de la problématique de l’ordonnancement temporel dans un texte, nous décrivons les S-langages qui offrent une représentation unifiée des relations temporelles et une opération (la jointure) permettant de calculer les combinaisons entre celles-ci.</abstract>
      <url hash="e6408f18">2004.jeptalnrecital-poster.3</url>
      <language>fra</language>
      <bibkey>battistelli-etal-2004-temporalite</bibkey>
    </paper>
    <paper id="4">
      <title>Modélisation de la modulation</title>
      <author><first>Emmanuel</first><last>Bellengier</last></author>
      <author><first>Béatrice</first><last>Priego-Valverde</last></author>
      <pages>19–24</pages>
      <abstract>Le dialogue est un processus interactif pendant lequel les différents agents impliqués vont s’engager sur un certain nombre d’éléments propositionnels. La modulation implique des ajouts propositionnels - révisés et atténués - qui ne constituent pas nécessairement une base pour un accord. L’objectif de cet article est donc de proposer une description formelle du phénomène de modulation dans le cadre du modèle de J. Ginzburg.</abstract>
      <url hash="380881e1">2004.jeptalnrecital-poster.4</url>
      <language>fra</language>
      <bibkey>bellengier-priego-valverde-2004-modelisation</bibkey>
    </paper>
    <paper id="5">
      <title>Traduction de dialogue: résultats du projet <fixed-case>NESPOLE</fixed-case>! et pistes pour le domaine</title>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>25–30</pages>
      <abstract>Dans cet article, nous détaillons les résultats de la seconde évaluation du projet européen NESPOLE! auquel nous avons pris part pour le français. Dans ce projet, ainsi que dans ceux qui l’ont précédé, des techniques d’évaluation subjectives — réalisées par des évaluateurs humains — ont été mises en oeuvre. Nous présentons aussi les nouvelles techniques objectives — automatiques — proposées en traduction de l’écrit et mises en oeuvre dans le projet C-STAR III. Nous conclurons en proposant quelques idées et perspectives pour le domaine.</abstract>
      <url hash="3a9ce0c2">2004.jeptalnrecital-poster.5</url>
      <language>fra</language>
      <bibkey>blanchon-besacier-2004-traduction</bibkey>
    </paper>
    <paper id="6">
      <title>Spécification et implantation informatique d’un langage de description des structures discursives</title>
      <author><first>Gustavo</first><last>Crispino</last></author>
      <author><first>Agata</first><last>Jackiewicz</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <pages>31–36</pages>
      <abstract>Cet article présente le langage de représentation des connaissances linguistiques LangTex qui permet de spécifier d’une manière unifiée les descriptions linguistiques nécessaires au repérage d’objets textuels qui organisent les textes écrits.</abstract>
      <url hash="505c1c91">2004.jeptalnrecital-poster.6</url>
      <language>fra</language>
      <bibkey>crispino-etal-2004-specification</bibkey>
    </paper>
    <paper id="7">
      <title>@<fixed-case>GEWEB</fixed-case> : Agents personnels d’aide à la recherche sur le Web</title>
      <author><first>Mohamed</first><last>Yassine El Amrani</last></author>
      <author><first>Sylvain</first><last>Delisle</last></author>
      <author><first>Ismaïl</first><last>Biskri</last></author>
      <pages>37–42</pages>
      <abstract>Nous présentons dans cet article un logiciel permettant d’assister l’usager, de manière personnalisée lors de la recherche documentaire sur le Web. L’architecture du logiciel est basée sur l’intégration d’outils numériques de traitements des langues naturelles (TLN). Le système utilise une stratégie de traitement semi-automatique où la contribution de l’utilisateur assure la concordance entre ses attentes et les résultats obtenus.</abstract>
      <url hash="6e3404c6">2004.jeptalnrecital-poster.7</url>
      <language>fra</language>
      <bibkey>yassine-el-amrani-etal-2004-geweb</bibkey>
    </paper>
    <paper id="8">
      <title>Prédiction d’actes et attentes en dialogue : expérience avec un assistant virtuel simulé</title>
      <author><first>Yannick</first><last>Fouquet</last></author>
      <pages>43–48</pages>
      <abstract>Dans cet article, nous présentons une plate-forme de test et de recueil de dialogue oral homme-machine. Dans son architecture générale, des magiciens d’Oz simulent la compréhension des énoncés des utilisateurs et le contrôle du dialogue. Puis, nous comparons, dans un tel corpus, la prédiction statistique d’acte de dialogue avec les attentes du locuteur.</abstract>
      <url hash="ce925f10">2004.jeptalnrecital-poster.8</url>
      <language>fra</language>
      <bibkey>fouquet-2004-prediction</bibkey>
    </paper>
    <paper id="9">
      <title>Disambiguation and Optional Co-Composition</title>
      <author><first>Pablo</first><last>Gamallo</last></author>
      <author><first>Gabriel</first><last>P. Lopes</last></author>
      <author><first>Alexandre</first><last>Agustini</last></author>
      <pages>49–54</pages>
      <abstract>This paper describes a specific semantic property underlying binary dependencies: co-composition. We propose a more general definition than that given by Pustejovsky, what we call “optional co-composition”. The aim of the paper is to explore the benefits of optional cocomposition in two disambiguation tasks: both word sense and structural disambiguation. Concerning the second task, some experiments were performed on large corpora.</abstract>
      <url hash="e7979479">2004.jeptalnrecital-poster.9</url>
      <bibkey>gamallo-etal-2004-disambiguation</bibkey>
    </paper>
    <paper id="10">
      <title>Le projet <fixed-case>GÉRAF</fixed-case> : Guide pour l’Évaluation des Résumés Automatiques Français</title>
      <author><first>Marie-Josée</first><last>Goulet</last></author>
      <author><first>Joël</first><last>Bourgeoys</last></author>
      <pages>55–60</pages>
      <abstract>Dans cet article, nous présentons le projet GÉRAF (Guide pour l’Évaluation des Résumés Automatiques Français), lequel vise l’élaboration de protocoles et la construction de corpus de résumés de référence pour l’évaluation des systèmes résumant des textes français. La finalité de ce projet est de mettre à la disposition des chercheurs les ressources ainsi créées.</abstract>
      <url hash="adea08c8">2004.jeptalnrecital-poster.10</url>
      <language>fra</language>
      <bibkey>goulet-bourgeoys-2004-le</bibkey>
    </paper>
    <paper id="11">
      <title>Repérage de relations terminologiques transversales en corpus</title>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Véronique</first><last>Malaisé</last></author>
      <author><first>Aurélia</first><last>Marcus</last></author>
      <author><first>Aleksandra</first><last>Krul</last></author>
      <pages>61–66</pages>
      <abstract>Les relations transversales encodent des relations spécifiques entre les termes, par exemple localisé-dans, consomme, etc. Elles sont très souvent dépendantes des domaines, voire des corpus. Les méthodes automatiques consacrées au repérage de relations terminologiques plus classiques (hyperonymie, synonymie), peuvent générer occasionnellement les relations transversales. Mais leur repérage et typage restent sujets à une conceptualisation : ces relations ne sont pas attendues et souvent pas connues à l’avance pour un nouveau domaine à explorer. Nous nous attachons ici à leur repérage mais surtout à leur typage. En supposant que les relations sont souvent exprimées par des verbes, nous misons sur l’étude des verbes du corpus et de leurs divers dérivés afin d’aborder plus directement la découverte des relations du domaine. Les expériences montrent que ce point d’attaque peut être intéressant, mais reste pourtant dépendant de la polysémie verbale et de la synonymie.</abstract>
      <url hash="57ea3486">2004.jeptalnrecital-poster.11</url>
      <language>fra</language>
      <bibkey>grabar-etal-2004-reperage</bibkey>
    </paper>
    <paper id="12">
      <title>Classification automatique de définitions en sens</title>
      <author><first>Fabien</first><last>Jalabert</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>67–72</pages>
      <abstract>Dans le cadre de la recherche en sémantique lexicale, l’équipe TAL du LIRMM développe actuellement un système d’analyse des aspects thématiques des textes et de désambiguisation lexicale basé sur les vecteurs conceptuels. Pour la construction des vecteurs, les définitions provenant de sources lexicales différentes (dictionnaires à usage humain, listes de synonymes, définitions de thésaurus, . . .) sont analysées. Aucun découpage du sens n’est présent dans la représentation : un vecteur conceptuel est associé à chaque définition et un autre pour représenter le sens global du mot. Nous souhaitons effectuer une catégorisation afin que chaque élément ne soit plus une définition mais un sens. Cette amélioration concerne bien sur directement les applications courantes (désambiguïsation, transfert lexical, . . .) mais a aussi pour objectif majeur d’améliorer l’apprentissage de la base.</abstract>
      <url hash="ae9bd31d">2004.jeptalnrecital-poster.12</url>
      <language>fra</language>
      <bibkey>jalabert-lafourcade-2004-classification</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>NLP</fixed-case> Applications Based on<fixed-case>W</fixed-case>eighted<fixed-case>M</fixed-case>ulti-Tape Automata</title>
      <author><first>André</first><last>Kempe</last></author>
      <pages>73–78</pages>
      <abstract>This article describes two practical applications of weighted multi-tape automata (WMTAs) in Natural Language Processing, that demonstrate the augmented descriptive power of WMTAs compared to weighted 1-tape and 2-tape automata. The two examples concern the preservation of intermediate results in transduction cascades and the search for similar words in two languages. As a basis for these applications, the article proposes a number of operations on WMTAs. Among others, it (re-)defines multi-tape intersection, where a number of tapes of one WMTA are intersected with the same number of tapes of another WMTA. In the proposed approach, multi-tape intersection is not an atomic operation but rather a sequence of more elementary ones, which facilitates its implementation.</abstract>
      <url hash="3321dd4c">2004.jeptalnrecital-poster.13</url>
      <bibkey>kempe-2004-nlp</bibkey>
    </paper>
    <paper id="14">
      <title>Multiple Lexicon Generation based on Phonological Feature Trees</title>
      <author><first>Moritz</first><last>Neugebauer</last></author>
      <author><first>Stephen</first><last>Wilson</last></author>
      <pages>79–84</pages>
      <abstract>Tree-based data structures are commonly used by computational linguists for the documentation and analysis of morphological and syntactic data. In this paper we apply such structures to phonological data and demonstrate how such representations can have practical and beneficial applications in computational lexicography. To this end, we describe three integrated modules: the first defines a multilingual feature set within a tree-based structure using XML; the second module traverses this tree and generalises over the data contained within it, optimising the phonological data and highlighting feature implications. The third uses the information contained within the tree representation as a knowledge base for the generation of multiple feature-based syllable lexica.</abstract>
      <url hash="fb578038">2004.jeptalnrecital-poster.14</url>
      <bibkey>neugebauer-wilson-2004-multiple</bibkey>
    </paper>
    <paper id="15">
      <title>Gestion de buts de dialogue</title>
      <author><first>Jean</first><last>Caelen</last></author>
      <author><first>Hoâ</first><last>Nguyen</last></author>
      <pages>85–90</pages>
      <abstract>La gestion du but de dialogue est une tâche délicate pour le contrôleur de dialogue, car bien souvent il est en concurrence avec le gestionnaire de tâches avec lequel on le confond parfois dans certains systèmes. Dans cet article, nous présentons une stratégie dynamique de gestion de buts qui permet au contrôleur de dialogue de réduire sa dépendance au gestionnaire de tâche et lui apporte une meilleure réutilisabilité. Nous expérimentons le système dans le cadre du projet PVE (Portail Vocal d’Entreprise) dans lequel le dialogue peut se dérouler en plusieurs sessions et avec des interlocuteurs différents.</abstract>
      <url hash="cfd1f5ce">2004.jeptalnrecital-poster.15</url>
      <language>fra</language>
      <bibkey>caelen-nguyen-2004-gestion</bibkey>
    </paper>
    <paper id="16">
      <title>Un modèle d’interprétation constructionnelle pour les expressions référentielles extensionnelles</title>
      <author><first>Guillaume</first><last>Pitel</last></author>
      <author><first>Jean-Paul</first><last>Sansonnet</last></author>
      <pages>91–96</pages>
      <abstract>Dans le dialogue finalisé, les expressions référentielles portant sur les objets du contexte peuvent contenir des prédicats vagues ou relationnels, qu’il est difficile de traiter avec une logique propositionnelle. Inversement, les approches adaptées à ces types de prédicats sont difficilement implémentables dans un modèle générique et adaptable aux théories d’analyse linguistique. Nous proposons un modèle d’interprétation constructionnelle inspiré des grammaires de construction qui permet de modéliser le processus de résolution d’expressions référentielles extensionnelles tout en restant compatible avec la grammaire dont nous nous sommes inspirés.</abstract>
      <url hash="828d7958">2004.jeptalnrecital-poster.16</url>
      <language>fra</language>
      <bibkey>pitel-sansonnet-2004-un</bibkey>
    </paper>
    <paper id="17">
      <title>Apprentissage collectif et lexique</title>
      <author><first>Julien</first><last>Poudade</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>97–102</pages>
      <abstract>Cet article présente l’influence de la zone de travail que possède une entité logicielle pour lui permettre de prédire l’état futur de son environnement, sur la constitution d’un lexique partagé par les différents membres d’une population, dans le cadre d’une variante “du jeu de désignation” (naming game).</abstract>
      <url hash="096241e9">2004.jeptalnrecital-poster.17</url>
      <language>fra</language>
      <bibkey>poudade-paroubek-2004-apprentissage</bibkey>
    </paper>
    <paper id="18">
      <title>L’outil de traitement de corpus <fixed-case>LIKES</fixed-case></title>
      <author><first>François</first><last>Rousselot</last></author>
      <pages>103–112</pages>
      <abstract>LIKES (LInguistic and Knowledge Engineering Station) est une station d’ingénierie linguistique destinée à traiter des corpus, elle fonctionne pour l’instant sur la plupart des langues européennes et slaves en utilisant des ressources minimales pour chaque langue. Les corpus sont constitués d’un ou plusieurs textes en ASCII ou en HTML, l’interface donne la possibilité de constituer son corpus et d’y exécuter un certain nombre de tâches allant de simples tâches de découpage en mot, de tri ou de recherche de motifs à des tâches plus complexes d’aide à la synthèse de grammaire, d’aide au repérage de relations, d’aide à la construction d’une terminologie. Nous décrivons ici les principales fonctionnalités de LIKES en rapport avec le traitement des corpus et ce qui fait sa spécificité par rapport à d’autres environnements comparables : l’utilisation minimale de ressources linguistiques.</abstract>
      <url hash="dbdfd3c2">2004.jeptalnrecital-poster.18</url>
      <language>fra</language>
      <bibkey>rousselot-2004-loutil</bibkey>
    </paper>
    <paper id="19">
      <title>Résolution automatique d’anaphores infidèles en français : Quelles ressources pour quels apports ?</title>
      <author><first>Susanne</first><last>Salmon-Alt</last></author>
      <pages>113–118</pages>
      <abstract>La performance d’une résolution automatique d’anaphores infidèles pour le français pourrait atteindre une F-mesure de 30%. Ce résultat repose toutefois sur une ressource équivalente à un bon dictionnaire de la langue française, une analyse syntaxique de qualité satisfaisante et un traitement performant des entités nommées. En l’absence de telles ressources, les meilleurs résultats plafonnent autour d’une F-mesure de 15%.</abstract>
      <url hash="d43159dc">2004.jeptalnrecital-poster.19</url>
      <language>fra</language>
      <bibkey>salmon-alt-2004-resolution</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>S</fixed-case>iby<fixed-case>M</fixed-case>ot : Modélisation stochastique du langage intégrant la notion de chunks</title>
      <author><first>Igor</first><last>Schadle</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Brigitte</first><last>Le Pévédic</last></author>
      <author><first>Franck</first><last>Poirier</last></author>
      <pages>119–124</pages>
      <abstract>Cet article présente le modèle de langage développé pour le système Sibylle, un système d’aide à la communication pour les personnes handicapées. L’utilisation d’un modèle de langage permet d’améliorer la pertinence des mots proposés en tenant compte du contexte gauche de la saisie en cours. L’originalité de notre modèle se situe dans l’intégration de la notion de chunks afin d’élargir la taille du contexte pris en compte pour l’estimation de la probabilité d’apparition des mots.</abstract>
      <url hash="6657507f">2004.jeptalnrecital-poster.20</url>
      <language>fra</language>
      <bibkey>schadle-etal-2004-sibymot</bibkey>
    </paper>
    <paper id="21">
      <title>Extracting Named Entities. A Statistical Approach</title>
      <author><first>Joaquim</first><last>Silva</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <author><first>Veska</first><last>Noncheva</last></author>
      <author><first>Gabriel</first><last>Lopes</last></author>
      <pages>125–130</pages>
      <abstract>Named entities and more generally Multiword Lexical Units (MWUs) are important for various applications. However, language independent methods for automatically extracting MWUs do not provide us with clean data. So, in this paper we propose a method for selecting possible named entities from automatically extracted MWUs, and later, a statistics-based language independent unsupervised approach is applied to possible named entities in order to cluster them according to their type. Statistical features used by our clustering process are described and motivated. The Model-Based Clustering Analysis (MBCA) software enabled us to obtain different clusters for proposed named entities. The method was applied to Bulgarian and English. For some clusters, precision is very high; other clusters still need further refinement. Based on the obtained clusters, it is also possible to classify new possible named entities.</abstract>
      <url hash="714e67eb">2004.jeptalnrecital-poster.21</url>
      <bibkey>silva-etal-2004-extracting</bibkey>
    </paper>
    <paper id="22">
      <title>Analogies dans les séquences : un solveur à états finis</title>
      <author><first>Nicolas</first><last>Stroppa</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>131–140</pages>
      <abstract>L’apprentissage par analogie se fonde sur un principe inférentiel potentiellement pertinent pour le traitement des langues naturelles. L’utilisation de ce principe pour des tâches d’analyse linguistique présuppose toutefois une définition formelle de l’analogie entre séquences. Dans cet article, nous proposons une telle définition et montrons qu’elle donne lieu à l’implantation efficace d’un solveur d’équations analogiques sous la forme d’un transducteur fini. Munis de ces résultats, nous caractérisons empiriquement l’extension analogique de divers langages finis, correspondant à des dictionnaires de quatre langues.</abstract>
      <url hash="13f841a2">2004.jeptalnrecital-poster.22</url>
      <language>fra</language>
      <bibkey>stroppa-yvon-2004-analogies</bibkey>
    </paper>
    <paper id="23">
      <title>An electronic dictionary as a basis for <fixed-case>NLP</fixed-case> tools: The <fixed-case>G</fixed-case>reek case</title>
      <author><first>Christos</first><last>Tsalidis</last></author>
      <author><first>Aristides</first><last>Vagelatos</last></author>
      <author><first>Giorgos</first><last>Orphanos</last></author>
      <pages>141–146</pages>
      <abstract>The existence of a Dictionary in electronic form for Modern Greek (MG) is mandatory if one is to process MG at the morphological and syntactic levels since MG is a highly inflectional language with marked stress and a spelling system with many characteristics carried over from Ancient Greek. Moreover, such a tool becomes necessary if one is to create efficient and sophisticated NLP applications with substantial linguistic backing and coverage. The present paper will focus on the deployment of such an electronic dictionary for Modern Greek, which was built in two phases: first it was constructed to be the basis for a spelling correction schema and then it was reconstructed in order to become the platform for the deployment of a wider spectrum of NLP tools.</abstract>
      <url hash="fe57356d">2004.jeptalnrecital-poster.23</url>
      <bibkey>tsalidis-etal-2004-electronic</bibkey>
    </paper>
    <paper id="24">
      <title>Modèle de langage sémantique pour la reconnaissance automatique de parole dans un contexte de traduction</title>
      <author><first>Quang</first><last>Vu-minh</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Hervé</first><last>Blanchon</last></author>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <pages>147–152</pages>
      <abstract>Le travail présenté dans cet article a été réalisé dans le cadre d’un projet global de traduction automatique de la parole. L’approche de traduction est fondée sur un langage pivot ou Interchange Format (IF), qui représente le sens de la phrase indépendamment de la langue. Nous proposons une méthode qui intègre des informations sémantiques dans le modèle statistique de langage du système de Reconnaissance Automatique de Parole. Le principe consiste a utiliser certaines classes définies dans l’IF comme des classes sémantiques dans le modèle de langage. Ceci permet au système de reconnaissance de la parole d’analyser partiellement en IF les tours de parole. Les expérimentations realisées montrent qu’avec cette approche, le système de reconnaissance peut analyser directement en IF une partie des données de dialogues de notre application, sans faire appel au système de traduction (35% des mots ; 58% des tours de parole), tout en maintenant le même niveau de performance du système global.</abstract>
      <url hash="6e80b67f">2004.jeptalnrecital-poster.24</url>
      <language>fra</language>
      <bibkey>vu-minh-etal-2004-modele</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Frédéric</first><last>Béchet</last></editor>
      <editor><first>Tristan</first><last>Vanrullen</last></editor>
      <publisher>ATALA</publisher>
      <address>Fès, Maroc</address>
      <month>April</month>
      <year>2004</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="e59dec1c">2004.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2004-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Système de Question Réponse : apport de l’analyse syntaxique lors de l’extraction de la réponse</title>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <pages>1–10</pages>
      <abstract>Dans cet article, nous présentons le système de Question Réponse QALC, et nous nous intéressons tout particulièrement à l’extraction de la réponse. Un appariement question-réponse fondé sur les relations syntaxiques a été développé, afin d’améliorer les performances du système. Un projet de génération de réponses à partir de plusieurs documents est également discuté.</abstract>
      <url hash="dcbb0b79">2004.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>ligozat-2004-systeme</bibkey>
    </paper>
    <paper id="2">
      <title>Acquisition de relations lexicales désambiguïsées à partir du Web</title>
      <author><first>Chrystel</first><last>Millon</last></author>
      <pages>11–20</pages>
      <abstract>Nous montrons dans cet article qu’un pré-étiquetage des usages des mots par un algorithme de désambiguïsation tel qu’HyperLex (Véronis, 2003, 2004) permet d’obtenir des relations lexicales (du type NOM-ADJECTIF, NOM de NOM, NOM-VERBE) beaucoup plus exploitables, parce qu’elles-mêmes catégorisées en fonction des usages. De plus, cette technique permet d’obtenir des relations pour des usages très peu fréquents, alors qu’une extraction indifférenciée « noie » ces relations au milieu de celles correspondant aux usages les plus fréquents. Nous avons conduit une évaluation sur un corpus de plusieurs milliers de pages Web comportant l’un des 10 mots-cibles très polysémiques choisis pour cette expérience, et nous montrons que la précision obtenue est très bonne, avec un rappel honorable, suffisant en tout cas pour de nombreuses applications. L’analyse des erreurs ouvre des perspectives d’améliorations pour la suite de notre travail de thèse.</abstract>
      <url hash="78469785">2004.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>millon-2004-acquisition</bibkey>
    </paper>
    <paper id="3">
      <title>Indexation automatique de ressources de santé à l’aide d’un vocabulaire contrôlé</title>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>21–30</pages>
      <abstract>Nous présentons ici le système d’indexation automatique actuellement en cours de développement dans l’équipe CISMeF afin d’aider les documentalistes lors de l’indexation de ressources de santé. Nous détaillons l’architecture du système pour l’extraction de mots clés MeSH, et présentons les résultats d’une première évaluation. La stratégie d’indexation choisie atteint une précision comparable à celle des systèmes existants. De plus, elle permet d’extraire des paires mot clé/qualificatif, et non des termes isolés, ce qui constitue une indexation beaucoup plus fine. Les travaux en cours s’attachent à étendre la couverture des dictionnaires, et des tests à plus grande échelle sont envisagés afin de valider le système et d’évaluer sa valeur ajoutée dans le travail quotidien des documentalistes.</abstract>
      <url hash="dfc427f2">2004.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>neveol-2004-indexation</bibkey>
    </paper>
    <paper id="4">
      <title>Appariement bilingue de mots par propagation syntaxique à partir de corpus français/anglais alignés</title>
      <author><first>Sylwia</first><last>Ozdowska</last></author>
      <pages>31–40</pages>
      <abstract>Nous présentons une méthode d’appariement de mots, à partir de corpus français/anglais alignés, qui s’appuie sur l’analyse syntaxique en dépendance des phrases. Tout d’abord, les mots sont appariés à un niveau global grâce au calcul des fréquences de cooccurrence dans des phrases alignées. Ces mots constituent les couples amorces qui servent de point de départ à la propagation des liens d’appariement à l’aide des différentes relations de dépendance identifiées par un analyseur syntaxique dans chacune des deux langues. Pour le moment, cette méthode dite d’appariement local traite majoritairement des cas de parallélisme, c’est-à-dire des cas où les relations syntaxiques sont identiques dans les deux langues et les mots appariés de même catégorie. Elle offre un taux de réussite de 95,4% toutes relations confondues.</abstract>
      <url hash="b942336f">2004.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>ozdowska-2004-appariement</bibkey>
    </paper>
    <paper id="5">
      <title>Quelques principes pour une grammaire multimodale non-modulaire du français</title>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <author><first>Emmanuel</first><last>Bellengier</last></author>
      <pages>41–50</pages>
      <abstract>Dans cet article, nous introduisons une approche de la représentation et de l’analyse des discours multimodaux, basée sur un traitement unimodulaire par contraintes. Le but de cet article est de présenter (i) un système de représentation des données et (ii) une méthode d’analyse, permettant une interaction simplifiée entre les différentes modalités de communication. L’avantage de cette méthode est qu’elle permet la prise en compte rigoureuse d’informations communicatives de natures diverses en un traitement unique, grâce à une représentation homogène des objets, de leurs relations, et de leur méthode d’analyse, selon le modèle des Grammaires de Propriétés.</abstract>
      <url hash="af369e36">2004.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>guenot-bellengier-2004-quelques</bibkey>
    </paper>
    <paper id="6">
      <title>L’annotation syntaxique de corpus oraux constitue-t-elle un problème spécifique ?</title>
      <author><first>Christophe</first><last>Benzitoun</last></author>
      <pages>51–60</pages>
      <abstract>Dans cet article, nous présentons une typologie des phénomènes qui posent problème pour l’annotation syntaxique de corpus oraux. Nous montrons également que ces phénomènes, même s’ils y sont d’une fréquence moindre, sont loin d’être absents à l’écrit (ils peuvent même être tout à fait significatifs dans certains corpus : e-mails, chats, SMS...), et que leur prise en compte peut améliorer l’annotation et fournir un cadre intégré pour l’oral et l’écrit.</abstract>
      <url hash="bf202814">2004.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>benzitoun-2004-lannotation</bibkey>
    </paper>
    <paper id="7">
      <title>Automates lexicaux avec structure de traits</title>
      <author><first>Olivier</first><last>Blanc</last></author>
      <author><first>Anne</first><last>Dister</last></author>
      <pages>61–70</pages>
      <abstract>Nous présentons les automates lexicaux avec structure de traits, une extension du modèle des automates finis sur le mots dans lesquels les transitions sont étiquetées par des motifs qui sélectionnent un sous-ensemble des mots étiquetés en fonction de leurs traits positionnés. Nous montrons l’adéquation de ce modèle avec les ressources linguistiques dont nous disposons et nous exposons les grandes lignes de nos méthodes pour effectuer des opérations telles que la déterminisation, l’intersection ou la complémentation sur ces objets. Nous terminons en présentant une application concrète de ces méthodes pour la levée d’ambiguïtés lexicales par intersection d’automates à l’aide de contraintes locales.</abstract>
      <url hash="03f50900">2004.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>blanc-dister-2004-automates</bibkey>
    </paper>
    <paper id="8">
      <title>Géométriser le sens</title>
      <author><first>Fabienne</first><last>Venant</last></author>
      <pages>71–80</pages>
      <abstract>Les recherches en sémantique lexicale s’appuient de plus en plus sur des ressources électroniques de grande taille (dictionnaires informatisés, corpus, ontologies) à partir desquelles on peut obtenir diverses relations sémantiques entre unités lexicales. Ces relations sont naturellement modélisées par des graphes. Bien qu’ils décrivent des phénomènes lexicaux très différents, ces graphes ont en commun des caractéristiques bien particulières. On dit qu’ils sont de type petit monde. Nous voulons mener une étude théorique mathématique et informatique de la structure de ces graphes pour le lexique. Il s’agit de les géométriser afin de faire apparaître l’organisation du lexique, qui est implicitement encodée dans leur structure. Les outils mis en place sont testés sur le graphe du dictionnaire électronique des synonymes (www.crisco.unicaen.fr). Ils constituent une extension du logiciel Visusyn développé par Ploux et Victorri (1998).</abstract>
      <url hash="4777c8f7">2004.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>venant-2004-geometriser</bibkey>
    </paper>
  </volume>
  <volume id="recitalposter" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (Posters)</booktitle>
      <editor><first>Frédéric</first><last>Béchet</last></editor>
      <editor><first>Tristan</first><last>Vanrullen</last></editor>
      <publisher>ATALA</publisher>
      <address>Fès, Maroc</address>
      <month>April</month>
      <year>2004</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="cea02cc8">2004.jeptalnrecital-recitalposter.0</url>
      <bibkey>jep-taln-recital-2004-actes-de-la-11eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>ICHARATE</fixed-case> : Un Atelier Logique pour les Grammaires Multimodales</title>
      <author><first>Houda</first><last>Anoun</last></author>
      <pages>1–6</pages>
      <abstract>Cet article présente le projet de l’atelier logique ICHARATE dédié à l’étude des grammaires catégorielles multimodales. Cet atelier se présente sous la forme de bibliothèques pour l’assistant de preuves Coq.</abstract>
      <url hash="b8a3ed5b">2004.jeptalnrecital-recitalposter.1</url>
      <language>fra</language>
      <bibkey>anoun-2004-icharate</bibkey>
    </paper>
    <paper id="2">
      <title>De la lexie au vocable : la représentation formelle des liens de polysémie</title>
      <author><first>Lucie</first><last>Barque</last></author>
      <pages>7–12</pages>
      <abstract>Cet article s’intéresse aux définitions formalisées de la base de données BDéf et montre en quoi la structure formelle de ces définitions est à même d’offrir une représentation originale de la polysémie lexicale.</abstract>
      <url hash="a6b906e7">2004.jeptalnrecital-recitalposter.2</url>
      <language>fra</language>
      <bibkey>barque-2004-de</bibkey>
    </paper>
    <paper id="3">
      <title>Système d’extraction d’information dédié à la veille Qui est qui? Qui fait quoi? Où? Quand? Comment?</title>
      <author><first>Asma</first><last>Bouhafs</last></author>
      <pages>13–18</pages>
      <abstract>Dans cet article nous présentons un outil d’extraction d’information dédié à la veille qui répond à un certain nombre de requêtes formulées par l’utilisateur, en combinant la puissance des outils et les ressources informatiques à une analyse linguistique. Cette analyse linguistique permet le repérage des entités nommées (acteurs, lieux, temps,...) ainsi que la mise en relation des acteurs avec leur environnement dans l’espace et le temps au moyen d’indices déclencheurs, d’indices complémentaires et de règles qui les combinent, c’est le principe de l’Exploration Contextuelle. Les résultats capitalisés dans des fichiers XML, sont proposés par le biais d’une interface, soit sous forme de graphes soit sous forme de base d’informations.</abstract>
      <url hash="b1723754">2004.jeptalnrecital-recitalposter.3</url>
      <language>fra</language>
      <bibkey>bouhafs-2004-systeme</bibkey>
    </paper>
    <paper id="4">
      <title>Développement d’un système de Résumé automatique de Textes Juridiques</title>
      <author><first>Atefeh</first><last>Farzindar</last></author>
      <pages>19–24</pages>
      <abstract>Nous décrivons notre méthode de production automatique du résumé de textes juridiques. C’est une nouvelle application du résumé qui permet aux juristes de consulter rapidement les idées clés d’une décision juridique pour trouver les jurisprudences pertinentes à leurs besoins. Notre approche est basée sur l’exploitation de l’architecture des documents et les structures thématiques, afin de constituer automatiquement des fiches de résumé qui augmentent la cohérence et la lisibilité du résumé. Dans cet article nous détaillons les conceptions des différentes composantes du système, appelé LetSum et le résultat d’évaluation.</abstract>
      <url hash="26184c21">2004.jeptalnrecital-recitalposter.4</url>
      <language>fra</language>
      <bibkey>farzindar-2004-developpement</bibkey>
    </paper>
    <paper id="5">
      <title>Méthodes statistiques et apprentissage automatique pour l’évaluation de requêtes en recherche documentaire</title>
      <author><first>Jens</first><last>Grivolla</last></author>
      <pages>25–30</pages>
      <abstract>Pour la recherche documentaire il est souvent intéressant d’avoir une bonne mesure de confiance dans les réponses trouvées par le moteur de recherche. Une bonne estimation de pertinence peut permettre de faire un choix entre plusieurs réponses (venant éventuellement de différents systèmes), d’appliquer des méthodes d’enrichissement additionnelles selon les besoins, ou encore de permettre à l’utilisateur de prendre des décisions (comme d’approfondir la recherche à travers un dialogue). Nous proposons une méthode permettant de faire une telle estimation, utilisant des connaissances extraites d’un ensemble de requˆetes connues pour en déduire des prédictions sur d’autres requˆetes posées au système de recherche documentaire.</abstract>
      <url hash="1c07bd3b">2004.jeptalnrecital-recitalposter.5</url>
      <language>fra</language>
      <bibkey>grivolla-2004-methodes</bibkey>
    </paper>
    <paper id="6">
      <title>Mot vide, mot plein ? Comment trancher localement</title>
      <author><first>Frédérick</first><last>Houben</last></author>
      <pages>31–36</pages>
      <abstract>Nous présentons une méthode multilingue de catégorisation en mot vide / mot plein à partir de corpus brut. Cette méthode fait appel à des propriétés très générales des langues ainsi qu’à des techniques issues de la communauté de la fouille de données.</abstract>
      <url hash="10435509">2004.jeptalnrecital-recitalposter.6</url>
      <language>fra</language>
      <bibkey>houben-2004-mot</bibkey>
    </paper>
    <paper id="7">
      <title>Génération sémantico-syntaxique pour la traduction automatique basée sur une architecture interlingue</title>
      <author><first>Mehand</first><last>Iheddadene</last></author>
      <pages>37–42</pages>
      <abstract>Dans cet article, nous présentons un processus de génération sémantico-syntaxique conçu et mis en oeuvre dans la réalisation d’un prototype de traduction automatique basée sur le modèle à structure intermédiaire (ou structure pivot). Dans une première partie de l’article, nous présentons l’organisation des ressources lexicales et sémantiques multilingues, ainsi que les mécanismes permettant d’exploiter ces ressources pour produire une représentation conceptuelle du sens de la phrase source. Dans une seconde partie, nous présentons la première phase de génération à partir d’une structure pivot (génération Sémantico-Syntaxique) permettant la construction d’une structure syntaxique profonde de la phrase cible à produire. Les autres phases de génération ne seront pas abordées dans cet article.</abstract>
      <url hash="0a7fe953">2004.jeptalnrecital-recitalposter.7</url>
      <language>fra</language>
      <bibkey>iheddadene-2004-generation</bibkey>
    </paper>
    <paper id="8">
      <title>Reconnaissance automatique des adjectifs durs et des adverbes réguliers lors de l’analyse morphologique automatique du slovaque</title>
      <author><first>Diana</first><last>Jamborova-Lemay</last></author>
      <pages>43–48</pages>
      <abstract>L’analyse morphologique automatique du slovaque constitue la première étape d’un système d’analyse automatique du contenu des textes scientifiques et techniques slovaques. Un tel système pourrait être utilisé par des applications telles que l’indexation automatique des textes, la recherche automatique de la terminologie ou par un système de traduction. Une description des régularités de la langue par un ensemble de règles ainsi que l’utilisation de tous les éléments au niveau de la forme du mot qui rendent possible son interprétation permettent de réduire d’une manière considérable le Volume des dictionnaires. Notamment s’il s’agît d’une langue à flexion très riche, comme le slovaque. La reconnaissance automatique des adjectifs durs et des adverbes réguliers constitue la partie la plus importante de nos travaux. Les résultats que nous obtenons lors de l’analyse morphologique confirment la faisabilité et la grande fiabilité d’une analyse morphologique basée sur la reconnaissance des formes et ceci pour toutes les catégories lexicales.</abstract>
      <url hash="a9e0e3a9">2004.jeptalnrecital-recitalposter.8</url>
      <language>fra</language>
      <bibkey>jamborova-lemay-2004-reconnaissance</bibkey>
    </paper>
    <paper id="9">
      <title>Traitement informatique de l’inflexion dans le Lunaf, dictionnaire électronique du luxembourgeois</title>
      <author><first>Francisca</first><last>Luna Garcia</last></author>
      <pages>49–54</pages>
      <abstract>Afin de générer les formes fléchies des noms luxembourgeois dans le dictionnaire luxembourgeois, nous utilisons un code flexionnel. Ce code s’étant révélé trop contraignant pour traiter l’inflexion (alternance vocalique/Umlaut), nous présentons ici un moyen efficace pour coder ce phénomène. La pertinence de ce type de code est double. D’une part, il correspond mieux aux besoins du linguiste qui aimerait établir des classes flexionnelles naturelles sans trop de contraintes informatiques. D’autre part, il permet de réduire significativement le nombre de classes flexionnelles. Le dictionnaire électronique luxembourgeois dispose ainsi de deux codes qui peuvent se combiner entre eux pour mieux traiter les particularités morphologiques des mots luxembourgeois.</abstract>
      <url hash="36194fd8">2004.jeptalnrecital-recitalposter.9</url>
      <language>fra</language>
      <bibkey>luna-garcia-2004-traitement</bibkey>
    </paper>
    <paper id="10">
      <title>Nouvelle méthode syntagmatique de vectorisation appliquée au self-organizing map des textes vietnamiens</title>
      <author><first>Tuan-Dang</first><last>Nguyen</last></author>
      <pages>55–60</pages>
      <abstract>Par ses caractéristiques éminentes dans la présentation des données, Self-Organizing Map (SOM) est particulièrement convenable à l’organisation des cartes. SOM se comporte d’un ensemble des vecteurs prototypes pour représenter les données d’entrée, et fait une projection, en conservant la topologie, à partir des vecteurs prototypes de n-dimensions sur une carte de 2-dimensions. Cette carte deviendra une vision qui reflète la structure des classes des données. Nous notons un problème crucial pour SOM, c’est la méthode de vectorisation des données. Dans nos études, les données se présentent sous forme des textes. Bien que le modèle général du SOM soit déjà créé, il nous faut de nouvelles recherches pour traiter des langues spécifiques, comme le vietnamien, qui sont de nature assez différente de l’anglais. Donc, nous avons appliqué la conception du syntagme pour établir un algorithme qui est capable de résoudre ce problème.</abstract>
      <url hash="2e68974c">2004.jeptalnrecital-recitalposter.10</url>
      <language>fra</language>
      <bibkey>nguyen-2004-nouvelle</bibkey>
    </paper>
    <paper id="11">
      <title>Représentation compositionnelle de la sémantique de aussi</title>
      <author><first>Céline</first><last>Raynal</last></author>
      <pages>61–66</pages>
      <abstract>L’objectif de notre travail est de dégager une représentation formelle compositionnelle de la contribution sémantique de aussi lorsqu’il a une valeur additive. Plusieurs problèmes de compositionnalité, liés surtout à la diversité des arguments concernés par l’adverbe, vont se poser. Nous proposons une alternative compositionnelle à la représentation proposée initialement en l-DRT.</abstract>
      <url hash="c072805e">2004.jeptalnrecital-recitalposter.11</url>
      <language>fra</language>
      <bibkey>raynal-2004-representation</bibkey>
    </paper>
    <paper id="12">
      <title>La Transcription Orthographique-Phonetique De La Langue Arabe</title>
      <author><first>Tahar</first><last>Saidane</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>67–72</pages>
      <abstract>Notre article présente les composants nécessaires à la synthèse de la parole arabe. Nous nous attarderons sur la transcription graphème phonème, étape primordiale pour l’élaboration d’un système de synthèse d’une qualité acceptable. Nous présenterons ensuite quelques-unes des règles utilisées pour la réalisation de notre système de traitement phonétique. Ces règles sont, pour notre système, stockées dans une base de données et sont parcourues plusieurs fois lors de la transcription.</abstract>
      <url hash="7b1b4a41">2004.jeptalnrecital-recitalposter.12</url>
      <language>fra</language>
      <bibkey>saidane-etal-2004-la</bibkey>
    </paper>
    <paper id="13">
      <title>Towards a rule-guided derivation of aspectual readings in <fixed-case>R</fixed-case>ussian</title>
      <author><first>Barbara</first><last>Sonnenhauser</last></author>
      <pages>73–78</pages>
      <abstract>Natural language expressions are underspecified and require enrichment to develop into full fledged propositions. Their sense-general semantics must be complemented with pragmatic inferences that have to be systematically figured out and pinned down in a principled way, so as to make them suitable inputs for NLP algorithms. This paper deals with the underspecified ipf1 aspect in Russian and introduces a semantic and pragmatic framework that might serve as the basis for a rule-guided derivation of its different readings.</abstract>
      <url hash="8f2431ff">2004.jeptalnrecital-recitalposter.13</url>
      <bibkey>sonnenhauser-2004-towards</bibkey>
    </paper>
    <paper id="14">
      <title>Un système adaptable pour l’initialisation automatique d’une base lexicale interlingue par acceptions</title>
      <author><first>Aree</first><last>Teeraparbseree</last></author>
      <pages>79–84</pages>
      <abstract>Cet article présente une stratégie de construction semi-automatique d’une base lexicale interlingue par acception, à partir de ressources existantes, qui utilise en synergie des techniques existantes de désambiguïsation. Les apports et limitations de chaque technique sont présentés. Notre proposition est de pouvoir composer arbitrairement des techniques, en fonction des ressources disponibles, afin d’obtenir une base interlingue de la qualité souhaitée. Jeminie, un système adaptable qui met en oeuvre cette stratégie, est introduit dans cet article.</abstract>
      <url hash="87241388">2004.jeptalnrecital-recitalposter.14</url>
      <language>fra</language>
      <bibkey>teeraparbseree-2004-un</bibkey>
    </paper>
    <paper id="15">
      <title>Analyse syntaxique et granularité variable</title>
      <author><first>Tristan</first><last>VanRullen</last></author>
      <pages>85–90</pages>
      <abstract>Il est souhaitable qu’une analyse syntaxique -en traitement automatique des langues naturellessoit réalisée avec plus ou moins de précision en fonction du contexte, c’est-à-dire que sa granularité soit réglable. Afin d’atteindre cet objectif, nous présentons ici des études préliminaires permettant d’appréhender les contextes technique et scientifique qui soulèvent ce problème. Nous établissons un cadre pour les développements à réaliser. Plusieurs types de granularité sont définis. Puis nous décrivons une technique basée sur la densité de satisfaction, développée dans ce cadre avec des algorithmes basés sur un formalisme de satisfaction de contraintes (celui des Grammaires de Propriétés) ayant l’avantage de permettre l’utilisation des mêmes ressources linguistiques avec un degré de précision réglable. Enfin, nous envisageons les développements ultérieurs pour une analyse syntaxique à granularité variable.</abstract>
      <url hash="e8592db2">2004.jeptalnrecital-recitalposter.15</url>
      <language>fra</language>
      <bibkey>vanrullen-2004-analyse</bibkey>
    </paper>
    <paper id="16">
      <title>Réutilisation de traducteurs gratuits pour développer des systèmes multilingues</title>
      <author><first>Hung</first><last>Vo Trung</last></author>
      <pages>91–96</pages>
      <abstract>Nous présentons ici une méthode de réutilisation de systèmes de traduction automatique gratuits en ligne pour développer des applications multilingues et évaluer ces mêmes systèmes. Nous avons développé un outil de traitement et de traduction de documents hétérogènes (multilingues et multicodage). Cet outil permet d’identifier la langue et le codage du texte, de segmenter un texte hétérogène en zones homogènes, d’appeler un traducteur correspondant avec une paire de langue source et cible, et de récupérer les résultats traduits dans la langue souhaitée. Cet outil est utilisable dans plusieurs applications différentes comme la recherche multilingue, la traduction des courriers électroniques, la construction de sites web multilingues, etc.</abstract>
      <url hash="f6832e3e">2004.jeptalnrecital-recitalposter.16</url>
      <language>fra</language>
      <bibkey>vo-trung-2004-reutilisation</bibkey>
    </paper>
    <paper id="17">
      <title>La relation de synonymie en génomique</title>
      <author><first>Davy</first><last>Weissenbacher</last></author>
      <pages>97–102</pages>
      <abstract>L’accès au contenu des textes de génomique est aujourd’hui un enjeu important. Cela suppose au départ d’identifier les noms d’entités biologiques comme les gènes ou les protéines. Se pose alors la question de la variation de ces noms. Cette question revêt une importance particulière en génomique où les noms de gènes sont soumis à de nombreuses variations, notamment la synonymie. A partir d’une étude de corpus montrant que la synonymie est une relation stable et linguistiquement marquée, cet article propose une modélisation de la synonymie et une méthode d’extraction spécifiquement adaptée à cette relation. Au vu de nos premières expériences, cette méthode semble plus prometteuse que les approches génériques utilisées pour l’extraction de cette relation.</abstract>
      <url hash="6f541e3f">2004.jeptalnrecital-recitalposter.17</url>
      <language>fra</language>
      <bibkey>weissenbacher-2004-la</bibkey>
    </paper>
    <paper id="18">
      <title>Analyse macro-sémantique: vers une analyse rhétorique du discours</title>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>103–108</pages>
      <abstract>S’inscrivant dans les domaines du TAL, de la linguistique sur corpus et de l’informatique documentaire, l’étude présentée ici opère plus précisément dans la perspective d’une analyse macrosémantique de la structuration discursive. Plus spécifiquement, nous proposons une analyse sémantique des structures rhétoriques du discours. Après avoir envisagé certaines voies ouvertes en la matière, nous définissons notre approche, et présentons les expérimentations conduites, dans le cadre du projet GeoSem, sur les structures énumératives dans le domaine géographique.</abstract>
      <url hash="efa9843a">2004.jeptalnrecital-recitalposter.18</url>
      <language>fra</language>
      <bibkey>widlocher-2004-analyse</bibkey>
    </paper>
  </volume>
</collection>
