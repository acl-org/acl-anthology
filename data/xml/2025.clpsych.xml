<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.clpsych">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 10th Workshop on Computational Linguistics and Clinical Psychology (CLPsych 2025)</booktitle>
      <editor><first>Ayah</first><last>Zirikly</last></editor>
      <editor><first>Andrew</first><last>Yates</last></editor>
      <editor><first>Bart</first><last>Desmet</last></editor>
      <editor id="molly-ireland"><first>Molly</first><last>Ireland</last></editor>
      <editor><first>Steven</first><last>Bedrick</last></editor>
      <editor><first>Sean</first><last>MacAvaney</last></editor>
      <editor><first>Kfir</first><last>Bar</last></editor>
      <editor><first>Yaakov</first><last>Ophir</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="9fdbbf57">2025.clpsych-1</url>
      <venue>clpsych</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-226-8</isbn>
      <doi>10.18653/v1/2025.clpsych-1</doi>
    </meta>
    <frontmatter>
      <url hash="e9b2eb11">2025.clpsych-1.0</url>
      <bibkey>clpsych-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Assessing the Reliability and Validity of <fixed-case>GPT</fixed-case>-4 in Annotating Emotion Appraisal Ratings</title>
      <author><first>Deniss</first><last>Ruder</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Andero</first><last>Uusberg</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Kairit</first><last>Sirts</last><affiliation>University of Tartu</affiliation></author>
      <pages>1-11</pages>
      <abstract>Appraisal theories suggest that emotions arise from subjective evaluations of events, referred to as appraisals. The taxonomy of appraisals is quite diverse, and they are usually given ratings on a Likert scale to be annotated in an experiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as a reader-annotator of 21 specific appraisal ratings in different prompt settings, aiming to evaluate and improve its performance compared to human annotators. We found that GPT-4 is an effective reader-annotator that performs close to or even slightly better than human annotators, and its results can be significantly improved by using a majority voting of five completions. GPT-4 also effectively predicts appraisal ratings and emotion labels using a single prompt, but adding instruction complexity results in poorer performance. We also found that longer event descriptions lead to more accurate annotations for both model and human annotator ratings. This work contributes to the growing usage of LLMs in psychology and the strategies for improving GPT-4 performance in annotating appraisals.</abstract>
      <url hash="769811c3">2025.clpsych-1.1</url>
      <bibkey>ruder-etal-2025-assessing</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.1</doi>
      <revision id="1" href="2025.clpsych-1.1v1" hash="84285fac"/>
      <revision id="2" href="2025.clpsych-1.1v2" hash="769811c3" date="2026-01-29">Add acknowledgment.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>A</fixed-case>uto<fixed-case>P</fixed-case>sy<fixed-case>C</fixed-case>: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models</title>
      <author><first>Sayed</first><last>Hossain</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Patrick</first><last>Gebhard</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Cord</first><last>Benecke</last><affiliation>Department of Psychology, University of Kassel, Kassel, Germany</affiliation></author>
      <author id="josef-van-genabith"><first>Josef</first><last>van Genabith</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Philipp</first><last>Müller</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <pages>12-25</pages>
      <abstract>Psychodynamic conflicts are persistent, often unconscious themes that shape a person’s behaviour and experiences. Accurate diagnosis of psychodynamic conflicts is crucial for effective patient treatment and is commonly done via long, manually scored semi-structured interviews. Existing automated solutions for psychiatric diagnosis tend to focus on the recognition of broad disorder categories such as depression, and it is unclear to what extent psychodynamic conflicts which even the patient themselves may not have conscious access to could be automatically recognised from conversation. In this paper, we propose AutoPsyC, the first method for recognising the presence and significance of psychodynamic conflicts from full-length Operationalized Psychodynamic Diagnostics (OPD) interviews using Large Language Models (LLMs). Our approach combines recent advances in parameter-efficient fine-tuning and Retrieval-Augmented Generation (RAG) with a summarisation strategy to effectively process entire 90 minute long conversations. In evaluations on a dataset of 141 diagnostic interviews we show that AutoPsyC consistently outperforms all baselines and ablation conditions on the recognition of four highly relevant psychodynamic conflicts.</abstract>
      <url hash="14392c53">2025.clpsych-1.2</url>
      <bibkey>hossain-etal-2025-autopsyc</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.2</doi>
    </paper>
    <paper id="3">
      <title>The Emotional Spectrum of <fixed-case>LLM</fixed-case>s: Leveraging Empathy and Emotion-Based Markers for Mental Health Support</title>
      <author><first>Alessandro</first><last>De Grandi</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <author><first>Federico</first><last>Ravenda</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <author><first>Andrea</first><last>Raballo</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <author><first>Fabio</first><last>Crestani</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <pages>26-43</pages>
      <abstract>The increasing demand for mental health services has highlighted the need for innovative solutions, particularly in the realm of psychological conversational AI, where the availability of sensitive data is scarce. In this work, we explored the development of a system tailored for mental health support with a novel approach to psychological assessment based on explainable emotional profiles in combination with empathetic conversational models, offering a promising tool for augmenting traditional care, particularly where immediate expertise is unavailable. Our work can be divided into two main parts, intrinsecaly connected to each other. First, we present RACLETTE, a conversational system that demonstrates superior emotional accuracy compared to considered benchmarks in both understanding users’ emotional states and generating empathetic responses during conversations, while progressively building an emotional profile of the user through their interactions. Second, we show how the emotional profiles of a user can be used as interpretable markers for mental health assessment. These profiles can be compared with characteristic emotional patterns associated with different mental disorders, providing a novel approach to preliminary screening and support.</abstract>
      <url hash="74b284e7">2025.clpsych-1.3</url>
      <bibkey>de-grandi-etal-2025-emotional</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.3</doi>
    </paper>
    <paper id="4">
      <title>Enhancing Depression Detection via Question-wise Modality Fusion</title>
      <author><first>Aishik</first><last>Mandal</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Dana</first><last>Atzil-Slonim</last><affiliation>Psychology Department Bar-Ilan University</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische UniversitÃ¤t Darmstadt</affiliation></author>
      <pages>44-61</pages>
      <abstract>Depression is a highly prevalent and disabling condition that incurs substantial personal and societal costs. Current depression diagnosis involves determining the depression severity of a person through self-reported questionnaires or interviews conducted by clinicians. This often leads to delayed treatment and involves substantial human resources. Thus, several works try to automate the process using multimodal data. However, they usually overlook the following: i) The variable contribution of each modality for each question in the questionnaire and ii) Using ordinal classification for the task. This results in sub-optimal fusion and training methods. In this work, we propose a novel Question-wise Modality Fusion (QuestMF) framework trained with a novel Imbalanced Ordinal Log-Loss (ImbOLL) function to tackle these issues. The performance of our framework is comparable to the current state-of-the-art models on the E-DAIC dataset and enhances interpretability by predicting scores for each question. This will help clinicians identify an individual’s symptoms, allowing them to customise their interventions accordingly. We also make the code for the QuestMF framework publicly available.</abstract>
      <url hash="e2e79849">2025.clpsych-1.4</url>
      <bibkey>mandal-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.4</doi>
    </paper>
    <paper id="5">
      <title>Linking Language-based Distortion Detection to Mental Health Outcomes</title>
      <author><first>Vasudha</first><last>Varadarajan</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Allison</first><last>Lahnala</last><affiliation>NIDA, NIH</affiliation></author>
      <author><first>Sujeeth</first><last>Vankudari</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Akshay</first><last>Raghavan</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Scott</first><last>Feltman</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Syeda</first><last>Mahwish</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Camilo</first><last>Ruggero</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Roman</first><last>Kotov</last><affiliation>Stony Brook University</affiliation></author>
      <author id="h-andrew-schwartz"><first>H. Andrew</first><last>Schwartz</last><affiliation>Stony Brook University</affiliation></author>
      <pages>62-68</pages>
      <abstract>Recent work has suggested detection of cognitive distortions as an impactful task for NLP in the clinical space, but the connection between language-detected distortions and validated mental health outcomes has been elusive. In this work, we evaluate the co-occurrence of (a) 10 distortions derived from language-based detectors trained over two common distortion datasets with (b) 12 mental health outcomes contained within two new language-to-mental-health datasets: DS4UD and iHiTOP. We find higher rates of distortions for those with greater mental health condition severity (ranging from r = 0.16 for thought disorders to r = 0.46 for depressed mood), and that the specific distortions of should statements and fortune telling were associated with a depressed mood and being emotionally drained, respectively. This suggested that language-based assessments of cognitive distortion could play a significant role in detection and monitoring of mental health conditions.</abstract>
      <url hash="8399624d">2025.clpsych-1.5</url>
      <bibkey>varadarajan-etal-2025-linking</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.5</doi>
    </paper>
    <paper id="6">
      <title>Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches</title>
      <author><first>Chen</first><last>Shani</last><affiliation>Stanford</affiliation></author>
      <author><first>Elizabeth</first><last>Stade</last><affiliation>Stanford University</affiliation></author>
      <pages>69-78</pages>
      <abstract>Computational mental health research develops models to predict and understand psychological phenomena, but often relies on inappropriate measures of psychopathology constructs, undermining validity. We identify three key issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis) over validated ones (e.g., diagnosis by clinician); (2) treating mental health constructs as categorical rather than dimensional; and (3) focusing on disorder-specific constructs instead of transdiagnostic ones. We outline the benefits of using validated, dimensional, and transdiagnostic measures and offer practical recommendations for practitioners. Using valid measures that reflect the nature and structure of psychopathology is essential for computational mental health research.</abstract>
      <url hash="3fc4286a">2025.clpsych-1.6</url>
      <bibkey>shani-stade-2025-measuring</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.6</doi>
    </paper>
    <paper id="7">
      <title>Automatic Scoring of an Open-Response Measure of Advanced Mind-Reading Using Large Language Models</title>
      <author><first>Yixiao</first><last>Wang</last><affiliation>Birmingham University</affiliation></author>
      <author><first>Russel</first><last>Dsouza</last><affiliation>School of Computer Science, University of Birmingham</affiliation></author>
      <author><first>Robert</first><last>Lee</last><affiliation>School of Psychology, University of Birmingham</affiliation></author>
      <author><first>Ian</first><last>Apperly</last><affiliation>School of Psychology, University of Birmingham</affiliation></author>
      <author><first>Rory</first><last>Devine</last><affiliation>School of Psychology, University of Birmingham</affiliation></author>
      <author><first>Sanne</first><last>van der Kleij</last><affiliation>School of Psychology, University of Birmingham</affiliation></author>
      <author id="mark-lee"><first>Mark</first><last>Lee</last><affiliation>School of Computer Science, University of Birmingham</affiliation></author>
      <pages>79-89</pages>
      <abstract>A rigorous psychometric approach is crucial for the accurate measurement of mind-reading abilities. Traditional scoring methods for such tests, which involve lengthy free-text responses, require considerable time and human effort. This study investigates the use of large language models (LLMs) to automate the scoring of psychometric tests. Data were collected from participants aged 13 to 30 years and scored by trained human coders to establish a benchmark. We evaluated multiple LLMs against human assessments, exploring various prompting strate- gies to optimize performance and fine-tuning the models using a subset of the collected data to enhance accuracy. Our results demonstrate that LLMs can assess advanced mind-reading abilities with over 90% accuracy on average. Notably, in most test items, the LLMs achieved higher Kappa agreement with the lead coder than two trained human coders, highlighting their potential to reliably score open-response psychometric tests.</abstract>
      <url hash="cdc13bb9">2025.clpsych-1.7</url>
      <bibkey>wang-etal-2025-automatic</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.7</doi>
    </paper>
    <paper id="8">
      <title>Bigger But Not Better: Small Neural Language Models Outperform <fixed-case>LLM</fixed-case>s in Detection of Thought Disorder</title>
      <author id="changye-li-umn"><first>Changye</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Weizhe</first><last>Xu</last><affiliation>University of Washington</affiliation></author>
      <author id="serguei-pakhomov"><first>Serguei</first><last>Pakhomov</last><affiliation>University of Minnesota</affiliation></author>
      <author><first>Ellen</first><last>Bradley</last><affiliation>University of California, San Francisco</affiliation></author>
      <author><first>Dror</first><last>Ben-Zeev</last><affiliation>University of Washington</affiliation></author>
      <author id="trevor-cohen"><first>Trevor</first><last>Cohen</last><affiliation>University of Washington</affiliation></author>
      <pages>90-105</pages>
      <abstract>Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum disorders. Recently, clinical estimates of the severity of disorganized thinking have been shown to correlate with measures of how difficult speech transcripts would be for large language models (LLMs) to predict. However, LLMs’ deployment challenges – including privacy concerns, computational and financial costs, and lack of transparency of training data – limit their clinical utility. We investigate whether smaller neural language models can serve as effective alternatives for detecting positive formal thought disorder, using the same sliding window based perplexity measurements that proved effective with larger models. Surprisingly, our results show that smaller models are more sensitive to linguistic differences associated with formal thought disorder than their larger counterparts. Detection capability declines beyond a certain model size and context length, challenging the common assumption of “bigger is better” for LLM-based applications. Our findings generalize across audio diaries and clinical interview speech samples from individuals with psychotic symptoms, suggesting a promising direction for developing efficient, cost-effective, and privacy-preserving screening tools that can be deployed in both clinical and naturalistic settings.</abstract>
      <url hash="bdfa9c75">2025.clpsych-1.8</url>
      <bibkey>li-etal-2025-bigger</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>CF</fixed-case>i<fixed-case>CS</fixed-case>: Graph-Based Classification of Common Factors and Microcounseling Skills</title>
      <author><first>Fabian</first><last>Schmidt</last><affiliation>KTH Royal Institute of Technology</affiliation></author>
      <author><first>Karin</first><last>Hammerfald</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Henrik</first><last>Haaland Jahren</last><affiliation>Braive AS</affiliation></author>
      <author><first>Vladimir</first><last>Vlassov</last><affiliation>KTH Royal Institute of Technology</affiliation></author>
      <pages>106-115</pages>
      <abstract>Common factors and microcounseling skills are critical to the effectiveness of psychotherapy. Understanding and measuring these elements provides valuable insights into therapeutic processes and outcomes. However, automatic identification of these change principles from textual data remains challenging due to the nuanced and context-dependent nature of therapeutic dialogue. This paper introduces CFiCS, a hierarchical classification framework integrating graph machine learning with pre-trained contextual embeddings. We represent common factors, intervention concepts, and microcounseling skills as a heterogeneous graph, where textual information from ClinicalBERT enriches each node. This structure captures both the hierarchical relationships (e.g., skill-level nodes linking to broad factors) and the semantic properties of therapeutic concepts. By leveraging graph neural networks, CFiCS learns inductive node embeddings that generalize to unseen text samples lacking explicit connections. Our results demonstrate that integrating ClinicalBERT node features and graph structure significantly improves classification performance, especially in fine-grained skill prediction. CFiCS achieves substantial gains in both micro and macro F1 scores across all tasks compared to baselines, including random forests, BERT-based multi-task models, and graph-based methods.</abstract>
      <url hash="fdc37b99">2025.clpsych-1.9</url>
      <bibkey>schmidt-etal-2025-cfics</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.9</doi>
    </paper>
    <paper id="10">
      <title>Datasets for Depression Modeling in Social Media: An Overview</title>
      <author><first>Ana-Maria</first><last>Bucur</last><affiliation>Interdisciplinary School of Doctoral Studies</affiliation></author>
      <author><first>Andreea</first><last>Moldovan</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Krutika</first><last>Parvatikar</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <author><first>Ashiqur</first><last>Khudabukhsh</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author id="liviu-p-dinu"><first>Liviu</first><last>Dinu</last><affiliation>University of Bucharest</affiliation></author>
      <pages>116-126</pages>
      <abstract>Depression is the most common mental health disorder, and its prevalence increased during the COVID-19 pandemic. As one of the most extensively researched psychological conditions, recent research has increasingly focused on leveraging social media data to enhance traditional methods of depression screening. This paper addresses the growing interest in interdisciplinary research on depression, and aims to support early-career researchers by providing a comprehensive and up-to-date list of datasets for analyzing and predicting depression through social media data. We present an overview of datasets published between 2019 and 2024. We also make the comprehensive list of datasets available online as a continuously updated resource, with the hope that it will facilitate further interdisciplinary research into the linguistic expressions of depression on social media.</abstract>
      <url hash="68638efc">2025.clpsych-1.10</url>
      <bibkey>bucur-etal-2025-datasets</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.10</doi>
    </paper>
    <paper id="11">
      <title>Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals</title>
      <author><first>Navneet</first><last>Agarwal</last><affiliation>EXAI, University of Tartu</affiliation></author>
      <author><first>Kairit</first><last>Sirts</last><affiliation>University of Tartu</affiliation></author>
      <pages>127-139</pages>
      <abstract>In recent years, there has been growing interest in studying cognitive distortions and emotional appraisals from both computational and psychological perspectives. Despite considerable similarities between emotional reappraisal and cognitive reframing as emotion regulation techniques, these concepts have largely been examined in isolation. This research explores the relationship between cognitive distortions and emotional appraisal dimensions, examining their potential connections and relevance for future interdisciplinary studies. Under this pretext, we conduct an exploratory computational study, aimed at investigating the relationship between cognitive distortion and emotional appraisals. We show that the patterns of statistically significant relationships between cognitive distortions and appraisal dimensions vary across different distortion categories, giving rise to distinct appraisal profiles for individual distortion classes. Additionally, we analyze the impact of cognitive restructuring on appraisal dimensions, exemplifying the emotion regulation aspect of cognitive restructuring.</abstract>
      <url hash="d0442f87">2025.clpsych-1.11</url>
      <bibkey>agarwal-sirts-2025-exploratory</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.11</doi>
      <revision id="1" href="2025.clpsych-1.11v1" hash="f1d16d59"/>
      <revision id="2" href="2025.clpsych-1.11v2" hash="d0442f87" date="2026-01-10">Add sponsor to acknowledgments.</revision>
    </paper>
    <paper id="12">
      <title>Socratic Reasoning Improves Positive Text Rewriting</title>
      <author><first>Anmol</first><last>Goel</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Nico</first><last>Daheim</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Christian</first><last>Montag</last><affiliation>Ulm University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>TU Darmstadt</affiliation></author>
      <pages>140-156</pages>
      <abstract>Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called SOCRATICREFRAME. SOCRATICREFRAME uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research. We validate our framework and the synthetic rationalizations with expert judgements from domain experts and psychology students in an IRB-approved annotation study. Our findings highlight the potential of utilizing the synergy between LLM reasoning and established psychotherapy techniques to build assistive solutions for reframing negative thoughts.</abstract>
      <url hash="b0e84c16">2025.clpsych-1.12</url>
      <bibkey>goel-etal-2025-socratic</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.12</doi>
    </paper>
    <paper id="13">
      <title>Synthetic Empathy: Generating and Evaluating Artificial Psychotherapy Dialogues to Detect Empathy in Counseling Sessions</title>
      <author><first>Daniel</first><last>Cabrera Lozoya</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Eloy</first><last>Hernandez Lua</last><affiliation>ITESM</affiliation></author>
      <author><first>Juan Alberto</first><last>Barajas Perches</last><affiliation>ITESM</affiliation></author>
      <author><first>Mike</first><last>Conway</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Simon</first><last>D’Alfonso</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>157-171</pages>
      <abstract>Natural language processing (NLP) holds potential for analyzing psychotherapy transcripts. Nonetheless, gathering the necessary data to train NLP models for clinical tasks is a challenging process due to patient confidentiality regulations that restrict data sharing. To overcome this obstacle, we propose leveraging large language models (LLMs) to create synthetic psychotherapy dialogues that can be used to train NLP models for downstream clinical tasks. To evaluate the quality of our synthetic data, we trained three multi-task RoBERTa-based bi-encoder models, originally developed by Sharma et al., to detect empathy in dialogues. These models, initially trained on Reddit data, were developed alongside EPITOME, a framework designed to characterize empathetic communication in conversations. We collected and annotated 579 therapeutic interactions between therapists and patients using the EPITOME framework. Additionally, we generated 10,464 synthetic therapeutic dialogues using various LLMs and prompting techniques, all of which were annotated following the EPITOME framework. We conducted two experiments: one where we augmented the original dataset with synthetic data and another where we replaced the Reddit dataset with synthetic data. Our first experiment showed that incorporating synthetic data can improve the F1 score of empathy detection by up to 10%. The second experiment revealed no substantial differences between organic and synthetic data, as their performance remained on par when substituted.</abstract>
      <url hash="9481f9cd">2025.clpsych-1.13</url>
      <bibkey>cabrera-lozoya-etal-2025-synthetic</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.13</doi>
    </paper>
    <paper id="14">
      <title>A Systematic Evaluation of <fixed-case>LLM</fixed-case> Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. <fixed-case>RAG</fixed-case></title>
      <author><first>Arshia</first><last>Kermani</last><affiliation>Texas state university</affiliation></author>
      <author><first>Veronica</first><last>Perez-Rosas</last><affiliation>Texas State University</affiliation></author>
      <author><first>Vangelis</first><last>Metsis</last><affiliation>Texas State University</affiliation></author>
      <pages>172-180</pages>
      <abstract>This study presents a systematic comparison of three approaches for the analysis of mental health text using large language models (LLMs): prompt engineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA 3, we evaluate these approaches on emotion classification and mental health condition detection tasks across two datasets. Fine-tuning achieves the highest accuracy (91% for emotion classification, 80% for mental health conditions) but requires substantial computational resources and large training sets, while prompt engineering and RAG offer more flexible deployment with moderate performance (40-68% accuracy). Our findings provide practical insights for implementing LLM-based solutions in mental health applications, highlighting the trade-offs between accuracy, computational requirements, and deployment flexibility.</abstract>
      <url hash="2f9d2678">2025.clpsych-1.14</url>
      <bibkey>kermani-etal-2025-systematic</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.14</doi>
    </paper>
    <paper id="15">
      <title>Using <fixed-case>LLM</fixed-case>s to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia</title>
      <author><first>Ankit</first><last>Aich</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Avery</first><last>Quynh</last><affiliation>UCSD</affiliation></author>
      <author><first>Pamela</first><last>Osseyi</last><affiliation>NIH</affiliation></author>
      <author><first>Amy</first><last>Pinkham</last><affiliation>UTDallas</affiliation></author>
      <author><first>Philip</first><last>Harvey</last><affiliation>UMiami</affiliation></author>
      <author><first>Brenda</first><last>Curtis</last><affiliation>NIH</affiliation></author>
      <author><first>Colin</first><last>Depp</last><affiliation>UCSD</affiliation></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>181-192</pages>
      <abstract>Natural Language Processing (NLP) in mental health has largely focused on social media data or classification problems, often shifting focus from high caseloads or domain-specific needs of real-world practitioners. This study utilizes a dataset of 644 participants, including those with Bipolar Disorder, Schizophrenia, and Healthy Controls, who completed tasks from a standardized mental health instrument. Clinical annotators were used to label this dataset on five clinical variables. Expert annotations across five clinical variables demonstrated that contempo- rary language models, particularly smaller, fine-tuned models, can enhance data collection and annotation with greater accuracy and trust than larger commercial models. We show that these models can effectively capture nuanced clinical variables, offering a powerful tool for advancing mental health research. We also show that for clinically advanced tasks such as domain-specific annotation LLMs provide wrong labels as compared to a fine-tuned smaller model.</abstract>
      <url hash="4094684b">2025.clpsych-1.15</url>
      <bibkey>aich-etal-2025-using</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.15</doi>
    </paper>
    <paper id="16">
      <title>Overview of the <fixed-case>CLP</fixed-case>sych 2025 Shared Task: Capturing Mental Health Dynamics from Social Media Timelines</title>
      <author><first>Talia</first><last>Tseriotou</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Jenny</first><last>Chim</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Ayal</first><last>Klein</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Aya</first><last>Shamir</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Guy</first><last>Dvir</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Iqra</first><last>Ali</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Cian</first><last>Kennedy</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Guneet</first><last>Singh Kohli</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Anthony</first><last>Hills</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Ayah</first><last>Zirikly</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dana</first><last>Atzil-Slonim</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University of London &amp; The Alan Turing Institute</affiliation></author>
      <pages>193-217</pages>
      <abstract>We provide an overview of the CLPsych 2025 Shared Task, which focuses on capturing mental health dynamics from social media timelines. Building on CLPsych 2022’s longitudinal modeling approach, this work combines monitoring mental states with evidence and summary generation through four subtasks: (A.1) Evidence Extraction, highlighting text spans reflecting adaptive or maladaptive self-states; (A.2) Well-Being Score Prediction, assigning posts a 1 to 10 score based on social, occupational, and psychological functioning; (B) Post-level Summarization of the interplay between adaptive and maladaptive states within individual posts; and (C) Timeline-level Summarization capturing temporal dynamics of self-states over posts in a timeline. We describe key findings and future directions.</abstract>
      <url hash="58ef293a">2025.clpsych-1.16</url>
      <bibkey>tseriotou-etal-2025-overview</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.16</doi>
    </paper>
    <paper id="17">
      <title>A baseline for self-state identification and classification in mental health data: <fixed-case>CLP</fixed-case>sych 2025 Task</title>
      <author><first>Laerdon</first><last>Kim</last><affiliation>Cornell University</affiliation></author>
      <pages>218-224</pages>
      <abstract>We present a baseline for the CLPsych 2025 A.1 task: classifying self-states in mental health data taken from Reddit. We use few-shot learning with a 4-bit quantized Gemma 2 9B model (Gemma Team, 2024; Brown et al., 2020; Daniel Han and team, 2023) and a data preprocessing step which first identifies relevant sentences indicating self-state evidence, and then performs a binary classification to determine whether the sentence is evidence of an adaptive or maladaptive self-state. This system outperforms our other method which relies on an LLM to highlight spans of variable length independently. We attribute the performance of our model to the benefits of this sentence chunking step for two reasons: partitioning posts into sentences 1) broadly matches the granularity at which self-states were human-annotated and 2) simplifies the task for our language model to a binary classification problem. Our system placed third out of fourteen systems submitted for Task A.1, earning a test-time recall of 0.579.</abstract>
      <url hash="4a442435">2025.clpsych-1.17</url>
      <bibkey>kim-2025-baseline</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.17</doi>
    </paper>
    <paper id="18">
      <title>Capturing the Dynamics of Mental Well-Being: Adaptive and Maladaptive States in Social Media</title>
      <author><first>Anastasia</first><last>Sandu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Teodor</first><last>Mihailescu</last><affiliation>Unibuc</affiliation></author>
      <author><first>Ana Sabina</first><last>Uban</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ana-Maria</first><last>Bucur</last><affiliation>Interdisciplinary School of Doctoral Studies</affiliation></author>
      <pages>225-234</pages>
      <abstract>This paper describes the contributions of the BLUE team in the CLPsych 2025 Shared Task on Capturing Mental Health Dynamics from Social Media Timelines. We participate in all tasks with three submissions, for which we use two sets of approaches: an unsupervised approach using prompting of various large language models (LLM) with no fine-tuning for this task or domain, and a supervised approach based on several lightweight machine learning models trained to classify sentences for evidence extraction, based on an augmented training dataset sourced from public psychological questionnaires. We obtain the best results for summarization Tasks B and C in terms of consistency, and the best F1 score in Task A.2.</abstract>
      <url hash="72d6e1ef">2025.clpsych-1.18</url>
      <bibkey>sandu-etal-2025-capturing</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>CIOL</fixed-case> at <fixed-case>CLP</fixed-case>sych 2025: Using Large Lanuage Models for Understanding and Summarizing Clinical Texts</title>
      <author><first>Md. Iqramul</first><last>Hoque</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <author><first>Mahfuz Ahmed</first><last>Anik</last><affiliation>Shahjalal University of Science &amp; Technology, Bangladesh</affiliation></author>
      <author><first>Azmine Toushik</first><last>Wasi</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <pages>235-241</pages>
      <abstract>The increasing prevalence of mental health discourse on social media has created a need for automated tools to assess psychological wellbeing. In this study, we propose a structured framework for evidence extraction, well-being scoring, and summary generation, developed as part of the CLPsych 2025 shared task. Our approach integrates feature-based classification with context-aware language modeling to identify self-state indicators, predict well-being scores, and generate clinically relevant summaries. Our system achieved a recall of 0.56 for evidence extraction, an MSE of 3.89 in well-being scoring, and high consistency scores (0.612 post-level, 0.801 timeline-level) in summary generation, ensuring strong alignment with extracted evidence. With an overall good rank, our framework demonstrates robustness in social media-based mental health monitoring. By providing interpretable assessments of psychological states, our work contributes to early detection and intervention strategies, assisting researchers and mental health professionals in understanding online well-being trends and enhancing digital mental health support systems.</abstract>
      <url hash="892430cd">2025.clpsych-1.19</url>
      <bibkey>hoque-etal-2025-ciol</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.19</doi>
    </paper>
    <paper id="20">
      <title>From Evidence Mining to Meta-Prediction: a Gradient of Methodologies for Task-Specific Challenges in Psychological Assessment</title>
      <author><first>Federico</first><last>Ravenda</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <author><first>Fawzia-Zehra</first><last>Kara-Isitt</last><affiliation>Brunel University</affiliation></author>
      <author><first>Stephen</first><last>Swift</last><affiliation>Brunel University</affiliation></author>
      <author><first>Antonietta</first><last>Mira</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <author><first>Andrea</first><last>Raballo</last><affiliation>Università della Svizzera italiana</affiliation></author>
      <pages>242-248</pages>
      <abstract>Large Language Models are increasingly used in the medical field, particularly in psychiatry where language plays a fundamental role in diagnosis. This study explores the use of open-source LLMs within the MIND framework. Specifically, we implemented a mixed-methods approach for the CLPsych 2025 shared task: (1) we used a combination of retrieval and few-shot learning approaches to highlight evidence of mental states within the text and to generate comprehensive summaries for post-level and timeline-level analysis, allowing for effective tracking of psychological state fluctuations over time (2) we developed different types of ensemble methods for well-being score prediction, combining Machine Learning and Optimization approaches on top of zero-shot LLMs predictions. Notably, for the latter task, our approach demonstrated the best performance within the competition.</abstract>
      <url hash="d576655d">2025.clpsych-1.20</url>
      <bibkey>ravenda-etal-2025-evidence</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.20</doi>
    </paper>
    <paper id="21">
      <title>From Posts to Timelines: Modeling Mental Health Dynamics from Social Media Timelines with Hybrid <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zimu</first><last>Wang</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Hongbin</first><last>Na</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Rena</first><last>Gao</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Jiayuan</first><last>Ma</last><affiliation>The University of Sydney</affiliation></author>
      <author><first>Yining</first><last>Hua</last><affiliation>Harvard University</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <pages>249-255</pages>
      <abstract>Social media data is recognized for its usefulness in the early detection of mental disorders; however, there is a lack of research focused on modeling individuals’ longitudinal mental health dynamics. Moreover, fine-tuning large language models (LLMs) on large-scale, annotated datasets presents challenges due to privacy concerns and the difficulties on data collection and annotation. In this paper, we propose a novel approach for modeling mental health dynamics using hybrid LLMs, where we first apply both classification-based and generation-based models to identify adaptive and maladaptive evidence from individual posts. This evidence is then used to predict well-being scores and generate post-level and timeline-level summaries. Experimental results on the CLPsych 2025 shared task demonstrate the effectiveness of our method, with the generative-based model showing a marked advantage in evidence identification.</abstract>
      <url hash="2476b3a5">2025.clpsych-1.21</url>
      <bibkey>wang-etal-2025-posts</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.21</doi>
    </paper>
    <paper id="22">
      <title>Prompt Engineering for Capturing Dynamic Mental Health Self States from Social Media Posts</title>
      <author><first>Callum</first><last>Chan</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Sunveer</first><last>Khunkhun</last><affiliation>University of Ottawa</affiliation></author>
      <author id="diana-inkpen"><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Juan Antonio</first><last>Lossio-Ventura</last><affiliation>National Institutes of Health</affiliation></author>
      <pages>256-267</pages>
      <abstract>With the advent of modern Computational Linguistic techniques and the growing societal mental health crisis, we contribute to the field of Clinical Psychology by participating in the CLPsych 2025 shared task. This paper describes the methods and results obtained by the uOttawa team’s submission (which included a researcher from the National Institutes of Health in the USA, in addition to three researchers from the University of Ottawa, Canada). The task consists of four subtasks focused on modeling longitudinal changes in social media users’ mental states and generating accurate summaries of these dynamic self-states. Through prompt engineering of a modern large language model (Llama-3.3-70B-Instruct), the uOttawa team placed first, sixth, fifth, and second, respectively, for each subtask, amongst the other submissions. This work demonstrates the capacity of modern large language models to recognize nuances in the analysis of mental states and to generate summaries through carefully crafted prompting.</abstract>
      <url hash="551de269">2025.clpsych-1.22</url>
      <bibkey>chan-etal-2025-prompt</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.22</doi>
    </paper>
    <paper id="23">
      <title>Retrieval-Enhanced Mental Health Assessment: Capturing Self-State Dynamics from Social Media Using In-Context Learning</title>
      <author><first>Anson</first><last>Antony</last><affiliation>Institute for Experiential AI</affiliation></author>
      <author><first>Annika</first><last>Schoene</last><affiliation>Institute for Experiential AI</affiliation></author>
      <pages>268-278</pages>
      <abstract>This paper presents our approach to the CLPsych 2025 (Tseriotou et al., 2025) shared task, where our proposed system implements a comprehensive solution using In-Context Learning (ICL) with vector similarity to retrieve relevant examples that guide Large Language Models (LLMs) without specific fine-tuning. We leverage ICL to analyze self-states and mental health indicators across three tasks. We developed a pipeline architecture using Ollama, where we are running Llama 3.3 70B locally and specialized vector databases for post- and timeline-level examples. We experimented with different numbers of retrieved examples (k=5 and k=10) to optimize performance. Our results demonstrate the effectiveness of ICL for clinical assessment tasks, particularly when dealing with limited training data in sensitive domains. The system shows strong performance across all tasks, with particular strength in capturing self-state dynamics.</abstract>
      <url hash="36370799">2025.clpsych-1.23</url>
      <bibkey>antony-schoene-2025-retrieval</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.23</doi>
    </paper>
    <paper id="24">
      <title>Self-State Evidence Extraction and Well-Being Prediction from Social Media Timelines</title>
      <author><first>Suchandra</first><last>Chakraborty</last><affiliation>TCS Research</affiliation></author>
      <author><first>Sudeshna</first><last>Jana</last><affiliation>TCS Research</affiliation></author>
      <author><first>Manjira</first><last>Sinha</last><affiliation>Assistant Professor, Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Tirthankar</first><last>Dasgupta</last><affiliation>Tata Consultancy Services Ltd.</affiliation></author>
      <pages>279-286</pages>
      <abstract>This study explores the application of Large Language Models (LLMs) and supervised learning to analyze social media posts from Reddit users, addressing two key objectives: first, to extract adaptive and maladaptive self-state evidence that supports psychological assessment (Task A1); and second, to predict a well-being score that reflects the user’s mental state (Task A2). We propose i) a fine-tuned RoBERTa (Liu et al., 2019) model for Task A1 to identify self-state evidence spans and ii) evaluate two approaches for Task A2: a retrieval-augmented DeepSeek-7B (DeepSeek-AI et al., 2025) model and a Random Forest regression model trained on sentence embeddings. While LLM-based prompting utilizes contextual reasoning, our findings indicate that supervised learning provides more reliable numerical predictions. The RoBERTa model achieves the highest recall (0.602) for Task A1, and Random Forest regression outperforms DeepSeek-7B for Task A2 (MSE: 2.994 vs. 6.610). These results highlight the strengths and limitations of generative vs. supervised methods in mental health NLP, contributing to the development of privacy-conscious, resource-efficient approaches for psychological assessment. This work is part of the CLPsych 2025 shared task (Tseriotou et al., 2025).</abstract>
      <url hash="ca5fdbb6">2025.clpsych-1.24</url>
      <bibkey>chakraborty-etal-2025-self</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.24</doi>
    </paper>
    <paper id="25">
      <title>Team <fixed-case>ISM</fixed-case> at <fixed-case>CLP</fixed-case>sych 2025: Capturing Mental Health Dynamics from Social Media Timelines using A Pretrained Large Language Model with In-Context Learning</title>
      <author><first>Vu</first><last>Tran</last><affiliation>The Institute of Statistical Mathematics</affiliation></author>
      <author><first>Tomoko</first><last>Matsui</last><affiliation>Institute of Statistical Mathematics</affiliation></author>
      <pages>287-291</pages>
      <abstract>We tackle the task by using a pretrained large language model (LLM) and in-context learning with template-based instructions to guide the LLM. To improve generation quality, we employ a two-step procedure: sampling and selection. For the sampling step, we randomly sample a subset of the provided training data for the context of LLM prompting. Next, for the selection step, we map the LLM generated outputs into a vector space and employ the Gaussian kernel density estimation to select the most likely output. The results show that the approach can achieve a certain degree of performance and there is still room for improvement.</abstract>
      <url hash="114963e5">2025.clpsych-1.25</url>
      <bibkey>tran-matsui-2025-team</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.25</doi>
    </paper>
    <paper id="26">
      <title>Transformer-Based Analysis of Adaptive and Maladaptive Self-States in Longitudinal Social Media Data</title>
      <author><first>Abhin</first><last>B</last><affiliation>National Institute of Technology Karnataka, Surathkal</affiliation></author>
      <author><first>Renukasakshi</first><last>V Patil</last><affiliation>NITK</affiliation></author>
      <pages>292-299</pages>
      <abstract>The CLPsych workshop, held annually since 2014, promotes the application of computational linguistics to behavioral analysis and neurological health assessment. The CLPsych 2025 shared task, extending the framework of the 2022 iteration, leverages the MIND framework to model temporal fluctuations in mental states. This shared task comprises three sub-tasks, each presenting substantial challenges to natural language processing (NLP) systems, requiring sensitive and precise outcomes in analyzing adaptive and maladaptive behaviors. In this study, we employed a range of modeling strategies tailored to the requirements and expected outputs of each subtask. Our approach mostly utilized traditional language models like BERT, LongFormer and Pegasus diverging from the prevalent trend of prompt-tuned large language models. We achieved an overall ranking of 13th, with subtask rankings of 8th in Task 1a, 13th in Task 1b, 8th in Task 2, and 7th in Task 3. These results highlight the efficacy of our methods while underscoring areas for further refinement in handling complex behavioral data.</abstract>
      <url hash="07a051b3">2025.clpsych-1.26</url>
      <bibkey>b-v-patil-2025-transformer</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.26</doi>
    </paper>
    <paper id="27">
      <title>Who We Are, Where We Are: Mental Health at the Intersection of Person, Situation, and Large Language Models</title>
      <author><first>Nikita</first><last>Soni</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>August Håkan</first><last>Nilsson</last><affiliation>OsloMet</affiliation></author>
      <author><first>Syeda</first><last>Mahwish</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Vasudha</first><last>Varadarajan</last><affiliation>Stony Brook University</affiliation></author>
      <author id="h-andrew-schwartz"><first>H. Andrew</first><last>Schwartz</last><affiliation>Stony Brook University</affiliation></author>
      <author id="ryan-boyd"><first>Ryan L.</first><last>Boyd</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>300-313</pages>
      <abstract>Mental health is not a fixed trait but a dynamic process shaped by the interplay between individual dispositions and situational contexts. Building on interactionist and constructionist psychological theories, we develop interpretable models to predict well-being and identify adaptive and maladaptive self-states in longitudinal social media data. Our approach integrates person-level psychological traits (e.g., resilience, cognitive distortions, implicit motives) with language-inferred situational features derived from the Situational 8 DIAMONDS framework. We compare these theory-grounded features to embeddings from a psychometrically-informed language model that captures temporal and individual-specific patterns. Results show that our principled, theory-driven features provide competitive performance while offering greater interpretability. Qualitative analyses further highlight the psychological coherence of features most predictive of well-being. These findings underscore the value of integrating computational modeling with psychological theory to assess dynamic mental states in contextually sensitive and human-understandable ways.</abstract>
      <url hash="e80fb674">2025.clpsych-1.27</url>
      <bibkey>soni-etal-2025-mental</bibkey>
      <doi>10.18653/v1/2025.clpsych-1.27</doi>
    </paper>
  </volume>
</collection>
