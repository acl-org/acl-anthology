<?xml version='1.0' encoding='UTF-8'?>
<collection id="Q13">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 1</booktitle>
      <year>2013</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <title>Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging</title>
      <author><first>Oscar</first><last>Täckström</last></author>
      <author><first>Dipanjan</first><last>Das</last></author>
      <author><first>Slav</first><last>Petrov</last></author>
      <author><first>Ryan</first><last>McDonald</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <doi>10.1162/tacl_a_00205</doi>
      <abstract>We consider the construction of part-of-speech taggers for resource-poor languages. Recently, manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. In this paper, we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext. We present several models to this end; in particular a partially observed conditional random field model, where coupled token and type constraints provide a partial signal for training. Averaged across eight previously studied Indo-European languages, our model achieves a 25% relative error reduction over the prior state of the art. We further present successful results on seven additional languages from different families, empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages.</abstract>
      <pages>1–12</pages>
      <url hash="a9d25241">Q13-1001</url>
    </paper>
    <paper id="2">
      <title>Finding Optimal 1-Endpoint-Crossing Trees</title>
      <author><first>Emily</first><last>Pitler</last></author>
      <author><first>Sampath</first><last>Kannan</last></author>
      <author><first>Mitchell</first><last>Marcus</last></author>
      <doi>10.1162/tacl_a_00206</doi>
      <abstract>Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. For 95.8–99.8% of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex. The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O(n4) parsing algorithm that recursively combines forests over intervals with one exterior point. 1-Endpoint-Crossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP.</abstract>
      <pages>13–24</pages>
      <url hash="84ba73ea">Q13-1002</url>
      <video href="https://techtalks.tv/talks/tacl-finding-optimal-1-endpoint-crossing-trees/58443/" tag="video"/>
    </paper>
    <paper id="3">
      <title>Grounding Action Descriptions in Videos</title>
      <author><first>Michaela</first><last>Regneri</last></author>
      <author><first>Marcus</first><last>Rohrbach</last></author>
      <author><first>Dominikus</first><last>Wetzel</last></author>
      <author><first>Stefan</first><last>Thater</last></author>
      <author><first>Bernt</first><last>Schiele</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <doi>10.1162/tacl_a_00207</doi>
      <abstract>Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions.</abstract>
      <pages>25–36</pages>
      <url hash="6a8ffda5">Q13-1003</url>
    </paper>
    <paper id="4">
      <title>Branch and Bound Algorithm for Dependency Parsing with Non-local Features</title>
      <author><first>Xian</first><last>Qian</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <doi>10.1162/tacl_a_00208</doi>
      <abstract>Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B&amp;B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17% for English and 87.25% for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to non-projective dependency parsing or other graphical models.</abstract>
      <pages>37–48</pages>
      <url hash="50edd996">Q13-1004</url>
    </paper>
    <paper id="5">
      <title>Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions</title>
      <author><first>Yoav</first><last>Artzi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <doi>10.1162/tacl_a_00209</doi>
      <abstract>The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60% more instruction sets relative to the previous state of the art.</abstract>
      <pages>49–62</pages>
      <url hash="66bd4fa0">Q13-1005</url>
      <video href="https://techtalks.tv/talks/tacl-weakly-supervised-learning-of-semantic-parsers-for-mapping-instructions-to-actions/58484/" tag="video"/>
    </paper>
    <paper id="6">
      <title>Unsupervised Dependency Parsing with Acoustic Cues</title>
      <author><first>John K</first><last>Pate</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <doi>10.1162/tacl_a_00210</doi>
      <abstract>Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. This paper explores the hypothesis that word duration can help with learning syntax. We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves (without punctuation or parts of speech). Our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech.</abstract>
      <pages>63–74</pages>
      <url hash="7ad15dfa">Q13-1006</url>
      <video href="https://techtalks.tv/talks/tacl-unsupervised-dependency-parsing-with-acoustic-cues/58509/" tag="video"/>
    </paper>
    <paper id="7">
      <title>An <fixed-case>HDP</fixed-case> Model for Inducing <fixed-case>C</fixed-case>ombinatory <fixed-case>C</fixed-case>ategorial <fixed-case>G</fixed-case>rammars</title>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <doi>10.1162/tacl_a_00211</doi>
      <abstract>We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text. It achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons.</abstract>
      <pages>75–88</pages>
      <url hash="383678f0">Q13-1007</url>
      <video href="https://techtalks.tv/talks/tacl-an-hdp-model-for-inducing-combinatory-categorial-grammars/58520/" tag="video"/>
    </paper>
    <paper id="8">
      <title>A Novel Feature-based <fixed-case>B</fixed-case>ayesian Model for Query Focused Multi-document Summarization</title>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <doi>10.1162/tacl_a_00212</doi>
      <abstract>Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach.</abstract>
      <pages>89–98</pages>
      <url hash="a726823e">Q13-1008</url>
    </paper>
    <paper id="9">
      <title>Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data</title>
      <author><first>Beata</first><last>Beigman Klebanov</last></author>
      <author><first>Nitin</first><last>Madnani</last></author>
      <author><first>Jill</first><last>Burstein</last></author>
      <doi>10.1162/tacl_a_00213</doi>
      <abstract>We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15% improvement in the accuracy of the seed lexicon on 3-way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7% improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews).</abstract>
      <pages>99–110</pages>
      <url hash="6b0262b7">Q13-1009</url>
    </paper>
    <paper id="10">
      <title>Incremental Tree Substitution Grammar for Parsing and Sentence Prediction</title>
      <author><first>Federico</first><last>Sangati</last></author>
      <author><first>Frank</first><last>Keller</last></author>
      <doi>10.1162/tacl_a_00214</doi>
      <abstract>In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word.</abstract>
      <pages>111–124</pages>
      <url hash="9c4fc44b">Q13-1010</url>
    </paper>
    <paper id="11">
      <title>Modeling Child Divergences from Adult Grammar</title>
      <author><first>Sam</first><last>Sahakian</last></author>
      <author><first>Benjamin</first><last>Snyder</last></author>
      <doi>10.1162/tacl_a_00215</doi>
      <abstract>During the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. Our corpus consists of child sentences with corrected adult forms. We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. Our method outperforms MT and ESL baselines, reducing child error by 20%. Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.</abstract>
      <pages>125–138</pages>
      <url hash="0074a495">Q13-1011</url>
    </paper>
    <paper id="12">
      <title>Efficient Stacked Dependency Parsing by Forest Reranking</title>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <author><first>Shuhei</first><last>Kondo</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <doi>10.1162/tacl_a_00216</doi>
      <abstract>This paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing. A dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker, using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features. To improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arc-standard transition systems. Testing on the English Penn Treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12.</abstract>
      <pages>139–150</pages>
      <url hash="3ed02eb3">Q13-1012</url>
    </paper>
    <paper id="13">
      <title>Dijkstra-<fixed-case>WSA</fixed-case>: A Graph-Based Approach to Word Sense Alignment</title>
      <author><first>Michael</first><last>Matuschek</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/tacl_a_00217</doi>
      <abstract>In this paper, we present Dijkstra-WSA, a novel graph-based algorithm for word sense alignment. We evaluate it on four different pairs of lexical-semantic resources with different characteristics (WordNet-OmegaWiki, WordNet-Wiktionary, GermaNet-Wiktionary and WordNet-Wikipedia) and show that it achieves competitive performance on 3 out of 4 datasets. Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall.</abstract>
      <pages>151–164</pages>
      <url hash="bad34ffb">Q13-1013</url>
    </paper>
    <paper id="14">
      <title>Learning to translate with products of novices: a suite of open-ended challenge problems for teaching <fixed-case>MT</fixed-case></title>
      <author><first>Adam</first><last>Lopez</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Jonathan</first><last>Weese</last></author>
      <author><first>Juri</first><last>Ganitkevitch</last></author>
      <author><first>Narges</first><last>Ahmidi</last></author>
      <author><first>Olivia</first><last>Buzek</last></author>
      <author><first>Leah</first><last>Hanson</last></author>
      <author><first>Beenish</first><last>Jamil</last></author>
      <author><first>Matthias</first><last>Lee</last></author>
      <author><first>Ya-Ting</first><last>Lin</last></author>
      <author><first>Henry</first><last>Pao</last></author>
      <author><first>Fatima</first><last>Rivera</last></author>
      <author><first>Leili</first><last>Shahriyari</last></author>
      <author><first>Debu</first><last>Sinha</last></author>
      <author><first>Adam</first><last>Teichert</last></author>
      <author><first>Stephen</first><last>Wampler</last></author>
      <author><first>Michael</first><last>Weinberger</last></author>
      <author><first>Daguang</first><last>Xu</last></author>
      <author><first>Lin</first><last>Yang</last></author>
      <author><first>Shang</first><last>Zhao</last></author>
      <doi>10.1162/tacl_a_00218</doi>
      <abstract>Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available.</abstract>
      <pages>165–178</pages>
      <url hash="0c060d88">Q13-1014</url>
    </paper>
    <paper id="15">
      <title>Combined Distributional and Logical Semantics</title>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/tacl_a_00219</doi>
      <abstract>We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite.</abstract>
      <pages>179–192</pages>
      <url hash="e2867e43">Q13-1015</url>
    </paper>
    <paper id="16">
      <title>Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World</title>
      <author><first>Jayant</first><last>Krishnamurthy</last></author>
      <author><first>Thomas</first><last>Kollar</last></author>
      <doi>10.1162/tacl_a_00220</doi>
      <abstract>This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment. For example, given an image, LSP can map the statement “blue mug on the table” to the set of image segments showing blue mugs on tables. LSP learns physical representations for both categorical (“blue,” “mug”) and relational (“on”) language, and also learns to compose these representations to produce the referents of entire statements. We further introduce a weakly supervised training procedure that estimates LSP’s parameters using annotated referents for entire statements, without annotated referents for individual words or the parse structure of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational language. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort.</abstract>
      <pages>193–206</pages>
      <url hash="29d23310">Q13-1016</url>
    </paper>
    <paper id="17">
      <title>Dual Coordinate Descent Algorithms for Efficient Large Margin Structured Prediction</title>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <doi>10.1162/tacl_a_00221</doi>
      <abstract>Due to the nature of complex NLP problems, structured prediction algorithms have been important modeling tools for a wide range of tasks. While there exists evidence showing that linear Structural Support Vector Machine (SSVM) algorithm performs better than structured Perceptron, the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed. In this paper, we propose a fast and easy-to-implement dual coordinate descent algorithm for SSVMs. Unlike algorithms such as Perceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively. As a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark NLP datasets for part-of-speech tagging, named-entity recognition and dependency parsing.</abstract>
      <pages>207–218</pages>
      <url hash="aaa21bdc">Q13-1017</url>
    </paper>
    <paper id="18">
      <title>Joint Arc-factored Parsing of Syntactic and Semantic Dependencies</title>
      <author><first>Xavier</first><last>Lluís</last></author>
      <author><first>Xavier</first><last>Carreras</last></author>
      <author><first>Lluís</first><last>Màrquez</last></author>
      <doi>10.1162/tacl_a_00222</doi>
      <abstract>In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. This process is framed as a linear assignment task, which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results.</abstract>
      <pages>219–230</pages>
      <url hash="f0cf6323">Q13-1018</url>
    </paper>
    <paper id="19">
      <title>Modeling Semantic Relations Expressed by Prepositions</title>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <doi>10.1162/tacl_a_00223</doi>
      <abstract>This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. We define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. The annotated data, however, only provides labels for the relation label, and not the arguments and types. We address this by presenting two models for preposition relation labeling. Our generalization of latent structure SVM gives close to 90% accuracy on relation labeling. Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also significantly improve sense prediction accuracy.</abstract>
      <pages>231–242</pages>
      <url hash="b8b06f11">Q13-1019</url>
      <video href="https://techtalks.tv/talks/tacl-modeling-semantic-relations-expressed-by-prepositions/58504/" tag="video"/>
    </paper>
    <paper id="20">
      <title>Unsupervised Tree Induction for Tree-based Translation</title>
      <author><first>Feifei</first><last>Zhai</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <doi>10.1162/tacl_a_00224</doi>
      <abstract>In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees.</abstract>
      <pages>243–254</pages>
      <url hash="e5593f02">Q13-1020</url>
    </paper>
    <paper id="21">
      <title>Minimally-Supervised Morphological Segmentation using <fixed-case>A</fixed-case>daptor <fixed-case>G</fixed-case>rammars</title>
      <author><first>Kairit</first><last>Sirts</last></author>
      <author><first>Sharon</first><last>Goldwater</last></author>
      <doi>10.1162/tacl_a_00225</doi>
      <abstract>This paper explores the use of Adaptor Grammars, a nonparametric Bayesian modelling framework, for minimally supervised morphological segmentation. We compare three training methods: unsupervised training, semi-supervised training, and a novel model selection method. In the model selection method, we train unsupervised Adaptor Grammars using an over-articulated metagrammar, then use a small labelled data set to select which potential morph boundaries identified by the metagrammar should be returned in the final output. We evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training, while the model selection method yields the best average results over all languages and is competitive with state-of-the-art semi-supervised systems. Moreover, this method provides the potential to tune performance according to different evaluation metrics or downstream tasks.</abstract>
      <pages>255–266</pages>
      <url hash="2904028e">Q13-1021</url>
    </paper>
    <paper id="22">
      <title>Efficient Parsing for Head-Split Dependency Trees</title>
      <author><first>Giorgio</first><last>Satta</last></author>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <doi>10.1162/tacl_a_00226</doi>
      <abstract>Head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees, under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage.</abstract>
      <pages>267–278</pages>
      <url hash="5028aa11">Q13-1022</url>
    </paper>
    <paper id="23">
      <title>Good, Great, Excellent: Global Inference of Semantic Intensities</title>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <doi>10.1162/tacl_a_00227</doi>
      <abstract>Adjectives like good, great, and excellent are similar in meaning, but differ in intensity. Intensity order information is very useful for language learners as well as in several NLP tasks, but is missing in most lexical resources (dictionaries, WordNet, and thesauri). In this paper, we present a primarily unsupervised approach that uses semantics from Web-scale data (e.g., phrases like good but not excellent) to rank words by assigning them positions on a continuous scale. We rely on Mixed Integer Linear Programming to jointly determine the ranks, such that individual decisions benefit from global information. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70% pairwise accuracy as compared to only 56% by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78%) and extends easily to new languages. We also make our code and data freely available.</abstract>
      <pages>279–290</pages>
      <url hash="a985c808">Q13-1023</url>
    </paper>
    <paper id="24">
      <title>Large-scale Word Alignment Using Soft Dependency Cohesion Constraints</title>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <doi>10.1162/tacl_a_00228</doi>
      <abstract>Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.</abstract>
      <pages>291–300</pages>
      <url hash="53bc036d">Q13-1024</url>
    </paper>
    <paper id="25">
      <title>Data-driven, <fixed-case>PCFG</fixed-case>-based and Pseudo-<fixed-case>PCFG</fixed-case>-based Models for <fixed-case>C</fixed-case>hinese Dependency Parsing</title>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <doi>10.1162/tacl_a_00229</doi>
      <abstract>We present a comparative study of transition-, graph- and PCFG-based models aimed at illuminating more precisely the likely contribution of CFGs in improving Chinese dependency parsing accuracy, especially by combining heterogeneous models. Inspired by the impact of a constituency grammar on dependency parsing, we propose several strategies to acquire pseudo CFGs only from dependency annotations. Compared to linguistic grammars learned from rich phrase-structure treebanks, well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble. Moreover, pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23%, resulting in a significant improvement of the state of the art.</abstract>
      <pages>301–314</pages>
      <url hash="1666a914">Q13-1025</url>
    </paper>
    <paper id="26">
      <title>Parsing entire discourses as very long strings: Capturing topic continuity in grounded language learning</title>
      <author><first>Minh-Thang</first><last>Luong</last></author>
      <author><first>Michael C.</first><last>Frank</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <doi>10.1162/tacl_a_00230</doi>
      <abstract>Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children’s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators.</abstract>
      <pages>315–326</pages>
      <url hash="5d101cce">Q13-1026</url>
    </paper>
    <paper id="27">
      <title>Dynamically Shaping the Reordering Search Space of Phrase-Based Statistical Machine Translation</title>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <doi>10.1162/tacl_a_00231</doi>
      <abstract>Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages. In fact, the optimal trade-off between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space. We propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time. The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another. The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost (Moore and Quirk, 2007) and hierarchical phrase orientation models (Galley and Manning, 2008). Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while bleu and meteor remain stable, or even increase, at a very high distortion limit.</abstract>
      <pages>327–340</pages>
      <url hash="3081111c">Q13-1027</url>
    </paper>
    <paper id="28">
      <title>What Makes Writing Great? First Experiments on Article Quality Prediction in the Science Journalism Domain</title>
      <author><first>Annie</first><last>Louis</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <doi>10.1162/tacl_a_00232</doi>
      <abstract>Great writing is rare and highly admired. Readers seek out articles that are beautifully written, informative and entertaining. Yet information-access technologies lack capabilities for predicting article quality at this level. In this paper we present first experiments on article quality prediction in the science journalism domain. We introduce a corpus of great pieces of science journalism, along with typical articles from the genre. We implement features to capture aspects of great writing, including surprising, visual and emotional content, as well as general features related to discourse organization and sentence structure. We show that the distinction between great and typical articles can be detected fairly accurately, and that the entire spectrum of our features contribute to the distinction.</abstract>
      <pages>341–352</pages>
      <url hash="25d672a9">Q13-1028</url>
    </paper>
    <paper id="29">
      <title>Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase</title>
      <author><first>Peter D.</first><last>Turney</last></author>
      <doi>10.1162/tacl_a_00233</doi>
      <abstract>There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).</abstract>
      <pages>353–366</pages>
      <url hash="19e19441">Q13-1029</url>
    </paper>
    <paper id="30">
      <title>Modeling Missing Data in Distant Supervision for Information Extraction</title>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first/><last>Mausam</last></author>
      <author><first>Oren</first><last>Etzioni</last></author>
      <doi>10.1162/tacl_a_00234</doi>
      <abstract>Distant supervision algorithms learn information extraction models given only large readily available databases and text collections. Most previous work has used heuristics for generating labeled data, for example assuming that facts not contained in the database are not mentioned in the text, and facts in the database must be mentioned at least once. In this paper, we propose a new latent-variable approach that models missing data. This provides a natural way to incorporate side information, for instance modeling the intuition that text will often mention rare entities which are likely to be missing in the database. Despite the added complexity introduced by reasoning about missing data, we demonstrate that a carefully designed local search approach to inference is very accurate and scales to large datasets. Experiments demonstrate improved performance for binary and unary relation extraction when compared to learning with heuristic labels, including on average a 27% increase in area under the precision recall curve in the binary case.</abstract>
      <pages>367–378</pages>
      <url hash="6d0ddd71">Q13-1030</url>
    </paper>
    <paper id="31">
      <title>Data-Driven Metaphor Recognition and Explanation</title>
      <author><first>Hongsong</first><last>Li</last></author>
      <author><first>Kenny Q.</first><last>Zhu</last></author>
      <author><first>Haixun</first><last>Wang</last></author>
      <doi>10.1162/tacl_a_00235</doi>
      <abstract>Recognizing metaphors and identifying the source-target mappings is an important task as metaphorical text poses a big challenge for machine reading. To address this problem, we automatically acquire a metaphor knowledge base and an isA knowledge base from billions of web pages. Using the knowledge bases, we develop an inference mechanism to recognize and explain the metaphors in the text. To our knowledge, this is the first purely data-driven approach of probabilistic metaphor acquisition, recognition, and explanation. Our results shows that it significantly outperforms other state-of-the-art methods in recognizing and explaining metaphors.</abstract>
      <pages>379–390</pages>
      <url hash="966b9ce3">Q13-1031</url>
    </paper>
    <paper id="32">
      <title><fixed-case>P</fixed-case>owergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading</title>
      <author><first>Sumit</first><last>Basu</last></author>
      <author><first>Chuck</first><last>Jacobs</last></author>
      <author><first>Lucy</first><last>Vanderwende</last></author>
      <doi>10.1162/tacl_a_00236</doi>
      <abstract>We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as “powergrading.” We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small “budget” of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents.</abstract>
      <pages>391–402</pages>
      <url hash="de400c1b">Q13-1032</url>
    </paper>
    <paper id="33">
      <title>Training Deterministic Parsers with Non-Deterministic Oracles</title>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <doi>10.1162/tacl_a_00237</doi>
      <abstract>Greedy transition-based parsers are very fast but tend to suffer from error propagation. This problem is aggravated by the fact that they are normally trained using oracles that are deterministic and incomplete in the sense that they assume a unique canonical path through the transition system and are only valid as long as the parser does not stray from this path. In this paper, we give a general characterization of oracles that are nondeterministic and complete, present a method for deriving such oracles for transition systems that satisfy a property we call arc decomposition, and instantiate this method for three well-known transition systems from the literature. We say that these oracles are dynamic, because they allow us to dynamically explore alternative and nonoptimal paths during training — in contrast to oracles that statically assume a unique optimal path. Experimental evaluation on a wide range of data sets clearly shows that using dynamic oracles to train greedy parsers gives substantial improvements in accuracy. Moreover, this improvement comes at no cost in terms of efficiency, unlike other techniques like beam search.</abstract>
      <pages>403–414</pages>
      <url hash="caff313f">Q13-1033</url>
    </paper>
    <paper id="34">
      <title>Joint Morphological and Syntactic Analysis for Richly Inflected Languages</title>
      <author><first>Bernd</first><last>Bohnet</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Igor</first><last>Boguslavsky</last></author>
      <author><first>Richárd</first><last>Farkas</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <doi>10.1162/tacl_a_00238</doi>
      <abstract>Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results improve the state of the art in dependency parsing for all languages.</abstract>
      <pages>415–428</pages>
      <url hash="3e7069ea">Q13-1034</url>
    </paper>
    <paper id="35">
      <title>Measuring Machine Translation Errors in New Domains</title>
      <author><first>Ann</first><last>Irvine</last></author>
      <author><first>John</first><last>Morgan</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <author><first>Dragos</first><last>Munteanu</last></author>
      <doi>10.1162/tacl_a_00239</doi>
      <abstract>We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation.</abstract>
      <pages>429–440</pages>
      <url hash="08f44601">Q13-1035</url>
    </paper>
  </volume>
</collection>
