<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.naacl">
  <volume id="main" ingest-date="2022-07-05">
    <meta>
      <booktitle>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <editor><first>Marie-Catherine</first><last>de Marneffe</last></editor>
      <editor><first>Ivan Vladimir</first><last>Meza Ruiz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, United States</address>
      <month>July</month>
      <year>2022</year>
      <url hash="17da532d">2022.naacl-main</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="235769b8">2022.naacl-main.0</url>
      <bibkey>naacl-2022-2022-north-american-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Social Norms Guide Reference Resolution</title>
      <author><first>Mitchell</first><last>Abrams</last></author>
      <author><first>Matthias</first><last>Scheutz</last></author>
      <pages>1-11</pages>
      <abstract>Humans use natural language, vision, and context to resolve referents in their environment. While some situated reference resolution is trivial, ambiguous cases arise when the language is underspecified or there are multiple candidate referents. This study investigates howpragmatic modulators external to the linguistic content are critical for the correct interpretation of referents in these scenarios. Inparticular, we demonstrate in a human subjects experiment how the social norms applicable in the given context influence theinterpretation of referring expressions. Additionally, we highlight how current coreference tools in natural language processing fail tohandle these ambiguous cases. We also briefly discuss the implications of this work for assistive robots which will routinely need to resolve referents in their environment.</abstract>
      <url hash="d6d6c103">2022.naacl-main.1</url>
      <bibkey>abrams-scheutz-2022-social</bibkey>
      <doi>10.18653/v1/2022.naacl-main.1</doi>
      <video href="2022.naacl-main.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Learning Natural Language Generation with Truncated Reinforcement Learning</title>
      <author><first>Alice</first><last>Martin</last></author>
      <author><first>Guillaume</first><last>Quispe</last></author>
      <author><first>Charles</first><last>Ollion</last></author>
      <author><first>Sylvain</first><last>Le Corff</last></author>
      <author><first>Florian</first><last>Strub</last></author>
      <author><first>Olivier</first><last>Pietquin</last></author>
      <pages>12-37</pages>
      <abstract>This paper introduces TRUncated ReinForcement Learning for Language (TrufLL), an original approach to train conditional languagemodels without a supervised learning phase, by only using reinforcement learning (RL). As RL methods unsuccessfully scale to large action spaces, we dynamically truncate the vocabulary space using a generic language model. TrufLL thus enables to train a language agent by solely interacting with its environment without any task-specific prior knowledge; it is only guided with a task-agnostic language model. Interestingly, this approach avoids the dependency to labelled datasets and inherently reduces pretrained policy flaws such as language or exposure biases. We evaluate TrufLL on two visual question generation tasks, for which we report positive results over performance and language metrics, which we then corroborate with a human evaluation. To our knowledge, it is the first approach that successfully learns a language generation policy without pre-training, using only reinforcement learning.</abstract>
      <url hash="ad739092">2022.naacl-main.2</url>
      <attachment type="software" hash="2f99d797">2022.naacl-main.2.software.zip</attachment>
      <bibkey>martin-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.2</doi>
      <video href="2022.naacl-main.2.mp4"/>
      <pwccode url="https://github.com/amdonati/rl-nlp" additional="false">amdonati/rl-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
    </paper>
    <paper id="3">
      <title>Language Model Augmented Monotonic Attention for Simultaneous Translation</title>
      <author><first>Sathish Reddy</first><last>Indurthi</last></author>
      <author><first>Mohd Abbas</first><last>Zaidi</last></author>
      <author><first>Beomseok</first><last>Lee</last></author>
      <author><first>Nikhil Kumar</first><last>Lakumarapu</last></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <pages>38-45</pages>
      <abstract>The state-of-the-art adaptive policies for Simultaneous Neural Machine Translation (SNMT) use monotonic attention to perform read/write decisions based on the partial source and target sequences. The lack of sufficient information might cause the monotonic attention to take poor read/write decisions, which in turn negatively affects the performance of the SNMT model. On the other hand, human translators make better read/write decisions since they can anticipate the immediate future words using linguistic information and domain knowledge. In this work, we propose a framework to aid monotonic attention with an external language model to improve its decisions. Experiments on MuST-C English-German and English-French speech-to-text translation tasks show the future information from the language model improves the state-of-the-art monotonic multi-head attention model further.</abstract>
      <url hash="083fa461">2022.naacl-main.3</url>
      <bibkey>indurthi-etal-2022-language</bibkey>
      <doi>10.18653/v1/2022.naacl-main.3</doi>
      <video href="2022.naacl-main.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="4">
      <title>What Makes a Good and Useful Summary? <fixed-case>I</fixed-case>ncorporating Users in Automatic Summarization Research</title>
      <author><first>Maartje</first><last>Ter Hoeve</last></author>
      <author><first>Julia</first><last>Kiseleva</last></author>
      <author><first>Maarten</first><last>Rijke</last></author>
      <pages>46-75</pages>
      <abstract>Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users’ needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target group. Hence, we design our survey in such a way that it can be easily adjusted to investigate different user groups. In this work we focus on university students, who make extensive use of summaries during their studies. We find that the current research directions of the automatic summarization community do not fully align with students’ needs. Motivated by our findings, we present ways to mitigate this mismatch in future research on automatic summarization: we propose research directions that impact the design, the development and the evaluation of automatically generated summaries.</abstract>
      <url hash="a5360123">2022.naacl-main.4</url>
      <bibkey>ter-hoeve-etal-2022-makes</bibkey>
      <doi>10.18653/v1/2022.naacl-main.4</doi>
      <video href="2022.naacl-main.4.mp4"/>
    </paper>
    <paper id="5">
      <title><fixed-case>E</fixed-case>r<fixed-case>AC</fixed-case>on<fixed-case>D</fixed-case>: Error Annotated Conversational Dialog Dataset for Grammatical Error Correction</title>
      <author><first>Xun</first><last>Yuan</last></author>
      <author><first>Derek</first><last>Pham</last></author>
      <author><first>Sam</first><last>Davidson</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>76-84</pages>
      <abstract>Currently available grammatical error correction (GEC) datasets are compiled using essays or other long-form text written by language learners, limiting the applicability of these datasets to other domains such as informal writing and conversational dialog. In this paper, we present a novel GEC dataset consisting of parallel original and corrected utterances drawn from open-domain chatbot conversations; this dataset is, to our knowledge, the first GEC dataset targeted to a human-machine conversational setting. We also present a detailed annotation scheme which ranks errors by perceived impact on comprehension, making our dataset more representative of real-world language learning applications. To demonstrate the utility of the dataset, we use our annotated data to fine-tune a state-of-the-art GEC model. Experimental results show the effectiveness of our data in improving GEC model performance in a conversational scenario.</abstract>
      <url hash="12bb6a14">2022.naacl-main.5</url>
      <bibkey>yuan-etal-2022-eracond</bibkey>
      <doi>10.18653/v1/2022.naacl-main.5</doi>
      <video href="2022.naacl-main.5.mp4"/>
      <pwccode url="https://github.com/yuanxun-yx/eracond" additional="false">yuanxun-yx/eracond</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eracond">ErAConD</pwcdataset>
    </paper>
    <paper id="6">
      <title>Semantic Diversity in Dialogue with Natural Language Inference</title>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <author><first>Marti</first><last>Hearst</last></author>
      <pages>85-98</pages>
      <abstract>Generating diverse, interesting responses to chitchat conversations is a problem for neural conversational agents. This paper makes two substantial contributions to improving diversity in dialogue generation. First, we propose a novel metric which uses Natural Language Inference (NLI) to measure the semantic diversity of a set of model responses for a conversation. We evaluate this metric using an established framework (Tevet and Berant, 2021) and find strong evidence indicating NLI Diversity is correlated with semantic diversity. Specifically, we show that the contradiction relation is more useful than the neutral relation for measuring this diversity and that incorporating the NLI model’s confidence achieves state-of-the-art results. Second, we demonstrate how to iteratively improve the semantic diversity of a sampled set of responses via a new generation procedure called Diversity Threshold Generation, which results in an average 137% increase in NLI Diversity compared to standard generation procedures.</abstract>
      <url hash="a6ae4c7e">2022.naacl-main.6</url>
      <bibkey>stasaski-hearst-2022-semantic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.6</doi>
      <video href="2022.naacl-main.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog-1">DailyDialog++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>LEA</fixed-case>: Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification</title>
      <author><first>S. K.</first><last>Hong</last></author>
      <author><first>Tae Young</first><last>Jang</last></author>
      <pages>99-106</pages>
      <abstract>Text classification has achieved great success with the prosperity of deep learning and pre-trained language models. However, we often encounter labeled data deficiency problems in real-world text-classification tasks. To overcome such challenging scenarios, interest in few-shot learning has increased, whereas most few-shot text classification studies suffer from a difficulty of utilizing pre-trained language models. In the study, we propose a novel learning method for learning how to attend, called LEA, through which meta-level attention aspects are derived based on our meta-learning strategy. This enables the generation of task-specific document embedding with leveraging pre-trained language models even though a few labeled data instances are given. We evaluate our proposed learning method on five benchmark datasets. The results show that the novel method robustly provides the competitive performance compared to recent few-shot learning methods for all the datasets.</abstract>
      <url hash="7bd5de79">2022.naacl-main.7</url>
      <bibkey>hong-jang-2022-lea</bibkey>
      <doi>10.18653/v1/2022.naacl-main.7</doi>
      <video href="2022.naacl-main.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
    </paper>
    <paper id="8">
      <title>Enhancing Self-Attention with Knowledge-Assisted Attention Maps</title>
      <author><first>Jiangang</first><last>Bai</last></author>
      <author><first>Yujing</first><last>Wang</last></author>
      <author><first>Hong</first><last>Sun</last></author>
      <author><first>Ruonan</first><last>Wu</last></author>
      <author><first>Tianmeng</first><last>Yang</last></author>
      <author><first>Pengfei</first><last>Tang</last></author>
      <author><first>Defu</first><last>Cao</last></author>
      <author><first>Mingliang</first><last>Zhang1</last></author>
      <author><first>Yunhai</first><last>Tong</last></author>
      <author><first>Yaming</first><last>Yang</last></author>
      <author><first>Jing</first><last>Bai</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Wei</first><last>Shen</last></author>
      <pages>107-115</pages>
      <abstract>Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.</abstract>
      <url hash="9a15b169">2022.naacl-main.8</url>
      <bibkey>bai-etal-2022-enhancing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.8</doi>
      <video href="2022.naacl-main.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Batch-Softmax Contrastive Loss for Pairwise Sentence Scoring Tasks</title>
      <author><first>Anton</first><last>Chernyavskiy</last></author>
      <author><first>Dmitry</first><last>Ilvovsky</last></author>
      <author><first>Pavel</first><last>Kalinin</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>116-126</pages>
      <abstract>The use of contrastive loss for representation learning has become prominent in computer vision, and it is now getting attention in Natural Language Processing (NLP).Here, we explore the idea of using a batch-softmax contrastive loss when fine-tuning large-scale pre-trained transformer models to learn better task-specific sentence embeddings for pairwise sentence scoring tasks.We introduce and study a number of variations in the calculation of the loss as well as in the overall training procedure; in particular, we find that a special data shuffling can be quite important.Our experimental results show sizable improvements on a number of datasets and pairwise sentence scoring tasks including classification, ranking, and regression.Finally, we offer detailed analysis and discussion, which should be useful for researchers aiming to explore the utility of contrastive loss in NLP.</abstract>
      <url hash="93a09c01">2022.naacl-main.9</url>
      <bibkey>chernyavskiy-etal-2022-batch</bibkey>
      <doi>10.18653/v1/2022.naacl-main.9</doi>
      <video href="2022.naacl-main.9.mp4"/>
    </paper>
    <paper id="10">
      <title><fixed-case>N</fixed-case>ews<fixed-case>E</fixed-case>dits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge</title>
      <award>Honorable mention for contributions to resources</award>
      <author><first>Alexander</first><last>Spangher</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>127-157</pages>
      <abstract>News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.</abstract>
      <url hash="4dc88f3d">2022.naacl-main.10</url>
      <bibkey>spangher-etal-2022-newsedits</bibkey>
      <doi>10.18653/v1/2022.naacl-main.10</doi>
      <video href="2022.naacl-main.10.mp4"/>
      <pwccode url="https://github.com/isi-nlp/newsedits" additional="false">isi-nlp/newsedits</pwccode>
    </paper>
    <paper id="11">
      <title>Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia</title>
      <author><first>Samee</first><last>Ibraheem</last></author>
      <author><first>Gaoyue</first><last>Zhou</last></author>
      <author><first>John</first><last>DeNero</last></author>
      <pages>158-168</pages>
      <abstract>While neural networks demonstrate a remarkable ability to model linguistic content, capturing contextual information related to a speaker’s conversational role is an open area of research. In this work, we analyze the effect of speaker role on language use through the game of Mafia, in which participants are assigned either an honest or a deceptive role. In addition to building a framework to collect a dataset of Mafia game records, we demonstrate that there are differences in the language produced by players with different roles. We confirm that classification models are able to rank deceptive players as more suspicious than honest ones based only on their use of language. Furthermore, we show that training models on two auxiliary tasks outperforms a standard BERT-based text classification approach. We also present methods for using our trained models to identify features that distinguish between player roles, which could be used to assist players during the Mafia game.</abstract>
      <url hash="b690463f">2022.naacl-main.11</url>
      <bibkey>ibraheem-etal-2022-putting</bibkey>
      <doi>10.18653/v1/2022.naacl-main.11</doi>
      <video href="2022.naacl-main.11.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/the-mafia-dataset">The Mafia Dataset</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>SUBS</fixed-case>: Subtree Substitution for Compositional Semantic Parsing</title>
      <author><first>Jingfeng</first><last>Yang</last></author>
      <author><first>Le</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>169-174</pages>
      <abstract>Although sequence-to-sequence models often achieve good performance in semantic parsing for i.i.d. data, their performance is still inferior in compositional generalization. Several data augmentation methods have been proposed to alleviate this problem. However, prior work only leveraged superficial grammar or rules for data augmentation, which resulted in limited improvement. We propose to use subtree substitution for compositional data augmentation, where we consider subtrees with similar semantic functions as exchangeable. Our experiments showed that such augmented data led to significantly better performance on Scan and GeoQuery, and reached new SOTA on compositional split of GeoQuery.</abstract>
      <url hash="aa43e698">2022.naacl-main.12</url>
      <bibkey>yang-etal-2022-subs</bibkey>
      <doi>10.18653/v1/2022.naacl-main.12</doi>
      <video href="2022.naacl-main.12.mp4"/>
      <pwccode url="https://github.com/gt-salt/subs" additional="false">gt-salt/subs</pwccode>
    </paper>
    <paper id="13">
      <title>Two Contrasting Data Annotation Paradigms for Subjective <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Paul</first><last>Rottger</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <pages>175-190</pages>
      <abstract>Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.</abstract>
      <url hash="623b6ac1">2022.naacl-main.13</url>
      <bibkey>rottger-etal-2022-two</bibkey>
      <doi>10.18653/v1/2022.naacl-main.13</doi>
      <video href="2022.naacl-main.13.mp4"/>
      <pwccode url="https://github.com/paul-rottger/annotation-paradigms" additional="false">paul-rottger/annotation-paradigms</pwccode>
    </paper>
    <paper id="14">
      <title>Do Deep Neural Nets Display Human-like Attention in Short Answer Scoring?</title>
      <author><first>Zijie</first><last>Zeng</last></author>
      <author><first>Xinyu</first><last>Li</last></author>
      <author><first>Dragan</first><last>Gasevic</last></author>
      <author><first>Guanliang</first><last>Chen</last></author>
      <pages>191-205</pages>
      <abstract>Deep Learning (DL) techniques have been increasingly adopted for Automatic Text Scoring in education. However, these techniques often suffer from their inabilities to explain and justify how a prediction is made, which, unavoidably, decreases their trustworthiness and hinders educators from embracing them in practice. This study aimed to investigate whether (and to what extent) DL-based graders align with human graders regarding the important words they identify when marking short answer questions. To this end, we first conducted a user study to ask human graders to manually annotate important words in assessing answer quality and then measured the overlap between these human-annotated words and those identified by DL-based graders (i.e., those receiving large attention weights). Furthermore, we ran a randomized controlled experiment to explore the impact of highlighting important words detected by DL-based graders on human grading. The results showed that: (i) DL-based graders, to a certain degree, displayed alignment with human graders no matter whether DL-based graders and human graders agreed on the quality of an answer; and (ii) it is possible to facilitate human grading by highlighting those DL-detected important words, though further investigations are necessary to understand how human graders exploit such highlighted words.</abstract>
      <url hash="0c558488">2022.naacl-main.14</url>
      <attachment type="software" hash="6c07b878">2022.naacl-main.14.software.zip</attachment>
      <bibkey>zeng-etal-2022-deep</bibkey>
      <doi>10.18653/v1/2022.naacl-main.14</doi>
      <video href="2022.naacl-main.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation</title>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Yi</first><last>Mao</last></author>
      <author><first>Lars</first><last>Liden</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>206-218</pages>
      <abstract>Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings.</abstract>
      <url hash="8b43548b">2022.naacl-main.15</url>
      <bibkey>li-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.naacl-main.15</doi>
      <video href="2022.naacl-main.15.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/inspired">Inspired</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="16">
      <title><fixed-case>CERES</fixed-case>: Pretraining of Graph-Conditioned Transformer for Semi-Structured Session Data</title>
      <author><first>Rui</first><last>Feng</last></author>
      <author><first>Chen</first><last>Luo</last></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>219-230</pages>
      <abstract>User sessions empower many search and recommendation tasks on a daily basis. Such session data are semi-structured, which encode heterogeneous relations between queries and products, and each item is described by the unstructured text. Despite recent advances in self-supervised learning for text or graphs, there lack of self-supervised learning models that can effectively capture both intra-item semantics and inter-item interactions for semi-structured sessions. To fill this gap, we propose CERES, a graph-based transformer model for semi-structured session data. CERES learns representations that capture both inter- and intra-item semantics with (1) a graph-conditioned masked language pretraining task that jointly learns from item text and item-item relations; and (2) a graph-conditioned transformer architecture that propagates inter-item contexts to item-level representations. We pretrained CERES using ~468 million Amazon sessions and find that CERES outperforms strong pretraining baselines by up to 9% in three session search and entity linking tasks.</abstract>
      <url hash="511bf60b">2022.naacl-main.16</url>
      <bibkey>feng-etal-2022-ceres</bibkey>
      <doi>10.18653/v1/2022.naacl-main.16</doi>
    </paper>
    <paper id="17">
      <title>Political Ideology and Polarization: A Multi-dimensional Approach</title>
      <author><first>Barea</first><last>Sinno</last></author>
      <author><first>Bernardo</first><last>Oviedo</last></author>
      <author><first>Katherine</first><last>Atwell</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>231-243</pages>
      <abstract>Analyzing ideology and polarization is of critical importance in advancing our grasp of modern politics. Recent research has made great strides towards understanding the ideological bias (i.e., stance) of news media along the left-right spectrum. In this work, we instead take a novel and more nuanced approach for the study of ideology based on its left or right positions on the issue being discussed. Aligned with the theoretical accounts in political science, we treat ideology as a multi-dimensional construct, and introduce the first diachronic dataset of news articles whose ideological positions are annotated by trained political scientists and linguists at the paragraph level. We showcase that, by controlling for the author’s stance, our method allows for the quantitative and temporal measurement and analysis of polarization as a multidimensional ideological distance. We further present baseline models for ideology prediction, outlining a challenging task distinct from stance detection.</abstract>
      <url hash="d9aa7560">2022.naacl-main.17</url>
      <bibkey>sinno-etal-2022-political</bibkey>
      <doi>10.18653/v1/2022.naacl-main.17</doi>
      <video href="2022.naacl-main.17.mp4"/>
      <pwccode url="https://github.com/bernovie/political-polarization" additional="false">bernovie/political-polarization</pwccode>
    </paper>
    <paper id="18">
      <title>Cooperative Self-training of Machine Reading Comprehension</title>
      <author><first>Hongyin</first><last>Luo</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Mingye</first><last>Gao</last></author>
      <author><first>Seunghak</first><last>Yu</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>244-257</pages>
      <abstract>Pretrained language models have significantly improved the performance of downstream language understanding tasks, including extractive question answering, by providing high-quality contextualized word embeddings. However, training question answering models still requires large amounts of annotated data for specific domains. In this work, we propose a cooperative self-training framework, RGX, for automatically generating more non-trivial question-answer pairs to improve model performance. RGX is built upon a masked answer extraction task with an interactive learning environment containing an answer entity Recognizer, a question Generator, and an answer eXtractor. Given a passage with a masked entity, the generator generates a question around the entity, and the extractor is trained to extract the masked entity with the generated question and raw texts. The framework allows the training of question generation and answering models on any text corpora without annotation. We further leverage a self-training technique to improve the performance of both question generation and answer extraction models. Experiment results show that RGX outperforms the state-of-the-art (SOTA) pretrained language models and transfer learning approaches on standard question-answering benchmarks, and yields the new SOTA performance under given model size and transfer learning settings.</abstract>
      <url hash="f1a4f8b4">2022.naacl-main.18</url>
      <bibkey>luo-etal-2022-cooperative</bibkey>
      <doi>10.18653/v1/2022.naacl-main.18</doi>
      <video href="2022.naacl-main.18.mp4"/>
      <pwccode url="https://github.com/luohongyin/RGX" additional="false">luohongyin/RGX</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/adversarialqa">AdversarialQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>G</fixed-case>lob<fixed-case>E</fixed-case>nc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers</title>
      <author><first>Ali</first><last>Modarressi</last></author>
      <author><first>Mohsen</first><last>Fayyaz</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>258-271</pages>
      <abstract>There has been a growing interest in interpreting the underlying dynamics of Transformers. While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions. Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings. Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores. Our code is freely available at https://github.com/mohsenfayyaz/GlobEnc.</abstract>
      <url hash="a1c0b2c0">2022.naacl-main.19</url>
      <bibkey>modarressi-etal-2022-globenc</bibkey>
      <doi>10.18653/v1/2022.naacl-main.19</doi>
      <video href="2022.naacl-main.19.mp4"/>
      <pwccode url="https://github.com/mohsenfayyaz/globenc" additional="false">mohsenfayyaz/globenc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hatexplain">HateXplain</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="20">
      <title>A Robustly Optimized <fixed-case>BMRC</fixed-case> for Aspect Sentiment Triplet Extraction</title>
      <author><first>Shu</first><last>Liu</last></author>
      <author><first>Kaiwen</first><last>Li</last></author>
      <author><first>Zuhe</first><last>Li</last></author>
      <pages>272-278</pages>
      <abstract>Aspect sentiment triplet extraction (ASTE) is a challenging subtask in aspect-based sentiment analysis. It aims to explore the triplets of aspects, opinions and sentiments with complex correspondence from the context. The bidirectional machine reading comprehension (BMRC), can effectively deal with ASTE task, but several problems remains, such as query conflict and probability unilateral decrease. Therefore, this paper presents a robustly optimized BMRC method by incorporating four improvements. The word segmentation is applied to facilitate the semantic learning. Exclusive classifiers are designed to avoid the interference between different queries. A span matching rule is proposed to select the aspects and opinions that better represent the expectations of the model. The probability generation strategy is also introduced to obtain the predicted probability for aspects, opinions and aspect-opinion pairs. We have conducted extensive experiments on multiple benchmark datasets, where our model achieves the state-of-the-art performance.</abstract>
      <url hash="ac5c24d2">2022.naacl-main.20</url>
      <bibkey>liu-etal-2022-robustly</bibkey>
      <doi>10.18653/v1/2022.naacl-main.20</doi>
      <video href="2022.naacl-main.20.mp4"/>
      <pwccode url="https://github.com/itkaven/robmrc" additional="false">itkaven/robmrc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aste-data-v2">ASTE-Data-V2</pwcdataset>
    </paper>
    <paper id="21">
      <title>Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds</title>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Meng</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>279-290</pages>
      <abstract>Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users’ particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.</abstract>
      <url hash="769c1171">2022.naacl-main.21</url>
      <bibkey>zhang-etal-2022-seed</bibkey>
      <doi>10.18653/v1/2022.naacl-main.21</doi>
      <video href="2022.naacl-main.21.mp4"/>
      <pwccode url="https://github.com/yuzhimanhua/seetopic" additional="false">yuzhimanhua/seetopic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scidocs">SciDocs</pwcdataset>
    </paper>
    <paper id="22">
      <title>Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs</title>
      <author><first>Xu</first><last>Wang</last></author>
      <author><first>Simin</first><last>Fan</last></author>
      <author><first>Jessica</first><last>Houghton</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>291-302</pages>
      <abstract>NLP-powered automatic question generation (QG) techniques carry great pedagogical potential of saving educators’ time and benefiting student learning. Yet, QG systems have not been widely adopted in classrooms to date. In this work, we aim to pinpoint key impediments and investigate how to improve the usability of automatic QG techniques for educational purposes by understanding how instructors construct questions and identifying touch points to enhance the underlying NLP models. We perform an in-depth need finding study with 11 instructors across 7 different universities, and summarize their thought processes and needs when creating questions. While instructors show great interests in using NLP systems to support question design, none of them has used such tools in practice. They resort to multiple sources of information, ranging from domain knowledge to students’ misconceptions, all of which missing from today’s QG systems. We argue that building effective human-NLP collaborative QG systems that emphasize instructor control and explainability is imperative for real-world adoption. We call for QG systems to provide process-oriented support, use modular design, and handle diverse sources of input.</abstract>
      <url hash="0398e609">2022.naacl-main.22</url>
      <bibkey>wang-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.naacl-main.22</doi>
      <video href="2022.naacl-main.22.mp4"/>
      <pwccode url="https://github.com/olivia-fsm/p2mcq" additional="false">olivia-fsm/p2mcq</pwccode>
    </paper>
    <paper id="23">
      <title><fixed-case>S</fixed-case>wah<fixed-case>BERT</fixed-case>: Language Model of <fixed-case>S</fixed-case>wahili</title>
      <author><first>Gati</first><last>Martin</last></author>
      <author><first>Medard Edmund</first><last>Mswahili</last></author>
      <author><first>Young-Seob</first><last>Jeong</last></author>
      <author><first>Jiyoung</first><last>Woo</last></author>
      <pages>303-313</pages>
      <abstract>The rapid development of social networks, electronic commerce, mobile Internet, and other technologies, has influenced the growth of Web data.Social media and Internet forums are valuable sources of citizens’ opinions, which can be analyzed for community development and user behavior analysis.Unfortunately, the scarcity of resources (i.e., datasets or language models) become a barrier to the development of natural language processing applications in low-resource languages.Thanks to the recent growth of online forums and news platforms of Swahili, we introduce two datasets of Swahili in this paper: a pre-training dataset of approximately 105MB with 16M words and annotated dataset of 13K instances for the emotion classification task.The emotion classification dataset is manually annotated by two native Swahili speakers.We pre-trained a new monolingual language model for Swahili, namely SwahBERT, using our collected pre-training data, and tested it with four downstream tasks including emotion classification.We found that SwahBERT outperforms multilingual BERT, a well-known existing language model, in almost all downstream tasks.</abstract>
      <url hash="b2d12886">2022.naacl-main.23</url>
      <bibkey>martin-etal-2022-swahbert</bibkey>
      <doi>10.18653/v1/2022.naacl-main.23</doi>
      <video href="2022.naacl-main.23.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/masakhaner">MasakhaNER</pwcdataset>
    </paper>
    <paper id="24">
      <title>Deconstructing <fixed-case>NLG</fixed-case> Evaluation: Evaluation Practices, Assumptions, and Their Implications</title>
      <author><first>Kaitlyn</first><last>Zhou</last></author>
      <author><first>Su Lin</first><last>Blodgett</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Alexandra</first><last>Olteanu</last></author>
      <pages>314-324</pages>
      <abstract>There are many ways to express similar things in text, which makes evaluating natural language generation (NLG) systems difficult. Compounding this difficulty is the need to assess varying quality criteria depending on the deployment setting. While the landscape of NLG evaluation has been well-mapped, practitioners’ goals, assumptions, and constraints—which inform decisions about what, when, and how to evaluate—are often partially or implicitly stated, or not stated at all. Combining a formative semi-structured interview study of NLG practitioners (N=18) with a survey study of a broader sample of practitioners (N=61), we surface goals, community practices, assumptions, and constraints that shape NLG evaluations, examining their implications and how they embody ethical considerations.</abstract>
      <url hash="982cc5db">2022.naacl-main.24</url>
      <bibkey>zhou-etal-2022-deconstructing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.24</doi>
      <video href="2022.naacl-main.24.mp4"/>
    </paper>
    <paper id="25">
      <title><fixed-case>TSTR</fixed-case>: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation</title>
      <author><first>Sajad</first><last>Sotudeh</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <pages>325-335</pages>
      <abstract>Many scientific papers such as those in arXiv and PubMed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstract-like summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form, such as in scientific documents, such summary is not able to go beyond the general and coarse overview and provide salient information from the source document. The recent interest to tackle this problem motivated curation of scientific datasets, arXiv-Long and PubMed-Long, containing human-written summaries of 400-600 words, hence, providing a venue for research in generating long/extended summaries. Extended summaries facilitate a faster read while providing details beyond coarse information. In this paper, we propose TSTR, an extractive summarizer that utilizes the introductory information of documents as pointers to their salient information. The evaluations on two existing large-scale extended summarization datasets indicate statistically significant improvement in terms of Rouge and average Rouge (F1) scores (except in one case) as compared to strong baselines and state-of-the-art. Comprehensive human evaluations favor our generated extended summaries in terms of cohesion and completeness.</abstract>
      <url hash="91fa9d7a">2022.naacl-main.25</url>
      <bibkey>sotudeh-goharian-2022-tstr</bibkey>
      <doi>10.18653/v1/2022.naacl-main.25</doi>
      <video href="2022.naacl-main.25.mp4"/>
      <pwccode url="https://github.com/georgetown-ir-lab/tstrsum" additional="false">georgetown-ir-lab/tstrsum</pwccode>
    </paper>
    <paper id="26">
      <title>Empathic Machines: Using Intermediate Features as Levers to Emulate Emotions in Text-To-Speech Systems</title>
      <author><first>Saiteja</first><last>Kosgi</last></author>
      <author><first>Sarath</first><last>Sivaprasad</last></author>
      <author><first>Niranjan</first><last>Pedanekar</last></author>
      <author><first>Anil</first><last>Nelakanti</last></author>
      <author><first>Vineet</first><last>Gandhi</last></author>
      <pages>336-347</pages>
      <abstract>We present a method to control the emotional prosody of Text to Speech (TTS) systems by using phoneme-level intermediate features (pitch, energy, and duration) as levers. As a key idea, we propose Differential Scaling (DS) to disentangle features relating to affective prosody from those arising due to acoustics conditions and speaker identity. With thorough experimental studies, we show that the proposed method improves over the prior art in accurately emulating the desired emotions while retaining the naturalness of speech. We extend the traditional evaluation of using individual sentences for a more complete evaluation of HCI systems. We present a novel experimental setup by replacing an actor with a TTS system in offline and live conversations. The emotion to be rendered is either predicted or manually assigned. The results show that the proposed method is strongly preferred over the state-of-the-art TTS system and adds the much-coveted “human touch” in machine dialogue. Audio samples from our experiments and the code are available at: https://emtts.github.io/tts-demo/</abstract>
      <url hash="f2fc7e29">2022.naacl-main.26</url>
      <bibkey>kosgi-etal-2022-empathic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.26</doi>
      <video href="2022.naacl-main.26.mp4"/>
    </paper>
    <paper id="27">
      <title>The Why and The How: A Survey on Natural Language Interaction in Visualization</title>
      <author><first>Henrik</first><last>Voigt</last></author>
      <author><first>Ozge</first><last>Alacam</last></author>
      <author><first>Monique</first><last>Meuschke</last></author>
      <author><first>Kai</first><last>Lawonn</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>348-374</pages>
      <abstract>Natural language as a modality of interaction is becoming increasingly popular in the field of visualization. In addition to the popular query interfaces, other language-based interactions such as annotations, recommendations, explanations, or documentation experience growing interest. In this survey, we provide an overview of natural language-based interaction in the research area of visualization. We discuss a renowned taxonomy of visualization tasks and classify 119 related works to illustrate the state-of-the-art of how current natural language interfaces support their performance. We examine applied NLP methods and discuss human-machine dialogue structures with a focus on initiative, duration, and communicative functions in recent visualization-oriented dialogue interfaces. Based on this overview, we point out interesting areas for the future application of NLP methods in the field of visualization.</abstract>
      <url hash="c61417a2">2022.naacl-main.27</url>
      <bibkey>voigt-etal-2022-survey</bibkey>
      <doi>10.18653/v1/2022.naacl-main.27</doi>
      <video href="2022.naacl-main.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding</title>
      <author><first>Hao</first><last>Huang</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Guodong</first><last>Long</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>375-384</pages>
      <abstract>This work studies temporal reading comprehension (TRC), which reads a free-text passage and answers temporal ordering questions. Precise question understanding is critical for temporal reading comprehension. For example, the question “What happened before the victory” and “What happened after the victory” share almost all words except one, while their answers are totally different. Moreover, even if two questions query about similar temporal relations, different varieties might also lead to various answers. For example, although both the question “What usually happened during the press release?” and “What might happen during the press release” query events which happen after “the press release”, they convey divergent semantics.To this end, we propose a novel reading comprehension approach with precise question understanding. Specifically, a temporal ordering question is embedded into two vectors to capture the referred event and the temporal relation. Then we evaluate the temporal relation between candidate events and the referred event based on that. Such fine-grained representations offer two benefits. First, it enables a better understanding of the question by focusing on different elements of a question. Second, it provides good interpretability when evaluating temporal relations. Furthermore, we also harness an auxiliary contrastive loss for representation learning of temporal relations, which aims to distinguish relations with subtle but critical changes. The proposed approach outperforms strong baselines and achieves state-of-the-art performance on the TORQUE dataset. It also increases the accuracy of four pre-trained language models (BERT base, BERT large, RoBERTa base, and RoBETRa large), demonstrating its generic effectiveness on divergent models.</abstract>
      <url hash="7084462c">2022.naacl-main.28</url>
      <bibkey>huang-etal-2022-understand</bibkey>
      <doi>10.18653/v1/2022.naacl-main.28</doi>
      <video href="2022.naacl-main.28.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/torque">Torque</pwcdataset>
    </paper>
    <paper id="29">
      <title>User-Driven Research of Medical Note Generation Software</title>
      <award>Best paper on human-centered NLP special theme</award>
      <author><first>Tom</first><last>Knoll</last></author>
      <author><first>Francesco</first><last>Moramarco</last></author>
      <author><first>Alex</first><last>Papadopoulos Korfiatis</last></author>
      <author><first>Rachel</first><last>Young</last></author>
      <author><first>Claudia</first><last>Ruffini</last></author>
      <author><first>Mark</first><last>Perera</last></author>
      <author><first>Christian</first><last>Perstl</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Aleksandar</first><last>Savkov</last></author>
      <pages>385-394</pages>
      <abstract>A growing body of work uses Natural Language Processing (NLP) methods to automatically generate medical notes from audio recordings of doctor-patient consultations. However, there are very few studies on how such systems could be used in clinical practice, how clinicians would adjust to using them, or how system design should be influenced by such considerations. In this paper, we present three rounds of user studies, carried out in the context of developing a medical note generation system. We present, analyse and discuss the participating clinicians’ impressions and views of how the system ought to be adapted to be of value to them. Next, we describe a three-week test run of the system in a live telehealth clinical practice. Major findings include (i) the emergence of five different note-taking behaviours; (ii) the importance of the system generating notes in real time during the consultation; and (iii) the identification of a number of clinical use cases that could prove challenging for automatic note generation systems.</abstract>
      <url hash="7a8dd40b">2022.naacl-main.29</url>
      <bibkey>knoll-etal-2022-user</bibkey>
      <doi>10.18653/v1/2022.naacl-main.29</doi>
      <video href="2022.naacl-main.29.mp4"/>
    </paper>
    <paper id="30">
      <title>Ask Me Anything in Your Native Language</title>
      <author><first>Nikita</first><last>Sorokin</last></author>
      <author><first>Dmitry</first><last>Abulkhanov</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <pages>395-406</pages>
      <abstract>Cross-lingual question answering is a thriving field in the modern world, helping people to search information on the web more efficiently. One of the important scenarios is to give an answer even there is no answer in the language a person asks a question with. We present a novel approach based on single encoder for query and passage for retrieval from multi-lingual collection, together with cross-lingual generative reader. It achieves a new state of the art in both retrieval and end-to-end tasks on the XOR TyDi dataset outperforming the previous results up to 10% on several languages. We find that our approach can be generalized to more than 20 languages in zero-shot approach and outperform all previous models by 12%.</abstract>
      <url hash="224fcc7f">2022.naacl-main.30</url>
      <bibkey>sorokin-etal-2022-ask</bibkey>
      <doi>10.18653/v1/2022.naacl-main.30</doi>
      <video href="2022.naacl-main.30.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="31">
      <title>Diversifying Neural Dialogue Generation via Negative Distillation</title>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Bin</first><last>Sun</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>407-418</pages>
      <abstract>Generative dialogue models suffer badly from the generic response problem, limiting their applications to a few toy scenarios. Recently, an interesting approach, namely negative training, has been proposed to alleviate this problem by reminding the model not to generate high-frequency responses during training. However, its performance is hindered by two issues, ignoring low-frequency but generic responses and bringing low-frequency but meaningless responses. In this paper, we propose a novel negative training paradigm, called negative distillation, to keep the model away from the undesirable generic responses while avoiding the above problems. First, we introduce a negative teacher model that can produce query-wise generic responses, and then the student model is required to maximize the distance with multi-level negative knowledge. Empirical results show that our method outperforms previous negative training methods significantly.</abstract>
      <url hash="4c5d1db9">2022.naacl-main.31</url>
      <bibkey>li-etal-2022-diversifying</bibkey>
      <doi>10.18653/v1/2022.naacl-main.31</doi>
      <video href="2022.naacl-main.31.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="32">
      <title>On Synthetic Data for Back Translation</title>
      <author><first>Jiahao</first><last>Xu</last></author>
      <author><first>Yubin</first><last>Ruan</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <author><first>Guoping</first><last>Huang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Lihui</first><last>Chen</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <pages>419-430</pages>
      <abstract>Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance?Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield the better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.</abstract>
      <url hash="b6aec70c">2022.naacl-main.32</url>
      <bibkey>xu-etal-2022-synthetic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.32</doi>
      <video href="2022.naacl-main.32.mp4"/>
      <pwccode url="https://github.com/jiahao004/data-for-bt" additional="false">jiahao004/data-for-bt</pwccode>
    </paper>
    <paper id="33">
      <title>Mapping the Design Space of Human-<fixed-case>AI</fixed-case> Interaction in Text Summarization</title>
      <author><first>Ruijia</first><last>Cheng</last></author>
      <author><first>Alison</first><last>Smith-Renner</last></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes-Larrarte</last></author>
      <pages>431-455</pages>
      <abstract>Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.</abstract>
      <url hash="7b50ba93">2022.naacl-main.33</url>
      <bibkey>cheng-etal-2022-mapping</bibkey>
      <doi>10.18653/v1/2022.naacl-main.33</doi>
      <video href="2022.naacl-main.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Towards Robust and Semantically Organised Latent Representations for Unsupervised Text Style Transfer</title>
      <author><first>Sharan</first><last>Narasimhan</last></author>
      <author><first>Suvodip</first><last>Dey</last></author>
      <author><first>Maunendra</first><last>Desarkar</last></author>
      <pages>456-474</pages>
      <abstract>Recent studies show that auto-encoder based approaches successfully perform language generation, smooth sentence interpolation, and style transfer over unseen attributes using unlabelled datasets in a zero-shot manner. The latent space geometry of such models is organised well enough to perform on datasets where the style is “coarse-grained” i.e. a small fraction of words alone in a sentence are enough to determine the overall style label. A recent study uses a discrete token-based perturbation approach to map “similar” sentences (“similar” defined by low Levenshtein distance/ high word overlap) close by in latent space. This definition of “similarity” does not look into the underlying nuances of the constituent words while mapping latent space neighbourhoods and therefore fails to recognise sentences with different style-based semantics while mapping latent neighbourhoods. We introduce EPAAEs (Embedding Perturbed Adversarial AutoEncoders) which completes this perturbation model, by adding a finely adjustable noise component on the continuous embeddings space. We empirically show that this (a) produces a better organised latent space that clusters stylistically similar sentences together, (b) performs best on a diverse set of text style transfer tasks than its counterparts, and (c) is capable of fine-grained control of Style Transfer strength. We also extend the text style transfer tasks to NLI datasets and show that these more complex definitions of style are learned best by EPAAE. To the best of our knowledge, extending style transfer to NLI tasks has not been explored before.</abstract>
      <url hash="ef02fecc">2022.naacl-main.34</url>
      <bibkey>narasimhan-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.naacl-main.34</doi>
      <video href="2022.naacl-main.34.mp4"/>
      <pwccode url="https://github.com/sharan21/epaae" additional="false">sharan21/epaae</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/styleptb">StylePTB</pwcdataset>
    </paper>
    <paper id="35">
      <title>An Exploration of Post-Editing Effectiveness in Text Summarization</title>
      <author><first>Vivian</first><last>Lai</last></author>
      <author><first>Alison</first><last>Smith-Renner</last></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Ruijia</first><last>Cheng</last></author>
      <author><first>Wenjuan</first><last>Zhang</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes-Larrarte</last></author>
      <pages>475-493</pages>
      <abstract>Automatic summarization methods are efficient but can suffer from low quality. In comparison, manual summarization is expensive but produces higher quality. Can humans and AI collaborate to improve summarization performance? In similar text generation tasks (e.g., machine translation), human-AI collaboration in the form of “post-editing” AI-generated text reduces human workload and improves the quality of AI output. Therefore, we explored whether post-editing offers advantages in text summarization. Specifically, we conducted an experiment with 72 participants, comparing post-editing provided summaries with manual summarization for summary quality, human efficiency, and user experience on formal (XSum news) and informal (Reddit posts) text. This study sheds valuable insights on when post-editing is useful for text summarization: it helped in some cases (e.g., when participants lacked domain knowledge) but not in others (e.g., when provided summaries include inaccurate information). Participants’ different editing strategies and needs for assistance offer implications for future human-AI summarization systems.</abstract>
      <url hash="c6e64334">2022.naacl-main.35</url>
      <bibkey>lai-etal-2022-exploration</bibkey>
      <doi>10.18653/v1/2022.naacl-main.35</doi>
      <video href="2022.naacl-main.35.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="36">
      <title>Automatic Correction of Human Translations</title>
      <award>Best new task (tied) and new resource paper</award>
      <award>Honorable mention for contribution to special theme on human-centered NLP</award>
      <author><first>Jessy</first><last>Lin</last></author>
      <author><first>Geza</first><last>Kovacs</last></author>
      <author><first>Aditya</first><last>Shastry</last></author>
      <author><first>Joern</first><last>Wuebker</last></author>
      <author><first>John</first><last>DeNero</last></author>
      <pages>494-507</pages>
      <abstract>We introduce translation error correction (TEC), the task of automatically correcting human-generated translations.Imperfections in machine translations (MT) have long motivated systems for improving translations post-hoc with automatic post-editing.In contrast, little attention has been devoted to the problem of automatically correcting human translations, despite the intuition that humans make distinct errors that machines would be well-suited to assist with, from typos to inconsistencies in translation conventions.To investigate this, we build and release the Aced corpus with three TEC datasets (available at: github.com/lilt/tec). We show that human errors in TEC exhibit a more diverse range of errors and far fewer translation fluency errors than the MT errors in automatic post-editing datasets, suggesting the need for dedicated TEC models that are specialized to correct human errors. We show that pre-training instead on synthetic errors based on human errors improves TEC F-score by as much as 5.1 points. We conducted a human-in-the-loop user study with nine professional translation editors and found that the assistance of our TEC system led them to produce significantly higher quality revised translations.</abstract>
      <url hash="2fe63501">2022.naacl-main.36</url>
      <bibkey>lin-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.36</doi>
      <pwccode url="https://github.com/lilt/tec" additional="false">lilt/tec</pwccode>
    </paper>
    <paper id="37">
      <title>On the Robustness of Reading Comprehension Models to Entity Renaming</title>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Yang</first><last>Xiao</last></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>508-520</pages>
      <abstract>We study the robustness of machine reading comprehension (MRC) models to entity renaming—do models make more wrong predictions when the same questions are asked about an entity whose name has been changed? Such failures imply that models overly rely on entity information to answer questions, and thus may generalize poorly when facts about the world change or questions are asked about novel entities. To systematically audit this issue, we present a pipeline to automatically generate test examples at scale, by replacing entity names in the original test sample with names from a variety of sources, ranging from names in the same test set, to common names in life, to arbitrary strings. Across five datasets and three pretrained model architectures, MRC models consistently perform worse when entities are renamed, with particularly large accuracy drops on datasets constructed via distant supervision. We also find large differences between models: SpanBERT, which is pretrained with span-level masking, is more robust than RoBERTa, despite having similar accuracy on unperturbed test data. We further experiment with different masking strategies as the continual pretraining objective and find that entity-based masking can improve the robustness of MRC models.</abstract>
      <url hash="0f36586f">2022.naacl-main.37</url>
      <bibkey>yan-etal-2022-robustness</bibkey>
      <doi>10.18653/v1/2022.naacl-main.37</doi>
      <video href="2022.naacl-main.37.mp4"/>
      <pwccode url="https://github.com/ink-usc/entity-robustness" additional="false">ink-usc/entity-robustness</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="38">
      <title>Explaining Why: How Instructions and User Interfaces Impact Annotator Rationales When Labeling Text Data</title>
      <author><first>Jamar</first><last>Sullivan Jr.</last></author>
      <author><first>Will</first><last>Brackenbury</last></author>
      <author><first>Andrew</first><last>McNutt</last></author>
      <author><first>Kevin</first><last>Bryson</last></author>
      <author><first>Kwam</first><last>Byll</last></author>
      <author><first>Yuxin</first><last>Chen</last></author>
      <author><first>Michael</first><last>Littman</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <author><first>Blase</first><last>Ur</last></author>
      <pages>521-531</pages>
      <abstract>In the context of data labeling, NLP researchers are increasingly interested in having humans select rationales, a subset of input tokens relevant to the chosen label. We conducted a 332-participant online user study to understand how humans select rationales, especially how different instructions and user interface affordances impact the rationales chosen. Participants labeled ten movie reviews as positive or negative, selecting words and phrases supporting their label as rationales. We varied the instructions given, the rationale-selection task, and the user interface. Participants often selected about 12% of input tokens as rationales, but selected fewer if unable to drag over multiple tokens at once. Whereas participants were near unanimous in their data labels, they were far less consistent in their rationales. The user interface affordances and task greatly impacted the types of rationales chosen. We also observed large variance across participants.</abstract>
      <url hash="53c8e5a8">2022.naacl-main.38</url>
      <attachment type="software" hash="4f0b0dff">2022.naacl-main.38.software.zip</attachment>
      <bibkey>sullivan-etal-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.38</doi>
      <video href="2022.naacl-main.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization</title>
      <author><first>Haode</first><last>Zhang</last></author>
      <author><first>Haowen</first><last>Liang</last></author>
      <author><first>Yuwei</first><last>Zhang</last></author>
      <author><first>Li-Ming</first><last>Zhan</last></author>
      <author><first>Xiao-Ming</first><last>Wu</last></author>
      <author><first>Xiaolei</first><last>Lu</last></author>
      <author><first>Albert</first><last>Lam</last></author>
      <pages>532-542</pages>
      <abstract>It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pre-training by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at https://github.com/fanolabs/isoIntentBert-main.</abstract>
      <url hash="5e5f256f">2022.naacl-main.39</url>
      <attachment type="software" hash="f3d4a0c2">2022.naacl-main.39.software.zip</attachment>
      <bibkey>zhang-etal-2022-fine</bibkey>
      <doi>10.18653/v1/2022.naacl-main.39</doi>
      <video href="2022.naacl-main.39.mp4"/>
      <pwccode url="https://github.com/fanolabs/isointentbert-main" additional="false">fanolabs/isointentbert-main</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hint3">HINT3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
    </paper>
    <paper id="40">
      <title>Cross-document Misinformation Detection based on Event Graph Reasoning</title>
      <author><first>Xueqing</first><last>Wu</last></author>
      <author><first>Kung-Hsiang</first><last>Huang</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>543-558</pages>
      <abstract>For emerging events, human readers are often exposed to both real news and fake news. Multiple news articles may contain complementary or contradictory information that readers can leverage to help detect fake news. Inspired by this process, we propose a novel task of cross-document misinformation detection. Given a cluster of topically related news documents, we aim to detect misinformation at both document level and a more fine-grained level, event level. Due to the lack of data, we generate fake news by manipulating real news, and construct 3 new datasets with 422, 276, and 1,413 clusters of topically related documents, respectively. We further propose a graph-based detector that constructs a cross-document knowledge graph using cross-document event coreference resolution and employs a heterogeneous graph neural network to conduct detection at two levels. We then feed the event-level detection results into the document-level detector. Experimental results show that our proposed method significantly outperforms existing methods by up to 7 F1 points on this new task.</abstract>
      <url hash="e13dc568">2022.naacl-main.40</url>
      <bibkey>wu-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.naacl-main.40</doi>
      <video href="2022.naacl-main.40.mp4"/>
      <pwccode url="https://github.com/shirley-wu/cross-doc-misinfo-detection" additional="false">shirley-wu/cross-doc-misinfo-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
    </paper>
    <paper id="41">
      <title>Disentangled Action Recognition with Knowledge Bases</title>
      <author><first>Zhekun</first><last>Luo</last></author>
      <author><first>Shalini</first><last>Ghosh</last></author>
      <author><first>Devin</first><last>Guillory</last></author>
      <author><first>Keizo</first><last>Kato</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <author><first>Huijuan</first><last>Xu</last></author>
      <pages>559-572</pages>
      <abstract>Action in video usually involves the interaction of human with objects. Action labels are typically composed of various combinations of verbs and nouns, but we may not have training data for all possible combinations. In this paper, we aim to improve the generalization ability of the compositional action recognition model to novel verbs or novel nouns that are unseen during training time, by leveraging the power of knowledge graphs. Previous work utilizes verb-noun compositional action nodes in the knowledge graph, making it inefficient to scale since the number of compositional action nodes grows quadratically with respect to the number of verbs and nouns. To address this issue, we propose our approach: Disentangled Action Recognition with Knowledge-bases (DARK), which leverages the inherent compositionality of actions. DARK trains a factorized model by first extracting disentangled feature representations for verbs and nouns, and then predicting classification weights using relations in external knowledge graphs. The type constraint between verb and noun is extracted from external knowledge bases and finally applied when composing actions. DARK has better scalability in the number of objects and verbs, and achieves state-of-the-art performance on the Charades dataset. We further propose a new benchmark split based on the Epic-kitchen dataset which is an order of magnitude bigger in the numbers of classes and samples, and benchmark various models on this benchmark.</abstract>
      <url hash="a323fdfe">2022.naacl-main.41</url>
      <bibkey>luo-etal-2022-disentangled</bibkey>
      <doi>10.18653/v1/2022.naacl-main.41</doi>
      <video href="2022.naacl-main.41.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
    </paper>
    <paper id="42">
      <title>Machine-in-the-Loop Rewriting for Creative Image Captioning</title>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>573-586</pages>
      <abstract>Machine-in-the-loop writing aims to build models that assist humans to accomplish their writing tasks more effectively. Prior work has found that providing users a machine-written draft or sentence-level continuations has limited success since the generated text tends to deviate from users’ intention. To allow the user to retain control over the content, we train a rewriting model that, when prompted, modifies specified spans of text within the user’s original draft to introduce descriptive and figurative elements in the text.We evaluate the model on its ability to collaborate with humans on the task of creative image captioning. On a user study through Amazon Mechanical Turk, our model is rated to be more helpful by users than a baseline infilling language model. In addition, third-party evaluation shows that users write more descriptive and figurative captions when collaborating with our model compared to completing the task alone.However, the improvement is not uniform across user groups: the model is more helpful to skilled users, which risks widening the gap between skilled and novice users, highlighting a need for careful, user-centric evaluation of interactive systems.</abstract>
      <url hash="ecf2fe3b">2022.naacl-main.42</url>
      <bibkey>padmakumar-he-2022-machine</bibkey>
      <doi>10.18653/v1/2022.naacl-main.42</doi>
      <video href="2022.naacl-main.42.mp4"/>
      <pwccode url="https://github.com/vishakhpk/mil-creative-captioning" additional="false">vishakhpk/mil-creative-captioning</pwccode>
    </paper>
    <paper id="43">
      <title>A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction</title>
      <author><first>Yong</first><last>Xie</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <author><first>Pin-Yu</first><last>Chen</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Oluwasanmi</first><last>Koyejo</last></author>
      <pages>587-599</pages>
      <abstract>More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather information and predict movements stock prices. Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability given necessary constraints is underexplored. In this paper, we experiment with a variety of adversarial attack configurations to fool three stock prediction victim models. We address the task of adversarial generation by solving combinatorial optimization problems with semantics and budget constraints. Our results show that the proposed attack method can achieve consistent success rates and cause significant monetary loss in trading simulation by simply concatenating a perturbed but semantically similar tweet.</abstract>
      <url hash="1b4343d5">2022.naacl-main.43</url>
      <bibkey>xie-etal-2022-word</bibkey>
      <doi>10.18653/v1/2022.naacl-main.43</doi>
      <video href="2022.naacl-main.43.mp4"/>
      <pwccode url="https://github.com/yonxie/advfintweet" additional="false">yonxie/advfintweet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/stocknet-1">StockNet</pwcdataset>
    </paper>
    <paper id="44">
      <title>Building Multilingual Machine Translation Systems That Serve Arbitrary <fixed-case>XY</fixed-case> Translations</title>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Shufang</first><last>Xie</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>600-606</pages>
      <abstract>Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems. The MNMT training benefit, however, is often limited to many-to-one directions. The model suffers from poor performance in one-to-many and many-to-many with zero-shot setup. To address this issue, this paper discusses how to practically build MNMT systems that serve arbitrary X-Y translation directions while leveraging multilinguality with a two-stage training strategy of pretraining and finetuning. Experimenting with the WMT’21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection. Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.</abstract>
      <url hash="190af786">2022.naacl-main.44</url>
      <bibkey>eriguchi-etal-2022-building</bibkey>
      <doi>10.18653/v1/2022.naacl-main.44</doi>
      <video href="2022.naacl-main.44.mp4"/>
    </paper>
    <paper id="45">
      <title>Non-Autoregressive Neural Machine Translation with Consistency Regularization Optimized Variational Framework</title>
      <author><first>Minghao</first><last>Zhu</last></author>
      <author><first>Junli</first><last>Wang</last></author>
      <author><first>Chungang</first><last>Yan</last></author>
      <pages>607-617</pages>
      <abstract>Variational Autoencoder (VAE) is an effective framework to model the interdependency for non-autoregressive neural machine translation (NAT). One of the prominent VAE-based NAT frameworks, LaNMT, achieves great improvements to vanilla models, but still suffers from two main issues which lower down the translation quality: (1) mismatch between training and inference circumstances and (2) inadequacy of latent representations. In this work, we target on addressing these issues by proposing posterior consistency regularization. Specifically, we first perform stochastic data augmentation on the input samples to better adapt the model for inference circumstance, and then conduct consistency training on posterior latent variables to construct a more robust latent representations without any expansion on latent size. Experiments on En&lt;-&gt;De and En&lt;-&gt;Ro benchmarks confirm the effectiveness of our methods with about 1.5/0.7 and 0.8/0.3 BLEU points improvement to the baseline model with about <tex-math>12.6\times</tex-math> faster than autoregressive Transformer.</abstract>
      <url hash="6818ed7d">2022.naacl-main.45</url>
      <bibkey>zhu-etal-2022-non</bibkey>
      <doi>10.18653/v1/2022.naacl-main.45</doi>
      <video href="2022.naacl-main.45.mp4"/>
    </paper>
    <paper id="46">
      <title>User-Centric Gender Rewriting</title>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <pages>618-631</pages>
      <abstract>In this paper, we define the task of gender rewriting in contexts involving two users (I and/or You) – first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. We develop a multi-step system that combines the positive aspects of both rule-based and neural rewriting models. Our results successfully demonstrate the viability of this approach on a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5 on a blind test set. Our proposed system improves over previous work on the first-person-only version of this task, by 3.05 absolute increase in M2 F0.5. We demonstrate a use case of our gender rewriting system by using it to post-edit the output of a commercial MT system to provide personalized outputs based on the users’ grammatical gender preferences. We make our code, data, and pretrained models publicly available.</abstract>
      <url hash="ebb820fd">2022.naacl-main.46</url>
      <bibkey>alhafni-etal-2022-user</bibkey>
      <doi>10.18653/v1/2022.naacl-main.46</doi>
      <video href="2022.naacl-main.46.mp4"/>
      <pwccode url="https://github.com/camel-lab/gender-rewriting" additional="false">camel-lab/gender-rewriting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="47">
      <title>Reframing Human-<fixed-case>AI</fixed-case> Collaboration for Generating Free-Text Explanations</title>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>632-658</pages>
      <abstract>Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using human-written examples in a few-shot manner. We find that (1) authoring higher quality prompts results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced explanations in existing datasets. Our human studies also show, however, that while models often produce factual, grammatical, and sufficient explanations, they have room to improve along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates binary acceptability judgments from humans in the loop. Despite the intrinsic subjectivity of acceptability judgments, we demonstrate that acceptability is partially correlated with various fine-grained attributes of explanations. Our approach is able to consistently filter GPT-3-generated explanations deemed acceptable by humans.</abstract>
      <url hash="95dfd53b">2022.naacl-main.47</url>
      <bibkey>wiegreffe-etal-2022-reframing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.47</doi>
      <video href="2022.naacl-main.47.mp4"/>
      <pwccode url="https://github.com/allenai/few_shot_explanations" additional="false">allenai/few_shot_explanations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cos-e">CoS-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecqa">ECQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="48">
      <title><fixed-case>E</fixed-case>m<fixed-case>R</fixed-case>el: Joint Representation of Entities and Embedded Relations for Multi-triple Extraction</title>
      <author><first>Benfeng</first><last>Xu</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Yabing</first><last>Shi</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <author><first>Jie</first><last>Gao</last></author>
      <author><first>Zhendong</first><last>Mao</last></author>
      <pages>659-665</pages>
      <abstract>Multi-triple extraction is a challenging task due to the existence of informative inter-triple correlations, and consequently rich interactions across the constituent entities and relations.While existing works only explore entity representations, we propose to explicitly introduce <i>relation</i> representation, jointly represent it with entities, and novelly align them to identify valid triples.We perform comprehensive experiments on document-level relation extraction and joint entity and relation extraction along with ablations to demonstrate the advantage of the proposed method.</abstract>
      <url hash="abb1d4bc">2022.naacl-main.48</url>
      <bibkey>xu-etal-2022-emrel</bibkey>
      <doi>10.18653/v1/2022.naacl-main.48</doi>
      <video href="2022.naacl-main.48.mp4"/>
      <pwccode url="https://github.com/benfengxu/emrel" additional="false">benfengxu/emrel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="49">
      <title>Meta Learning for Natural Language Processing: A Survey</title>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Thang</first><last>Vu</last></author>
      <pages>666-684</pages>
      <abstract>Deep learning has been the mainstream technique in the natural language processing (NLP) area. However, deep learning requires many labeled data and is less generalizable across domains. Meta-learning is an arising field in machine learning. It studies approaches to learning better learning algorithms and aims to improve algorithms in various aspects, including data efficiency and generalizability. The efficacy of meta-learning has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings, applications of meta-learning for various NLP problems and review the development of meta-learning in the NLP community.</abstract>
      <url hash="56921e2d">2022.naacl-main.49</url>
      <bibkey>lee-etal-2022-meta</bibkey>
      <doi>10.18653/v1/2022.naacl-main.49</doi>
      <video href="2022.naacl-main.49.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="50">
      <title>Analyzing Modality Robustness in Multimodal Sentiment Analysis</title>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Yingting</first><last>Li</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Roger</first><last>Zimmermann</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>685-696</pages>
      <abstract>Building robust multimodal models are crucial for achieving reliable deployment in the wild. Despite its importance, less attention has been paid to identifying and improving the robustness of Multimodal Sentiment Analysis (MSA) models. In this work, we hope to address that by (i) Proposing simple diagnostic checks for modality robustness in a trained multimodal model. Using these checks, we find MSA models to be highly sensitive to a single modality, which creates issues in their robustness; (ii) We analyze well-known robust training strategies to alleviate the issues. Critically, we observe that robustness can be achieved without compromising on the original performance. We hope our extensive study–performed across five models and two benchmark datasets–and proposed procedures would make robustness an integral component in MSA research. Our diagnostic checks and robust training solutions are simple to implement and available at https://github.com/declare-lab/MSA-Robustness</abstract>
      <url hash="8ecd1e5e">2022.naacl-main.50</url>
      <bibkey>hazarika-etal-2022-analyzing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.50</doi>
      <video href="2022.naacl-main.50.mp4"/>
      <pwccode url="https://github.com/declare-lab/msa-robustness" additional="false">declare-lab/msa-robustness</pwccode>
    </paper>
    <paper id="51">
      <title>Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation</title>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>697-716</pages>
      <abstract>The past several years have witnessed Variational Auto-Encoder’s superiority in various text generation tasks. However, due to the sequential nature of the text, auto-regressive decoders tend to ignore latent variables and then reduce to simple language models, known as the <tex-math>\textit{KL vanishing}</tex-math> problem, which would further deteriorate when VAE is combined with Transformer-based structures. To ameliorate this problem, we propose Della, a novel variational Transformer framework. Della learns a series of layer-wise latent variables with each inferred from those of lower layers and tightly coupled with the hidden states by low-rank tensor product. In this way, Della forces these posterior latent variables to be fused deeply with the whole computation path and hence incorporate more information. We theoretically demonstrate that our method can be regarded as entangling latent variables to avoid posterior information decrease through layers, enabling Della to get higher non-zero KL values even without any annealing or thresholding tricks. Experiments on four unconditional and three conditional generation tasks show that Della could better alleviate KL vanishing and improve both quality and diversity compared to several strong baselines.</abstract>
      <url hash="9ca8c0d8">2022.naacl-main.51</url>
      <attachment type="software" hash="d5f65f1d">2022.naacl-main.51.software.zip</attachment>
      <bibkey>hu-etal-2022-fuse</bibkey>
      <doi>10.18653/v1/2022.naacl-main.51</doi>
      <video href="2022.naacl-main.51.mp4"/>
      <pwccode url="https://github.com/openvlg/della" additional="false">openvlg/della</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="52">
      <title>Easy Adaptation to Mitigate Gender Bias in Multilingual Text Classification</title>
      <author><first>Xiaolei</first><last>Huang</last></author>
      <pages>717-723</pages>
      <abstract>Existing approaches to mitigate demographic biases evaluate on monolingual data, however, multilingual data has not been examined. In this work, we treat the gender as domains (e.g., male vs. female) and present a standard domain adaptation model to reduce the gender bias and improve performance of text classifiers under multilingual settings. We evaluate our approach on two text classification tasks, hate speech detection and rating prediction, and demonstrate the effectiveness of our approach with three fair-aware baselines.</abstract>
      <url hash="4d5185b7">2022.naacl-main.52</url>
      <attachment type="software" hash="c32828df">2022.naacl-main.52.software.zip</attachment>
      <bibkey>huang-2022-easy</bibkey>
      <doi>10.18653/v1/2022.naacl-main.52</doi>
      <video href="2022.naacl-main.52.mp4"/>
      <pwccode url="https://github.com/xiaoleihuang/domainfairness" additional="false">xiaoleihuang/domainfairness</pwccode>
    </paper>
    <paper id="53">
      <title>On the Use of External Data for Spoken Named Entity Recognition</title>
      <author><first>Ankita</first><last>Pasad</last></author>
      <author><first>Felix</first><last>Wu</last></author>
      <author><first>Suwon</first><last>Shon</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Kyu</first><last>Han</last></author>
      <pages>724-737</pages>
      <abstract>Spoken language understanding (SLU) tasks involve mapping from speech signals to semantic labels. Given the complexity of such tasks, good performance is expected to require large labeled datasets, which are difficult to collect for each new task and domain. However, recent advances in self-supervised speech representations have made it feasible to consider learning SLU models with limited labeled data. In this work, we focus on low-resource spoken named entity recognition (NER) and address the question: Beyond self-supervised pre-training, how can we use external speech and/or text data that are not annotated for the task? We consider self-training, knowledge distillation, and transfer learning for end-to-end (E2E) and pipeline (speech recognition followed by text NER) approaches. We find that several of these approaches improve performance in resource-constrained settings beyond the benefits from pre-trained representations. Compared to prior work, we find relative improvements in F1 of up to 16%. While the best baseline model is a pipeline approach, the best performance using external data is ultimately achieved by an E2E model. We provide detailed comparisons and analyses, developing insights on, for example, the effects of leveraging external data on (i) different categories of NER errors and (ii) the switch in performance trends between pipeline and E2E models.</abstract>
      <url hash="b137cf71">2022.naacl-main.53</url>
      <bibkey>pasad-etal-2022-use</bibkey>
      <doi>10.18653/v1/2022.naacl-main.53</doi>
      <video href="2022.naacl-main.53.mp4"/>
      <pwccode url="https://github.com/asappresearch/spoken-ner" additional="false">asappresearch/spoken-ner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/slue">SLUE</pwcdataset>
    </paper>
    <paper id="54">
      <title>Long-term Control for Dialogue Generation: Methods and Evaluation</title>
      <author><first>Ramya</first><last>Ramakrishnan</last></author>
      <author><first>Hashan</first><last>Narangodage</last></author>
      <author><first>Mauro</first><last>Schilman</last></author>
      <author><first>Kilian</first><last>Weinberger</last></author>
      <author><first>Ryan</first><last>McDonald</last></author>
      <pages>738-753</pages>
      <abstract>Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.</abstract>
      <url hash="7bb0ab7c">2022.naacl-main.54</url>
      <attachment type="software" hash="0942af77">2022.naacl-main.54.software.zip</attachment>
      <bibkey>ramakrishnan-etal-2022-long</bibkey>
      <doi>10.18653/v1/2022.naacl-main.54</doi>
      <video href="2022.naacl-main.54.mp4"/>
      <pwccode url="https://github.com/asappresearch/constrained-dialogue-generation" additional="false">asappresearch/constrained-dialogue-generation</pwccode>
    </paper>
    <paper id="55">
      <title>Learning Dialogue Representations from Consecutive Utterances</title>
      <author><first>Zhihan</first><last>Zhou</last></author>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Nicholas</first><last>Dingwall</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>754-768</pages>
      <abstract>Learning high-quality dialogue representations is essential for solving a variety of dialogue-oriented tasks, especially considering that dialogue systems often suffer from data scarcity. In this paper, we introduce Dialogue Sentence Embedding (DSE), a self-supervised contrastive learning method that learns effective dialogue representations suitable for a wide range of dialogue tasks. DSE learns from dialogues by taking consecutive utterances of the same dialogue as positive pairs for contrastive learning. Despite its simplicity, DSE achieves significantly better representation capability than other dialogue representation and universal sentence representation models. We evaluate DSE on five downstream dialogue tasks that examine dialogue representation at different semantic granularities. Experiments in few-shot and zero-shot settings show that DSE outperforms baselines by a large margin, for example, it achieves 13% average performance improvement over the strongest unsupervised baseline in 1-shot intent classification on 6 datasets. We also provide analyses on the benefits and limitations of our model.</abstract>
      <url hash="09238d11">2022.naacl-main.55</url>
      <bibkey>zhou-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.55</doi>
      <video href="2022.naacl-main.55.mp4"/>
      <pwccode url="https://github.com/amazon-research/dse" additional="false">amazon-research/dse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/amazonqa">AmazonQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77-oos">BANKING77-OOS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc-single-domain-oos">CLINC-Single-Domain-OOS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dstc7-task-1">DSTC7 Task 1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="56">
      <title>On the Machine Learning of Ethical Judgments from Natural Language</title>
      <author><first>Zeerak</first><last>Talat</last></author>
      <author><first>Hagen</first><last>Blix</last></author>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Maya Indira</first><last>Ganesh</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>769-779</pages>
      <abstract>Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making. Through an audit of recent work on computational approaches for predicting morality, we examine the broader issues that arise from such efforts. We conclude with a discussion of how machine ethics could usefully proceed in NLP, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.</abstract>
      <url hash="28d81010">2022.naacl-main.56</url>
      <bibkey>talat-etal-2022-machine</bibkey>
      <doi>10.18653/v1/2022.naacl-main.56</doi>
      <video href="2022.naacl-main.56.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
    </paper>
    <paper id="57">
      <title><fixed-case>N</fixed-case>euro<fixed-case>L</fixed-case>ogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics</title>
      <award>Best new method paper</award>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Youngjae</first><last>Yu</last></author>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>780-799</pages>
      <abstract>The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the <tex-math>A^*</tex-math> search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-<tex-math>k</tex-math> sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.</abstract>
      <url hash="af15dc46">2022.naacl-main.57</url>
      <bibkey>lu-etal-2022-neurologic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.57</doi>
      <pwccode url="https://github.com/GXimingLu/a_star_neurologic" additional="false">GXimingLu/a_star_neurologic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="58">
      <title><fixed-case>PARADISE</fixed-case>: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining</title>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <pages>800-810</pages>
      <abstract>Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel &amp;Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the reference translation according to a parallel corpus instead of recovering the original sequence. Our experiments on machine translation and cross-lingual natural language inference show an average improvement of 2.0 BLEU points and 6.7 accuracy points from integrating parallel data into pretraining, respectively, obtaining results that are competitive with several popular models at a fraction of their computational cost.</abstract>
      <url hash="d27abd21">2022.naacl-main.58</url>
      <bibkey>reid-artetxe-2022-paradise</bibkey>
      <doi>10.18653/v1/2022.naacl-main.58</doi>
      <video href="2022.naacl-main.58.mp4"/>
      <pwccode url="https://github.com/machelreid/paradise" additional="false">machelreid/paradise</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="59">
      <title>Explaining Toxic Text via Knowledge Enhanced Text Generation</title>
      <author><first>Rohit</first><last>Sridhar</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>811-826</pages>
      <abstract>Warning: This paper contains content that is offensive and may be upsetting.Biased or toxic speech can be harmful to various demographic groups. Therefore, it is not only important for models to detect these speech, but to also output explanations of why a given text is toxic. Previous literature has mostly focused on classifying and detecting toxic speech, and existing efforts on explaining stereotypes in toxic speech mainly use standard text generation approaches, resulting in generic and repetitive explanations. Building on these prior works, we introduce a novel knowledge-informed encoder-decoder framework to utilize multiple knowledge sources to generate implications of biased text.Experiments show that our knowledge informed models outperform prior state-of-the-art models significantly, and can generate detailed explanations of stereotypes in toxic speech compared to baselines, both quantitatively and qualitatively.</abstract>
      <url hash="9d16517e">2022.naacl-main.59</url>
      <bibkey>sridhar-yang-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.59</doi>
      <video href="2022.naacl-main.59.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/implicit-hate">Implicit Hate</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sbic">SBIC</pwcdataset>
    </paper>
    <paper id="60">
      <title>Teaching <fixed-case>BERT</fixed-case> to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection</title>
      <author><first>Angelica</first><last>Chen</last></author>
      <author><first>Vicky</first><last>Zayats</last></author>
      <author><first>Daniel</first><last>Walker</last></author>
      <author><first>Dirk</first><last>Padfield</last></author>
      <pages>827-838</pages>
      <abstract>In modern interactive speech-based systems, speech is consumed and transcribed incrementally prior to having disfluencies removed. While this post-processing step is crucial for producing clean transcripts and high performance on downstream tasks (e.g. machine translation), most current state-of-the-art NLP models such as the Transformer operate non-incrementally, potentially causing unacceptable delays for the user. In this work we propose a streaming BERT-based sequence tagging model that, combined with a novel training objective, is capable of detecting disfluencies in real-time while balancing accuracy and latency. This is accomplished by training the model to decide whether to immediately output a prediction for the current input or to wait for further context, in essence learning to dynamically size the lookahead window. Our results demonstrate that our model produces comparably accurate predictions and does so sooner than our baselines, with lower flicker. Furthermore, the model attains state-of-the-art latency and stability scores when compared with recent work on incremental disfluency detection.</abstract>
      <url hash="9d36635c">2022.naacl-main.60</url>
      <bibkey>chen-etal-2022-teaching</bibkey>
      <doi>10.18653/v1/2022.naacl-main.60</doi>
      <video href="2022.naacl-main.60.mp4"/>
    </paper>
    <paper id="61">
      <title><fixed-case>GRAM</fixed-case>: <fixed-case>F</fixed-case>ast <fixed-case>F</fixed-case>ine-tuning of <fixed-case>P</fixed-case>re-trained <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels for <fixed-case>C</fixed-case>ontent-based <fixed-case>C</fixed-case>ollaborative <fixed-case>F</fixed-case>iltering</title>
      <author><first>Yoonseok</first><last>Yang</last></author>
      <author><first>Kyu Seok</first><last>Kim</last></author>
      <author><first>Minsam</first><last>Kim</last></author>
      <author><first>Juneyoung</first><last>Park</last></author>
      <pages>839-851</pages>
      <abstract>Content-based collaborative filtering (CCF) predicts user-item interactions based on both users’ interaction history and items’ content information. Recently, pre-trained language models (PLM) have been used to extract high-quality item encodings for CCF. However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence. To tackle this issue, we propose GRAM (GRadient Accumulation for Multi-modality in CCF), which exploits the fact that a given item often appears multiple times within a batch of interaction histories. Specifically, Single-step GRAM aggregates each item encoding’s gradients for back-propagation, with theoretic equivalence to the standard E2E training. As an extension of Single-step GRAM, we propose Multi-step GRAM, which increases the gradient update latency, achieving a further speedup with drastically less GPU memory. GRAM significantly improves training efficiency (up to 146x) on five datasets from two task domains of Knowledge Tracing and News Recommendation. Our code is available at https://github.com/yoonseok312/GRAM.</abstract>
      <url hash="ca98ce53">2022.naacl-main.61</url>
      <bibkey>yang-etal-2022-gram</bibkey>
      <doi>10.18653/v1/2022.naacl-main.61</doi>
      <video href="2022.naacl-main.61.mp4"/>
      <pwccode url="https://github.com/yoonseok312/gram" additional="false">yoonseok312/gram</pwccode>
    </paper>
    <paper id="62">
      <title>Generating Repetitions with Appropriate Repeated Words</title>
      <author><first>Toshiki</first><last>Kawamoto</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>852-859</pages>
      <abstract>A repetition is a response that repeats words in the previous speaker’s utterance in a dialogue. Repetitions are essential in communication to build trust with others, as investigated in linguistic studies. In this work, we focus on repetition generation. To the best of our knowledge, this is the first neural approach to address repetition generation. We propose Weighted Label Smoothing, a smoothing method for explicitly learning which words to repeat during fine-tuning, and a repetition scoring method that can output more appropriate repetitions during decoding. We conducted automatic and human evaluations involving applying these methods to the pre-trained language model T5 for generating repetitions. The experimental results indicate that our methods outperformed baselines in both evaluations.</abstract>
      <url hash="f007b031">2022.naacl-main.62</url>
      <bibkey>kawamoto-etal-2022-generating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.62</doi>
      <video href="2022.naacl-main.62.mp4"/>
      <pwccode url="https://github.com/titech-nlp/repetition-generation" additional="false">titech-nlp/repetition-generation</pwccode>
    </paper>
    <paper id="63">
      <title>Textless Speech-to-Speech Translation on Real Data</title>
      <author><first>Ann</first><last>Lee</last></author>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Paul-Ambroise</first><last>Duquenne</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Sravya</first><last>Popuri</last></author>
      <author><first>Yossi</first><last>Adi</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Wei-Ning</first><last>Hsu</last></author>
      <pages>860-872</pages>
      <abstract>We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of any text data. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automatically mined S2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs.</abstract>
      <url hash="970ed237">2022.naacl-main.63</url>
      <bibkey>lee-etal-2022-textless</bibkey>
      <doi>10.18653/v1/2022.naacl-main.63</doi>
      <video href="2022.naacl-main.63.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl-st">Europarl-ST</pwcdataset>
    </paper>
    <paper id="64">
      <title><fixed-case>WALNUT</fixed-case>: A Benchmark on Semi-weakly Supervised Learning for Natural Language Understanding</title>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Kai</first><last>Shu</last></author>
      <author><first>Ahmed</first><last>Awadallah</last></author>
      <pages>873-899</pages>
      <abstract>Building machine learning models for natural language understanding (NLU) tasks relies heavily on labeled data. Weak supervision has been proven valuable when large amount of labeled data is unavailable or expensive to obtain. Existing works studying weak supervision for NLU either mostly focus on a specific task or simulate weak supervision signals from ground-truth labels. It is thus hard to compare different approaches and evaluate the benefit of weak supervision without access to a unified and systematic benchmark with diverse tasks and real-world weak labeling rules. In this paper, we propose such a benchmark, named WALNUT, to advocate and facilitate research on weak supervision for NLU. WALNUT consists of NLU tasks with different types, including document-level and token-level prediction tasks. WALNUT is the first semi-weakly supervised learning benchmark for NLU, where each task contains weak labels generated by multiple real-world weak sources, together with a small set of clean labels. We conduct baseline evaluations on WALNUT to systematically evaluate the effectiveness of various weak supervision methods and model architectures. Our results demonstrate the benefit of weak supervision for low-resource NLU tasks and highlight interesting patterns across tasks. We expect WALNUT to stimulate further research on methodologies to leverage weak supervision more effectively. The benchmark and code for baselines are available at aka.ms/walnut_benchmark.</abstract>
      <url hash="b022292d">2022.naacl-main.64</url>
      <bibkey>zheng-etal-2022-walnut</bibkey>
      <doi>10.18653/v1/2022.naacl-main.64</doi>
      <video href="2022.naacl-main.64.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="65">
      <title><fixed-case>C</fixed-case>ompact<fixed-case>IE</fixed-case>: Compact Facts in Open Information Extraction</title>
      <author><first>Farima</first><last>Fatahi Bayat</last></author>
      <author><first>Nikita</first><last>Bhutani</last></author>
      <author><first>H.</first><last>Jagadish</last></author>
      <pages>900-910</pages>
      <abstract>A major drawback of modern neural OpenIE systems and benchmarks is that they prioritize high coverage of information in extractions over compactness of their constituents. This severely limits the usefulness of OpenIE extractions in many downstream tasks. The utility of extractions can be improved if extractions are compact and share constituents. To this end, we study the problem of identifying compact extractions with neural-based methods. We propose CompactIE, an OpenIE system that uses a novel pipelined approach to produce compact extractions with overlapping constituents. It first detects constituents of the extractions and then links them to build extractions. We train our system on compact extractions obtained by processing existing benchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE finds 1.5x-2x more compact extractions than previous systems, with high precision, establishing a new state-of-the-art performance in OpenIE.</abstract>
      <url hash="36df58a5">2022.naacl-main.65</url>
      <bibkey>fatahi-bayat-etal-2022-compactie</bibkey>
      <doi>10.18653/v1/2022.naacl-main.65</doi>
      <video href="2022.naacl-main.65.mp4"/>
      <pwccode url="https://github.com/farimafatahi/compactie" additional="false">farimafatahi/compactie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/benchie">BenchIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wire57">WiRe57</pwcdataset>
    </paper>
    <paper id="66">
      <title><fixed-case>C</fixed-case>o<fixed-case>SI</fixed-case>m: Commonsense Reasoning for Counterfactual Scene Imagination</title>
      <author><first>Hyounghun</first><last>Kim</last></author>
      <author><first>Abhay</first><last>Zala</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>911-923</pages>
      <abstract>As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be specific, in this multimodal task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.</abstract>
      <url hash="aac7fae1">2022.naacl-main.66</url>
      <attachment type="software" hash="8b8bee80">2022.naacl-main.66.software.zip</attachment>
      <bibkey>kim-etal-2022-cosim</bibkey>
      <doi>10.18653/v1/2022.naacl-main.66</doi>
      <video href="2022.naacl-main.66.mp4"/>
      <pwccode url="https://github.com/hyounghk/cosim" additional="false">hyounghk/cosim</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="67">
      <title>Abstraction not Memory: <fixed-case>BERT</fixed-case> and the <fixed-case>E</fixed-case>nglish Article System</title>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Dagmar</first><last>Divjak</last></author>
      <author><first>Petar</first><last>Milin</last></author>
      <pages>924-931</pages>
      <abstract>Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to humans at detecting the zero article, possibly because we insert them using rules that the deep neural model can easily pick up. More interestingly, we find that BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high but switches to agreeing more with the corpus as inter-annotator agreement drops. We contend that this alignment with annotators, despite being trained on the corpus, suggests that BERT is not memorising article use, but captures a high level generalisation of article use akin to human intuition.</abstract>
      <url hash="e01e2baf">2022.naacl-main.67</url>
      <bibkey>tayyar-madabushi-etal-2022-abstraction</bibkey>
      <doi>10.18653/v1/2022.naacl-main.67</doi>
      <video href="2022.naacl-main.67.mp4"/>
    </paper>
    <paper id="68">
      <title><fixed-case>O</fixed-case>mni<fixed-case>T</fixed-case>ab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering</title>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Yi</first><last>Mao</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>932-942</pages>
      <abstract>The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.</abstract>
      <url hash="0def31a3">2022.naacl-main.68</url>
      <bibkey>jiang-etal-2022-omnitab</bibkey>
      <doi>10.18653/v1/2022.naacl-main.68</doi>
      <video href="2022.naacl-main.68.mp4"/>
      <pwccode url="https://github.com/jzbjyb/omnitab" additional="false">jzbjyb/omnitab</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="69">
      <title>Provably Confidential Language Modelling</title>
      <author><first>Xuandong</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yu-Xiang</first><last>Wang</last></author>
      <pages>943-955</pages>
      <abstract>Large language models are shown to memorize privacy information such as social security numbers in training data. Given the sheer scale of the training corpus, it is challenging to screen and filter these privacy data, either manually or automatically. In this paper, we propose Confidentially Redacted Training (CRT), a method to train language generation models while protecting the confidential segments. We borrow ideas from differential privacy (which solves a related but distinct problem) and show that our method is able to provably prevent unintended memorization by randomizing parts of the training process. Moreover, we show that redaction with an approximately correct screening policy amplifies the confidentiality guarantee. We implement the method for both LSTM and GPT language models. Our experimental results show that the models trained by CRT obtain almost the same perplexity while preserving strong confidentiality.</abstract>
      <url hash="a01e8fad">2022.naacl-main.69</url>
      <bibkey>zhao-etal-2022-provably</bibkey>
      <doi>10.18653/v1/2022.naacl-main.69</doi>
      <video href="2022.naacl-main.69.mp4"/>
      <pwccode url="https://github.com/xuandongzhao/crt" additional="false">xuandongzhao/crt</pwccode>
    </paper>
    <paper id="70">
      <title><fixed-case>KAT</fixed-case>: A Knowledge Augmented Transformer for Vision-and-Language</title>
      <author><first>Liangke</first><last>Gui</last></author>
      <author><first>Borui</first><last>Wang</last></author>
      <author><first>Qiuyuan</first><last>Huang</last></author>
      <author><first>Alexander</first><last>Hauptmann</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>956-968</pages>
      <abstract>The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model’s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.</abstract>
      <url hash="75b29e05">2022.naacl-main.70</url>
      <bibkey>gui-etal-2022-kat</bibkey>
      <doi>10.18653/v1/2022.naacl-main.70</doi>
      <video href="2022.naacl-main.70.mp4"/>
      <pwccode url="https://github.com/guilk/kat" additional="false">guilk/kat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
    </paper>
    <paper id="71">
      <title>When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it</title>
      <author><first>Sebastian</first><last>Schuster</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>969-982</pages>
      <abstract>Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as ‘a dog’, frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, ‘a dog’ in ‘Arthur doesn’t own a dog’ does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduce an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs. We use this evaluation suite for a fine-grained investigation of the entity tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.</abstract>
      <url hash="56725da6">2022.naacl-main.71</url>
      <bibkey>schuster-linzen-2022-sentence</bibkey>
      <doi>10.18653/v1/2022.naacl-main.71</doi>
      <video href="2022.naacl-main.71.mp4"/>
      <pwccode url="https://github.com/sebschu/discourse-entity-lm" additional="false">sebschu/discourse-entity-lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
    </paper>
    <paper id="72">
      <title>On Curriculum Learning for Commonsense Reasoning</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>983-992</pages>
      <abstract>Commonsense reasoning tasks follow a standard paradigm of finetuning pretrained language models on the target task data, where samples are introduced to the model in a random order during training. However, recent research suggests that data order can have a significant impact on the performance of finetuned models for natural language understanding. Hence, we examine the effect of a human-like easy-to-difficult curriculum during finetuning of language models for commonsense reasoning tasks. We use paced curriculum learning to rank data and sample training mini-batches with increasing levels of difficulty from the ranked dataset during finetuning. Further, we investigate the effect of an adaptive curriculum, i.e., the data ranking is dynamically updated during training based on the current state of the learner model. We use a teacher model to measure difficulty of each sample and experiment with three measures based on question answering probability, variability and out-of-distribution. To understand the effectiveness of curriculum learning in various scenarios, we apply it on full model fine-tuning as well as parameter-efficient prompt-tuning settings. Our results show that fixed as well as adaptive curriculum learning significantly improve performance for five commonsense reasoning tasks, i.e., SocialIQA, CosmosQA, CODAH, HellaSwag, WinoGrande in both tuning settings. Further, we find that prioritizing the difficult samples in the tail end of training improves generalization to unseen in-domain data as well as out-of-domain data. Our work provides evidence and encourages research into curriculum learning for commonsense reasoning.</abstract>
      <url hash="97fe11a4">2022.naacl-main.72</url>
      <attachment type="software" hash="416f8321">2022.naacl-main.72.software.zip</attachment>
      <bibkey>maharana-bansal-2022-curriculum</bibkey>
      <doi>10.18653/v1/2022.naacl-main.72</doi>
      <video href="2022.naacl-main.72.mp4"/>
      <pwccode url="https://github.com/adymaharana/curriculum_learning" additional="false">adymaharana/curriculum_learning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codah">CODAH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="73">
      <title><fixed-case>D</fixed-case>oc<fixed-case>T</fixed-case>ime: A Document-level Temporal Dependency Graph Parser</title>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <author><first>Verena</first><last>Kaynig-Fittkau</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Quan</first><last>Tran</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <pages>993-1009</pages>
      <abstract>We introduce DocTime - a novel temporal dependency graph (TDG) parser that takes as input a text document and produces a temporal dependency graph. It outperforms previous BERT-based solutions by a relative 4-8% on three datasets from modeling the problem as a graph network with path-prediction loss to incorporate longer range dependencies. This work also demonstrates how the TDG graph can be used to improve the downstream tasks of temporal questions answering and NLI by a relative 4-10% with a new framework that incorporates the temporal dependency graph into the self-attention layer of Transformer models (Time-transformer). Finally, we develop and evaluate on a new temporal dependency graph dataset for the domain of contractual documents, which has not been previously explored in this setting.</abstract>
      <url hash="6d6087f2">2022.naacl-main.73</url>
      <bibkey>mathur-etal-2022-doctime</bibkey>
      <doi>10.18653/v1/2022.naacl-main.73</doi>
      <video href="2022.naacl-main.73.mp4"/>
    </paper>
    <paper id="74">
      <title><fixed-case>F</fixed-case>act<fixed-case>PEGASUS</fixed-case>: Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization</title>
      <author><first>David</first><last>Wan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1010-1028</pages>
      <abstract>We present FactPEGASUS, an abstractive summarization model that addresses the problem of factuality during pre-training and fine-tuning: (1) We augment the sentence selection strategy of PEGASUS’s (Zhang et al., 2019) pre-training objective to create pseudo-summaries that are both important and factual; (2) We introduce three complementary components for fine-tuning. The corrector removes hallucinations present in the reference summary, the contrastor uses contrastive learning to better differentiate nonfactual summaries from factual ones, and the connector bridges the gap between the pre-training and fine-tuning for better transfer of knowledge. Experiments on three downstream tasks demonstrate that FactPEGASUS substantially improves factuality evaluated by multiple automatic metrics and humans. Our thorough analysis suggests that FactPEGASUS is more factual than using the original pre-training objective in zero-shot and few-shot settings, retains factual behavior more robustly than strong baselines, and does not rely entirely on becoming more extractive to improve factuality.</abstract>
      <url hash="0cba0104">2022.naacl-main.74</url>
      <bibkey>wan-bansal-2022-factpegasus</bibkey>
      <doi>10.18653/v1/2022.naacl-main.74</doi>
      <video href="2022.naacl-main.74.mp4"/>
      <pwccode url="https://github.com/meetdavidwan/factpegasus" additional="false">meetdavidwan/factpegasus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="75">
      <title><fixed-case>S</fixed-case>c<fixed-case>AN</fixed-case>: Suicide Attempt and Ideation Events Dataset</title>
      <author><first>Bhanu Pratap Singh</first><last>Rawat</last></author>
      <author><first>Samuel</first><last>Kovaly</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <author><first>Wilfred</first><last>Pigeon</last></author>
      <pages>1029-1040</pages>
      <abstract>Suicide is an important public health concern and one of the leading causes of death worldwide. Suicidal behaviors, including suicide attempts (SA) and suicide ideations (SI), are leading risk factors for death by suicide. Information related to patients’ previous and current SA and SI are frequently documented in the electronic health record (EHR) notes. Accurate detection of such documentation may help improve surveillance and predictions of patients’ suicidal behaviors and alert medical professionals for suicide prevention efforts. In this study, we first built Suicide Attempt and Ideation Events (ScAN) dataset, a subset of the publicly available MIMIC III dataset spanning over 12k+ EHR notes with 19k+ annotated SA and SI events information. The annotations also contain attributes such as method of suicide attempt. We also provide a strong baseline model ScANER (Suicide Attempt and Ideation Events Retriever), a multi-task RoBERTa-based model with a retrieval module to extract all the relevant suicidal behavioral evidences from EHR notes of an hospital-stay and, and a prediction module to identify the type of suicidal behavior (SA and SI) concluded during the patient’s stay at the hospital. ScANER achieved a macro-weighted F1-score of 0.83 for identifying suicidal behavioral evidences and a macro F1-score of 0.78 and 0.60 for classification of SA and SI for the patient’s hospital-stay, respectively. ScAN and ScANER are publicly available.</abstract>
      <url hash="7d835022">2022.naacl-main.75</url>
      <bibkey>rawat-etal-2022-scan</bibkey>
      <doi>10.18653/v1/2022.naacl-main.75</doi>
      <video href="2022.naacl-main.75.mp4"/>
      <pwccode url="https://github.com/bsinghpratap/scan" additional="false">bsinghpratap/scan</pwccode>
    </paper>
    <paper id="76">
      <title>Socially Aware Bias Measurements for <fixed-case>H</fixed-case>indi Language Representations</title>
      <author><first>Vijit</first><last>Malik</last></author>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Akihiro</first><last>Nishi</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1041-1052</pages>
      <abstract>Language representations are an efficient tool used across NLP, but they are strife with encoded societal biases. These biases are studied extensively, but with a primary focus on English language representations and biases common in the context of Western society. In this work, we investigate the biases present in Hindi language representations such as caste and religion associated biases. We demonstrate how biases are unique to specific language representations based on the history and culture of the region they are widely spoken in, and also how the same societal bias (such as binary gender associated biases) when investigated across languages is encoded by different words and text spans. With this work, we emphasize on the necessity of social-awareness along with linguistic and grammatical artefacts when modeling language representations, in order to understand the biases encoded.</abstract>
      <url hash="81c6fefe">2022.naacl-main.76</url>
      <attachment type="software" hash="a3ac98e1">2022.naacl-main.76.software.zip</attachment>
      <bibkey>malik-etal-2022-socially</bibkey>
      <doi>10.18653/v1/2022.naacl-main.76</doi>
      <video href="2022.naacl-main.76.mp4"/>
      <pwccode url="https://github.com/vijit-m/sochindi" additional="true">vijit-m/sochindi</pwccode>
    </paper>
    <paper id="77">
      <title><fixed-case>A</fixed-case>mbi<fixed-case>P</fixed-case>un: Generating Humorous Puns with Ambiguous Context</title>
      <author><first>Anirudh</first><last>Mittal</last></author>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>1053-1062</pages>
      <abstract>In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.</abstract>
      <url hash="b10f7706">2022.naacl-main.77</url>
      <bibkey>mittal-etal-2022-ambipun</bibkey>
      <doi>10.18653/v1/2022.naacl-main.77</doi>
      <video href="2022.naacl-main.77.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/ambipun" additional="false">pluslabnlp/ambipun</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
    </paper>
    <paper id="78">
      <title><fixed-case>E</fixed-case>mp<fixed-case>H</fixed-case>i: Generating Empathetic Responses with Human-like Intents</title>
      <author><first>Mao Yan</first><last>Chen</last></author>
      <author><first>Siheng</first><last>Li</last></author>
      <author><first>Yujiu</first><last>Yang</last></author>
      <pages>1063-1074</pages>
      <abstract>In empathetic conversations, humans express their empathy to others with empathetic intents. However, most existing empathetic conversational methods suffer from a lack of empathetic intents, which leads to monotonous empathy. To address the bias of the empathetic intents distribution between empathetic dialogue models and humans, we propose a novel model to generate empathetic responses with human-consistent empathetic intents, EmpHi for short. Precisely, EmpHi learns the distribution of potential empathetic intents with a discrete latent variable, then combines both implicit and explicit intent representation to generate responses with various empathetic intents. Experiments show that EmpHi outperforms state-of-the-art models in terms of empathy, relevance, and diversity on both automatic and human evaluation. Moreover, the case studies demonstrate the high interpretability and outstanding performance of our model.</abstract>
      <url hash="b46d81ea">2022.naacl-main.78</url>
      <bibkey>chen-etal-2022-emphi</bibkey>
      <doi>10.18653/v1/2022.naacl-main.78</doi>
      <video href="2022.naacl-main.78.mp4"/>
      <pwccode url="https://github.com/mattc95/emphi" additional="false">mattc95/emphi</pwccode>
    </paper>
    <paper id="79">
      <title>Yes, No or <fixed-case>IDK</fixed-case>: The Challenge of Unanswerable Yes/No Questions</title>
      <author><first>Elior</first><last>Sulem</last></author>
      <author><first>Jamaal</first><last>Hay</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1075-1085</pages>
      <abstract>The Yes/No QA task (Clark et al., 2019) consists of “Yes” or “No” questions about a given context. However, in realistic scenarios, the information provided in the context is not always sufficient in order to answer the question. For example, given the context “She married a lawyer from New-York.”, we don’t know whether the answer to the question “Did she marry in New York?” is “Yes” or “No”. In this paper, we extend the Yes/No QA task, adding questions with an IDK answer, and show its considerable difficulty compared to the original 2-label task. For this purpose, we (i) enrich the BoolQ dataset (Clark et al., 2019) to include unanswerable questions and (ii) create out-of-domain test sets for the Yes/No/IDK QA task. We study the contribution of training on other Natural Language Understanding tasks. We focus in particular on Extractive QA (Rajpurkar et al., 2018) and Recognizing Textual Entailments (RTE; Dagan et al., 2013), analyzing the differences between 2 and 3 labels using the new data.</abstract>
      <url hash="249a7f86">2022.naacl-main.79</url>
      <bibkey>sulem-etal-2022-yes</bibkey>
      <doi>10.18653/v1/2022.naacl-main.79</doi>
      <video href="2022.naacl-main.79.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="80">
      <title>Inducing and Using Alignments for Transition-based <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>Ramón</first><last>Astudillo</last></author>
      <pages>1086-1098</pages>
      <abstract>Transition-based parsers for Abstract Meaning Representation (AMR) rely on node-to-word alignments. These alignments are learned separately from parser training and require a complex pipeline of rule-based components, pre-processing, and post-processing to satisfy domain-specific constraints. Parsers also train on a point-estimate of the alignment pipeline, neglecting the uncertainty due to the inherent ambiguity of alignment. In this work we explore two avenues for overcoming these limitations. First, we propose a neural aligner for AMR that learns node-to-word alignments without relying on complex pipelines. We subsequently explore a tighter integration of aligner and parser training by considering a distribution over oracle action sequences arising from aligner uncertainty. Empirical results show this approach leads to more accurate alignments and generalization better from the AMR2.0 to AMR3.0 corpora. We attain a new state-of-the art for gold-only trained models, matching silver-trained performance without the need for beam search on AMR3.0.</abstract>
      <url hash="2c37b181">2022.naacl-main.80</url>
      <bibkey>drozdov-etal-2022-inducing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.80</doi>
      <video href="2022.naacl-main.80.mp4"/>
      <pwccode url="https://github.com/IBM/transition-amr-parser" additional="false">IBM/transition-amr-parser</pwccode>
    </paper>
    <paper id="81">
      <title>Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised <fixed-case>POS</fixed-case>-tagging?</title>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Shiyue</first><last>Zhang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1099-1114</pages>
      <abstract>Previous Part-Of-Speech (POS) induction models usually assume certain independence assumptions (e.g., Markov, unidirectional, local dependency) that do not hold in real languages. For example, the subject-verb agreement can be both long-term and bidirectional. To facilitate flexible dependency modeling, we propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent success of Masked Language Models (MLM). MPoSM can model arbitrary tag dependency and perform POS induction through the objective of masked POS reconstruction. We achieve competitive results on both the English Penn WSJ dataset as well as the universal treebank containing 10 diverse languages. Though modeling the long-term dependency should ideally help this task, our ablation study shows mixed trends in different languages. To better understand this phenomenon, we design a novel synthetic experiment that can specifically diagnose the model’s ability to learn tag agreement. Surprisingly, we find that even strong baselines fail to solve this problem consistently in a very simplified setting: the agreement between adjacent words. Nonetheless, MPoSM achieves overall better performance. Lastly, we conduct a detailed error analysis to shed light on other remaining challenges.</abstract>
      <url hash="f32c9498">2022.naacl-main.81</url>
      <bibkey>zhou-etal-2022-masked</bibkey>
      <doi>10.18653/v1/2022.naacl-main.81</doi>
      <video href="2022.naacl-main.81.mp4"/>
      <pwccode url="https://github.com/owenzx/mposm" additional="false">owenzx/mposm</pwccode>
    </paper>
    <paper id="82">
      <title><fixed-case>DREAM</fixed-case>: Improving Situational <fixed-case>QA</fixed-case> by First Elaborating the Situation</title>
      <author><first>Yuling</first><last>Gu</last></author>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>1115-1127</pages>
      <abstract>When people answer questions about a specific situation, e.g., “I cheated on my mid-term exam last week. Was that wrong?”, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the “scene”. To test this conjecture, we train a new model, DREAM, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (QA) model. We find that DREAM is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream QA system, including beyond that obtainable by simply further fine-tuning the QA system on DREAM’s training data. These results suggest that adding focused elaborations about a situation can improve a system’s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into QA models. Finally, our approach is dataset-neutral; we observe improved QA performance across different models, with even bigger gains on models with fewer parameters.</abstract>
      <url hash="8ced68dd">2022.naacl-main.82</url>
      <bibkey>gu-etal-2022-dream</bibkey>
      <doi>10.18653/v1/2022.naacl-main.82</doi>
      <video href="2022.naacl-main.82.mp4"/>
      <pwccode url="https://github.com/allenai/dream" additional="false">allenai/dream</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codah">CODAH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/moral-stories">Moral Stories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/story-commonsense">Story Commonsense</pwcdataset>
    </paper>
    <paper id="83">
      <title><fixed-case>C</fixed-case>o<fixed-case>S</fixed-case>e-Co: Text Conditioned Generative <fixed-case>C</fixed-case>ommon<fixed-case>S</fixed-case>ense Contextualizer</title>
      <author><first>Rachit</first><last>Bansal</last></author>
      <author><first>Milan</first><last>Aggarwal</last></author>
      <author><first>Sumit</first><last>Bhatia</last></author>
      <author><first>Jivat</first><last>Kaur</last></author>
      <author><first>Balaji</first><last>Krishnamurthy</last></author>
      <pages>1128-1143</pages>
      <abstract>Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.</abstract>
      <url hash="3dc5b86e">2022.naacl-main.83</url>
      <bibkey>bansal-etal-2022-cose</bibkey>
      <doi>10.18653/v1/2022.naacl-main.83</doi>
      <video href="2022.naacl-main.83.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
    </paper>
    <paper id="84">
      <title>Probing via Prompting</title>
      <author><first>Jiaoda</first><last>Li</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>1144-1157</pages>
      <abstract>Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model’s performance on language modeling.</abstract>
      <url hash="8f39a4da">2022.naacl-main.84</url>
      <bibkey>li-etal-2022-probing-via</bibkey>
      <doi>10.18653/v1/2022.naacl-main.84</doi>
      <video href="2022.naacl-main.84.mp4"/>
      <pwccode url="https://github.com/rycolab/probing-via-prompting" additional="false">rycolab/probing-via-prompting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="85">
      <title>Database Search Results Disambiguation for Task-Oriented Dialog Systems</title>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Satwik</first><last>Kottur</last></author>
      <author><first>Ahmad</first><last>Beirami</last></author>
      <author><first>Shahin</first><last>Shayandeh</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Alborz</first><last>Geramifard</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Chinnadhurai</first><last>Sankar</last></author>
      <pages>1158-1173</pages>
      <abstract>As task-oriented dialog systems are becoming increasingly popular in our lives, more realistic tasks have been proposed and explored. However, new practical challenges arise. For instance, current dialog systems cannot effectively handle multiplesearch results when querying a database, due to the lack of such scenarios in existing public datasets. In this paper, we propose Database Search Result (DSR) Disambiguation, a novel task that focuses on disambiguating database search results, which enhances user experience by allowing them to choose from multiple options instead of just one. To study this task, we augment the popular task-oriented dialog datasets (MultiWOZ and SGD) with turns that resolve ambiguities by (a) synthetically generating turns through a pre-defined grammar, and (b) collecting human paraphrases for a subset. We find that training on our augmented dialog data improves the model’s ability to deal with ambiguous scenarios, without sacrificing performance on unmodified turns. Furthermore, pre-fine tuning and multi-task learning help our model to improve performance on DSR-disambiguation even in the absence of in-domain data, suggesting that it can be learned as a universal dialog skill. Our data and code will be made publicly available.</abstract>
      <url hash="a177e4f5">2022.naacl-main.85</url>
      <bibkey>qian-etal-2022-database</bibkey>
      <doi>10.18653/v1/2022.naacl-main.85</doi>
      <video href="2022.naacl-main.85.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simmc">SIMMC</pwcdataset>
    </paper>
    <paper id="86">
      <title>Unsupervised Slot Schema Induction for Task-oriented Dialog</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Mingqiu</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Izhak</first><last>Shafran</last></author>
      <author><first>Laurent</first><last>Shafey</last></author>
      <author><first>Hagen</first><last>Soltau</last></author>
      <pages>1174-1193</pages>
      <abstract>Carefully-designed schemas describing how to collect and annotate dialog corpora are a prerequisite towards building task-oriented dialog systems. In practical applications, manually designing schemas can be error-prone, laborious, iterative, and slow, especially when the schema is complicated. To alleviate this expensive and time consuming process, we propose an unsupervised approach for slot schema induction from unlabeled dialog corpora. Leveraging in-domain language models and unsupervised parsing structures, our data-driven approach extracts candidate slots without constraints, followed by coarse-to-fine clustering to induce slot types. We compare our method against several strong supervised baselines, and show significant performance improvement in slot schema induction on MultiWoz and SGD datasets. We also demonstrate the effectiveness of induced schemas on downstream applications including dialog state tracking and response generation.</abstract>
      <url hash="3c5ade4b">2022.naacl-main.86</url>
      <bibkey>yu-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.naacl-main.86</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="87">
      <title>Towards a Progression-Aware Autonomous Dialogue Agent</title>
      <author><first>Abraham</first><last>Sanders</last></author>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Mei</first><last>Si</last></author>
      <author><first>Albert</first><last>Chang</last></author>
      <author><first>Deepanshu</first><last>Dey</last></author>
      <author><first>Jonas</first><last>Braasch</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <pages>1194-1212</pages>
      <abstract>Recent advances in large-scale language modeling and generation have enabled the creation of dialogue agents that exhibit human-like responses in a wide range of conversational scenarios spanning a diverse set of tasks, from general chit-chat to focused goal-oriented discourse. While these agents excel at generating high-quality responses that are relevant to prior context, they suffer from a lack of awareness of the overall direction in which the conversation is headed, and the likelihood of task success inherent therein. Thus, we propose a framework in which dialogue agents can evaluate the progression of a conversation toward or away from desired outcomes, and use this signal to inform planning for subsequent responses. Our framework is composed of three key elements: (1) the notion of a “global” dialogue state (GDS) space, (2) a task-specific progression function (PF) computed in terms of a conversation’s trajectory through this space, and (3) a planning mechanism based on dialogue rollouts by which an agent may use progression signals to select its next response.</abstract>
      <url hash="99e88881">2022.naacl-main.87</url>
      <bibkey>sanders-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.naacl-main.87</doi>
      <video href="2022.naacl-main.87.mp4"/>
    </paper>
    <paper id="88">
      <title>Cross-Domain Detection of <fixed-case>GPT</fixed-case>-2-Generated Technical Text</title>
      <author><first>Juan</first><last>Rodriguez</last></author>
      <author><first>Todd</first><last>Hay</last></author>
      <author><first>David</first><last>Gros</last></author>
      <author><first>Zain</first><last>Shamsi</last></author>
      <author><first>Ravi</first><last>Srinivasan</last></author>
      <pages>1213-1233</pages>
      <abstract>Machine-generated text presents a potential threat not only to the public sphere, but also to the scientific enterprise, whereby genuine research is undermined by convincing, synthetic text. In this paper we examine the problem of detecting GPT-2-generated technical research text. We first consider the realistic scenario where the defender does not have full information about the adversary’s text generation pipeline, but is able to label small amounts of in-domain genuine and synthetic text in order to adapt to the target distribution. Even in the extreme scenario of adapting a physics-domain detector to a biomedical detector, we find that only a few hundred labels are sufficient for good performance. Finally, we show that paragraph-level detectors can be used to detect the tampering of full-length documents under a variety of threat models.</abstract>
      <url hash="7ceb236d">2022.naacl-main.88</url>
      <bibkey>rodriguez-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.naacl-main.88</doi>
      <video href="2022.naacl-main.88.mp4"/>
      <pwccode url="https://github.com/ciads-ut/cross-domain-detection-gpt-2" additional="false">ciads-ut/cross-domain-detection-gpt-2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-scholar">Semantic Scholar</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="89">
      <title><fixed-case>DISAPERE</fixed-case>: A Dataset for Discourse Structure in Peer Review Discussions</title>
      <author><first>Neha</first><last>Kennard</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Akshay</first><last>Sharma</last></author>
      <author><first>Chhandak</first><last>Bagchi</last></author>
      <author><first>Matthew</first><last>Clinton</last></author>
      <author><first>Pranay Kumar</first><last>Yelugam</last></author>
      <author><first>Hamed</first><last>Zamani</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1234-1249</pages>
      <abstract>At the foundation of scientific evaluation is the labor-intensive process of peer review. This critical task requires participants to consume vast amounts of highly technical text. Prior work has annotated different aspects of review argumentation, but discourse relations between reviews and rebuttals have yet to be examined. We present DISAPERE, a labeled dataset of 20k sentences contained in 506 review-rebuttal pairs in English, annotated by experts. DISAPERE synthesizes label sets from prior work and extends them to include fine-grained annotation of the rebuttal sentences, characterizing their context in the review and the authors’ stance towards review arguments. Further, we annotate <i>every</i> review and rebuttal sentence. We show that discourse cues from rebuttals can shed light on the quality and interpretation of reviews. Further, an understanding of the argumentative strategies employed by the reviewers and authors provides useful signal for area chairs and other decision makers.</abstract>
      <url hash="3971e932">2022.naacl-main.89</url>
      <bibkey>kennard-etal-2022-disapere</bibkey>
      <doi>10.18653/v1/2022.naacl-main.89</doi>
      <video href="2022.naacl-main.89.mp4"/>
    </paper>
    <paper id="90">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>S</fixed-case>pan<fixed-case>QA</fixed-case>: A Dataset for Multi-Span Question Answering</title>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Martin</first><last>Tomko</last></author>
      <author><first>Maria</first><last>Vasardani</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>1250-1260</pages>
      <abstract>Most existing reading comprehension datasets focus on single-span answers, which can be extracted as a single contiguous span from a given text passage. Multi-span questions, i.e., questions whose answer is a series of multiple discontiguous spans in the text, are common real life but are less studied. In this paper, we present MultiSpanQA, a new dataset that focuses on multi-span questions. Raw questions and contexts are extracted from the Natural Questions dataset. After multi-span re-annotation, MultiSpanQA consists of over a total of 6,000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version. We introduce new metrics for the purposes of multi-span question answering evaluation, and establish several baselines using advanced models. Finally, we propose a new model which beats all baselines and achieves state-of-the-art on our dataset.</abstract>
      <url hash="e1a6fc51">2022.naacl-main.90</url>
      <bibkey>li-etal-2022-multispanqa</bibkey>
      <doi>10.18653/v1/2022.naacl-main.90</doi>
      <video href="2022.naacl-main.90.mp4"/>
      <pwccode url="https://github.com/haonan-li/MultiSpanQA" additional="false">haonan-li/MultiSpanQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/booktest">BookTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="91">
      <title>Context-Aware Abbreviation Expansion Using Large Language Models</title>
      <author><first>Shanqing</first><last>Cai</last></author>
      <author><first>Subhashini</first><last>Venugopalan</last></author>
      <author><first>Katrin</first><last>Tomanek</last></author>
      <author><first>Ajit</first><last>Narayanan</last></author>
      <author><first>Meredith</first><last>Morris</last></author>
      <author><first>Michael</first><last>Brenner</last></author>
      <pages>1261-1275</pages>
      <abstract>Motivated by the need for accelerating text entry in augmentative and alternative communication (AAC) for people with severe motor impairments, we propose a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters. Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs). Through zero-shot, few-shot, and fine-tuning experiments on four public conversation datasets, we show that for replies to the initial turn of a dialog, an LLM with 64B parameters is able to exactly expand over 70% of phrases with abbreviation length up to 10, leading to an effective keystroke saving rate of up to about 77% on these exact expansions. Including a small amount of context in the form of a single conversation turn more than doubles abbreviation expansion accuracies compared to having no context, an effect that is more pronounced for longer phrases. Additionally, the robustness of models against typo noise can be enhanced through fine-tuning on noisy data.</abstract>
      <url hash="90e8e85b">2022.naacl-main.91</url>
      <bibkey>cai-etal-2022-context</bibkey>
      <doi>10.18653/v1/2022.naacl-main.91</doi>
      <video href="2022.naacl-main.91.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="92">
      <title>Theory-Grounded Measurement of <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. Social Stereotypes in <fixed-case>E</fixed-case>nglish Language Models</title>
      <author><first>Yang Trista</first><last>Cao</last></author>
      <author><first>Anna</first><last>Sotnikova</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <author><first>Linda</first><last>Zou</last></author>
      <pages>1276-1295</pages>
      <abstract>NLP models trained on text have been shown to reproduce human stereotypes, which can magnify harms to marginalized groups when systems are deployed at scale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et al. (2016) from social psychology as a framework for the systematic study and discovery of stereotypic group-trait associations in language models (LMs). We introduce the sensitivity test (SeT) for measuring stereotypical associations from language models. To evaluate SeT and other measures using the ABC model, we collect group-trait judgments from U.S.-based subjects to compare with English LM stereotypes. Finally, we extend this framework to measure LM stereotyping of intersectional identities.</abstract>
      <url hash="f6567915">2022.naacl-main.92</url>
      <bibkey>cao-etal-2022-theory</bibkey>
      <doi>10.18653/v1/2022.naacl-main.92</doi>
      <video href="2022.naacl-main.92.mp4"/>
      <pwccode url="https://github.com/tristacao/u.s_stereotypes" additional="false">tristacao/u.s_stereotypes</pwccode>
    </paper>
    <paper id="93">
      <title>Sort by Structure: Language Model Ranking as Dependency Probing</title>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>1296-1307</pages>
      <abstract>Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM—RemBERT—and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.</abstract>
      <url hash="798ce526">2022.naacl-main.93</url>
      <bibkey>muller-eberstein-etal-2022-sort</bibkey>
      <doi>10.18653/v1/2022.naacl-main.93</doi>
      <video href="2022.naacl-main.93.mp4"/>
    </paper>
    <paper id="94">
      <title>Quantifying Synthesis and Fusion and their Impact on Machine Translation</title>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Niels</first><last>Van Berkel</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Johannes</first><last>Bjerva</last></author>
      <pages>1308-1321</pages>
      <abstract>Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017)’s approach to classify morphology using two indices: synthesis (e.g. analytic to polysynthetic) and fusion (agglutinative to fusional). For computing synthesis, we test unsupervised and supervised morphological segmentation methods for English, German and Turkish, whereas for fusion, we propose a semi-automatic method using Spanish as a case study. Then, we analyse the relationship between machine translation quality and the degree of synthesis and fusion at word (nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment level (previous language pairs plus English-German in both directions). We complement the word-level analysis with human evaluation, and overall, we observe a consistent impact of both indexes on machine translation quality.</abstract>
      <url hash="02c840f4">2022.naacl-main.94</url>
      <bibkey>oncevay-etal-2022-quantifying</bibkey>
      <doi>10.18653/v1/2022.naacl-main.94</doi>
      <video href="2022.naacl-main.94.mp4"/>
    </paper>
    <paper id="95">
      <title>Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation</title>
      <author><first>Deeksha</first><last>Varshney</last></author>
      <author><first>Akshara</first><last>Prabhakar</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>1322-1335</pages>
      <abstract>Grounding dialogue on external knowledge and interpreting linguistic patterns in dialogue history context, such as ellipsis, anaphora, and co-reference is critical for dialogue comprehension and generation. In this paper, we present a novel open-domain dialogue generation model which effectively utilizes the large-scale commonsense and named entity based knowledge in addition to the unstructured topic-specific knowledge associated with each utterance. We enhance the commonsense knowledge with named entity-aware structures using co-references. Our proposed model utilizes a multi-hop attention layer to preserve the most accurate and critical parts of the dialogue history and the associated knowledge. In addition, we employ a Commonsense and Named Entity Enhanced Attention Module, which starts with the extracted triples from various sources and gradually finds the relevant supporting set of triples using multi-hop attention with the query vector obtained from the interactive dialogue-knowledge module. Empirical results on two benchmark datasets demonstrate that our model significantly outperforms the state-of-the-art methods in terms of both automatic evaluation metrics and human judgment. Our code is publicly available at https://github.com/deekshaVarshney/CNTF; https://www.iitp.ac.in/-ai-nlp-ml/resources/codes/CNTF.zip.</abstract>
      <url hash="913ee56a">2022.naacl-main.95</url>
      <attachment type="software" hash="1c46b5a6">2022.naacl-main.95.software.zip</attachment>
      <bibkey>varshney-etal-2022-commonsense</bibkey>
      <doi>10.18653/v1/2022.naacl-main.95</doi>
      <video href="2022.naacl-main.95.mp4"/>
      <pwccode url="https://github.com/deekshavarshney/cntf" additional="false">deekshavarshney/cntf</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="96">
      <title>Efficient Hierarchical Domain Adaptation for Pretrained Language Models</title>
      <author><first>Alexandra</first><last>Chronopoulou</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <author><first>Jesse</first><last>Dodge</last></author>
      <pages>1336-1351</pages>
      <abstract>The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.</abstract>
      <url hash="9cda6040">2022.naacl-main.96</url>
      <bibkey>chronopoulou-etal-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.naacl-main.96</doi>
      <video href="2022.naacl-main.96.mp4"/>
      <pwccode url="https://github.com/alexandra-chron/hierarchical-domain-adaptation" additional="false">alexandra-chron/hierarchical-domain-adaptation</pwccode>
    </paper>
    <paper id="97">
      <title><fixed-case>H</fixed-case>atemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-Based Hate</title>
      <author><first>Hannah</first><last>Kirk</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Paul</first><last>Rottger</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Scott</first><last>Hale</last></author>
      <pages>1352-1368</pages>
      <abstract>Detecting online hate is a complex task, and low-performing models have harmful consequences when used for sensitive applications such as content moderation. Emoji-based hate is an emerging challenge for automated detection. We present HatemojiCheck, a test suite of 3,930 short-form statements that allows us to evaluate performance on hateful language expressed with emoji. Using the test suite, we expose weaknesses in existing hate detection models. To address these weaknesses, we create the HatemojiBuild dataset using a human-and-model-in-the-loop approach. Models built with these 5,912 adversarial examples perform substantially better at detecting emoji-based hate, while retaining strong performance on text-only hate. Both HatemojiCheck and HatemojiBuild are made publicly available.</abstract>
      <url hash="9484df21">2022.naacl-main.97</url>
      <bibkey>kirk-etal-2022-hatemoji</bibkey>
      <doi>10.18653/v1/2022.naacl-main.97</doi>
      <video href="2022.naacl-main.97.mp4"/>
      <pwccode url="https://github.com/HannahKirk/Hatemoji" additional="false">HannahKirk/Hatemoji</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hatemojicheck">HatemojiCheck</pwcdataset>
    </paper>
    <paper id="98">
      <title>On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data</title>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <pages>1369-1384</pages>
      <abstract>Borrowing ideas from Production functions in micro-economics, in this paper we introduce a framework to systematically evaluate the performance and cost trade-offs between machine-translated and manually-created labelled data for task-specific fine-tuning of massively multilingual language models. We illustrate the effectiveness of our framework through a case-study on the TyDIQA-GoldP dataset. One of the interesting conclusion of the study is that if the cost of machine translation is greater than zero, the optimal performance at least cost is always achieved with at least some or only manually-created data. To our knowledge, this is the first attempt towards extending the concept of production functions to study data collection strategies for training multilingual models, and can serve as a valuable tool for other similar cost vs data trade-offs in NLP.</abstract>
      <url hash="67b74b01">2022.naacl-main.98</url>
      <bibkey>ahuja-etal-2022-economics</bibkey>
      <doi>10.18653/v1/2022.naacl-main.98</doi>
      <video href="2022.naacl-main.98.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydiqa-goldp">TyDiQA-GoldP</pwcdataset>
    </paper>
    <paper id="99">
      <title>Learning to Selectively Learn for Weakly Supervised Paraphrase Generation with Model-based Reinforcement Learning</title>
      <author><first>Haiyan</first><last>Yin</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>1385-1395</pages>
      <abstract>Paraphrase generation is an important language generation task attempting to interpret user intents and systematically generate new phrases of identical meanings to the given ones. However, the effectiveness of paraphrase generation is constrained by the access to the golden labeled data pairs where both the amount and the quality of the training data pairs are restricted. In this paper, we propose a new weakly supervised paraphrase generation approach that extends the success of a recent work that leverages reinforcement learning for effective model training with data selection. While data selection is privileged for the target task which has noisy data, developing a reinforced selective learning regime faces several unresolved challenges. In this paper, we carry on important discussions about the above problem and present a new model that could partially overcome the discussed issues with a model-based planning feature and a reward normalization feature. We perform extensive evaluation on four weakly supervised paraphrase generation tasks where the results show that our method could significantly improve the state-of-the-art performance on the evaluation datasets.</abstract>
      <url hash="19c8fc8d">2022.naacl-main.99</url>
      <bibkey>yin-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.99</doi>
      <video href="2022.naacl-main.99.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="100">
      <title>Quality-Aware Decoding for Neural Machine Translation</title>
      <author><first>Patrick</first><last>Fernandes</last></author>
      <author><first>António</first><last>Farinhas</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>José G.</first><last>C. de Souza</last></author>
      <author><first>Perez</first><last>Ogayo</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Andre</first><last>Martins</last></author>
      <pages>1396-1412</pages>
      <abstract>Despite the progress in machine translation quality estimation and evaluation in the last years, decoding in neural machine translation (NMT) is mostly oblivious to this and centers around finding the most probable translation according to the model (MAP decoding), approximated with beam search. In this paper, we bring together these two lines of research and propose <i>quality-aware decoding</i> for NMT, by leveraging recent breakthroughs in reference-free and reference-based MT evaluation through various inference methods like <tex-math>N</tex-math>-best reranking and minimum Bayes risk decoding. We perform an extensive comparison of various possible candidate generation and ranking methods across four datasets and two model classes and find that quality-aware decoding consistently outperforms MAP-based decoding according both to state-of-the-art automatic metrics (COMET and BLEURT) and to human assessments. </abstract>
      <url hash="08b1237d">2022.naacl-main.100</url>
      <bibkey>fernandes-etal-2022-quality</bibkey>
      <doi>10.18653/v1/2022.naacl-main.100</doi>
      <video href="2022.naacl-main.100.mp4"/>
      <pwccode url="https://github.com/deep-spin/qaware-decode" additional="false">deep-spin/qaware-decode</pwccode>
    </paper>
    <paper id="101">
      <title>Pretrained Models for Multilingual Federated Learning</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Marc</first><last>Marone</last></author>
      <author><first>Vladimir</first><last>Braverman</last></author>
      <author><first>Dawn</first><last>Lawrie</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>1413-1421</pages>
      <abstract>Since the advent of Federated Learning (FL), research has applied these methods to natural language processing (NLP) tasks. Despite a plethora of papers in FL for NLP, no previous works have studied how multilingual text impacts FL algorithms. Furthermore, multilingual text provides an interesting avenue to examine the impact of non-IID text (e.g. different languages) on FL in naturally occurring data. We explore three multilingual language tasks, language modeling, machine translation, and text classification using differing federated and non-federated learning algorithms. Our results show that using pretrained models reduces the negative effects of FL, helping them to perform near or better than centralized (no privacy) learning, even when using non-IID partitioning.</abstract>
      <url hash="cdb7dcf3">2022.naacl-main.101</url>
      <bibkey>weller-etal-2022-pretrained</bibkey>
      <doi>10.18653/v1/2022.naacl-main.101</doi>
      <video href="2022.naacl-main.101.mp4"/>
      <pwccode url="https://github.com/orionw/multilingual-federated-learning" additional="false">orionw/multilingual-federated-learning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="102">
      <title><fixed-case>A</fixed-case>c<fixed-case>T</fixed-case>une: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models</title>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Lingkai</first><last>Kong</last></author>
      <author><first>Jieyu</first><last>Zhang</last></author>
      <author><first>Rongzhi</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>1422-1436</pages>
      <abstract>Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model’s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average. Our implementation is available at <url>https://github.com/yueyu1030/actune</url>.</abstract>
      <url hash="49cd916b">2022.naacl-main.102</url>
      <bibkey>yu-etal-2022-actune</bibkey>
      <doi>10.18653/v1/2022.naacl-main.102</doi>
      <video href="2022.naacl-main.102.mp4"/>
      <pwccode url="https://github.com/yueyu1030/actune" additional="false">yueyu1030/actune</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed-rct">PubMed RCT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wrench">Wrench</pwcdataset>
    </paper>
    <paper id="103">
      <title>Label Anchored Contrastive Learning for Language Understanding</title>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Yuming</first><last>Zhao</last></author>
      <author><first>Meng</first><last>Chen</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>1437-1449</pages>
      <abstract>Contrastive learning (CL) has achieved astonishing progress in computer vision, speech, and natural language processing fields recently with self-supervised learning. However, CL approach to the supervised setting is not fully explored, especially for the natural language understanding classification task. Intuitively, the class label itself has the intrinsic ability to perform hard positive/negative mining, which is crucial for CL. Motivated by this, we propose a novel label anchored contrastive learning approach (denoted as LaCon) for language understanding. Specifically, three contrastive objectives are devised, including a multi-head instance-centered contrastive loss (ICL), a label-centered contrastive loss (LCL), and a label embedding regularizer (LER). Our approach does not require any specialized network architecture or any extra data augmentation, thus it can be easily plugged into existing powerful pre-trained language models. Compared to the state-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular datasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates significant advantages under the few-shot and data imbalance settings, which obtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.</abstract>
      <url hash="26d09960">2022.naacl-main.103</url>
      <bibkey>zhang-etal-2022-label</bibkey>
      <doi>10.18653/v1/2022.naacl-main.103</doi>
      <video href="2022.naacl-main.103.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewclue">FewCLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewglue">FewGlue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="104">
      <title>Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>1450-1470</pages>
      <abstract>Stories or narratives are comprised of a sequence of events. To compose interesting stories, professional writers often leverage a creative writing technique called *flashback* that inserts past events into current storylines as we commonly observe in novels and plays. However, it is challenging for machines to generate *flashback* as it requires a solid understanding of event **temporal order** (e.g. *feeling hungry* before *eat*, not vice versa), and the creativity to arrange storylines so that earlier events do not always appear first in **narrative order**. Two major issues in existing systems that exacerbate the challenges: 1) temporal bias in pertaining and story datasets that leads to monotonic event temporal orders; 2) lack of explicit guidance that helps machines decide where to insert *flashbacks*. We propose to address these issues using structured storylines to encode events and their pair-wise temporal relations (before, after and vague) as **temporal prompts** that guide how stories should unfold temporally. We leverage a Plan-and-Write framework enhanced by reinforcement learning to generate storylines and stories end-to-end. Evaluation results show that the proposed method can generate more interesting stories with *flashbacks* while maintaining textual diversity, fluency, and temporal coherence.</abstract>
      <url hash="f1f852f3">2022.naacl-main.104</url>
      <attachment type="software" hash="f03ad003">2022.naacl-main.104.software.zip</attachment>
      <bibkey>han-etal-2022-go</bibkey>
      <doi>10.18653/v1/2022.naacl-main.104</doi>
      <video href="2022.naacl-main.104.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/flashback_gen" additional="false">pluslabnlp/flashback_gen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="105">
      <title>Forecasting <fixed-case>COVID</fixed-case>-19 Caseloads Using Unsupervised Embedding Clusters of Social Media Posts</title>
      <author><first>Felix</first><last>Drinkall</last></author>
      <author><first>Stefan</first><last>Zohren</last></author>
      <author><first>Janet</first><last>Pierrehumbert</last></author>
      <pages>1471-1484</pages>
      <abstract>We present a novel approach incorporating transformer-based language models into infectious disease modelling. Text-derived features are quantified by tracking high-density clusters of sentence-level representations of Reddit posts within specific US states’ COVID-19 subreddits. We benchmark these clustered embedding features against features extracted from other high-quality datasets. In a threshold-classification task, we show that they outperform all other feature types at predicting upward trend signals, a significant result for infectious disease modelling in areas where epidemiological data is unreliable. Subsequently, in a time-series forecasting task, we fully utilise the predictive power of the caseload and compare the relative strengths of using different supplementary datasets as covariate feature sets in a transformer-based time-series model.</abstract>
      <url hash="82ccf3e0">2022.naacl-main.105</url>
      <bibkey>drinkall-etal-2022-forecasting</bibkey>
      <doi>10.18653/v1/2022.naacl-main.105</doi>
      <video href="2022.naacl-main.105.mp4"/>
    </paper>
    <paper id="106">
      <title>Many Hands Make Light Work: Using Essay Traits to Automatically Score Essays</title>
      <author><first>Rahul</first><last>Kumar</last></author>
      <author><first>Sandeep</first><last>Mathias</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1485-1495</pages>
      <abstract>Most research in the area of automatic essay grading (AEG) is geared towards scoring the essay <i>holistically</i> while there has also been little work done on scoring individual essay traits. In this paper, we describe a way to score essays using a multi-task learning (MTL) approach, where scoring the essay holistically is the primary task, and scoring the essay traits is the auxiliary task. We compare our results with a single-task learning (STL) approach, using both LSTMs and BiLSTMs. To find out which traits work best for different types of essays, we conduct ablation tests for each of the essay traits. We also report the runtime and number of training parameters for each system. We find that MTL-based BiLSTM system gives the best results for scoring the essay holistically, as well as performing well on scoring the essay traits. The MTL systems also give a speed-up of between <b>2.30</b> to <b>3.70</b> times the speed of the STL system, when it comes to scoring the essay and all the traits.</abstract>
      <url hash="10a6d923">2022.naacl-main.106</url>
      <attachment type="software" hash="3b10db76">2022.naacl-main.106.software.zip</attachment>
      <bibkey>kumar-etal-2022-many</bibkey>
      <doi>10.18653/v1/2022.naacl-main.106</doi>
      <video href="2022.naacl-main.106.mp4"/>
      <pwccode url="https://github.com/ASAP-AEG/MTL-Essay-Traits-Scoring" additional="false">ASAP-AEG/MTL-Essay-Traits-Scoring</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asap">ASAP</pwcdataset>
    </paper>
    <paper id="107">
      <title>Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims</title>
      <author><first>Miguel</first><last>Arana-Catania</last></author>
      <author><first>Elena</first><last>Kochkina</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <author><first>Robert</first><last>Procter</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>1496-1511</pages>
      <abstract>We present a comprehensive work on automated veracity assessment from dataset creation to developing novel methods based on Natural Language Inference (NLI), focusing on misinformation related to the COVID-19 pandemic. We first describe the construction of the novel PANACEA dataset consisting of heterogeneous claims on COVID-19 and their respective information sources. The dataset construction includes work on retrieval techniques and similarity measurements to ensure a unique set of claims. We then propose novel techniques for automated veracity assessment based on Natural Language Inference including graph convolutional networks and attention based approaches. We have carried out experiments on evidence retrieval and veracity assessment on the dataset using the proposed techniques and found them competitive with SOTA methods, and provided a detailed discussion.</abstract>
      <url hash="8b84cf88">2022.naacl-main.107</url>
      <bibkey>arana-catania-etal-2022-natural</bibkey>
      <doi>10.18653/v1/2022.naacl-main.107</doi>
      <video href="2022.naacl-main.107.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coaid">CoAID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mm-covid">MM-COVID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="108">
      <title>Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding</title>
      <author><first>Ao</first><last>Jia</last></author>
      <author><first>Yu</first><last>He</last></author>
      <author><first>Yazhou</first><last>Zhang</last></author>
      <author><first>Sagar</first><last>Uprety</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <author><first>Christina</first><last>Lioma</last></author>
      <pages>1512-1522</pages>
      <abstract>Desire is a strong wish to do or have something, which involves not only a linguistic expression, but also underlying cognitive phenomena driving human feelings. As the most primitive and basic human instinct, conscious desire is often accompanied by a range of emotional responses. As a strikingly understudied task, it is difficult for machines to model and understand desire due to the unavailability of benchmarking datasets with desire and emotion labels. To bridge this gap, we present MSED, the first multi-modal and multi-task sentiment, emotion and desire dataset, which contains 9,190 text-image pairs, with English text. Each multi-modal sample is annotated with six desires, three sentiments and six emotions. We also propose the state-of-the-art baselines to evaluate the potential of MSED and show the importance of multi-task and multi-modal clues for desire understanding. We hope this study provides a benchmark for human desire analysis. MSED will be publicly available for research.</abstract>
      <url hash="f571fdc0">2022.naacl-main.108</url>
      <bibkey>jia-etal-2022-beyond</bibkey>
      <doi>10.18653/v1/2022.naacl-main.108</doi>
      <video href="2022.naacl-main.108.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="109">
      <title>Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction</title>
      <author><first>Jiaxin</first><last>Yu</last></author>
      <author><first>Deqing</first><last>Yang</last></author>
      <author><first>Shuyu</first><last>Tian</last></author>
      <pages>1523-1529</pages>
      <abstract>Compared with traditional sentence-level relation extraction, document-level relation extraction is a more challenging task where an entity in a document may be mentioned multiple times and associated with multiple relations. However, most methods of document-level relation extraction do not distinguish between mention-level features and entity-level features, and just apply simple pooling operation for aggregating mention-level features into entity-level features. As a result, the distinct semantics between the different mentions of an entity are overlooked. To address this problem, we propose RSMAN in this paper which performs selective attentions over different entity mentions with respect to candidate relations. In this manner, the flexible and relation-specific representations of entities are obtained which indeed benefit relation classification. Our extensive experiments upon two benchmark datasets show that our RSMAN can bring significant improvements for some backbone models to achieve state-of-the-art performance, especially when an entity have multiple mentions in the document.</abstract>
      <url hash="e56d5211">2022.naacl-main.109</url>
      <bibkey>yu-etal-2022-relation</bibkey>
      <doi>10.18653/v1/2022.naacl-main.109</doi>
      <video href="2022.naacl-main.109.mp4"/>
      <pwccode url="https://github.com/fduyjx/rsman" additional="false">fduyjx/rsman</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dwie">DWIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="110">
      <title><fixed-case>T</fixed-case>witter-<fixed-case>COMM</fixed-case>s: Detecting Climate, <fixed-case>COVID</fixed-case>, and Military Multimodal Misinformation</title>
      <author><first>Giscard</first><last>Biamby</last></author>
      <author><first>Grace</first><last>Luo</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <author><first>Anna</first><last>Rohrbach</last></author>
      <pages>1530-1549</pages>
      <abstract>Detecting out-of-context media, such as “miscaptioned” images on Twitter, is a relevant problem, especially in domains of high public significance. In this work we aim to develop defenses against such misinformation for the topics of Climate Change, COVID-19, and Military Vehicles. We first present a large-scale multimodal dataset with over 884k tweets relevant to these topics. Next, we propose a detection method, based on the state-of-the-art CLIP model, that leverages automatically generated hard image-text mismatches. While this approach works well on our automatically constructed out-of-context tweets, we aim to validate its usefulness on data representative of the real world. Thus, we test it on a set of human-generated fakes, created by mimicking in-the-wild misinformation. We achieve an 11% detection improvement in a high precision regime over a strong baseline. Finally, we share insights about our best model design and analyze the challenges of this emerging threat.</abstract>
      <url hash="528b0797">2022.naacl-main.110</url>
      <bibkey>biamby-etal-2022-twitter</bibkey>
      <doi>10.18653/v1/2022.naacl-main.110</doi>
      <video href="2022.naacl-main.110.mp4"/>
      <pwccode url="https://github.com/giscardbiamby/twitter-comms" additional="false">giscardbiamby/twitter-comms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/twitter-comms">Twitter-COMMs</pwcdataset>
    </paper>
    <paper id="111">
      <title><fixed-case>BlonDe</fixed-case>: An Automatic Evaluation Metric for Document-level Machine Translation</title>
      <author><first>Yuchen</first><last>Jiang</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Haoyang</first><last>Huang</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>1550-1565</pages>
      <abstract>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level, and is more sensitive to document-level nuances. In a large-scale human study, BlonDe also achieves significantly higher Pearson’s r correlation with human judgments compared to previous metrics.</abstract>
      <url hash="f3cc51da">2022.naacl-main.111</url>
      <bibkey>jiang-etal-2022-blonde</bibkey>
      <doi>10.18653/v1/2022.naacl-main.111</doi>
      <video href="2022.naacl-main.111.mp4"/>
      <pwccode url="https://github.com/eleanorjiang/blonde" additional="false">eleanorjiang/blonde</pwccode>
    </paper>
    <paper id="112">
      <title>Disentangled Learning of Stance and Aspect Topics for Vaccine Attitude Detection in Social Media</title>
      <author><first>Lixing</first><last>Zhu</last></author>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Robert</first><last>Procter</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>1566-1580</pages>
      <abstract>Building models to detect vaccine attitudes on social media is challenging because of the composite, often intricate aspects involved, and the limited availability of annotated data. Existing approaches have relied heavily on supervised training that requires abundant annotations and pre-defined aspect categories. Instead, with the aim of leveraging the large amount of unannotated data now available on vaccination, we propose a novel semi-supervised approach for vaccine attitude detection, called VADet. A variational autoencoding architecture based on language models is employed to learn from unlabelled data the topical information of the domain. Then, the model is fine-tuned with a few manually annotated examples of user attitudes. We validate the effectiveness of VADet on our annotated data and also on an existing vaccination corpus annotated with opinions on vaccines. Our results show that VADet is able to learn disentangled stance and aspect topics, and outperforms existing aspect-based sentiment analysis models on both stance detection and tweet clustering.</abstract>
      <url hash="eedddad4">2022.naacl-main.112</url>
      <bibkey>zhu-etal-2022-disentangled</bibkey>
      <doi>10.18653/v1/2022.naacl-main.112</doi>
      <video href="2022.naacl-main.112.mp4"/>
      <pwccode url="https://github.com/somethingx1202/vadet" additional="false">somethingx1202/vadet</pwccode>
    </paper>
    <paper id="113">
      <title><fixed-case>SKILL</fixed-case>: Structured Knowledge Infusion for Large Language Models</title>
      <author><first>Fedor</first><last>Moiseev</last></author>
      <author><first>Zhe</first><last>Dong</last></author>
      <author><first>Enrique</first><last>Alfonseca</last></author>
      <author><first>Martin</first><last>Jaggi</last></author>
      <pages>1581-1588</pages>
      <abstract>Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.</abstract>
      <url hash="bc287d1f">2022.naacl-main.113</url>
      <bibkey>moiseev-etal-2022-skill</bibkey>
      <doi>10.18653/v1/2022.naacl-main.113</doi>
      <video href="2022.naacl-main.113.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kelm">KELM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/metaqa">MetaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimovies">WikiMovies</pwcdataset>
    </paper>
    <paper id="114">
      <title>Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models</title>
      <author><first>Karolina</first><last>Stanczak</last></author>
      <author><first>Edoardo</first><last>Ponti</last></author>
      <author><first>Lucas</first><last>Torroba Hennigen</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>1589-1598</pages>
      <abstract>The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pre-trained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages. We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.</abstract>
      <url hash="5a4e85c1">2022.naacl-main.114</url>
      <bibkey>stanczak-etal-2022-neurons</bibkey>
      <doi>10.18653/v1/2022.naacl-main.114</doi>
      <video href="2022.naacl-main.114.mp4"/>
      <pwccode url="https://github.com/copenlu/multilingual-typology-probing" additional="false">copenlu/multilingual-typology-probing</pwccode>
    </paper>
    <paper id="115">
      <title>Aspect Is Not You Need: No-aspect Differential Sentiment Framework for Aspect-based Sentiment Analysis</title>
      <author><first>Jiahao</first><last>Cao</last></author>
      <author><first>Rui</first><last>Liu</last></author>
      <author><first>Huailiang</first><last>Peng</last></author>
      <author><first>Lei</first><last>Jiang</last></author>
      <author><first>Xu</first><last>Bai</last></author>
      <pages>1599-1609</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Most recent efforts adopt pre-trained model to classify the sentences with aspects. However, the aspect sentiment bias from pre-trained model brings some noise to the ABSA task. Besides, traditional methods using cross-entropy loss are hard to find the potential associations between sentiment polarities. In this work, we analyze the ABSA task from a novel cognition perspective: humans can often judge the sentiment of an aspect even if they do not know what the aspect is. Moreover, it is easier to distinguish positive and negative sentiments than others for human beings because positive and negative are two opposite sentiments. To this end, we propose a no-aspect differential sentiment (NADS) framework for the ABSA task. We first design a no-aspect template by replacing the aspect with a special unbiased character to eliminate the sentiment bias and obtain a stronger representation. To better get the benefits from the template, we adopt contrastive learning between the no-aspect template and the original sentence. Then we propose a differential sentiment loss instead of the cross-entropy loss to better classify the sentiments by distinguishing the different distances between sentiments. Our proposed model is a general framework and can be combined with almost all traditional ABSA methods. Experiments on SemEval 2014 show that our framework is still able to predict the sentiment of the aspect even we don’t konw what the aspect is. Moreover, our NADS framework boosts three typical ABSA methods and achieves state-of-the-art performance.</abstract>
      <url hash="f8baaf83">2022.naacl-main.115</url>
      <bibkey>cao-etal-2022-aspect</bibkey>
      <doi>10.18653/v1/2022.naacl-main.115</doi>
      <video href="2022.naacl-main.115.mp4"/>
    </paper>
    <paper id="116">
      <title><fixed-case>M</fixed-case>o<fixed-case>EBERT</fixed-case>: from <fixed-case>BERT</fixed-case> to Mixture-of-Experts via Importance-Guided Adaptation</title>
      <author><first>Simiao</first><last>Zuo</last></author>
      <author><first>Qingru</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Liang</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>1610-1623</pages>
      <abstract>Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications. Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We propose MoEBERT, which uses a Mixture-of-Experts structure to increase model capacity and inference speed. We initialize MoEBERT by adapting the feed-forward neural networks in a pre-trained model into multiple experts. As such, representation power of the pre-trained model is largely retained. During inference, only one of the experts is activated, such that speed can be improved. We also propose a layer-wise distillation method to train MoEBERT. We validate the efficiency and efficacy of MoEBERT on natural language understanding and question answering tasks. Results show that the proposed method outperforms existing task-specific distillation algorithms. For example, our method outperforms previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is publicly available at https://github.com/SimiaoZuo/MoEBERT.</abstract>
      <url hash="5d0ec1f0">2022.naacl-main.116</url>
      <bibkey>zuo-etal-2022-moebert</bibkey>
      <doi>10.18653/v1/2022.naacl-main.116</doi>
      <video href="2022.naacl-main.116.mp4"/>
      <pwccode url="https://github.com/simiaozuo/moebert" additional="false">simiaozuo/moebert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="117">
      <title>Implicit n-grams Induced by Recurrence</title>
      <author><first>Xiaobing</first><last>Sun</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>1624-1639</pages>
      <abstract>Although self-attention based models such as Transformers have achieved remarkable successes on natural language processing (NLP)tasks, recent studies reveal that they have limitations on modeling sequential transformations (Hahn, 2020), which may promptre-examinations of recurrent neural networks (RNNs) that demonstrated impressive results on handling sequential data. Despite manyprior attempts to interpret RNNs, their internal mechanisms have not been fully understood, and the question on how exactly they capturesequential features remains largely unclear. In this work, we present a study that shows there actually exist some explainable componentsthat reside within the hidden states, which are reminiscent of the classical n-grams features. We evaluated such extracted explainable features from trained RNNs on downstream sentiment analysis tasks and found they could be used to model interesting linguistic phenomena such as negation and intensification. Furthermore, we examined the efficacy of using such n-gram components alone as encoders on tasks such as sentiment analysis and language modeling, revealing they could be playing important roles in contributing to the overall performance of RNNs. We hope our findings could add interpretability to RNN architectures, and also provide inspirations for proposing new architectures for sequential data.</abstract>
      <url hash="b2ec504b">2022.naacl-main.117</url>
      <bibkey>sun-lu-2022-implicit</bibkey>
      <doi>10.18653/v1/2022.naacl-main.117</doi>
      <video href="2022.naacl-main.117.mp4"/>
      <pwccode url="https://github.com/richardsun-voyager/inibr" additional="false">richardsun-voyager/inibr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="118">
      <title>Guiding Visual Question Generation</title>
      <author><first>Nihir</first><last>Vedd</last></author>
      <author><first>Zixu</first><last>Wang</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Yishu</first><last>Miao</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>1640-1654</pages>
      <abstract>In traditional Visual Question Generation (VQG), most images have multiple concepts (e.g. objects and categories) for which a question could be generated, but models are trained to mimic an arbitrary choice of concept as given in their training data. This makes training difficult and also poses issues for evaluation – multiple valid questions exist for most images but only one or a few are captured by the human references. We present Guiding Visual Question Generation - a variant of VQG which conditions the question generator on categorical information based on expectations on the type of question and the objects it should explore. We propose two variant families: (i) an explicitly guided model that enables an actor (human or automated) to select which objects and categories to generate a question for; and (ii) 2 types of implicitly guided models that learn which objects and categories to condition on, based on discrete variables. The proposed models are evaluated on an answer-category augmented VQA dataset and our quantitative results show a substantial improvement over the current state of the art (over 9 BLEU-4 increase). Human evaluation validates that guidance helps the generation of questions that are grammatically coherent and relevant to the given image and objects.</abstract>
      <url hash="2b4d5a17">2022.naacl-main.118</url>
      <bibkey>vedd-etal-2022-guiding</bibkey>
      <doi>10.18653/v1/2022.naacl-main.118</doi>
      <video href="2022.naacl-main.118.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="119">
      <title><fixed-case>OPERA</fixed-case>: Operation-Pivoted Discrete Reasoning over Text</title>
      <author><first>Yongwei</first><last>Zhou</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Chaoqun</first><last>Duan</last></author>
      <author><first>Haipeng</first><last>Sun</last></author>
      <author><first>Jiahui</first><last>Liang</last></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Jing</first><last>Zhao</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>1655-1666</pages>
      <abstract>Machine reading comprehension (MRC) that requires discrete reasoning involving symbolic operations, e.g., addition, sorting, and counting, is a challenging task. According to this nature, semantic parsing-based methods predict interpretable but complex logical forms. However, logical form generation is nontrivial and even a little perturbation in a logical form will lead to wrong answers. To alleviate this issue, multi-predictor -based methods are proposed to directly predict different types of answers and achieve improvements. However, they ignore the utilization of symbolic operations and encounter a lack of reasoning ability and interpretability. To inherit the advantages of these two types of methods, we propose OPERA, an operation-pivoted discrete reasoning framework, where lightweight symbolic operations (compared with logical forms) as neural modules are utilized to facilitate the reasoning ability and interpretability. Specifically, operations are first selected and then softly executed to simulate the answer reasoning procedure. Extensive experiments on both DROP and RACENum datasets show the reasoning ability of OPERA. Moreover, further analysis verifies its interpretability.</abstract>
      <url hash="d21c4d72">2022.naacl-main.119</url>
      <bibkey>zhou-etal-2022-opera</bibkey>
      <doi>10.18653/v1/2022.naacl-main.119</doi>
      <video href="2022.naacl-main.119.mp4"/>
      <pwccode url="https://github.com/jd-ai-research-nlp/opera" additional="false">jd-ai-research-nlp/opera</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="120">
      <title>Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness</title>
      <author><first>Yun-Zhu</first><last>Song</last></author>
      <author><first>Yi-Syuan</first><last>Chen</last></author>
      <author><first>Hong-Han</first><last>Shuai</last></author>
      <pages>1667-1681</pages>
      <abstract>A notable challenge in Multi-Document Summarization (MDS) is the extremely-long length of the input. In this paper, we present an extract-then-abstract Transformer framework to overcome the problem. Specifically, we leverage pre-trained language models to construct a hierarchical extractor for salient sentence selection across documents and an abstractor for rewriting the selected contents as summaries. However, learning such a framework is challenging since the optimal contents for the abstractor are generally unknown. Previous works typically create <i>pseudo extraction oracle</i> to enable the supervised learning for both the extractor and the abstractor. Nevertheless, we argue that the performance of such methods could be restricted due to the insufficient information for prediction and inconsistent objectives between training and testing. To this end, we propose a loss weighting mechanism that makes the model aware of the unequal importance for the sentences not in the pseudo extraction oracle, and leverage the fine-tuned abstractor to generate summary references as auxiliary signals for learning the extractor. Moreover, we propose a reinforcement learning method that can efficiently apply to the extractor for harmonizing the optimization between training and testing. Experiment results show that our framework substantially outperforms strong baselines with comparable model sizes and achieves the best results on the Multi-News, Multi-XScience, and WikiCatSum corpora.</abstract>
      <url hash="6ec2bffa">2022.naacl-main.120</url>
      <bibkey>song-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.120</doi>
      <video href="2022.naacl-main.120.mp4"/>
      <pwccode url="https://github.com/yunzhusong/NAACL2022-REFLECT" additional="false">yunzhusong/NAACL2022-REFLECT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-xscience">Multi-XScience</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicatsum">WikiCatSum</pwcdataset>
    </paper>
    <paper id="121">
      <title>Improving Constituent Representation with Hypertree Neural Networks</title>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Gongshen</first><last>Liu</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1682-1692</pages>
      <abstract>Many natural language processing tasks involve text spans and thus high-quality span representations are needed to enhance neural approaches to these tasks. Most existing methods of span representation are based on simple derivations (such as max-pooling) from word representations and do not utilize compositional structures of natural language. In this paper, we aim to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with constituency parse trees. Each node in the HTNN represents a constituent of the input sentence and each hyperedge represents a composition of smaller child constituents into a larger parent constituent. In each update iteration of the HTNN, the representation of each constituent is computed based on all the hyperedges connected to it, thus incorporating both bottom-up and top-down compositional information. We conduct comprehensive experiments to evaluate HTNNs against other span representation models and the results show the effectiveness of HTNN.</abstract>
      <url hash="67fc824b">2022.naacl-main.121</url>
      <bibkey>zhou-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.121</doi>
      <video href="2022.naacl-main.121.mp4"/>
    </paper>
    <paper id="122">
      <title>Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models</title>
      <author><first>Pieter</first><last>Delobelle</last></author>
      <author><first>Ewoenam</first><last>Tokpo</last></author>
      <author><first>Toon</first><last>Calders</last></author>
      <author><first>Bettina</first><last>Berendt</last></author>
      <pages>1693-1706</pages>
      <abstract>An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify ‘bias’ and ‘fairness’ in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.</abstract>
      <url hash="e702557a">2022.naacl-main.122</url>
      <attachment type="software" hash="c266c9ba">2022.naacl-main.122.software.zip</attachment>
      <bibkey>delobelle-etal-2022-measuring</bibkey>
      <doi>10.18653/v1/2022.naacl-main.122</doi>
      <video href="2022.naacl-main.122.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/crows-pairs">CrowS-Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/stereoset">StereoSet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="123">
      <title><fixed-case>M</fixed-case>u<fixed-case>CPAD</fixed-case>: A Multi-Domain <fixed-case>C</fixed-case>hinese Predicate-Argument Dataset</title>
      <author><first>Yahui</first><last>Liu</last></author>
      <author><first>Haoping</first><last>Yang</last></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Qingrong</first><last>Xia</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>1707-1717</pages>
      <abstract>During the past decade, neural network models have made tremendous progress on in-domain semantic role labeling (SRL). However, performance drops dramatically under the out-of-domain setting. In order to facilitate research on cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese predicate-argument dataset, which consists of 30,897 sentences and 92,051 predicates from six different domains. MuCPAD exhibits three important features. 1) Based on a frame-free annotation methodology, we avoid writing complex frames for new predicates. 2) We explicitly annotate omitted core arguments to recover more complete semantic structure, considering that omission of content words is ubiquitous in multi-domain Chinese texts. 3) We compile 53 pages of annotation guidelines and adopt strict double annotation for improving data quality. This paper describes in detail the annotation methodology and annotation process of MuCPAD, and presents in-depth data analysis. We also give benchmark results on cross-domain SRL based on MuCPAD.</abstract>
      <url hash="aa5afceb">2022.naacl-main.123</url>
      <bibkey>liu-etal-2022-mucpad</bibkey>
      <doi>10.18653/v1/2022.naacl-main.123</doi>
      <video href="2022.naacl-main.123.mp4"/>
      <pwccode url="https://github.com/suda-la/mucpad" additional="false">suda-la/mucpad</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="124">
      <title>Representation Learning for Conversational Data using Discourse Mutual Information Maximization</title>
      <author><first>Bishal</first><last>Santra</last></author>
      <author><first>Sumegh</first><last>Roychowdhury</last></author>
      <author><first>Aishik</first><last>Mandal</last></author>
      <author><first>Vasu</first><last>Gurram</last></author>
      <author><first>Atharva</first><last>Naik</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>1718-1734</pages>
      <abstract>Although many pretrained models exist for text or images, there have been relatively fewer attempts to train representations specifically for dialog understanding. Prior works usually relied on finetuned representations based on generic text representation models like BERT or GPT-2. But such language modeling pretraining objectives do not take the structural information of conversational text into consideration. Although generative dialog models can learn structural features too, we argue that the structure-unaware word-by-word generation is not suitable for effective conversation modeling. We empirically demonstrate that such representations do not perform consistently across various dialog understanding tasks. Hence, we propose a structure-aware Mutual Information based loss-function DMI (Discourse Mutual Information) for training dialog-representation models, that additionally captures the inherent uncertainty in response prediction. Extensive evaluation on nine diverse dialog modeling tasks shows that our proposed DMI-based models outperform strong baselines by significant margins.</abstract>
      <url hash="210cfedd">2022.naacl-main.124</url>
      <bibkey>santra-etal-2022-representation</bibkey>
      <doi>10.18653/v1/2022.naacl-main.124</doi>
      <video href="2022.naacl-main.124.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog-1">DailyDialog++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mutual">MuTual</pwcdataset>
    </paper>
    <paper id="125">
      <title><fixed-case>V</fixed-case>al<fixed-case>CAT</fixed-case>: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model</title>
      <author><first>Chuyun</first><last>Deng</last></author>
      <author><first>Mingxuan</first><last>Liu</last></author>
      <author><first>Yue</first><last>Qin</last></author>
      <author><first>Jia</first><last>Zhang</last></author>
      <author><first>Hai-Xin</first><last>Duan</last></author>
      <author><first>Donghong</first><last>Sun</last></author>
      <pages>1735-1746</pages>
      <abstract>Adversarial texts help explore vulnerabilities in language models, improve model robustness, and explain their working mechanisms. However, existing word-level attack methods trap in a one-to-one attack pattern, i.e., only a single word can be modified in one transformation round, and they ignore the interactions between several consecutive words. In this paper, we propose ValCAT, a black-box attack framework that misleads the language model by applying variable-length contextualized transformations to the original text. Compared to word-level methods, ValCAT expands the basic units of perturbation from single words to spans composed of multiple consecutive words, enhancing the perturbation capability. Experiments show that our method outperforms state-of-the-art methods in terms of attack success rate, perplexity, and semantic similarity on several classification tasks and inference tasks. The comprehensive human evaluation demonstrates that ValCAT has a significant advantage in ensuring the fluency of the adversarial examples and achieves better semantic consistency. We release the code at https://github.com/linerxliner/ValCAT.</abstract>
      <url hash="5fdf850d">2022.naacl-main.125</url>
      <bibkey>deng-etal-2022-valcat</bibkey>
      <doi>10.18653/v1/2022.naacl-main.125</doi>
      <video href="2022.naacl-main.125.mp4"/>
      <pwccode url="https://github.com/linerxliner/valcat" additional="false">linerxliner/valcat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="126">
      <title>A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</title>
      <author><first>Kexun</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Junliang</first><last>Guo</last></author>
      <author><first>Yi</first><last>Ren</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>1747-1757</pages>
      <abstract>It is difficult for non-autoregressive translation (NAT) models to capture the multi-modal distribution of target translations due to their conditional independence assumption, which is known as the “multi-modality problem”, including the lexical multi-modality and the syntactic multi-modality. While the first one has been well studied, the syntactic multi-modality brings severe challenges to the standard cross entropy (XE) loss in NAT and is understudied. In this paper, we conduct a systematic study on the syntactic multi-modality problem. Specifically, we decompose it into short- and long-range syntactic multi-modalities and evaluate several recent NAT algorithms with advanced loss functions on both carefully designed synthesized datasets and real datasets. We find that the Connectionist Temporal Classification (CTC) loss and the Order-Agnostic Cross Entropy (OAXE) loss can better handle short- and long-range syntactic multi-modalities respectively. Furthermore, we take the best of both and design a new loss function to better handle the complicated syntactic multi-modality in real-world datasets. To facilitate practical usage, we provide a guide to using different loss functions for different kinds of syntactic multi-modality.</abstract>
      <url hash="289b5008">2022.naacl-main.126</url>
      <bibkey>zhang-etal-2022-study</bibkey>
      <doi>10.18653/v1/2022.naacl-main.126</doi>
      <video href="2022.naacl-main.126.mp4"/>
    </paper>
    <paper id="127">
      <title><fixed-case>CIA</fixed-case>ug: Equipping Interpolative Augmentation with Curriculum Learning</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Ritesh</first><last>Soun</last></author>
      <author><first>Shrey</first><last>Pandit</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Sarvagya</first><last>Malaviya</last></author>
      <author><first>Yuval</first><last>Pinter</last></author>
      <pages>1758-1764</pages>
      <abstract>Interpolative data augmentation has proven to be effective for NLP tasks. Despite its merits, the sample selection process in mixup is random, which might make it difficult for the model to generalize better and converge faster. We propose CIAug, a novel curriculum-based learning method that builds upon mixup. It leverages the relative position of samples in hyperbolic embedding space as a complexity measure to gradually mix up increasingly difficult and diverse samples along training. CIAug achieves state-of-the-art results over existing interpolative augmentation methods on 10 benchmark datasets across 4 languages in text classification and named-entity recognition tasks. It also converges and achieves benchmark F1 scores 3 times faster. We empirically analyze the various components of CIAug, and evaluate its robustness against adversarial attacks.</abstract>
      <url hash="620d93f1">2022.naacl-main.127</url>
      <attachment type="software" hash="4fbd855e">2022.naacl-main.127.software.zip</attachment>
      <bibkey>sawhney-etal-2022-ciaug</bibkey>
      <doi>10.18653/v1/2022.naacl-main.127</doi>
      <video href="2022.naacl-main.127.mp4"/>
      <pwccode url="https://github.com/sounritesh/ciaug-naacl" additional="false">sounritesh/ciaug-naacl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="128">
      <title>Proposition-Level Clustering for Multi-Document Summarization</title>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>1765-1779</pages>
      <abstract>Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly, clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences, even though closely related sentences usually contain also non-aligned parts. In this work, we revisit the clustering approach, grouping together sub-sentential propositions, aiming at more precise information alignment. Specifically, our method detects salient propositions, clusters them into paraphrastic clusters, and generates a representative sentence for each cluster via text fusion.Our summarization method improves over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011 datasets, both in automatic ROUGE scores and human preference.</abstract>
      <url hash="fac5d0cf">2022.naacl-main.128</url>
      <bibkey>ernst-etal-2022-proposition</bibkey>
      <doi>10.18653/v1/2022.naacl-main.128</doi>
      <video href="2022.naacl-main.128.mp4"/>
      <pwccode url="https://github.com/oriern/procluster" additional="true">oriern/procluster</pwccode>
    </paper>
    <paper id="129">
      <title>Non-Autoregressive Machine Translation: It’s Not as Fast as it Seems</title>
      <author><first>Jindřich</first><last>Helcl</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>1780-1790</pages>
      <abstract>Efficient machine translation models are commercially important as they can increase inference speeds, and reduce costs and carbon emissions. Recently, there has been much interest in non-autoregressive (NAR) models, which promise faster translation. In parallel to the research on NAR models, there have been successful attempts to create optimized autoregressive models as part of the WMT shared task on efficient translation. In this paper, we point out flaws in the evaluation methodology present in the literature on NAR models and we provide a fair comparison between a state-of-the-art NAR model and the autoregressive submissions to the shared task. We make the case for consistent evaluation of NAR models, and also for the importance of comparing NAR models with other widely used methods for improving efficiency. We run experiments with a connectionist-temporal-classification-based (CTC) NAR model implemented in C++ and compare it with AR models using wall clock times. Our results show that, although NAR models are faster on GPUs, with small batch sizes, they are almost always slower under more realistic usage conditions. We call for more realistic and extensive evaluation of NAR models in future work.</abstract>
      <url hash="a3e43735">2022.naacl-main.129</url>
      <bibkey>helcl-etal-2022-non</bibkey>
      <doi>10.18653/v1/2022.naacl-main.129</doi>
      <video href="2022.naacl-main.129.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="130">
      <title><fixed-case>BAD</fixed-case>-<fixed-case>X</fixed-case>: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer</title>
      <author><first>Marinela</first><last>Parović</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>1791-1799</pages>
      <abstract>Adapter modules enable modular and efficient zero-shot cross-lingual transfer, where current state-of-the-art adapter-based approaches learn specialized language adapters (LAs) for individual languages. In this work, we show that it is more effective to learn bilingual language pair adapters (BAs) when the goal is to optimize performance for a particular source-target transfer direction. Our novel BAD-X adapter framework trades off some modularity of dedicated LAs for improved transfer performance: we demonstrate consistent gains in three standard downstream tasks, and for the majority of evaluated low-resource languages.</abstract>
      <url hash="552d3746">2022.naacl-main.130</url>
      <bibkey>parovic-etal-2022-bad</bibkey>
      <doi>10.18653/v1/2022.naacl-main.130</doi>
      <video href="2022.naacl-main.130.mp4"/>
      <pwccode url="https://github.com/parovicm/badx" additional="false">parovicm/badx</pwccode>
    </paper>
    <paper id="131">
      <title>Combining Humor and Sarcasm for Improving Political Parody Detection</title>
      <author><first>Xiao</first><last>Ao</last></author>
      <author><first>Danae</first><last>Sanchez Villegas</last></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>1800-1807</pages>
      <abstract>Parody is a figurative device used for mimicking entities for comedic or critical purposes. Parody is intentionally humorous and often involves sarcasm. This paper explores jointly modelling these figurative tropes with the goal of improving performance of political parody detection in tweets. To this end, we present a multi-encoder model that combines three parallel encoders to enrich parody-specific representations with humor and sarcasm information. Experiments on a publicly available data set of political parody tweets demonstrate that our approach outperforms previous state-of-the-art methods.</abstract>
      <url hash="58160488">2022.naacl-main.131</url>
      <bibkey>ao-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.131</doi>
      <video href="2022.naacl-main.131.mp4"/>
      <pwccode url="https://github.com/iamoscar1/multi_encoder_model_for_political_parody_prediction" additional="false">iamoscar1/multi_encoder_model_for_political_parody_prediction</pwccode>
    </paper>
    <paper id="132">
      <title><fixed-case>TIE</fixed-case>: Topological Information Enhanced Structural Reading Comprehension on Web Pages</title>
      <author><first>Zihan</first><last>Zhao</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Ruisheng</first><last>Cao</last></author>
      <author><first>Hongshen</first><last>Xu</last></author>
      <author><first>Xingyu</first><last>Chen</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>1808-1821</pages>
      <abstract>Recently, the structural reading comprehension (SRC) task on web pages has attracted increasing research interests. Although previous SRC work has leveraged extra information such as HTML tags or XPaths, the informative topology of web pages is not effectively exploited. In this work, we propose a Topological Information Enhanced model (TIE), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that, TIE integrates Graph Attention Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological information of both logical structures and spatial structures. Experimental results demonstrate that our model outperforms strong baselines and achieves state-of-the-art performances on the web-based SRC benchmark WebSRC at the time of writing. The code of TIE will be publicly available at https://github.com/X-LANCE/TIE.</abstract>
      <url hash="b52e9ca7">2022.naacl-main.132</url>
      <bibkey>zhao-etal-2022-tie</bibkey>
      <doi>10.18653/v1/2022.naacl-main.132</doi>
      <video href="2022.naacl-main.132.mp4"/>
      <pwccode url="https://github.com/x-lance/tie" additional="false">x-lance/tie</pwccode>
    </paper>
    <paper id="133">
      <title><fixed-case>RSTG</fixed-case>en: Imbuing Fine-Grained Interpretable Control into Long-<fixed-case>F</fixed-case>orm<fixed-case>T</fixed-case>ext Generators</title>
      <author><first>Rilwan</first><last>Adewoyin</last></author>
      <author><first>Ritabrata</first><last>Dutta</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>1822-1835</pages>
      <abstract>In this paper, we study the task of improving the cohesion and coherence of long-form text generated by language models.To this end, we propose RSTGen, a framework that utilises Rhetorical Structure Theory (RST), a classical language theory, to control the discourse structure, semantics and topics of generated text. Firstly, we demonstrate our model’s ability to control structural discourse and semantic features of generated text in open generation evaluation. Then we experiment on the two challenging long-form text tasks of argument generation and story generation. Evaluation using automated metrics and a metric with high correlation to human evaluation, shows that our model performs competitively against existing models, while offering significantly more controls over generated text than alternative methods.</abstract>
      <url hash="4aaace1b">2022.naacl-main.133</url>
      <bibkey>adewoyin-etal-2022-rstgen</bibkey>
      <doi>10.18653/v1/2022.naacl-main.133</doi>
      <video href="2022.naacl-main.133.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="134">
      <title>Intent Detection and Discovery from User Logs via Deep Semi-Supervised Contrastive Clustering</title>
      <author><first>Rajat</first><last>Kumar</last></author>
      <author><first>Mayur</first><last>Patidar</last></author>
      <author><first>Vaibhav</first><last>Varshney</last></author>
      <author><first>Lovekesh</first><last>Vig</last></author>
      <author><first>Gautam</first><last>Shroff</last></author>
      <pages>1836-1853</pages>
      <abstract>Intent Detection is a crucial component of Dialogue Systems wherein the objective is to classify a user utterance into one of multiple pre-defined intents. A pre-requisite for developing an effective intent identifier is a training dataset labeled with all possible user intents. However, even skilled domain experts are often unable to foresee all possible user intents at design time and for practical applications, novel intents may have to be inferred incrementally on-the-fly from user utterances. Therefore, for any real-world dialogue system, the number of intents increases over time and new intents have to be discovered by analyzing the utterances outside the existing set of intents. In this paper, our objective is to i) detect known intent utterances from a large number of unlabeled utterance samples given a few labeled samples and ii) discover new unknown intents from the remaining unlabeled samples. Existing SOTA approaches address this problem via alternate representation learning and clustering wherein pseudo labels are used for updating the representations and clustering is used for generating the pseudo labels. Unlike existing approaches that rely on epoch wise cluster alignment, we propose an end-to-end deep contrastive clustering algorithm that jointly updates model parameters and cluster centers via supervised and self-supervised learning and optimally utilizes both labeled and unlabeled data. Our proposed approach outperforms competitive baselines on five public datasets for both settings: (i) where the number of undiscovered intents are known in advance, and (ii) where the number of intents are estimated by an algorithm. We also propose a human-in-the-loop variant of our approach for practical deployment which does not require an estimate of new intents and outperforms the end-to-end approach.</abstract>
      <url hash="a4c59c16">2022.naacl-main.134</url>
      <bibkey>kumar-etal-2022-intent</bibkey>
      <doi>10.18653/v1/2022.naacl-main.134</doi>
      <video href="2022.naacl-main.134.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="135">
      <title>Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations</title>
      <author><first>Daniela</first><last>Brook Weiss</last></author>
      <author><first>Paul</first><last>Roit</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>1854-1860</pages>
      <abstract>NLP models that process multiple texts often struggle in recognizing corresponding and salient information that is often differently phrased, and consolidating the redundancies across texts. To facilitate research of such challenges, the sentence fusion task was proposed, yet previous datasets for this task were very limited in their size and scope. In this paper, we revisit and substantially extend previous dataset creation efforts. With careful modifications, relabeling, and employing complementing data sources, we were able to more than triple the size of a notable earlier dataset.Moreover, we show that our extended version uses more representative texts for multi-document tasks and provides a more diverse training set, which substantially improves model performance.</abstract>
      <url hash="79b7bd4b">2022.naacl-main.135</url>
      <bibkey>brook-weiss-etal-2022-extending</bibkey>
      <doi>10.18653/v1/2022.naacl-main.135</doi>
      <video href="2022.naacl-main.135.mp4"/>
      <pwccode url="https://github.com/danielabweiss/extending-sentence-fusion-resources" additional="false">danielabweiss/extending-sentence-fusion-resources</pwccode>
    </paper>
    <paper id="136">
      <title>The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation</title>
      <author><first>Tobias</first><last>Domhan</last></author>
      <author><first>Eva</first><last>Hasler</last></author>
      <author><first>Ke</first><last>Tran</last></author>
      <author><first>Sony</first><last>Trenous</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <author><first>Felix</first><last>Hieber</last></author>
      <pages>1861-1874</pages>
      <abstract>Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference. The chosen set is typically determined by separately trained alignment model parameters, independent of the source-sentence context at inference time. While vocabulary selection appears competitive with respect to automatic quality metrics in prior work, we show that it can fail to select the right set of output words, particularly for semantically non-compositional linguistic phenomena such as idiomatic expressions, leading to reduced translation quality as perceived by humans. Trading off latency for quality by increasing the size of the allowed set is often not an option in real-world scenarios. We propose a model of vocabulary selection, integrated into the neural translation model, that predicts the set of allowed output words from contextualized encoder representations. This restores translation quality of an unconstrained system, as measured by human evaluations on WMT newstest2020 and idiomatic expressions, at an inference latency competitive with alignment-based selection using aggressive thresholds, thereby removing the dependency on separately trained alignment models.</abstract>
      <url hash="617c4829">2022.naacl-main.136</url>
      <bibkey>domhan-etal-2022-devil</bibkey>
      <doi>10.18653/v1/2022.naacl-main.136</doi>
      <video href="2022.naacl-main.136.mp4"/>
      <pwccode url="https://github.com/awslabs/sockeye" additional="false">awslabs/sockeye</pwccode>
    </paper>
    <paper id="137">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>C</fixed-case>ite: Modeling realistic citations requires moving beyond the single-sentence single-label setting</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Brandon</first><last>Ko</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Sophie</first><last>Johnson</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <pages>1875-1889</pages>
      <abstract>Citation context analysis (CCA) is an important task in natural language processing that studies how and why scholars discuss each others’ work. Despite decades of study, computational methods for CCA have largely relied on overly-simplistic assumptions of how authors cite, which ignore several important phenomena. For instance, scholarly papers often contain rich discussions of cited work that span multiple sentences and express multiple intents concurrently. Yet, recent work in CCA is often approached as a single-sentence, single-label classification task, and thus many datasets used to develop modern computational approaches fail to capture this interesting discourse. To address this research gap, we highlight three understudied phenomena for CCA and release MULTICITE, a new dataset of 12.6K citation contexts from 1.2K computational linguistics papers that fully models these phenomena. Not only is it the largest collection of expert-annotated citation contexts to-date, MULTICITE contains multi-sentence, multi-label citation contexts annotated through-out entire full paper texts. We demonstrate how MULTICITE can enable the development of new computational methods on three important CCA tasks. We release our code and dataset at https://github.com/allenai/multicite.</abstract>
      <url hash="e5ee3c87">2022.naacl-main.137</url>
      <bibkey>lauscher-etal-2022-multicite</bibkey>
      <doi>10.18653/v1/2022.naacl-main.137</doi>
      <video href="2022.naacl-main.137.mp4"/>
      <pwccode url="https://github.com/allenai/multicite" additional="false">allenai/multicite</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multicite">MultiCite</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasper">QASPER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="138">
      <title><fixed-case>DEGREE</fixed-case>: A Data-Efficient Generation-Based Event Extraction Model</title>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Elizabeth</first><last>Boschee</last></author>
      <author><first>Scott</first><last>Miller</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>1890-1908</pages>
      <abstract>Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.</abstract>
      <url hash="e44bdfb7">2022.naacl-main.138</url>
      <attachment type="software" hash="69d2f976">2022.naacl-main.138.software.zip</attachment>
      <bibkey>hsu-etal-2022-degree</bibkey>
      <doi>10.18653/v1/2022.naacl-main.138</doi>
      <video href="2022.naacl-main.138.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/degree" additional="false">pluslabnlp/degree</pwccode>
    </paper>
    <paper id="139">
      <title>Bridging the Gap between Language Models and Cross-Lingual Sequence Labeling</title>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>1909-1923</pages>
      <abstract>Large-scale cross-lingual pre-trained language models (xPLMs) have shown effective in cross-lingual sequence labeling tasks (xSL), such as machine reading comprehension (xMRC) by transferring knowledge from a high-resource language to low-resource languages.Despite the great success, we draw an empirical observation that there is an training objective gap between pre-training and fine-tuning stages: e.g., mask language modeling objective requires <i>local</i> understanding of the masked token and the span-extraction objective requires understanding and reasoning of the <i>global</i> input passage/paragraph and question, leading to the discrepancy between pre-training and xMRC. In this paper, we first design a pre-training task tailored for xSL named Cross-lingual Language Informative Span Masking (CLISM) to eliminate the objective gap in a self-supervised manner. Second, we present ContrAstive-Consistency Regularization (CACR), which utilizes contrastive learning to encourage the consistency between representations of input parallel sequences via unsupervised cross-lingual instance-wise training signals during pre-training. By these means, our methods not only bridge the gap between pretrain-finetune, but also enhance PLMs to better capture the alignment between different languages. Extensive experiments prove that our method achieves clearly superior results on multiple xSL benchmarks with limited pre-training data. Our methods also surpass the previous state-of-the-art methods by a large margin in few-shot data setting, where only a few hundred training examples are available.</abstract>
      <url hash="33202627">2022.naacl-main.139</url>
      <bibkey>chen-etal-2022-bridging</bibkey>
      <doi>10.18653/v1/2022.naacl-main.139</doi>
      <video href="2022.naacl-main.139.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiAnn</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="140">
      <title>Hero-Gang Neural Model For Named Entity Recognition</title>
      <author><first>Jinpeng</first><last>Hu</last></author>
      <author><first>Yaling</first><last>Shen</last></author>
      <author id="yang-liu-hk"><first>Yang</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <author><first>Tsung-Hui</first><last>Chang</last></author>
      <pages>1924-1936</pages>
      <abstract>Named entity recognition (NER) is a fundamental and important task in NLP, aiming at identifying named entities (NEs) from free text. Recently, since the multi-head attention mechanism applied in the Transformer model can effectively capture longer contextual information, Transformer-based models have become the mainstream methods and have achieved significant performance in this task. Unfortunately, although these models can capture effective global context information, they are still limited in the local feature and position information extraction, which is critical in NER. In this paper, to address this limitation, we propose a novel Hero-Gang Neural structure (HGN), including the Hero and Gang module, to leverage both global and local information to promote NER. Specifically, the Hero module is composed of a Transformer-based encoder to maintain the advantage of the self-attention mechanism, and the Gang module utilizes a multi-window recurrent module to extract local features and position information under the guidance of the Hero module. Afterward, the proposed multi-window attention effectively combines global information and multiple local features for predicting entity labels. Experimental results on several benchmark datasets demonstrate the effectiveness of our proposed model.</abstract>
      <url hash="bedacb48">2022.naacl-main.140</url>
      <attachment type="software" hash="4e060248">2022.naacl-main.140.software.zip</attachment>
      <bibkey>hu-etal-2022-hero</bibkey>
      <doi>10.18653/v1/2022.naacl-main.140</doi>
      <video href="2022.naacl-main.140.mp4"/>
      <pwccode url="https://github.com/jinpeng01/hgn" additional="false">jinpeng01/hgn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc2gm">BC2GM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2016-ner">WNUT 2016 NER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="141">
      <title><fixed-case>MGIMN</fixed-case>: Multi-Grained Interactive Matching Network for Few-shot Text Classification</title>
      <author><first>Jianhai</first><last>Zhang</last></author>
      <author><first>Mieradilijiang</first><last>Maimaiti</last></author>
      <author><first>Gao</first><last>Xing</last></author>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <pages>1937-1946</pages>
      <abstract>Text classification struggles to generalize to unseen classes with very few labeled text instances per class.In such a few-shot learning (FSL) setting, metric-based meta-learning approaches have shown promising results. Previous studies mainly aim to derive a prototype representation for each class.However, they neglect that it is challenging-yet-unnecessary to construct a compact representation which expresses the entire meaning for each class.They also ignore the importance to capture the inter-dependency between query and the support set for few-shot text classification. To deal with these issues, we propose a meta-learning based method MGIMN which performs instance-wise comparison followed by aggregation to generate class-wise matching vectors instead of prototype learning.The key of instance-wise comparison is the interactive matching within the class-specific context and episode-specific context. Extensive experiments demonstrate that the proposed method significantly outperforms the existing SOTA approaches, under both the standard FSL and generalized FSL settings.</abstract>
      <url hash="de8050d0">2022.naacl-main.141</url>
      <bibkey>zhang-etal-2022-mgimn</bibkey>
      <doi>10.18653/v1/2022.naacl-main.141</doi>
      <video href="2022.naacl-main.141.mp4"/>
    </paper>
    <paper id="142">
      <title>All You May Need for <fixed-case>VQA</fixed-case> are Image Captions</title>
      <author><first>Soravit</first><last>Changpinyo</last></author>
      <author><first>Doron</first><last>Kukliansy</last></author>
      <author><first>Idan</first><last>Szpektor</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Nan</first><last>Ding</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>1947-1963</pages>
      <abstract>Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.</abstract>
      <url hash="502cc410">2022.naacl-main.142</url>
      <bibkey>changpinyo-etal-2022-may</bibkey>
      <doi>10.18653/v1/2022.naacl-main.142</doi>
      <video href="2022.naacl-main.142.mp4"/>
      <pwccode url="https://github.com/google-research-datasets/maverics" additional="false">google-research-datasets/maverics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maverics">MAVERICS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-qa">COCO-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="143">
      <title>Frustratingly Easy System Combination for Grammatical Error Correction</title>
      <author><first>Muhammad</first><last>Qorib</last></author>
      <author><first>Seung-Hoon</first><last>Na</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <pages>1964-1974</pages>
      <abstract>In this paper, we formulate system combination for grammatical error correction (GEC) as a simple machine learning task: binary classification. We demonstrate that with the right problem formulation, a simple logistic regression algorithm can be highly effective for combining GEC models. Our method successfully increases the F0.5 score from the highest base GEC system by 4.2 points on the CoNLL-2014 test set and 7.2 points on the BEA-2019 test set. Furthermore, our method outperforms the state of the art by 4.0 points on the BEA-2019 test set, 1.2 points on the CoNLL-2014 test set with original annotation, and 3.4 points on the CoNLL-2014 test set with alternative annotation. We also show that our system combination generates better corrections with higher F0.5 scores than the conventional ensemble.</abstract>
      <url hash="1c921b0f">2022.naacl-main.143</url>
      <attachment type="software" hash="631ff638">2022.naacl-main.143.software.zip</attachment>
      <bibkey>qorib-etal-2022-frustratingly</bibkey>
      <doi>10.18653/v1/2022.naacl-main.143</doi>
      <video href="2022.naacl-main.143.mp4"/>
      <pwccode url="https://github.com/nusnlp/esc" additional="false">nusnlp/esc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="144">
      <title>Simple Local Attentions Remain Competitive for Long-Context Tasks</title>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Anchit</first><last>Gupta</last></author>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Diana</first><last>Liskovich</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <pages>1975-1986</pages>
      <abstract>Many NLP tasks require processing long contexts beyond the length limit of pretrained models. In order to scale these models to longer text sequences, many efficient long-range attention variants have been proposed. Despite the abundance of research along this direction, it is still difficult to gauge the relative effectiveness of these models in practical use cases, e.g., if we apply these models following the pretrain-and-finetune paradigm. In this work, we aim to conduct a thorough analysis of these emerging models with large-scale and controlled experiments. For each attention variant, we pretrain large-size models using the same long-doc corpus and then finetune these models for real-world long-context tasks. Our findings reveal pitfalls of an existing widely-used long-range benchmark and show none of the tested efficient attentions can beat a simple local window attention under standard pretraining paradigms. Further analysis on local attention variants suggests that even the commonly used attention-window overlap is not necessary to achieve good downstream results — using disjoint local attentions, we are able to build a simpler and more efficient long-doc QA model that matches the performance of Longformer with half of its pretraining compute.</abstract>
      <url hash="c873f011">2022.naacl-main.144</url>
      <bibkey>xiong-etal-2022-simple</bibkey>
      <doi>10.18653/v1/2022.naacl-main.144</doi>
      <video href="2022.naacl-main.144.mp4"/>
      <pwccode url="https://github.com/pytorch/fairseq" additional="false">pytorch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
    </paper>
    <paper id="145">
      <title><fixed-case>E</fixed-case>ven the Simplest Baseline Needs Careful Re-investigation: A Case Study on <fixed-case>XML</fixed-case>-<fixed-case>CNN</fixed-case></title>
      <author><first>Si-An</first><last>Chen</last></author>
      <author><first>Jie-jyun</first><last>Liu</last></author>
      <author><first>Tsung-Han</first><last>Yang</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last></author>
      <author><first>Chih-Jen</first><last>Lin</last></author>
      <pages>1987-2000</pages>
      <abstract>The power and the potential of deep learning models attract many researchers to design advanced and sophisticated architectures. Nevertheless, the progress is sometimes unreal due to various possible reasons. In this work, through an astonishing example we argue that more efforts should be paid to ensure the progress in developing a new deep learning method. For a highly influential multi-label text classification method XML-CNN, we show that the superior performance claimed in the original paper was mainly due to some unbelievable coincidences. We re-examine XML-CNN and make a re-implementation which reveals some contradictory findings to the claims in the original paper. Our study suggests suitable baselines for multi-label text classification tasks and confirms that the progress on a new architecture cannot be confidently justified without a cautious investigation.</abstract>
      <url hash="144b68dd">2022.naacl-main.145</url>
      <attachment type="software" hash="a8267bc0">2022.naacl-main.145.software.zip</attachment>
      <bibkey>chen-etal-2022-even</bibkey>
      <doi>10.18653/v1/2022.naacl-main.145</doi>
      <video href="2022.naacl-main.145.mp4"/>
    </paper>
    <paper id="146">
      <title>Multi-Relational Graph Transformer for Automatic Short Answer Grading</title>
      <author><first>Rajat</first><last>Agarwal</last></author>
      <author><first>Varun</first><last>Khurana</last></author>
      <author><first>Karish</first><last>Grover</last></author>
      <author><first>Mukesh</first><last>Mohania</last></author>
      <author><first>Vikram</first><last>Goyal</last></author>
      <pages>2001-2012</pages>
      <abstract>The recent transition to the online educational domain has increased the need for Automatic Short Answer Grading (ASAG). ASAG automatically evaluates a student’s response against a (given) correct response and thus has been a prevalent semantic matching task. Most existing methods utilize sequential context to compare two sentences and ignore the structural context of the sentence; therefore, these methods may not result in the desired performance. In this paper, we overcome this problem by proposing a Multi-Relational Graph Transformer, MitiGaTe, to prepare token representations considering the structural context. Abstract Meaning Representation (AMR) graph is created by parsing the text response and then segregated into multiple subgraphs, each corresponding to a particular relationship in AMR. A Graph Transformer is used to prepare relation-specific token embeddings within each subgraph, then aggregated to obtain a subgraph representation. Finally, we compare the correct answer and the student response subgraph representations to yield a final score. Experimental results on Mohler’s dataset show that our system outperforms the existing state-of-the-art methods. We have released our implementation https://github.com/kvarun07/asag-gt, as we believe that our model can be useful for many future applications.</abstract>
      <url hash="f34de3f4">2022.naacl-main.146</url>
      <bibkey>agarwal-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.naacl-main.146</doi>
      <video href="2022.naacl-main.146.mp4"/>
      <pwccode url="https://github.com/kvarun07/asag-gt" additional="false">kvarun07/asag-gt</pwccode>
    </paper>
    <paper id="147">
      <title>Event Schema Induction with Double Graph Autoencoders</title>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>2013-2025</pages>
      <abstract>Event schema depicts the typical structure of complex events, serving as a scaffolding to effectively analyze, predict, and possibly intervene in the ongoing events. To induce event schemas from historical events, previous work uses an event-by-event scheme, ignoring the global structure of the entire schema graph. We propose a new event schema induction framework using double graph autoencoders, which captures the global dependencies among nodes in event graphs. Specifically, we first extract the event skeleton from an event graph and design a variational directed acyclic graph (DAG) autoencoder to learn its global structure. Then we further fill in the event arguments for the skeleton, and use another Graph Convolutional Network (GCN) based autoencoder to reconstruct entity-entity relations as well as to detect coreferential entities. By performing this two-stage induction decomposition, the model can avoid reconstructing the entire graph in one step, allowing it to focus on learning global structures between events. Experimental results on three event graph datasets demonstrate that our method achieves state-of-the-art performance and induces high-quality event schemas with global consistency.</abstract>
      <url hash="2a5d6c9e">2022.naacl-main.147</url>
      <attachment type="software" hash="a29300d0">2022.naacl-main.147.software.zip</attachment>
      <bibkey>jin-etal-2022-event</bibkey>
      <doi>10.18653/v1/2022.naacl-main.147</doi>
      <video href="2022.naacl-main.147.mp4"/>
    </paper>
    <paper id="148">
      <title><fixed-case>CS</fixed-case>1<fixed-case>QA</fixed-case>: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course</title>
      <author><first>Changyoon</first><last>Lee</last></author>
      <author><first>Yeon</first><last>Seonwoo</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>2026-2040</pages>
      <abstract>We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student’s code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and retrieving an answer from the annotated corpus.Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to understand both the code and natural language. This unique dataset can be used as a benchmark for source code comprehension and question answering in the educational setting.</abstract>
      <url hash="ded73ba9">2022.naacl-main.148</url>
      <attachment type="software" hash="23532b79">2022.naacl-main.148.software.zip</attachment>
      <bibkey>lee-etal-2022-cs1qa</bibkey>
      <doi>10.18653/v1/2022.naacl-main.148</doi>
      <video href="2022.naacl-main.148.mp4"/>
      <pwccode url="https://github.com/cyoon47/cs1qa" additional="false">cyoon47/cs1qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cs1qa">CS1QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codeqa">CodeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="149">
      <title>Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data</title>
      <author><first>Kemal</first><last>Kurniawan</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <author><first>Philip</first><last>Schulz</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>2041-2054</pages>
      <abstract>Providing technologies to communities or domains where training data is scarce or protected e.g., for privacy reasons, is becoming increasingly important. To that end, we generalise methods for unsupervised transfer from multiple input models for structured prediction. We show that the means of aggregating over the input models is critical, and that multiplying marginal probabilities of substructures to obtain high-probability structures for distant supervision is substantially better than taking the union of such structures over the input models, as done in prior work. Testing on 18 languages, we demonstrate that the method works in a cross-lingual setting, considering both dependency parsing and part-of-speech structured prediction problems. Our analyses show that the proposed method produces less noisy labels for the distant supervision.</abstract>
      <url hash="12355c86">2022.naacl-main.149</url>
      <bibkey>kurniawan-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.naacl-main.149</doi>
      <video href="2022.naacl-main.149.mp4"/>
      <pwccode url="https://github.com/kmkurn/uxtspwsd" additional="false">kmkurn/uxtspwsd</pwccode>
    </paper>
    <paper id="150">
      <title>Don’t Take It Literally: An Edit-Invariant Sequence Loss for Text Generation</title>
      <author><first>Guangyi</first><last>Liu</last></author>
      <author><first>Zichao</first><last>Yang</last></author>
      <author><first>Tianhua</first><last>Tao</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Zhen</first><last>Li</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Shuguang</first><last>Cui</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>2055-2078</pages>
      <abstract>Neural text generation models are typically trained by maximizing log-likelihood with the sequence cross entropy (CE) loss, which encourages an exact token-by-token match between a target sequence with a generated sequence. Such training objective is sub-optimal when the target sequence is not perfect, e.g., when the target sequence is corrupted with noises, or when only weak sequence supervision is available. To address the challenge, we propose a novel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a target <tex-math>n</tex-math>-gram with all <tex-math>n</tex-math>-grams in the generated sequence. EISL is designed to be robust to various noises and edits in the target sequences. Moreover, the EISL computation is essentially an approximate convolution operation with target <tex-math>n</tex-math>-grams as kernels, which is easy to implement and efficient to compute with existing libraries. To demonstrate the effectiveness of EISL, we conduct experiments on a wide range of tasks, including machine translation with noisy target sequences, unsupervised text style transfer with only weak training signals, and non-autoregressive generation with non-predefined generation order. Experimental results show our method significantly outperforms the common CE loss and other strong baselines on all the tasks. EISL has a simple API that can be used as a drop-in replacement of the CE loss: https://github.com/guangyliu/EISL.</abstract>
      <url hash="67dc4f73">2022.naacl-main.150</url>
      <bibkey>liu-etal-2022-dont</bibkey>
      <doi>10.18653/v1/2022.naacl-main.150</doi>
      <video href="2022.naacl-main.150.mp4"/>
      <pwccode url="https://github.com/guangyliu/EISL" additional="false">guangyliu/EISL</pwccode>
    </paper>
    <paper id="151">
      <title>Modeling Exemplification in Long-form Question Answering via Retrieval</title>
      <author><first>Shufan</first><last>Wang</last></author>
      <author><first>Fangyuan</first><last>Xu</last></author>
      <author><first>Laure</first><last>Thompson</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>2079-2092</pages>
      <abstract>Exemplification is a process by which writers explain or clarify a concept by providing an example. While common in all forms of writing, exemplification is particularly useful in the task of long-form question answering (LFQA), where a complicated answer can be made more understandable through simple examples. In this paper, we provide the first computational study of exemplification in QA, performing a fine-grained annotation of different types of examples (e.g., hypotheticals, anecdotes) in three corpora. We show that not only do state-of-the-art LFQA models struggle to generate relevant examples, but also that standard evaluation metrics such as ROUGE are insufficient to judge exemplification quality. We propose to treat exemplification as a <i>retrieval</i> problem in which a partially-written answer is used to query a large set of human-written examples extracted from a corpus. Our approach allows a reliable ranking-type automatic metrics that correlates well with human evaluation. A human evaluation shows that our model’s retrieved examples are more relevant than examples generated from a state-of-the-art LFQA model. </abstract>
      <url hash="96a00ece">2022.naacl-main.151</url>
      <bibkey>wang-etal-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.naacl-main.151</doi>
      <video href="2022.naacl-main.151.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="152">
      <title><fixed-case>D</fixed-case>2<fixed-case>U</fixed-case>: Distance-to-Uniform Learning for Out-of-Scope Detection</title>
      <author><first>Eyup</first><last>Yilmaz</last></author>
      <author><first>Cagri</first><last>Toraman</last></author>
      <pages>2093-2108</pages>
      <abstract>Supervised training with cross-entropy loss implicitly forces models to produce probability distributions that follow a discrete delta distribution. Model predictions in test time are expected to be similar to delta distributions if the classifier determines the class of an input correctly. However, the shape of the predicted probability distribution can become similar to the uniform distribution when the model cannot infer properly. We exploit this observation for detecting out-of-scope (OOS) utterances in conversational systems. Specifically, we propose a zero-shot post-processing step, called Distance-to-Uniform (D2U), exploiting not only the classification confidence score, but the shape of the entire output distribution. We later combine it with a learning procedure that uses D2U for loss calculation in the supervised setup. We conduct experiments using six publicly available datasets. Experimental results show that the performance of OOS detection is improved with our post-processing when there is no OOS training data, as well as with D2U learning procedure when OOS training data is available.</abstract>
      <url hash="d6e18667">2022.naacl-main.152</url>
      <bibkey>yilmaz-toraman-2022-d2u</bibkey>
      <doi>10.18653/v1/2022.naacl-main.152</doi>
      <video href="2022.naacl-main.152.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="153">
      <title>Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio</title>
      <author><first>Yizhu</first><last>Liu</last></author>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>2109-2115</pages>
      <abstract>A document can be summarized in a number of ways. Reference-based evaluation of summarization has been criticized for its inflexibility. The more sufficient the number of abstracts, the more accurate the evaluation results. However, it is difficult to collect sufficient reference summaries. In this paper, we propose a new automatic reference-free evaluation metric that compares semantic distribution between source document and summary by pretrained language models and considers summary compression ratio. The experiments show that this metric is more consistent with human evaluation in terms of coherence, consistency, relevance and fluency.</abstract>
      <url hash="f81bda6f">2022.naacl-main.153</url>
      <attachment type="software" hash="60d1a0dd">2022.naacl-main.153.software.zip</attachment>
      <bibkey>liu-etal-2022-reference</bibkey>
      <doi>10.18653/v1/2022.naacl-main.153</doi>
      <video href="2022.naacl-main.153.mp4"/>
      <pwccode url="https://github.com/yizhuliu/summeval" additional="false">yizhuliu/summeval</pwccode>
    </paper>
    <paper id="154">
      <title><fixed-case>K</fixed-case>ronecker<fixed-case>BERT</fixed-case>: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation</title>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Ella</first><last>Charlaix</last></author>
      <author><first>Vahid</first><last>Nia</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>2116-2127</pages>
      <abstract>The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT_BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7x and 21x outperforms state-of-the-art compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.</abstract>
      <url hash="6b8a0d22">2022.naacl-main.154</url>
      <bibkey>tahaei-etal-2022-kroneckerbert</bibkey>
      <doi>10.18653/v1/2022.naacl-main.154</doi>
      <video href="2022.naacl-main.154.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="155">
      <title>Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models</title>
      <author><first>Sanghwan</first><last>Bae</last></author>
      <author><first>Donghyun</first><last>Kwak</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Donghoon</first><last>Ham</last></author>
      <author><first>Soyoung</first><last>Kang</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Woomyoung</first><last>Park</last></author>
      <pages>2128-2150</pages>
      <abstract>Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.</abstract>
      <url hash="8ff9b15d">2022.naacl-main.155</url>
      <bibkey>bae-etal-2022-building</bibkey>
      <doi>10.18653/v1/2022.naacl-main.155</doi>
      <video href="2022.naacl-main.155.mp4"/>
      <pwccode url="https://github.com/naver-ai/carecall-corpus" additional="false">naver-ai/carecall-corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/carecall">CareCall</pwcdataset>
    </paper>
    <paper id="156">
      <title>Sentence-Level Resampling for Named Entity Recognition</title>
      <author><first>Xiaochen</first><last>Wang</last></author>
      <author><first>Yue</first><last>Wang</last></author>
      <pages>2151-2165</pages>
      <abstract>As a fundamental task in natural language processing, named entity recognition (NER) aims to locate and classify named entities in unstructured text. However, named entities are always the minority among all tokens in the text. This data imbalance problem presents a challenge to machine learning models as their learning objective is usually dominated by the majority of non-entity tokens. To alleviate data imbalance, we propose a set of sentence-level resampling methods where the importance of each training sentence is computed based on its tokens and entities. We study the generalizability of these resampling methods on a wide variety of NER models (CRF, Bi-LSTM, and BERT) across corpora from diverse domains (general, social, and medical texts). Extensive experiments show that the proposed methods improve span-level macro F1-scores of the evaluated NER models on multiple corpora, frequently outperforming sub-sentence-level resampling, data augmentation, and special loss functions such as focal and Dice loss.</abstract>
      <url hash="bd827ef4">2022.naacl-main.156</url>
      <bibkey>wang-wang-2022-sentence</bibkey>
      <doi>10.18653/v1/2022.naacl-main.156</doi>
      <video href="2022.naacl-main.156.mp4"/>
      <pwccode url="https://github.com/xiaochen-w/ner_adaptive_resampling" additional="false">xiaochen-w/ner_adaptive_resampling</pwccode>
    </paper>
    <paper id="157">
      <title>Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem</title>
      <author><first>Ryoma</first><last>Sato</last></author>
      <pages>2166-2172</pages>
      <abstract>Word embeddings are one of the most fundamental technologies used in natural language processing. Existing word embeddings are high-dimensional and consume considerable computational resources. In this study, we propose WordTour, unsupervised one-dimensional word embeddings. To achieve the challenging goal, we propose a decomposition of the desiderata of word embeddings into two parts, completeness and soundness, and focus on soundness in this paper. Owing to the single dimensionality, WordTour is extremely efficient and provides a minimal means to handle word embeddings. We experimentally confirmed the effectiveness of the proposed method via user study and document classification.</abstract>
      <url hash="219399c5">2022.naacl-main.157</url>
      <attachment type="software" hash="8ff4e947">2022.naacl-main.157.software.zip</attachment>
      <bibkey>sato-2022-word</bibkey>
      <doi>10.18653/v1/2022.naacl-main.157</doi>
      <video href="2022.naacl-main.157.mp4"/>
      <pwccode url="https://github.com/joisino/wordtour" additional="false">joisino/wordtour</pwccode>
    </paper>
    <paper id="158">
      <title>On the Diversity and Limits of Human Explanations</title>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>2173-2188</pages>
      <abstract>A growing effort in NLP aims to build datasets of human explanations. However, it remains unclear whether these datasets serve their intended goals. This problem is exacerbated by the fact that the term explanation is overloaded and refers to a broad range of notions with different properties and ramifications. Our goal is to provide an overview of the diversity of explanations, discuss human limitations in providing explanations, and ultimately provide implications for collecting and using human explanations in NLP.Inspired by prior work in psychology and cognitive sciences, we group existing human explanations in NLP into three categories: proximal mechanism, evidence, and procedure. These three types differ in nature and have implications for the resultant explanations. For instance, procedure is not considered explanation in psychology and connects with a rich body of work on learning from instructions. The diversity of explanations is further evidenced by proxy questions that are needed for annotators to interpret and answer “why is [input] assigned [label]”. Finally, giving explanations may require different, often deeper, understandings than predictions, which casts doubt on whether humans can provide valid explanations in some tasks.</abstract>
      <url hash="97fe95b2">2022.naacl-main.158</url>
      <bibkey>tan-2022-diversity</bibkey>
      <doi>10.18653/v1/2022.naacl-main.158</doi>
      <video href="2022.naacl-main.158.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="159">
      <title>Locally Aggregated Feature Attribution on Natural Language Model Understanding</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Jin</first><last>Wang</last></author>
      <author><first>Haitao</first><last>Jiang</last></author>
      <author><first>Rui</first><last>Song</last></author>
      <pages>2189-2201</pages>
      <abstract>With the growing popularity of deep-learning models, model understanding becomes more important. Much effort has been devoted to demystify deep neural networks for better explainability. Some feature attribution methods have shown promising results in computer vision, especially the gradient-based methods where effectively smoothing the gradients with reference data is the key to a robust and faithful result. However, direct application of these gradient-based methods to NLP tasks is not trivial due to the fact that the input consists of discrete tokens and the “reference” tokens are not explicitly defined. In this work, we propose Locally Aggregated Feature Attribution (LAFA), a novel gradient-based feature attribution method for NLP models. Instead of relying on obscure reference tokens, it smooths gradients by aggregating similar reference texts derived from language model embeddings. For evaluation purpose, we also design experiments on different NLP tasks including Entity Recognition and Sentiment Analysis on public datasets and key words detection on constructed Amazon catalogue dataset. The superior performance of the proposed method is demonstrated through experiments.</abstract>
      <url hash="d373a805">2022.naacl-main.159</url>
      <bibkey>zhang-etal-2022-locally</bibkey>
      <doi>10.18653/v1/2022.naacl-main.159</doi>
      <video href="2022.naacl-main.159.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="160">
      <title>Generic and Trend-aware Curriculum Learning for Relation Extraction</title>
      <author><first>Nidhi</first><last>Vakil</last></author>
      <author><first>Hadi</first><last>Amiri</last></author>
      <pages>2202-2213</pages>
      <abstract>We present a generic and trend-aware curriculum learning approach that effectively integrates textual and structural information in text graphs for relation extraction between entities, which we consider as node pairs in graphs. The proposed model extends existing curriculum learning approaches by incorporating sample-level loss trends to better discriminate easier from harder samples and schedule them for training. The model results in a robust estimation of sample difficulty and shows sizable improvement over the state-of-the-art approaches across several datasets.</abstract>
      <url hash="3ff0792d">2022.naacl-main.160</url>
      <bibkey>vakil-amiri-2022-generic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.160</doi>
      <video href="2022.naacl-main.160.mp4"/>
    </paper>
    <paper id="161">
      <title>On Systematic Style Differences between Unsupervised and Supervised <fixed-case>MT</fixed-case> and an Application for High-Resource Machine Translation</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <pages>2214-2225</pages>
      <abstract>Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask whether the different training methods result in systematically different output beyond what is visible via quality metrics like adequacy or BLEU. We compare translations from supervised and unsupervised MT systems of similar quality, finding that unsupervised output is more fluent and more structurally different in comparison to human translation than is supervised MT. We then demonstrate a way to combine the benefits of both methods into a single system which results in improved adequacy and fluency as rated by human evaluators. Our results open the door to interesting discussions about how supervised and unsupervised MT might be different yet mutually-beneficial.</abstract>
      <url hash="caab7332">2022.naacl-main.161</url>
      <bibkey>marchisio-etal-2022-systematic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.161</doi>
      <video href="2022.naacl-main.161.mp4"/>
    </paper>
    <paper id="162">
      <title>Evidentiality-guided Generation for Knowledge-Intensive <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>2226-2243</pages>
      <abstract>Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate <i>evidentiality</i> of passages—whether a passage contains correct evidence to support the output—into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the <i>evidentiality</i> of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality <i>silver</i> evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at https://github.com/AkariAsai/evidentiality_qa</abstract>
      <url hash="be7c3f9a">2022.naacl-main.162</url>
      <bibkey>asai-etal-2022-evidentiality</bibkey>
      <doi>10.18653/v1/2022.naacl-main.162</doi>
      <video href="2022.naacl-main.162.mp4"/>
      <pwccode url="https://github.com/akariasai/evidentiality_qa" additional="false">akariasai/evidentiality_qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/faviq">FaVIQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="163">
      <title>Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning</title>
      <author><first>Yu Jin</first><last>Kim</last></author>
      <author><first>Beong-woo</first><last>Kwak</last></author>
      <author><first>Youngwook</first><last>Kim</last></author>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Jinyoung</first><last>Yeo</last></author>
      <pages>2244-2257</pages>
      <abstract>Commonsense reasoning systems should be able to generalize to diverse reasoning cases. However, most state-of-the-art approaches depend on expensive data annotations and overfit to a specific benchmark without learning how to perform general semantic reasoning. To overcome these drawbacks, zero-shot QA systems have shown promise as a robust learning scheme by transforming a commonsense knowledge graph (KG) into synthetic QA-form samples for model training. Considering the increasing type of different commonsense KGs, this paper aims to extend the zero-shot transfer learning scenario into multiple-source settings, where different KGs can be utilized synergetically. Towards this goal, we propose to mitigate the loss of knowledge from the interference among the different knowledge sources, by developing a modular variant of the knowledge aggregation as a new zero-shot commonsense reasoning framework. Results on five commonsense reasoning benchmarks demonstrate the efficacy of our framework, improving the performance with multiple KGs.</abstract>
      <url hash="0a3c4a56">2022.naacl-main.163</url>
      <bibkey>kim-etal-2022-modularized</bibkey>
      <doi>10.18653/v1/2022.naacl-main.163</doi>
      <video href="2022.naacl-main.163.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/social-iqa">SIQA</pwcdataset>
    </paper>
    <paper id="164">
      <title>Learning to Express in Knowledge-Grounded Conversation</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>2258-2273</pages>
      <abstract>Grounding dialogue generation by extra knowledge has shown great potentials towards building a system capable of replying with knowledgeable and engaging responses. Existing studies focus on how to synthesize a response with proper knowledge, yet neglect that the same knowledge could be expressed differently by speakers even under the same context. In this work, we mainly consider two aspects of knowledge expression, namely the structure of the response and style of the content in each part. We therefore introduce two sequential latent variables to represent the structure and the content style respectively. We propose a segmentation-based generation model and optimize the model by a variational approach to discover the underlying pattern of knowledge expression in a response. Evaluation results on two benchmarks indicate that our model can learn the structure style defined by a few examples and generate responses in desired content style.</abstract>
      <url hash="aea7fe34">2022.naacl-main.164</url>
      <bibkey>zhao-etal-2022-learning-express</bibkey>
      <doi>10.18653/v1/2022.naacl-main.164</doi>
      <video href="2022.naacl-main.164.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="165">
      <title>End-to-End <fixed-case>C</fixed-case>hinese Speaker Identification</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>2274-2285</pages>
      <abstract>Speaker identification (SI) in texts aims to identify the speaker(s) for each utterance in texts. Previous studies divide SI into several sub-tasks (e.g., quote extraction, named entity recognition, gender identification, and coreference resolution). However, we are still far from solving these sub-tasks, making SI systems that rely on them seriously suffer from error propagation. End-to-end SI systems, on the other hand, are not limited by individual modules, but suffer from insufficient training data from the existing small-scale datasets. To make large end-to-end models possible, we design a new annotation guideline that regards SI as span extraction from the local context, and we annotate by far the largest SI dataset for Chinese named CSI based on eighteen novels. Viewing SI as a span selection task also introduces the possibility of applying existing storng extractive machine reading comprehension (MRC) baselines. Surprisingly, simply using such a baseline without human-annotated character names and carefully designed rules, we can already achieve performance comparable or better than those of previous state-of-the-art SI methods on all public SI datasets for Chinese. Furthermore, we show that our dataset can serve as additional training data for existing benchmarks, which leads to further gains (up to 6.5% in accuracy). Finally, using CSI as a clean source, we design an effective self-training paradigm to continuously leverage hundreds of unlabeled novels.</abstract>
      <url hash="13e37f2f">2022.naacl-main.165</url>
      <bibkey>yu-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.naacl-main.165</doi>
      <video href="2022.naacl-main.165.mp4"/>
      <pwccode url="https://github.com/yudiandoris/csi" additional="false">yudiandoris/csi</pwccode>
    </paper>
    <paper id="166">
      <title><fixed-case>MINION</fixed-case>: a Large-Scale and Diverse Dataset for Multilingual Event Detection</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>2286-2299</pages>
      <abstract>Event Detection (ED) is the task of identifying and classifying trigger words of event mentions in text. Despite considerable research efforts in recent years for English text, the task of ED in other languages has been significantly less explored. Switching to non-English languages, important research questions for ED include how well existing ED models perform on different languages, how challenging ED is in other languages, and how well ED knowledge and annotation can be transferred across languages. To answer those questions, it is crucial to obtain multilingual ED datasets that provide consistent event annotation for multiple languages. There exist some multilingual ED datasets; however, they tend to cover a handful of languages and mainly focus on popular ones. Many languages are not covered in existing multilingual ED datasets. In addition, the current datasets are often small and not accessible to the public. To overcome those shortcomings, we introduce a new large-scale multilingual dataset for ED (called MINION) that consistently annotates events for 8 different languages; 5 of them have not been supported by existing multilingual datasets. We also perform extensive experiments and analysis to demonstrate the challenges and transferability of ED across languages in MINION that in all call for more research effort in this area. We will release the dataset to promote future research on multilingual ED.</abstract>
      <url hash="0b6a29c0">2022.naacl-main.166</url>
      <bibkey>pouran-ben-veyseh-etal-2022-minion</bibkey>
      <doi>10.18653/v1/2022.naacl-main.166</doi>
      <video href="2022.naacl-main.166.mp4"/>
    </paper>
    <paper id="167">
      <title>Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</title>
      <author><first>Albert</first><last>Webson</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>2300-2344</pages>
      <abstract>Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models’ impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans’ use of task instructions.</abstract>
      <url hash="832f220f">2022.naacl-main.167</url>
      <bibkey>webson-pavlick-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.naacl-main.167</doi>
      <video href="2022.naacl-main.167.mp4"/>
      <pwccode url="https://github.com/awebson/prompt_semantics" additional="false">awebson/prompt_semantics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="168">
      <title><fixed-case>GPL</fixed-case>: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval</title>
      <author><first>Kexin</first><last>Wang</last></author>
      <author><first>Nandan</first><last>Thakur</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2345-2360</pages>
      <abstract>Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method <i>Generative Pseudo Labeling</i> (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at https://github.com/UKPLab/gpl.</abstract>
      <url hash="5f5c0ec7">2022.naacl-main.168</url>
      <attachment type="software" hash="aa55541f">2022.naacl-main.168.software.zip</attachment>
      <bibkey>wang-etal-2022-gpl</bibkey>
      <doi>10.18653/v1/2022.naacl-main.168</doi>
      <video href="2022.naacl-main.168.mp4"/>
      <pwccode url="https://github.com/ukplab/gpl" additional="true">ukplab/gpl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="169">
      <title>Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models</title>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Aaron</first><last>Jaech</last></author>
      <pages>2361-2375</pages>
      <abstract>Distilling state-of-the-art transformer models into lightweight student models is an effective way to reduce computation cost at inference time. The student models are typically compact transformers with fewer parameters, while expensive operations such as self-attention persist. Therefore, the improved inference speed may still be unsatisfactory for real-time or high-volume use cases. In this paper, we aim to further push the limit of inference speed by distilling teacher models into bigger, sparser student models – bigger in that they scale up to billions of parameters; sparser in that most of the model parameters are n-gram embeddings. Our experiments on six single-sentence text classification tasks show that these student models retain 97% of the RoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x speed-up on both GPUs and CPUs at inference time. Further investigation reveals that our pipeline is also helpful for sentence-pair classification tasks, and in domain generalization settings.</abstract>
      <url hash="0a4799f3">2022.naacl-main.169</url>
      <bibkey>ye-etal-2022-sparse</bibkey>
      <doi>10.18653/v1/2022.naacl-main.169</doi>
      <video href="2022.naacl-main.169.mp4"/>
      <pwccode url="https://github.com/ink-usc/sparse-distillation" additional="false">ink-usc/sparse-distillation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="170">
      <title>Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models</title>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>2376-2394</pages>
      <abstract>In this paper, we extend the line of BERTology work by focusing on the important, yet less explored, alignment of pre-trained and fine-tuned PLMs with large-scale discourse structures. We propose a novel approach to infer discourse information for arbitrarily long documents. In our experiments, we find that the captured discourse information is local and general, even across a collection of fine-tuning tasks. We compare the inferred discourse trees with supervised, distantly supervised and simple baselines to explore the structural overlap, finding that constituency discourse trees align well with supervised models, however, contain complementary discourse information.Lastly, we individually explore self-attention matrices to analyze the information redundancy. We find that similar discourse information is consistently captured in the same heads.</abstract>
      <url hash="8e11db33">2022.naacl-main.170</url>
      <bibkey>huber-carenini-2022-towards</bibkey>
      <doi>10.18653/v1/2022.naacl-main.170</doi>
      <video href="2022.naacl-main.170.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="171">
      <title><fixed-case>SAIS</fixed-case>: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction</title>
      <author><first>Yuxin</first><last>Xiao</last></author>
      <author><first>Zecheng</first><last>Zhang</last></author>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Carl</first><last>Yang</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>2395-2409</pages>
      <abstract>Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources—relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.</abstract>
      <url hash="b89942ab">2022.naacl-main.171</url>
      <attachment type="software" hash="33064a91">2022.naacl-main.171.software.zip</attachment>
      <bibkey>xiao-etal-2022-sais</bibkey>
      <doi>10.18653/v1/2022.naacl-main.171</doi>
      <video href="2022.naacl-main.171.mp4"/>
      <pwccode url="https://github.com/xiaoyuxin1002/sais" additional="false">xiaoyuxin1002/sais</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cdr">CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gda">GDA</pwcdataset>
    </paper>
    <paper id="172">
      <title><fixed-case>LITE</fixed-case>: <fixed-case>I</fixed-case>ntent-based Task Representation Learning Using Weak Supervision</title>
      <author><first>Naoki</first><last>Otani</last></author>
      <author><first>Michael</first><last>Gamon</last></author>
      <author><first>Sujay Kumar</first><last>Jauhar</last></author>
      <author><first>Mei</first><last>Yang</last></author>
      <author><first>Sri Raghu</first><last>Malireddi</last></author>
      <author><first>Oriana</first><last>Riva</last></author>
      <pages>2410-2424</pages>
      <abstract>Users write to-dos as personal notes to themselves, about things they need to complete, remember or organize. To-do texts are usually short and under-specified, which poses a challenge for current text representation models. Yet, understanding and representing their meaning is the first step towards providing intelligent assistance for to-do management. We address this problem by proposing a neural multi-task learning framework, LITE, which extracts representations of English to-do tasks with a multi-head attention mechanism on top of a pre-trained text encoder. To adapt representation models to to-do texts, we collect weak-supervision labels from semantically rich external resources (e.g., dynamic commonsense knowledge bases), following the principle that to-do tasks with similar intents have similar labels. We then train the model on multiple generative/predictive training objectives jointly. We evaluate our representation model on four downstream tasks and show that our approach consistently improves performance over baseline models, achieving error reduction of up to 38.7%.</abstract>
      <url hash="13d492ba">2022.naacl-main.172</url>
      <bibkey>otani-etal-2022-lite</bibkey>
      <doi>10.18653/v1/2022.naacl-main.172</doi>
      <video href="2022.naacl-main.172.mp4"/>
      <pwccode url="https://github.com/microsoft/intent-based-task-representation-learning" additional="false">microsoft/intent-based-task-representation-learning</pwccode>
    </paper>
    <paper id="173">
      <title>Does Summary Evaluation Survive Translation to Other Languages?</title>
      <author><first>Spencer</first><last>Braun</last></author>
      <author><first>Oleg</first><last>Vasilyev</last></author>
      <author><first>Neslihan</first><last>Iskender</last></author>
      <author><first>John</first><last>Bohannon</last></author>
      <pages>2425-2435</pages>
      <abstract>The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https://github.com/PrimerAI/primer-research/.</abstract>
      <url hash="c47d16e2">2022.naacl-main.173</url>
      <bibkey>braun-etal-2022-summary</bibkey>
      <doi>10.18653/v1/2022.naacl-main.173</doi>
      <video href="2022.naacl-main.173.mp4"/>
    </paper>
    <paper id="174">
      <title>A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony</title>
      <author><first>Tulika</first><last>Saha</last></author>
      <author><first>Saichethan</first><last>Reddy</last></author>
      <author><first>Anindya</first><last>Das</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>2436-2449</pages>
      <abstract>Mental Health Disorders continue plaguing humans worldwide. Aggravating this situation is the severe shortage of qualified and competent mental health professionals (MHPs), which underlines the need for developing Virtual Assistants (VAs) that can <i>assist</i> MHPs. The data+ML for automation can come from platforms that allow visiting and posting messages in peer-to-peer anonymous manner for sharing their experiences (frequently stigmatized) and seeking support. In this paper, we propose a VA that can act as the first point of contact and comfort for mental health patients. We curate a dataset, Motivational VA: MotiVAte comprising of 7k dyadic conversations collected from a peer-to-peer support platform. The system employs two mechanisms: (i) Mental Illness Classification: an attention based BERT classifier that outputs the mental disorder category out of the 4 categories, viz., Major Depressive Disorder (MDD), Anxiety, Obsessive Compulsive Disorder (OCD) and Post-traumatic Stress Disorder (PTSD), based on the input ongoing dialog between the support seeker and the VA; and (ii) Mental Illness Conditioned Motivational Dialogue Generation (MI-MDG): a sentiment driven Reinforcement Learning (RL) based motivational response generator. The empirical evaluation demonstrates the system capability by way of outperforming several baselines.</abstract>
      <url hash="87513f45">2022.naacl-main.174</url>
      <bibkey>saha-etal-2022-shoulder</bibkey>
      <doi>10.18653/v1/2022.naacl-main.174</doi>
      <video href="2022.naacl-main.174.mp4"/>
    </paper>
    <paper id="175">
      <title><fixed-case>S</fixed-case>ue<fixed-case>N</fixed-case>es: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling</title>
      <author><first>Forrest</first><last>Bao</last></author>
      <author><first>Ge</first><last>Luo</last></author>
      <author><first>Hebi</first><last>Li</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Youbiao</first><last>He</last></author>
      <author><first>Cen</first><last>Chen</last></author>
      <pages>2450-2458</pages>
      <abstract>Canonical automatic summary evaluation metrics, such as ROUGE, focus on lexical similarity which cannot well capture semantics nor linguistic quality and require a reference summary which is costly to obtain. Recently, there have been a growing number of efforts to alleviate either or both of the two drawbacks. In this paper, we present a proof-of-concept study to a weakly supervised summary evaluation approach without the presence of reference summaries. Massive data in existing summarization datasets are transformed for training by pairing documents with corrupted reference summaries. In cross-domain tests, our strategy outperforms baselines with promising improvements, and show a great advantage in gauging linguistic qualities over all metrics.</abstract>
      <url hash="684e1c74">2022.naacl-main.175</url>
      <bibkey>bao-etal-2022-suenes</bibkey>
      <doi>10.18653/v1/2022.naacl-main.175</doi>
      <video href="2022.naacl-main.175.mp4"/>
      <pwccode url="https://github.com/forrestbao/suenes" additional="false">forrestbao/suenes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/billsum">BillSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="176">
      <title>Combating the Curse of Multilinguality in Cross-Lingual <fixed-case>WSD</fixed-case> by Aligning Sparse Contextualized Word Representations</title>
      <author><first>Gábor</first><last>Berend</last></author>
      <pages>2459-2471</pages>
      <abstract>In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.</abstract>
      <url hash="7bb3a339">2022.naacl-main.176</url>
      <bibkey>berend-2022-combating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.176</doi>
      <video href="2022.naacl-main.176.mp4"/>
      <pwccode url="https://github.com/begab/sparsity_makes_sense" additional="false">begab/sparsity_makes_sense</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word2word">word2word</pwcdataset>
    </paper>
    <paper id="177">
      <title>Cheat Codes to Quantify Missing Source Information in Neural Machine Translation</title>
      <author><first>Proyag</first><last>Pal</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>2472-2477</pages>
      <abstract>This paper describes a method to quantify the amount of information <tex-math>H(t|s)</tex-math> added by the target sentence <tex-math>t</tex-math> that is not present in the source <tex-math>s</tex-math> in a neural machine translation system. We do this by providing the model the target sentence in a highly compressed form (a “cheat code”), and exploring the effect of the size of the cheat code. We find that the model is able to capture extra information from just a single float representation of the target and nearly reproduces the target with two 32-bit floats per target token.</abstract>
      <url hash="dfb36df8">2022.naacl-main.177</url>
      <bibkey>pal-heafield-2022-cheat</bibkey>
      <doi>10.18653/v1/2022.naacl-main.177</doi>
      <video href="2022.naacl-main.177.mp4"/>
    </paper>
    <paper id="178">
      <title><fixed-case>W</fixed-case>i<fixed-case>C</fixed-case> = <fixed-case>TSV</fixed-case> = <fixed-case>WSD</fixed-case>: On the Equivalence of Three Semantic Tasks</title>
      <author><first>Bradley</first><last>Hauer</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>2478-2486</pages>
      <abstract>The Word-in-Context (WiC) task has attracted considerable attention in the NLP community, as demonstrated by the popularity of the recent MCL-WiC SemEval shared task. Systems and lexical resources from word sense disambiguation (WSD) are often used for the WiC task and WiC dataset construction. In this paper, we establish the exact relationship between WiC and WSD, as well as the related task of target sense verification (TSV). Building upon a novel hypothesis on the equivalence of sense and meaning distinctions, we demonstrate through the application of tools from theoretical computer science that these three semantic classification problems can be pairwise reduced to each other, and therefore are equivalent. The results of experiments that involve systems and datasets for both WiC and WSD provide strong empirical evidence that our problem reductions work in practice.</abstract>
      <url hash="a0dc41a2">2022.naacl-main.178</url>
      <bibkey>hauer-kondrak-2022-wic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.178</doi>
      <video href="2022.naacl-main.178.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic-tsv">WiC-TSV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="179">
      <title>What do tokens know about their characters and how do they know it?</title>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <pages>2487-2507</pages>
      <abstract>Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require character-level information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for “cat” encodes that it contains the character “a”). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.</abstract>
      <url hash="b15e1db1">2022.naacl-main.179</url>
      <bibkey>kaushal-mahowald-2022-tokens</bibkey>
      <doi>10.18653/v1/2022.naacl-main.179</doi>
      <video href="2022.naacl-main.179.mp4"/>
      <pwccode url="https://github.com/ayushk4/character-probing-pytorch" additional="false">ayushk4/character-probing-pytorch</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
    </paper>
    <paper id="180">
      <title><fixed-case>A</fixed-case>nswer<fixed-case>S</fixed-case>umm: A Manually-Curated Dataset and Pipeline for Answer Summarization</title>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Xiaojian</first><last>Wu</last></author>
      <author><first>Srini</first><last>Iyer</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>2508-2520</pages>
      <abstract>Community Question Answering (CQA) fora such as Stack Overflow and Yahoo! Answers contain a rich resource of answers to a wide range of community-based questions. Each question thread can receive a large number of answers with different perspectives. One goal of answer summarization is to produce a summary that reflects the range of answer perspectives. A major obstacle for this task is the absence of a dataset to provide supervision for producing such summaries. Recent works propose heuristics to create such data, but these are often noisy and do not cover all answer perspectives present. This work introduces a novel dataset of 4,631 CQA threads for answer summarization curated by professional linguists. Our pipeline gathers annotations for all subtasks of answer summarization, including relevant answer sentence selection, grouping these sentences based on perspectives, summarizing each perspective, and producing an overall summary. We analyze and benchmark state-of-the-art models on these subtasks and introduce a novel unsupervised approach for multi-perspective data augmentation that boosts summarization performance according to automatic evaluation. Finally, we propose reinforcement learning rewards to improve factual consistency and answer coverage and analyze areas for improvement.</abstract>
      <url hash="fca315ec">2022.naacl-main.180</url>
      <bibkey>fabbri-etal-2022-answersumm</bibkey>
      <doi>10.18653/v1/2022.naacl-main.180</doi>
      <video href="2022.naacl-main.180.mp4"/>
      <pwccode url="https://github.com/alex-fabbri/answersumm" additional="false">alex-fabbri/answersumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/answersumm">AnswerSumm</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cqasumm">CQASUMM</pwcdataset>
    </paper>
    <paper id="181">
      <title>Paragraph-based Transformer Pre-training for Multi-Sentence Inference</title>
      <author><first>Luca</first><last>Di Liello</last></author>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Luca</first><last>Soldaini</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>2521-2531</pages>
      <abstract>Inference tasks such as answer sentence selection (AS2) or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.</abstract>
      <url hash="c82bbff2">2022.naacl-main.181</url>
      <bibkey>di-liello-etal-2022-paragraph</bibkey>
      <doi>10.18653/v1/2022.naacl-main.181</doi>
      <video href="2022.naacl-main.181.mp4"/>
      <pwccode url="https://github.com/amazon-research/wqa-multi-sentence-inference" additional="false">amazon-research/wqa-multi-sentence-inference</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asnq">ASNQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trecqa">TrecQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="182">
      <title>Text Style Transfer via Optimal Transport</title>
      <author><first>Nasim</first><last>Nouri</last></author>
      <pages>2532-2541</pages>
      <abstract>Text style transfer (TST) is a well-known task whose goal is to convert the style of the text (e.g., from formal to informal) while preserving its content. Recently, it has been shown that both syntactic and semantic similarities between the source and the converted text are important for TST. However, the interaction between these two concepts has not been modeled. In this work, we propose a novel method based on Optimal Transport for TST to simultaneously incorporate syntactic and semantic information into similarity computation between the source and the converted text. We evaluate the proposed method in both supervised and unsupervised settings. Our analysis reveal the superiority of the proposed model in both settings.</abstract>
      <url hash="dd961775">2022.naacl-main.182</url>
      <attachment type="software" hash="82f5ae4f">2022.naacl-main.182.software.zip</attachment>
      <bibkey>nouri-2022-text</bibkey>
      <doi>10.18653/v1/2022.naacl-main.182</doi>
      <video href="2022.naacl-main.182.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="183">
      <title>Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning</title>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>Leonard</first><last>Lausen</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Sheng</first><last>Zha</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <pages>2542-2550</pages>
      <abstract>Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.</abstract>
      <url hash="f1735ef4">2022.naacl-main.183</url>
      <bibkey>padmakumar-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.naacl-main.183</doi>
      <video href="2022.naacl-main.183.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="184">
      <title>Interactive Query-Assisted Summarization via Deep Reinforcement Learning</title>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Yael</first><last>Amsterdamer</last></author>
      <pages>2551-2568</pages>
      <abstract>Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience.</abstract>
      <url hash="3b56dbdc">2022.naacl-main.184</url>
      <bibkey>shapira-etal-2022-interactive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.184</doi>
      <video href="2022.naacl-main.184.mp4"/>
      <pwccode url="https://github.com/orishapira/interexp_deeprl" additional="false">orishapira/interexp_deeprl</pwccode>
    </paper>
    <paper id="185">
      <title>Data Augmentation with Dual Training for Offensive Span Detection</title>
      <author><first>Nasim</first><last>Nouri</last></author>
      <pages>2569-2575</pages>
      <abstract>Recognizing offensive text is an important requirement for every content management system, especially for social networks. While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. One of the challenges to train a model for this novel setting is the lack of enough training data. To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD. In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples. Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.</abstract>
      <url hash="66d5ff3b">2022.naacl-main.185</url>
      <bibkey>nouri-2022-data</bibkey>
      <doi>10.18653/v1/2022.naacl-main.185</doi>
      <video href="2022.naacl-main.185.mp4"/>
    </paper>
    <paper id="186">
      <title>Training Mixed-Domain Translation Models via Federated Learning</title>
      <author><first>Peyman</first><last>Passban</last></author>
      <author><first>Tanya</first><last>Roosta</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Ankit</first><last>Chadha</last></author>
      <author><first>Clement</first><last>Chung</last></author>
      <pages>2576-2586</pages>
      <abstract>Training mixed-domain translation models is a complex task that demands tailored architec- tures and costly data preparation techniques. In this work, we leverage federated learning (FL) in order to tackle the problem. Our investiga- tion demonstrates that with slight modifications in the training process, neural machine trans- lation (NMT) engines can be easily adapted when an FL-based aggregation is applied to fuse different domains. Experimental results also show that engines built via FL are able to perform on par with state-of-the-art baselines that rely on centralized training techniques.We evaluate our hypothesis in the presence of five datasets with different sizes, from different domains, to translate from German into English and discuss how FL and NMT can mutually benefit from each other. In addition to provid- ing benchmarking results on the union of FL and NMT, we also propose a novel technique to dynamically control the communication band- width by selecting impactful parameters during FL updates. This is a significant achievement considering the large size of NMT engines that need to be exchanged between FL parties.</abstract>
      <url hash="4ba52dd5">2022.naacl-main.186</url>
      <bibkey>passban-etal-2022-training</bibkey>
      <doi>10.18653/v1/2022.naacl-main.186</doi>
      <video href="2022.naacl-main.186.mp4"/>
    </paper>
    <paper id="187">
      <title><fixed-case>QAF</fixed-case>act<fixed-case>E</fixed-case>val: Improved <fixed-case>QA</fixed-case>-Based Factual Consistency Evaluation for Summarization</title>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2587-2601</pages>
      <abstract>Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.</abstract>
      <url hash="570d7c30">2022.naacl-main.187</url>
      <bibkey>fabbri-etal-2022-qafacteval</bibkey>
      <doi>10.18653/v1/2022.naacl-main.187</doi>
      <video href="2022.naacl-main.187.mp4"/>
      <pwccode url="https://github.com/salesforce/qafacteval" additional="false">salesforce/qafacteval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa2d">QA2D</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="188">
      <title>How Gender Debiasing Affects Internal Model Representations, and Why It Matters</title>
      <author><first>Hadas</first><last>Orgad</last></author>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>2602-2628</pages>
      <abstract>Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available.</abstract>
      <url hash="5d293c2a">2022.naacl-main.188</url>
      <bibkey>orgad-etal-2022-gender</bibkey>
      <doi>10.18653/v1/2022.naacl-main.188</doi>
      <video href="2022.naacl-main.188.mp4"/>
      <pwccode url="https://github.com/technion-cs-nlp/gender_internal" additional="true">technion-cs-nlp/gender_internal</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="189">
      <title>A Structured Span Selector</title>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Yuchen</first><last>Jiang</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>2629-2641</pages>
      <abstract>Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling; and show improvements on both.</abstract>
      <url hash="3dcc3ded">2022.naacl-main.189</url>
      <bibkey>liu-etal-2022-structured</bibkey>
      <doi>10.18653/v1/2022.naacl-main.189</doi>
      <video href="2022.naacl-main.189.mp4"/>
      <pwccode url="https://github.com/lyutyuh/structured-span-selector" additional="false">lyutyuh/structured-span-selector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
    </paper>
    <paper id="190">
      <title>Unified Semantic Typing with Meaningful Label Inference</title>
      <author><first>James Y.</first><last>Huang</last></author>
      <author><first>Bangzheng</first><last>Li</last></author>
      <author><first>Jiashu</first><last>Xu</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>2642-2654</pages>
      <abstract>Semantic typing aims at classifying tokens or spans of interest in a textual context into semantic categories such as relations, entity types, and event types. The inferred labels of semantic categories meaningfully interpret how machines understand components of text. In this paper, we present UniST, a unified framework for semantic typing that captures label semantics by projecting both inputs and labels into a joint semantic embedding space. To formulate different lexical and relational semantic typing tasks as a unified task, we incorporate task descriptions to be jointly encoded with the input, allowing UniST to be adapted to different tasks without introducing task-specific model components. UniST optimizes a margin ranking loss such that the semantic relatedness of the input and labels is reflected from their embedding similarity. Our experiments demonstrate that UniST achieves strong performance across three semantic typing tasks: entity typing, relation classification and event typing. Meanwhile, UniST effectively transfers semantic knowledge of labels and substantially improves generalizability on inferring rarely seen and unseen types. In addition, multiple semantic typing tasks can be jointly trained within the unified framework, leading to a single compact multi-tasking model that performs comparably to dedicated single-task models, while offering even better transferability.</abstract>
      <url hash="80a98892">2022.naacl-main.190</url>
      <bibkey>huang-etal-2022-unified</bibkey>
      <doi>10.18653/v1/2022.naacl-main.190</doi>
      <video href="2022.naacl-main.190.mp4"/>
      <pwccode url="https://github.com/luka-group/unist" additional="false">luka-group/unist</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="191">
      <title>Learning To Retrieve Prompts for In-Context Learning</title>
      <author><first>Ohad</first><last>Rubin</last></author>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>2655-2671</pages>
      <abstract>In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.</abstract>
      <url hash="7ce00ed1">2022.naacl-main.191</url>
      <bibkey>rubin-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.191</doi>
      <video href="2022.naacl-main.191.mp4"/>
      <pwccode url="https://github.com/ohadrubin/epr" additional="true">ohadrubin/epr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
    </paper>
    <paper id="192">
      <title>Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection</title>
      <author><first>Esma</first><last>Balkir</last></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Kathleen</first><last>Fraser</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last></author>
      <pages>2672-2686</pages>
      <abstract>We present a novel feature attribution method for explaining text classifiers, and analyze it in the context of hate speech detection. Although feature attribution models usually provide a single importance score for each token, we instead provide two complementary and theoretically-grounded scores – necessity and sufficiency – resulting in more informative explanations. We propose a transparent method that calculates these values by generating explicit perturbations of the input text, allowing the importance scores themselves to be explainable. We employ our method to explain the predictions of different hate speech detection models on the same set of curated examples from a test suite, and show that different values of necessity and sufficiency for identity terms correspond to different kinds of false positive errors, exposing sources of classifier bias against marginalized groups.</abstract>
      <url hash="9d2d6b94">2022.naacl-main.192</url>
      <attachment type="software" hash="87845767">2022.naacl-main.192.software.zip</attachment>
      <bibkey>balkir-etal-2022-necessity</bibkey>
      <doi>10.18653/v1/2022.naacl-main.192</doi>
      <video href="2022.naacl-main.192.mp4"/>
      <pwccode url="https://github.com/esmab/necessity-sufficiency" additional="false">esmab/necessity-sufficiency</pwccode>
    </paper>
    <paper id="193">
      <title>Learning to Retrieve Passages without Supervision</title>
      <author><first>Ori</first><last>Ram</last></author>
      <author><first>Gal</first><last>Shachaf</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <author><first>Amir</first><last>Globerson</last></author>
      <pages>2687-2700</pages>
      <abstract>Dense retrievers for open-domain question answering (ODQA) have been shown to achieve impressive performance by training on large datasets of question-passage pairs. In this work we ask whether this dependence on labeled data can be reduced via unsupervised pretraining that is geared towards ODQA. We show this is in fact possible, via a novel pretraining scheme designed for retrieval. Our “recurring span retrieval” approach uses recurring spans across passages in a document to create pseudo examples for contrastive learning. Our pretraining scheme directly controls for term overlap across pseudo queries and relevant passages, thus allowing to model both lexical and semantic relations between them. The resulting model, named Spider, performs surprisingly well without any labeled training examples on a wide range of ODQA datasets. Specifically, it significantly outperforms all other pretrained baselines in a zero-shot setting, and is competitive with BM25, a strong sparse baseline. Moreover, a hybrid retriever over Spider and BM25 improves over both, and is often competitive with DPR models, which are trained on tens of thousands of examples. Last, notable gains are observed when using Spider as an initialization for supervised training.</abstract>
      <url hash="2f5ccc30">2022.naacl-main.193</url>
      <attachment type="software" hash="051e8336">2022.naacl-main.193.software.zip</attachment>
      <bibkey>ram-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.193</doi>
      <video href="2022.naacl-main.193.mp4"/>
      <pwccode url="https://github.com/oriram/spider" additional="false">oriram/spider</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/entityquestions">EntityQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="194">
      <title><fixed-case>R</fixed-case>e2<fixed-case>G</fixed-case>: Retrieve, Rerank, Generate</title>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Gaetano</first><last>Rossiello</last></author>
      <author><first>Md Faisal Mahbub</first><last>Chowdhury</last></author>
      <author><first>Ankita</first><last>Naik</last></author>
      <author><first>Pengshan</first><last>Cai</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <pages>2701-2715</pages>
      <abstract>As demonstrated by GPT-3 and T5, transformers grow in capability as parameter spaces become larger and larger. However, for tasks that require a large amount of knowledge, non-parametric memory allows models to grow dramatically with a sub-linear increase in computational cost and GPU memory requirements. Recent models such as RAG and REALM have introduced retrieval into conditional generation. These models incorporate neural initial retrieval from a corpus of passages. We build on this line of research, proposing Re2G, which combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation. Our reranking approach also permits merging retrieval results from sources with incomparable scores, enabling an ensemble of BM25 and neural initial retrieval. To train our system end-to-end, we introduce a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output. We find large gains in four diverse tasks: zero-shot slot filling, question answering, fact checking and dialog, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard. We make our code available as open source.</abstract>
      <url hash="b2321d67">2022.naacl-main.194</url>
      <bibkey>glass-etal-2022-re2g</bibkey>
      <doi>10.18653/v1/2022.naacl-main.194</doi>
      <video href="2022.naacl-main.194.mp4"/>
      <pwccode url="https://github.com/ibm/kgi-slot-filling" additional="false">ibm/kgi-slot-filling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="195">
      <title>Don’t sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks</title>
      <author><first>Jonathan</first><last>Rusert</last></author>
      <author><first>Padmini</first><last>Srinivasan</last></author>
      <pages>2716-2725</pages>
      <abstract>Deep learning (DL) is being used extensively for text classification. However, researchers have demonstrated the vulnerability of such classifiers to adversarial attacks. Attackers modify the text in a way which misleads the classifier while keeping the original meaning close to intact. State-of-the-art (SOTA) attack algorithms follow the general principle of making minimal changes to the text so as to not jeopardize semantics. Taking advantage of this we propose a novel and intuitive defense strategy called Sample Shielding.It is attacker and classifier agnostic, does not require any reconfiguration of the classifier or external resources and is simple to implement. Essentially, we sample subsets of the input text, classify them and summarize these into a final decision. We shield three popular DL text classifiers with Sample Shielding, test their resilience against four SOTA attackers across three datasets in a realistic threat setting. Even when given the advantage of knowing about our shielding strategy the adversary’s attack success rate is &lt;=10% with only one exception and often &lt; 5%. Additionally, Sample Shielding maintains near original accuracy when applied to original texts. Crucially, we show that the ‘make minimal changes’ approach of SOTA attackers leads to critical vulnerabilities that can be defended against with an intuitive sampling strategy.</abstract>
      <url hash="54833bf7">2022.naacl-main.195</url>
      <attachment type="software" hash="215de7b2">2022.naacl-main.195.software.zip</attachment>
      <bibkey>rusert-srinivasan-2022-dont</bibkey>
      <doi>10.18653/v1/2022.naacl-main.195</doi>
      <video href="2022.naacl-main.195.mp4"/>
      <pwccode url="https://github.com/jonrusert/sampleshielding" additional="false">jonrusert/sampleshielding</pwccode>
    </paper>
    <paper id="196">
      <title>Federated Learning with Noisy User Feedback</title>
      <author><first>Rahul</first><last>Sharma</last></author>
      <author><first>Anil</first><last>Ramakrishna</last></author>
      <author><first>Ansel</first><last>MacLaughlin</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <author><first>Jimit</first><last>Majmudar</last></author>
      <author><first>Clement</first><last>Chung</last></author>
      <author><first>Salman</first><last>Avestimehr</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>2726-2739</pages>
      <abstract>Machine Learning (ML) systems are getting increasingly popular, and drive more and more applications and services in our daily life. Thishas led to growing concerns over user privacy, since human interaction data typically needs to be transmitted to the cloud in order to trainand improve such systems. Federated learning (FL) has recently emerged as a method for training ML models on edge devices using sensitive user data and is seen as a way to mitigate concerns over data privacy. However, since ML models are most commonly trained with label supervision, we need a way to extract labels on edge to make FL viable. In this work, we propose a strategy for training FL models using positive and negative user feedback. We also design a novel framework to study different noise patterns in user feedback, and explore how well standard noise-robust objectives can help mitigate this noise when training models in a federated setting. We evaluate our proposed training setup through detailed experiments on two text classification datasets and analyze the effects of varying levels of user reliability and feedback noise on model performance. We show that our method improves substantially over a self-training baseline, achieving performance closer to models trained with full supervision.</abstract>
      <url hash="665da144">2022.naacl-main.196</url>
      <bibkey>sharma-etal-2022-federated</bibkey>
      <doi>10.18653/v1/2022.naacl-main.196</doi>
      <video href="2022.naacl-main.196.mp4"/>
    </paper>
    <paper id="197">
      <title>Gender Bias in Masked Language Models for Multiple Languages</title>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>2740-2750</pages>
      <abstract>Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages.Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race.Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated.Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators.Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse).We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data.We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages.We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE.The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.</abstract>
      <url hash="2c3d20cc">2022.naacl-main.197</url>
      <bibkey>kaneko-etal-2022-gender</bibkey>
      <doi>10.18653/v1/2022.naacl-main.197</doi>
      <video href="2022.naacl-main.197.mp4"/>
      <pwccode url="https://github.com/kanekomasahiro/bias_eval_in_multiple_mlm" additional="false">kanekomasahiro/bias_eval_in_multiple_mlm</pwccode>
    </paper>
    <paper id="198">
      <title>Multi-Domain Targeted Sentiment Analysis</title>
      <author><first>Orith</first><last>Toledo-Ronen</last></author>
      <author><first>Matan</first><last>Orbach</last></author>
      <author><first>Yoav</first><last>Katz</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>2751-2762</pages>
      <abstract>Targeted Sentiment Analysis (TSA) is a central task for generating insights from consumer reviews. Such content is extremely diverse, with sites like Amazon or Yelp containing reviews on products and businesses from many different domains. A real-world TSA system should gracefully handle that diversity. This can be achieved by a multi-domain model – one that is robust to the domain of the analyzed texts, and performs well on various domains. To address this scenario, we present a multi-domain TSA system based on augmenting a given training set with diverse weak labels from assorted domains. These are obtained through self-training on the Yelp reviews corpus. Extensive experiments with our approach on three evaluation datasets across different domains demonstrate the effectiveness of our solution. We further analyze how restrictions imposed on the available labeled data affect the performance, and compare the proposed method to the costly alternative of manually gathering diverse TSA labeled data. Our results and analysis show that our approach is a promising step towards a practical domain-robust TSA system.</abstract>
      <url hash="34d5f352">2022.naacl-main.198</url>
      <bibkey>toledo-ronen-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.naacl-main.198</doi>
      <video href="2022.naacl-main.198.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yaso">YASO</pwcdataset>
    </paper>
    <paper id="199">
      <title>Falsesum: Generating Document-level <fixed-case>NLI</fixed-case> Examples for Recognizing Factual Inconsistency in Summarization</title>
      <author><first>Prasetya</first><last>Utama</last></author>
      <author><first>Joshua</first><last>Bambrick</last></author>
      <author><first>Nafise</first><last>Moosavi</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>2763-2776</pages>
      <abstract>Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (NLI). However, state-of-the-art NLI models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that NLI models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce Falsesum, a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level NLI datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a Falsesum-augmented NLI dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.</abstract>
      <url hash="05fd4734">2022.naacl-main.199</url>
      <bibkey>utama-etal-2022-falsesum</bibkey>
      <doi>10.18653/v1/2022.naacl-main.199</doi>
      <video href="2022.naacl-main.199.mp4"/>
      <pwccode url="https://github.com/joshbambrick/falsesum" additional="false">joshbambrick/falsesum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="200">
      <title>Dynamic Gazetteer Integration in Multilingual Models for Cross-Lingual and Cross-Domain Named Entity Recognition</title>
      <author><first>Besnik</first><last>Fetahu</last></author>
      <author><first>Anjie</first><last>Fang</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <pages>2777-2790</pages>
      <abstract>Named entity recognition (NER) in a real-world setting remains challenging and is impacted by factors like text genre, corpus quality, and data availability. NER models trained on CoNLL do not transfer well to other domains, even within the same language. This is especially the case for multi-lingual models when applied to low-resource languages, and is mainly due to missing entity information. We propose an approach that with limited effort and data, addresses the NER knowledge gap across languages and domains. Our novel approach uses a token-level gating layer to augment pre-trained multilingual transformers with gazetteers containing named entities (NE) from a target language or domain.This approach provides the flexibility to jointly integrate both textual and gazetteer information dynamically: entity knowledge from gazetteers is used only when a token’s textual representation is insufficient for the NER task.Evaluation on several languages and domains demonstrates: (i) a high mismatch of reported NER performance on CoNLL vs. domain specific datasets, (ii) gazetteers significantly improve NER performance across languages and domains, and (iii) gazetteers can be flexibly incorporated to guide knowledge transfer. On cross-lingual transfer we achieve an improvement over the baseline with F1=+17.6%, and with F1=+21.3% for cross-domain transfer.</abstract>
      <url hash="c7f56069">2022.naacl-main.200</url>
      <bibkey>fetahu-etal-2022-dynamic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.200</doi>
      <video href="2022.naacl-main.200.mp4"/>
    </paper>
    <paper id="201">
      <title><fixed-case>M</fixed-case>eta<fixed-case>ICL</fixed-case>: Learning to Learn In Context</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>2791-2809</pages>
      <abstract>We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task training data, and outperforms much bigger models with nearly 8x parameters.</abstract>
      <url hash="fe97cc8a">2022.naacl-main.201</url>
      <bibkey>min-etal-2022-metaicl</bibkey>
      <doi>10.18653/v1/2022.naacl-main.201</doi>
      <video href="2022.naacl-main.201.mp4"/>
      <pwccode url="https://github.com/facebookresearch/metaicl" additional="false">facebookresearch/metaicl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-instructions">Natural Instructions</pwcdataset>
    </paper>
    <paper id="202">
      <title>Enhancing Knowledge Selection for Grounded Dialogues via Document Semantic Graphs</title>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Mahdi</first><last>Namazifar</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>2810-2823</pages>
      <abstract>Providing conversation models with background knowledge has been shown to make open-domain dialogues more informative and engaging. Existing models treat knowledge selection as a sentence ranking or classification problem where each sentence is handled individually, ignoring the internal semantic connection between sentences. In this work, we propose to automatically convert the background knowledge documents into document semantic graphs and then perform knowledge selection over such graphs. Our document semantic graphs preserve sentence-level information through the use of sentence nodes and provide concept connections between sentences. We apply multi-task learning to perform sentence-level knowledge selection and concept-level knowledge selection, showing that it improves sentence-level selection. Our experiments show that our semantic graph-based knowledge selection improves over sentence selection baselines for both the knowledge selection task and the end-to-end response generation task on HollE and improves generalization on unseen topics in WoW.</abstract>
      <url hash="c72a62f4">2022.naacl-main.202</url>
      <bibkey>li-etal-2022-enhancing-knowledge</bibkey>
      <doi>10.18653/v1/2022.naacl-main.202</doi>
      <video href="2022.naacl-main.202.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="203">
      <title>Using Natural Sentence Prompts for Understanding Biases in Language Models</title>
      <author><first>Sarah</first><last>Alnegheimish</last></author>
      <author><first>Alicia</first><last>Guo</last></author>
      <author><first>Yi</first><last>Sun</last></author>
      <pages>2824-2830</pages>
      <abstract>Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia.We aim to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models. We find bias evaluations are very sensitiveto the design choices of template prompts, and we propose using natural sentence prompts as a way of more systematically using real-world sentences to move away from design decisions that may bias the results.</abstract>
      <url hash="21231b3c">2022.naacl-main.203</url>
      <bibkey>alnegheimish-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.naacl-main.203</doi>
      <video href="2022.naacl-main.203.mp4"/>
    </paper>
    <paper id="204">
      <title>Robust Conversational Agents against Imperceptible Toxicity Triggers</title>
      <author><first>Ninareh</first><last>Mehrabi</last></author>
      <author><first>Ahmad</first><last>Beirami</last></author>
      <author><first>Fred</first><last>Morstatter</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>2831-2847</pages>
      <abstract>Warning: this paper contains content that maybe offensive or upsetting.Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss. In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language. We then propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow. Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy. Lastly, we establish the generalizability of such a defense mechanism on language generation models beyond conversational agents.</abstract>
      <url hash="21302ef2">2022.naacl-main.204</url>
      <bibkey>mehrabi-etal-2022-robust</bibkey>
      <doi>10.18653/v1/2022.naacl-main.204</doi>
      <video href="2022.naacl-main.204.mp4"/>
      <pwccode url="https://github.com/ninarehm/robust-agents" additional="false">ninarehm/robust-agents</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="205">
      <title>Selective Differential Privacy for Language Modeling</title>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Aiqi</first><last>Cui</last></author>
      <author><first>Evan</first><last>Li</last></author>
      <author><first>Ruoxi</first><last>Jia</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>2848-2859</pages>
      <abstract>With the increasing applications of language models, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees.However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens in the data. Given that the private information in natural language is sparse (for example, the bulk of an email might not carry personally identifiable information), we propose a new privacy notion, selective differential privacy, to provide rigorous privacy guarantees on the sensitive portion of the data to improve model utility. To realize such a new notion, we develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based language models. Besides language modeling, we also apply the method to a more concrete application – dialog systems. Experiments on both language modeling and dialog system building show that the proposed privacy-preserving mechanism achieves better utilities while remaining safe under various privacy attacks compared to the baselines. The data and code are released at https://github.com/wyshi/lm_privacy to facilitate future research.</abstract>
      <url hash="4dd1438c">2022.naacl-main.205</url>
      <attachment type="software" hash="1fece6c3">2022.naacl-main.205.software.zip</attachment>
      <bibkey>shi-etal-2022-selective</bibkey>
      <doi>10.18653/v1/2022.naacl-main.205</doi>
      <video href="2022.naacl-main.205.mp4"/>
      <pwccode url="https://github.com/wyshi/lm_privacy" additional="false">wyshi/lm_privacy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="206">
      <title>Do Trajectories Encode Verb Meaning?</title>
      <author><first>Dylan</first><last>Ebert</last></author>
      <author><first>Chen</first><last>Sun</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>2860-2871</pages>
      <abstract>Distributional models learn representations of words from text, but are criticized for their lack of grounding, or the linking of text to the non-linguistic world. Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos, but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur. In this paper, we investigate the extent to which trajectories (i.e. the position and rotation of objects over time) naturally encode verb semantics. We build a procedurally generated agent-object-interaction dataset, obtain human annotations for the verbs that occur in this data, and compare several methods for representation learning given the trajectories. We find that trajectories correlate as-is with some verbs (e.g., fall), and that additional abstraction via self-supervised pretraining can further capture nuanced differences in verb meaning (e.g., roll and slide).</abstract>
      <url hash="d5bd30e7">2022.naacl-main.206</url>
      <bibkey>ebert-etal-2022-trajectories</bibkey>
      <doi>10.18653/v1/2022.naacl-main.206</doi>
      <video href="2022.naacl-main.206.mp4"/>
      <pwccode url="https://github.com/dylanebert/simulated" additional="false">dylanebert/simulated</pwccode>
    </paper>
    <paper id="207">
      <title>Long Context Question Answering via Supervised Contrastive Learning</title>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <author><first>Jacob</first><last>Goldberger</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <pages>2872-2879</pages>
      <abstract>Long-context question answering (QA) tasks require reasoning over a long document or multiple documents. Addressing these tasks often benefits from identifying a set of evidence spans (e.g., sentences), which provide supporting evidence for answering the question.In this work, we propose a novel method for equipping long-context QA models with an additional sequence-level objective for better identification of the supporting evidence.We achieve this via an additional contrastive supervision signal in finetuning, where the model is encouraged to explicitly discriminate supporting evidence sentences from negative ones by maximizing question-evidence similarity. The proposed additional loss exhibits consistent improvements on three different strong long-context transformer models, across two challenging question answering benchmarks – HotpotQA and QAsper.</abstract>
      <url hash="a9ea0ff8">2022.naacl-main.207</url>
      <bibkey>caciularu-etal-2022-long</bibkey>
      <doi>10.18653/v1/2022.naacl-main.207</doi>
      <video href="2022.naacl-main.207.mp4"/>
      <pwccode url="https://github.com/aviclu/long-context-qa-contrast" additional="true">aviclu/long-context-qa-contrast</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasper">QASPER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quality">QuALITY</pwcdataset>
    </paper>
    <paper id="208">
      <title>The <fixed-case>USMLE</fixed-case>® Step 2 Clinical Skills Patient Note Corpus</title>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Janet</first><last>Mee</last></author>
      <author><first>Le</first><last>Ha</last></author>
      <author><first>Polina</first><last>Harik</last></author>
      <author><first>Michael</first><last>Jodoin</last></author>
      <author><first>Alex</first><last>Mechaber</last></author>
      <pages>2880-2886</pages>
      <abstract>This paper presents a corpus of 43,985 clinical patient notes (PNs) written by 35,156 examinees during the high-stakes USMLE® Step 2 Clinical Skills examination. In this exam, examinees interact with standardized patients - people trained to portray simulated scenarios called clinical cases. For each encounter, an examinee writes a PN, which is then scored by physician raters using a rubric of clinical concepts, expressions of which should be present in the PN. The corpus features PNs from 10 clinical cases, as well as the clinical concepts from the case rubrics. A subset of 2,840 PNs were annotated by 10 physician experts such that all 143 concepts from the case rubrics (e.g., shortness of breath) were mapped to 34,660 PN phrases (e.g., dyspnea, difficulty breathing). The corpus is available via a data sharing agreement with NBME and can be requested at https://www.nbme.org/services/data-sharing.</abstract>
      <url hash="c9a0a64d">2022.naacl-main.208</url>
      <bibkey>yaneva-etal-2022-usmle</bibkey>
      <doi>10.18653/v1/2022.naacl-main.208</doi>
      <video href="2022.naacl-main.208.mp4"/>
    </paper>
    <paper id="209">
      <title>Learning to Borrow– Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion</title>
      <author><first>Huda</first><last>Hakami</last></author>
      <author><first>Mona</first><last>Hakami</last></author>
      <author><first>Angrosh</first><last>Mandya</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>2887-2898</pages>
      <abstract>Prior work on integrating text corpora with knowledge graphs (KGs) to improve Knowledge Graph Embedding (KGE) have obtained good performance for entities that co-occur in sentences in text corpora. Such sentences (textual mentions of entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between two entities. However, it is not possible to represent relations between entities that do not co-occur in a single sentence using LDPs. In this paper, we propose and evaluate several methods to address this problem, where we <i>borrow</i> LDPs from the entity pairs that co-occur in sentences in the corpus (i.e. <i>with mentions</i> entity pairs) to represent entity pairs that do <i>not</i> co-occur in any sentence in the corpus (i.e. <i>without mention</i> entity pairs). We propose a supervised borrowing method, <i>SuperBorrow</i>, that learns to score the suitability of an LDP to represent a without-mentions entity pair using pre-trained entity embeddings and contextualised LDP representations. Experimental results show that SuperBorrow improves the link prediction performance of multiple widely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE. </abstract>
      <url hash="f565784c">2022.naacl-main.209</url>
      <bibkey>hakami-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.209</doi>
      <video href="2022.naacl-main.209.mp4"/>
      <pwccode url="https://github.com/huda-hakami/learning-to-borrow-for-kgs" additional="false">huda-hakami/learning-to-borrow-for-kgs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="210">
      <title>Improving Entity Disambiguation by Reasoning over a Knowledge Base</title>
      <author><first>Tom</first><last>Ayoola</last></author>
      <author><first>Joseph</first><last>Fisher</last></author>
      <author><first>Andrea</first><last>Pierleoni</last></author>
      <pages>2899-2912</pages>
      <abstract>Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses state-of-the-art baselines on six well-established ED datasets by 1.3 F1 on average. By allowing access to all KB information, our model is less reliant on popularity-based entity priors, and improves performance on the challenging ShadowLink dataset (which emphasises infrequent and ambiguous entities) by 12.7 F1.</abstract>
      <url hash="2c17196a">2022.naacl-main.210</url>
      <bibkey>ayoola-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.210</doi>
      <video href="2022.naacl-main.210.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aquaint">AQUAINT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="211">
      <title>Modal Dependency Parsing via Language Model Priming</title>
      <author><first>Jiarui</first><last>Yao</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <pages>2913-2919</pages>
      <abstract>The task of modal dependency parsing aims to parse a text into its modal dependency structure, which is a representation for the factuality of events in the text. We design a modal dependency parser that is based on priming pre-trained language models, and evaluate the parser on two data sets. Compared to baselines, we show an improvement of 2.6% in F-score for English and 4.6% for Chinese. To the best of our knowledge, this is also the first work on Chinese modal dependency parsing.</abstract>
      <url hash="d6d9f51f">2022.naacl-main.211</url>
      <bibkey>yao-etal-2022-modal</bibkey>
      <doi>10.18653/v1/2022.naacl-main.211</doi>
      <video href="2022.naacl-main.211.mp4"/>
      <pwccode url="https://github.com/jryao/mdp_prompt" additional="false">jryao/mdp_prompt</pwccode>
    </paper>
    <paper id="212">
      <title>Document-Level Relation Extraction with Sentences Importance Estimation and Focusing</title>
      <author><first>Wang</first><last>Xu</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>2920-2929</pages>
      <abstract>Document-level relation extraction (DocRE) aims to determine the relation between two entities from a document of multiple sentences. Recent studies typically represent the entire document by sequence- or graph-based models to predict the relations of all entity pairs. However, we find that such a model is not robust and exhibits bizarre behaviors: it predicts correctly when an entire test document is fed as input, but errs when non-evidence sentences are removed. To this end, we propose a Sentence Importance Estimation and Focusing (SIEF) framework for DocRE, where we design a sentence importance score and a sentence focusing loss, encouraging DocRE models to focus on evidence sentences. Experimental results on two domains show that our SIEF not only improves overall performance, but also makes DocRE models more robust. Moreover, SIEF is a general framework, shown to be effective when combined with a variety of base DocRE models.</abstract>
      <url hash="416be969">2022.naacl-main.212</url>
      <bibkey>xu-etal-2022-document</bibkey>
      <doi>10.18653/v1/2022.naacl-main.212</doi>
      <video href="2022.naacl-main.212.mp4"/>
      <pwccode url="https://github.com/xwjim/sief" additional="false">xwjim/sief</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogre">DialogRE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="213">
      <title>Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification</title>
      <author><first>Yang</first><last>Xiao</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>2930-2941</pages>
      <abstract>In this paper, we ask the research question of whether all the datasets in the benchmark are necessary. We approach this by first characterizing the distinguishability of datasets when comparing different systems. Experiments on 9 datasets and 36 systems show that several existing benchmark datasets contribute little to discriminating top-scoring systems, while those less used datasets exhibit impressive discriminative power. We further, taking the text classification task as a case study, investigate the possibility of predicting dataset discrimination based on its properties (e.g., average sentence length). Our preliminary experiments promisingly show that given a sufficient number of training experimental records, a meaningful predictor can be learned to estimate dataset discrimination over unseen datasets. We released all datasets with features explored in this work on DataLab.</abstract>
      <url hash="b666e113">2022.naacl-main.213</url>
      <bibkey>xiao-etal-2022-datasets</bibkey>
      <doi>10.18653/v1/2022.naacl-main.213</doi>
      <video href="2022.naacl-main.213.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="214">
      <title>Triggerless Backdoor Attack for <fixed-case>NLP</fixed-case> Tasks with Clean Labels</title>
      <author><first>Leilei</first><last>Gan</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Xiaoya</first><last>Li</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Shangwei</first><last>Guo</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <pages>2942-2952</pages>
      <abstract>Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model; the changed target label leads the example to be mistakenly labeled, and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attack which does not require an external trigger and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.</abstract>
      <url hash="b29da357">2022.naacl-main.214</url>
      <bibkey>gan-etal-2022-triggerless</bibkey>
      <doi>10.18653/v1/2022.naacl-main.214</doi>
      <video href="2022.naacl-main.214.mp4"/>
      <pwccode url="https://github.com/leileigan/clean_label_textual_backdoor_attack" additional="false">leileigan/clean_label_textual_backdoor_attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="215">
      <title><fixed-case>PPL-MCTS</fixed-case>: <fixed-case>C</fixed-case>onstrained Textual Generation Through Discriminator-Guided <fixed-case>MCTS</fixed-case> Decoding</title>
      <author><first>Antoine</first><last>Chaffin</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Ewa</first><last>Kijak</last></author>
      <pages>2953-2967</pages>
      <abstract>Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically.We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.</abstract>
      <url hash="c5f9e51f">2022.naacl-main.215</url>
      <bibkey>chaffin-etal-2022-ppl</bibkey>
      <doi>10.18653/v1/2022.naacl-main.215</doi>
      <video href="2022.naacl-main.215.mp4"/>
      <pwccode url="https://github.com/NohTow/PPL-MCTS" additional="false">NohTow/PPL-MCTS</pwccode>
    </paper>
    <paper id="216">
      <title>Interpretable Proof Generation via Iterative Backward Reasoning</title>
      <author><first>Hanhao</first><last>Qu</last></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>2968-2981</pages>
      <abstract>We present IBR, an Iterative Backward Reasoning model to solve the proof generation tasks on rule-based Question Answering (QA), where models are required to reason over a series of textual rules and facts to find out the related proof path and derive the final answer. We handle the limitations of existed works in two folds: 1) enhance the interpretability of reasoning procedures with detailed tracking, by predicting nodes and edges in the proof path iteratively backward from the question; 2) promote the efficiency and accuracy via reasoning on the elaborate representations of nodes and history paths, without any intermediate texts that may introduce external noise during proof generation. There are three main modules in IBR, QA and proof strategy prediction to obtain the answer and offer guidance for the following procedure; parent node prediction to determine a node in the existing proof that a new child node will link to; child node prediction to find out which new node will be added to the proof. Experiments on both synthetic and paraphrased datasets demonstrate that IBR has better in-domain performance as well as cross-domain transferability than several strong baselines. Our code and models are available at https://github. com/find-knowledge/IBR.</abstract>
      <url hash="c60dee4d">2022.naacl-main.216</url>
      <bibkey>qu-etal-2022-interpretable</bibkey>
      <doi>10.18653/v1/2022.naacl-main.216</doi>
      <video href="2022.naacl-main.216.mp4"/>
      <pwccode url="https://github.com/find-knowledge/ibr" additional="false">find-knowledge/ibr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
    </paper>
    <paper id="217">
      <title>Domain Confused Contrastive Learning for Unsupervised Domain Adaptation</title>
      <author><first>Quanyu</first><last>Long</last></author>
      <author><first>Tianze</first><last>Luo</last></author>
      <author><first>Wenya</first><last>Wang</last></author>
      <author><first>Sinno</first><last>Pan</last></author>
      <pages>2982-2995</pages>
      <abstract>In this work, we study Unsupervised Domain Adaptation (UDA) in a challenging self-supervised approach. One of the difficulties is how to learn task discrimination in the absence of target labels. Unlike previous literature which directly aligns cross-domain distributions or leverages reverse gradient, we propose Domain Confused Contrastive Learning (DCCL), which can bridge the source and target domains via domain puzzles, and retain discriminative representations after adaptation. Technically, DCCL searches for a most domain-challenging direction and exquisitely crafts domain confused augmentations as positive pairs, then it contrastively encourages the model to pull representations towards the other domain, thus learning more stable and effective domain invariances. We also investigate whether contrastive learning necessarily helps with UDA when performing other data augmentations. Extensive experiments demonstrate that DCCL significantly outperforms baselines, further ablation study and analysis also show the effectiveness and availability of DCCL.</abstract>
      <url hash="20af2c89">2022.naacl-main.217</url>
      <bibkey>long-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.naacl-main.217</doi>
      <video href="2022.naacl-main.217.mp4"/>
    </paper>
    <paper id="218">
      <title>Incorporating Centering Theory into Neural Coreference Resolution</title>
      <author><first>Haixia</first><last>Chai</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <pages>2996-3002</pages>
      <abstract>In recent years, transformer-based coreference resolution systems have achieved remarkable improvements on the CoNLL dataset. However, how coreference resolvers can benefit from discourse coherence is still an open question. In this paper, we propose to incorporate centering transitions derived from centering theory in the form of a graph into a neural coreference model. Our method improves the performance over the SOTA baselines, especially on pronoun resolution in long documents, formal well-structured text, and clusters with scattered mentions.</abstract>
      <url hash="180bc9d0">2022.naacl-main.218</url>
      <attachment type="software" hash="e6b2e048">2022.naacl-main.218.software.zip</attachment>
      <bibkey>chai-strube-2022-incorporating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.218</doi>
      <video href="2022.naacl-main.218.mp4"/>
      <pwccode url="https://github.com/haixiachai/ct-coref" additional="false">haixiachai/ct-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="219">
      <title>Progressive Class Semantic Matching for Semi-supervised Text Classification</title>
      <author><first>Haiming</first><last>Xu</last></author>
      <author><first>Lingqiao</first><last>Liu</last></author>
      <author><first>Ehsan</first><last>Abbasnejad</last></author>
      <pages>3003-3013</pages>
      <abstract>Semi-supervised learning is a promising way to reduce the annotation cost for text-classification. Combining with pre-trained language models (PLMs), e.g., BERT, recent semi-supervised learning methods achieved impressive performance. In this work, we further investigate the marriage between semi-supervised learning and a pre-trained language model. Unlike existing approaches that utilize PLMs only for model parameter initialization, we explore the inherent topic matching capability inside PLMs for building a more powerful semi-supervised learning approach. Specifically, we propose a joint semi-supervised learning process that can progressively build a standard <tex-math>K</tex-math>-way classifier and a matching network for the input text and the Class Semantic Representation (CSR). The CSR will be initialized from the given labeled sentences and progressively updated through the training process. By means of extensive experiments, we show that our method can not only bring remarkable improvement to baselines, but also overall be more stable, and achieves state-of-the-art performance in semi-supervised text classification.</abstract>
      <url hash="3c492f58">2022.naacl-main.219</url>
      <bibkey>xu-etal-2022-progressive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.219</doi>
      <video href="2022.naacl-main.219.mp4"/>
      <pwccode url="https://github.com/heimingx/pcm" additional="false">heimingx/pcm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="220">
      <title>Low Resource Style Transfer via Domain Adaptive Meta Learning</title>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Xiang</first><last>Long</last></author>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>3014-3026</pages>
      <abstract>Text style transfer (TST) without parallel data has achieved some practical success. However, most of the existing unsupervised text style transfer methods suffer from (i) requiring massive amounts of non-parallel data to guide transferring different text styles. (ii) colossal performance degradation when fine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain Adaptive Meta-Learning with Adversarial Transfer Model), which consists of two parts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn general knowledge in multiple heterogeneous source domains, capable of adapting to new unseen domains with a small amount of data.Moreover, we propose a new unsupervised TST approach Adversarial Transfer Model (ATM), composed of a sequence-to-sequence pre-trained language model and uses adversarial style training for better content preservation and style transfer.Results on multi-domain datasets demonstrate that our approach generalizes well on unseen low-resource domains, achieving state-of-the-art results against ten strong baselines.</abstract>
      <url hash="4b4b5550">2022.naacl-main.220</url>
      <bibkey>li-etal-2022-low</bibkey>
      <doi>10.18653/v1/2022.naacl-main.220</doi>
      <video href="2022.naacl-main.220.mp4"/>
    </paper>
    <paper id="221">
      <title>Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection</title>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>3027-3040</pages>
      <abstract>Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective data-centric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data statements practice.</abstract>
      <url hash="80b763a5">2022.naacl-main.221</url>
      <bibkey>ramponi-tonelli-2022-features</bibkey>
      <doi>10.18653/v1/2022.naacl-main.221</doi>
      <video href="2022.naacl-main.221.mp4"/>
      <pwccode url="https://github.com/dhfbk/hate-speech-artifacts" additional="false">dhfbk/hate-speech-artifacts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="222">
      <title>Document-Level Event Argument Extraction by Leveraging Redundant Information and Closed Boundary Loss</title>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Kezhi</first><last>Mao</last></author>
      <pages>3041-3052</pages>
      <abstract>In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class “others”, i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper, to make use of redundant event information underlying a document, we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class, we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35% in F1-score.</abstract>
      <url hash="7dad8e6c">2022.naacl-main.222</url>
      <bibkey>zhou-mao-2022-document</bibkey>
      <doi>10.18653/v1/2022.naacl-main.222</doi>
      <video href="2022.naacl-main.222.mp4"/>
    </paper>
    <paper id="223">
      <title>A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for <fixed-case>A</fixed-case>frican News Translation</title>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Jesujoba</first><last>Alabi</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Peter</first><last>Nabende</last></author>
      <author><first>Ernie</first><last>Chang</last></author>
      <author><first>Tajuddeen</first><last>Gwadabe</last></author>
      <author><first>Freshia</first><last>Sackey</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Chris</first><last>Emezue</last></author>
      <author><first>Colin</first><last>Leong</last></author>
      <author><first>Michael</first><last>Beukman</last></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last></author>
      <author><first>Guyo</first><last>Jarso</last></author>
      <author><first>Oreen</first><last>Yousuf</last></author>
      <author><first>Andre</first><last>Niyongabo Rubungo</last></author>
      <author><first>Gilles</first><last>Hacheme</last></author>
      <author><first>Eric Peter</first><last>Wairagala</last></author>
      <author><first>Muhammad Umair</first><last>Nasir</last></author>
      <author><first>Benjamin</first><last>Ajibade</last></author>
      <author><first>Tunde</first><last>Ajayi</last></author>
      <author><first>Yvonne</first><last>Gitau</last></author>
      <author><first>Jade</first><last>Abbott</last></author>
      <author><first>Mohamed</first><last>Ahmed</last></author>
      <author><first>Millicent</first><last>Ochieng</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Perez</first><last>Ogayo</last></author>
      <author><first>Jonathan</first><last>Mukiibi</last></author>
      <author><first>Fatoumata</first><last>Ouoba Kabore</last></author>
      <author><first>Godson</first><last>Kalipe</last></author>
      <author><first>Derguene</first><last>Mbaye</last></author>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Victoire</first><last>Memdjokam Koagne</last></author>
      <author><first>Edwin</first><last>Munkoh-Buabeng</last></author>
      <author><first>Valencia</first><last>Wagner</last></author>
      <author><first>Idris</first><last>Abdulmumin</last></author>
      <author><first>Ayodele</first><last>Awokoya</last></author>
      <author><first>Happy</first><last>Buzaaba</last></author>
      <author><first>Blessing</first><last>Sibanda</last></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Sam</first><last>Manthalu</last></author>
      <pages>3053-3070</pages>
      <abstract>Recent advances in the pre-training for language models leverage large-scale datasets to create multilingual models. However, low-resource languages are mostly left out in these datasets. This is primarily because many widely spoken languages that are not well represented on the web and therefore excluded from the large-scale crawls for datasets. Furthermore, downstream users of these models are restricted to the selection of languages originally chosen for pre-training. This work investigates how to optimally leverage existing pre-trained models to create low-resource translation systems for 16 African languages. We focus on two questions: 1) How can pre-trained models be used for languages not included in the initial pretraining? and 2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we create a novel African news corpus covering 16 languages, of which eight languages are not part of any existing evaluation dataset. We demonstrate that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.</abstract>
      <url hash="6726dbb0">2022.naacl-main.223</url>
      <bibkey>adelani-etal-2022-thousand</bibkey>
      <doi>10.18653/v1/2022.naacl-main.223</doi>
      <video href="2022.naacl-main.223.mp4"/>
      <pwccode url="https://github.com/masakhane-io/lafand-mt" additional="false">masakhane-io/lafand-mt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="224">
      <title>Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis</title>
      <author><first>Yiwei</first><last>Wang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Yujun</first><last>Cai</last></author>
      <author><first>Yuxuan</first><last>Liang</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Juncheng</first><last>Liu</last></author>
      <author><first>Bryan</first><last>Hooi</last></author>
      <pages>3071-3081</pages>
      <abstract>Recent literature focuses on utilizing the entity information in the sentence-level relation extraction (RE), but this risks leaking superficial and spurious clues of relations. As a result, RE still suffers from unintended entity bias, i.e., the spurious correlation between entity mentions (names) and relations. Entity bias can mislead the RE models to extract the relations that do not exist in the text. To combat this issue, some previous work masks the entity mentions to prevent the RE models from over-fitting entity mentions. However, this strategy degrades the RE performance because it loses the semantic information of entities. In this paper, we propose the CoRE (Counterfactual Analysis based Relation Extraction) debiasing method that guides the RE models to focus on the main effects of textual context without losing the entity information. We first construct a causal graph for RE, which models the dependencies between variables in RE models. Then, we propose to conduct counterfactual analysis on our causal graph to distill and mitigate the entity bias, that captures the causal effects of specific entity mentions in each instance. Note that our CoRE method is model-agnostic to debias existing RE systems during inference without changing their training processes. Extensive experimental results demonstrate that our CoRE yields significant gains on both effectiveness and generalization for RE. The source code is provided at: https://github.com/vanoracai/CoRE.</abstract>
      <url hash="cc56e225">2022.naacl-main.224</url>
      <bibkey>wang-etal-2022-rely</bibkey>
      <doi>10.18653/v1/2022.naacl-main.224</doi>
      <video href="2022.naacl-main.224.mp4"/>
      <pwccode url="https://github.com/vanoracai/core" additional="false">vanoracai/core</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/re-tacred">Re-TACRED</pwcdataset>
    </paper>
    <paper id="225">
      <title>Analyzing Encoded Concepts in Transformer Language Models</title>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <author><first>Abdul</first><last>Khan</last></author>
      <author><first>Jia</first><last>Xu</last></author>
      <pages>3082-3101</pages>
      <abstract>We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.</abstract>
      <url hash="ffb80af2">2022.naacl-main.225</url>
      <attachment type="software" hash="5d6c4088">2022.naacl-main.225.software.zip</attachment>
      <bibkey>sajjad-etal-2022-analyzing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.225</doi>
      <video href="2022.naacl-main.225.mp4"/>
      <pwccode url="https://github.com/hsajjad/conceptx" additional="false">hsajjad/conceptx</pwccode>
    </paper>
    <paper id="226">
      <title>Boosted Dense Retriever</title>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Barlas</first><last>Oguz</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>3102-3117</pages>
      <abstract>We propose DrBoost, a dense retrieval ensemble inspired by boosting. DrBoost is trained in stages: each component model is learned sequentially and specialized by focusing only on retrieval mistakes made by the current ensemble. The final representation is the concatenation of the output vectors of all the component models, making it a drop-in replacement for standard dense retrievers at test time. DrBoost enjoys several advantages compared to standard dense retrieval models. It produces representations which are 4x more compact, while delivering comparable retrieval results. It also performs surprisingly well under approximate search with coarse quantization, reducing latency and bandwidth needs by another 4x. In practice, this can make the difference between serving indices from disk versus from memory, paving the way for much cheaper deployments.</abstract>
      <url hash="44a4375c">2022.naacl-main.226</url>
      <bibkey>lewis-etal-2022-boosted</bibkey>
      <doi>10.18653/v1/2022.naacl-main.226</doi>
      <video href="2022.naacl-main.226.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="227">
      <title><fixed-case>M</fixed-case>u<fixed-case>CGEC</fixed-case>: a Multi-Reference Multi-Source Evaluation Dataset for <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Zuyi</first><last>Bao</last></author>
      <author><first>Jiacheng</first><last>Li</last></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3118-3130</pages>
      <abstract>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at https://github.com/HillZhang1999/MuCGEC.</abstract>
      <url hash="f7aad9d0">2022.naacl-main.227</url>
      <bibkey>zhang-etal-2022-mucgec</bibkey>
      <doi>10.18653/v1/2022.naacl-main.227</doi>
      <video href="2022.naacl-main.227.mp4"/>
      <pwccode url="https://github.com/hillzhang1999/mucgec" additional="false">hillzhang1999/mucgec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mucgec">MuCGEC</pwcdataset>
    </paper>
    <paper id="228">
      <title><fixed-case>N</fixed-case>eu<fixed-case>S</fixed-case>: Neutral Multi-News Summarization for Mitigating Framing Bias</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>3131-3148</pages>
      <abstract>Media news framing bias can increase political polarization and undermine civil society. The need for automatic mitigation methods is therefore growing. We propose a new task, a neutral summary generation from multiple news articles of the varying political leaningsto facilitate balanced and unbiased news reading.In this paper, we first collect a new dataset, illustrate insights about framing bias through a case study, and propose a new effective metric and model (NeuS-Title) for the task. Based on our discovery that title provides a good signal for framing bias, we present NeuS-Title that learns to neutralize news content in hierarchical order from title to article. Our hierarchical multi-task learning is achieved by formatting our hierarchical data pair (title, article) sequentially with identifier-tokens (“TITLE=&gt;”, “ARTICLE=&gt;”) and fine-tuning the auto-regressive decoder with the standard negative log-likelihood objective.We then analyze and point out the remaining challenges and future directions. One of the most interesting observations is that neural NLG models can hallucinate not only factually inaccurate or unverifiable content but also politically biased content.</abstract>
      <url hash="bf818692">2022.naacl-main.228</url>
      <bibkey>lee-etal-2022-neus</bibkey>
      <doi>10.18653/v1/2022.naacl-main.228</doi>
      <video href="2022.naacl-main.228.mp4"/>
      <pwccode url="https://github.com/hltchkust/framing-bias-metric" additional="false">hltchkust/framing-bias-metric</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="229">
      <title>Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation</title>
      <author><first>Shumpei</first><last>Inoue</last></author>
      <author><first>Tsungwei</first><last>Liu</last></author>
      <author><first>Son</first><last>Nguyen</last></author>
      <author><first>Minh-Tien</first><last>Nguyen</last></author>
      <pages>3149-3158</pages>
      <abstract>This paper introduces a model for incomplete utterance restoration (IUR) called JET (Joint learning token Extraction and Text generation). Different from prior studies that only work on extraction or abstraction datasets, we design a simple but effective model, working for both scenarios of IUR. Our design simulates the nature of IUR, where omitted tokens from the context contribute to restoration. From this, we construct a Picker that identifies the omitted tokens. To support the picker, we design two label creation methods (soft and hard labels), which can work in cases of no annotation data for the omitted tokens. The restoration is done by using a Generator with the help of the Picker on joint learning. Promising results on four benchmark datasets in extraction and abstraction scenarios show that our model is better than the pretrained T5 and non-generative language model methods in both rich and limited training data settings.</abstract>
      <url hash="90ce45bc">2022.naacl-main.229</url>
      <bibkey>inoue-etal-2022-enhance</bibkey>
      <doi>10.18653/v1/2022.naacl-main.229</doi>
      <video href="2022.naacl-main.229.mp4"/>
      <pwccode url="https://github.com/shumpei19/jet" additional="false">shumpei19/jet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
    </paper>
    <paper id="230">
      <title>Efficient Constituency Tree based Encoding for Natural Language to Bash Translation</title>
      <author><first>Shikhar</first><last>Bharadwaj</last></author>
      <author><first>Shirish</first><last>Shevade</last></author>
      <pages>3159-3168</pages>
      <abstract>Bash is a Unix command language used for interacting with the Operating System. Recent works on natural language to Bash translation have made significant advances, but none of the previous methods utilize the problem’s inherent structure. We identify this structure andpropose a Segmented Invocation Transformer (SIT) that utilizes the information from the constituency parse tree of the natural language text. Our method is motivated by the alignment between segments in the natural language text and Bash command components. Incorporating the structure in the modelling improves the performance of the model. Since such systems must be universally accessible, we benchmark the inference times on a CPU rather than a GPU. We observe a 1.8x improvement in the inference time and a 5x reduction in model parameters. Attribution analysis using Integrated Gradients reveals that the proposed method can capture the problem structure.</abstract>
      <url hash="185a6d0f">2022.naacl-main.230</url>
      <bibkey>bharadwaj-shevade-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.naacl-main.230</doi>
      <video href="2022.naacl-main.230.mp4"/>
      <pwccode url="https://github.com/shikhar-s/segmented-invocation-transformer" additional="false">shikhar-s/segmented-invocation-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nlc2cmd">NLC2CMD</pwcdataset>
    </paper>
    <paper id="231">
      <title>Privacy-Preserving Text Classification on <fixed-case>BERT</fixed-case> Embeddings with Homomorphic Encryption</title>
      <author><first>Garam</first><last>Lee</last></author>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Jai Hyun</first><last>Park</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Jung Hee</first><last>Cheon</last></author>
      <pages>3169-3175</pages>
      <abstract>Embeddings, which compress information in raw text into semantics-preserving low-dimensional vectors, have been widely adopted for their efficacy. However, recent research has shown that embeddings can potentially leak private information about sensitive attributes of the text, and in some cases, can be inverted to recover the original input text. To address these growing privacy challenges, we propose a privatization mechanism for embeddings based on homomorphic encryption, to prevent potential leakage of any piece of information in the process of text classification. In particular, our method performs text classification on the encryption of embeddings from state-of-the-art models like BERT, supported by an efficient GPU implementation of CKKS encryption scheme. We show that our method offers encrypted protection of BERT embeddings, while largely preserving their utility on downstream text classification tasks.</abstract>
      <url hash="bdf0d4a2">2022.naacl-main.231</url>
      <attachment type="software" hash="ef8b3925">2022.naacl-main.231.software.zip</attachment>
      <bibkey>lee-etal-2022-privacy</bibkey>
      <doi>10.18653/v1/2022.naacl-main.231</doi>
      <video href="2022.naacl-main.231.mp4"/>
    </paper>
    <paper id="232">
      <title><fixed-case>ITA</fixed-case>: Image-Text Alignments for Multi-Modal Named Entity Recognition</title>
      <author><first>Xinyu</first><last>Wang</last></author>
      <author><first>Min</first><last>Gui</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Zixia</first><last>Jia</last></author>
      <author><first>Nguyen</first><last>Bach</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>3176-3189</pages>
      <abstract>Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of attention. Most of the work utilizes image information through region-level visual representations obtained from a pretrained object detector and relies on an attention mechanism to model the interactions between image and text representations. However, it is difficult to model such interactions as image and text representations are trained separately on the data of their respective modality and are not aligned in the same space. As text representations take the most important role in MNER, in this paper, we propose <b>I</b>mage-<b>t</b>ext <b>A</b>lignments (ITA) to align image features into the textual space, so that the attention mechanism in transformer-based pretrained textual embeddings can be better utilized. ITA first aligns the image into regional object tags, image-level captions and optical characters as visual contexts, concatenates them with the input texts as a new cross-modal input, and then feeds it into a pretrained textual embedding model. This makes it easier for the attention module of a pretrained textual embedding model to model the interaction between the two modalities since they are both represented in the textual space. ITA further aligns the output distributions predicted from the cross-modal input and textual input views so that the MNER model can be more practical in dealing with text-only inputs and robust to noises from images. In our experiments, we show that ITA models can achieve state-of-the-art accuracy on multi-modal Named Entity Recognition datasets, even without image information.</abstract>
      <url hash="a0b84d99">2022.naacl-main.232</url>
      <bibkey>wang-etal-2022-ita</bibkey>
      <doi>10.18653/v1/2022.naacl-main.232</doi>
      <video href="2022.naacl-main.232.mp4"/>
      <pwccode url="https://github.com/alibaba-nlp/kb-ner" additional="false">alibaba-nlp/kb-ner</pwccode>
    </paper>
    <paper id="233">
      <title>A Dataset for N-ary Relation Extraction of Drug Combinations</title>
      <author><first>Aryeh</first><last>Tiktinsky</last></author>
      <author><first>Vijay</first><last>Viswanathan</last></author>
      <author><first>Danna</first><last>Niezni</last></author>
      <author><first>Dana</first><last>Meron Azagury</last></author>
      <author><first>Yosi</first><last>Shamay</last></author>
      <author><first>Hillel</first><last>Taub-Tabib</last></author>
      <author><first>Tom</first><last>Hope</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>3190-3203</pages>
      <abstract>Combination therapies have become the standard of care for diseases such as cancer, tuberculosis, malaria and HIV. However, the combinatorial set of available multi-drug treatments creates a challenge in identifying effective combination therapies available in a situation.To assist medical professionals in identifying beneficial drug-combinations, we construct an expert-annotated dataset for extracting information about the efficacy of drug combinations from the scientific literature. Beyond its practical utility, the dataset also presents a unique NLP challenge, as the first relation extraction dataset consisting of variable-length relations. Furthermore, the relations in this dataset predominantly require language understanding beyond the sentence level, adding to the challenge of this task. We provide a promising baseline model and identify clear areas for further improvement. We release our dataset (https://huggingface.co/datasets/allenai/drug-combo-extraction), code (https://github.com/allenai/drug-combo-extraction) and baseline models (https://huggingface.co/allenai/drug-combo-classifier-pubmedbert-dapt) publicly to encourage the NLP community to participate in this task.</abstract>
      <url hash="947486d6">2022.naacl-main.233</url>
      <bibkey>tiktinsky-etal-2022-dataset</bibkey>
      <doi>10.18653/v1/2022.naacl-main.233</doi>
      <video href="2022.naacl-main.233.mp4"/>
      <pwccode url="https://github.com/allenai/drug-combo-extraction" additional="true">allenai/drug-combo-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
    </paper>
    <paper id="234">
      <title>Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding</title>
      <author><first>Zeming</first><last>Chen</last></author>
      <author><first>Qiyue</first><last>Gao</last></author>
      <pages>3204-3219</pages>
      <abstract>In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.</abstract>
      <url hash="f20c920c">2022.naacl-main.234</url>
      <bibkey>chen-gao-2022-curriculum</bibkey>
      <doi>10.18653/v1/2022.naacl-main.234</doi>
      <video href="2022.naacl-main.234.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="235">
      <title>Neural Language Taskonomy: Which <fixed-case>NLP</fixed-case> Tasks are the most Predictive of f<fixed-case>MRI</fixed-case> Brain Activity?</title>
      <author><first>Subba Reddy</first><last>Oota</last></author>
      <author><first>Jashn</first><last>Arora</last></author>
      <author><first>Veeral</first><last>Agarwal</last></author>
      <author><first>Mounika</first><last>Marreddy</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Bapi</first><last>Surampudi</last></author>
      <pages>3220-3237</pages>
      <abstract>Several popular Transformer based language models have been found to be successful for text-driven brain encoding. However, existing literature leverages only pretrained text Transformer models and has not explored the efficacy of task-specific learned Transformer representations. In this work, we explore transfer learning from representations learned for ten popular natural language processing tasks (two syntactic and eight semantic) for predicting brain responses from two diverse datasets: Pereira (subjects reading sentences from paragraphs) and Narratives (subjects listening to the spoken stories). Encoding models based on task features are used to predict activity in different regions across the whole brain. Features from coreference resolution, NER, and shallow syntax parsing explain greater variance for the reading activity. On the other hand, for the listening activity, tasks such as paraphrase generation, summarization, and natural language inference show better encoding performance. Experiments across all 10 task representations provide the following cognitive insights: (i) language left hemisphere has higher predictive brain activity versus language right hemisphere, (ii) posterior medial cortex, temporo-parieto-occipital junction, dorsal frontal lobe have higher correlation versus early auditory and auditory association cortex, (iii) syntactic and semantic tasks display a good predictive performance across brain regions for reading and listening stimuli resp.</abstract>
      <url hash="648c3290">2022.naacl-main.235</url>
      <attachment type="software" hash="5cfabadb">2022.naacl-main.235.software.zip</attachment>
      <bibkey>oota-etal-2022-neural</bibkey>
      <doi>10.18653/v1/2022.naacl-main.235</doi>
      <video href="2022.naacl-main.235.mp4"/>
    </paper>
    <paper id="236">
      <title><fixed-case>F</fixed-case>act<fixed-case>G</fixed-case>raph: Evaluating Factuality in Summarization with Semantic Graph Representations</title>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Mengwen</first><last>Liu</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Markus</first><last>Dreyer</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>3238-3253</pages>
      <abstract>Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.</abstract>
      <url hash="6a1b1a56">2022.naacl-main.236</url>
      <attachment type="software" hash="4a70a8bf">2022.naacl-main.236.software.zip</attachment>
      <bibkey>ribeiro-etal-2022-factgraph</bibkey>
      <doi>10.18653/v1/2022.naacl-main.236</doi>
      <video href="2022.naacl-main.236.mp4"/>
      <pwccode url="https://github.com/amazon-research/fact-graph" additional="false">amazon-research/fact-graph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="237">
      <title>Unsupervised Paraphrasability Prediction for Compound Nominalizations</title>
      <author><first>John Sie Yuen</first><last>Lee</last></author>
      <author><first>Ho Hung</first><last>Lim</last></author>
      <author><first>Carol</first><last>Webster</last></author>
      <pages>3254-3263</pages>
      <abstract>Commonly found in academic and formal texts, a nominalization uses a deverbal noun to describe an event associated with its corresponding verb. Nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Automatic generation of clausal paraphrases for nominalizations can help disambiguate their meaning. However, previous work has not identified cases where it is awkward or impossible to paraphrase a compound nominalization. This paper investigates unsupervised prediction of paraphrasability, which determines whether the prenominal modifier of a nominalization can be re-written as a noun or adverb in a clausal paraphrase. We adopt the approach of overgenerating candidate paraphrases followed by candidate ranking with a neural language model. In experiments on an English dataset, we show that features from an Abstract Meaning Representation graph lead to statistically significant improvement in both paraphrasability prediction and paraphrase generation.</abstract>
      <url hash="b5505126">2022.naacl-main.237</url>
      <bibkey>lee-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.naacl-main.237</doi>
      <video href="2022.naacl-main.237.mp4"/>
    </paper>
    <paper id="238">
      <title>Global Entity Disambiguation with <fixed-case>BERT</fixed-case></title>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Koki</first><last>Washio</last></author>
      <author><first>Hiroyuki</first><last>Shindo</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>3264-3271</pages>
      <abstract>We propose a global entity disambiguation (ED) model based on BERT. To capture global contextual information for ED, our model treats not only words but also entities as input tokens, and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step. We train the model using a large entity-annotated corpus obtained from Wikipedia. We achieve new state-of-the-art results on five standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The source code and model checkpoint are available at https://github.com/studio-ousia/luke.</abstract>
      <url hash="4e2e4869">2022.naacl-main.238</url>
      <bibkey>yamada-etal-2022-global</bibkey>
      <doi>10.18653/v1/2022.naacl-main.238</doi>
      <video href="2022.naacl-main.238.mp4"/>
      <pwccode url="https://github.com/studio-ousia/luke" additional="false">studio-ousia/luke</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aquaint">AQUAINT</pwcdataset>
    </paper>
    <paper id="239">
      <title>Clues Before Answers: Generation-Enhanced Multiple-Choice <fixed-case>QA</fixed-case></title>
      <author><first>Zixian</first><last>Huang</last></author>
      <author><first>Ao</first><last>Wu</last></author>
      <author><first>Jiaying</first><last>Zhou</last></author>
      <author><first>Yu</first><last>Gu</last></author>
      <author><first>Yue</first><last>Zhao</last></author>
      <author><first>Gong</first><last>Cheng</last></author>
      <pages>3272-3287</pages>
      <abstract>A trending paradigm for multiple-choice question answering (MCQA) is using a text-to-text framework. By unifying data in different tasks into a single text-to-text format, it trains a generative encoder-decoder model which is both powerful and universal. However, a side effect of twisting a generation target to fit the classification nature of MCQA is the under-utilization of the decoder and the knowledge that can be decoded. To exploit the generation capability and underlying knowledge of a pre-trained encoder-decoder model, in this paper, we propose a generation-enhanced MCQA model named GenMC. It generates a clue from the question and then leverages the clue to enhance a reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.</abstract>
      <url hash="83d965d7">2022.naacl-main.239</url>
      <bibkey>huang-etal-2022-clues</bibkey>
      <doi>10.18653/v1/2022.naacl-main.239</doi>
      <video href="2022.naacl-main.239.mp4"/>
      <pwccode url="https://github.com/nju-websoft/genmc" additional="false">nju-websoft/genmc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
    </paper>
    <paper id="240">
      <title>Towards Efficient <fixed-case>NLP</fixed-case>: A Standard Evaluation and A Strong Baseline</title>
      <author><first>Xiangyang</first><last>Liu</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Junliang</first><last>He</last></author>
      <author><first>Jiawen</first><last>Wu</last></author>
      <author><first>Lingling</first><last>Wu</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Jiang</last></author>
      <author><first>Zhao</first><last>Cao</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>3288-3303</pages>
      <abstract>Supersized pre-trained language models have pushed the accuracy of various natural language processing (NLP) tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless SOTA accuracy, more and more researchers start paying attention to model efficiency and usability. Different from accuracy, the metric for efficiency varies across different studies, making them hard to be fairly compared. To that end, this work presents ELUE (Efficient Language Understanding Evaluation), a standard evaluation, and a public leaderboard for efficient NLP models. ELUE is dedicated to depicting the Pareto Frontier for various language understanding tasks, such that it can tell whether and how much a method achieves Pareto improvement. Along with the benchmark, we also release a strong baseline, ElasticBERT, which allows BERT to exit at any layer in both static and dynamic ways. We demonstrate the ElasticBERT, despite its simplicity, outperforms or performs on par with SOTA compressed and early exiting models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier and makes a better evaluation for efficient NLP models.</abstract>
      <url hash="c6033c5d">2022.naacl-main.240</url>
      <bibkey>liu-etal-2022-towards-efficient</bibkey>
      <doi>10.18653/v1/2022.naacl-main.240</doi>
      <video href="2022.naacl-main.240.mp4"/>
      <pwccode url="https://github.com/fastnlp/ElasticBERT" additional="false">fastnlp/ElasticBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="241">
      <title>Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting</title>
      <author><first>Qingfeng</first><last>Sun</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Huang</first><last>Hu</last></author>
      <author><first>Yujing</first><last>Wang</last></author>
      <author><first>Jian</first><last>Miao</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Yining</first><last>Chen</last></author>
      <author><first>Fei</first><last>Xu</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>3304-3318</pages>
      <abstract>Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in producing rational and factual responses. However, to establish long-term relationships with users, the KDG model needs the capability to generate responses in a desired style or attribute. Thus, we study a new problem: Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two challenges: (1) How to train a SKDG model where no &lt;context, knowledge, stylized response&gt; triples are available. (2) How to cohere with context and preserve the knowledge when generating a stylized response. In this paper, we propose a novel disentangled template rewriting (DTR) method which generates responses via combing disentangled style templates (from monolingual stylized corpus) and content templates (from KDG corpus). The entire framework is end-to-end differentiable and learned without supervision. Extensive experiments on two benchmarks indicate that DTR achieves a significant improvement on all evaluation metrics compared with previous state-of-the-art stylized dialogue generation methods. Besides, DTR achieves comparable performance with the state-of-the-art KDG methods in standard KDG evaluation setting.</abstract>
      <url hash="7cee0f16">2022.naacl-main.241</url>
      <bibkey>sun-etal-2022-stylized</bibkey>
      <doi>10.18653/v1/2022.naacl-main.241</doi>
      <video href="2022.naacl-main.241.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="242">
      <title><fixed-case>LUNA</fixed-case>: Learning Slot-Turn Alignment for Dialogue State Tracking</title>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Jing</first><last>Zhao</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Chaoqun</first><last>Duan</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>3319-3328</pages>
      <abstract>Dialogue state tracking (DST) aims to predict the current dialogue state given the dialogue history. Existing methods generally exploit the utterances of all dialogue turns to assign value for each slot. This could lead to suboptimal results due to the information introduced from irrelevant utterances in the dialogue history, which may be useless and can even cause confusion. To address this problem, we propose LUNA, a SLot-TUrN Alignment enhanced approach. It first explicitly aligns each slot with its most relevant utterance, then further predicts the corresponding value based on this aligned utterance instead of all dialogue utterances. Furthermore, we design a slot ranking auxiliary task to learn the temporal correlation among slots which could facilitate the alignment. Comprehensive experiments are conducted on three multi-domain task-oriented dialogue datasets, MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art results on these datasets.</abstract>
      <url hash="76ece311">2022.naacl-main.242</url>
      <attachment type="software" hash="fd4a5197">2022.naacl-main.242.software.tgz</attachment>
      <bibkey>wang-etal-2022-luna</bibkey>
      <doi>10.18653/v1/2022.naacl-main.242</doi>
      <video href="2022.naacl-main.242.mp4"/>
      <pwccode url="https://github.com/nlper27149/luna-dst" additional="false">nlper27149/luna-dst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="243">
      <title>Crossroads, Buildings and Neighborhoods: A Dataset for Fine-grained Location Recognition</title>
      <author><first>Pei</first><last>Chen</last></author>
      <author><first>Haotian</first><last>Xu</last></author>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <pages>3329-3339</pages>
      <abstract>General domain Named Entity Recognition (NER) datasets like CoNLL-2003 mostly annotate coarse-grained location entities such as a country or a city. But many applications require identifying fine-grained locations from texts and mapping them precisely to geographic sites, e.g., a crossroad, an apartment building, or a grocery store. In this paper, we introduce a new dataset HarveyNER with fine-grained locations annotated in tweets. This dataset presents unique challenges and characterizes many complex and long location mentions in informal descriptions. We built strong baseline models using Curriculum Learning and experimented with different heuristic curricula to better recognize difficult location mentions. Experimental results show that the simple curricula can improve the system’s performance on hard cases and its overall performance, and outperform several other baseline systems. The dataset and the baseline models can be found at https://github.com/brickee/HarveyNER.</abstract>
      <url hash="bd766e71">2022.naacl-main.243</url>
      <bibkey>chen-etal-2022-crossroads</bibkey>
      <doi>10.18653/v1/2022.naacl-main.243</doi>
      <video href="2022.naacl-main.243.mp4"/>
      <pwccode url="https://github.com/brickee/harveyner" additional="false">brickee/harveyner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/harveyner">HarveyNER</pwcdataset>
    </paper>
    <paper id="244">
      <title>Tricks for Training Sparse Translation Models</title>
      <author><first>Dheeru</first><last>Dua</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>Vedanuj</first><last>Goswami</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <pages>3340-3345</pages>
      <abstract>Multi-task learning with an unbalanced data distribution skews model learning towards high resource tasks, especially when model capacity is fixed and fully shared across all tasks. Sparse scaling architectures, such as BASELayers, provide flexible mechanisms for different tasks to have a variable number of parameters, which can be useful to counterbalance skewed data distributions. We find that that sparse architectures for multilingual machine translation can perform poorly out of the box and propose two straightforward techniques to mitigate this — a temperature heating mechanism and dense pre-training. Overall, these methods improve performance on two multilingual translation benchmarks compared to standard BASELayers and Dense scaling baselines, and in combination, more than 2x model convergence speed.</abstract>
      <url hash="1bc5ac55">2022.naacl-main.244</url>
      <bibkey>dua-etal-2022-tricks</bibkey>
      <doi>10.18653/v1/2022.naacl-main.244</doi>
      <video href="2022.naacl-main.244.mp4"/>
    </paper>
    <paper id="245">
      <title>Persona-Guided Planning for Controlling the Protagonist’s Persona in Story Generation</title>
      <author><first>Zhexin</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Wen</last></author>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>3346-3361</pages>
      <abstract>Endowing the protagonist with a specific personality is essential for writing an engaging story. In this paper, we aim to control the protagonist’s persona in story generation, i.e., generating a story from a leading context and a persona description, where the protagonist should exhibit the specified personality through a coherent event sequence. Considering that personas are usually embodied implicitly and sparsely in stories, we propose a planning-based generation model named ConPer to explicitly model the relationship between personas and events. ConPer first plans events of the protagonist’s behavior which are motivated by the specified persona through predicting one target sentence, then plans the plot as a sequence of keywords with the guidance of the predicted persona-related events and commonsense knowledge, and finally generates the whole story. Both automatic and manual evaluation results demonstrate that ConPer outperforms state-of-the-art baselines for generating more coherent and persona-controllable stories. Our code is available at <url>https://github.com/thu-coai/ConPer</url>.</abstract>
      <url hash="a792cd69">2022.naacl-main.245</url>
      <attachment type="software" hash="508c896a">2022.naacl-main.245.software.zip</attachment>
      <bibkey>zhang-etal-2022-persona</bibkey>
      <doi>10.18653/v1/2022.naacl-main.245</doi>
      <video href="2022.naacl-main.245.mp4"/>
      <pwccode url="https://github.com/thu-coai/conper" additional="false">thu-coai/conper</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="246">
      <title><fixed-case>CHEF</fixed-case>: A Pilot <fixed-case>C</fixed-case>hinese Dataset for Evidence-Based Fact-Checking</title>
      <author><first>Xuming</first><last>Hu</last></author>
      <author><first>Zhijiang</first><last>Guo</last></author>
      <author><first>GuanYu</first><last>Wu</last></author>
      <author><first>Aiwei</first><last>Liu</last></author>
      <author><first>Lijie</first><last>Wen</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>3362-3376</pages>
      <abstract>The explosion of misinformation spreading in the media ecosystem urges for automated fact-checking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public health, and provides annotated evidence retrieved from the Internet. Further, we develop established baselines and a novel approach that is able to model the evidence retrieval as a latent variable, allowing jointly training with the veracity prediction model in an end-to-end fashion. Extensive experiments show that CHEF will provide a challenging testbed for the development of fact-checking systems designed to retrieve and reason over non-English claims.</abstract>
      <url hash="5d11e91a">2022.naacl-main.246</url>
      <bibkey>hu-etal-2022-chef</bibkey>
      <doi>10.18653/v1/2022.naacl-main.246</doi>
      <video href="2022.naacl-main.246.mp4"/>
      <pwccode url="https://github.com/thu-bpm/chef" additional="false">thu-bpm/chef</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/x-fact">X-Fact</pwcdataset>
    </paper>
    <paper id="247">
      <title><fixed-case>VGNMN</fixed-case>: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems</title>
      <author><first>Hung</first><last>Le</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Steven</first><last>Hoi</last></author>
      <pages>3377-3393</pages>
      <abstract>Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.</abstract>
      <url hash="4feeeb83">2022.naacl-main.247</url>
      <bibkey>le-etal-2022-vgnmn</bibkey>
      <doi>10.18653/v1/2022.naacl-main.247</doi>
      <video href="2022.naacl-main.247.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="248">
      <title>Multimodal Dialogue State Tracking</title>
      <author><first>Hung</first><last>Le</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Steven</first><last>Hoi</last></author>
      <pages>3394-3415</pages>
      <abstract>Designed for tracking user goals in dialogues, a dialogue state tracker is an essential component in a dialogue system. However, the research of dialogue state tracking has largely been limited to unimodality, in which slots and slot values are limited by knowledge domains (e.g. restaurant domain with slots of restaurant name and price range) and are defined by specific database schema. In this paper, we propose to extend the definition of dialogue state tracking to multimodality. Specifically, we introduce a novel dialogue state tracking task to track the information of visual objects that are mentioned in video-grounded dialogues. Each new dialogue utterance may introduce a new video segment, new visual objects, or new object attributes and a state tracker is required to update these information slots accordingly. We created a new synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer Network (VDTN), for this task. VDTN combines both object-level features and segment-level features and learns contextual dependencies between videos and dialogues to generate multimodal dialogue states. We optimized VDTN for a state generation task as well as a self-supervised video understanding task which recovers video segment or object representations. Finally, we trained VDTN to use the decoded states in a response prediction task. Together with comprehensive ablation and qualitative analysis, we discovered interesting insights towards building more capable multimodal dialogue systems.</abstract>
      <url hash="08eca058">2022.naacl-main.248</url>
      <bibkey>le-etal-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.naacl-main.248</doi>
      <video href="2022.naacl-main.248.mp4"/>
      <pwccode url="https://github.com/henryhungle/mm_dst" additional="false">henryhungle/mm_dst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cater">CATER</pwcdataset>
    </paper>
    <paper id="249">
      <title>On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation</title>
      <author><first>Yongjie</first><last>Wang</last></author>
      <author><first>Chuang</first><last>Wang</last></author>
      <author><first>Ruobing</first><last>Li</last></author>
      <author><first>Hui</first><last>Lin</last></author>
      <pages>3416-3425</pages>
      <abstract>In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.</abstract>
      <url hash="8da437dd">2022.naacl-main.249</url>
      <bibkey>wang-etal-2022-use</bibkey>
      <doi>10.18653/v1/2022.naacl-main.249</doi>
      <video href="2022.naacl-main.249.mp4"/>
      <pwccode url="https://github.com/lingochamp/multi-scale-bert-aes" additional="false">lingochamp/multi-scale-bert-aes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asap">ASAP</pwcdataset>
    </paper>
    <paper id="250">
      <title>Recognition of They/Them as Singular Personal Pronouns in Coreference Resolution</title>
      <author><first>Connor</first><last>Baumler</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>3426-3432</pages>
      <abstract>As using they/them as personal pronouns becomes increasingly common in English, it is important that coreference resolution systems work as well for individuals who use personal “they” as they do for those who use gendered personal pronouns. We introduce a new benchmark for coreference resolution systems which evaluates singular personal “they” recognition. Using these WinoNB schemas, we evaluate a number of publicly available coreference resolution systems and confirm their bias toward resolving “they” pronouns as plural.</abstract>
      <url hash="5c55d018">2022.naacl-main.250</url>
      <bibkey>baumler-rudinger-2022-recognition</bibkey>
      <doi>10.18653/v1/2022.naacl-main.250</doi>
      <video href="2022.naacl-main.250.mp4"/>
      <pwccode url="https://github.com/ctbaumler/winonb" additional="false">ctbaumler/winonb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/winonb">WinoNB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gicoref">GICoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogender-schemas">Winogender Schemas</pwcdataset>
    </paper>
    <paper id="251">
      <title><fixed-case>TWEETSPIN</fixed-case>: Fine-grained Propaganda Detection in Social Media Using Multi-View Representations</title>
      <author><first>Prashanth</first><last>Vijayaraghavan</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>3433-3448</pages>
      <abstract>Recently, several studies on propaganda detection have involved document and fragment-level analyses of news articles. However, there are significant data and modeling challenges dealing with fine-grained detection of propaganda on social media. In this work, we present TWEETSPIN, a dataset containing tweets that are weakly annotated with different fine-grained propaganda techniques, and propose a neural approach to detect and categorize propaganda tweets across those fine-grained categories. These categories include specific rhetorical and psychological techniques, ranging from leveraging emotions to using logical fallacies. Our model relies on multi-view representations of the input tweet data to (a) extract different aspects of the input text including the context, entities, their relationships, and external knowledge; (b) model their mutual interplay; and (c) effectively speed up the learning process by requiring fewer training examples. Our method allows for representation enrichment leading to better detection and categorization of propaganda on social media. We verify the effectiveness of our proposed method on TWEETSPIN and further probe how the implicit relations between the views impact the performance. Our experiments show that our model is able to outperform several benchmark methods and transfer the knowledge to relatively low-resource news domains.</abstract>
      <url hash="8052df46">2022.naacl-main.251</url>
      <bibkey>vijayaraghavan-vosoughi-2022-tweetspin</bibkey>
      <doi>10.18653/v1/2022.naacl-main.251</doi>
      <video href="2022.naacl-main.251.mp4"/>
    </paper>
    <paper id="252">
      <title><fixed-case>U</fixed-case>ser<fixed-case>I</fixed-case>dentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Vaishnavi</first><last>Shrivastava</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Robert</first><last>Sim</last></author>
      <author><first>Dimitrios</first><last>Dimitriadis</last></author>
      <pages>3449-3456</pages>
      <abstract>Global models are typically trained to be as generalizable as possible. Invariance to the specific user is considered desirable since models are shared across multitudes of users. However, these models are often unable to produce personalized responses for individual users, based on their data. Contrary to widely-used personalization techniques based on few-shot and meta-learning, we propose UserIdentifier, a novel scheme for training a single shared model for all users. Our approach produces personalized responses by prepending a fixed, user-specific non-trainable string (called “user identifier”) to each user’s input text. Unlike prior work, this method doesn’t need any additional model parameters, any extra rounds of personal few-shot learning or any change made to the vocabulary. We empirically study different types of user identifiers (numeric, alphanumeric, and also randomly generated) and demonstrate that, surprisingly, randomly generated user identifiers outperform the prefix-tuning based state-of-the-art approach by up to 13, on a suite of sentiment analysis datasets.</abstract>
      <url hash="ea99bd6b">2022.naacl-main.252</url>
      <bibkey>mireshghallah-etal-2022-useridentifier</bibkey>
      <doi>10.18653/v1/2022.naacl-main.252</doi>
      <video href="2022.naacl-main.252.mp4"/>
    </paper>
    <paper id="253">
      <title>Improving Neural Models for Radiology Report Retrieval with Lexicon-based Automated Annotation</title>
      <author><first>Luyao</first><last>Shi</last></author>
      <author><first>Tanveer</first><last>Syeda-mahmood</last></author>
      <author><first>Tyler</first><last>Baldwin</last></author>
      <pages>3457-3463</pages>
      <abstract>Many clinical informatics tasks that are based on electronic health records (EHR) need relevant patient cohorts to be selected based on findings, symptoms and diseases. Frequently, these conditions are described in radiology reports which can be retrieved using information retrieval (IR) methods. The latest of these techniques utilize neural IR models such as BERT trained on clinical text. However, these methods still lack semantic understanding of the underlying clinical conditions as well as ruled out findings, resulting in poor precision during retrieval. In this paper we combine clinical finding detection with supervised query match learning. Specifically, we use lexicon-driven concept detection to detect relevant findings in sentences. These findings are used as queries to train a Sentence-BERT (SBERT) model using triplet loss on matched and unmatched query-sentence pairs. We show that the proposed supervised training task remarkably improves the retrieval performance of SBERT. The trained model generalizes well to unseen queries and reports from different collections.</abstract>
      <url hash="2c2256a8">2022.naacl-main.253</url>
      <bibkey>shi-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.253</doi>
      <video href="2022.naacl-main.253.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="254">
      <title>Transparent Human Evaluation for Image Captioning</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Lavinia</first><last>Dunagan</last></author>
      <author><first>Jacob</first><last>Morrison</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>3464-3478</pages>
      <abstract>We establish THumB, a rubric-based human evaluation protocol for image captioning models. Our scoring rubrics and their definitions are carefully developed based on machine- and human-generated captions on the MSCOCO dataset. Each caption is evaluated along two main dimensions in a tradeoff (precision and recall) as well as other aspects that measure the text quality (fluency, conciseness, and inclusive language). Our evaluations demonstrate several critical problems of the current evaluation practice. Human-generated captions show substantially higher quality than machine-generated ones, especially in coverage of salient information (i.e., recall), while most automatic metrics say the opposite. Our rubric-based results reveal that CLIPScore, a recent metric that uses image features, better correlates with human judgments than conventional text-only metrics because it is more sensitive to recall. We hope that this work will promote a more transparent evaluation protocol for image captioning and its automatic metrics.</abstract>
      <url hash="d248880d">2022.naacl-main.254</url>
      <bibkey>kasai-etal-2022-transparent</bibkey>
      <doi>10.18653/v1/2022.naacl-main.254</doi>
      <video href="2022.naacl-main.254.mp4"/>
      <pwccode url="https://github.com/jungokasai/thumb" additional="true">jungokasai/thumb</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="255">
      <title>Lifting the Curse of Multilinguality by Pre-training Modular Transformers</title>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Xi</first><last>Lin</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <pages>3479-3495</pages>
      <abstract>Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.</abstract>
      <url hash="21d99f27">2022.naacl-main.255</url>
      <bibkey>pfeiffer-etal-2022-lifting</bibkey>
      <doi>10.18653/v1/2022.naacl-main.255</doi>
      <video href="2022.naacl-main.255.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="256">
      <title><fixed-case>D</fixed-case>oc<fixed-case>AMR</fixed-case>: Multi-Sentence <fixed-case>AMR</fixed-case> Representation and Evaluation</title>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Austin</first><last>Blodgett</last></author>
      <author><first>Sadhana</first><last>Kumaravel</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <author><first>Ramón</first><last>Astudillo</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>3496-3505</pages>
      <abstract>Despite extensive research on parsing of English sentences into Abstract Meaning Representation (AMR) graphs, which are compared to gold graphs via the Smatch metric, full-document parsing into a unified graph representation lacks well-defined representation and evaluation. Taking advantage of a super-sentential level of coreference annotation from previous work, we introduce a simple algorithm for deriving a unified graph representation, avoiding the pitfalls of information loss from over-merging and lack of coherence from under merging. Next, we describe improvements to the Smatch metric to make it tractable for comparing document-level graphs and use it to re-evaluate the best published document-level AMR parser. We also present a pipeline approach combining the top-performing AMR parser and coreference resolution systems, providing a strong baseline for future research.</abstract>
      <url hash="b2ae53ca">2022.naacl-main.256</url>
      <bibkey>naseem-etal-2022-docamr</bibkey>
      <doi>10.18653/v1/2022.naacl-main.256</doi>
      <video href="2022.naacl-main.256.mp4"/>
      <pwccode url="https://github.com/ibm/docamr" additional="false">ibm/docamr</pwccode>
    </paper>
    <paper id="257">
      <title>Learning to Transfer Prompts for Text Generation</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <author><first>Xin</first><last>Zhao</last></author>
      <pages>3506-3518</pages>
      <abstract>Pretrained language models (PLMs) have made remarkable progress in text generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in a data-scarce situation. Therefore, it is non-trivial to develop a general and lightweight model that can adapt to various text generation tasks based on PLMs. To fulfill this purpose, the recent prompt-based learning offers a potential solution. In this paper, we improve this technique and propose a novel prompt-based method (PTG) for text generation in a transferable setting. First, PTG learns a set of source prompts for various source generation tasks and then transfers these prompts as target prompts to perform target generation tasks. To consider both task- and instance-level information, we design an adaptive attention mechanism to derive the target prompts. For each data instance, PTG learns a specific target prompt by attending to highly relevant source prompts. In extensive experiments, PTG yields competitive or better results than fine-tuning methods. We release our source prompts as an open resource, where users can add or reuse them to improve new text generation tasks for future research. Code and data can be available at <url>https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation</url>.</abstract>
      <url hash="e3609a45">2022.naacl-main.257</url>
      <bibkey>li-etal-2022-learning-transfer</bibkey>
      <doi>10.18653/v1/2022.naacl-main.257</doi>
      <video href="2022.naacl-main.257.mp4"/>
      <pwccode url="https://github.com/rucaibox/transfer-prompts-for-text-generation" additional="false">rucaibox/transfer-prompts-for-text-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="258">
      <title><fixed-case>E</fixed-case>lite<fixed-case>PLM</fixed-case>: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Zheng</first><last>Gong</last></author>
      <author><first>Lixin</first><last>Yang</last></author>
      <author><first>Zhuohao</first><last>Yu</last></author>
      <author><first>Zhipeng</first><last>Chen</last></author>
      <author><first>Jingyuan</first><last>Wang</last></author>
      <author><first>Xin</first><last>Zhao</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>3519-3539</pages>
      <abstract>Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, memory, comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results demonstrate that: (1) PLMs with varying training objectives and strategies are good at different ability tests; (2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size and distribution; (3) PLMs have excellent transferability between similar tasks. Moreover, the prediction results of PLMs in our experiments are released as an open resource for more deep and detailed analysis on the language abilities of PLMs. This paper can guide the future work to select, apply, and design PLMs for specific tasks. We have made all the details of experiments publicly available at <url>https://github.com/RUCAIBox/ElitePLM</url>. </abstract>
      <url hash="b87e5222">2022.naacl-main.258</url>
      <bibkey>li-etal-2022-eliteplm</bibkey>
      <doi>10.18653/v1/2022.naacl-main.258</doi>
      <video href="2022.naacl-main.258.mp4"/>
      <pwccode url="https://github.com/rucaibox/eliteplm" additional="false">rucaibox/eliteplm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="259">
      <title>Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Lavinia</first><last>Dunagan</last></author>
      <author><first>Jacob</first><last>Morrison</last></author>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>3540-3557</pages>
      <abstract>Natural language processing researchers have identified limitations of evaluation methodology for generation tasks, with new questions raised about the validity of automatic metrics and of crowdworker judgments. Meanwhile, efforts to improve generation models tend to depend on simple n-gram overlap metrics (e.g., BLEU, ROUGE). We argue that new advances on models and metrics should each more directly benefit and inform the other. We therefore propose a generalization of leaderboards, bidimensional leaderboards (Billboards), that simultaneously tracks progress in language generation models and metrics for their evaluation. Unlike conventional unidimensional leaderboards that sort submitted systems by predetermined metrics, a Billboard accepts both generators and evaluation metrics as competing entries. A Billboard automatically creates an ensemble metric that selects and linearly combines a few metrics based on a global analysis across generators. Further, metrics are ranked based on their correlation with human judgments. We release four Billboards for machine translation, summarization, and image captioning. We demonstrate that a linear ensemble of a few diverse metrics sometimes substantially outperforms existing metrics in isolation. Our mixed-effects model analysis shows that most automatic metrics, especially the reference-based ones, overrate machine over human generation, demonstrating the importance of updating metrics as generation models become stronger (and perhaps more similar to humans) in the future.</abstract>
      <url hash="07e02193">2022.naacl-main.259</url>
      <bibkey>kasai-etal-2022-bidimensional</bibkey>
      <doi>10.18653/v1/2022.naacl-main.259</doi>
      <video href="2022.naacl-main.259.mp4"/>
      <pwccode url="https://github.com/jungokasai/billboard" additional="true">jungokasai/billboard</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="260">
      <title>Improving In-Context Few-Shot Learning via Self-Supervised Training</title>
      <author><first>Mingda</first><last>Chen</last></author>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Todor</first><last>Mihaylov</last></author>
      <author><first>Srini</first><last>Iyer</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <pages>3558-3573</pages>
      <abstract>Self-supervised pretraining has made few-shot learning possible for many NLP tasks. But the pretraining objectives are not typically adapted specifically for in-context few-shot learning. In this paper, we propose to use self-supervision in an intermediate training stage between pretraining and downstream few-shot usage with the goal to teach the model to perform in-context few shot learning. We propose and evaluate four self-supervised objectives on two benchmarks. We find that the intermediate self-supervision stage produces models that outperform strong baselines. Ablation study shows that several factors affect the downstream performance, such as the amount of training data and the diversity of the self-supervised objectives. Human-annotated cross-task supervision and self-supervision are complementary. Qualitative analysis suggests that the self-supervised-trained models are better at following task requirements.</abstract>
      <url hash="ed048f11">2022.naacl-main.260</url>
      <bibkey>chen-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.260</doi>
      <video href="2022.naacl-main.260.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-instructions">Natural Instructions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="261">
      <title>Exposing the Limits of Video-Text Models through Contrast Sets</title>
      <author><first>Jae Sung</first><last>Park</last></author>
      <author><first>Sheng</first><last>Shen</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Anna</first><last>Rohrbach</last></author>
      <pages>3574-3586</pages>
      <abstract>Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video to-text classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.</abstract>
      <url hash="af9eeeca">2022.naacl-main.261</url>
      <bibkey>park-etal-2022-exposing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.261</doi>
      <video href="2022.naacl-main.261.mp4"/>
      <pwccode url="https://github.com/jamespark3922/video-lang-contrast-set" additional="false">jamespark3922/video-lang-contrast-set</pwccode>
    </paper>
    <paper id="262">
      <title>Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features</title>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>3587-3597</pages>
      <abstract>Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.</abstract>
      <url hash="799cef14">2022.naacl-main.262</url>
      <bibkey>tian-peng-2022-zero</bibkey>
      <doi>10.18653/v1/2022.naacl-main.262</doi>
      <video href="2022.naacl-main.262.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/sonnet-gen" additional="false">pluslabnlp/sonnet-gen</pwccode>
    </paper>
    <paper id="263">
      <title>Benchmarking Intersectional Biases in <fixed-case>NLP</fixed-case></title>
      <author><first>John</first><last>Lalor</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Kendall</first><last>Smith</last></author>
      <author><first>Nicole</first><last>Forsgren</last></author>
      <author><first>Ahmed</first><last>Abbasi</last></author>
      <pages>3598-3609</pages>
      <abstract>There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.</abstract>
      <url hash="475c4d05">2022.naacl-main.263</url>
      <attachment type="software" hash="1e0df685">2022.naacl-main.263.software.zip</attachment>
      <bibkey>lalor-etal-2022-benchmarking</bibkey>
      <doi>10.18653/v1/2022.naacl-main.263</doi>
      <video href="2022.naacl-main.263.mp4"/>
      <pwccode url="https://github.com/nd-hal/naacl-2022" additional="false">nd-hal/naacl-2022</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/psychometric-nlp">Psychometric NLP</pwcdataset>
    </paper>
    <paper id="264">
      <title>When is <fixed-case>BERT</fixed-case> Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer</title>
      <author><first>Ameet</first><last>Deshpande</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <pages>3610-3623</pages>
      <abstract>While recent work on multilingual language models has demonstrated their capacity for cross-lingual zero-shot transfer on downstream tasks, there is a lack of consensus in the community as to what shared properties between languages enable such transfer. Analyses involving pairs of natural languages are often inconclusive and contradictory since languages simultaneously differ in many linguistic aspects. In this paper, we perform a large-scale empirical study to isolate the effects of various linguistic properties by measuring zero-shot transfer between four diverse natural languages and their counterparts constructed by modifying aspects such as the script, word order, and syntax. Among other things, our experiments show that the absence of sub-word overlap significantly affects zero-shot transfer when languages differ in their word order, and there is a strong correlation between transfer performance and word embedding alignment between languages (e.g., <tex-math>\rho_s=0.94</tex-math> on the task of NLI). Our results call for focus in multilingual models on explicitly improving word embedding alignment between languages rather than relying on its implicit emergence.</abstract>
      <url hash="33edf9dd">2022.naacl-main.264</url>
      <bibkey>deshpande-etal-2022-bert</bibkey>
      <doi>10.18653/v1/2022.naacl-main.264</doi>
      <video href="2022.naacl-main.264.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/MultilingualAnalysis" additional="true">princeton-nlp/MultilingualAnalysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="265">
      <title>How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns</title>
      <author><first>Stephanie</first><last>Brandl</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>3624-3630</pages>
      <abstract>Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.</abstract>
      <url hash="277075d4">2022.naacl-main.265</url>
      <bibkey>brandl-etal-2022-conservative</bibkey>
      <doi>10.18653/v1/2022.naacl-main.265</doi>
      <video href="2022.naacl-main.265.mp4"/>
      <pwccode url="https://github.com/stephaniebrandl/gender-neutral-pronouns" additional="false">stephaniebrandl/gender-neutral-pronouns</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="266">
      <title>Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts</title>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Xinxi</first><last>Lyu</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>3631-3643</pages>
      <abstract>Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a “wayward” behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.</abstract>
      <url hash="938ffc8d">2022.naacl-main.266</url>
      <bibkey>khashabi-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.naacl-main.266</doi>
      <video href="2022.naacl-main.266.mp4"/>
      <pwccode url="https://github.com/alrope123/prompt-waywardness" additional="false">alrope123/prompt-waywardness</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-instructions">Natural Instructions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="267">
      <title>Contrastive Representation Learning for Cross-Document Coreference Resolution of Events and Entities</title>
      <author><first>Benjamin</first><last>Hsu</last></author>
      <author><first>Graham</first><last>Horwood</last></author>
      <pages>3644-3655</pages>
      <abstract>Identifying related entities and events within and across documents is fundamental to natural language understanding. We present an approach to entity and event coreference resolution utilizing contrastive representation learning. Earlier state-of-the-art methods have formulated this problem as a binary classification problem and leveraged large transformers in a cross-encoder architecture to achieve their results. For large collections of documents and corresponding set of <tex-math>n</tex-math> mentions, the necessity of performing <tex-math>n^{2}</tex-math> transformer computations in these earlier approaches can be computationally intensive. We show that it is possible to reduce this burden by applying contrastive learning techniques that only require <tex-math>n</tex-math> transformer computations at inference time. Our method achieves state-of-the-art results on a number of key metrics on the ECB+ corpus and is competitive on others.</abstract>
      <url hash="2202f76d">2022.naacl-main.267</url>
      <bibkey>hsu-horwood-2022-contrastive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.267</doi>
      <video href="2022.naacl-main.267.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="268">
      <title>Learning the Ordering of Coordinate Compounds and Elaborate Expressions in <fixed-case>H</fixed-case>mong, <fixed-case>L</fixed-case>ahu, and <fixed-case>C</fixed-case>hinese</title>
      <author><first>Chenxuan</first><last>Cui</last></author>
      <author><first>Katherine J.</first><last>Zhang</last></author>
      <author><first>David</first><last>Mortensen</last></author>
      <pages>3656-3669</pages>
      <abstract>Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate constructions common in languages of East and Southeast Asia. Mortensen (2006) claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese can be predicted via phonological hierarchies and (2) that these phonological hierarchies lack a clear phonetic rationale. These claims are significant because morphosyntax has often been seen as in a feed-forward relationship with phonology, and phonological generalizations have often been assumed to be phonetically “natural”. We investigate whether the ordering of CCs and EEs can be learned empirically and whether computational models (classifiers and sequence-labeling models) learn unnatural hierarchies similar to those posited by Mortensen (2006). We find that decision trees and SVMs learn to predict the order of CCs/EEs on the basis of phonology, beating strong baselines for all three languages, with DTs learning hierarchies strikingly similar to those proposed by Mortensen. However, we also find that a neural sequence labeling model is able to learn the ordering of elaborate expressions in Hmong very effectively without using any phonological information. We argue that EE ordering can be learned through two independent routes: phonology and lexical distribution, presenting a more nuanced picture than previous work.</abstract>
      <url hash="c8b09308">2022.naacl-main.268</url>
      <bibkey>cui-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.268</doi>
      <video href="2022.naacl-main.268.mp4"/>
    </paper>
    <paper id="269">
      <title><fixed-case>FRUIT</fixed-case>: Faithfully Reflecting Updated Information in Text</title>
      <award>Best new task paper (tied)</award>
      <author><first>Robert</first><last>Iv</last></author>
      <author><first>Alexandre</first><last>Passos</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <pages>3670-3686</pages>
      <abstract>Textual knowledge bases such as Wikipedia require considerable effort to keep up to date and consistent. While automated writing assistants could potentially ease this burden, the problem of suggesting edits grounded in external knowledge has been under-explored. In this paper, we introduce the novel generation task of *faithfully reflecting updated information in text* (FRUIT) where the goal is to update an existing article given new evidence. We release the FRUIT-WIKI dataset, a collection of over 170K distantly supervised data produced from pairs of Wikipedia snapshots, along with our data generation pipeline and a gold evaluation set of 914 instances whose edits are guaranteed to be supported by the evidence. We provide benchmark results for popular generation systems as well as EDIT5 – a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for neural generation models, and opens doors to many new applications.</abstract>
      <url hash="eb5e69df">2022.naacl-main.269</url>
      <bibkey>iv-etal-2022-fruit</bibkey>
      <doi>10.18653/v1/2022.naacl-main.269</doi>
      <video href="2022.naacl-main.269.mp4"/>
    </paper>
    <paper id="270">
      <title><fixed-case>M</fixed-case>ulti2<fixed-case>WOZ</fixed-case>: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog</title>
      <author><first>Chia-Chien</first><last>Hung</last></author>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Simone</first><last>Ponzetto</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>3687-3703</pages>
      <abstract>Research on (multi-domain) task-oriented dialog (TOD) has predominantly focused on the English language, primarily due to the shortage of robust TOD datasets in other languages, preventing the systematic investigation of cross-lingual transfer for this crucial NLP application area. In this work, we introduce Multi2WOZ, a new multilingual multi-domain TOD dataset, derived from the well-established English dataset MultiWOZ, that spans four typologically diverse languages: Chinese, German, Arabic, and Russian. In contrast to concurrent efforts, Multi2WOZ contains gold-standard dialogs in target languages that are directly comparable with development and test portions of the English dataset, enabling reliable and comparative estimates of cross-lingual transfer performance for TOD. We then introduce a new framework for multilingual conversational specialization of pretrained language models (PrLMs) that aims to facilitate cross-lingual transfer for arbitrary downstream TOD tasks. Using such conversational PrLMs specialized for concrete target languages, we systematically benchmark a number of zero-shot and few-shot cross-lingual transfer approaches on two standard TOD tasks: Dialog State Tracking and Response Retrieval. Our experiments show that, in most setups, the best performance entails the combination of (i) conversational specialization in the target language and (ii) few-shot transfer for the concrete TOD task. Most importantly, we show that our conversational specialization in the target language allows for an exceptionally sample-efficient few-shot transfer for downstream TOD tasks.</abstract>
      <url hash="b69c63cb">2022.naacl-main.270</url>
      <bibkey>hung-etal-2022-multi2woz</bibkey>
      <doi>10.18653/v1/2022.naacl-main.270</doi>
      <video href="2022.naacl-main.270.mp4"/>
      <pwccode url="https://github.com/umanlp/multi2woz" additional="false">umanlp/multi2woz</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="271">
      <title><fixed-case>C</fixed-case>hapter<fixed-case>B</fixed-case>reak: A Challenge Dataset for Long-Range Language Models</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Katherine</first><last>Thai</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>3704-3714</pages>
      <abstract>While numerous architectures for long-range language models (LRLMs) have recently been proposed, a meaningful evaluation of their discourse-level language understanding capabilities has not yet followed. To this end, we introduce ChapterBreak, a challenge dataset that provides an LRLM with a long segment from a narrative that ends at a chapter boundary and asks it to distinguish the beginning of the ground-truth next chapter from a set of negative segments sampled from the same narrative. A fine-grained human annotation reveals that our dataset contains many complex types of chapter transitions (e.g., parallel narratives, cliffhanger endings) that require processing global context to comprehend. Experiments on ChapterBreak show that existing LRLMs fail to effectively leverage long-range context, substantially underperforming a segment-level model trained directly for this task. We publicly release our ChapterBreak dataset to spur more principled future research into LRLMs.</abstract>
      <url hash="3d050cc1">2022.naacl-main.271</url>
      <bibkey>sun-etal-2022-chapterbreak</bibkey>
      <doi>10.18653/v1/2022.naacl-main.271</doi>
      <video href="2022.naacl-main.271.mp4"/>
      <pwccode url="https://github.com/simengsun/chapterbreak" additional="false">simengsun/chapterbreak</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
    </paper>
    <paper id="272">
      <title><fixed-case>C</fixed-case>ol<fixed-case>BERT</fixed-case>v2: Effective and Efficient Retrieval via Lightweight Late Interaction</title>
      <author><first>Keshav</first><last>Santhanam</last></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Jon</first><last>Saad-Falcon</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <author><first>Matei</first><last>Zaharia</last></author>
      <pages>3715-3734</pages>
      <abstract>Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6–10x.</abstract>
      <url hash="bc109502">2022.naacl-main.272</url>
      <bibkey>santhanam-etal-2022-colbertv2</bibkey>
      <doi>10.18653/v1/2022.naacl-main.272</doi>
      <video href="2022.naacl-main.272.mp4"/>
      <pwccode url="https://github.com/stanford-futuredata/ColBERT" additional="true">stanford-futuredata/ColBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gooaq">GooAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="273">
      <title>Quantifying Language Variation Acoustically with Few Resources</title>
      <author><first>Martijn</first><last>Bartelds</last></author>
      <author><first>Martijn</first><last>Wieling</last></author>
      <pages>3735-3741</pages>
      <abstract>Deep acoustic models represent linguistic information based on massive amounts of data. Unfortunately, for regional languages and dialects such resources are mostly not available. However, deep acoustic models might have learned linguistic information that transfers to low-resource languages. In this study, we evaluate whether this is the case through the task of distinguishing low-resource (Dutch) regional varieties. By extracting embeddings from the hidden layers of various wav2vec 2.0 models (including new models which are pre-trained and/or fine-tuned on Dutch) and using dynamic time warping, we compute pairwise pronunciation differences averaged over 10 words for over 100 individual dialects from four (regional) languages. We then cluster the resulting difference matrix in four groups and compare these to a gold standard, and a partitioning on the basis of comparing phonetic transcriptions. Our results show that acoustic models outperform the (traditional) transcription-based approach without requiring phonetic transcriptions, with the best performance achieved by the multilingual XLSR-53 model fine-tuned on Dutch. On the basis of only six seconds of speech, the resulting clustering closely matches the gold standard.</abstract>
      <url hash="def1ef78">2022.naacl-main.273</url>
      <bibkey>bartelds-wieling-2022-quantifying</bibkey>
      <doi>10.18653/v1/2022.naacl-main.273</doi>
      <video href="2022.naacl-main.273.mp4"/>
      <pwccode url="https://github.com/bartelds/language-variation" additional="false">bartelds/language-variation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="274">
      <title>Adaptable Adapters</title>
      <author><first>Nafise</first><last>Moosavi</last></author>
      <author><first>Quentin</first><last>Delfosse</last></author>
      <author><first>Kristian</first><last>Kersting</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>3742-3753</pages>
      <abstract>State-of-the-art pretrained NLP models contain a hundred million to trillion parameters. Adapters provide a parameter-efficient alternative for the full finetuning in which we can only finetune lightweight neural network layers on top of pretrained weights. Adapter layers are initialized randomly. However, existing work uses the same adapter architecture—i.e., the same adapter layer on top of each layer of the pretrained model—for every dataset, regardless of the properties of the dataset or the amount of available training data. In this work, we introduce adaptable adapters that contain (1) learning different activation functions for different layers and different input data, and (2) a learnable switch to select and only use the beneficial adapter layers. We show that adaptable adapters achieve on-par performances with the standard adapter architecture while using a considerably smaller number of adapter layers. In addition, we show that the selected adapter architecture by adaptable adapters transfers well across different data settings and similar tasks. We propose to use adaptable adapters for designing efficient and effective adapter architectures. The resulting adapters (a) contain about 50% of the learning parameters of the standard adapter and are therefore more efficient at training and inference, and require less storage space, and (b) achieve considerably higher performances in low-data settings.</abstract>
      <url hash="e30cfb74">2022.naacl-main.274</url>
      <attachment type="software" hash="aeb95eb2">2022.naacl-main.274.software.zip</attachment>
      <bibkey>moosavi-etal-2022-adaptable</bibkey>
      <doi>10.18653/v1/2022.naacl-main.274</doi>
      <video href="2022.naacl-main.274.mp4"/>
      <pwccode url="https://github.com/ukplab/adaptable-adapters" additional="false">ukplab/adaptable-adapters</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="275">
      <title>Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants</title>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>3754-3767</pages>
      <abstract>In Dynamic Adversarial Data Collection (DADC), human annotators are tasked with finding examples that models struggle to predict correctly. Models trained on DADC-collected training data have been shown to be more robust in adversarial and out-of-domain settings, and are considerably harder for humans to fool. However, DADC is more time-consuming than traditional data collection and thus more costly per annotated example. In this work, we examine whether we can maintain the advantages of DADC, without incurring the additional cost. To that end, we introduce Generative Annotation Assistants (GAAs), generator-in-the-loop models that provide real-time suggestions that annotators can either approve, modify, or reject entirely. We collect training datasets in twenty experimental settings and perform a detailed analysis of this approach for the task of extractive question answering (QA) for both standard and adversarial data collection. We demonstrate that GAAs provide significant efficiency benefits with over a 30% annotation speed-up, while leading to over a 5x improvement in model fooling rates. In addition, we find that using GAA-assisted training data leads to higher downstream model performance on a variety of question answering tasks over adversarial data collection.</abstract>
      <url hash="395b52c3">2022.naacl-main.275</url>
      <bibkey>bartolo-etal-2022-models</bibkey>
      <doi>10.18653/v1/2022.naacl-main.275</doi>
      <video href="2022.naacl-main.275.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/adversarialqa">AdversarialQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="276">
      <title><fixed-case>GMN</fixed-case>: Generative Multi-modal Network for Practical Document Information Extraction</title>
      <author><first>Haoyu</first><last>Cao</last></author>
      <author><first>Jiefeng</first><last>Ma</last></author>
      <author><first>Antai</first><last>Guo</last></author>
      <author><first>Yiqing</first><last>Hu</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Deqiang</first><last>Jiang</last></author>
      <author><first>Yinsong</first><last>Liu</last></author>
      <author><first>Bo</first><last>Ren</last></author>
      <pages>3768-3778</pages>
      <abstract>Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results, these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems, which is a robust multi-modal generation method without predefined label categories. With the carefully designed spatial encoder and modal-aware mask module, GMN can deal with complex documents that are hard to serialized into sequential order. Moreover, GMN tolerates errors in OCR results and requires no character-level annotation, which is vital because fine-grained annotation of numerous documents is laborious and even requires annotators with specialized domain knowledge. Extensive experiments show that GMN achieves new state-of-the-art performance on several public DIE datasets and surpasses other methods by a large margin, especially in realistic scenes.</abstract>
      <url hash="23966e4e">2022.naacl-main.276</url>
      <bibkey>cao-etal-2022-gmn</bibkey>
      <doi>10.18653/v1/2022.naacl-main.276</doi>
      <video href="2022.naacl-main.276.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sroie">SROIE</pwcdataset>
    </paper>
    <paper id="277">
      <title>One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation</title>
      <author><first>Chenze</first><last>Shao</last></author>
      <author><first>Xuanfu</first><last>Wu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>3779-3791</pages>
      <abstract>Non-autoregressive neural machine translation (NAT) suffers from the multi-modality problem: the source sentence may have multiple correct translations, but the loss function is calculated only according to the reference sentence. Sequence-level knowledge distillation makes the target more deterministic by replacing the target with the output from an autoregressive model. However, the multi-modality problem in the distilled dataset is still nonnegligible. Furthermore, learning from a specific teacher limits the upper bound of the model capability, restricting the potential of NAT models. In this paper, we argue that one reference is not enough and propose diverse distillation with reference selection (DDRS) for NAT. Specifically, we first propose a method called SeedDiv for diverse machine translation, which enables us to generate a dataset containing multiple high-quality reference translations for each source sentence. During the training, we compare the NAT output with all references and select the one that best fits the NAT output to train the model. Experiments on widely-used machine translation benchmarks demonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one decoding pass on WMT14 En-De, improving the state-of-the-art performance for NAT by over 1 BLEU.</abstract>
      <url hash="1a678664">2022.naacl-main.277</url>
      <bibkey>shao-etal-2022-one</bibkey>
      <doi>10.18653/v1/2022.naacl-main.277</doi>
      <video href="2022.naacl-main.277.mp4"/>
      <pwccode url="https://github.com/ictnlp/ddrs-nat" additional="false">ictnlp/ddrs-nat</pwccode>
    </paper>
    <paper id="278">
      <title>Can Rationalization Improve Robustness?</title>
      <author><first>Howard</first><last>Chen</last></author>
      <author><first>Jacqueline</first><last>He</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>3792-3805</pages>
      <abstract>A growing line of work has investigated the development of neural NLP models that can produce rationales–subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (“rationalizer”) before making predictions (“predictor”), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end, we systematically generate various types of ‘AddText’ attacks for both token and sentence-level rationalization tasks and perform an extensive empirical evaluation of state-of-the-art rationale models across five different tasks. Our experiments reveal that the rationale models promise to improve robustness over AddText attacks while they struggle in certain scenarios–when the rationalizer is sensitive to position bias or lexical choices of attack text. Further, leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize-then-predict framework.</abstract>
      <url hash="043eb0c7">2022.naacl-main.278</url>
      <bibkey>chen-etal-2022-rationalization</bibkey>
      <doi>10.18653/v1/2022.naacl-main.278</doi>
      <video href="2022.naacl-main.278.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/rationale-robustness" additional="false">princeton-nlp/rationale-robustness</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="279">
      <title>On the Effectiveness of Sentence Encoding for Intent Detection Meta-Learning</title>
      <author><first>Tingting</first><last>Ma</last></author>
      <author><first>Qianhui</first><last>Wu</last></author>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>3806-3818</pages>
      <abstract>Recent studies on few-shot intent detection have attempted to formulate the task as a meta-learning problem, where a meta-learning model is trained with a certain capability to quickly adapt to newly specified few-shot tasks with potentially unseen intent categories. Prototypical networks have been commonly used in this setting, with the hope that good prototypical representations could be learned to capture the semantic similarity between the query and a few labeled instances. This intuition naturally leaves a question of whether or not a good sentence representation scheme could suffice for the task without further domain-specific adaptation. In this paper, we conduct empirical studies on a number of general-purpose sentence embedding schemes, showing that good sentence embeddings without any fine-tuning on intent detection data could produce a non-trivially strong performance. Inspired by the results from our qualitative analysis, we propose a frustratingly easy modification, which leads to consistent improvements over all sentence encoding schemes, including those from the state-of-the-art prototypical network variants with task-specific fine-tuning.</abstract>
      <url hash="f4cab2c4">2022.naacl-main.279</url>
      <bibkey>ma-etal-2022-effectiveness</bibkey>
      <doi>10.18653/v1/2022.naacl-main.279</doi>
      <video href="2022.naacl-main.279.mp4"/>
      <pwccode url="https://github.com/microsoft/KC" additional="false">microsoft/KC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="280">
      <title>A Computational Acquisition Model for Multimodal Word Categorization</title>
      <author><first>Uri</first><last>Berger</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>3819-3835</pages>
      <abstract>Recent advances in self-supervised modeling of text and images open new opportunities for computational models of child language acquisition, which is believed to rely heavily on cross-modal signals. However, prior studies has been limited by their reliance on vision models trained on large image datasets annotated with a pre-defined set of depicted object categories. This is (a) not faithful to the information children receive and (b) prohibits the evaluation of such models with respect to category learning tasks, due to the pre-imposed category structure. We address this gap, and present a cognitively-inspired, multimodal acquisition model, trained from image-caption pairs on naturalistic data using cross-modal self-supervision. We show that the model learns word categories and object recognition abilities, and presents trends reminiscent of ones reported in the developmental literature.</abstract>
      <url hash="dd366b21">2022.naacl-main.280</url>
      <bibkey>berger-etal-2022-computational</bibkey>
      <doi>10.18653/v1/2022.naacl-main.280</doi>
      <video href="2022.naacl-main.280.mp4"/>
      <pwccode url="https://github.com/slab-nlp/multimodal_clustering" additional="false">slab-nlp/multimodal_clustering</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="281">
      <title>Residue-Based Natural Language Adversarial Attack Detection</title>
      <author><first>Vyas</first><last>Raina</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>3836-3848</pages>
      <abstract>Deep learning based systems are susceptible to adversarial attacks, where a small, imperceptible change at the input alters the model prediction. However, to date the majority of the approaches to detect these attacks have been designed for image processing systems. Many popular image adversarial detection approaches are able to identify adversarial examples from embedding feature spaces, whilst in the NLP domain existing state of the art detection approaches solely focus on input text features, without consideration of model embedding spaces. This work examines what differences result when porting these image designed strategies to Natural Language Processing (NLP) tasks - these detectors are found to not port over well. This is expected as NLP systems have a very different form of input: discrete and sequential in nature, rather than the continuous and fixed size inputs for images. As an equivalent model-focused NLP detection approach, this work proposes a simple sentence-embedding “residue” based detector to identify adversarial examples. On many tasks, it out-performs ported image domain detectors and recent state of the art NLP specific detectors.</abstract>
      <url hash="f3b1f3bf">2022.naacl-main.281</url>
      <bibkey>raina-gales-2022-residue</bibkey>
      <doi>10.18653/v1/2022.naacl-main.281</doi>
      <video href="2022.naacl-main.281.mp4"/>
      <pwccode url="https://github.com/rainavyas/naacl-2022-residue-detector" additional="false">rainavyas/naacl-2022-residue-detector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="282">
      <title>Does it Really Generalize Well on Unseen Data? Systematic Evaluation of Relational Triple Extraction Methods</title>
      <author><first>Juhyuk</first><last>Lee</last></author>
      <author><first>Min-Joong</first><last>Lee</last></author>
      <author><first>June Yong</first><last>Yang</last></author>
      <author><first>Eunho</first><last>Yang</last></author>
      <pages>3849-3858</pages>
      <abstract>The ability to extract entities and their relations from unstructured text is essential for the automated maintenance of large-scale knowledge graphs. To keep a knowledge graph up-to-date, an extractor needs not only the ability to recall the triples it encountered during training, but also the ability to extract the new triples from the context that it has never seen before. In this paper, we show that although existing extraction models are able to easily memorize and recall already seen triples, they cannot generalize effectively for unseen triples. This alarming observation was previously unknown due to the composition of the test sets of the go-to benchmark datasets, which turns out to contain only 2% unseen data, rendering them incapable to measure the generalization performance. To separately measure the generalization performance from the memorization performance, we emphasize unseen data by rearranging datasets, sifting out training instances, or augmenting test sets. In addition to that, we present a simple yet effective augmentation technique to promote generalization of existing extraction models, and experimentally confirm that the proposed method can significantly increase the generalization performance of existing models.</abstract>
      <url hash="fbc32ab1">2022.naacl-main.282</url>
      <bibkey>lee-etal-2022-really</bibkey>
      <doi>10.18653/v1/2022.naacl-main.282</doi>
      <video href="2022.naacl-main.282.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="283">
      <title>From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization</title>
      <author><first>Yue</first><last>Fang</last></author>
      <author><first>Hainan</first><last>Zhang</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Zhuoye</first><last>Ding</last></author>
      <author><first>Bo</first><last>Long</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <author><first>Yanquan</first><last>Zhou</last></author>
      <pages>3859-3869</pages>
      <abstract>Due to the dialogue characteristics of unstructured contexts and multi-parties with first-person perspective, many successful text summarization works have failed when dealing with dialogue summarization. In dialogue summarization task, the input dialogue is usually spoken style with ellipsis and co-references but the output summaries are more formal and complete. Therefore, the dialogue summarization model should be able to complete the ellipsis content and co-reference information and then produce a suitable summary accordingly. However, the current state-of-the-art models pay more attention on the topic or structure of summary, rather than the consistency of dialogue summary with its input dialogue context, which may suffer from the personal and logical inconsistency problem. In this paper, we propose a new model, named ReWriteSum, to tackle this problem. Firstly, an utterance rewriter is conducted to complete the ellipsis content of dialogue content and then obtain the rewriting utterances. Then, the co-reference data augmentation mechanism is utilized to replace the referential person name with its specific name to enhance the personal information. Finally, the rewriting utterances and the co-reference replacement data are used in the standard BART model. Experimental results on both SAMSum and DialSum datasets show that our ReWriteSum significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on multi-speakers also shows that ReWriteSum can obtain relatively higher improvement with more speakers, validating the correctness and property of ReWriteSum.</abstract>
      <url hash="be5f11e0">2022.naacl-main.283</url>
      <attachment type="software" hash="86d40779">2022.naacl-main.283.software.zip</attachment>
      <bibkey>fang-etal-2022-spoken</bibkey>
      <doi>10.18653/v1/2022.naacl-main.283</doi>
      <video href="2022.naacl-main.283.mp4"/>
    </paper>
    <paper id="284">
      <title><fixed-case>EASE</fixed-case>: Entity-Aware Contrastive Learning of Sentence Embedding</title>
      <author><first>Sosuke</first><last>Nishikawa</last></author>
      <author><first>Ryokan</first><last>Ri</last></author>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <author><first>Isao</first><last>Echizen</last></author>
      <pages>3870-3885</pages>
      <abstract>We present EASE, a novel method for learning sentence embeddings via contrastive learning between sentences and their related entities.The advantage of using entity supervision is twofold: (1) entities have been shown to be a strong indicator of text semantics and thus should provide rich training signals for sentence embeddings; (2) entities are defined independently of languages and thus offer useful cross-lingual alignment supervision.We evaluate EASE against other unsupervised models both in monolingual and multilingual settings.We show that EASE exhibits competitive or better performance in English semantic textual similarity (STS) and short text clustering (STC) tasks and it significantly outperforms baseline methods in multilingual settings on a variety of tasks.Our source code, pre-trained models, and newly constructed multi-lingual STC dataset are available at https://github.com/studio-ousia/ease.</abstract>
      <url hash="2ef6b13d">2022.naacl-main.284</url>
      <bibkey>nishikawa-etal-2022-ease</bibkey>
      <doi>10.18653/v1/2022.naacl-main.284</doi>
      <video href="2022.naacl-main.284.mp4"/>
      <pwccode url="https://github.com/studio-ousia/ease" additional="false">studio-ousia/ease</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="285">
      <title>Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics</title>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Ling</first><last>Chen</last></author>
      <author><first>Mohammad Reza</first><last>Namazi Rad</last></author>
      <pages>3886-3893</pages>
      <abstract>Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse topics than NTMs, achieving also higher efficiency and simplicity.</abstract>
      <url hash="46b2a6ef">2022.naacl-main.285</url>
      <bibkey>zhang-etal-2022-neural</bibkey>
      <doi>10.18653/v1/2022.naacl-main.285</doi>
      <video href="2022.naacl-main.285.mp4"/>
      <pwccode url="https://github.com/hyintell/topicx" additional="false">hyintell/topicx</pwccode>
    </paper>
    <paper id="286">
      <title>Dynamic Multistep Reasoning based on Video Scene Graph for Video Question Answering</title>
      <author><first>Jianguo</first><last>Mao</last></author>
      <author><first>Wenbin</first><last>Jiang</last></author>
      <author><first>Xiangdong</first><last>Wang</last></author>
      <author><first>Zhifan</first><last>Feng</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>Hong</first><last>Liu</last></author>
      <author><first>Yong</first><last>Zhu</last></author>
      <pages>3894-3904</pages>
      <abstract>Existing video question answering (video QA) models lack the capacity for deep video understanding and flexible multistep reasoning. We propose for video QA a novel model which performs dynamic multistep reasoning between questions and videos. It creates video semantic representation based on the video scene graph composed of semantic elements of the video and semantic relations among these elements. Then, it performs multistep reasoning for better answer decision between the representations of the question and the video, and dynamically integrate the reasoning results. Experiments show the significant advantage of the proposed model against previous methods in accuracy and interpretability. Against the existing state-of-the-art model, the proposed model dramatically improves more than <tex-math>4\%/3.1\%/2\%</tex-math> on the three widely used video QA datasets, MSRVTT-QA, MSRVTT multi-choice, and TGIF-QA, and displays better interpretability by backtracing along with the attention mechanisms to the video scene graphs.</abstract>
      <url hash="607accf0">2022.naacl-main.286</url>
      <bibkey>mao-etal-2022-dynamic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.286</doi>
      <video href="2022.naacl-main.286.mp4"/>
    </paper>
    <paper id="287">
      <title><fixed-case>TRUE</fixed-case>: Re-evaluating Factual Consistency Evaluation</title>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Hagai</first><last>Taitelbaum</last></author>
      <author><first>Doron</first><last>Kukliansy</last></author>
      <author><first>Vered</first><last>Cohen</last></author>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Idan</first><last>Szpektor</last></author>
      <author><first>Avinatan</first><last>Hassidim</last></author>
      <author><first>Yossi</first><last>Matias</last></author>
      <pages>3905-3920</pages>
      <abstract>Grounded text generation systems often generate text that contains factual inconsistencies, hindering their real-world applicability. Automatic factual consistency evaluation may help alleviate this limitation by accelerating evaluation cycles, filtering inconsistent outputs and augmenting training data. While attracting increasing attention, such evaluation metrics are usually developed and evaluated in silo for a single task or dataset, slowing their adoption. Moreover, previous meta-evaluation protocols focused on system-level correlations with human annotations, which leave the example-level accuracy of such metrics unclear.In this work, we introduce TRUE: a comprehensive survey and assessment of factual consistency metrics on a standardized collection of existing texts from diverse tasks, manually annotated for factual consistency. Our standardization enables an example-level meta-evaluation protocol that is more actionable and interpretable than previously reported correlations, yielding clearer quality measures. Across diverse state-of-the-art metrics and 11 datasets we find that large-scale NLI and question generation-and-answering-based approaches achieve strong and complementary results. We recommend those methods as a starting point for model and metric developers, and hope TRUE will foster progress towards even better evaluation methods.</abstract>
      <url hash="a6fe642a">2022.naacl-main.287</url>
      <bibkey>honovich-etal-2022-true-evaluating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.287</doi>
      <pwccode url="https://github.com/google-research/true" additional="false">google-research/true</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dialfact">DialFact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quality">QuALITY</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vitaminc">VitaminC</pwcdataset>
    </paper>
    <paper id="288">
      <title>Knowledge Inheritance for Pre-trained Language Models</title>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Jing</first><last>Yi</last></author>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Yusheng</first><last>Su</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3921-3937</pages>
      <abstract>Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named “knowledge inheritance” (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs’ pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.</abstract>
      <url hash="ae5c29c8">2022.naacl-main.288</url>
      <bibkey>qin-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.naacl-main.288</doi>
      <video href="2022.naacl-main.288.mp4"/>
      <pwccode url="https://github.com/thunlp/Knowledge-Inheritance" additional="true">thunlp/Knowledge-Inheritance</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="289">
      <title><fixed-case>B</fixed-case>i-<fixed-case>S</fixed-case>im<fixed-case>C</fixed-case>ut: A Simple Strategy for Boosting Neural Machine Translation</title>
      <author><first>Pengzhi</first><last>Gao</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>3938-3948</pages>
      <abstract>We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves strong translation performance across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU scores of 31.16 for <tex-math>\texttt{en}\rightarrow\texttt{de}</tex-math> and 38.37 for <tex-math>\texttt{de}\rightarrow\texttt{en}</tex-math> on the IWSLT14 dataset, 30.78 for <tex-math>\texttt{en}\rightarrow\texttt{de}</tex-math> and 35.15 for <tex-math>\texttt{de}\rightarrow\texttt{en}</tex-math> on the WMT14 dataset, and 27.17 for <tex-math>\texttt{zh}\rightarrow\texttt{en}</tex-math> on the WMT17 dataset. SimCut is not a new method, but a version of Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be considered as a perturbation-based method. Given the universality and simplicity of Bi-SimCut and SimCut, we believe they can serve as strong baselines for future NMT research.</abstract>
      <url hash="ab196a68">2022.naacl-main.289</url>
      <bibkey>gao-etal-2022-bi</bibkey>
      <doi>10.18653/v1/2022.naacl-main.289</doi>
      <video href="2022.naacl-main.289.mp4"/>
      <pwccode url="https://github.com/gpengzhi/Bi-SimCut" additional="false">gpengzhi/Bi-SimCut</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="290">
      <title>On Transferability of Prompt Tuning for Natural Language Processing</title>
      <author><first>Yusheng</first><last>Su</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Chi-Min</first><last>Chan</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Huadong</first><last>Wang</last></author>
      <author><first>Kaiyue</first><last>Wen</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3949-3969</pages>
      <abstract>Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts’ stimulation to PLMs. The source code can be obtained from https://github.com/thunlp/Prompt-Transferability.</abstract>
      <url hash="c06f4d8e">2022.naacl-main.290</url>
      <bibkey>su-etal-2022-transferability</bibkey>
      <doi>10.18653/v1/2022.naacl-main.290</doi>
      <video href="2022.naacl-main.290.mp4"/>
      <pwccode url="https://github.com/thunlp/Prompt-Transferability" additional="false">thunlp/Prompt-Transferability</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="291">
      <title><fixed-case>D</fixed-case>oc<fixed-case>EE</fixed-case>: A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction</title>
      <author><first>MeiHan</first><last>Tong</last></author>
      <author><first>Bin</first><last>Xu</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Meihuan</first><last>Han</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Jiangqi</first><last>Zhu</last></author>
      <author><first>Siyu</first><last>Chen</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>3970-3982</pages>
      <abstract>Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% Vs 85% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.</abstract>
      <url hash="77df3ff6">2022.naacl-main.291</url>
      <bibkey>tong-etal-2022-docee</bibkey>
      <doi>10.18653/v1/2022.naacl-main.291</doi>
      <video href="2022.naacl-main.291.mp4"/>
      <pwccode url="https://github.com/tongmeihan1995/docee" additional="false">tongmeihan1995/docee</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikievents">WikiEvents</pwcdataset>
    </paper>
    <paper id="292">
      <title>Towards Debiasing Translation Artifacts</title>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Rricha</first><last>Jalota</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Josef</first><last>Genabith</last></author>
      <pages>3983-3991</pages>
      <abstract>Cross-lingual natural language processing relies on translation, either by humans or machines, at different levels, from translating training data to translating test sets. However, compared to original texts in the same language, translations possess distinct qualities referred to as translationese. Previous research has shown that these translation artifacts influence the performance of a variety of cross-lingual tasks. In this work, we propose a novel approach to reducing translationese by extending an established bias-removal technique. We use the Iterative Null-space Projection (INLP) algorithm, and show by measuring classification accuracy before and after debiasing, that translationese is reduced at both sentence and word level. We evaluate the utility of debiasing translationese on a natural language inference (NLI) task, and show that by reducing this bias, NLI accuracy improves. To the best of our knowledge, this is the first study to debias translationese as represented in latent embedding space.</abstract>
      <url hash="dadd9226">2022.naacl-main.292</url>
      <bibkey>dutta-chowdhury-etal-2022-towards</bibkey>
      <doi>10.18653/v1/2022.naacl-main.292</doi>
      <video href="2022.naacl-main.292.mp4"/>
      <pwccode url="https://github.com/koeldc/towards-debiasing-translation-artifacts" additional="false">koeldc/towards-debiasing-translation-artifacts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="293">
      <title><fixed-case>WECHSEL</fixed-case>: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models</title>
      <author><first>Benjamin</first><last>Minixhofer</last></author>
      <author><first>Fabian</first><last>Paischer</last></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <pages>3992-4006</pages>
      <abstract>Large pretrained language models (LMs) have become the central building block of many NLP applications. Training these models requires ever more computational resources and most of the existing models are trained on English text only. It is exceedingly expensive to train these models in other languages. To alleviate this problem, we introduce a novel method – called WECHSEL – to efficiently and effectively transfer pretrained LMs to new languages. WECHSEL can be applied to any model which uses subword-based tokenization and learns an embedding for each subword. The tokenizer of the source model (in English) is replaced with a tokenizer in the target language and token embeddings are initialized such that they are semantically similar to the English tokens by utilizing multilingual static word embeddings covering English and the target language. We use WECHSEL to transfer the English RoBERTa and GPT-2 models to four languages (French, German, Chinese and Swahili). We also study the benefits of our method on very low-resource languages. WECHSEL improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort. Our method makes training large language models for new languages more accessible and less damaging to the environment. We make our code and models publicly available.</abstract>
      <url hash="c46a5528">2022.naacl-main.293</url>
      <bibkey>minixhofer-etal-2022-wechsel</bibkey>
      <doi>10.18653/v1/2022.naacl-main.293</doi>
      <video href="2022.naacl-main.293.mp4"/>
      <pwccode url="https://github.com/cpjku/wechsel" additional="false">cpjku/wechsel</pwccode>
    </paper>
    <paper id="294">
      <title>A New Concept of Knowledge based Question Answering (<fixed-case>KBQA</fixed-case>) System for Multi-hop Reasoning</title>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>V.srinivasan@samsung.com</first><last>V.srinivasan@samsung.com</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>4007-4017</pages>
      <abstract>Knowledge based question answering (KBQA) is a complex task for natural language understanding. Many KBQA approaches have been proposed in recent years, and most of them are trained based on labeled reasoning path. This hinders the system’s performance as many correct reasoning paths are not labeled as ground truth, and thus they cannot be learned. In this paper, we introduce a new concept of KBQA system which can leverage multiple reasoning paths’ information and only requires labeled answer as supervision. We name it as <b>M</b>utliple <b>R</b>easoning <b>P</b>aths KB<b>QA</b> System (MRP-QA). We conduct experiments on several benchmark datasets containing both single-hop simple questions as well as muti-hop complex questions, including WebQuestionSP (WQSP), ComplexWebQuestion-1.1 (CWQ), and PathQuestion-Large (PQL), and demonstrate strong performance.</abstract>
      <url hash="855acab0">2022.naacl-main.294</url>
      <bibkey>wang-etal-2022-new</bibkey>
      <doi>10.18653/v1/2022.naacl-main.294</doi>
      <video href="2022.naacl-main.294.mp4"/>
    </paper>
    <paper id="295">
      <title>Bilingual Tabular Inference: A Case Study on Indic Languages</title>
      <author><first>Chaitanya</first><last>Agarwal</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>4018-4037</pages>
      <abstract>Existing research on Tabular Natural Language Inference (TNLI) exclusively examines the task in a monolingual setting where the tabular premise and hypothesis are in the same language. However, due to the uneven distribution of text resources on the web across languages, it is common to have the tabular premise in a high resource language and the hypothesis in a low resource language. As a result, we present the challenging task of bilingual Tabular Natural Language Inference (bTNLI), in which the tabular premise and a hypothesis over it are in two separate languages. We construct EI-InfoTabS: an English-Indic bTNLI dataset by translating the textual hypotheses of the English TNLI dataset InfoTabS into eleven major Indian languages. We thoroughly investigate how pre-trained multilingual models learn and perform on EI-InfoTabS. Our study shows that the performance on bTNLI can be close to its monolingual counterpart, with translate-train, translate-test and unified-train being strongly competitive baselines.</abstract>
      <url hash="fcec7995">2022.naacl-main.295</url>
      <bibkey>agarwal-etal-2022-bilingual</bibkey>
      <doi>10.18653/v1/2022.naacl-main.295</doi>
      <video href="2022.naacl-main.295.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="296">
      <title>Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning</title>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Sheng</first><last>Yu</last></author>
      <pages>4038-4048</pages>
      <abstract>Entities lie in the heart of biomedical natural language understanding, and the biomedical entity linking (EL) task remains challenging due to the fine-grained and diversiform concept names.Generative methods achieve remarkable performances in general domain EL with less memory usage while requiring expensive pre-training.Previous biomedical EL methods leverage synonyms from knowledge bases (KB) which is not trivial to inject into a generative method.In this work, we use a generative approach to model biomedical EL and propose to inject synonyms knowledge in it.We propose KB-guided pre-training by constructing synthetic samples with synonyms and definitions from KB and require the model to recover concept names.We also propose synonyms-aware fine-tuning to select concept names for training, and propose decoder prompt and multi-synonyms constrained prefix tree for inference.Our method achieves state-of-the-art results on several biomedical EL tasks without candidate selection which displays the effectiveness of proposed pre-training and fine-tuning strategies. The source code is available at <url>https://github.com/Yuanhy1997/GenBioEL</url>.</abstract>
      <url hash="14190a4c">2022.naacl-main.296</url>
      <bibkey>yuan-etal-2022-generative</bibkey>
      <doi>10.18653/v1/2022.naacl-main.296</doi>
      <video href="2022.naacl-main.296.mp4"/>
      <pwccode url="https://github.com/yuanhy1997/genbioel" additional="false">yuanhy1997/genbioel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
    </paper>
    <paper id="297">
      <title>Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting</title>
      <author><first>Linzhi</first><last>Wu</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Ma</first><last>Chunping</last></author>
      <author><first>Guangwei</first><last>Xu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>4049-4060</pages>
      <abstract>Self-augmentation has received increasing research interest recently to improve named entity recognition (NER) performance in low-resource scenarios. Token substitution and mixup are two feasible heterogeneous self-augmentation techniques for NER that can achieve effective performance with certain specialized efforts. Noticeably, self-augmentation may introduce potentially noisy augmented data. Prior research has mainly resorted to heuristic rule-based constraints to reduce the noise for specific self-augmentation methods individually. In this paper, we revisit these two typical self-augmentation methods for NER, and propose a unified meta-reweighting strategy for them to achieve a natural integration. Our method is easily extensible, imposing little effort on a specific self-augmentation method. Experiments on different Chinese and English NER benchmarks show that our token substitution and mixup method, as well as their integration, can achieve effective performance improvement. Based on the meta-reweighting mechanism, we can enhance the advantages of the self-augmentation techniques without much extra effort.</abstract>
      <url hash="14ff00fd">2022.naacl-main.297</url>
      <attachment type="software" hash="005744a6">2022.naacl-main.297.software.zip</attachment>
      <bibkey>wu-etal-2022-robust</bibkey>
      <doi>10.18653/v1/2022.naacl-main.297</doi>
      <video href="2022.naacl-main.297.mp4"/>
      <pwccode url="https://github.com/LindgeW/MetaAug4NER" additional="false">LindgeW/MetaAug4NER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/weibo-ner">Weibo NER</pwcdataset>
    </paper>
    <paper id="298">
      <title>Unsupervised Stem-based Cross-lingual Part-of-Speech Tagging for Morphologically Rich Low-Resource Languages</title>
      <author><first>Ramy</first><last>Eskander</last></author>
      <author><first>Cass</first><last>Lowry</last></author>
      <author><first>Sujay</first><last>Khandagale</last></author>
      <author><first>Judith</first><last>Klavans</last></author>
      <author><first>Maria</first><last>Polinsky</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>4061-4072</pages>
      <abstract>Unsupervised cross-lingual projection for part-of-speech (POS) tagging relies on the use of parallel data to project POS tags from a source language for which a POS tagger is available onto a target language across word-level alignments. The projected tags then form the basis for learning a POS model for the target language. However, languages with rich morphology often yield sparse word alignments because words corresponding to the same citation form do not align well. We hypothesize that for morphologically complex languages, it is more efficient to use the stem rather than the word as the core unit of abstraction. Our contributions are: 1) we propose an unsupervised stem-based cross-lingual approach for POS tagging for low-resource languages of rich morphology; 2) we further investigate morpheme-level alignment and projection; and 3) we examine whether the use of linguistic priors for morphological segmentation improves POS tagging. We conduct experiments using six source languages and eight morphologically complex target languages of diverse typologies. Our results show that the stem-based approach improves the POS models for all the target languages, with an average relative error reduction of 10.3% in accuracy per target language, and outperforms the word-based approach that operates on three-times more data for about two thirds of the language pairs we consider. Moreover, we show that morpheme-level alignment and projection and the use of linguistic priors for morphological segmentation further improve POS tagging.</abstract>
      <url hash="5c178e09">2022.naacl-main.298</url>
      <bibkey>eskander-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.naacl-main.298</doi>
      <video href="2022.naacl-main.298.mp4"/>
    </paper>
    <paper id="299">
      <title>Optimising Equal Opportunity Fairness in Model Training</title>
      <author><first>Aili</first><last>Shen</last></author>
      <author><first>Xudong</first><last>Han</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>4073-4084</pages>
      <abstract>Real-world datasets often encode stereotypes and societal biases. Such biases can be implicitly captured by trained models, leading to biased predictions and exacerbating existing societal preconceptions. Existing debiasing methods, such as adversarial training and removing protected information from representations, have been shown to reduce bias. However, a disconnect between fairness criteria and training objectives makes it difficult to reason theoretically about the effectiveness of different techniques. In this work, we propose two novel training objectives which directly optimise for the widely-used criterion of <i>equal opportunity</i>, and show that they are effective in reducing bias while maintaining high performance over two classification tasks.</abstract>
      <url hash="fb53e034">2022.naacl-main.299</url>
      <bibkey>shen-etal-2022-optimising</bibkey>
      <doi>10.18653/v1/2022.naacl-main.299</doi>
      <video href="2022.naacl-main.299.mp4"/>
      <pwccode url="https://github.com/ailiaili/difference_mean_fair_models" additional="false">ailiaili/difference_mean_fair_models</pwccode>
    </paper>
    <paper id="300">
      <title>Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval</title>
      <author><first>Siyu</first><last>Ren</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>4085-4090</pages>
      <abstract>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder architecture using pre-trained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework to compress large pre-trained dual-encoder for lightweight text-image retrieval. The resulting model is smaller (39% of the original), faster (1.6x/2.9x for processing image/text respectively), yet performs on par with or better than the original full model on Flickr30K and MSCOCO benchmarks. We also open-source an accompanying realistic mobile image search application.</abstract>
      <url hash="730804a4">2022.naacl-main.300</url>
      <attachment type="software" hash="bd167234">2022.naacl-main.300.software.zip</attachment>
      <bibkey>ren-zhu-2022-leaner</bibkey>
      <doi>10.18653/v1/2022.naacl-main.300</doi>
      <video href="2022.naacl-main.300.mp4"/>
      <pwccode url="https://github.com/drsy/motis" additional="false">drsy/motis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="301">
      <title>Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization</title>
      <author><first>Jingyi</first><last>You</last></author>
      <author><first>Dongyuan</first><last>Li</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>4091-4104</pages>
      <abstract>Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a <i>joint learning-based heterogeneous graph attention network for TLS</i> (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics.</abstract>
      <url hash="c6a7b2dd">2022.naacl-main.301</url>
      <bibkey>you-etal-2022-joint</bibkey>
      <doi>10.18653/v1/2022.naacl-main.301</doi>
      <video href="2022.naacl-main.301.mp4"/>
    </paper>
    <paper id="302">
      <title><fixed-case>E</fixed-case>arly Rumor Detection Using Neural <fixed-case>H</fixed-case>awkes Process with a New Benchmark Dataset</title>
      <author><first>Fengzhu</first><last>Zeng</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <pages>4105-4117</pages>
      <abstract>Little attention has been paid on EArly Rumor Detection (EARD), and EARD performance was evaluated inappropriately on a few datasets where the actual early-stage information is largely missing. To reverse such situation, we construct BEARD, a new Benchmark dataset for EARD, based on claims from fact-checking websites by trying to gather as many early relevant posts as possible. We also propose HEARD, a novel model based on neural Hawkes process for EARD, which can guide a generic rumor detection model to make timely, accurate and stable predictions. Experiments show that HEARD achieves effective EARD performance on two commonly used general rumor detection datasets and our BEARD dataset.</abstract>
      <url hash="f948a974">2022.naacl-main.302</url>
      <bibkey>zeng-gao-2022-early</bibkey>
      <doi>10.18653/v1/2022.naacl-main.302</doi>
      <video href="2022.naacl-main.302.mp4"/>
      <pwccode url="https://github.com/znhy1024/heard" additional="false">znhy1024/heard</pwccode>
    </paper>
    <paper id="303">
      <title>Emp-<fixed-case>RFT</fixed-case>: Empathetic Response Generation via Recognizing Feature Transitions between Utterances</title>
      <author><first>Wongyu</first><last>Kim</last></author>
      <author><first>Youbin</first><last>Ahn</last></author>
      <author><first>Donghyun</first><last>Kim</last></author>
      <author><first>Kyong-Ho</first><last>Lee</last></author>
      <pages>4118-4128</pages>
      <abstract>Each utterance in multi-turn empathetic dialogues has features such as emotion, keywords, and utterance-level meaning. Feature transitions between utterances occur naturally. However, existing approaches fail to perceive the transitions because they extract features for the context at the coarse-grained level. To solve the above issue, we propose a novel approach of recognizing feature transitions between utterances, which helps understand the dialogue flow and better grasp the features of utterance that needs attention. Also, we introduce a response generation strategy to help focus on emotion and keywords related to appropriate features when generating responses. Experimental results show that our approach outperforms baselines and especially, achieves significant improvements on multi-turn dialogues.</abstract>
      <url hash="830c14bb">2022.naacl-main.303</url>
      <bibkey>kim-etal-2022-emp</bibkey>
      <doi>10.18653/v1/2022.naacl-main.303</doi>
      <video href="2022.naacl-main.303.mp4"/>
    </paper>
    <paper id="304">
      <title><fixed-case>KCD</fixed-case>: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media</title>
      <author><first>Wenqian</first><last>Zhang</last></author>
      <author><first>Shangbin</first><last>Feng</last></author>
      <author><first>Zilong</first><last>Chen</last></author>
      <author><first>Zhenyu</first><last>Lei</last></author>
      <author><first>Jundong</first><last>Li</last></author>
      <author><first>Minnan</first><last>Luo</last></author>
      <pages>4129-4140</pages>
      <abstract>Political perspective detection has become an increasingly important task that can help combat echo chambers and political polarization. Previous approaches generally focus on leveraging textual content to identify stances, while they fail to reason with background knowledge or leverage the rich semantic and syntactic textual labels in news articles. In light of these limitations, we propose KCD, a political perspective detection approach to enable multi-hop knowledge reasoning and incorporate textual cues as paragraph-level labels. Specifically, we firstly generate random walks on external knowledge graphs and infuse them with news text representations. We then construct a heterogeneous information network to jointly model news content as well as semantic, syntactic and entity cues in news articles. Finally, we adopt relational graph neural networks for graph-level representation learning and conduct political perspective detection. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on two benchmark datasets. We further examine the effect of knowledge walks and textual cues and how they contribute to our approach’s data efficiency.</abstract>
      <url hash="7d8f9411">2022.naacl-main.304</url>
      <bibkey>zhang-etal-2022-kcd</bibkey>
      <doi>10.18653/v1/2022.naacl-main.304</doi>
      <video href="2022.naacl-main.304.mp4"/>
      <pwccode url="https://github.com/wenqian-zhang/kcd" additional="false">wenqian-zhang/kcd</pwccode>
    </paper>
    <paper id="305">
      <title>Collective Relevance Labeling for Passage Retrieval</title>
      <author><first>Jihyuk</first><last>Kim</last></author>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>4141-4147</pages>
      <abstract>Deep learning for Information Retrieval (IR) requires a large amount of high-quality query-document relevance labels, but such labels are inherently sparse. Label smoothing redistributes some observed probability mass over unobserved instances, often uniformly, uninformed of the true distribution. In contrast, we propose knowledge distillation for informed labeling, without incurring high computation overheads at evaluation time. Our contribution is designing a simple but efficient teacher model which utilizes collective knowledge, to outperform state-of-the-arts distilled from a more complex teacher model. Specifically, we train up to <tex-math>\times8</tex-math> faster than the state-of-the-art teacher, while distilling the rankings better. Our code is publicly available at <url>https://github.com/jihyukkim-nlp/CollectiveKD</url>.</abstract>
      <url hash="98e30287">2022.naacl-main.305</url>
      <attachment type="software" hash="440e9bde">2022.naacl-main.305.software.tgz</attachment>
      <bibkey>kim-etal-2022-collective</bibkey>
      <doi>10.18653/v1/2022.naacl-main.305</doi>
      <video href="2022.naacl-main.305.mp4"/>
      <pwccode url="https://github.com/jihyukkim-nlp/CollectiveKD" additional="false">jihyukkim-nlp/CollectiveKD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="306">
      <title><fixed-case>COGMEN</fixed-case>: <fixed-case>CO</fixed-case>ntextualized <fixed-case>GNN</fixed-case> based Multimodal Emotion recognitio<fixed-case>N</fixed-case></title>
      <author><first>Abhinav</first><last>Joshi</last></author>
      <author><first>Ashwani</first><last>Bhat</last></author>
      <author><first>Ayush</first><last>Jain</last></author>
      <author><first>Atin</first><last>Singh</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>4148-4164</pages>
      <abstract>Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person’s emotions are influenced by the other speaker’s utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multi- modal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.</abstract>
      <url hash="5dcac128">2022.naacl-main.306</url>
      <attachment type="software" hash="bfbf21d1">2022.naacl-main.306.software.zip</attachment>
      <bibkey>joshi-etal-2022-cogmen</bibkey>
      <doi>10.18653/v1/2022.naacl-main.306</doi>
      <video href="2022.naacl-main.306.mp4"/>
      <pwccode url="https://github.com/exploration-lab/cogmen" additional="false">exploration-lab/cogmen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="307">
      <title>Revisit Overconfidence for <fixed-case>OOD</fixed-case> Detection: Reassigned Contrastive Learning with Adaptive Class-dependent Threshold</title>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>QiXiang</first><last>Gao</last></author>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Fujia</first><last>Zheng</last></author>
      <author><first>Lulu</first><last>Zhao</last></author>
      <author><first>Huixing</first><last>Jiang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>4165-4179</pages>
      <abstract>Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is the overconfidence of neural models. In this paper, we comprehensively analyze overconfidence and classify it into two perspectives: over-confident OOD and in-domain (IND). Then according to intrinsic reasons, we respectively propose a novel reassigned contrastive learning (RCL) to discriminate IND intents for over-confident OOD and an adaptive class-dependent local threshold mechanism to separate similar IND and OOD intents for over-confident IND. Experiments and analyses show the effectiveness of our proposed method for both aspects of overconfidence issues.</abstract>
      <url hash="f8704bdc">2022.naacl-main.307</url>
      <bibkey>wu-etal-2022-revisit</bibkey>
      <doi>10.18653/v1/2022.naacl-main.307</doi>
      <video href="2022.naacl-main.307.mp4"/>
      <pwccode url="https://github.com/pris-nlp/naacl2022-reassigned_contrastive_learning_ood" additional="false">pris-nlp/naacl2022-reassigned_contrastive_learning_ood</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="308">
      <title><fixed-case>AISFG</fixed-case>: Abundant Information Slot Filling Generator</title>
      <author><first>Yang</first><last>Yan</last></author>
      <author><first>Junda</first><last>Ye</last></author>
      <author><first>Zhongbao</first><last>Zhang</last></author>
      <author><first>Liwen</first><last>Wang</last></author>
      <pages>4180-4187</pages>
      <abstract>As an essential component of task-oriented dialogue systems, slot filling requires enormous labeled training data in a certain domain. However, in most cases, there is little or no target domain training data is available in the training stage. Thus, cross-domain slot filling has to cope with the data scarcity problem by zero/few-shot learning. Previous researches on zero/few-shot cross-domain slot filling focus on slot descriptions and examples while ignoring the slot type ambiguity and example ambiguity issues. To address these problems, we propose Abundant Information Slot Filling Generator (AISFG), a generative model with a novel query template that incorporates domain descriptions, slot descriptions, and examples with context. Experimental results show that our model outperforms state-of-the-art approaches in zero/few-shot slot filling task.</abstract>
      <url hash="0ea65e0c">2022.naacl-main.308</url>
      <bibkey>yan-etal-2022-aisfg</bibkey>
      <doi>10.18653/v1/2022.naacl-main.308</doi>
      <video href="2022.naacl-main.308.mp4"/>
    </paper>
    <paper id="309">
      <title>Improving negation detection with negation-focused pre-training</title>
      <author><first>Thinh</first><last>Truong</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>4188-4193</pages>
      <abstract>Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text. Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains. We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better incorporate negation information into language models. Extensive experiments on common benchmarks show that our proposed approach improves negation detection performance and generalizability over the strong baseline NegBERT (Khandelwal and Sawant, 2020).</abstract>
      <url hash="59410d4d">2022.naacl-main.309</url>
      <bibkey>truong-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.309</doi>
      <video href="2022.naacl-main.309.mp4"/>
    </paper>
    <paper id="310">
      <title>Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers</title>
      <author><first>Vivek</first><last>Kumar</last></author>
      <author><first>Rishabh</first><last>Maheshwary</last></author>
      <author><first>Vikram</first><last>Pudi</last></author>
      <pages>4194-4206</pages>
      <abstract>Existing Math Word Problem (MWP) solvers have achieved high accuracy on benchmark datasets. However, prior works have shown that such solvers do not generalize well and rely on superficial cues to achieve high performance. In this paper, we first conduct experiments to showcase that this behaviour is mainly associated with the limited size and diversity present in existing MWP datasets. Next, we propose several data augmentation techniques broadly categorized into Substitution and Paraphrasing based methods. By deploying these methods we increase the size of existing datasets by five folds. Extensive experiments on two benchmark datasets across three state-of-the-art MWP solvers shows that proposed methods increase the generalization and robustness of existing solvers. On average, proposed methods significantly increase the state-of-the-art results by over five percentage points on benchmark datasets. Further, the solvers trained on the augmented dataset performs comparatively better on the challenge test set. We also show the effectiveness of proposed techniques through ablation studies and verify the quality of augmented samples through human evaluation.</abstract>
      <url hash="352b1a25">2022.naacl-main.310</url>
      <attachment type="software" hash="4c79ba6d">2022.naacl-main.310.software.zip</attachment>
      <bibkey>kumar-etal-2022-practice</bibkey>
      <doi>10.18653/v1/2022.naacl-main.310</doi>
      <video href="2022.naacl-main.310.mp4"/>
      <pwccode url="https://github.com/kevivk/mwp-augmentation" additional="false">kevivk/mwp-augmentation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/svamp">SVAMP</pwcdataset>
    </paper>
    <paper id="311">
      <title><fixed-case>D</fixed-case>iff<fixed-case>CSE</fixed-case>: Difference-based Contrastive Learning for Sentence Embeddings</title>
      <author><first>Yung-Sung</first><last>Chuang</last></author>
      <author><first>Rumen</first><last>Dangovski</last></author>
      <author><first>Hongyin</first><last>Luo</last></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Shiyu</first><last>Chang</last></author>
      <author><first>Marin</first><last>Soljacic</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>4207-4218</pages>
      <abstract>We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning, which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other “harmful” types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.</abstract>
      <url hash="6b2e04d4">2022.naacl-main.311</url>
      <bibkey>chuang-etal-2022-diffcse</bibkey>
      <doi>10.18653/v1/2022.naacl-main.311</doi>
      <video href="2022.naacl-main.311.mp4"/>
      <pwccode url="https://github.com/voidism/diffcse" additional="false">voidism/diffcse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="312">
      <title>Generative Cross-Domain Data Augmentation for Aspect and Opinion Co-Extraction</title>
      <author><first>Junjie</first><last>Li</last></author>
      <author><first>Jianfei</first><last>Yu</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <pages>4219-4229</pages>
      <abstract>As a fundamental task in opinion mining, aspect and opinion co-extraction aims to identify the aspect terms and opinion terms in reviews. However, due to the lack of fine-grained annotated resources, it is hard to train a robust model for many domains. To alleviate this issue, unsupervised domain adaptation is proposed to transfer knowledge from a labeled source domain to an unlabeled target domain. In this paper, we propose a new Generative Cross-Domain Data Augmentation framework for unsupervised domain adaptation. The proposed framework is aimed to generate target-domain data with fine-grained annotation by exploiting the labeled data in the source domain. Specifically, we remove the domain-specific segments in a source-domain labeled sentence, and then use this as input to a pre-trained sequence-to-sequence model BART to simultaneously generate a target-domain sentence and predict the corresponding label for each word. Experimental results on three datasets demonstrate that our approach is more effective than previous domain adaptation methods.</abstract>
      <url hash="abbd5863">2022.naacl-main.312</url>
      <bibkey>li-etal-2022-generative</bibkey>
      <doi>10.18653/v1/2022.naacl-main.312</doi>
      <video href="2022.naacl-main.312.mp4"/>
      <pwccode url="https://github.com/nustm/gcdda" additional="false">nustm/gcdda</pwccode>
    </paper>
    <paper id="313">
      <title><fixed-case>P</fixed-case>ro<fixed-case>QA</fixed-case>: Structural Prompt-based Pre-training for Unified Question Answering</title>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Yifan</first><last>Gao</last></author>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Jiahai</first><last>Wang</last></author>
      <author><first>Jian</first><last>Yin</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>4230-4243</pages>
      <abstract>Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.</abstract>
      <url hash="a0246799">2022.naacl-main.313</url>
      <bibkey>zhong-etal-2022-proqa</bibkey>
      <doi>10.18653/v1/2022.naacl-main.313</doi>
      <video href="2022.naacl-main.313.mp4"/>
      <pwccode url="https://github.com/zhongwanjun/proqa" additional="false">zhongwanjun/proqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="314">
      <title>A Data Cartography based <fixed-case>M</fixed-case>ix<fixed-case>U</fixed-case>p for Pre-trained Language Models</title>
      <author><first>Seo Yeon</first><last>Park</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>4244-4250</pages>
      <abstract>MixUp is a data augmentation strategy where additional samples are generated during training by combining random pairs of training samples and their labels. However, selecting random pairs is not potentially an optimal choice. In this work, we propose TDMixUp, a novel MixUp strategy that leverages Training Dynamics and allows more informative samples to be combined for generating new data samples. Our proposed TDMixUp first measures confidence, variability, (Swayamdipta et al., 2020), and Area Under the Margin (AUM) (Pleiss et al., 2020) to identify the characteristics of training samples (e.g., as easy-to-learn or ambiguous samples), and then interpolates these characterized samples. We empirically validate that our method not only achieves competitive performance using a smaller subset of the training data compared with strong baselines, but also yields lower expected calibration error on the pre-trained language model, BERT, on both in-domain and out-of-domain settings in a wide range of NLP tasks. We publicly release our code.</abstract>
      <url hash="c08d05dc">2022.naacl-main.314</url>
      <attachment type="software" hash="ef0d0a05">2022.naacl-main.314.software.zip</attachment>
      <bibkey>park-caragea-2022-data</bibkey>
      <doi>10.18653/v1/2022.naacl-main.314</doi>
      <video href="2022.naacl-main.314.mp4"/>
      <pwccode url="https://github.com/seoyeon-p/tdmixup" additional="false">seoyeon-p/tdmixup</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="315">
      <title>Grapheme-to-Phoneme Conversion for <fixed-case>T</fixed-case>hai using Neural Regression Models</title>
      <author><first>Tomohiro</first><last>Yamasaki</last></author>
      <pages>4251-4255</pages>
      <abstract>We propose a novel Thai grapheme-to-phoneme conversion method based on a neural regression model that is trained using neural networks to predict the similarity between a candidate and the correct pronunciation. After generating a set of candidates for an input word or phrase using the orthography rules, this model selects the best-similarity pronunciation from the candidates. This method can be applied to languages other than Thai simply by preparing enough orthography rules, and can reduce the mistakes that neural network models often make. We show that the accuracy of the proposed method is .931, which is comparable to that of encoder-decoder sequence models. We also demonstrate that the proposed method is superior in terms of the difference between correct and predicted pronunciations because incorrect, strange output sometimes occurs when using encoder-decoder sequence models but the error is within the expected range when using the proposed method.</abstract>
      <url hash="bce8e0a8">2022.naacl-main.315</url>
      <attachment type="software" hash="747f2671">2022.naacl-main.315.software.zip</attachment>
      <bibkey>yamasaki-2022-grapheme</bibkey>
      <doi>10.18653/v1/2022.naacl-main.315</doi>
      <video href="2022.naacl-main.315.mp4"/>
    </paper>
    <paper id="316">
      <title>Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation</title>
      <author><first>Siyu</first><last>Lai</last></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Xue</first><last>Zhang</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>4256-4266</pages>
      <abstract>Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through the RTT can be related to either. To remedy this problem, we propose a new definition for NMT adversarial examples based on the Doubly Round-Trip Translation (DRTT). Specifically, apart from the source-target-source RTT, we also consider the target-source-target one, which is utilized to pick out the authentic adversarial examples for the target NMT model. Additionally, to enhance the robustness of the NMT model, we introduce the masked language models to construct bilingual adversarial pairs based on DRTT, which are used to train the NMT model directly. Extensive experiments on both the clean and noisy test sets (including the artificial and natural noise) show that our approach substantially improves the robustness of NMT models.</abstract>
      <url hash="a6dbbc13">2022.naacl-main.316</url>
      <bibkey>lai-etal-2022-generating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.316</doi>
      <video href="2022.naacl-main.316.mp4"/>
      <pwccode url="https://github.com/lisasiyu/drtt" additional="false">lisasiyu/drtt</pwccode>
    </paper>
    <paper id="317">
      <title><fixed-case>TVS</fixed-case>how<fixed-case>G</fixed-case>uess: Character Comprehension in Stories as Speaker Guessing</title>
      <author><first>Yisi</first><last>Sang</last></author>
      <author><first>Xiangyang</first><last>Mou</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Shunyu</first><last>Yao</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Jeffrey</first><last>Stanton</last></author>
      <pages>4267-4287</pages>
      <abstract>We propose a new task for assessing machines’ skills of understanding fictional characters in narrative stories. The task, TVShowGuess, builds on the scripts of TV series and takes the form of guessing the anonymous main characters based on the backgrounds of the scenes and the dialogues. Our human study supports that this form of task covers comprehension of multiple types of character persona, including understanding characters’ personalities, facts and memories of personal experience, which are well aligned with the psychological and literary theories about the theory of mind (ToM) of human beings on understanding fictional characters during reading. We further propose new model architectures to support the contextualized encoding of long scene texts. Experiments show that our proposed approaches significantly outperform baselines, yet still largely lag behind the (nearly perfect) human performance.Our work serves as a first step toward the goal of narrative character comprehension.</abstract>
      <url hash="8d45e64c">2022.naacl-main.317</url>
      <bibkey>sang-etal-2022-tvshowguess</bibkey>
      <doi>10.18653/v1/2022.naacl-main.317</doi>
      <video href="2022.naacl-main.317.mp4"/>
      <pwccode url="https://github.com/yisisang/tvshowguess" additional="false">yisisang/tvshowguess</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
    </paper>
    <paper id="318">
      <title>Causal Distillation for Language Models</title>
      <author><first>Zhengxuan</first><last>Wu</last></author>
      <author><first>Atticus</first><last>Geiger</last></author>
      <author><first>Joshua</first><last>Rozner</last></author>
      <author><first>Elisa</first><last>Kreiss</last></author>
      <author><first>Hanson</first><last>Lu</last></author>
      <author><first>Thomas</first><last>Icard</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <pages>4288-4295</pages>
      <abstract>Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the <i>causal</i> dynamics of the teacher through a distillation interchange intervention training objective (DIITO). DIITO pushes the student model to become a <i>causal abstraction</i> of the teacher model – a faithful model with simpler causal structure. DIITO is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared against standard distillation with the same setting, DIITO results in lower perplexity on the WikiText-103M corpus (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL-2003 (named entity recognition).</abstract>
      <url hash="693b594a">2022.naacl-main.318</url>
      <bibkey>wu-etal-2022-causal</bibkey>
      <doi>10.18653/v1/2022.naacl-main.318</doi>
      <video href="2022.naacl-main.318.mp4"/>
      <pwccode url="https://github.com/frankaging/Causal-Distill" additional="false">frankaging/Causal-Distill</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="319">
      <title><fixed-case>FN</fixed-case>et: Mixing Tokens with <fixed-case>F</fixed-case>ourier Transforms</title>
      <award>Best efficient NLP paper</award>
      <author><first>James</first><last>Lee-Thorp</last></author>
      <author><first>Joshua</first><last>Ainslie</last></author>
      <author><first>Ilya</first><last>Eckstein</last></author>
      <author><first>Santiago</first><last>Ontanon</last></author>
      <pages>4296-4313</pages>
      <abstract>We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the “efficient Transformers” on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.</abstract>
      <url hash="95b32209">2022.naacl-main.319</url>
      <bibkey>lee-thorp-etal-2022-fnet</bibkey>
      <doi>10.18653/v1/2022.naacl-main.319</doi>
      <video href="2022.naacl-main.319.mp4"/>
      <pwccode url="https://github.com/google-research/google-research" additional="true">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="320">
      <title>Answer Consolidation: Formulation and Benchmarking</title>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Heba</first><last>Elfardy</last></author>
      <author><first>Kevin</first><last>Small</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>4314-4325</pages>
      <abstract>Current question answering (QA) systems primarily consider the single-answer scenario, where each question is assumed to be paired with one correct answer. However, in many real-world QA applications, multiple answer scenarios arise where consolidating answers into a comprehensive and non-redundant set of answers is a more efficient user interface. In this paper, we formulate the problem of answer consolidation, where answers are partitioned into multiple groups, each representing different aspects of the answer set. Then, given this partitioning, a comprehensive and non-redundant set of answers can be constructed by picking one answer from each group. To initiate research on answer consolidation, we construct a dataset consisting of 4,699 questions and 24,006 sentences and evaluate multiple models. Despite a promising performance achieved by the best-performing supervised models, we still believe this task has room for further improvements.</abstract>
      <url hash="286f0fb8">2022.naacl-main.320</url>
      <bibkey>zhou-etal-2022-answer</bibkey>
      <doi>10.18653/v1/2022.naacl-main.320</doi>
      <video href="2022.naacl-main.320.mp4"/>
      <pwccode url="https://github.com/amazon-research/question-answer-consolidation" additional="false">amazon-research/question-answer-consolidation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="321">
      <title>Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language</title>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>4326-4331</pages>
      <abstract>Spurious correlations are a threat to the trustworthiness of natural language processing systems, motivating research into methods for identifying and eliminating them. However, addressing the problem of spurious correlations requires more clarity on what they are and how they arise in language data. Gardner et al (2021) argue that the compositional nature of language implies that <i>all</i> correlations between labels and individual “input features” are spurious. This paper analyzes this proposal in the context of a toy example, demonstrating three distinct conditions that can give rise to feature-label correlations in a simple PCFG. Linking the toy example to a structured causal model shows that (1) feature-label correlations can arise even when the label is invariant to interventions on the feature, and (2) feature-label correlations may be absent even when the label is sensitive to interventions on the feature. Because input features will be individually correlated with labels in all but very rare circumstances, domain knowledge must be applied to identify spurious correlations that pose genuine robustness threats.</abstract>
      <url hash="a52cc6cf">2022.naacl-main.321</url>
      <bibkey>eisenstein-2022-informativeness</bibkey>
      <doi>10.18653/v1/2022.naacl-main.321</doi>
      <video href="2022.naacl-main.321.mp4"/>
    </paper>
    <paper id="322">
      <title><fixed-case>FOAM</fixed-case>: A Follower-aware Speaker Model For Vision-and-Language Navigation</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4332-4340</pages>
      <abstract>The speaker-follower models have proven to be effective in vision-and-language navigation, where a speaker model is used to synthesize new instructions to augment the training data for a follower navigation model. However, in previous work, the speaker model is follower-agnostic and fails to take the state of the follower into consideration. In this paper, we present FOAM, a FOllower-Aware speaker Model that is constantly updated given the follower feedback, so that the generated instructions can be more suitable to the current learning state of the follower. Specifically, we optimize the speaker using a bi-level optimization framework and obtain its training signals by evaluating the follower on labeled data. Experimental results on the Room-to-Room and Room-across-Room datasets demonstrate that our methods can outperform strong baseline models across settings. Analyses also reveal that our generated instructions are of higher quality than the baselines.</abstract>
      <url hash="c62ff915">2022.naacl-main.322</url>
      <bibkey>dou-peng-2022-foam</bibkey>
      <doi>10.18653/v1/2022.naacl-main.322</doi>
      <video href="2022.naacl-main.322.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/follower_aware_speaker" additional="false">pluslabnlp/follower_aware_speaker</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="323">
      <title>Improving Compositional Generalization with Latent Structure and Data Augmentation</title>
      <author><first>Linlu</first><last>Qiu</last></author>
      <author><first>Peter</first><last>Shaw</last></author>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Pawel</first><last>Nowak</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <author><first>Kristina</first><last>Toutanova</last></author>
      <pages>4341-4362</pages>
      <abstract>Generic unstructured neural networks have been shown to struggle on out-of-distribution compositional generalization. Compositional data augmentation via example recombination has transferred some prior knowledge about compositionality to such black-box neural models for several semantic parsing tasks, but this often required task-specific engineering or provided limited gains. We present a more powerful data recombination method using a model called Compositional Structure Learner (CSL). CSL is a generative model with a quasi-synchronous context-free grammar backbone, which we induce from the training data. We sample recombined examples from CSL and add them to the fine-tuning data of a pre-trained sequence-to-sequence model (T5). This procedure effectively transfers most of CSL’s compositional bias to T5 for diagnostic tasks, and results in a model even stronger than a T5-CSL ensemble on two real world compositional generalization tasks. This results in new state-of-the-art performance for these challenging semantic parsing tasks requiring generalization to both natural language variation and novel compositions of elements.</abstract>
      <url hash="14ae79a8">2022.naacl-main.323</url>
      <bibkey>qiu-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-main.323</doi>
      <video href="2022.naacl-main.323.mp4"/>
      <pwccode url="https://github.com/google-research/language" additional="true">google-research/language</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="324">
      <title>Joint Extraction of Entities, Relations, and Events via Modeling Inter-Instance and Inter-Label Dependencies</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>4363-4374</pages>
      <abstract>Event trigger detection, entity mention recognition, event argument extraction, and relation extraction are the four important tasks in information extraction that have been performed jointly (Joint Information Extraction - JointIE) to avoid error propagation and leverage dependencies between the task instances (i.e., event triggers, entity mentions, relations, and event arguments). However, previous JointIE models often assume heuristic manually-designed dependency between the task instances and mean-field factorization for the joint distribution of instance labels, thus unable to capture optimal dependencies among instances and labels to improve representation learning and IE performance. To overcome these limitations, we propose to induce a dependency graph among task instances from data to boost representation learning. To better capture dependencies between instance labels, we propose to directly estimate their joint distribution via Conditional Random Fields. Noise Contrastive Estimation is introduced to address the maximization of the intractable joint likelihood for model training. Finally, to improve the decoding with greedy or beam search in prior work, we present Simulated Annealing to better find the globally optimal assignment for instance labels at decoding time. Experimental results show that our proposed model outperforms previous models on multiple IE tasks across 5 datasets and 2 languages.</abstract>
      <url hash="ead5ee4c">2022.naacl-main.324</url>
      <bibkey>nguyen-etal-2022-joint</bibkey>
      <doi>10.18653/v1/2022.naacl-main.324</doi>
      <video href="2022.naacl-main.324.mp4"/>
    </paper>
    <paper id="325">
      <title>Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling</title>
      <author><first>Jakob</first><last>Prange</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <pages>4375-4391</pages>
      <abstract>We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance—outpacing syntactic constituency structures as well as syntactic and semantic dependency structures.</abstract>
      <url hash="ed6ddadb">2022.naacl-main.325</url>
      <bibkey>prange-etal-2022-linguistic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.325</doi>
      <video href="2022.naacl-main.325.mp4"/>
      <pwccode url="https://github.com/jakpra/linguisticstructurelm" additional="false">jakpra/linguisticstructurelm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="326">
      <title>Imagination-Augmented Natural Language Understanding</title>
      <author><first>Yujie</first><last>Lu</last></author>
      <author><first>Wanrong</first><last>Zhu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Miguel</first><last>Eckstein</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>4392-4402</pages>
      <abstract>Human brains integrate linguistic and perceptual information simultaneously to understand natural language, and hold the critical ability to render imaginations. Such abilities enable us to construct new abstract concepts or concrete objects, and are essential in involving practical knowledge to solve problems in low-resource scenarios. However, most existing methods for Natural Language Understanding (NLU) are mainly focused on textual signals. They do not simulate human visual imagination ability, which hinders models from inferring and learning efficiently from limited data samples. Therefore, we introduce an Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language understanding tasks from a novel learning perspective—imagination-augmented cross-modal understanding. iACE enables visual imagination with external knowledge transferred from the powerful generative and pre-trained vision-and-language models. Extensive experiments on GLUE and SWAG show that iACE achieves consistent improvement over visually-supervised pre-trained models. More importantly, results in extreme and normal few-shot settings validate the effectiveness of iACE in low-resource natural language understanding circumstances.</abstract>
      <url hash="9900317a">2022.naacl-main.326</url>
      <bibkey>lu-etal-2022-imagination</bibkey>
      <doi>10.18653/v1/2022.naacl-main.326</doi>
      <video href="2022.naacl-main.326.mp4"/>
      <pwccode url="https://github.com/yujielu10/iace-nlu" additional="false">yujielu10/iace-nlu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="327">
      <title>What company do words keep? Revisiting the distributional semantics of <fixed-case>J</fixed-case>.<fixed-case>R</fixed-case>. Firth &amp; Zellig <fixed-case>H</fixed-case>arris</title>
      <author><first>Mikael</first><last>Brunila</last></author>
      <author><first>Jack</first><last>LaViolette</last></author>
      <pages>4403-4417</pages>
      <abstract>The power of word embeddings is attributed to the linguistic theory that similar words will appear in similar contexts. This idea is specifically invoked by noting that “you shall know a word by the company it keeps,” a quote from British linguist J.R. Firth who, along with his American colleague Zellig Harris, is often credited with the invention of “distributional semantics.” While both Firth and Harris are cited in all major NLP textbooks and many foundational papers, the content and differences between their theories is seldom discussed. Engaging in a close reading of their work, we discover two distinct and in many ways divergent theories of meaning. One focuses exclusively on the internal workings of linguistic forms, while the other invites us to consider words in new company—not just with other linguistic elements, but also in a broader cultural and situational context. Contrasting these theories from the perspective of current debates in NLP, we discover in Firth a figure who could guide the field towards a more culturally grounded notion of semantics. We consider how an expanded notion of “context” might be modeled in practice through two different strategies: comparative stratification and syntagmatic extension.</abstract>
      <url hash="48dcd0f5">2022.naacl-main.327</url>
      <bibkey>brunila-laviolette-2022-company</bibkey>
      <doi>10.18653/v1/2022.naacl-main.327</doi>
      <video href="2022.naacl-main.327.mp4"/>
    </paper>
    <paper id="328">
      <title>Compositional Task-Oriented Parsing as Abstractive Question Answering</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Konstantine</first><last>Arkoudas</last></author>
      <author><first>Weiqi</first><last>Sun</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>4418-4427</pages>
      <abstract>Task-oriented parsing (TOP) aims to convert natural language into machine-readable representations of specific tasks, such as setting an alarm. A popular approach to TOP is to apply seq2seq models to generate linearized parse trees. A more recent line of work argues that pretrained seq2seq2 models are better at generating outputs that are themselves natural language, so they replace linearized parse trees with canonical natural-language paraphrases that can then be easily translated into parse trees, resulting in so-called naturalized parsers. In this work we continue to explore naturalized semantic parsing by presenting a general reduction of TOP to abstractive question answering that overcomes some limitations of canonical paraphrasing. Experimental results show that our QA-based technique outperforms state-of-the-art methods in full-data settings while achieving dramatic improvements in few-shot settings.</abstract>
      <url hash="7504204f">2022.naacl-main.328</url>
      <bibkey>zhao-etal-2022-compositional</bibkey>
      <doi>10.18653/v1/2022.naacl-main.328</doi>
      <video href="2022.naacl-main.328.mp4"/>
      <pwccode url="https://github.com/amazon-research/semantic-parsing-as-abstractive-qa" additional="false">amazon-research/semantic-parsing-as-abstractive-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/topv2">TOPv2</pwcdataset>
    </paper>
    <paper id="329">
      <title>Learning Cross-Lingual <fixed-case>IR</fixed-case> from an <fixed-case>E</fixed-case>nglish Retriever</title>
      <author><first>Yulong</first><last>Li</last></author>
      <author><first>Martin</first><last>Franz</last></author>
      <author><first>Md Arafat</first><last>Sultan</last></author>
      <author><first>Bhavani</first><last>Iyer</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Avirup</first><last>Sil</last></author>
      <pages>4428-4436</pages>
      <abstract>We present DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual Representation), a new cross-lingual information retrieval (CLIR) system trained using multi-stage knowledge distillation (KD). The teacher of DR.DECR relies on a highly effective but computationally expensive two-stage inference process consisting of query translation and monolingual IR, while the student, DR.DECR, executes a single CLIR step. We teach DR.DECR powerful multilingual representations as well as CLIR by optimizing two corresponding KD objectives. Learning useful representations of non-English text from an English-only retriever is accomplished through a cross-lingual token alignment algorithm that relies on the representation capabilities of the underlying multilingual encoders. In both in-domain and zero-shot out-of-domain evaluation, DR.DECR demonstrates far superior accuracy over direct fine-tuning with labeled CLIR data. It is also the best single-model retriever on the XOR-TyDi benchmark at the time of this writing.</abstract>
      <url hash="8286a81f">2022.naacl-main.329</url>
      <bibkey>li-etal-2022-learning-cross</bibkey>
      <doi>10.18653/v1/2022.naacl-main.329</doi>
      <video href="2022.naacl-main.329.mp4"/>
      <pwccode url="https://github.com/primeqa/primeqa" additional="false">primeqa/primeqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="330">
      <title>Testing the Ability of Language Models to Interpret Figurative Language</title>
      <author><first>Emmy</first><last>Liu</last></author>
      <author><first>Chenxuan</first><last>Cui</last></author>
      <author><first>Kenneth</first><last>Zheng</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>4437-4452</pages>
      <abstract>Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.</abstract>
      <url hash="269f205a">2022.naacl-main.330</url>
      <bibkey>liu-etal-2022-testing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.330</doi>
      <video href="2022.naacl-main.330.mp4"/>
      <pwccode url="https://github.com/nightingal3/fig-qa" additional="false">nightingal3/fig-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fig-qa">Fig-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="331">
      <title>Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity</title>
      <author><first>Sheshera</first><last>Mysore</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Tom</first><last>Hope</last></author>
      <pages>4453-4470</pages>
      <abstract>We present a new scientific document similarity model based on matching fine-grained aspects of texts. To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations). Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related. This novel form of textual supervision is used for learning to match aspects across papers. We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover’s Distance between aspects. Our approach improves performance on document similarity tasks in four datasets. Further, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora.</abstract>
      <url hash="777a6cbe">2022.naacl-main.331</url>
      <bibkey>mysore-etal-2022-multi</bibkey>
      <doi>10.18653/v1/2022.naacl-main.331</doi>
      <video href="2022.naacl-main.331.mp4"/>
      <pwccode url="https://github.com/allenai/aspire" additional="false">allenai/aspire</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
    </paper>
    <paper id="332">
      <title><fixed-case>CHAI</fixed-case>: A <fixed-case>CH</fixed-case>atbot <fixed-case>AI</fixed-case> for Task-Oriented Dialogue with Offline Reinforcement Learning</title>
      <author><first>Siddharth</first><last>Verma</last></author>
      <author><first>Justin</first><last>Fu</last></author>
      <author><first>Sherry</first><last>Yang</last></author>
      <author><first>Sergey</first><last>Levine</last></author>
      <pages>4471-4491</pages>
      <abstract>Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL, through trial-and-error learning in human conversations, is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.</abstract>
      <url hash="9611a489">2022.naacl-main.332</url>
      <bibkey>verma-etal-2022-chai</bibkey>
      <doi>10.18653/v1/2022.naacl-main.332</doi>
      <pwccode url="https://github.com/siddharthverma314/chai-naacl-2022" additional="false">siddharthverma314/chai-naacl-2022</pwccode>
    </paper>
    <paper id="333">
      <title>Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer</title>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Youngjae</first><last>Yu</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4492-4507</pages>
      <abstract>Machines that can represent and describe environmental soundscapes have practical potential, e.g., for audio tagging and captioning. Prevailing learning paradigms of audio-text connections have been relying on parallel audio-text data, which is, however, scarcely available on the web. We propose VIP-ANT that induces Audio-Text alignment without using any parallel audio-text data. Our key idea is to share the image modality between bi-modal image-text representations and bi-modal image-audio representations; the image modality functions as a pivot and connects audio and text in a tri-modal embedding space implicitly. In a difficult zero-shot setting with no paired audio-text data, our model demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio classification tasks, and even surpasses the supervised state of the art for Clotho caption retrieval (with audio queries) by 2.2% R@1. We further investigate cases of minimal audio-text supervision, finding that, e.g., just a few hundred supervised audio-text pairs increase the zero-shot audio classification accuracy by 8% on US8K. However, to match human parity on some zero-shot tasks, our empirical scaling experiments suggest that we would need about <tex-math>2^{21} \approx 2\text{M}</tex-math> supervised audio-caption pairs. Our work opens up new avenues for learning audio-text connections with little to no parallel audio-text data.</abstract>
      <url hash="061739da">2022.naacl-main.333</url>
      <bibkey>zhao-etal-2022-connecting</bibkey>
      <doi>10.18653/v1/2022.naacl-main.333</doi>
      <video href="2022.naacl-main.333.mp4"/>
      <pwccode url="https://github.com/zhaoyanpeng/vipant" additional="false">zhaoyanpeng/vipant</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/audiocaps">AudioCaps</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/audioset">AudioSet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clotho">Clotho</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/esc-50">ESC-50</pwcdataset>
    </paper>
    <paper id="334">
      <title><fixed-case>SURF</fixed-case>: Semantic-level Unsupervised Reward Function for Machine Translation</title>
      <author><first>Atijit</first><last>Anuchitanukul</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <pages>4508-4522</pages>
      <abstract>The performance of Reinforcement Learning (RL) for natural language tasks including Machine Translation (MT) is crucially dependent on the reward formulation. This is due to the intrinsic difficulty of the task in the high-dimensional discrete action space as well as the sparseness of the standard reward functions defined for limited set of ground-truth sequences biased towards singular lexical choices. To address this issue, we formulate SURF, a maximally dense semantic-level unsupervised reward function which mimics human evaluation by considering both sentence fluency and semantic similarity. We demonstrate the strong potential of SURF to leverage a family of Actor-Critic Transformer-based Architectures with synchronous and asynchronous multi-agent variants. To tackle the problem of large action-state spaces, each agent is equipped with unique exploration strategies, promoting diversity during its exploration of the hypothesis space. When BLEU scores are compared, our dense unsupervised reward outperforms the standard sparse reward by 2% on average for in- and out-of-domain settings.</abstract>
      <url hash="9d6e3a05">2022.naacl-main.334</url>
      <bibkey>anuchitanukul-ive-2022-surf</bibkey>
      <doi>10.18653/v1/2022.naacl-main.334</doi>
      <video href="2022.naacl-main.334.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="335">
      <title>Disentangling Categorization in Multi-agent Emergent Communication</title>
      <author><first>Washington</first><last>Garcia</last></author>
      <author><first>Hamilton</first><last>Clouse</last></author>
      <author><first>Kevin</first><last>Butler</last></author>
      <pages>4523-4540</pages>
      <abstract>The emergence of language between artificial agents is a recent focus of computational linguistics, as it offers a synthetic substrate for reasoning about human language evolution. From the perspective of cognitive science, sophisticated categorization in humans is thought to enable reasoning about novel observations, and thus compose old information to describe new phenomena. Unfortunately, the literature to date has not managed to isolate the effect of categorization power in artificial agents on their inter-communication ability, particularly on novel, unseen objects. In this work, we propose the use of disentangled representations from representation learning to quantify the categorization power of agents, enabling a differential analysis between combinations of heterogeneous systems, e.g., pairs of agents which learn to communicate despite mismatched concept realization. Through this approach, we observe that agent heterogeneity can cut signaling accuracy by up to 40%, despite encouraging compositionality in the artificial language. We conclude that the reasoning process of agents plays a key role in their communication, with unexpected benefits arising from their mixing, such as better language compositionality.</abstract>
      <url hash="f28be4bc">2022.naacl-main.335</url>
      <attachment type="software" hash="cf76dd0e">2022.naacl-main.335.software.zip</attachment>
      <bibkey>garcia-etal-2022-disentangling</bibkey>
      <doi>10.18653/v1/2022.naacl-main.335</doi>
      <video href="2022.naacl-main.335.mp4"/>
      <pwccode url="https://github.com/fics/disentangling_categorization" additional="false">fics/disentangling_categorization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mini-imagenet">mini-Imagenet</pwcdataset>
    </paper>
    <paper id="336">
      <title>Show, Don’t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue</title>
      <author><first>Raghav</first><last>Gupta</last></author>
      <author><first>Harrison</first><last>Lee</last></author>
      <author><first>Jeffrey</first><last>Zhao</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Abhinav</first><last>Rastogi</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <pages>4541-4549</pages>
      <abstract>Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge. Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics. In this work, we propose Show, Don’t Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions. While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zero-shot generalization - the Schema-Guided Dialogue dataset and the MultiWOZ leave-one-out benchmark.</abstract>
      <url hash="4375e212">2022.naacl-main.336</url>
      <bibkey>gupta-etal-2022-show</bibkey>
      <doi>10.18653/v1/2022.naacl-main.336</doi>
      <video href="2022.naacl-main.336.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="337">
      <title>Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge</title>
      <author><first>Ian</first><last>Porada</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Jackie</first><last>Cheung</last></author>
      <pages>4550-4557</pages>
      <abstract>Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the pre-training minibatches of BERT and evaluate how well the model generalizes to supported inferences after pre-training on the injected knowledge. We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.</abstract>
      <url hash="13095938">2022.naacl-main.337</url>
      <attachment type="software" hash="05333834">2022.naacl-main.337.software.zip</attachment>
      <bibkey>porada-etal-2022-pre</bibkey>
      <doi>10.18653/v1/2022.naacl-main.337</doi>
      <video href="2022.naacl-main.337.mp4"/>
    </paper>
    <paper id="338">
      <title>Using Paraphrases to Study Properties of Contextual Embeddings</title>
      <author><first>Laura</first><last>Burdick</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>4558-4568</pages>
      <abstract>We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database’s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases. We confirm previous findings that BERT is sensitive to word order, but find slightly different patterns than prior work in terms of the level of contextualization across BERT’s layers.</abstract>
      <url hash="7d067c53">2022.naacl-main.338</url>
      <bibkey>burdick-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.naacl-main.338</doi>
      <video href="2022.naacl-main.338.mp4"/>
    </paper>
    <paper id="339">
      <title>Measure and Improve Robustness in <fixed-case>NLP</fixed-case> Models: A Survey</title>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <author><first>Haohan</first><last>Wang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>4569-4586</pages>
      <abstract>As NLP models achieved state-of-the-art performances over benchmarks and gained wide applications, it has been increasingly important to ensure the safe deployment of these models in the real world, e.g., making sure the models are robust against unseen or challenging scenarios. Despite robustness being an increasingly studied topic, it has been separately explored in applications like vision and NLP, with various definitions, evaluation and mitigation strategies in multiple lines of research. In this paper, we aim to provide a unifying survey of how to define, measure and improve robustness in NLP. We first connect multiple definitions of robustness, then unify various lines of work on identifying robustness failures and evaluating models’ robustness. Correspondingly, we present mitigation strategies that are data-driven, model-driven, and inductive-prior-based, with a more systematic view of how to effectively improve robustness in NLP models. Finally, we conclude by outlining open challenges and future directions to motivate further research in this area.</abstract>
      <url hash="2ecf6e3e">2022.naacl-main.339</url>
      <bibkey>wang-etal-2022-measure</bibkey>
      <doi>10.18653/v1/2022.naacl-main.339</doi>
      <video href="2022.naacl-main.339.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
    </paper>
    <paper id="340">
      <title>Learning to Generate Examples for Semantic Processing Tasks</title>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Simone</first><last>Filice</last></author>
      <author><first>Giuseppe</first><last>Castellucci</last></author>
      <author><first>Roberto</first><last>Basili</last></author>
      <pages>4587-4601</pages>
      <abstract>Even if recent Transformer-based architectures, such as BERT, achieved impressive results in semantic processing tasks, their fine-tuning stage still requires large scale training resources. Usually, Data Augmentation (DA) techniques can help to deal with low resource settings. In Text Classification tasks, the objective of DA is the generation of well-formed sentences that i) represent the desired task category and ii) are novel with respect to existing sentences. In this paper, we propose a neural approach to automatically learn to generate new examples using a pre-trained sequence-to-sequence model. We first learn a task-oriented similarity function that we use to pair similar examples. Then, we use these example pairs to train a model to generate examples. Experiments in low resource settings show that augmenting the training material with the proposed strategy systematically improves the results on text classification and natural language inference tasks by up to 10% accuracy, outperforming existing DA approaches.</abstract>
      <url hash="064b6820">2022.naacl-main.340</url>
      <bibkey>croce-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.340</doi>
      <video href="2022.naacl-main.340.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="341">
      <title>Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</title>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Jena</first><last>Hwang</last></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4602-4625</pages>
      <abstract>The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically–as text–in addition to the neural model. We distill only one aspect–the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model’s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.</abstract>
      <url hash="4e593252">2022.naacl-main.341</url>
      <bibkey>west-etal-2022-symbolic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.341</doi>
      <video href="2022.naacl-main.341.mp4"/>
      <pwccode url="https://github.com/peterwestai2/symbolic-knowledge-distillation" additional="false">peterwestai2/symbolic-knowledge-distillation</pwccode>
    </paper>
    <paper id="342">
      <title><fixed-case>G</fixed-case>en<fixed-case>IE</fixed-case>: Generative Information Extraction</title>
      <author><first>Martin</first><last>Josifoski</last></author>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Maxime</first><last>Peyrard</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Robert</first><last>West</last></author>
      <pages>4626-4643</pages>
      <abstract>Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.</abstract>
      <url hash="dbdf25a3">2022.naacl-main.342</url>
      <bibkey>josifoski-etal-2022-genie</bibkey>
      <doi>10.18653/v1/2022.naacl-main.342</doi>
      <video href="2022.naacl-main.342.mp4"/>
      <pwccode url="https://github.com/epfl-dlab/genie" additional="false">epfl-dlab/genie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="343">
      <title>Entity Linking via Explicit Mention-Mention Coreference Modeling</title>
      <author><first>Dhruv</first><last>Agarwal</last></author>
      <author><first>Rico</first><last>Angell</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>4644-4658</pages>
      <abstract>Learning representations of entity mentions is a core component of modern entity linking systems for both candidate generation and making linking predictions. In this paper, we present and empirically analyze a novel training approach for learning mention and entity representations that is based on building minimum spanning arborescences (i.e., directed spanning trees) over mentions and entities across documents to explicitly model mention coreference relationships. We demonstrate the efficacy of our approach by showing significant improvements in both candidate generation recall and linking accuracy on the Zero-Shot Entity Linking dataset and MedMentions, the largest publicly available biomedical dataset. In addition, we show that our improvements in candidate generation yield higher quality re-ranking models downstream, setting a new SOTA result in linking accuracy on MedMentions. Finally, we demonstrate that our improved mention representations are also effective for the discovery of new entities via cross-document coreference.</abstract>
      <url hash="24d206ad">2022.naacl-main.343</url>
      <bibkey>agarwal-etal-2022-entity</bibkey>
      <doi>10.18653/v1/2022.naacl-main.343</doi>
      <video href="2022.naacl-main.343.mp4"/>
      <pwccode url="https://github.com/dhdhagar/arboEL" additional="false">dhdhagar/arboEL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="344">
      <title>Massive-scale Decoding for Text Generation using Lattices</title>
      <author><first>Jiacheng</first><last>Xu</last></author>
      <author><first>Siddhartha</first><last>Jonnalagadda</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>4659-4676</pages>
      <abstract>Conditional neural text generation models generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify pairs of similar generation candidates during search and merge them as an approximation. On both summarization and machine translation, we show that our algorithm encodes thousands of diverse options that remain grammatical and high-quality into one lattice. This algorithm provides a foundation for building downstream generation applications on top of massive-scale diverse outputs.</abstract>
      <url hash="75291262">2022.naacl-main.344</url>
      <bibkey>xu-etal-2022-massive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.344</doi>
      <video href="2022.naacl-main.344.mp4"/>
      <pwccode url="https://github.com/jiacheng-xu/lattice-generation" additional="false">jiacheng-xu/lattice-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="345">
      <title>Disentangling Indirect Answers to Yes-No Questions in Real Conversations</title>
      <author><first>Krishna</first><last>Sanagavarapu</last></author>
      <author><first>Jathin</first><last>Singaraju</last></author>
      <author><first>Anusha</first><last>Kakileti</last></author>
      <author><first>Anirudh</first><last>Kaza</last></author>
      <author><first>Aaron</first><last>Mathews</last></author>
      <author><first>Helen</first><last>Li</last></author>
      <author><first>Nathan</first><last>Brito</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>4677-4695</pages>
      <abstract>In this paper, we explore the task of determining indirect answers to yes-no questions in real conversations. We work with transcripts of phone conversations in the Switchboard Dialog Act (SwDA) corpus and create SwDA-IndirectAnswers (SwDA-IA), a subset of SwDA consisting of all conversations containing a yes-no question with an indirect answer. We annotate the underlying direct answers to the yes-no questions (yes, probably yes, middle, probably no, or no). We show that doing so requires taking into account conversation context: the indirect answer alone is insufficient to determine the ground truth. Experimental results also show that taking into account context is beneficial. More importantly, our results demonstrate that existing corpora with synthetic indirect answers to yes-no questions are not beneficial when working with real conversations. Our best models outperform the majority baseline by a substantial margin, but the task remains a challenge (F1: 0.46).</abstract>
      <url hash="af65cd40">2022.naacl-main.345</url>
      <bibkey>sanagavarapu-etal-2022-disentangling</bibkey>
      <doi>10.18653/v1/2022.naacl-main.345</doi>
      <video href="2022.naacl-main.345.mp4"/>
      <pwccode url="https://github.com/krishna-chaitanya-sanagavarapu/swda-ia" additional="false">krishna-chaitanya-sanagavarapu/swda-ia</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="346">
      <title>Quantifying Adaptability in Pre-trained Language Models with 500 Tasks</title>
      <author><first>Belinda</first><last>Li</last></author>
      <author><first>Jane</first><last>Yu</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Alon</first><last>Halevy</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>4696-4715</pages>
      <abstract>When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.</abstract>
      <url hash="5e175ff1">2022.naacl-main.346</url>
      <bibkey>li-etal-2022-quantifying</bibkey>
      <doi>10.18653/v1/2022.naacl-main.346</doi>
      <video href="2022.naacl-main.346.mp4"/>
      <pwccode url="https://github.com/facebookresearch/task_bench" additional="false">facebookresearch/task_bench</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="347">
      <title>Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection</title>
      <author><first>Indira</first><last>Sen</last></author>
      <author><first>Mattia</first><last>Samory</last></author>
      <author><first>Claudia</first><last>Wagner</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>4716-4726</pages>
      <abstract>Counterfactually Augmented Data (CAD) aims to improve out-of-domain generalizability, an indicator of model robustness. The improvement is credited to promoting core features of the construct over spurious artifacts that happen to correlate with it. Yet, over-relying on core features may lead to unintended model bias. Especially, construct-driven CAD—perturbations of core features—may induce models to ignore the context in which core features are used. Here, we test models for sexism and hate speech detection on challenging data: non-hate and non-sexist usage of identity and gendered terms. On these hard cases, models trained on CAD, especially construct-driven CAD, show higher false positive rates than models trained on the original, unperturbed data. Using a diverse set of CAD—construct-driven and construct-agnostic—reduces such unintended bias.</abstract>
      <url hash="e3ab12f8">2022.naacl-main.347</url>
      <attachment type="software" hash="a6e23482">2022.naacl-main.347.software.zip</attachment>
      <bibkey>sen-etal-2022-counterfactually</bibkey>
      <doi>10.18653/v1/2022.naacl-main.347</doi>
      <video href="2022.naacl-main.347.mp4"/>
    </paper>
    <paper id="348">
      <title>A Study of the Attention Abnormality in Trojaned <fixed-case>BERT</fixed-case>s</title>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Songzhu</first><last>Zheng</last></author>
      <author><first>Tengfei</first><last>Ma</last></author>
      <author><first>Chao</first><last>Chen</last></author>
      <pages>4727-4741</pages>
      <abstract>Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based Trojan detector to distinguish Trojaned models from clean ones. To the best of our knowledge, we are the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer’s attention.</abstract>
      <url hash="b7201aea">2022.naacl-main.348</url>
      <bibkey>lyu-etal-2022-study</bibkey>
      <doi>10.18653/v1/2022.naacl-main.348</doi>
      <video href="2022.naacl-main.348.mp4"/>
      <pwccode url="https://github.com/weimin17/attention_abnormality_in_trojaned_berts" additional="false">weimin17/attention_abnormality_in_trojaned_berts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="349">
      <title><fixed-case>EP</fixed-case>i<fixed-case>DA</fixed-case>: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification</title>
      <author><first>Minyi</first><last>Zhao</last></author>
      <author><first>Lu</first><last>Zhang</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Jiandong</first><last>Ding</last></author>
      <author><first>Jihong</first><last>Guan</last></author>
      <author><first>Shuigeng</first><last>Zhou</last></author>
      <pages>4742-4752</pages>
      <abstract>Recent works have empirically shown the effectiveness of data augmentation (DA) in NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fully mine the potential of DA for NLP. In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effective text classification. EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EPiDA outperforms existing SOTA methods in most cases, though not using any agent networks or pre-trained generation networks, and it works well with various DA algorithms and classification models.</abstract>
      <url hash="90f9912c">2022.naacl-main.349</url>
      <attachment type="software" hash="d5dbfa51">2022.naacl-main.349.software.zip</attachment>
      <bibkey>zhao-etal-2022-epida</bibkey>
      <doi>10.18653/v1/2022.naacl-main.349</doi>
      <video href="2022.naacl-main.349.mp4"/>
      <pwccode url="https://github.com/zhaominyiz/epida" additional="false">zhaominyiz/epida</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="350">
      <title>Partial-input baselines show that <fixed-case>NLI</fixed-case> models can ignore context, but they don’t.</title>
      <author><first>Neha</first><last>Srikanth</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>4753-4763</pages>
      <abstract>When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether state-of-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model’s sensitivity to edited contexts. Our results indicate that NLI models are still capable of learning to condition on context—a necessary component of inferential reasoning—despite being trained on artifact-ridden datasets.</abstract>
      <url hash="8c91473e">2022.naacl-main.350</url>
      <bibkey>srikanth-rudinger-2022-partial</bibkey>
      <doi>10.18653/v1/2022.naacl-main.350</doi>
      <video href="2022.naacl-main.350.mp4"/>
      <pwccode url="https://github.com/nehasrikn/context-editing" additional="false">nehasrikn/context-editing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="351">
      <title>Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora</title>
      <author><first>Xisen</first><last>Jin</last></author>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Xiaokai</first><last>Wei</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>4764-4780</pages>
      <abstract>Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM’s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.</abstract>
      <url hash="b700d71b">2022.naacl-main.351</url>
      <bibkey>jin-etal-2022-lifelong-pretraining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.351</doi>
      <video href="2022.naacl-main.351.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="352">
      <title>Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition</title>
      <author><first>Pengshan</first><last>Cai</last></author>
      <author><first>Hui</first><last>Wan</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>4781-4796</pages>
      <abstract>We propose novel AI-empowered chat bots for learning as conversation where a user does not read a passage but gains information and knowledge through conversation with a teacher bot. Our information acquisition-oriented dialogue system employs a novel adaptation of reinforced self-play so that the system can be transferred to various domains without in-domain dialogue data, and can carry out conversations both informative and attentive to users.</abstract>
      <url hash="45dc1055">2022.naacl-main.352</url>
      <bibkey>cai-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.352</doi>
      <video href="2022.naacl-main.352.mp4"/>
      <pwccode url="https://github.com/ibm/reinforced-dialog-system-for-learning" additional="false">ibm/reinforced-dialog-system-for-learning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="353">
      <title>Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank <fixed-case>HMM</fixed-case>s and <fixed-case>PCFG</fixed-case>s</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>4797-4809</pages>
      <abstract>Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) are widely used structured models, both of which can be represented as factor graph grammars (FGGs), a powerful formalism capable of describing a wide range of models. Recent research found it beneficial to use large state spaces for HMMs and PCFGs. However, inference with large state spaces is computationally demanding, especially for PCFGs. To tackle this challenge, we leverage tensor rank decomposition (aka. CPD) to decrease inference computational complexities for a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of an FGG and then construct a new FGG defined in the rank space. Inference with the new FGG produces the same result but has a lower time complexity when the rank size is smaller than the state size. We conduct experiments on HMM language modeling and unsupervised PCFG parsing, showing better performance than previous work. Our code is publicly available at <url>https://github.com/VPeterV/RankSpace-Models</url>.</abstract>
      <url hash="5f55c923">2022.naacl-main.353</url>
      <bibkey>yang-etal-2022-dynamic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.353</doi>
      <video href="2022.naacl-main.353.mp4"/>
      <pwccode url="https://github.com/sustcsonglin/TN-PCFG" additional="true">sustcsonglin/TN-PCFG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="354">
      <title>What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review</title>
      <author><first>Terne</first><last>Thorn Jakobsen</last></author>
      <author><first>Anna</first><last>Rogers</last></author>
      <pages>4810-4823</pages>
      <abstract>Both scientific progress and individual researcher careers depend on the quality of peer review, which in turn depends on paper-reviewer matching. Surprisingly, this problem has been mostly approached as an automated recommendation problem rather than as a matter where different stakeholders (area chairs, reviewers, authors) have accumulated experience worth taking into account. We present the results of the first survey of the NLP community, identifying common issues and perspectives on what factors should be considered by paper-reviewer matching systems. This study contributes actionable recommendations for improving future NLP conferences, and desiderata for interpretable peer review assignments.</abstract>
      <url hash="c9f6cd1b">2022.naacl-main.354</url>
      <bibkey>thorn-jakobsen-rogers-2022-factors</bibkey>
      <doi>10.18653/v1/2022.naacl-main.354</doi>
      <video href="2022.naacl-main.354.mp4"/>
      <pwccode url="https://github.com/terne/paper-reviewer-matching-surveys" additional="false">terne/paper-reviewer-matching-surveys</pwccode>
    </paper>
    <paper id="355">
      <title>Reducing Disambiguation Biases in <fixed-case>NMT</fixed-case> by Leveraging Explicit Word Sense Information</title>
      <author><first>Niccolò</first><last>Campolungo</last></author>
      <author><first>Tommaso</first><last>Pasini</last></author>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>4824-4838</pages>
      <abstract>Recent studies have shed some light on a common pitfall of Neural Machine Translation (NMT) models, stemming from their struggle to disambiguate polysemous words without lapsing into their most frequently occurring senses in the training corpus.In this paper, we first provide a novel approach for automatically creating high-precision sense-annotated parallel corpora, and then put forward a specifically tailored fine-tuning strategy for exploiting these sense annotations during training without introducing any additional requirement at inference time.The use of explicit senses proved to be beneficial to reduce the disambiguation bias of a baseline NMT model, while, at the same time, leading our system to attain higher BLEU scores than its vanilla counterpart in 3 language pairs.</abstract>
      <url hash="67350ae6">2022.naacl-main.355</url>
      <attachment type="software" hash="dfbd348a">2022.naacl-main.355.software.zip</attachment>
      <bibkey>campolungo-etal-2022-reducing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.355</doi>
      <video href="2022.naacl-main.355.mp4"/>
    </paper>
    <paper id="356">
      <title>Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting</title>
      <author><first>Shuzheng</first><last>Si</last></author>
      <author><first>Shuang</first><last>Zeng</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <pages>4839-4847</pages>
      <abstract>Incomplete utterance rewriting has recently raised wide attention. However, previous works do not consider the semantic structural information between incomplete utterance and rewritten utterance or model the semantic structure implicitly and insufficiently. To address this problem, we propose a QUEry-Enhanced Network(QUEEN) to solve this problem. Firstly, our proposed query template explicitly brings guided semantic structural knowledge between the incomplete utterance and the rewritten utterance making model perceive where to refer back to or recover omitted tokens. Then, we adopt a fast and effective edit operation scoring network to model the relation between two tokens. Benefiting from extra information and the well-designed network, QUEEN achieves state-of-the-art performance on several public datasets.</abstract>
      <url hash="75037155">2022.naacl-main.356</url>
      <bibkey>si-etal-2022-mining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.356</doi>
      <video href="2022.naacl-main.356.mp4"/>
    </paper>
    <paper id="357">
      <title>Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization</title>
      <author><first>Lulu</first><last>Zhao</last></author>
      <author><first>Fujia</first><last>Zheng</last></author>
      <author><first>Weihao</first><last>Zeng</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Huixing</first><last>Jiang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <pages>4848-4862</pages>
      <abstract>The most advanced abstractive dialogue summarizers lack generalization ability on new domains and the existing researches for domain adaptation in summarization generally rely on large-scale pre-trainings. To explore the lightweight fine-tuning methods for domain adaptation of dialogue summarization, in this paper, we propose an efficient and generalizable Domain-Oriented Prefix-tuning model, which utilizes a domain word initialized prefix module to alleviate domain entanglement and adopts discrete prompts to guide the model to focus on key contents of dialogues and enhance model generalization. We conduct zero-shot experiments and build domain adaptation benchmarks on two multi-domain dialogue summarization datasets, TODSum and QMSum. Adequate experiments and qualitative analysis prove the effectiveness of our methods.</abstract>
      <url hash="1769416e">2022.naacl-main.357</url>
      <bibkey>zhao-etal-2022-domain</bibkey>
      <doi>10.18653/v1/2022.naacl-main.357</doi>
      <video href="2022.naacl-main.357.mp4"/>
      <pwccode url="https://github.com/Zeng-WH/DOP-Tuning" additional="false">Zeng-WH/DOP-Tuning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="358">
      <title>Interactive Symbol Grounding with Complex Referential Expressions</title>
      <author><first>Rimvydas</first><last>Rubavicius</last></author>
      <author><first>Alex</first><last>Lascarides</last></author>
      <pages>4863-4874</pages>
      <abstract>We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. “all but one green square above both red circles.”) and its designation in the visual scene. Our distinctive approach combines: a) lazy few-shot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation. We use this combination to estimate new training examples for grounding symbols that occur <i>within</i> a noun phrase but aren’t designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency. We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts. </abstract>
      <url hash="c369122f">2022.naacl-main.358</url>
      <attachment type="software" hash="b081d068">2022.naacl-main.358.software.zip</attachment>
      <bibkey>rubavicius-lascarides-2022-interactive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.358</doi>
      <video href="2022.naacl-main.358.mp4"/>
    </paper>
    <paper id="359">
      <title>Generalized Quantifiers as a Source of Error in Multilingual <fixed-case>NLU</fixed-case> Benchmarks</title>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>4875-4893</pages>
      <abstract>Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today’s NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task (GQNLI) and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.</abstract>
      <url hash="981b5c06">2022.naacl-main.359</url>
      <bibkey>cui-etal-2022-generalized-quantifiers</bibkey>
      <doi>10.18653/v1/2022.naacl-main.359</doi>
    </paper>
    <paper id="360">
      <title>Exact Paired-Permutation Testing for Structured Test Statistics</title>
      <author><first>Ran</first><last>Zmigrod</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>4894-4902</pages>
      <abstract>Significance testing—especially the paired-permutation test—has played a vital role in developing NLP systems to provide confidence that the difference in performance between two systems (i.e., the test statistic) is not due to luck. However, practitioners rely on Monte Carlo approximation to perform this test due to a lack of a suitable exact algorithm. In this paper, we provide an efficient exact algorithm for the paired-permutation test for a family of structured test statistics. Our algorithm runs in <tex-math>\mathcal{O}(G N (\log GN )(\log N))</tex-math> time where <tex-math>N</tex-math> is the dataset size and <tex-math>G</tex-math> is the range of the test statistic. We found that our exact algorithm was 10x faster than the Monte Carlo approximation with 20000 samples on a common dataset</abstract>
      <url hash="8f967f54">2022.naacl-main.360</url>
      <bibkey>zmigrod-etal-2022-exact</bibkey>
      <doi>10.18653/v1/2022.naacl-main.360</doi>
      <pwccode url="https://github.com/rycolab/paired-perm-test" additional="false">rycolab/paired-perm-test</pwccode>
    </paper>
    <paper id="361">
      <title>A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank</title>
      <award>Honorable mention for contribution to methods</award>
      <author><first>Dan</first><last>Malkin</last></author>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>4903-4915</pages>
      <abstract>We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zero-shot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential exhaustive computation of all possible combinations. We find that our method is effective on a diverse set of languages spanning different linguistic features and two downstream tasks.Our findings can inform developers of large-scale multilingual language models in choosing better pretraining configurations.</abstract>
      <url hash="805dd911">2022.naacl-main.361</url>
      <bibkey>malkin-etal-2022-balanced</bibkey>
      <doi>10.18653/v1/2022.naacl-main.361</doi>
      <video href="2022.naacl-main.361.mp4"/>
      <pwccode url="https://github.com/slab-nlp/linguistic-blood-bank" additional="false">slab-nlp/linguistic-blood-bank</pwccode>
    </paper>
    <paper id="362">
      <title><fixed-case>SSEGCN</fixed-case>: Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis</title>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Zili</first><last>Zhou</last></author>
      <author><first>Yanna</first><last>Wang</last></author>
      <pages>4916-4925</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA) aims to predict the sentiment polarity towards a particular aspect in a sentence. Recently, graph neural networks based on dependency tree convey rich structural information which is proven to be utility for ABSA. However, how to effectively harness the semantic and syntactic structure information from the dependency tree remains a challenging research question. In this paper, we propose a novel Syntactic and Semantic Enhanced Graph Convolutional Network (SSEGCN) model for ABSA task. Specifically, we propose an aspect-aware attention mechanism combined with self-attention to obtain attention score matrices of a sentence, which can not only learn the aspect-related semantic correlations, but also learn the global semantics of the sentence. In order to obtain comprehensive syntactic structure information, we construct syntactic mask matrices of the sentence according to the different syntactic distances between words. Furthermore, to combine syntactic structure and semantic information, we equip the attention score matrices by syntactic mask matrices. Finally, we enhance the node representations with graph convolutional network over attention score matrices for ABSA. Experimental results on benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods.</abstract>
      <url hash="6e45c9b6">2022.naacl-main.362</url>
      <bibkey>zhang-etal-2022-ssegcn</bibkey>
      <doi>10.18653/v1/2022.naacl-main.362</doi>
      <video href="2022.naacl-main.362.mp4"/>
      <pwccode url="https://github.com/zhangzheng1997/ssegcn-absa" additional="false">zhangzheng1997/ssegcn-absa</pwccode>
    </paper>
    <paper id="363">
      <title>Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy</title>
      <author><first>Allison</first><last>Lahnala</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <author><first>Béla</first><last>Neuendorf</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>4926-4938</pages>
      <abstract>Large pre-trained neural language models have supported the effectiveness of many NLP tasks, yet are still prone to generating toxic language hindering the safety of their use. Using empathetic data, we improve over recent work on controllable text generation that aims to reduce the toxicity of generated text. We find we are able to dramatically reduce the size of fine-tuning data to 7.5-30k samples while at the same time making significant improvements over state-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26% relative) from the original work on 2.3m samples, by strategically sampling data based on empathy scores. We observe that the degree of improvements is subject to specific communication components of empathy. In particular, the more cognitive components of empathy significantly beat the original dataset in almost all experiments, while emotional empathy was tied to less improvement and even underperforming random samples of the original data. This is a particularly implicative insight for NLP work concerning empathy as until recently the research and resources built for it have exclusively considered empathy as an emotional concept.</abstract>
      <url hash="a0a593d6">2022.naacl-main.363</url>
      <bibkey>lahnala-etal-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.363</doi>
      <video href="2022.naacl-main.363.mp4"/>
    </paper>
    <paper id="364">
      <title><fixed-case>DUCK</fixed-case>: Rumour Detection on Social Media by Modelling User and Comment Propagation Networks</title>
      <author><first>Lin</first><last>Tian</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>4939-4949</pages>
      <abstract>Social media rumours, a form of misinformation, can mislead the public and cause significant economic and social disruption. Motivated by the observation that the user network — which captures <tex-math>\textit{who}</tex-math> engage with a story — and the comment network — which captures <tex-math>\textit{how}</tex-math> they react to it — provide complementary signals for rumour detection, in this paper, we propose DUCK (rumour <tex-math>\underline{d}</tex-math>etection with <tex-math>\underline{u}</tex-math>ser and <tex-math>\underline{c}</tex-math>omment networ<tex-math>\underline{k}</tex-math>s) for rumour detection on social media. We study how to leverage transformers and graph attention networks to jointly model the contents and structure of social media conversations, as well as the network of users who engaged in these conversations. Over four widely used benchmark rumour datasets in English and Chinese, we show that DUCK produces superior performance for detecting rumours, creating a new state-of-the-art. Source code for DUCK is available at: https://github.com/l tian678/DUCK-code.</abstract>
      <url hash="c63b3395">2022.naacl-main.364</url>
      <attachment type="software" hash="97bc25ea">2022.naacl-main.364.software.zip</attachment>
      <bibkey>tian-etal-2022-duck</bibkey>
      <doi>10.18653/v1/2022.naacl-main.364</doi>
      <video href="2022.naacl-main.364.mp4"/>
      <pwccode url="https://github.com/ltian678/duck-code" additional="false">ltian678/duck-code</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coaid">CoAID</pwcdataset>
    </paper>
    <paper id="365">
      <title>Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with <fixed-case>SCONES</fixed-case></title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <pages>4950-4961</pages>
      <abstract>The softmax layer in neural machine translation is designed to model the distribution over mutually exclusive tokens. Machine translation, however, is intrinsically uncertain: the same source sentence can have multiple semantically equivalent translations. Therefore, we propose to replace the softmax activation with a multi-label classification layer that can model ambiguity more effectively. We call our loss function Single-label Contrastive Objective for Non-Exclusive Sequences (SCONES). We show that the multi-label output layer can still be trained on single reference training data using the SCONES loss function. SCONES yields consistent BLEU score gains across six translation directions, particularly for medium-resource language pairs and small beam sizes. By using smaller beam sizes we can speed up inference by a factor of 3.9x and still match or improve the BLEU score obtained using softmax. Furthermore, we demonstrate that SCONES can be used to train NMT models that assign the highest probability to adequate translations, thus mitigating the “beam search curse”. Additional experiments on synthetic language pairs with varying levels of uncertainty suggest that the improvements from SCONES can be attributed to better handling of ambiguity.</abstract>
      <url hash="7fbf1ca1">2022.naacl-main.365</url>
      <bibkey>stahlberg-kumar-2022-jam</bibkey>
      <doi>10.18653/v1/2022.naacl-main.365</doi>
      <video href="2022.naacl-main.365.mp4"/>
    </paper>
    <paper id="366">
      <title><fixed-case>S</fixed-case>kill<fixed-case>S</fixed-case>pan: Hard and Soft Skill Extraction from <fixed-case>E</fixed-case>nglish Job Postings</title>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Kristian</first><last>Jensen</last></author>
      <author><first>Sif</first><last>Sonniks</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>4962-4984</pages>
      <abstract>Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.</abstract>
      <url hash="e80151ba">2022.naacl-main.366</url>
      <bibkey>zhang-etal-2022-skillspan</bibkey>
      <doi>10.18653/v1/2022.naacl-main.366</doi>
      <video href="2022.naacl-main.366.mp4"/>
      <pwccode url="https://github.com/Kaleidophon/deep-significance" additional="true">Kaleidophon/deep-significance</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/skillspan">SkillSpan</pwcdataset>
    </paper>
    <paper id="367">
      <title><fixed-case>RAAT</fixed-case>: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction</title>
      <author><first>Yuan</first><last>Liang</last></author>
      <author><first>Zhuoxuan</first><last>Jiang</last></author>
      <author><first>Di</first><last>Yin</last></author>
      <author><first>Bo</first><last>Ren</last></author>
      <pages>4985-4997</pages>
      <abstract>In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.</abstract>
      <url hash="5a37bec8">2022.naacl-main.367</url>
      <bibkey>liang-etal-2022-raat</bibkey>
      <doi>10.18653/v1/2022.naacl-main.367</doi>
      <video href="2022.naacl-main.367.mp4"/>
      <pwccode url="https://github.com/TencentYoutuResearch/EventExtraction-RAAT" additional="false">TencentYoutuResearch/EventExtraction-RAAT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chfinann">ChFinAnn</pwcdataset>
    </paper>
    <paper id="368">
      <title>A Double-Graph Based Framework for Frame Semantic Parsing</title>
      <author><first>Ce</first><last>Zheng</last></author>
      <author><first>Xudong</first><last>Chen</last></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <pages>4998-5011</pages>
      <abstract>Frame semantic parsing is a fundamental NLP task, which consists of three subtasks: frame identification, argument identification and role classification. Most previous studies tend to neglect relations between different subtasks and arguments and pay little attention to ontological frame knowledge defined in FrameNet. In this paper, we propose a Knowledge-guided Incremental semantic parser with Double-graph (KID). We first introduce Frame Knowledge Graph (FKG), a heterogeneous graph containing both frames and FEs (Frame Elements) built on the frame knowledge so that we can derive knowledge-enhanced representations for frames and FEs. Besides, we propose Frame Semantic Graph (FSG) to represent frame semantic structures extracted from the text with graph structures. In this way, we can transform frame semantic parsing into an incremental graph construction problem to strengthen interactions between subtasks and relations between arguments. Our experiments show that KID outperforms the previous state-of-the-art method by up to 1.7 F1-score on two FrameNet datasets. Our code is availavle at https://github.com/PKUnlp-icler/KID.</abstract>
      <url hash="7cb68d7d">2022.naacl-main.368</url>
      <bibkey>zheng-etal-2022-double</bibkey>
      <doi>10.18653/v1/2022.naacl-main.368</doi>
      <video href="2022.naacl-main.368.mp4"/>
      <pwccode url="https://github.com/pkunlp-icler/kid" additional="false">pkunlp-icler/kid</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="369">
      <title>An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling</title>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Qingyu</first><last>Zhou</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>5012-5024</pages>
      <abstract>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging models, e.g., named entity recognition and slot filling, to generalize on an emerging, resource-scarce domain. Recently, the metric-based meta-learning framework has been recognized as a promising approach for FSSL. However, most prior works assign a label to each token based on the token-level similarities, which ignores the integrality of named entities or slots. To this end, in this paper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL. ESD formulates FSSL as a span-level matching problem between test query and supporting instances. Specifically, ESD decomposes the span matching problem into a series of span-level procedures, mainly including enhanced span representation, class prototype aggregation and span conflicts resolution. Extensive experiments show that ESD achieves the new state-of-the-art results on two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more robust in the noisy and nested tagging scenarios.</abstract>
      <url hash="2f295e5c">2022.naacl-main.369</url>
      <attachment type="software" hash="9c72225c">2022.naacl-main.369.software.zip</attachment>
      <bibkey>wang-etal-2022-enhanced</bibkey>
      <doi>10.18653/v1/2022.naacl-main.369</doi>
      <video href="2022.naacl-main.369.mp4"/>
      <pwccode url="https://github.com/wangpeiyi9979/esd" additional="false">wangpeiyi9979/esd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/few-nerd">Few-NERD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="370">
      <title>A Two-Stream <fixed-case>AMR</fixed-case>-enhanced Model for Document-level Event Argument Extraction</title>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Shuang</first><last>Zeng</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>5025-5036</pages>
      <abstract>Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a <b>T</b>wo-<b>S</b>tream <b>A</b>bstract meaning <b>R</b>epresentation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.</abstract>
      <url hash="da6ff932">2022.naacl-main.370</url>
      <bibkey>xu-etal-2022-two</bibkey>
      <doi>10.18653/v1/2022.naacl-main.370</doi>
      <video href="2022.naacl-main.370.mp4"/>
      <pwccode url="https://github.com/pkunlp-icler/tsar" additional="false">pkunlp-icler/tsar</pwccode>
    </paper>
    <paper id="371">
      <title>Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning</title>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Zhewei</first><last>Xu</last></author>
      <author><first>Pedro</first><last>Szekely</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>5037-5048</pages>
      <abstract>Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.</abstract>
      <url hash="dab45fe3">2022.naacl-main.371</url>
      <bibkey>wang-etal-2022-robust</bibkey>
      <doi>10.18653/v1/2022.naacl-main.371</doi>
      <video href="2022.naacl-main.371.mp4"/>
      <pwccode url="https://github.com/luka-group/lattice" additional="false">luka-group/lattice</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
    </paper>
    <paper id="372">
      <title><fixed-case>J</fixed-case>oint<fixed-case>LK</fixed-case>: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering</title>
      <author><first>Yueqing</first><last>Sun</last></author>
      <author><first>Qi</first><last>Shi</last></author>
      <author><first>Le</first><last>Qi</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <pages>5049-5060</pages>
      <abstract>Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning.</abstract>
      <url hash="d01aa0c1">2022.naacl-main.372</url>
      <bibkey>sun-etal-2022-jointlk</bibkey>
      <doi>10.18653/v1/2022.naacl-main.372</doi>
      <video href="2022.naacl-main.372.mp4"/>
      <pwccode url="https://github.com/yueqing-sun/jointlk" additional="false">yueqing-sun/jointlk</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
    </paper>
    <paper id="373">
      <title>Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens</title>
      <author><first>Itay</first><last>Itzhak</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>5061-5068</pages>
      <abstract>Standard pretrained language models operateon sequences of subword tokens without direct access to the characters that compose eachtoken’s string representation. We probe theembedding layer of pretrained language models and show that models learn the internalcharacter composition of whole word and subword tokens to a surprising extent, withoutever seeing the characters coupled with the tokens. Our results show that the embedding layers of RoBERTa and GPT2 each hold enoughinformation to accurately spell up to a thirdof the vocabulary and reach high characterngram overlap across all token types. We further test whether enriching subword modelswith character information can improve language modeling, and observe that this methodhas a near-identical learning curve as training without spelling-based enrichment. Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesnot appear to enhance its performance on suchtasks.</abstract>
      <url hash="1c94303b">2022.naacl-main.373</url>
      <bibkey>itzhak-levy-2022-models</bibkey>
      <doi>10.18653/v1/2022.naacl-main.373</doi>
      <video href="2022.naacl-main.373.mp4"/>
      <pwccode url="https://github.com/itay1itzhak/spellingbee" additional="false">itay1itzhak/spellingbee</pwccode>
    </paper>
    <paper id="374">
      <title>A Corpus for Understanding and Generating Moral Stories</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Ziqi</first><last>Liu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>5069-5087</pages>
      <abstract>Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.</abstract>
      <url hash="d3cf0354">2022.naacl-main.374</url>
      <attachment type="software" hash="6e23a2bb">2022.naacl-main.374.software.zip</attachment>
      <bibkey>guan-etal-2022-corpus</bibkey>
      <doi>10.18653/v1/2022.naacl-main.374</doi>
      <video href="2022.naacl-main.374.mp4"/>
      <pwccode url="https://github.com/thu-coai/moralstory" additional="false">thu-coai/moralstory</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/moral-stories">Moral Stories</pwcdataset>
    </paper>
    <paper id="375">
      <title>Modeling Multi-Granularity Hierarchical Features for Relation Extraction</title>
      <author><first>Xinnian</first><last>Liang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>5088-5098</pages>
      <abstract>Relation extraction is a key task in Natural Language Processing (NLP), which aims to extract relations between entity pairs from given texts. Recently, relation extraction (RE) has achieved remarkable progress with the development of deep neural networks. Most existing research focuses on constructing explicit structured features using external knowledge such as knowledge graph and dependency tree. In this paper, we propose a novel method to extract multi-granularity features based solely on the original input sentences. We show that effective structured features can be attained even without external knowledge. Three kinds of features based on the input sentences are fully exploited, which are in entity mention level, segment level, and sentence level. All the three are jointly and hierarchically modeled. We evaluate our method on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred Revisited. To verify the effectiveness, we apply our method to different encoders such as LSTM and BERT. Experimental results show that our method significantly outperforms existing state-of-the-art models that even use external knowledge. Extensive analyses demonstrate that the performance of our model is contributed by the capture of multi-granularity features and the model of their hierarchical structure.</abstract>
      <url hash="743e21b2">2022.naacl-main.375</url>
      <bibkey>liang-etal-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.naacl-main.375</doi>
      <video href="2022.naacl-main.375.mp4"/>
      <pwccode url="https://github.com/xnliang98/sms" additional="false">xnliang98/sms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="376">
      <title>Cross-modal Contrastive Learning for Speech Translation</title>
      <author><first>Rong</first><last>Ye</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>5099-5113</pages>
      <abstract>How can we learn unified representations for spoken utterances and their written text? Learning similar representations for semantically similar speech and text is important for speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms the previous methods, and achieves an average BLEU of 29.4. The analysis further verifies that ConST indeed closes the representation gap of different modalities — its learned representation improves the accuracy of cross-modal speech-text retrieval from 4% to 88%. Code and models are available at https://github.com/ReneeYe/ConST.</abstract>
      <url hash="049e5219">2022.naacl-main.376</url>
      <bibkey>ye-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.naacl-main.376</doi>
      <video href="2022.naacl-main.376.mp4"/>
      <pwccode url="https://github.com/reneeye/const" additional="false">reneeye/const</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="377">
      <title>Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances</title>
      <author><first>Seungju</first><last>Han</last></author>
      <author><first>Beomsu</first><last>Kim</last></author>
      <author><first>Jin Yong</first><last>Yoo</last></author>
      <author><first>Seokjun</first><last>Seo</last></author>
      <author><first>Sangbum</first><last>Kim</last></author>
      <author><first>Enkhbayar</first><last>Erdenee</last></author>
      <author><first>Buru</first><last>Chang</last></author>
      <pages>5114-5132</pages>
      <abstract>In this paper, we consider mimicking fictional characters as a promising direction for building engaging conversation models. To this end, we present a new practical task where only a few utterances of each fictional character are available to generate responses mimicking them. Furthermore, we propose a new method named Pseudo Dialog Prompting (PDP) that generates responses by leveraging the power of large-scale language models with prompts containing the target character’s utterances. To better reflect the style of the character, PDP builds the prompts in the form of dialog that includes the character’s utterances as dialog history. Since only utterances of the characters are available in the proposed task, PDP matches each utterance with an appropriate pseudo-context from a predefined set of context candidates using a retrieval model. Through human and automatic evaluation, we show that PDP generates responses that better reflect the style of fictional characters than baseline methods.</abstract>
      <url hash="5b8df361">2022.naacl-main.377</url>
      <bibkey>han-etal-2022-meet</bibkey>
      <doi>10.18653/v1/2022.naacl-main.377</doi>
      <video href="2022.naacl-main.377.mp4"/>
      <pwccode url="https://github.com/hyperconnect/pseudo-dialog-prompting" additional="false">hyperconnect/pseudo-dialog-prompting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hla-chat">HLA-Chat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
    </paper>
    <paper id="378">
      <title><fixed-case>D</fixed-case>ynamic<fixed-case>TOC</fixed-case>: Persona-based Table of Contents for Consumption of Long Documents</title>
      <author><first>Himanshu</first><last>Maheshwari</last></author>
      <author><first>Nethraa</first><last>Sivakumar</last></author>
      <author><first>Shelly</first><last>Jain</last></author>
      <author><first>Tanvi</first><last>Karandikar</last></author>
      <author><first>Vinay</first><last>Aggarwal</last></author>
      <author><first>Navita</first><last>Goyal</last></author>
      <author><first>Sumit</first><last>Shekhar</last></author>
      <pages>5133-5143</pages>
      <abstract>Long documents like contracts, financial documents, etc., are often tedious to read through. Linearly consuming (via scrolling or navigation through default table of content) these documents is time-consuming and challenging. These documents are also authored to be consumed by varied entities (referred to as persona in the paper) interested in only certain parts of the document. In this work, we describe DynamicToC, a dynamic table of content-based navigator, to aid in the task of non-linear, persona-based document consumption. DynamicToC highlights sections of interest in the document as per the aspects relevant to different personas. DynamicToC is augmented with short questions to assist the users in understanding underlying content. This uses a novel deep-reinforcement learning technique to generate questions on these persona-clustered paragraphs. Human and automatic evaluations suggest the efficacy of both end-to-end pipeline and different components of DynamicToC.</abstract>
      <url hash="e6b34452">2022.naacl-main.378</url>
      <bibkey>maheshwari-etal-2022-dynamictoc</bibkey>
      <doi>10.18653/v1/2022.naacl-main.378</doi>
      <video href="2022.naacl-main.378.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
    </paper>
    <paper id="379">
      <title><fixed-case>KALA:</fixed-case> Knowledge-Augmented Language Model Adaptation</title>
      <author><first>Minki</first><last>Kang</last></author>
      <author><first>Jinheon</first><last>Baek</last></author>
      <author><first>Sung Ju</first><last>Hwang</last></author>
      <pages>5144-5167</pages>
      <abstract>Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training.</abstract>
      <url hash="83c8490b">2022.naacl-main.379</url>
      <bibkey>kang-etal-2022-kala</bibkey>
      <doi>10.18653/v1/2022.naacl-main.379</doi>
      <video href="2022.naacl-main.379.mp4"/>
      <pwccode url="https://github.com/nardien/kala" additional="false">nardien/kala</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="380">
      <title>On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model</title>
      <author><first>Seongjin</first><last>Shin</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Hwijeen</first><last>Ahn</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>HyoungSeok</first><last>Kim</last></author>
      <author><first>Boseop</first><last>Kim</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Gichang</first><last>Lee</last></author>
      <author><first>Woomyoung</first><last>Park</last></author>
      <author><first>Jung-Woo</first><last>Ha</last></author>
      <author><first>Nako</first><last>Sung</last></author>
      <pages>5168-5186</pages>
      <abstract>Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.</abstract>
      <url hash="9f32bc9a">2022.naacl-main.380</url>
      <bibkey>shin-etal-2022-effect</bibkey>
      <doi>10.18653/v1/2022.naacl-main.380</doi>
      <video href="2022.naacl-main.380.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/korquad">KorQuAD</pwcdataset>
    </paper>
    <paper id="381">
      <title>Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences</title>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Yun</first><last>Yang</last></author>
      <pages>5187-5199</pages>
      <abstract>Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection, respectively. These two models are intrinsically connected, and to understand their connection we introduce a theoretical framework of matrix sketching. Based on the theoretical analysis, we propose Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-attention with column sampling, adaptive row normalization and pilot sampling reutilization. Experiments on the Long Range Arena benchmark demonstrate that our methods outperform alternatives with a consistently smaller time/space footprint.</abstract>
      <url hash="84227ce0">2022.naacl-main.381</url>
      <attachment type="software" hash="09e435ab">2022.naacl-main.381.software.zip</attachment>
      <bibkey>chen-etal-2022-sketching</bibkey>
      <doi>10.18653/v1/2022.naacl-main.381</doi>
      <video href="2022.naacl-main.381.mp4"/>
      <pwccode url="https://github.com/pkuzengqi/skeinformer" additional="false">pkuzengqi/skeinformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lra">LRA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/listops">ListOps</pwcdataset>
    </paper>
    <paper id="382">
      <title>Partner Personas Generation for Dialogue Response Generation</title>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Hong</first><last>Cheng</last></author>
      <author><first>Helen</first><last>Meng</last></author>
      <pages>5200-5212</pages>
      <abstract>Incorporating personas information allows diverse and engaging responses in dialogue response generation. Unfortunately, prior works have primarily focused on self personas and have overlooked the value of partner personas. Moreover, in practical applications, the availability of the gold partner personas is often not the case. This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation. Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement. Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant, interesting, coherent and informative partner personas, even compared to the ground truth partner personas. This enhances the succeeding dialogue response generation, which surpasses our competitive baselines that condition on the ground truth partner personas.</abstract>
      <url hash="9c133761">2022.naacl-main.382</url>
      <attachment type="software" hash="b87fdd7a">2022.naacl-main.382.software.zip</attachment>
      <bibkey>lu-etal-2022-partner</bibkey>
      <doi>10.18653/v1/2022.naacl-main.382</doi>
      <video href="2022.naacl-main.382.mp4"/>
    </paper>
    <paper id="383">
      <title>Semantically Informed Slang Interpretation</title>
      <author><first>Zhewei</first><last>Sun</last></author>
      <author><first>Richard</first><last>Zemel</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>5213-5231</pages>
      <abstract>Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret. Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang. We perform rigorous evaluation on two large-scale online slang dictionaries and show that our approach not only achieves state-of-the-art accuracy for slang interpretation in English, but also does so in zero-shot and few-shot scenarios where training data is sparse. Furthermore, we show how the same framework can be applied to enhancing machine translation of slang from English to other languages. Our work creates opportunities for the automated interpretation and translation of informal language.</abstract>
      <url hash="b0343783">2022.naacl-main.383</url>
      <bibkey>sun-etal-2022-semantically</bibkey>
      <doi>10.18653/v1/2022.naacl-main.383</doi>
      <video href="2022.naacl-main.383.mp4"/>
      <pwccode url="https://github.com/zhewei-sun/slanginterp" additional="false">zhewei-sun/slanginterp</pwccode>
    </paper>
    <paper id="384">
      <title>Dual-Channel Evidence Fusion for Fact Verification over Texts and Tables</title>
      <author><first>Nan</first><last>Hu</last></author>
      <author><first>Zirui</first><last>Wu</last></author>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <pages>5232-5242</pages>
      <abstract>Different from previous fact extraction and verification tasks that only consider evidence of a single format, FEVEROUS brings further challenges by extending the evidence format to both plain text and tables. Existing works convert all candidate evidence into either sentences or tables, thus often failing to fully capture the rich context in their original format from the converted evidence, let alone the context information lost during conversion. In this paper, we propose a Dual Channel Unified Format fact verification model (DCUF), which unifies various evidence into parallel streams, i.e., natural language sentences and a global evidence table, simultaneously. With carefully-designed evidence conversion and organization methods, DCUF makes the most of pre-trained table/language models to encourage each evidence piece to perform early and thorough interactions with other pieces in its original format. Experiments show that our model can make better use of existing pre-trained models to absorb evidence of two formats, thus outperforming previous works by a large margin. Our code and models are publicly available.</abstract>
      <url hash="84f86456">2022.naacl-main.384</url>
      <bibkey>hu-etal-2022-dual</bibkey>
      <doi>10.18653/v1/2022.naacl-main.384</doi>
      <video href="2022.naacl-main.384.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/feverous">FEVEROUS</pwcdataset>
    </paper>
    <paper id="385">
      <title><fixed-case>T</fixed-case>ree<fixed-case>M</fixed-case>ix: Compositional Constituency-based Data Augmentation for Natural Language Understanding</title>
      <author><first>Le</first><last>Zhang</last></author>
      <author><first>Zichao</first><last>Yang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>5243-5258</pages>
      <abstract>Data augmentation is an effective approach to tackle over-fitting. Many previous works have proposed different data augmentations strategies for NLP, such as noise injection, word replacement, back-translation etc. Though effective, they missed one important characteristic of language–compositionality, meaning of a complex expression is built from its sub-parts. Motivated by this, we propose a compositional data augmentation approach for natural language understanding called TreeMix. Specifically, TreeMix leverages constituency parsing tree to decompose sentences into constituent sub-structures and the Mixup data augmentation technique to recombine them to generate new sentences. Compared with previous approaches, TreeMix introduces greater diversity to the samples generated and encourages models to learn compositionality of NLP data. Extensive experiments on text classification and SCAN demonstrate that TreeMix outperforms current state-of-the-art data augmentation methods.</abstract>
      <url hash="0fd3668c">2022.naacl-main.385</url>
      <bibkey>zhang-etal-2022-treemix</bibkey>
      <doi>10.18653/v1/2022.naacl-main.385</doi>
      <video href="2022.naacl-main.385.mp4"/>
      <pwccode url="https://github.com/magiccircuit/treemix" additional="false">magiccircuit/treemix</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="386">
      <title><fixed-case>S</fixed-case>yn2<fixed-case>V</fixed-case>ec: Synset Colexification Graphs for Lexical Semantic Similarity</title>
      <author><first>John</first><last>Harvill</last></author>
      <author><first>Roxana</first><last>Girju</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <pages>5259-5270</pages>
      <abstract>In this paper we focus on patterns of colexification (co-expressions of form-meaning mapping in the lexicon) as an aspect of lexical-semantic organization, and use them to build large scale synset graphs across BabelNet’s typologically diverse set of 499 world languages. We introduce and compare several approaches: monolingual and cross-lingual colexification graphs, popular distributional models, and fusion approaches. The models are evaluated against human judgments on a semantic similarity task for nine languages. Our strong empirical findings also point to the importance of universality of our graph synset embedding representations with no need for any language-specific adaptation when evaluated on the lexical similarity task. The insights of our exploratory investigation of large-scale colexification graphs could inspire significant advances in NLP across languages, especially for tasks involving languages which lack dedicated lexical resources, and can benefit from language transfer from large shared cross-lingual semantic spaces.</abstract>
      <url hash="4a87e363">2022.naacl-main.386</url>
      <attachment type="software" hash="0dba9096">2022.naacl-main.386.software.zip</attachment>
      <bibkey>harvill-etal-2022-syn2vec</bibkey>
      <doi>10.18653/v1/2022.naacl-main.386</doi>
      <video href="2022.naacl-main.386.mp4"/>
      <pwccode url="https://github.com/jharvill23/syn2vec" additional="false">jharvill23/syn2vec</pwccode>
    </paper>
    <paper id="387">
      <title>On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Sivan</first><last>Milton</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Osmar</first><last>Zaiane</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>5271-5285</pages>
      <abstract>Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of &gt; 60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.</abstract>
      <url hash="f721e6eb">2022.naacl-main.387</url>
      <bibkey>dziri-etal-2022-origin</bibkey>
      <doi>10.18653/v1/2022.naacl-main.387</doi>
      <video href="2022.naacl-main.387.mp4"/>
      <pwccode url="https://github.com/mcgill-nlp/faithdial" additional="false">mcgill-nlp/faithdial</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="388">
      <title>Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases</title>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Zheng</first><last>Hua</last></author>
      <author><first>Daoxin</first><last>Li</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>5286-5302</pages>
      <abstract>Recursive noun phrases (NPs) have interesting semantic properties. For example, “my favorite new movie” is not necessarily my favorite movie, whereas “my new favorite movie” is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.</abstract>
      <url hash="3ceb859c">2022.naacl-main.388</url>
      <bibkey>lyu-etal-2022-favorite</bibkey>
      <doi>10.18653/v1/2022.naacl-main.388</doi>
      <video href="2022.naacl-main.388.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="389">
      <title>Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance</title>
      <author><first>Jingwei</first><last>Ni</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Bernhard</first><last>Schölkopf</last></author>
      <pages>5303-5320</pages>
      <abstract>Human-translated text displays distinct features from naturally written text in the same language. This phenomena, known as translationese, has been argued to confound the machine translation (MT) evaluation. Yet, we find that existing work on translationese neglects some important factors and the conclusions are mostly correlational but not causal. In this work, we collect CausalMT, a dataset where the MT training data are also labeled with the human translation directions. We inspect two critical factors, the train-test direction match (whether the human translation directions in the training and test sets are aligned), and data-model direction match (whether the model learns in the same direction as the human translation direction in the dataset). We show that these two factors have a large causal effect on the MT performance, in addition to the test-model direction mismatch highlighted by existing work on the impact of translationese. In light of our findings, we provide a set of suggestions for MT training and evaluation. Our code and data are at https://github.com/EdisonNi-hku/CausalMT</abstract>
      <url hash="dab747b6">2022.naacl-main.389</url>
      <bibkey>ni-etal-2022-original</bibkey>
      <doi>10.18653/v1/2022.naacl-main.389</doi>
      <video href="2022.naacl-main.389.mp4"/>
      <pwccode url="https://github.com/edisonni-hku/causalmt" additional="false">edisonni-hku/causalmt</pwccode>
    </paper>
    <paper id="390">
      <title>Visual Commonsense in Pretrained Unimodal and Multimodal Models</title>
      <author><first>Chenyu</first><last>Zhang</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Zhuowan</first><last>Li</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <pages>5321-5335</pages>
      <abstract>Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this world-knowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we create the Visual Commonsense Tests (ViComTe) dataset covering 5 property types (color, shape, material, size, and visual co-occurrence) for over 5000 subjects. We validate this dataset by showing that our grounded color data correlates much better than ungrounded text-only data with crowdsourced color judgments provided by Paik et al. (2021). We then use our dataset to evaluate pretrained unimodal models and multimodal models. Our results indicate that multimodal models better reconstruct attribute distributions, but are still subject to reporting bias. Moreover, increasing model size does not enhance performance, suggesting that the key to visual commonsense lies in the data.</abstract>
      <url hash="814734ac">2022.naacl-main.390</url>
      <bibkey>zhang-etal-2022-visual</bibkey>
      <doi>10.18653/v1/2022.naacl-main.390</doi>
      <video href="2022.naacl-main.390.mp4"/>
      <pwccode url="https://github.com/chenyuheidizhang/vl-commonsense" additional="false">chenyuheidizhang/vl-commonsense</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coda">CoDa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="391">
      <title><fixed-case>Q</fixed-case>u<fixed-case>ALITY</fixed-case>: Question Answering with Long Input Texts, Yes!</title>
      <author><first>Richard Yuanzhe</first><last>Pang</last></author>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Nitish</first><last>Joshi</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Angelica</first><last>Chen</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>Johnny</first><last>Ma</last></author>
      <author><first>Jana</first><last>Thompson</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Samuel</first><last>Bowman</last></author>
      <pages>5336-5358</pages>
      <abstract>To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Our baseline models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).</abstract>
      <url hash="12ffbf57">2022.naacl-main.391</url>
      <bibkey>pang-etal-2022-quality</bibkey>
      <doi>10.18653/v1/2022.naacl-main.391</doi>
      <video href="2022.naacl-main.391.mp4"/>
      <pwccode url="https://github.com/nyu-mll/quality" additional="true">nyu-mll/quality</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/quality">QuALITY</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="392">
      <title><fixed-case>ExSum</fixed-case>: <fixed-case>F</fixed-case>rom Local Explanations to Model Understanding</title>
      <author><first>Yilun</first><last>Zhou</last></author>
      <author><first>Marco Tulio</first><last>Ribeiro</last></author>
      <author><first>Julie</first><last>Shah</last></author>
      <pages>5359-5378</pages>
      <abstract>Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce explanation summary (ExSum), a mathematical framework for quantifying model understanding, and propose metrics for its quality assessment. On two domains, ExSum highlights various limitations in the current practice, helps develop accurate model understanding, and reveals easily overlooked properties of the model. We also connect understandability to other properties of explanations such as human alignment, robustness, and counterfactual similarity and plausibility.</abstract>
      <url hash="3c4b1cc8">2022.naacl-main.392</url>
      <bibkey>zhou-etal-2022-exsum</bibkey>
      <doi>10.18653/v1/2022.naacl-main.392</doi>
      <video href="2022.naacl-main.392.mp4"/>
      <pwccode url="https://github.com/YilunZhou/ExSum" additional="false">YilunZhou/ExSum</pwccode>
    </paper>
    <paper id="393">
      <title>Maximum <fixed-case>B</fixed-case>ayes <fixed-case>S</fixed-case>match Ensemble Distillation for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Ramón</first><last>Astudillo</last></author>
      <author><first>Hoang</first><last>Thanh Lam</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <pages>5379-5392</pages>
      <abstract>AMR parsing has experienced an unprecendented increase in performance in the last three years, due to a mixture of effects including architecture improvements and transfer learning. Self-learning techniques have also played a role in pushing performance forward. However, for most recent high performant parsers, the effect of self-learning and silver data augmentation seems to be fading. In this paper we propose to overcome this diminishing returns of silver data by combining Smatch-based ensembling techniques with ensemble distillation. In an extensive experimental setup, we push single model English parser performance to a new state-of-the-art, 85.9 (AMR2.0) and 84.3 (AMR3.0), and return to substantial gains from silver data augmentation. We also attain a new state-of-the-art for cross-lingual AMR parsing for Chinese, German, Italian and Spanish. Finally we explore the impact of the proposed technique on domain adaptation, and show that it can produce gains rivaling those of human annotated data for QALD-9 and achieve a new state-of-the-art for BioAMR.</abstract>
      <url hash="861a2b39">2022.naacl-main.393</url>
      <bibkey>lee-etal-2022-maximum</bibkey>
      <doi>10.18653/v1/2022.naacl-main.393</doi>
      <video href="2022.naacl-main.393.mp4"/>
      <pwccode url="https://github.com/IBM/transition-amr-parser" additional="true">IBM/transition-amr-parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2020t02">LDC2020T02</pwcdataset>
    </paper>
    <paper id="394">
      <title>When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes</title>
      <author><first>Mycal</first><last>Tucker</last></author>
      <author><first>Tiwalayo</first><last>Eisape</last></author>
      <author><first>Peng</first><last>Qian</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Julie</first><last>Shah</last></author>
      <pages>5393-5408</pages>
      <abstract>Recent causal probing literature reveals when language models and syntactic probes use similar representations. Such techniques may yield “false negative” causality results: models may use representations of syntax, but probes may have learned to use redundant encodings of the same syntactic information. We demonstrate that models do encode syntactic information redundantly and introduce a new probe design that guides probes to consider all syntactic information present in embeddings. Using these probes, we find evidence for the use of syntax in models where prior methods did not, allowing us to boost model performance by injecting syntactic information into representations.</abstract>
      <url hash="48ac5b31">2022.naacl-main.394</url>
      <bibkey>tucker-etal-2022-syntax</bibkey>
      <doi>10.18653/v1/2022.naacl-main.394</doi>
      <video href="2022.naacl-main.394.mp4"/>
      <pwccode url="https://github.com/mycal-tucker/mlm_dropout_probes" additional="false">mycal-tucker/mlm_dropout_probes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="395">
      <title>Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction</title>
      <author><first>Liyan</first><last>Xu</last></author>
      <author><first>Jinho</first><last>Choi</last></author>
      <pages>5409-5416</pages>
      <abstract>We target on the document-level relation extraction in an end-to-end setting, where the model needs to jointly perform mention extraction, coreference resolution (COREF) and relation extraction (RE) at once, and gets evaluated in an entity-centric way. Especially, we address the two-way interaction between COREF and RE that has not been the focus by previous work, and propose to introduce explicit interaction namely Graph Compatibility (GC) that is specifically designed to leverage task characteristics, bridging decisions of two tasks for direct task interference. Our experiments are conducted on DocRED and DWIE; in addition to GC, we implement and compare different multi-task settings commonly adopted in previous work, including pipeline, shared encoders, graph propagation, to examine the effectiveness of different interactions. The result shows that GC achieves the best performance by up to 2.3/5.1 F1 improvement over the baseline.</abstract>
      <url hash="2879c972">2022.naacl-main.395</url>
      <attachment type="software" hash="24040b16">2022.naacl-main.395.software.zip</attachment>
      <bibkey>xu-choi-2022-modeling</bibkey>
      <doi>10.18653/v1/2022.naacl-main.395</doi>
      <video href="2022.naacl-main.395.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dwie">DWIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="396">
      <title>Few-Shot Semantic Parsing with Language Models Trained on Code</title>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>5417-5425</pages>
      <abstract>Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.</abstract>
      <url hash="9a62e3e1">2022.naacl-main.396</url>
      <bibkey>shin-van-durme-2022-shot</bibkey>
      <doi>10.18653/v1/2022.naacl-main.396</doi>
      <video href="2022.naacl-main.396.mp4"/>
    </paper>
    <paper id="397">
      <title><fixed-case>CORWA</fixed-case>: A Citation-Oriented Related Work Annotation Dataset</title>
      <author><first>Xiangci</first><last>Li</last></author>
      <author><first>Biswadip</first><last>Mandal</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <pages>5426-5440</pages>
      <abstract>Academic research is an exploratory activity to discover new solutions to problems. By this nature, academic research works perform literature reviews to distinguish their novelties from prior work. In natural language processing, this literature review is usually conducted under the “Related Work” section. The task of related work generation aims to automatically generate the related work section given the rest of the research paper and a list of papers to cite. Prior work on this task has focused on the sentence as the basic unit of generation, neglecting the fact that related work sections consist of variable length text fragments derived from different information sources. As a first step toward a linguistically-motivated related work generation framework, we present a Citation Oriented Related Work Annotation (CORWA) dataset that labels different types of citation text fragments from different information sources. We train a strong baseline model that automatically tags the CORWA labels on massive unlabeled related work section texts. We further suggest a novel framework for human-in-the-loop, iterative, abstractive related work generation.</abstract>
      <url hash="79569750">2022.naacl-main.397</url>
      <attachment type="software" hash="5b7ba07a">2022.naacl-main.397.software.zip</attachment>
      <bibkey>li-etal-2022-corwa</bibkey>
      <doi>10.18653/v1/2022.naacl-main.397</doi>
      <video href="2022.naacl-main.397.mp4"/>
      <pwccode url="https://github.com/jacklxc/corwa" additional="false">jacklxc/corwa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="398">
      <title>Overcoming Catastrophic Forgetting During Domain Adaptation of Seq2seq Language Generation</title>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Zheng</first><last>Chen</last></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Xiaohu</first><last>Liu</last></author>
      <author><first>Fan</first><last>Xing</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <pages>5441-5454</pages>
      <abstract>Seq2seq language generation models that are trained offline with multiple domains in a sequential fashion often suffer from catastrophic forgetting. Lifelong learning has been proposed to handle this problem. However, existing work such as experience replay or elastic weighted consolidation requires incremental memory space. In this work, we propose an innovative framework, RMR_DSEthat leverages a recall optimization mechanism to selectively memorize important parameters of previous tasks via regularization, and uses a domain drift estimation algorithm to compensate the drift between different do-mains in the embedding space. These designs enable the model to be trained on the current task while keep-ing the memory of previous tasks, and avoid much additional data storage. Furthermore, RMR_DSE can be combined with existing lifelong learning approaches. Our experiments on two seq2seq language generation tasks, paraphrase and dialog response generation, show thatRMR_DSE outperforms SOTA models by a considerable margin and reduces forgetting greatly.</abstract>
      <url hash="e5409b1e">2022.naacl-main.398</url>
      <bibkey>li-etal-2022-overcoming</bibkey>
      <doi>10.18653/v1/2022.naacl-main.398</doi>
      <video href="2022.naacl-main.398.mp4"/>
    </paper>
    <paper id="399">
      <title>Extreme <fixed-case>Z</fixed-case>ero-<fixed-case>S</fixed-case>hot Learning for Extreme Text Classification</title>
      <author><first>Yuanhao</first><last>Xiong</last></author>
      <author><first>Wei-Cheng</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Hsiang-Fu</first><last>Yu</last></author>
      <author><first>Inderjit</first><last>Dhillon</last></author>
      <pages>5455-5468</pages>
      <abstract>The eXtreme Multi-label text Classification (XMC) problem concerns finding most relevant labels for an input text instance from a large label set. However, the XMC setup faces two challenges: (1) it is not generalizable to predict unseen labels in dynamic environments, and (2) it requires a large amount of supervised (instance, label) pairs, which can be difficult to obtain for emerging domains. In this paper, we consider a more practical scenario called Extreme Zero-Shot XMC (EZ-XMC), in which no supervision is needed and merely raw text of instances and labels are accessible. Few-Shot XMC (FS-XMC), an extension to EZ-XMC with limited supervision is also investigated. To learn the semantic embeddings of instances and labels with raw text, we propose to pre-train Transformer-based encoders with self-supervised contrastive losses. Specifically, we develop a pre-training method <tex-math>\textbf{MACLR}</tex-math>, which thoroughly leverages the raw text with techniques including <tex-math>\textbf{M}</tex-math>ulti-scale <tex-math>\textbf{A}</tex-math>daptive <tex-math>\textbf{C}</tex-math>lustering, <tex-math>\textbf{L}</tex-math>abel <tex-math>\textbf{R}</tex-math>egularization, and self-training with pseudo positive pairs. Experimental results on four public EZ-XMC datasets demonstrate that MACLR achieves superior performance compared to all other leading baseline methods, in particular with approximately 5-10% improvement in precision and recall on average. Moreover, we show that our pre-trained encoder can be further improved on FS-XMC when there are a limited number of ground-truth positive pairs in training. Our code is available at https://github.com/amzn/pecos/tree/mainline/examples/MACLR.</abstract>
      <url hash="47987fa1">2022.naacl-main.399</url>
      <bibkey>xiong-etal-2022-extreme</bibkey>
      <doi>10.18653/v1/2022.naacl-main.399</doi>
      <video href="2022.naacl-main.399.mp4"/>
      <pwccode url="https://github.com/amzn/pecos" additional="false">amzn/pecos</pwccode>
    </paper>
    <paper id="400">
      <title><fixed-case>C</fixed-case>onfli<fixed-case>BERT</fixed-case>: A Pre-trained Language Model for Political Conflict and Violence</title>
      <author><first>Yibo</first><last>Hu</last></author>
      <author><first>MohammadSaleh</first><last>Hosseini</last></author>
      <author><first>Erick</first><last>Skorupa Parolin</last></author>
      <author><first>Javier</first><last>Osorio</last></author>
      <author><first>Latifur</first><last>Khan</last></author>
      <author><first>Patrick</first><last>Brandt</last></author>
      <author><first>Vito</first><last>D’Orazio</last></author>
      <pages>5469-5482</pages>
      <abstract>Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build ConfliBERT using two approaches: pre-training from scratch and continual pre-training. To evaluate ConfliBERT, we collect 12 datasets and implement 18 tasks to assess the models’ practical application in conflict research. Finally, we evaluate several versions of ConfliBERT in multiple experiments. Results consistently show that ConfliBERT outperforms BERT when analyzing political violence and conflict.</abstract>
      <url hash="7160e7d7">2022.naacl-main.400</url>
      <bibkey>hu-etal-2022-conflibert</bibkey>
      <doi>10.18653/v1/2022.naacl-main.400</doi>
      <video href="2022.naacl-main.400.mp4"/>
      <pwccode url="https://github.com/eventdata/conflibert" additional="false">eventdata/conflibert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="401">
      <title>Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification</title>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>5483-5492</pages>
      <abstract>Prompt-based learning (i.e., prompting) is an emerging paradigm for exploiting knowledge learned by a pretrained language model. In this paper, we propose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method to automatically select label mappings for few-shot text classification with prompting. Our method exploits one-to-many label mappings and a statistics-based algorithm to select label mappings given a prompt template. Our experiments demonstrate that AMuLaP achieves competitive performance on the GLUE benchmark without human effort or external resources.</abstract>
      <url hash="b3248ed2">2022.naacl-main.401</url>
      <bibkey>wang-etal-2022-automatic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.401</doi>
      <video href="2022.naacl-main.401.mp4"/>
      <pwccode url="https://github.com/hannight/amulap" additional="false">hannight/amulap</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="402">
      <title>Few-shot Subgoal Planning with Language Models</title>
      <author><first>Lajanugen</first><last>Logeswaran</last></author>
      <author><first>Yao</first><last>Fu</last></author>
      <author><first>Moontae</first><last>Lee</last></author>
      <author><first>Honglak</first><last>Lee</last></author>
      <pages>5493-5506</pages>
      <abstract>Pre-trained language models have shown successful progress in many text understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.</abstract>
      <url hash="ba833f7e">2022.naacl-main.402</url>
      <bibkey>logeswaran-etal-2022-shot</bibkey>
      <doi>10.18653/v1/2022.naacl-main.402</doi>
      <video href="2022.naacl-main.402.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ai2-thor">AI2-THOR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
    </paper>
    <paper id="403">
      <title><fixed-case>IDPG</fixed-case>: An Instance-Dependent Prompt Generation Method</title>
      <author><first>Zhuofeng</first><last>Wu</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Rui</first><last>Hou</last></author>
      <author><first>Yuxiao</first><last>Dong</last></author>
      <author><first>V.G.Vinod</first><last>Vydiswaran</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <pages>5507-5521</pages>
      <abstract>Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.</abstract>
      <url hash="bf93ed4c">2022.naacl-main.403</url>
      <bibkey>wu-etal-2022-idpg</bibkey>
      <doi>10.18653/v1/2022.naacl-main.403</doi>
      <video href="2022.naacl-main.403.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="404">
      <title>Embedding Hallucination for Few-shot Language Fine-tuning</title>
      <author><first>Yiren</first><last>Jian</last></author>
      <author><first>Chongyang</first><last>Gao</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>5522-5530</pages>
      <abstract>Few-shot language learners adapt knowledge from a pre-trained model to recognize novel classes from a few-labeled sentences. In such settings, fine-tuning a pre-trained language model can cause severe over-fitting. In this paper, we propose an Embedding Hallucination (EmbedHalluc) method, which generates auxiliary embedding-label pairs to expand the fine-tuning dataset. The hallucinator is trained by playing an adversarial game with the discriminator, such that the hallucinated embedding is indiscriminative to the real ones in the fine-tuning dataset. By training with the extended dataset, the language learner effectively learns from the diverse hallucinated embeddings to overcome the over-fitting issue. Experiments demonstrate that our proposed method is effective in a wide range of language tasks, outperforming current fine-tuning methods. Further, we show that EmbedHalluc outperforms other methods that address this over-fitting problem, such as common data augmentation, semi-supervised pseudo-labeling, and regularization.</abstract>
      <url hash="d80957b7">2022.naacl-main.404</url>
      <bibkey>jian-etal-2022-embedding</bibkey>
      <doi>10.18653/v1/2022.naacl-main.404</doi>
      <video href="2022.naacl-main.404.mp4"/>
      <pwccode url="https://github.com/yiren-jian/embedhalluc" additional="false">yiren-jian/embedhalluc</pwccode>
    </paper>
    <paper id="405">
      <title>Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial Task &amp; Hyperbolic Models</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Vivek</first><last>Mittal</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <author><first>Vikram</first><last>Nanda</last></author>
      <author><first>Sudheer</first><last>Chava</last></author>
      <pages>5531-5545</pages>
      <abstract>The rapid spread of information over social media influences quantitative trading and investments. The growing popularity of speculative trading of highly volatile assets such as cryptocurrencies and meme stocks presents a fresh challenge in the financial realm. Investigating such “bubbles” - periods of sudden anomalous behavior of markets are critical in better understanding investor behavior and market dynamics. However, high volatility coupled with massive volumes of chaotic social media texts, especially for underexplored assets like cryptocoins pose a challenge to existing methods. Taking the first step towards NLP for cryptocoins, we present and publicly release CryptoBubbles, a novel multi- span identification task for bubble detection, and a dataset of more than 400 cryptocoins from 9 exchanges over five years spanning over two million tweets. Further, we develop a set of sequence-to-sequence hyperbolic models suited to this multi-span identification task based on the power-law dynamics of cryptocurrencies and user behavior on social media. We further test the effectiveness of our models under zero-shot settings on a test set of Reddit posts pertaining to 29 “meme stocks”, which see an increase in trade volume due to social media hype. Through quantitative, qualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins and meme-stocks, we show the practical applicability of CryptoBubbles and hyperbolic models.</abstract>
      <url hash="bb011196">2022.naacl-main.405</url>
      <attachment type="software" hash="5a99e4e1">2022.naacl-main.405.software.zip</attachment>
      <bibkey>sawhney-etal-2022-cryptocurrency</bibkey>
      <doi>10.18653/v1/2022.naacl-main.405</doi>
      <video href="2022.naacl-main.405.mp4"/>
      <pwccode url="https://github.com/gtfintechlab/cryptobubbles-naacl" additional="false">gtfintechlab/cryptobubbles-naacl</pwccode>
    </paper>
    <paper id="406">
      <title>Nearest Neighbor Knowledge Distillation for Neural Machine Translation</title>
      <author><first>Zhixian</first><last>Yang</last></author>
      <author><first>Renliang</first><last>Sun</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>5546-5556</pages>
      <abstract>k-nearest-neighbor machine translation (<tex-math>k</tex-math>NN-MT), proposed by Khandelwal et al. (2021), has achieved many state-of-the-art results in machine translation tasks. Although effective, <tex-math>k</tex-math>NN-MT requires conducting <tex-math>k</tex-math>NN searches through the large datastore for each decoding step during inference, prohibitively increasing the decoding cost and thus leading to the difficulty for the deployment in real-world applications. In this paper, we propose to move the time-consuming <tex-math>k</tex-math>NN search forward to the preprocessing phase, and then introduce <tex-math>k</tex-math> Nearest Neighbor Knowledge Distillation (<tex-math>k</tex-math>NN-KD) that trains the base NMT model to directly learn the knowledge of <tex-math>k</tex-math>NN. Distilling knowledge retrieved by <tex-math>k</tex-math>NN can encourage the NMT model to take more reasonable target tokens into consideration, thus addressing the overcorrection problem. Extensive experimental results show that, the proposed method achieves consistent improvement over the state-of-the-art baselines including <tex-math>k</tex-math>NN-MT, while maintaining the same training and decoding speed as the standard NMT model.</abstract>
      <url hash="0fc9bfad">2022.naacl-main.406</url>
      <bibkey>yang-etal-2022-nearest</bibkey>
      <doi>10.18653/v1/2022.naacl-main.406</doi>
      <video href="2022.naacl-main.406.mp4"/>
      <pwccode url="https://github.com/fadedcosine/knn-kd" additional="false">fadedcosine/knn-kd</pwccode>
    </paper>
    <paper id="407">
      <title><fixed-case>DEM</fixed-case>ix Layers: Disentangling Domains for Modular Language Modeling</title>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>5557-5576</pages>
      <abstract>We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer includes a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added, or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity (especially for out-of-domain data), increase training efficiency, and enable rapid adaptation. Mixing experts during inference, using a parameter-free weighted ensemble, enables better generalization to heterogeneous or unseen domains. We also show it is possible to add experts to adapt to new domains without forgetting older ones, and remove experts to restrict access to unwanted domains. Overall, these results demonstrate benefits of domain modularity in language models.</abstract>
      <url hash="2de807a8">2022.naacl-main.407</url>
      <bibkey>gururangan-etal-2022-demix</bibkey>
      <doi>10.18653/v1/2022.naacl-main.407</doi>
      <video href="2022.naacl-main.407.mp4"/>
      <pwccode url="https://github.com/kernelmachine/demix" additional="true">kernelmachine/demix</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="408">
      <title>Contrastive Learning for Prompt-based Few-shot Language Learners</title>
      <author><first>Yiren</first><last>Jian</last></author>
      <author><first>Chongyang</first><last>Gao</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>5577-5587</pages>
      <abstract>The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented “views” and repel the ones from different classes. We create different “views” of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.</abstract>
      <url hash="0c9299cd">2022.naacl-main.408</url>
      <bibkey>jian-etal-2022-contrastive</bibkey>
      <doi>10.18653/v1/2022.naacl-main.408</doi>
      <video href="2022.naacl-main.408.mp4"/>
      <pwccode url="https://github.com/yiren-jian/lm-supcon" additional="false">yiren-jian/lm-supcon</pwccode>
    </paper>
    <paper id="409">
      <title>Cross-Lingual Event Detection via Optimized Adversarial Training</title>
      <author><first>Luis</first><last>Guzman-Nateras</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>5588-5599</pages>
      <abstract>In this work, we focus on Cross-Lingual Event Detection where a model is trained on data from a <tex-math>\textit{source}</tex-math> language but its performance is evaluated on data from a second, <tex-math>\textit{target}</tex-math>, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models. Their performance, however, reveals there is room for improvement as the cross-lingual setting entails particular challenges. We employ Adversarial Language Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data. The discriminator is trained in an adversarial manner so that the encoder learns to produce refined, language-invariant representations that lead to improved performance. More importantly, we optimize the adversarial training process by only presenting the discriminator with the most informative samples. We base our intuition about what makes a sample informative on two disparate metrics: sample similarity and event presence. Thus, we propose leveraging Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process. Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves state-of-the-art results.</abstract>
      <url hash="0d9d4c94">2022.naacl-main.409</url>
      <bibkey>guzman-nateras-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.naacl-main.409</doi>
      <video href="2022.naacl-main.409.mp4"/>
    </paper>
    <paper id="410">
      <title>Identifying Implicitly Abusive Remarks about Identity Groups using a Linguistically Informed Approach</title>
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <pages>5600-5612</pages>
      <abstract>We address the task of distinguishing implicitly abusive sentences on identity groups (“Muslims contaminate our planet”) from other group-related negative polar sentences (“Muslims despise terrorism”). Implicitly abusive language are utterances not conveyed by abusive words (e.g. “bimbo” or “scum”). So far, the detection of such utterances could not be properly addressed since existing datasets displaying a high degree of implicit abuse are fairly biased. Following the recently-proposed strategy to solve implicit abuse by separately addressing its different subtypes, we present a new focused and less biased dataset that consists of the subtype of atomic negative sentences about identity groups. For that task, we model components that each address one facet of such implicit abuse, i.e. depiction as perpetrators, aspectual classification and non-conformist views. The approach generalizes across different identity groups and languages.</abstract>
      <url hash="f5453c0d">2022.naacl-main.410</url>
      <bibkey>wiegand-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.naacl-main.410</doi>
      <video href="2022.naacl-main.410.mp4"/>
      <pwccode url="https://github.com/miwieg/naacl2022_identity_groups" additional="false">miwieg/naacl2022_identity_groups</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="411">
      <title>Label Definitions Improve Semantic Role Labeling</title>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Ishan</first><last>Jindal</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <pages>5613-5620</pages>
      <abstract>Argument classification is at the core of Semantic Role Labeling. Given a sentence and the predicate, a semantic role label is assigned to each argument of the predicate. While semantic roles come with meaningful definitions, existing work has treated them as symbolic. Learning symbolic labels usually requires ample training data, which is frequently unavailable due to the cost of annotation. We instead propose to retrieve and leverage the definitions of these labels from the annotation guidelines. For example, the verb predicate “work” has arguments defined as “worker”, “job”, “employer”, etc. Our model achieves state-of-the-art performance on the CoNLL09 dataset injected with label definitions given the predicate senses. The performance improvement is even more pronounced in low-resource settings when training data is scarce.</abstract>
      <url hash="a001c3c0">2022.naacl-main.411</url>
      <bibkey>zhang-etal-2022-label-definitions</bibkey>
      <doi>10.18653/v1/2022.naacl-main.411</doi>
      <video href="2022.naacl-main.411.mp4"/>
      <pwccode url="https://github.com/system-t/labelawaresrl" additional="false">system-t/labelawaresrl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="412">
      <title>Shedding New Light on the Language of the Dark Web</title>
      <author><first>Youngjin</first><last>Jin</last></author>
      <author><first>Eugene</first><last>Jang</last></author>
      <author><first>Yongjae</first><last>Lee</last></author>
      <author><first>Seungwon</first><last>Shin</last></author>
      <author><first>Jin-Woo</first><last>Chung</last></author>
      <pages>5621-5637</pages>
      <abstract>The hidden nature and the limited accessibility of the Dark Web, combined with the lack of public datasets in this domain, make it difficult to study its inherent characteristics such as linguistic properties. Previous works on text classification of Dark Web domain have suggested that the use of deep neural models may be ineffective, potentially due to the linguistic differences between the Dark and Surface Webs. However, not much work has been done to uncover the linguistic characteristics of the Dark Web. This paper introduces CoDA, a publicly available Dark Web dataset consisting of 10000 web documents tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a thorough linguistic analysis of the Dark Web and examine the textual differences between the Dark Web and the Surface Web. We also assess the performance of various methods of Dark Web page classification. Finally, we compare CoDA with an existing public Dark Web dataset and evaluate their suitability for various use cases.</abstract>
      <url hash="2b8f6903">2022.naacl-main.412</url>
      <attachment type="software" hash="f4b2a47d">2022.naacl-main.412.software.zip</attachment>
      <bibkey>jin-etal-2022-shedding</bibkey>
      <doi>10.18653/v1/2022.naacl-main.412</doi>
      <video href="2022.naacl-main.412.mp4"/>
    </paper>
    <paper id="413">
      <title>Conceptualizing Treatment Leakage in Text-based Causal Inference</title>
      <author><first>Adel</first><last>Daoud</last></author>
      <author><first>Connor</first><last>Jerzak</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <pages>5638-5645</pages>
      <abstract>Causal inference methods that control for text-based confounders are becoming increasingly important in the social sciences and other disciplines where text is readily available. However, these methods rely on a critical assumption that there is no treatment leakage: that is, the text only contains information about the confounder and no information about treatment assignment. When this assumption does not hold, methods that control for text to adjust for confounders face the problem of post-treatment (collider) bias. However, the assumption that there is no treatment leakage may be unrealistic in real-world situations involving text, as human language is rich and flexible. Language appearing in a public policy document or health records may refer to the future and the past simultaneously, and thereby reveal information about the treatment assignment.In this article, we define the treatment-leakage problem, and discuss the identification as well as the estimation challenges it raises. Second, we delineate the conditions under which leakage can be addressed by removing the treatment-related signal from the text in a pre-processing step we define as text distillation. Lastly, using simulation, we show how treatment leakage introduces a bias in estimates of the average treatment effect (ATE) and how text distillation can mitigate this bias.</abstract>
      <url hash="a692ea63">2022.naacl-main.413</url>
      <bibkey>daoud-etal-2022-conceptualizing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.413</doi>
      <video href="2022.naacl-main.413.mp4"/>
    </paper>
    <paper id="414">
      <title>Consistency Training with Virtual Adversarial Discrete Perturbation</title>
      <author><first>Jungsoo</first><last>Park</last></author>
      <author><first>Gyuwan</first><last>Kim</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <pages>5646-5656</pages>
      <abstract>Consistency training regularizes a model by enforcing predictions of original and perturbed inputs to be similar. Previous studies have proposed various augmentation methods for the perturbation but are limited in that they are agnostic to the training model. Thus, the perturbed samples may not aid in regularization due to their ease of classification from the model. In this context, we propose an augmentation method of adding a discrete noise that would incur the highest divergence between predictions. This virtual adversarial discrete noise obtained by replacing a small portion of tokens while keeping original semantics as much as possible efficiently pushes a training model’s decision boundary. Experimental results show that our proposed method outperforms other consistency training baselines with text editing, paraphrasing, or a continuous noise on semi-supervised text classification tasks and a robustness benchmark.</abstract>
      <url hash="4fba7a5d">2022.naacl-main.414</url>
      <attachment type="software" hash="5d95d0b9">2022.naacl-main.414.software.zip</attachment>
      <bibkey>park-etal-2022-consistency</bibkey>
      <doi>10.18653/v1/2022.naacl-main.414</doi>
      <video href="2022.naacl-main.414.mp4"/>
      <pwccode url="https://github.com/clovaai/vat-d" additional="false">clovaai/vat-d</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="415">
      <title><fixed-case>CONFIT</fixed-case>: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning</title>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Arjun</first><last>Nair</last></author>
      <author><first>Borui</first><last>Wang</last></author>
      <author><first>Bingyao</first><last>Wang</last></author>
      <author><first>Jai</first><last>Desai</last></author>
      <author><first>Aaron</first><last>Wade</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>5657-5668</pages>
      <abstract>Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual errors from our annotation, we introduce additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. We show that our model significantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using both automatic metrics, ROUGE and BARTScore, and human evaluation.</abstract>
      <url hash="ceabb3a3">2022.naacl-main.415</url>
      <bibkey>tang-etal-2022-confit</bibkey>
      <doi>10.18653/v1/2022.naacl-main.415</doi>
      <video href="2022.naacl-main.415.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="416">
      <title><fixed-case>C</fixed-case>o<fixed-case>MPM</fixed-case>: Context Modeling with Speaker’s Pre-trained Memory Tracking for Emotion Recognition in Conversation</title>
      <author><first>Joosung</first><last>Lee</last></author>
      <author><first>Wooin</first><last>Lee</last></author>
      <pages>5669-5679</pages>
      <abstract>As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important. If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible. Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances. Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data. However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages. Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge. We introduce CoMPM, which combines the speaker’s pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model. CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data. In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods. Our code is available on github .</abstract>
      <url hash="68b5b2f9">2022.naacl-main.416</url>
      <bibkey>lee-lee-2022-compm</bibkey>
      <doi>10.18653/v1/2022.naacl-main.416</doi>
      <video href="2022.naacl-main.416.mp4"/>
      <pwccode url="https://github.com/rungjoo/compm" additional="false">rungjoo/compm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="417">
      <title>Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries</title>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Ziming</first><last>Mao</last></author>
      <author><first>Griffin</first><last>Adams</last></author>
      <author><first>Borui</first><last>Wang</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>5680-5692</pages>
      <abstract>Current pre-trained models applied for summarization are prone to factual inconsistencies that misrepresent the source text. Evaluating the factual consistency of summaries is thus necessary to develop better models. However, the human evaluation setup for evaluating factual consistency has not been standardized. To determine the factors that affect the reliability of the human evaluation, we crowdsource evaluations for factual consistency across state-of-the-art models on two news summarization datasets using the rating-based Likert Scale and ranking-based Best-Worst Scaling. Our analysis reveals that the ranking-based Best-Worst Scaling offers a more reliable measure of summary quality across datasets and that the reliability of Likert ratings highly depends on the target dataset and the evaluation design. To improve crowdsourcing reliability, we extend the scale of the Likert rating and present a scoring algorithm for Best-Worst Scaling that we call value learning. Our crowdsourcing guidelines will be publicly available to facilitate future work on factual consistency in summarization.</abstract>
      <url hash="45cd2eeb">2022.naacl-main.417</url>
      <bibkey>tang-etal-2022-investigating</bibkey>
      <doi>10.18653/v1/2022.naacl-main.417</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="418">
      <title><fixed-case>D</fixed-case>ial<fixed-case>S</fixed-case>umm<fixed-case>E</fixed-case>val: Revisiting Summarization Evaluation for Dialogues</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>5693-5709</pages>
      <abstract>Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multi-faceted dataset of human judgments containing the outputs of 14 models on SAMSum.</abstract>
      <url hash="9fa789e2">2022.naacl-main.418</url>
      <bibkey>gao-wan-2022-dialsummeval</bibkey>
      <doi>10.18653/v1/2022.naacl-main.418</doi>
      <video href="2022.naacl-main.418.mp4"/>
      <pwccode url="https://github.com/kite99520/dialsummeval" additional="false">kite99520/dialsummeval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/convosumm">ConvoSumm</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="419">
      <title>Hyperbolic Relevance Matching for Neural Keyphrase Extraction</title>
      <author><first>Mingyang</first><last>Song</last></author>
      <author><first>Yi</first><last>Feng</last></author>
      <author><first>Liping</first><last>Jing</last></author>
      <pages>5710-5720</pages>
      <abstract>Keyphrase extraction is a fundamental task in natural language processing that aims to extract a set of phrases with important information from a source document. Identifying important keyphrases is the central component of keyphrase extraction, and its main challenge is learning to represent information comprehensively and discriminate importance accurately. In this paper, to address the above issues, we design a new hyperbolic matching model (HyperMatch) to explore keyphrase extraction in hyperbolic space. Concretely, to represent information comprehensively, HyperMatch first takes advantage of the hidden representations in the middle layers of RoBERTa and integrates them as the word embeddings via an adaptive mixing layer to capture the hierarchical syntactic and semantic structures. Then, considering the latent structure information hidden in natural languages, HyperMatch embeds candidate phrases and documents in the same hyperbolic space via a hyperbolic phrase encoder and a hyperbolic document encoder. To discriminate importance accurately, HyperMatch estimates the importance of each candidate phrase by explicitly modeling the phrase-document relevance via the Poincaré distance and optimizes the whole model by minimizing the hyperbolic margin-based triplet loss. Extensive experiments are conducted on six benchmark datasets and demonstrate that HyperMatch outperforms the recent state-of-the-art baselines.</abstract>
      <url hash="06e02ffc">2022.naacl-main.419</url>
      <bibkey>song-etal-2022-hyperbolic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.419</doi>
      <video href="2022.naacl-main.419.mp4"/>
      <pwccode url="https://github.com/mysong7nlper/hypermatch" additional="false">mysong7nlper/hypermatch</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="420">
      <title>Template-free Prompt Tuning for Few-shot <fixed-case>NER</fixed-case></title>
      <author><first>Ruotian</first><last>Ma</last></author>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Yiding</first><last>Tan</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>5721-5732</pages>
      <abstract>Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding the complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot settings. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.</abstract>
      <url hash="6ef766f4">2022.naacl-main.420</url>
      <attachment type="software" hash="c2ab8788">2022.naacl-main.420.software.zip</attachment>
      <bibkey>ma-etal-2022-template</bibkey>
      <doi>10.18653/v1/2022.naacl-main.420</doi>
      <video href="2022.naacl-main.420.mp4"/>
      <pwccode url="https://github.com/rtmaww/EntLM" additional="false">rtmaww/EntLM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="421">
      <title>Few-Shot Document-Level Relation Extraction</title>
      <author><first>Nicholas</first><last>Popovic</last></author>
      <author><first>Michael</first><last>Färber</last></author>
      <pages>5733-5746</pages>
      <abstract>We present FREDo, a few-shot document-level relation extraction (FSDLRE) benchmark. As opposed to existing benchmarks which are built on sentence-level relation extraction corpora, we argue that document-level corpora provide more realism, particularly regarding none-of-the-above (NOTA) distributions. Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on two existing supervised learning data sets, DocRED and sciERC. We adapt the state-of-the-art sentence-level method MNAV to the document-level and develop it further for improved domain adaptation. We find FSDLRE to be a challenging setting with interesting new characteristics such as the ability to sample NOTA instances from the support set. The data, code, and trained models are available online (https://github.com/nicpopovic/FREDo).</abstract>
      <url hash="2e862075">2022.naacl-main.421</url>
      <bibkey>popovic-farber-2022-shot</bibkey>
      <doi>10.18653/v1/2022.naacl-main.421</doi>
      <video href="2022.naacl-main.421.mp4"/>
      <pwccode url="https://github.com/nicpopovic/fredo" additional="false">nicpopovic/fredo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fredo">FREDo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="422">
      <title><fixed-case>L</fixed-case>a<fixed-case>M</fixed-case>emo: Language Modeling with Look-Ahead Memory</title>
      <author><first>Haozhe</first><last>Ji</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Zhenyu</first><last>Yang</last></author>
      <author><first>Zhipeng</first><last>Hu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>5747-5762</pages>
      <abstract>Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory mechanisms.</abstract>
      <url hash="8cd195d6">2022.naacl-main.422</url>
      <bibkey>ji-etal-2022-lamemo</bibkey>
      <doi>10.18653/v1/2022.naacl-main.422</doi>
      <video href="2022.naacl-main.422.mp4"/>
      <pwccode url="https://github.com/thu-coai/lamemo" additional="false">thu-coai/lamemo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="423">
      <title>Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with <fixed-case>VAE</fixed-case>s</title>
      <author><first>Ghazi</first><last>Felhi</last></author>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>5763-5776</pages>
      <abstract>We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers. In the attention of Transformers, <tex-math>keys</tex-math> handle information selection while <tex-math>values</tex-math> specify what information is conveyed. Our model, dubbed QKVAE, uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values. We run experiments on latent representations and experiments on syntax/semantics transfer which show that QKVAE displays clear signs of disentangled syntax and semantics. We also show that our model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer. The code for our experiments is publicly available.</abstract>
      <url hash="048551e7">2022.naacl-main.423</url>
      <bibkey>felhi-etal-2022-exploiting</bibkey>
      <doi>10.18653/v1/2022.naacl-main.423</doi>
      <video href="2022.naacl-main.423.mp4"/>
      <pwccode url="https://github.com/ghazi-f/qkvae" additional="false">ghazi-f/qkvae</pwccode>
    </paper>
    <paper id="424">
      <title>Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints</title>
      <author><first>Chun</first><last>Zeng</last></author>
      <author><first>Jiangjie</first><last>Chen</last></author>
      <author><first>Tianyi</first><last>Zhuang</last></author>
      <author><first>Rui</first><last>Xu</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Qin</first><last>Ying</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <pages>5777-5790</pages>
      <abstract>Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains.However, current autoregressive approaches suffer from high latency.In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage.We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well.To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints.Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints.</abstract>
      <url hash="200a9710">2022.naacl-main.424</url>
      <bibkey>zeng-etal-2022-neighbors</bibkey>
      <doi>10.18653/v1/2022.naacl-main.424</doi>
      <video href="2022.naacl-main.424.mp4"/>
      <pwccode url="https://github.com/sted-byte/act4nat" additional="false">sted-byte/act4nat</pwccode>
    </paper>
    <paper id="425">
      <title>What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured</title>
      <author><first>Alexander</first><last>Henlein</last></author>
      <author><first>Alexander</first><last>Mehler</last></author>
      <pages>5791-5807</pages>
      <abstract>Transformer-based models are now predominant in NLP.They outperform approaches based on static models in many respects.This success has in turn prompted research that reveals a number of biases in the language models generated by transformers.In this paper we utilize this research on biases to investigate to what extent transformer-based language models allow for extracting knowledge about object relations (X occurs in Y; X consists of Z; action A involves using X).To this end, we compare contextualized models with their static counterparts. We make this comparison dependent on the application of a number of similarity measures and classifiers.Our results are threefold:Firstly, we show that the models combined with the different similarity measures differ greatly in terms of the amount of knowledge they allow for extracting.Secondly, our results suggest that similarity measures perform much worse than classifier-based approaches.Thirdly, we show that, surprisingly, static models perform almost as well as contextualized models – in some cases even better.</abstract>
      <url hash="5e115295">2022.naacl-main.425</url>
      <bibkey>henlein-mehler-2022-toothbrushes</bibkey>
      <doi>10.18653/v1/2022.naacl-main.425</doi>
      <video href="2022.naacl-main.425.mp4"/>
    </paper>
    <paper id="426">
      <title>Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation</title>
      <author><first>Hanxun</first><last>Zhong</last></author>
      <author><first>Zhicheng</first><last>Dou</last></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Hongjin</first><last>Qian</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>5808-5820</pages>
      <abstract>Personalized dialogue systems explore the problem of generating responses that are consistent with the user’s personality, which has raised much attention in recent years. Existing personalized dialogue systems have tried to extract user profiles from dialogue history to guide personalized response generation. Since the dialogue history is usually long and noisy, most existing methods truncate the dialogue history to model the user’s personality. Such methods can generate some personalized responses, but a large part of dialogue history is wasted, leading to sub-optimal performance of personalized response generation. In this work, we propose to refine the user dialogue history on a large scale, based on which we can handle more dialogue history and obtain more abundant and accurate persona information. Specifically, we design an MSP model which consists of three personal information refiners and a personalized response generator. With these multi-level refiners, we can sparsely extract the most valuable information (tokens) from the dialogue history and leverage other similar users’ data to enhance personalization. Experimental results on two real-world datasets demonstrate the superiority of our model in generating more informative and personalized responses.</abstract>
      <url hash="ad85b2c2">2022.naacl-main.426</url>
      <bibkey>zhong-etal-2022-less</bibkey>
      <doi>10.18653/v1/2022.naacl-main.426</doi>
      <video href="2022.naacl-main.426.mp4"/>
    </paper>
    <paper id="427">
      <title>A Holistic Framework for Analyzing the <fixed-case>COVID</fixed-case>-19 Vaccine Debate</title>
      <author><first>Maria Leonor</first><last>Pacheco</last></author>
      <author><first>Tunazzina</first><last>Islam</last></author>
      <author><first>Monal</first><last>Mahajan</last></author>
      <author><first>Andrey</first><last>Shor</last></author>
      <author><first>Ming</first><last>Yin</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>5821-5839</pages>
      <abstract>The Covid-19 pandemic has led to infodemic of low quality information leading to poor health decisions. Combating the outcomes of this infodemic is not only a question of identifying false claims, but also reasoning about the decisions individuals make.In this work we propose a holistic analysis framework connecting stance and reason analysis, and fine-grained entity level moral sentiment analysis. We study how to model the dependencies between the different level of analysis and incorporate human insights into the learning process. Experiments show that our framework provides reliable predictions even in the low-supervision settings.</abstract>
      <url hash="607fab2b">2022.naacl-main.427</url>
      <bibkey>pacheco-etal-2022-holistic</bibkey>
      <doi>10.18653/v1/2022.naacl-main.427</doi>
      <video href="2022.naacl-main.427.mp4"/>
      <pwccode url="https://gitlab.com/mlpacheco/covid-moral-foundations" additional="false">mlpacheco/covid-moral-foundations</pwccode>
    </paper>
    <paper id="428">
      <title>Learning to Win Lottery Tickets in <fixed-case>BERT</fixed-case> Transfer via Task-agnostic Mask Training</title>
      <author><first>Yuanxin</first><last>Liu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Zheng</first><last>Lin</last></author>
      <author><first>Peng</first><last>Fu</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>5840-5857</pages>
      <abstract>Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained language models (PLMs) like BERT contain matching subnetworks that have similar transfer learning performance as the original PLM. These subnetworks are found using magnitude-based pruning. In this paper, we find that the BERT subnetworks have even more potential than these studies have shown. Firstly, we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance, which correlates with the downstream transferability. Inspired by this, we propose to directly optimize the subnetwork structure towards the pre-training objectives, which can better preserve the pre-training performance. Specifically, we train binary masks over model weights on the pre-training tasks, with the aim of preserving the universal transferability of the subnetwork, which is agnostic to any specific downstream tasks. We then fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The results show that, compared with magnitude pruning, mask training can effectively find BERT subnetworks with improved overall performance on downstream tasks. Moreover, our method is also more efficient in searching subnetworks and more advantageous when fine-tuning within a certain range of data scarcity. Our code is available at https://github.com/llyx97/TAMT.</abstract>
      <url hash="ff444c19">2022.naacl-main.428</url>
      <bibkey>liu-etal-2022-learning-win</bibkey>
      <doi>10.18653/v1/2022.naacl-main.428</doi>
      <video href="2022.naacl-main.428.mp4"/>
      <pwccode url="https://github.com/llyx97/TAMT" additional="false">llyx97/TAMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="429">
      <title>You Don’t Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers’ Private Personas</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Lixin</first><last>Fan</last></author>
      <pages>5858-5870</pages>
      <abstract>Social chatbots, also known as chit-chat chatbots, evolve rapidly with large pretrained language models. Despite the huge progress, privacy concerns have arisen recently: training data of large language models can be extracted via model inversion attacks. On the other hand, the datasets used for training chatbots contain many private conversations between two individuals. In this work, we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet. We show that speakers’ personas can be inferred through a simple neural network with high accuracy. To this end, we propose effective defense objectives to protect persona leakage from hidden states. We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve language models’ powerful generation ability.</abstract>
      <url hash="b2ef77f7">2022.naacl-main.429</url>
      <attachment type="software" hash="6b041950">2022.naacl-main.429.software.zip</attachment>
      <bibkey>li-etal-2022-dont</bibkey>
      <doi>10.18653/v1/2022.naacl-main.429</doi>
      <video href="2022.naacl-main.429.mp4"/>
      <pwccode url="https://github.com/hkust-knowcomp/persona_leakage_and_defense_in_gpt-2" additional="false">hkust-knowcomp/persona_leakage_and_defense_in_gpt-2</pwccode>
    </paper>
    <paper id="430">
      <title>Explaining Dialogue Evaluation Metrics using Adversarial Behavioral Analysis</title>
      <author><first>Baber</first><last>Khalid</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <pages>5871-5883</pages>
      <abstract>There is an increasing trend in using neural methods for dialogue model evaluation. Lack of a framework to investigate these metrics can cause dialogue models to reflect their biases and cause unforeseen problems during interactions. In this work, we propose an adversarial test-suite which generates problematic variations of various dialogue aspects, e.g. logical entailment, using automatic heuristics. We show that dialogue metrics for both open-domain and task-oriented settings are biased in their assessments of different conversation behaviors and fail to properly penalize problematic conversations, by analyzing their assessments of these problematic examples. We conclude that variability in training methodologies and data-induced biases are some of the main causes of these problems. We also conduct an investigation into the metric behaviors using a black-box interpretability model which corroborates our findings and provides evidence that metrics pay attention to the problematic conversational constructs signaling a misunderstanding of different conversation semantics.</abstract>
      <url hash="cb828148">2022.naacl-main.430</url>
      <bibkey>khalid-lee-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.430</doi>
      <video href="2022.naacl-main.430.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="431">
      <title>Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection</title>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Laura</first><last>Vianna</last></author>
      <author><first>Xuhui</first><last>Zhou</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>5884-5906</pages>
      <abstract>The perceived toxicity of language can vary based on someone’s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system’s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.</abstract>
      <url hash="413b20ff">2022.naacl-main.431</url>
      <bibkey>sap-etal-2022-annotators</bibkey>
      <doi>10.18653/v1/2022.naacl-main.431</doi>
      <video href="2022.naacl-main.431.mp4"/>
    </paper>
    <paper id="432">
      <title>Non-Autoregressive <fixed-case>C</fixed-case>hinese <fixed-case>ASR</fixed-case> Error Correction with Phonological Training</title>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Ruiqing</first><last>Zhang</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <pages>5907-5917</pages>
      <abstract>Automatic Speech Recognition (ASR) is an efficient and widely used input method that transcribes speech signals into text. As the errors introduced by ASR systems will impair the performance of downstream tasks, we introduce a post-processing error correction method, PhVEC, to correct errors in text space. For the errors in ASR result, existing works mainly focus on fixed-length corrections, modifying each wrong token to a correct one (one-to-one correction), but rarely consider the variable-length correction (one-to-many or many-to-one correction). In this paper, we propose an efficient non-autoregressive (NAR) method for Chinese ASR error correction for both cases. Instead of conventionally predicting the sentence length in NAR methods, we propose a novel approach that uses phonological tokens to extend the source sentence for variable-length correction, enabling our model to generate phonetically similar corrections. Experimental results on datasets of different domains show that our method achieves significant improvement in word error rate reduction and speeds up the inference by 6.2 times compared with the autoregressive model.</abstract>
      <url hash="e83a0ed5">2022.naacl-main.432</url>
      <attachment type="software" hash="ef45caf1">2022.naacl-main.432.software.zip</attachment>
      <bibkey>fang-etal-2022-non</bibkey>
      <doi>10.18653/v1/2022.naacl-main.432</doi>
      <video href="2022.naacl-main.432.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
    </paper>
    <paper id="433">
      <title>Hate Speech and Counter Speech Detection: Conversational Context Does Matter</title>
      <author><first>Xinchen</first><last>Yu</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <author><first>Lingzi</first><last>Hong</last></author>
      <pages>5918-5930</pages>
      <abstract>Hate speech is plaguing the cyberspace along with user-generated content. Adding counter speech has become an effective way to combat hate speech online. Existing datasets and models target either (a) hate speech or (b) hate and counter speech but disregard the context. This paper investigates the role of context in the annotation and detection of online hate and counter speech, where context is defined as the preceding comment in a conversation thread. We created a context-aware dataset for a 3-way classification task on Reddit comments: hate speech, counter speech, or neutral. Our analyses indicate that context is critical to identify hate and counter speech: human judgments change for most comments depending on whether we show annotators the context. A linguistic analysis draws insights into the language people use to express hate and counter speech. Experimental results show that neural networks obtain significantly better results if context is taken into account. We also present qualitative error analyses shedding light into (a) when and why context is beneficial and (b) the remaining errors made by our best model when context is taken into account.</abstract>
      <url hash="8607b354">2022.naacl-main.433</url>
      <bibkey>yu-etal-2022-hate</bibkey>
      <doi>10.18653/v1/2022.naacl-main.433</doi>
      <video href="2022.naacl-main.433.mp4"/>
      <pwccode url="https://github.com/xinchenyu/counter_context" additional="false">xinchenyu/counter_context</pwccode>
    </paper>
    <paper id="434">
      <title><fixed-case>DACSA</fixed-case>: A large-scale Dataset for Automatic summarization of <fixed-case>C</fixed-case>atalan and <fixed-case>S</fixed-case>panish newspaper Articles</title>
      <author><first>Encarnación</first><last>Segarra Soriano</last></author>
      <author><first>Vicent</first><last>Ahuir</last></author>
      <author><first>Lluís-F.</first><last>Hurtado</last></author>
      <author><first>José</first><last>González</last></author>
      <pages>5931-5943</pages>
      <abstract>The application of supervised methods to automatic summarization requires the availability of adequate corpora consisting of a set of document-summary pairs. As in most Natural Language Processing tasks, the great majority of available datasets for summarization are in English, making it difficult to develop automatic summarization models for other languages. Although Spanish is gradually forming part of some recent summarization corpora, it is not the same for minority languages such as Catalan.In this work, we describe the construction of a corpus of Catalan and Spanish newspapers, the Dataset for Automatic summarization of Catalan and Spanish newspaper Articles (DACSA) corpus. It is a high-quality large-scale corpus that can be used to train summarization models for Catalan and Spanish.We have carried out an analysis of the corpus, both in terms of the style of the summaries and the difficulty of the summarization task. In particular, we have used a set of well-known metrics in the summarization field in order to characterize the corpus. Additionally, for benchmarking purposes, we have evaluated the performances of some extractive and abstractive summarization systems on the DACSA corpus.</abstract>
      <url hash="527c57a3">2022.naacl-main.434</url>
      <bibkey>segarra-soriano-etal-2022-dacsa</bibkey>
      <doi>10.18653/v1/2022.naacl-main.434</doi>
      <video href="2022.naacl-main.434.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="435">
      <title>Time Waits for No One! Analysis and Challenges of Temporal Misalignment</title>
      <author><first>Kelvin</first><last>Luu</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Karishma</first><last>Mandyam</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>5944-5958</pages>
      <abstract>When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domain-specific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.</abstract>
      <url hash="bfe7bbf4">2022.naacl-main.435</url>
      <bibkey>luu-etal-2022-time</bibkey>
      <doi>10.18653/v1/2022.naacl-main.435</doi>
      <video href="2022.naacl-main.435.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="436">
      <title><fixed-case>MCSE</fixed-case>: <fixed-case>M</fixed-case>ultimodal Contrastive Learning of Sentence Embeddings</title>
      <author><first>Miaoran</first><last>Zhang</last></author>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Michael</first><last>Hedderich</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>5959-5969</pages>
      <abstract>Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman’s correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.</abstract>
      <url hash="e2322c56">2022.naacl-main.436</url>
      <bibkey>zhang-etal-2022-mcse</bibkey>
      <doi>10.18653/v1/2022.naacl-main.436</doi>
      <video href="2022.naacl-main.436.mp4"/>
      <pwccode url="https://github.com/uds-lsv/mcse" additional="false">uds-lsv/mcse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="437">
      <title><fixed-case>H</fixed-case>i<fixed-case>URE</fixed-case>: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction</title>
      <author><first>Shuliang</first><last>Liu</last></author>
      <author><first>Xuming</first><last>Hu</last></author>
      <author><first>Chenwei</first><last>Zhang</last></author>
      <author><first>Shu’ang</first><last>Li</last></author>
      <author><first>Lijie</first><last>Wen</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>5970-5980</pages>
      <abstract>Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.</abstract>
      <url hash="f6ac9275">2022.naacl-main.437</url>
      <attachment type="software" hash="c4d93f3d">2022.naacl-main.437.software.zip</attachment>
      <bibkey>liu-etal-2022-hiure</bibkey>
      <doi>10.18653/v1/2022.naacl-main.437</doi>
      <video href="2022.naacl-main.437.mp4"/>
      <pwccode url="https://github.com/thu-bpm/hiure" additional="false">thu-bpm/hiure</pwccode>
    </paper>
    <paper id="438">
      <title>Diagnosing Vision-and-Language Navigation: What Really Matters</title>
      <author><first>Wanrong</first><last>Zhu</last></author>
      <author><first>Yuankai</first><last>Qi</last></author>
      <author><first>Pradyumna</first><last>Narayana</last></author>
      <author><first>Kazoo</first><last>Sone</last></author>
      <author><first>Sugato</first><last>Basu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Qi</first><last>Wu</last></author>
      <author><first>Miguel</first><last>Eckstein</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>5981-5993</pages>
      <abstract>Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, there still exist non-negligible gaps between machines’ performance and human benchmarks. Moreover, the agents’ inner mechanisms for navigation decisions remain unclear. To the best of our knowledge, how the agents perceive the multimodal input is under-studied and needs investigation. In this work, we conduct a series of diagnostic experiments to unveil agents’ focus during navigation. Results show that indoor navigation agents refer to both object and direction tokens when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and poorly understand the object tokens. Transformer-based agents acquire a better cross-modal understanding of objects and display strong numerical reasoning ability than non-Transformer-based agents. When it comes to vision-and-language alignments, many models claim that they can align object tokens with specific visual targets. We find unbalanced attention on the vision and text input and doubt the reliability of such cross-modal alignments.</abstract>
      <url hash="ab662d81">2022.naacl-main.438</url>
      <bibkey>zhu-etal-2022-diagnosing</bibkey>
      <doi>10.18653/v1/2022.naacl-main.438</doi>
      <video href="2022.naacl-main.438.mp4"/>
      <pwccode url="https://github.com/VegB/Diagnose_VLN" additional="false">VegB/Diagnose_VLN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/room-to-room">R2R</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/touchdown-dataset">Touchdown Dataset</pwcdataset>
    </paper>
    <paper id="439">
      <title>Aligning to Social Norms and Values in Interactive Narratives</title>
      <author><first>Prithviraj</first><last>Ammanabrolu</last></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>5994-6017</pages>
      <abstract>We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games—environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms—causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people—in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others.We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.</abstract>
      <url hash="646d4a44">2022.naacl-main.439</url>
      <bibkey>ammanabrolu-etal-2022-aligning</bibkey>
      <doi>10.18653/v1/2022.naacl-main.439</doi>
      <video href="2022.naacl-main.439.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ethics-1">ETHICS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jericho">Jericho</pwcdataset>
    </paper>
    <paper id="440">
      <title><fixed-case>MOVER</fixed-case>: Mask, Over-generate and Rank for Hyperbole Generation</title>
      <author><first>Yunxiang</first><last>Zhang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>6018-6030</pages>
      <abstract>Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literal-hyperbole pairs. During training, we fine-tune BART to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.</abstract>
      <url hash="52187e74">2022.naacl-main.440</url>
      <bibkey>zhang-wan-2022-mover</bibkey>
      <doi>10.18653/v1/2022.naacl-main.440</doi>
      <video href="2022.naacl-main.440.mp4"/>
      <pwccode url="https://github.com/yunx-z/mover" additional="false">yunx-z/mover</pwccode>
    </paper>
    <paper id="441">
      <title>Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference</title>
      <author><first>Emīls</first><last>Kadiķis</last></author>
      <author><first>Vaibhav</first><last>Srivastav</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>6031-6037</pages>
      <abstract>The task of natural language inference (NLI), to decide if a hypothesis entails or contradicts a premise, received considerable attention in recent years. All competitive systems build on top of contextualized representations and make use of transformer architectures for learning an NLI model. When somebody is faced with a particular NLI task, they need to select the best model that is available. This is a time-consuming and resource-intense endeavour. To solve this practical problem, we propose a simple method for predicting the performance without actually fine-tuning the model. We do this by testing how well the pre-trained models perform on the aNLI task when just comparing sentence embeddings with cosine similarity to what kind of performance is achieved when training a classifier on top of these embeddings. We show that the accuracy of the cosine similarity approach correlates strongly with the accuracy of the classification approach with a Pearson correlation coefficient of 0.65. Since the similarity is orders of magnitude faster to compute on a given dataset (less than a minute vs. hours), our method can lead to significant time savings in the process of model selection.</abstract>
      <url hash="e7198ed1">2022.naacl-main.441</url>
      <bibkey>kadikis-etal-2022-embarrassingly</bibkey>
      <doi>10.18653/v1/2022.naacl-main.441</doi>
      <video href="2022.naacl-main.441.mp4"/>
      <pwccode url="https://github.com/Vaibhavs10/anli-performance-prediction" additional="false">Vaibhavs10/anli-performance-prediction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/art-dataset">ART Dataset</pwcdataset>
    </paper>
    <paper id="442">
      <title>Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Rotem</first><last>Dror</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>6038-6052</pages>
      <abstract>How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.</abstract>
      <url hash="05f47b57">2022.naacl-main.442</url>
      <bibkey>deutsch-etal-2022-examining</bibkey>
      <doi>10.18653/v1/2022.naacl-main.442</doi>
      <video href="2022.naacl-main.442.mp4"/>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2022-06-28">
    <meta>
      <booktitle>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</booktitle>
      <editor><first>Daphne</first><last>Ippolito</last></editor>
      <editor><first>Liunian Harold</first><last>Li</last></editor>
      <editor><first>Maria Leonor</first><last>Pacheco</last></editor>
      <editor><first>Danqi</first><last>Chen</last></editor>
      <editor><first>Nianwen</first><last>Xue</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hybrid: Seattle, Washington + Online</address>
      <month>July</month>
      <year>2022</year>
      <url hash="a57edf60">2022.naacl-srw</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="b8bc7597">2022.naacl-srw.0</url>
      <bibkey>naacl-2022-2022-north</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention</title>
      <author><first>Ayush K</first><last>Chakravarthy</last></author>
      <author><first>Jacob Labe</first><last>Russin</last></author>
      <author><first>Randall</first><last>O’Reilly</last></author>
      <pages>1-8</pages>
      <abstract>Systematicity is thought to be a key inductive bias possessed by humans that is lacking in standard natural language processing systems such as those utilizing transformers. In this work, we investigate the extent to which the failure of transformers on systematic generalization tests can be attributed to a lack of linguistic abstraction in its attention mechanism. We develop a novel modification to the transformer by implementing two separate input streams: a role stream controls the attention distributions (i.e., queries and keys) at each layer, and a filler stream determines the values. Our results show that when abstract role labels are assigned to input sequences and provided to the role stream, systematic generalization is improved.</abstract>
      <url hash="78d83891">2022.naacl-srw.1</url>
      <bibkey>chakravarthy-etal-2022-systematicity</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.1</doi>
      <video href="2022.naacl-srw.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="2">
      <title>Grounding in social media: An approach to building a chit-chat dialogue model</title>
      <author><first>Ritvik</first><last>Choudhary</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>9-15</pages>
      <abstract>Building open-domain dialogue systems capable of rich human-like conversational ability is one of the fundamental challenges in language generation. However, even with recent advancements in the field, existing open-domain generative models fail to capture and utilize external knowledge, leading to repetitive or generic responses to unseen utterances. Current work on knowledge-grounded dialogue generation primarily focuses on persona incorporation or searching a fact-based structured knowledge source such as Wikipedia. Our method takes a broader and simpler approach, which aims to improve the raw conversation ability of the system by mimicking the human response behavior through casual interactions found on social media. Utilizing a joint retriever-generator setup, the model queries a large set of filtered comment data from Reddit to act as additional context for the seq2seq generator. Automatic and human evaluations on open-domain dialogue datasets demonstrate the effectiveness of our approach.</abstract>
      <url hash="2bc40cf5">2022.naacl-srw.2</url>
      <bibkey>choudhary-kawahara-2022-grounding</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.2</doi>
      <video href="2022.naacl-srw.2.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog-1">DailyDialog++</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>E</fixed-case>xtra<fixed-case>P</fixed-case>hrase: Efficient Data Augmentation for Abstractive Summarization</title>
      <author><first>Mengsay</first><last>Loem</last></author>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <pages>16-24</pages>
      <abstract>Neural models trained with large amount of parallel data have achieved impressive performance in abstractive summarization tasks. However, large-scale parallel corpora are expensive and challenging to construct. In this work, we introduce a low-cost and effective strategy, ExtraPhrase, to augment training data for abstractive summarization tasks. ExtraPhrase constructs pseudo training data in two steps: extractive summarization and paraphrasing. We extract major parts of an input text in the extractive summarization step and obtain its diverse expressions with the paraphrasing step. Through experiments, we show that ExtraPhrase improves the performance of abstractive summarization tasks by more than 0.50 points in ROUGE scores compared to the setting without data augmentation. ExtraPhrase also outperforms existing methods such as back-translation and self-training. We also show that ExtraPhrase is significantly effective when the amount of genuine training data is remarkably small, i.e., a low-resource setting. Moreover, ExtraPhrase is more cost-efficient than the existing approaches</abstract>
      <url hash="2b495afb">2022.naacl-srw.3</url>
      <bibkey>loem-etal-2022-extraphrase</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.3</doi>
      <video href="2022.naacl-srw.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="4">
      <title>Regularized Training of Nearest Neighbor Language Models</title>
      <author><first>Jean-Francois</first><last>Ton</last></author>
      <author><first>Walter</first><last>Talbott</last></author>
      <author><first>Shuangfei</first><last>Zhai</last></author>
      <author><first>Joshua M.</first><last>Susskind</last></author>
      <pages>25-30</pages>
      <abstract>Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon <tex-math>k</tex-math>NN-LM (CITATION), which uses a pre-trained language model together with an exhaustive <tex-math>k</tex-math>NN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the <tex-math>k</tex-math>NN-LM performance by instead training a LM with the knowledge that we will be using a <tex-math>k</tex-math>NN post-hoc. We achieved significant improvement using our method on language modeling tasks on WIKI-2 and WIKI-103. The main phenomenon that we encounter is that adding a simple L2 regularization on the activations (not weights) of the model, a transformer, improves the post-hoc <tex-math>k</tex-math>NN classification performance. We explore some possible reasons for this improvement. In particular, we find that the added L2 regularization seems to improve the performance for high-frequency words without deteriorating the performance for low frequency ones.</abstract>
      <url hash="95d9852b">2022.naacl-srw.4</url>
      <bibkey>ton-etal-2022-regularized</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.4</doi>
      <video href="2022.naacl-srw.4.mp4"/>
    </paper>
    <paper id="5">
      <title>“Again, Dozens of Refugees Drowned”: A Computational Study of Political Framing Evoked by Presuppositions</title>
      <author><first>Qi</first><last>Yu</last></author>
      <pages>31-43</pages>
      <abstract>Earlier NLP studies on framing in political discourse have focused heavily on shallow classification of issue framing, while framing effect arising from pragmatic cues remains neglected. We put forward this latter type of framing as “pragmatic framing”. To bridge this gap, we take presupposition-triggering adverbs such as ‘again’ as a study case, and quantitatively investigate how different German newspapers use them to covertly evoke different attitudinal subtexts in their report on the event “European Refugee Crisis” (2014-2018). Our study demonstrates the crucial role of presuppositions in framing, and emphasizes the necessity of more attention on pragmatic framing in the research of automated framing detection.</abstract>
      <url hash="df0caf23">2022.naacl-srw.5</url>
      <bibkey>yu-2022-dozens</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.5</doi>
      <video href="2022.naacl-srw.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Methods for Estimating and Improving Robustness of Language Models</title>
      <author><first>Michal</first><last>Stefanik</last></author>
      <pages>44-51</pages>
      <abstract>Despite their outstanding performance, large language models (LLMs) suffer notorious flaws related to their preference for shallow textual relations over full semantic complexity of the problem. This proposal investigates a common denominator of this problem in their weak ability to generalise outside of the training domain. We survey diverse research directions providing estimations of model generalisation ability and find that incorporating some of these measures in the training objectives leads to enhanced distributional robustness of neural models. Based on these findings, we present future research directions enhancing the robustness of LLMs.</abstract>
      <url hash="90567f54">2022.naacl-srw.6</url>
      <bibkey>stefanik-2022-methods</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.6</doi>
      <video href="2022.naacl-srw.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Retrieval-augmented Generation across Heterogeneous Knowledge</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <pages>52-58</pages>
      <abstract>Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.</abstract>
      <url hash="2b8ed428">2022.naacl-srw.7</url>
      <bibkey>yu-2022-retrieval</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.7</doi>
      <video href="2022.naacl-srw.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/creak">CREAK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="8">
      <title>Neural Retriever and Go Beyond: A Thesis Proposal</title>
      <author><first>Man</first><last>Luo</last></author>
      <pages>59-67</pages>
      <abstract>Information Retriever (IR) aims to find the relevant documents (e.g. snippets, passages, and articles) to a given query at large scale. IR plays an important role in many tasks such as open domain question answering and dialogue systems, where external knowledge is needed. In the past, searching algorithms based on term matching have been widely used. Recently, neural-based algorithms (termed as neural retrievers) have gained more attention which can mitigate the limitations of traditional methods. Regardless of the success achieved by neural retrievers, they still face many challenges, e.g. suffering from a small amount of training data and failing to answer simple entity-centric questions. Furthermore, most of the existing neural retrievers are developed for pure-text query. This prevents them from handling multi-modality queries (i.e. the query is composed of textual description and images). This proposal has two goals. First, we introduce methods to address the abovementioned issues of neural retrievers from three angles, new model architectures, IR-oriented pretraining tasks, and generating large scale training data. Second, we identify the future research direction and propose potential corresponding solution.</abstract>
      <url hash="3e721e03">2022.naacl-srw.8</url>
      <bibkey>luo-2022-neural</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.8</doi>
      <video href="2022.naacl-srw.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
    </paper>
    <paper id="9">
      <title>Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation</title>
      <author><first>Xiruo</first><last>Ding</last></author>
      <author><first>Kevin</first><last>Lybarger</last></author>
      <author><first>Justin</first><last>Tauscher</last></author>
      <author><first>Trevor</first><last>Cohen</last></author>
      <pages>68-75</pages>
      <abstract>Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we address this sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The first approach includes Easy Data Augmentation, back translation, and mixup techniques. The second approach utilizes a domain-specific pretrained language model, MentalBERT. To examine the viability of different data augmentation methods, we utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking. We found that with optimized parameter settings, mixup was helpful for rare classes. Performance improvements with an augmented model, MentalBERT, exceed those obtained with data augmentation.</abstract>
      <url hash="0b5fac5c">2022.naacl-srw.9</url>
      <bibkey>ding-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.9</doi>
      <video href="2022.naacl-srw.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Generate, Evaluate, and Select: A Dialogue System with a Response Evaluator for Diversity-Aware Response Generation</title>
      <author><first>Ryoma</first><last>Sakaeda</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>76-82</pages>
      <abstract>We aim to overcome the lack of diversity in responses of current dialogue systems and to develop a dialogue system that is engaging as a conversational partner. We propose a generator-evaluator model that evaluates multiple responses generated by a response generator and selects the best response by an evaluator. By generating multiple responses, we obtain diverse responses. We conduct human evaluations to compare the output of the proposed system with that of a baseline system. The results of the human evaluations showed that the proposed system’s responses were often judged to be better than the baseline system’s, and indicated the effectiveness of the proposed method.</abstract>
      <url hash="652d78e4">2022.naacl-srw.10</url>
      <bibkey>sakaeda-kawahara-2022-generate</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.10</doi>
      <video href="2022.naacl-srw.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Impact of Training Instance Selection on Domain-Specific Entity Extraction using <fixed-case>BERT</fixed-case></title>
      <author><first>Eileen</first><last>Salhofer</last></author>
      <author><first>Xing Lan</first><last>Liu</last></author>
      <author><first>Roman</first><last>Kern</last></author>
      <pages>83-88</pages>
      <abstract>State of the art performances for entity extraction tasks are achieved by supervised learning, specifically, by fine-tuning pretrained language models such as BERT. As a result, annotating application specific data is the first step in many use cases. However, no practical guidelines are available for annotation requirements. This work supports practitioners by empirically answering the frequently asked questions (1) how many training samples to annotate? (2) which examples to annotate? We found that BERT achieves up to 80% F1 when fine-tuned on only 70 training examples, especially on biomedical domain. The key features for guiding the selection of high performing training instances are identified to be pseudo-perplexity and sentence-length. The best training dataset constructed using our proposed selection strategy shows F1 score that is equivalent to a random selection with twice the sample size. The requirement of only a small number of training data implies cheaper implementations and opens door to wider range of applications.</abstract>
      <url hash="82d160d5">2022.naacl-srw.11</url>
      <bibkey>salhofer-etal-2022-impact</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.11</doi>
      <video href="2022.naacl-srw.11.mp4"/>
      <pwccode url="https://github.com/tugraz-isds/kd" additional="false">tugraz-isds/kd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="12">
      <title>Analysing the Correlation between Lexical Ambiguity and Translation Quality in a Multimodal Setting using <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Ali</first><last>Hatami</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <pages>89-95</pages>
      <abstract>Multimodal Neural Machine Translation is focusing on using visual information to translate sentences in the source language into the target language. The main idea is to utilise information from visual modalities to promote the output quality of the text-based translation model. Although the recent multimodal strategies extract the most relevant visual information in images, the effectiveness of using visual information on translation quality changes based on the text dataset. Due to this, this work studies the impact of leveraging visual information in multimodal translation models of ambiguous sentences. Our experiments analyse the Multi30k evaluation dataset and calculate ambiguity scores of sentences based on the WordNet hierarchical structure. To calculate the ambiguity of a sentence, we extract the ambiguity scores for all nouns based on the number of senses in WordNet. The main goal is to find in which sentences, visual content can improve the text-based translation model. We report the correlation between the ambiguity scores and translation quality extracted for all sentences in the English-German dataset.</abstract>
      <url hash="af9ad37b">2022.naacl-srw.12</url>
      <bibkey>hatami-etal-2022-analysing</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.12</doi>
      <video href="2022.naacl-srw.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Building a Personalized Dialogue System with Prompt-Tuning</title>
      <author><first>Tomohito</first><last>Kasahara</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Nguyen</first><last>Tung</last></author>
      <author><first>Shengzhe</first><last>Li</last></author>
      <author><first>Kenta</first><last>Shinzato</last></author>
      <author><first>Toshinori</first><last>Sato</last></author>
      <pages>96-105</pages>
      <abstract>Dialogue systems without consistent responses are not attractive. In this study, we build a dialogue system that can respond based on a given character setting (persona) to bring consistency. Considering the trend of the rapidly increasing scale of language models, we propose an approach that uses prompt-tuning, which has low learning costs, on pre-trained large-scale language models. The results of the automatic and manual evaluations in English and Japanese show that it is possible to build a dialogue system with more natural and personalized responses with less computational resources than fine-tuning.</abstract>
      <url hash="19f01cfb">2022.naacl-srw.13</url>
      <bibkey>kasahara-etal-2022-building</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.13</doi>
      <video href="2022.naacl-srw.13.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>MM</fixed-case>-<fixed-case>GATBT</fixed-case>: Enriching Multimodal Representation Using Graph Attention Network</title>
      <author><first>Seung Byum</first><last>Seo</last></author>
      <author><first>Hyoungwook</first><last>Nam</last></author>
      <author><first>Payam</first><last>Delgosha</last></author>
      <pages>106-112</pages>
      <abstract>While there have been advances in Natural Language Processing (NLP), their success is mainly gained by applying a self-attention mechanism into single or multi-modalities. While this approach has brought significant improvements in multiple downstream tasks, it fails to capture the interaction between different entities. Therefore, we propose MM-GATBT, a multimodal graph representation learning model that captures not only the relational semantics within one modality but also the interactions between different modalities. Specifically, the proposed method constructs image-based node embedding which contains relational semantics of entities. Our empirical results show that MM-GATBT achieves state-of-the-art results among all published papers on the MM-IMDb dataset.</abstract>
      <url hash="84191ea8">2022.naacl-srw.14</url>
      <bibkey>seo-etal-2022-mm</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.14</doi>
      <video href="2022.naacl-srw.14.mp4"/>
      <pwccode url="https://github.com/sbseo/mm-gatbt" additional="false">sbseo/mm-gatbt</pwccode>
    </paper>
    <paper id="15">
      <title>Simulating Feature Structures with Simple Types</title>
      <author><first>Valentin D.</first><last>Richard</last></author>
      <pages>113-122</pages>
      <abstract>Feature structures have been several times considered to enrich categorial grammars in order to build fine-grained grammars. Most attempts to unify both frameworks either model categorial types as feature structures or add feature structures on top of categorial types. We pursue a different approach: using feature structure as categorial atomic types. In this article, we present a procedure to create, from a simplified HPSG grammar, an equivalent abstract categorial grammar (ACG). We represent a feature structure by the enumeration of its totally well-typed upper bounds, so that unification can be simulated as intersection. We implement this idea as a meta-ACG preprocessor.</abstract>
      <url hash="59c29c02">2022.naacl-srw.15</url>
      <bibkey>richard-2022-simulating</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.15</doi>
      <video href="2022.naacl-srw.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Dr. Livingstone, <fixed-case>I</fixed-case> presume? Polishing of foreign character identification in literary texts</title>
      <author><first>Aleksandra</first><last>Konovalova</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Kristiina</first><last>Taivalkoski-Shilov</last></author>
      <pages>123-128</pages>
      <abstract>Character identification is a key element for many narrative-related tasks. To implement it, the baseform of the name of the character (or lemma) needs to be identified, so different appearances of the same character in the narrative could be aligned. In this paper we tackle this problem in translated texts (English–Finnish translation direction), where the challenge regarding lemmatizing foreign names in an agglutinative language appears. To solve this problem, we present and compare several methods. The results show that the method based on a search for the shortest version of the name proves to be the easiest, best performing (83.4% F1), and most resource-independent.</abstract>
      <url hash="1f44c27e">2022.naacl-srw.16</url>
      <bibkey>konovalova-etal-2022-dr</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.16</doi>
      <video href="2022.naacl-srw.16.mp4"/>
    </paper>
    <paper id="17">
      <title>Zuo Zhuan <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Dataset for Word Sense Disambiguation</title>
      <author><first>Xiaomeng</first><last>Pan</last></author>
      <author><first>Hongfei</first><last>Wang</last></author>
      <author><first>Teruaki</first><last>Oka</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>129-135</pages>
      <abstract>Word Sense Disambiguation (WSD) is a core task in Natural Language Processing (NLP). Ancient Chinese has rarely been used in WSD tasks, however, as no public dataset for ancient Chinese WSD tasks exists. Creation of an ancient Chinese dataset is considered a significant challenge because determining the most appropriate sense in a context is difficult and time-consuming owing to the different usages in ancient and modern Chinese. Actually, no public dataset for ancient Chinese WSD tasks exists. To solve the problem of ancient Chinese WSD, we annotate part of Pre-Qin (221 BC) text <i>Zuo Zhuan</i> using a copyright-free dictionary to create a public sense-tagged dataset. Then, we apply a simple Nearest Neighbors (k-NN) method using a pre-trained language model to the dataset. Our code and dataset will be available on GitHub.</abstract>
      <url hash="b4c635d0">2022.naacl-srw.17</url>
      <bibkey>pan-etal-2022-zuo</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.17</doi>
      <video href="2022.naacl-srw.17.mp4"/>
    </paper>
    <paper id="18">
      <title><fixed-case>V</fixed-case>i<fixed-case>T</fixed-case>5: Pretrained Text-to-Text Transformer for <fixed-case>V</fixed-case>ietnamese Language Generation</title>
      <author><first>Long</first><last>Phan</last></author>
      <author><first>Hieu</first><last>Tran</last></author>
      <author><first>Hieu</first><last>Nguyen</last></author>
      <author><first>Trieu H.</first><last>Trinh</last></author>
      <pages>136-142</pages>
      <abstract>We present ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained on a large corpus of high-quality and diverse Vietnamese texts. We benchmark ViT5 on two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text Summarization has been widely studied for the English language thanks to its rich and large source of data, there has been minimal research into the same task in Vietnamese, a much lower resource language. In this work, we perform exhaustive experiments on both Vietnamese Abstractive Summarization and Named Entity Recognition, validating the performance of ViT5 against many other pretrained Transformer-based encoder-decoder models. Our experiments show that ViT5 significantly outperforms existing models and achieves state-of-the-art results on Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5 is competitive against previous best results from pretrained encoder-based Transformer models. Further analysis shows the importance of context length during the self-supervised pretraining on downstream performance across different settings.</abstract>
      <url hash="ee2cd4a9">2022.naacl-srw.18</url>
      <bibkey>phan-etal-2022-vit5</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.18</doi>
      <video href="2022.naacl-srw.18.mp4"/>
      <pwccode url="https://github.com/vietai/vit5" additional="false">vietai/vit5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/phoner-covid19">PhoNER COVID19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vnds">VNDS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
    </paper>
    <paper id="19">
      <title>Compositional Generalization in Grounded Language Learning via Induced Model Sparsity</title>
      <author><first>Sam</first><last>Spilsbury</last></author>
      <author><first>Alexander</first><last>Ilin</last></author>
      <pages>143-155</pages>
      <abstract>We provide a study of how induced model sparsity can help achieve compositional generalization and better sample efficiency in grounded language learning problems. We consider simple language-conditioned navigation problems in a grid world environment with disentangled observations. We show that standard neural architectures do not always yield compositional generalization. To address this, we design an agent that contains a goal identification module that encourages sparse correlations between words in the instruction and attributes of objects, composing them together to find the goal. The output of the goal identification module is the input to a value iteration network planner. Our agent maintains a high level of performance on goals containing novel combinations of properties even when learning from a handful of demonstrations. We examine the internal representations of our agent and find the correct correspondences between words in its dictionary and attributes in the environment.</abstract>
      <url hash="b31f5351">2022.naacl-srw.19</url>
      <bibkey>spilsbury-ilin-2022-compositional</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.19</doi>
      <video href="2022.naacl-srw.19.mp4"/>
      <pwccode url="https://github.com/aalto-ai/sparse-compgen" additional="false">aalto-ai/sparse-compgen</pwccode>
    </paper>
    <paper id="20">
      <title>How do people talk about images? A study on open-domain conversations with images.</title>
      <author><first>Yi-Pei</first><last>Chen</last></author>
      <author><first>Nobuyuki</first><last>Shimizu</last></author>
      <author><first>Takashi</first><last>Miyazaki</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <pages>156-162</pages>
      <abstract>This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, ImageChat. We examined the conversations with images from the perspectives of <tex-math>\textit{image relevancy}</tex-math> and <tex-math>\textit{image information}</tex-math>. We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable. After inspecting the causes, we suggested that understanding the overall scenario of image and connecting objects based on their high-level attributes might be very helpful to generate more engaging open-domain conversations when an image is presented. We proposed enriching the image information with image caption and object tags based on our analysis. With our proposed <tex-math>\textit{image}^{+}</tex-math> features, we improved automatic metrics including BLEU and Bert Score, and increased the diversity and image-relevancy of generated responses to the strong baseline. The result verifies that our analysis provides valuable insights and could facilitate future research on open-domain conversations with images.</abstract>
      <url hash="2e0bb714">2022.naacl-srw.20</url>
      <bibkey>chen-etal-2022-people</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.20</doi>
      <video href="2022.naacl-srw.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Text Style Transfer for Bias Mitigation using Masked Language Modeling</title>
      <author><first>Ewoenam Kwaku</first><last>Tokpo</last></author>
      <author><first>Toon</first><last>Calders</last></author>
      <pages>163-171</pages>
      <abstract>It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy.</abstract>
      <url hash="125037de">2022.naacl-srw.21</url>
      <bibkey>tokpo-calders-2022-text</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.21</doi>
      <video href="2022.naacl-srw.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Differentially Private Instance Encoding against Privacy Attacks</title>
      <author><first>Shangyu</first><last>Xie</last></author>
      <author><first>Yuan</first><last>Hong</last></author>
      <pages>172-180</pages>
      <abstract>TextHide was recently proposed to protect the training data via instance encoding in natural language domain. Due to the lack of theoretic privacy guarantee, such instance encoding scheme has been shown to be vulnerable against privacy attacks, e.g., reconstruction attack. To address such limitation, we revise the instance encoding scheme with differential privacy and thus provide a provable guarantee against privacy attacks. The experimental results also show that the proposed scheme can defend against privacy attacks while ensuring learning utility (as a trade-off).</abstract>
      <url hash="d78e031c">2022.naacl-srw.22</url>
      <bibkey>xie-hong-2022-differentially</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.22</doi>
      <video href="2022.naacl-srw.22.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="23">
      <title>A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the <fixed-case>OBQA</fixed-case> Context</title>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>181-187</pages>
      <abstract>In the open book question answering (OBQA) task, selecting the relevant passages and sentences from distracting information is crucial to reason the answer to a question. HotpotQA dataset is designed to teach and evaluate systems to do both passage ranking and sentence selection. Many existing frameworks use separate models to select relevant passages and sentences respectively. Such systems not only have high complexity in terms of the parameters of models but also fail to take the advantage of training these two tasks together since one task can be beneficial for the other one. In this work, we present a simple yet effective framework to address these limitations by jointly ranking passages and selecting sentences. Furthermore, we propose consistency and similarity constraints to promote the correlation and interaction between passage ranking and sentence selection.The experiments demonstrate that our framework can achieve competitive results with previous systems and outperform the baseline by 28% in terms of exact matching of relevant sentences on the HotpotQA dataset.</abstract>
      <url hash="8d0a2897">2022.naacl-srw.23</url>
      <bibkey>luo-etal-2022-simple</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.23</doi>
      <video href="2022.naacl-srw.23.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="24">
      <title>Multimodal Modeling of Task-Mediated Confusion</title>
      <author><first>Camille</first><last>Mince</last></author>
      <author><first>Skye</first><last>Rhomberg</last></author>
      <author><first>Cecilia</first><last>Alm</last></author>
      <author><first>Reynold</first><last>Bailey</last></author>
      <author><first>Alex</first><last>Ororbia</last></author>
      <pages>188-194</pages>
      <abstract>In order to build more human-like cognitive agents, systems capable of detecting various human emotions must be designed to respond appropriately. Confusion, the combination of an emotional and cognitive state, is under-explored. In this paper, we build upon prior work to develop models that detect confusion from three modalities: video (facial features), audio (prosodic features), and text (transcribed speech features). Our research improves the data collection process by allowing for continuous (as opposed to discrete) annotation of confusion levels. We also craft models based on recurrent neural networks (RNNs) given their ability to predict sequential data. In our experiments, we find that text and video modalities are the most important in predicting confusion while the explored audio features are relatively unimportant predictors of confusion in our data.</abstract>
      <url hash="0e23f0f0">2022.naacl-srw.24</url>
      <bibkey>mince-etal-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.24</doi>
      <video href="2022.naacl-srw.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Probe-Less Probing of <fixed-case>BERT</fixed-case>’s Layer-Wise Linguistic Knowledge with Masked Word Prediction</title>
      <author><first>Tatsuya</first><last>Aoyama</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <pages>195-201</pages>
      <abstract>The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT’s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of likely potential candidates of the masked word prediction, we also investigate how the results differ for tokens within multiword expressions.</abstract>
      <url hash="c5ffc658">2022.naacl-srw.25</url>
      <bibkey>aoyama-schneider-2022-probe</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.25</doi>
      <video href="2022.naacl-srw.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Multimodal large language models for inclusive collaboration learning tasks</title>
      <author><first>Armanda</first><last>Lewis</last></author>
      <pages>202-210</pages>
      <abstract>This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.</abstract>
      <url hash="be160a18">2022.naacl-srw.26</url>
      <bibkey>lewis-2022-multimodal</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.26</doi>
      <video href="2022.naacl-srw.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Neural Networks in a Product of Hyperbolic Spaces</title>
      <author><first>Jun</first><last>Takeuchi</last></author>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <pages>211-221</pages>
      <abstract>Machine learning in hyperbolic spaces has attracted much attention in natural language processing and many other fields. In particular, Hyperbolic Neural Networks (HNNs) have improved a wide variety of tasks, from machine translation to knowledge graph embedding. Although some studies have reported the effectiveness of embedding into the product of multiple hyperbolic spaces, HNNs have mainly been constructed in a single hyperbolic space, and their extension to product spaces has not been sufficiently studied. Therefore, we propose a novel method to extend a given HNN in a single space to a product of hyperbolic spaces. We apply our method to Hyperbolic Graph Convolutional Networks (HGCNs), extending several HNNs. Our model improved the graph node classification accuracy especially on datasets with tree-like structures. The results suggest that neural networks in a product of hyperbolic spaces can be more effective than in a single space in representing structural data.</abstract>
      <url hash="b9553eb8">2022.naacl-srw.27</url>
      <bibkey>takeuchi-etal-2022-neural</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.27</doi>
      <video href="2022.naacl-srw.27.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmed">Pubmed</pwcdataset>
    </paper>
    <paper id="28">
      <title>Explicit Use of Topicality in Dialogue Response Generation</title>
      <author><first>Takumi</first><last>Yoshikoshi</last></author>
      <author><first>Hayato</first><last>Atarashi</last></author>
      <author><first>Takashi</first><last>Kodama</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>222-228</pages>
      <abstract>The current chat dialogue systems implicitly consider the topic given the context, but not explicitly. As a result, these systems often generate inconsistent responses with the topic of the moment. In this study, we propose a dialogue system that responds appropriately following the topic by selecting the entity with the highest “topicality.” In topicality estimation, the model is trained through self-supervised learning that regards entities that appear in both context and response as the topic entities. In response generation, the model is trained to generate topic-relevant responses based on the estimated topicality. Experimental results show that our proposed system can follow the topic more than the existing dialogue system that considers only the context.</abstract>
      <url hash="779bb697">2022.naacl-srw.28</url>
      <bibkey>yoshikoshi-etal-2022-explicit</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.28</doi>
      <video href="2022.naacl-srw.28.mp4"/>
    </paper>
    <paper id="29">
      <title>Automating Human Evaluation of Dialogue Systems</title>
      <author><first>Sujan Reddy</first><last>A</last></author>
      <pages>229-234</pages>
      <abstract>Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent, and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models.</abstract>
      <url hash="203aea07">2022.naacl-srw.29</url>
      <bibkey>a-2022-automating</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.29</doi>
      <video href="2022.naacl-srw.29.mp4"/>
    </paper>
    <paper id="30">
      <title>Strong Heuristics for Named Entity Linking</title>
      <author><first>Marko</first><last>Čuljak</last></author>
      <author><first>Andreas</first><last>Spitz</last></author>
      <author><first>Robert</first><last>West</last></author>
      <author><first>Akhil</first><last>Arora</last></author>
      <pages>235-246</pages>
      <abstract>Named entity linking (NEL) in news is a challenging endeavour due to the frequency of unseen and emerging entities, which necessitates the use of unsupervised or zero-shot methods. However, such methods tend to come with caveats, such as no integration of suitable knowledge bases (like Wikidata) for emerging entities, a lack of scalability, and poor interpretability. Here, we consider person disambiguation in Quotebank, a massive corpus of speaker-attributed quotations from the news, and investigate the suitability of intuitive, lightweight, and scalable heuristics for NEL in web-scale corpora. Our best performing heuristic disambiguates 94% and 63% of the mentions on Quotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the proposed heuristics compare favourably to the state-of-the-art unsupervised and zero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as strong baselines for unsupervised and zero-shot entity linking.</abstract>
      <url hash="745ae789">2022.naacl-srw.30</url>
      <bibkey>culjak-etal-2022-strong</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.30</doi>
      <video href="2022.naacl-srw.30.mp4"/>
      <pwccode url="https://github.com/epfl-dlab/nelight" additional="false">epfl-dlab/nelight</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="31">
      <title>Static and Dynamic Speaker Modeling based on Graph Neural Network for Emotion Recognition in Conversation</title>
      <author><first>Prakhar</first><last>Saxena</last></author>
      <author><first>Yin Jou</first><last>Huang</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>247-253</pages>
      <abstract>Each person has a unique personality which affects how they feel and convey emotions. Hence, speaker modeling is important for the task of emotion recognition in conversation (ERC). In this paper, we propose a novel graph-based ERC model which considers both conversational context and speaker personality. We model the internal state of the speaker (personality) as Static and Dynamic speaker state, where the Dynamic speaker state is modeled with a graph neural network based encoder. Experiments on benchmark dataset shows the effectiveness of our model. Our model outperforms baseline and other graph-based methods. Analysis of results also show the importance of explicit speaker modeling.</abstract>
      <url hash="14d255a7">2022.naacl-srw.31</url>
      <bibkey>saxena-etal-2022-static</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.31</doi>
      <video href="2022.naacl-srw.31.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="32">
      <title>Few-shot fine-tuning <fixed-case>SOTA</fixed-case> summarization models for medical dialogues</title>
      <author><first>David Fraile</first><last>Navarro</last></author>
      <author><first>Mark</first><last>Dras</last></author>
      <author><first>Shlomo</first><last>Berkovsky</last></author>
      <pages>254-266</pages>
      <abstract>Abstractive summarization of medical dialogues presents a challenge for standard training approaches, given the paucity of suitable datasets. We explore the performance of state-of-the-art models with zero-shot and few-shot learning strategies and measure the impact of pretraining with general domain and dialogue-specific text on the summarization performance.</abstract>
      <url hash="b58218c5">2022.naacl-srw.32</url>
      <bibkey>navarro-etal-2022-shot</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.32</doi>
      <video href="2022.naacl-srw.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations</title>
      <author><first>Antoine</first><last>Simoulin</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>267-276</pages>
      <abstract>We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with Bert base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performances. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.</abstract>
      <url hash="14a0d134">2022.naacl-srw.33</url>
      <bibkey>simoulin-crabbe-2022-unifying</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.33</doi>
      <video href="2022.naacl-srw.33.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="34">
      <title>Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation</title>
      <author><first>Gerard</first><last>Sant</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Marta Ruiz</first><last>Costa-jussà</last></author>
      <pages>277-284</pages>
      <abstract>Transformer-based models have been achieving state-of-the-art results in several fields of Natural Language Processing. However, its direct application to speech tasks is not trivial. The nature of this sequences carries problems such as long sequence lengths and redundancy between adjacent tokens. Therefore, we believe that regular self-attention mechanism might not be well suited for it. Different approaches have been proposed to overcome these problems, such as the use of efficient attention mechanisms. However, the use of these methods usually comes with a cost, which is a performance reduction caused by information loss. In this study, we present the Multiformer, a Transformer-based model which allows the use of different attention mechanisms on each head. By doing this, the model is able to bias the self-attention towards the extraction of more diverse token interactions, and the information loss is reduced. Finally, we perform an analysis of the head contributions, and we observe that those architectures where all heads relevance is uniformly distributed obtain better results. Our results show that mixing attention patterns along the different heads and layers outperforms our baseline by up to 0.7 BLEU.</abstract>
      <url hash="094b9633">2022.naacl-srw.34</url>
      <bibkey>sant-etal-2022-multiformer</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.34</doi>
      <video href="2022.naacl-srw.34.mp4"/>
    </paper>
    <paper id="35">
      <title>Defending Compositionality in Emergent Languages</title>
      <author><first>Michal</first><last>Auersperger</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>285-291</pages>
      <abstract>Compositionality has traditionally been understood as a major factor in productivity of language and, more broadly, human cognition. Yet, recently some research started to question its status showing that artificial neural networks are good at generalization even without noticeable compositional behavior. We argue some of these conclusions are too strong and/or incomplete. In the context of a two-agent communication game, we show that compositionality indeed seems essential for successful generalization when the evaluation is done on a suitable dataset.</abstract>
      <url hash="bc9777b5">2022.naacl-srw.35</url>
      <bibkey>auersperger-pecina-2022-defending</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.35</doi>
      <video href="2022.naacl-srw.35.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="36">
      <title>Exploring the Effect of Dialect Mismatched Language Models in <fixed-case>T</fixed-case>elugu Automatic Speech Recognition</title>
      <author><first>Aditya</first><last>Yadavalli</last></author>
      <author><first>Ganesh Sai</first><last>Mirishkar</last></author>
      <author><first>Anil</first><last>Vuppala</last></author>
      <pages>292-301</pages>
      <abstract>Previous research has found that Acoustic Models (AM) of an Automatic Speech Recognition (ASR) system are susceptible to dialect variations within a language, thereby adversely affecting the ASR. To counter this, researchers have proposed to build a dialect-specific AM while keeping the Language Model (LM) constant for all the dialects. This study explores the effect of dialect mismatched LM by considering three different Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We show that dialect variations that surface in the form of a different lexicon, grammar, and occasionally semantics can significantly degrade the performance of the LM under mismatched conditions. Therefore, this degradation has an adverse effect on the ASR even when dialect-specific AM is used. We show a degradation of up to 13.13 perplexity points when LM is used under mismatched conditions. Furthermore, we show a degradation of over 9% and over 15% in Character Error Rate (CER) and Word Error Rate (WER), respectively, in the ASR systems when using mismatched LMs over matched LMs.</abstract>
      <url hash="12d93502">2022.naacl-srw.36</url>
      <bibkey>yadavalli-etal-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.naacl-srw.36</doi>
      <video href="2022.naacl-srw.36.mp4"/>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2022-06-30">
    <meta>
      <booktitle>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</booktitle>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <editor><first>Qiang</first><last>Ning</last></editor>
      <editor><first>Avi</first><last>Sil</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hybrid: Seattle, Washington + Online</address>
      <month>July</month>
      <year>2022</year>
      <url hash="99758d0d">2022.naacl-demo</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="0808d7a2">2022.naacl-demo.0</url>
      <bibkey>naacl-2022-2022-north-american</bibkey>
    </frontmatter>
    <paper id="1">
      <title>textless-lib: a Library for Textless Spoken Language Processing</title>
      <author><first>Eugene</first><last>Kharitonov</last></author>
      <author><first>Jade</first><last>Copet</last></author>
      <author><first>Kushal</first><last>Lakhotia</last></author>
      <author><first>Tu Anh</first><last>Nguyen</last></author>
      <author><first>Paden</first><last>Tomasello</last></author>
      <author><first>Ann</first><last>Lee</last></author>
      <author><first>Ali</first><last>Elkahky</last></author>
      <author><first>Wei-Ning</first><last>Hsu</last></author>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <author><first>Yossi</first><last>Adi</last></author>
      <pages>1-9</pages>
      <abstract>Textless spoken language processing is an exciting area of research that promises to extend applicability of the standard NLP toolset onto spoken language and languages with few or no textual resources.Here, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in the area. We describe the building blocks that the library provides and demonstrate its usability by discuss three different use-case examples: (i) speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation. We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large.</abstract>
      <url hash="d8bd5057">2022.naacl-demo.1</url>
      <bibkey>kharitonov-etal-2022-textless</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.1</doi>
      <video href="2022.naacl-demo.1.mp4"/>
      <pwccode url="https://github.com/facebookresearch/textlesslib" additional="false">facebookresearch/textlesslib</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/libri-light">Libri-Light</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="2">
      <title>Web-based Annotation Interface for Derivational Morphology</title>
      <author><first>Lukáš</first><last>Kyjánek</last></author>
      <pages>10-16</pages>
      <abstract>The paper presents a visual interface for manual annotation of language resources for derivational morphology. The interface is web-based and created using relatively simple programming techniques, and yet it rapidly facilitates and speeds up the annotation process, especially in languages with rich derivational morphology. As such, it can reduce the cost of the process. After introducing manual annotation tasks in derivational morphology, the paper describes the new visual interface and a case study that compares the current annotation method to the annotation using the interface. In addition, it also demonstrates the opportunity to use the interface for manual annotation of syntactic trees. The source codes are freely available under the MIT License on GitHub.</abstract>
      <url hash="ee4421b1">2022.naacl-demo.2</url>
      <bibkey>kyjanek-2022-web</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.2</doi>
      <video href="2022.naacl-demo.2.mp4"/>
    </paper>
    <paper id="3">
      <title><fixed-case>T</fixed-case>urkish<fixed-case>D</fixed-case>elight<fixed-case>NLP</fixed-case>: A Neural <fixed-case>T</fixed-case>urkish <fixed-case>NLP</fixed-case> Toolkit</title>
      <author><first>Huseyin</first><last>Alecakir</last></author>
      <author><first>Necva</first><last>Bölücü</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <pages>17-26</pages>
      <abstract>We introduce a neural Turkish NLP toolkit called TurkishDelightNLP that performs computational linguistic analyses from morphological level to semantic level that involves tasks such as stemming, morphological segmentation, morphological tagging, part-of-speech tagging, dependency parsing, and semantic parsing, as well as high-level NLP tasks such as named entity recognition. We publicly share the open-source Turkish NLP toolkit through a web interface that allows an input text to be analysed in real-time, as well as the open source implementation of the components provided in the toolkit, an API, and several annotated datasets such as word similarity test set to evaluate word embeddings and UCCA-based semantic annotation in Turkish. This will be the first open-source Turkish NLP toolkit that involves a range of NLP tasks in all levels. We believe that it will be useful for other researchers in Turkish NLP and will be also beneficial for other high-level NLP tasks in Turkish.</abstract>
      <url hash="a06ba4f1">2022.naacl-demo.3</url>
      <bibkey>alecakir-etal-2022-turkishdelightnlp</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.3</doi>
      <video href="2022.naacl-demo.3.mp4"/>
      <pwccode url="https://github.com/halecakir/turkish-delight-nlp-api" additional="false">halecakir/turkish-delight-nlp-api</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>ZS</fixed-case>4<fixed-case>IE</fixed-case>: A toolkit for Zero-Shot Information Extraction with simple Verbalizations</title>
      <author><first>Oscar</first><last>Sainz</last></author>
      <author><first>Haoling</first><last>Qiu</last></author>
      <author><first>Oier</first><last>Lopez de Lacalle</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <pages>27-38</pages>
      <abstract>The current workflow for Information Extraction (IE) analysts involves the definition of the entities/relations of interest and a training corpus with annotated examples. In this demonstration we introduce a new workflow where the analyst directly verbalizes the entities/relations, which are then used by a Textual Entailment model to perform zero-shot IE. We present the design and implementation of a toolkit with a user interface, as well as experiments on four IE tasks that show that the system achieves very good performance at zero-shot learning using only 5–15 minutes per type of a user’s effort. Our demonstration system is open-sourced at https://github.com/BBN-E/ZS4IE. A demonstration video is available at https://vimeo.com/676138340.</abstract>
      <url hash="5e063d12">2022.naacl-demo.4</url>
      <bibkey>sainz-etal-2022-zs4ie</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.4</doi>
      <pwccode url="https://github.com/bbn-e/zs4ie" additional="true">bbn-e/zs4ie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="5">
      <title>Flowstorm: Open-Source Platform with Hybrid Dialogue Architecture</title>
      <author><first>Jan</first><last>Pichl</last></author>
      <author><first>Petr</first><last>Marek</last></author>
      <author><first>Jakub</first><last>Konrád</last></author>
      <author><first>Petr</first><last>Lorenc</last></author>
      <author><first>Ondrej</first><last>Kobza</last></author>
      <author><first>Tomáš</first><last>Zajíček</last></author>
      <author><first>Jan</first><last>Šedivý</last></author>
      <pages>39-45</pages>
      <abstract>This paper presents a conversational AI platform called Flowstorm. Flowstorm is an open-source SaaS project suitable for creating, running, and analyzing conversational applications. Thanks to the fast and fully automated build process, the dialogues created within the platform can be executed in seconds. Furthermore, we propose a novel dialogue architecture that uses a combination of tree structures with generative models. The tree structures are also used for training NLU models suitable for specific dialogue scenarios. However, the generative models are globally used across applications and extend the functionality of the dialogue trees. Moreover, the platform functionality benefits from out-of-the-box components, such as the one responsible for extracting data from utterances or working with crawled data. Additionally, it can be extended using a custom code directly in the platform. One of the essential features of the platform is the possibility to reuse the created assets across applications. There is a library of prepared assets where each developer can contribute. All of the features are available through a user-friendly visual editor.</abstract>
      <url hash="1f51ce04">2022.naacl-demo.5</url>
      <bibkey>pichl-etal-2022-flowstorm</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.5</doi>
      <video href="2022.naacl-demo.5.mp4"/>
      <pwccode url="https://gitlab.com/promethistai/flowstorm" additional="false">promethistai/flowstorm</pwccode>
    </paper>
    <paper id="6">
      <title>Contrastive Explanations of Text Classifiers as a Service</title>
      <author><first>Lorenzo</first><last>Malandri</last></author>
      <author><first>Fabio</first><last>Mercorio</last></author>
      <author><first>Mario</first><last>Mezzanzanica</last></author>
      <author><first>Navid</first><last>Nobani</last></author>
      <author><first>Andrea</first><last>Seveso</last></author>
      <pages>46-53</pages>
      <abstract>The recent growth of black-box machine-learning methods in data analysis has increased the demand for explanation methods and tools to understand their behaviour and assist human-ML model cooperation. In this paper, we demonstrate ContrXT, a novel approach that uses natural language explanations to help users to comprehend how a back-box model works. ContrXT provides time contrastive (t-contrast) explanations by computing the differences in the classification logic of two different trained models and then reasoning on their symbolic representations through Binary Decision Diagrams. ContrXT is publicly available at ContrXT.ai as a python pip package.</abstract>
      <url hash="0d8a19ab">2022.naacl-demo.6</url>
      <bibkey>malandri-etal-2022-contrastive</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.6</doi>
      <video href="2022.naacl-demo.6.mp4"/>
    </paper>
    <paper id="7">
      <title><fixed-case>RESIN</fixed-case>-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Pengfei</first><last>Yu</last></author>
      <author><first>Hongwei</first><last>Wang</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Xudong</first><last>Lin</last></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Iris</first><last>Liu</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Darryl</first><last>Hannan</last></author>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Hyounghun</first><last>Kim</last></author>
      <author><first>Rotem</first><last>Dror</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Michael</first><last>Regan</last></author>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Charles</first><last>Yu</last></author>
      <author><first>Carl</first><last>Edwards</last></author>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Yizhu</first><last>Jiao</last></author>
      <author><first>Ghazaleh</first><last>Kazeminejad</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Carl</first><last>Vondrick</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>54-63</pages>
      <abstract>We introduce RESIN-11, a new schema-guided event extraction&amp;prediction framework that can be applied to a large variety of newsworthy scenarios. The framework consists of two parts: (1) an open-domain end-to-end multimedia multilingual information extraction system with weak-supervision and zero-shot learningbased techniques. (2) schema matching and schema-guided event prediction based on our curated schema library. We build a demo website based on our dockerized system and schema library publicly available for installation (https://github.com/RESIN-KAIROS/RESIN-11). We also include a video demonstrating the system.</abstract>
      <url hash="abe4dd6a">2022.naacl-demo.7</url>
      <bibkey>du-etal-2022-resin</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.7</doi>
      <pwccode url="https://github.com/resin-kairos/resin-11" additional="false">resin-kairos/resin-11</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/m2e2">M2E2</pwcdataset>
    </paper>
    <paper id="8">
      <title>A Human-machine Interface for Few-shot Rule Synthesis for Information Extraction</title>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>George C.G.</first><last>Barbosa</last></author>
      <author><first>Enrique</first><last>Noriega-Atala</last></author>
      <author><first>Gus</first><last>Hahn-Powell</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <author><first>Marco A.</first><last>Valenzuela-Escárcega</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>64-70</pages>
      <abstract>We propose a system that assists a user in constructing transparent information extraction models, consisting of patterns (or rules) written in a declarative language, through program synthesis.Users of our system can specify their requirements through the use of examples,which are collected with a search interface.The rule-synthesis system proposes rule candidates and the results of applying them on a textual corpus; the user has the option to accept the candidate, request another option, or adjust the examples provided to the system.Through an interactive evaluation, we show that our approach generates high-precision rules even in a 1-shot setting. On a second evaluation on a widely-used relation extraction dataset (TACRED), our method generates rules that outperform considerably manually written patterns.Our code, demo, and documentation is available at https://clulab.github.io/odinsynth.</abstract>
      <url hash="bd46514c">2022.naacl-demo.8</url>
      <bibkey>vacareanu-etal-2022-human</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.8</doi>
      <video href="2022.naacl-demo.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>SETS</fixed-case>um: Summarization and Visualization of Student Evaluations of Teaching</title>
      <author><first>Yinuo</first><last>Hu</last></author>
      <author><first>Shiyue</first><last>Zhang</last></author>
      <author><first>Viji</first><last>Sathy</last></author>
      <author><first>Abigail</first><last>Panter</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>71-89</pages>
      <abstract>Student Evaluations of Teaching (SETs) are widely used in colleges and universities. Typically SET results are summarized for instructors in a static PDF report. The report often includes summary statistics for quantitative ratings and an unsorted list of open-ended student comments. The lack of organization and summarization of the raw comments hinders those interpreting the reports from fully utilizing informative feedback, making accurate inferences, and designing appropriate instructional improvements. In this work, we introduce a novel system, SETSUM, that leverages sentiment analysis, aspect extraction, summarization, and visualization techniques to provide organized illustrations of SET findings to instructors and other reviewers. Ten university professors from diverse departments serve as evaluators of the system and all agree that SETSUM help them interpret SET results more efficiently; and 6 out of 10 instructors prefer our system over the standard static PDF report (while the remaining 4 would like to have both). This demonstrates that our work holds the potential of reforming the SET reporting conventions in the future.</abstract>
      <url hash="d7bc554b">2022.naacl-demo.9</url>
      <bibkey>hu-etal-2022-setsum</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.9</doi>
      <video href="2022.naacl-demo.9.mp4"/>
      <pwccode url="https://github.com/evahuyn/setsum" additional="false">evahuyn/setsum</pwccode>
    </paper>
    <paper id="10">
      <title>Towards Open-Domain Topic Classification</title>
      <author><first>Hantian</first><last>Ding</last></author>
      <author><first>Jinrui</first><last>Yang</last></author>
      <author><first>Yuqian</first><last>Deng</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>90-98</pages>
      <abstract>We introduce an open-domain topic classification system that accepts user-defined taxonomy in real time. Users will be able to classify a text snippet with respect to any candidate labels they want, and get instant response from our web interface. To obtain such flexibility, we build the backend model in a zero-shot way. By training on a new dataset constructed from Wikipedia, our label-aware text classifier can effectively utilize implicit knowledge in the pretrained language model to handle labels it has never seen before. We evaluate our model across four datasets from various domains with different label sets. Experiments show that the model significantly improves over existing zero-shot baselines in open-domain scenarios, and performs competitively with weakly-supervised models trained on in-domain data.</abstract>
      <url hash="34b0a0a6">2022.naacl-demo.10</url>
      <bibkey>ding-etal-2022-towards-open</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.10</doi>
      <video href="2022.naacl-demo.10.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="11">
      <title><fixed-case>S</fixed-case>ent<fixed-case>S</fixed-case>pace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features</title>
      <author><first>Greta</first><last>Tuckute</last></author>
      <author><first>Aalok</first><last>Sathe</last></author>
      <author><first>Mingye</first><last>Wang</last></author>
      <author><first>Harley</first><last>Yoder</last></author>
      <author><first>Cory</first><last>Shain</last></author>
      <author><first>Evelina</first><last>Fedorenko</last></author>
      <pages>99-113</pages>
      <abstract>SentSpace is a modular framework for streamlined evaluation of text. SentSpacecharacterizes textual input using diverse lexical, syntactic, and semantic features derivedfrom corpora and psycholinguistic experiments. Core sentence features fall into three primaryfeature spaces: 1) Lexical, 2) Contextual, and 3) Embeddings. To aid in the analysis of computed features, SentSpace provides a web interface for interactive visualization and comparison with text from large corpora. The modular design of SentSpace allows researchersto easily integrate their own feature computation into the pipeline while benefiting from acommon framework for evaluation and visualization. In this manuscript we will describe thedesign of SentSpace, its core feature spaces, and demonstrate an example use case by comparing human-written and machine-generated (GPT2-XL) sentences to each other. We findthat while GPT2-XL-generated text appears fluent at the surface level, psycholinguistic normsand measures of syntactic processing reveal key differences between text produced by humansand machines. Thus, SentSpace provides a broad set of cognitively motivated linguisticfeatures for evaluation of text within natural language processing, cognitive science, as wellas the social sciences.</abstract>
      <url hash="6f9f5c4d">2022.naacl-demo.11</url>
      <bibkey>tuckute-etal-2022-sentspace</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>P</fixed-case>addle<fixed-case>S</fixed-case>peech: An Easy-to-Use All-in-One Speech Toolkit</title>
      <author><first>Hui</first><last>Zhang</last></author>
      <author><first>Tian</first><last>Yuan</last></author>
      <author><first>Junkun</first><last>Chen</last></author>
      <author><first>Xintong</first><last>Li</last></author>
      <author><first>Renjie</first><last>Zheng</last></author>
      <author><first>Yuxin</first><last>Huang</last></author>
      <author><first>Xiaojie</first><last>Chen</last></author>
      <author><first>Enlei</first><last>Gong</last></author>
      <author><first>Zeyu</first><last>Chen</last></author>
      <author><first>Xiaoguang</first><last>Hu</last></author>
      <author><first>Dianhai</first><last>Yu</last></author>
      <author><first>Yanjun</first><last>Ma</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <pages>114-123</pages>
      <abstract>PaddleSpeech is an open-source all-in-one speech toolkit. It aims at facilitating the development and research of speech processing technologies by providing an easy-to-use command-line interface and a simple code structure. This paper describes the design philosophy and core architecture of PaddleSpeech to support several essential speech-to-text and text-to-speech tasks. PaddleSpeech achieves competitive or state-of-the-art performance on various speech datasets and implements the most popular methods. It also provides recipes and pretrained models to quickly reproduce the experimental results in this paper. PaddleSpeech is publicly avaiable at https://github.com/PaddlePaddle/PaddleSpeech.</abstract>
      <url hash="434ed40b">2022.naacl-demo.12</url>
      <bibkey>zhang-etal-2022-paddlespeech</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.12</doi>
      <video href="2022.naacl-demo.12.mp4"/>
      <pwccode url="https://github.com/PaddlePaddle/PaddleSpeech" additional="true">PaddlePaddle/PaddleSpeech</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-3">AISHELL-3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/audioset">AudioSet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/esc-50">ESC-50</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ljspeech">LJSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb1">VoxCeleb1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/voxceleb2">VoxCeleb2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wenetspeech">WenetSpeech</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>D</fixed-case>adma<fixed-case>T</fixed-case>ools: Natural Language Processing Toolkit for <fixed-case>P</fixed-case>ersian Language</title>
      <author><first>Romina</first><last>Etezadi</last></author>
      <author><first>Mohammad</first><last>Karrabi</last></author>
      <author><first>Najmeh</first><last>Zare</last></author>
      <author><first>Mohamad Bagher</first><last>Sajadi</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>124-130</pages>
      <abstract>We introduce DadmaTools, an open-source Python Natural Language Processing toolkit for the Persian language. The toolkit is a neural pipeline based on spaCy for several text processing tasks, including normalization, tokenization, lemmatization, part-of-speech, dependency parsing, constituency parsing, chunking, and ezafe detecting. DadmaTools relies on fine-tuning of ParsBERT using the PerDT dataset for most of the tasks. Dataset module and embedding module are included in DadmaTools that support different Persian datasets, embeddings, and commonly used functions for them. Our evaluations show that DadmaTools can attain state-of-the-art performance on multiple NLP tasks. The source code is freely available at https://github.com/Dadmatech/DadmaTools.</abstract>
      <url hash="6ad8dc62">2022.naacl-demo.13</url>
      <bibkey>etezadi-etal-2022-dadmatools</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.13</doi>
      <video href="2022.naacl-demo.13.mp4"/>
      <pwccode url="https://github.com/dadmatech/dadmatools" additional="false">dadmatech/dadmatools</pwccode>
    </paper>
    <paper id="14">
      <title><fixed-case>FAMIE</fixed-case>: A Fast Active Learning Framework for Multilingual Information Extraction</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Nghia</first><last>Ngo</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>131-139</pages>
      <abstract>This paper presents FAMIE, a comprehensive and efficient active learning (AL) toolkit for multilingual information extraction. FAMIE is designed to address a fundamental problem in existing AL frameworks where annotators need to wait for a long time between annotation batches due to the time-consuming nature of model training and data selection at each AL iteration. This hinders the engagement, productivity, and efficiency of annotators. Based on the idea of using a small proxy network for fast data selection, we introduce a novel knowledge distillation mechanism to synchronize the proxy network with the main large model (i.e., BERT-based) to ensure the appropriateness of the selected annotation examples for the main model. Our AL framework can support multiple languages. The experiments demonstrate the advantages of FAMIE in terms of competitive performance and time efficiency for sequence labeling with AL. We publicly release our code (https://github.com/nlp-uoregon/famie) and demo website (http://nlp.uoregon.edu:9000/). A demo video for FAMIE is provided at: https://youtu.be/I2i8n_jAyrY</abstract>
      <url hash="dee3dc0f">2022.naacl-demo.14</url>
      <bibkey>nguyen-etal-2022-famie</bibkey>
      <doi>10.18653/v1/2022.naacl-demo.14</doi>
      <video href="2022.naacl-demo.14.mp4"/>
      <pwccode url="https://github.com/nlp-uoregon/famie" additional="false">nlp-uoregon/famie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2022-06-28">
    <meta>
      <booktitle>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts</booktitle>
      <editor><first>Miguel</first><last>Ballesteros</last></editor>
      <editor><first>Yulia</first><last>Tsvetkov</last></editor>
      <editor><first>Cecilia O.</first><last>Alm</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, United States</address>
      <month>July</month>
      <year>2022</year>
      <url hash="b80dea0e">2022.naacl-tutorials</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="056bdfe6">2022.naacl-tutorials.0</url>
      <bibkey>naacl-2022-2022</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Text Generation with Text-Editing Models</title>
      <author><first>Eric</first><last>Malmi</last></author>
      <author><first>Yue</first><last>Dong</last></author>
      <author><first>Jonathan</first><last>Mallinson</last></author>
      <author><first>Aleksandr</first><last>Chuklin</last></author>
      <author><first>Jakub</first><last>Adamek</last></author>
      <author><first>Daniil</first><last>Mirylenka</last></author>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Sebastian</first><last>Krause</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <author><first>Aliaksei</first><last>Severyn</last></author>
      <pages>1-7</pages>
      <abstract>Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, text simplification, and style transfer. These tasks share a common trait – they exhibit a large amount of textual overlap between the source and target texts. Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs. This tutorial provides a comprehensive overview of the text-edit based models and current state-of-the-art approaches analyzing their pros and cons. We discuss challenges related to deployment and how these models help to mitigate hallucination and bias, both pressing challenges in the field of text generation.</abstract>
      <url hash="4d9cc505">2022.naacl-tutorials.1</url>
      <bibkey>malmi-etal-2022-text</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.1</doi>
    </paper>
    <paper id="2">
      <title>Self-supervised Representation Learning for Speech Processing</title>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <author><first>Tara</first><last>Sainath</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Shu-wen</first><last>Yang</last></author>
      <author><first>Katrin</first><last>Kirchhoff</last></author>
      <pages>8-13</pages>
      <abstract>There is a trend in the machine learning community to adopt self-supervised approaches to pre-train deep networks. Self-supervised representation learning (SSL) utilizes proxy supervised learning tasks, for example, distinguishing parts of the input signal from distractors, or generating masked input segments conditioned on the unmasked ones, to obtain training data from unlabeled corpora. BERT and GPT in NLP and SimCLR and BYOL in CV are famous examples in this direction. These approaches make it possible to use a tremendous amount of unlabeled data available on the web to train large networks and solve complicated tasks. Thus, SSL has the potential to scale up current machine learning technologies, especially for low-resourced, under-represented use cases, and democratize the technologies. Recently self-supervised approaches for speech processing are also gaining popularity. There are several workshops in relevant topics hosted at ICML 2020 (https://icml-sas.gitlab.io/), NeurIPS 2020 (https://neurips-sas-2020.github.io/), and AAAI 2022 (https://aaai-sas-2022.github.io/). However, there is no previous tutorial about a similar topic based on the authors’ best knowledge. Due to the growing popularity of SSL, and the shared mission of the areas in bringing speech and language technologies to more use cases with better quality and scaling the technologies for under-represented languages, we propose this tutorial to systematically survey the latest SSL techniques, tools, datasets, and performance achievement in speech processing. The proposed tutorial is highly relevant to the special theme of ACL about language diversity. One of the main focuses of the tutorial is leveraging SSL to reduce the dependence of speech technologies on labeled data, and to scale up the technologies especially for under-represented languages and use cases.</abstract>
      <url hash="06a41104">2022.naacl-tutorials.2</url>
      <bibkey>lee-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.2</doi>
      <pwccode url="https://github.com/s3prl/s3prl" additional="false">s3prl/s3prl</pwccode>
    </paper>
    <paper id="3">
      <title>New Frontiers of Information Extraction</title>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>14-25</pages>
      <abstract>This tutorial targets researchers and practitioners who are interested in AI and ML technologies for structural information extraction (IE) from unstructured textual sources. Particularly, this tutorial will provide audience with a systematic introduction to recent advances of IE, by answering several important research questions. These questions include (i) how to develop an robust IE system from noisy, insufficient training data, while ensuring the reliability of its prediction? (ii) how to foster the generalizability of IE through enhancing the system’s cross-lingual, cross-domain, cross-task and cross-modal transferability? (iii) how to precisely support extracting structural information with extremely fine-grained, diverse and boundless labels? (iv) how to further improve IE by leveraging indirect supervision from other NLP tasks, such as NLI, QA or summarization, and pre-trained language models? (v) how to acquire knowledge to guide the inference of IE systems? We will discuss several lines of frontier research that tackle those challenges, and will conclude the tutorial by outlining directions for further investigation.</abstract>
      <url hash="7b5b3e2d">2022.naacl-tutorials.3</url>
      <bibkey>chen-etal-2022-new</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
    </paper>
    <paper id="4">
      <title>Human-Centered Evaluation of Explanations</title>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Samuel</first><last>Carton</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Q. Vera</first><last>Liao</last></author>
      <author><first>Tania</first><last>Lombrozo</last></author>
      <author><first>Alison</first><last>Smith-Renner</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>26-32</pages>
      <abstract>The NLP community are increasingly interested in providing explanations for NLP models to help people make sense of model behavior and potentially improve human interaction with models. In addition to computational challenges in generating these explanations, evaluations of the generated explanations require human-centered perspectives and approaches. This tutorial will provide an overview of human-centered evaluations of explanations. First, we will give a brief introduction to the psychological foundation of explanations as well as types of NLP model explanations and their corresponding presentation, to provide the necessary background. We will then present a taxonomy of human-centered evaluation of explanations and dive into depth in the two categories: 1) evaluation based on human-annotated explanations; 2) evaluation with human-subjects studies. We will conclude by discussing future directions. We will also adopt a flipped format to maximize the in- teractive components for the live audience.</abstract>
      <url hash="65505c52">2022.naacl-tutorials.4</url>
      <bibkey>boyd-graber-etal-2022-human</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.4</doi>
    </paper>
    <paper id="5">
      <title>Tutorial on Multimodal Machine Learning</title>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <author><first>Paul Pu</first><last>Liang</last></author>
      <author><first>Amir</first><last>Zadeh</last></author>
      <pages>33-38</pages>
      <abstract>Multimodal machine learning involves integrating and modeling information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, HCI, and healthcare. This tutorial, building upon a new edition of a survey paper on multimodal ML as well as previously-given tutorials and academic courses, will describe an updated taxonomy on multimodal machine learning synthesizing its core technical challenges and major directions for future research.</abstract>
      <url hash="6052e7c9">2022.naacl-tutorials.5</url>
      <bibkey>morency-etal-2022-tutorial</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="6">
      <title>Contrastive Data and Learning for Natural Language Processing</title>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <pages>39-47</pages>
      <abstract>Current NLP models heavily rely on effective representation learning algorithms. Contrastive learning is one such technique to learn an embedding space such that similar data sample pairs have close representations while dissimilar samples stay far apart from each other. It can be used in supervised or unsupervised settings using different loss functions to produce task-specific or general-purpose representations. While it has originally enabled the success for vision tasks, recent years have seen a growing number of publications in contrastive NLP. This first line of works not only delivers promising performance improvements in various NLP tasks, but also provides desired characteristics such as task-agnostic sentence representation, faithful text generation, data-efficient learning in zero-shot and few-shot settings, interpretability and explainability. In this tutorial, we aim to provide a gentle introduction to the fundamentals of contrastive learning approaches and the theory behind them. We then survey the benefits and the best practices of contrastive learning for various downstream NLP applications including Text Classification, Question Answering, Summarization, Text Generation, Interpretability and Explainability, Commonsense Knowledge and Reasoning, Vision-and-Language.This tutorial intends to help researchers in the NLP and computational linguistics community to understand this emerging topic and promote future research directions of using contrastive learning for NLP applications.</abstract>
      <url hash="46b089ea">2022.naacl-tutorials.6</url>
      <bibkey>zhang-etal-2022-contrastive-data</bibkey>
      <doi>10.18653/v1/2022.naacl-tutorials.6</doi>
    </paper>
  </volume>
  <volume id="industry" ingest-date="2022-06-28">
    <meta>
      <booktitle>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track</booktitle>
      <editor><first>Anastassia</first><last>Loukina</last></editor>
      <editor><first>Rashmi</first><last>Gangadharaiah</last></editor>
      <editor><first>Bonan</first><last>Min</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Hybrid: Seattle, Washington + Online</address>
      <month>July</month>
      <year>2022</year>
      <url hash="b5f7d586">2022.naacl-industry</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="72112e02">2022.naacl-industry.0</url>
      <bibkey>naacl-2022-naacl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational <fixed-case>AI</fixed-case> Systems</title>
      <author><first>Mohammad</first><last>Kachuee</last></author>
      <author><first>Jinseok</first><last>Nam</last></author>
      <author><first>Sarthak</first><last>Ahuja</last></author>
      <author><first>Jin-Myung</first><last>Won</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <pages>1-8</pages>
      <abstract>Skill routing is an important component in large-scale conversational systems. In contrast to traditional rule-based skill routing, state-of-the-art systems use a model-based approach to enable natural conversations. To provide supervision signal required to train such models, ideas such as human annotation, replication of a rule-based system, relabeling based on user paraphrases, and bandit-based learning were suggested. However, these approaches: (a) do not scale in terms of the number of skills and skill on-boarding, (b) require a very costly expert annotation/rule-design, (c) introduce risks in the user experience with each model update. In this paper, we present a scalable self-learning approach to explore routing alternatives without causing abrupt policy changes that break the user experience, learn from the user interaction, and incrementally improve the routing via frequent model refreshes. To enable such robust frequent model updates, we suggest a simple and effective approach that ensures controlled policy updates for individual domains, followed by an off-policy evaluation for making deployment decisions without any need for lengthy A/B experimentation. We conduct various offline and online A/B experiments on a commercial large-scale conversational system to demonstrate the effectiveness of the proposed method in real-world production settings.</abstract>
      <url hash="98471458">2022.naacl-industry.1</url>
      <bibkey>kachuee-etal-2022-scalable</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.1</doi>
      <video href="2022.naacl-industry.1.mp4"/>
    </paper>
    <paper id="2">
      <title><fixed-case>CREATER</fixed-case>: <fixed-case>CTR</fixed-case>-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning</title>
      <author><first>Penghui</first><last>Wei</last></author>
      <author><first>Xuanhua</first><last>Yang</last></author>
      <author><first>ShaoGuo</first><last>Liu</last></author>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Bo</first><last>Zheng</last></author>
      <pages>9-17</pages>
      <abstract>This paper focuses on automatically generating the text of an ad, and the goal is that the generated text can capture user interest for achieving higher click-through rate (CTR). We propose CREATER, a CTR-driven advertising text generation approach, to generate ad texts based on high-quality user reviews. To incorporate CTR objective, our model learns from online A/B test data with contrastive learning, which encourages the model to generate ad texts that obtain higher CTR. To make use of large-scale unpaired reviews, we design a customized self-supervised objective reducing the gap between pre-training and fine-tuning. Experiments on industrial datasets show that CREATER significantly outperforms current approaches. It has been deployed online in a leading advertising platform and brings uplift on core online metrics.</abstract>
      <url hash="eea50163">2022.naacl-industry.2</url>
      <bibkey>wei-etal-2022-creater</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.2</doi>
      <video href="2022.naacl-industry.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Augmenting Poetry Composition with <fixed-case>V</fixed-case>erse by <fixed-case>V</fixed-case>erse</title>
      <author><first>David</first><last>Uthus</last></author>
      <author><first>Maria</first><last>Voitovich</last></author>
      <author><first>R.J.</first><last>Mical</last></author>
      <pages>18-26</pages>
      <abstract>We describe Verse by Verse, our experiment in augmenting the creative process of writing poetry with an AI. We have created a group of AI poets, styled after various American classic poets, that are able to offer as suggestions generated lines of verse while a user is composing a poem. In this paper, we describe the underlying system to offer these suggestions. This includes a generative model, which is tasked with generating a large corpus of lines of verse offline and which are then stored in an index, and a dual-encoder model that is tasked with recommending the next possible set of verses from our index given the previous line of verse.</abstract>
      <url hash="2235db9a">2022.naacl-industry.3</url>
      <bibkey>uthus-etal-2022-augmenting</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.3</doi>
      <video href="2022.naacl-industry.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>AB</fixed-case>/<fixed-case>BA</fixed-case> analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy</title>
      <author><first>Raphael</first><last>Petegrosso</last></author>
      <author><first>VasistaKrishna</first><last>Baderdinnni</last></author>
      <author><first>Thibaud</first><last>Senechal</last></author>
      <author><first>Benjamin</first><last>Bullough</last></author>
      <pages>27-36</pages>
      <abstract>Evaluation of keyword spotting (KWS) systems that detect keywords in speech is a challenging task under realistic privacy constraints. The KWS is designed to only collect data when the keyword is present, limiting the availability of hard samples that may contain false negatives, and preventing direct estimation of model recall from production data. Alternatively, complementary data collected from other sources may not be fully representative of the real application. In this work, we propose an evaluation technique which we call AB/BA analysis. Our framework evaluates a candidate KWS model B against a baseline model A, using cross-dataset offline decoding for relative recall estimation, without requiring negative examples. Moreover, we propose a formulation with assumptions that allow estimation of relative false positive rate between models with low variance even when the number of false positives is small. Finally, we propose to leverage machine-generated soft labels, in a technique we call Semi-Supervised AB/BA analysis, that improves the analysis time, privacy, and cost. Experiments with both simulation and real data show that AB/BA analysis is successful at measuring recall improvement in conjunction with the trade-off in relative false positive rate.</abstract>
      <url hash="bdc84369">2022.naacl-industry.4</url>
      <bibkey>petegrosso-etal-2022-ab</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.4</doi>
      <video href="2022.naacl-industry.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Temporal Generalization for Spoken Language Understanding</title>
      <author><first>Judith</first><last>Gaspers</last></author>
      <author><first>Anoop</first><last>Kumar</last></author>
      <author><first>Greg</first><last>Ver Steeg</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>37-44</pages>
      <abstract>Spoken Language Understanding (SLU) models in industry applications are usually trained offline on historic data, but have to perform well on incoming user requests after deployment. Since the application data is not available at training time, this is formally similar to the domain generalization problem, where domains correspond to different temporal segments of the data, and the goal is to build a model that performs well on unseen domains, e.g., upcoming data. In this paper, we explore different strategies for achieving good temporal generalization, including instance weighting, temporal fine-tuning, learning temporal features and building a temporally-invariant model. Our results on data of large-scale SLU systems show that temporal information can be leveraged to improve temporal generalization for SLU models.</abstract>
      <url hash="c34842ac">2022.naacl-industry.5</url>
      <bibkey>gaspers-etal-2022-temporal</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.5</doi>
      <video href="2022.naacl-industry.5.mp4"/>
    </paper>
    <paper id="6">
      <title>An End-to-End Dialogue Summarization System for Sales Calls</title>
      <author><first>Abedelkadir</first><last>Asi</last></author>
      <author><first>Song</first><last>Wang</last></author>
      <author><first>Roy</first><last>Eisenstadt</last></author>
      <author><first>Dean</first><last>Geckt</last></author>
      <author><first>Yarin</first><last>Kuper</last></author>
      <author><first>Yi</first><last>Mao</last></author>
      <author><first>Royi</first><last>Ronen</last></author>
      <pages>45-53</pages>
      <abstract>Summarizing sales calls is a routine task performed manually by salespeople. We present a production system which combines generative models fine-tuned for customer-agent setting, with a human-in-the-loop user experience for an interactive summary curation process. We address challenging aspects of dialogue summarization task in a real-world setting including long input dialogues, content validation, lack of labeled data and quality evaluation. We show how GPT-3 can be leveraged as an offline data labeler to handle training data scarcity and accommodate privacy constraints in an industrial setting. Experiments show significant improvements by our models in tackling the summarization and content validation tasks on public datasets.</abstract>
      <url hash="00306085">2022.naacl-industry.6</url>
      <bibkey>asi-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.6</doi>
      <video href="2022.naacl-industry.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogsum">DialogSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="7">
      <title>Controlled Data Generation via Insertion Operations for <fixed-case>NLU</fixed-case></title>
      <author><first>Manoj</first><last>Kumar</last></author>
      <author><first>Yuval</first><last>Merhav</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <pages>54-61</pages>
      <abstract>Use of synthetic data is rapidly emerging as a realistic alternative to manually annotating live traffic for industry-scale model building. Manual data annotation is slow, expensive and not preferred for meeting customer privacy expectations. Further, commercial natural language applications are required to support continuously evolving features as well as newly added experiences. To address these requirements, we propose a targeted synthetic data generation technique by inserting tokens into a given semantic signature. The generated data are used as additional training samples in the tasks of intent classification and named entity recognition. We evaluate on a real-world voice assistant dataset, and using only 33% of the available training set, we achieve the same accuracy as training with all available data. Further, we analyze the effects of data generation across varied real-world applications and propose heuristics that improve the task performance further.</abstract>
      <url hash="5a1d7102">2022.naacl-industry.7</url>
      <bibkey>kumar-etal-2022-controlled</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.7</doi>
      <video href="2022.naacl-industry.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Easy and Efficient Transformer: Scalable Inference Solution For Large <fixed-case>NLP</fixed-case> Model</title>
      <author><first>Gongzheng</first><last>Li</last></author>
      <author><first>Yadong</first><last>Xi</last></author>
      <author><first>Jingzhen</first><last>Ding</last></author>
      <author><first>Duan</first><last>Wang</last></author>
      <author><first>Ziyang</first><last>Luo</last></author>
      <author><first>Rongsheng</first><last>Zhang</last></author>
      <author><first>Bai</first><last>Liu</last></author>
      <author><first>Changjie</first><last>Fan</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Zeng</first><last>Zhao</last></author>
      <pages>62-68</pages>
      <abstract>Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: <b>Easy and Efficient Transformer (EET)</b>, including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU.</abstract>
      <url hash="7d9a8de2">2022.naacl-industry.8</url>
      <bibkey>li-etal-2022-easy</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.8</doi>
      <video href="2022.naacl-industry.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Aspect-based Analysis of Advertising Appeals for Search Engine Advertising</title>
      <author><first>Soichiro</first><last>Murakami</last></author>
      <author><first>Peinan</first><last>Zhang</last></author>
      <author><first>Sho</first><last>Hoshino</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>69-78</pages>
      <abstract>Writing an ad text that attracts people and persuades them to click or act is essential for the success of search engine advertising. Therefore, ad creators must consider various aspects of advertising appeals (A<tex-math>^3</tex-math>) such as the price, product features, and quality. However, products and services exhibit unique effective A<tex-math>^3</tex-math> for different industries. In this work, we focus on exploring the effective A<tex-math>^3</tex-math> for different industries with the aim of assisting the ad creation process. To this end, we created a dataset of advertising appeals and used an existing model that detects various aspects for ad texts. Our experiments demonstrated %through correlation analysis that different industries have their own effective A<tex-math>^3</tex-math> and that the identification of the A<tex-math>^3</tex-math> contributes to the estimation of advertising performance. </abstract>
      <url hash="a2ec38f9">2022.naacl-industry.9</url>
      <bibkey>murakami-etal-2022-aspect</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.9</doi>
      <video href="2022.naacl-industry.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Self-supervised Product Title Rewrite for Product Listing Ads</title>
      <author><first>Xue</first><last>Zhao</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Junwei</first><last>Ding</last></author>
      <author><first>Liang</first><last>Yao</last></author>
      <author><first>Mahone</first><last>Yan</last></author>
      <author><first>Huibo</first><last>Wang</last></author>
      <author><first>Wenqing</first><last>Yao</last></author>
      <pages>79-85</pages>
      <abstract>Product Listing Ads (PLAs) are primary online advertisements merchants pay to attract more customers. However, merchants prefer to stack various attributes to the title and neglect the fluency and information priority. These seller-created titles are not suitable for PLAs as they fail to highlight the core information in the visible part in PLAs titles. In this work, we present a title rewrite solution. Specifically, we train a self-supervised language model to generate high-quality titles in terms of fluency and information priority. Extensive offline test and real-world online test have demonstrated that our solution is effective in reducing the cost and gaining more profit as it lowers our CPC, CPB while improving CTR in the online test by a large amount.</abstract>
      <url hash="25eaf32a">2022.naacl-industry.10</url>
      <bibkey>zhao-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.10</doi>
      <video href="2022.naacl-industry.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Efficient Semi-supervised Consistency Training for Natural Language Understanding</title>
      <author><first>George</first><last>Leung</last></author>
      <author><first>Joshua</first><last>Tan</last></author>
      <pages>86-93</pages>
      <abstract>Manually labeled training data is expensive, noisy, and often scarce, such as when developing new features or localizing existing features for a new region. In cases where labeled data is limited but unlabeled data is abundant, semi-supervised learning methods such as consistency training can be used to improve model performance, by training models to output consistent predictions between original and augmented versions of unlabeled data.In this work, we explore different data augmentation methods for consistency training (CT) on Natural Language Understanding (NLU) domain classification (DC) in the limited labeled data regime. We explore three types of augmentation techniques (human paraphrasing, back-translation, and dropout) for unlabeled data and train DC models to jointly minimize both the supervised loss and the consistency loss on unlabeled data. Our results demonstrate that DC models trained with CT methods and dropout based augmentation on only 0.1% (2,998 instances) of labeled data with the remainder as unlabeled can achieve a top-1 relative accuracy reduction of 12.25% compared to fully supervised model trained with 100% of labeled data, outperforming fully supervised models trained on 10x that amount of labeled data. The dropout-based augmentation achieves similar performance compare to back-translation based augmentation with much less computational resources. This paves the way for applications of using large scale unlabeled data for semi-supervised learning in production NLU systems.</abstract>
      <url hash="fac24315">2022.naacl-industry.11</url>
      <bibkey>leung-tan-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.11</doi>
      <video href="2022.naacl-industry.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Distantly Supervised Aspect Clustering And Naming For <fixed-case>E</fixed-case>-Commerce Reviews</title>
      <author><first>Prateek</first><last>Sircar</last></author>
      <author><first>Aniket</first><last>Chakrabarti</last></author>
      <author><first>Deepak</first><last>Gupta</last></author>
      <author><first>Anirban</first><last>Majumdar</last></author>
      <pages>94-102</pages>
      <abstract>Product aspect extraction from reviews is a critical task for e-commerce services to understand customer preferences and pain points. While aspect phrases extraction and sentiment analysis have received a lot of attention, clustering of aspect phrases and assigning human readable names to clusters in e-commerce reviews is an extremely important and challenging problem due to the scale of the reviews that makes human review infeasible. In this paper, we propose fully automated methods for clustering aspect words and generating human readable names for the clusters without any manually labeled data. We train transformer based sentence embeddings that are aware of unique e-commerce language characteristics (eg. incomplete sentences, spelling and grammar errors, vernacular etc.). We also train transformer based sequence to sequence models to generate human readable aspect names from clusters. Both the models are trained using heuristic based distant supervision. Additionally, the models are used to improve each other. Extensive empirical testing showed that the clustering model improves the Silhouette Score by 64% when compared to the state-of-the-art baseline and the aspect naming model achieves a high ROUGE-L score of 0.79.</abstract>
      <url hash="a2eeb8cc">2022.naacl-industry.12</url>
      <bibkey>sircar-etal-2022-distantly</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.12</doi>
      <video href="2022.naacl-industry.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Local-to-global learning for iterative training of production <fixed-case>SLU</fixed-case> models on new features</title>
      <author><first>Yulia</first><last>Grishina</last></author>
      <author><first>Daniil</first><last>Sorokin</last></author>
      <pages>103-111</pages>
      <abstract>In production SLU systems, new training data becomes available with time so that ML models need to be updated on a regular basis. Specifically, releasing new features adds new classes of data while the old data remains constant. However, retraining the full model each time from scratch is computationally expensive. To address this problem, we propose to consider production releases from the curriculum learning perspective and to adapt the local-to-global learning (LGL) schedule (Cheng et. al, 2019) for a statistical model that starts with fewer output classes and adds more classes with each iteration. We report experiments for the tasks of intent classification and slot filling in the context of a production voice-assistant. First, we apply the original LGL schedule on our data and then adapt LGL to the production setting where the full data is not available at initial training iterations. We demonstrate that our method improves model error rates by 7.3% and saves up to 25% training time for individual iterations.</abstract>
      <url hash="bce801a1">2022.naacl-industry.13</url>
      <bibkey>grishina-sorokin-2022-local</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.13</doi>
      <video href="2022.naacl-industry.13.mp4"/>
    </paper>
    <paper id="14">
      <title><fixed-case>CULG</fixed-case>: Commercial Universal Language Generation</title>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Yameng</first><last>Huang</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>112-120</pages>
      <abstract>Pre-trained language models (PLMs) have dramatically improved performance for many natural language processing (NLP) tasks in domains such as finance and healthcare. However, the application of PLMs in the domain of commerce, especially marketing and advertising, remains less studied. In this work, we adapt pre-training methods to the domain of commerce, by proposing CULG, a large-scale commercial universal language generation model which is pre-trained on a corpus drawn from 10 markets across 7 languages. We propose 4 commercial generation tasks and a two-stage training strategy for pre-training, and demonstrate that the proposed strategy yields performance improvements on three generation tasks as compared to single-stage pre-training. Extensive experiments show that our model outperforms other models by a large margin on commercial generation tasks, and we conclude with a discussion on additional applications over other markets, languages, and tasks.</abstract>
      <url hash="f76f0776">2022.naacl-industry.14</url>
      <bibkey>li-etal-2022-culg</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.14</doi>
    </paper>
    <paper id="15">
      <title>Constraining word alignments with posterior regularization for label transfer</title>
      <author><first>Kevin</first><last>Jose</last></author>
      <author><first>Thomas</first><last>Gueudre</last></author>
      <pages>121-129</pages>
      <abstract>Unsupervised word alignments offer a lightweight and interpretable method to transfer labels from high- to low-resource languages, as long as semantically related words have the same label across languages. But such an assumption is often not true in industrial NLP pipelines, where multilingual annotation guidelines are complex and deviate from semantic consistency due to various factors (such as annotation difficulty, conflicting ontology, upcoming feature launches etc.);We address this difficulty by constraining the alignments models to remain consistent with both source and target annotation guidelines , leveraging posterior regularization and labeled examples. We illustrate the overall approach using IBM 2 (fast_align) as a base model, and report results on both internal and external annotated datasets. We measure consistent accuracy improvements on the MultiATIS++ dataset over AWESoME, a popular transformer-based alignment model, in the label projection task (<tex-math>+2.7\%</tex-math> at word-level and <tex-math>+15\%</tex-math> at sentence-level), and show how even a small amount of target language annotations help substantially.</abstract>
      <url hash="b9b4a34d">2022.naacl-industry.15</url>
      <bibkey>gueudre-jose-2022-constraining</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.15</doi>
      <video href="2022.naacl-industry.15.mp4"/>
      <pwccode url="https://github.com/amazon-research/fast_label" additional="false">amazon-research/fast_label</pwccode>
    </paper>
    <paper id="16">
      <title>Explaining the Effectiveness of Multi-Task Learning for Efficient Knowledge Extraction from Spine <fixed-case>MRI</fixed-case> Reports</title>
      <author><first>Arijit</first><last>Sehanobish</last></author>
      <author><first>McCullen</first><last>Sandora</last></author>
      <author><first>Nabila</first><last>Abraham</last></author>
      <author><first>Jayashri</first><last>Pawar</last></author>
      <author><first>Danielle</first><last>Torres</last></author>
      <author><first>Anasuya</first><last>Das</last></author>
      <author><first>Murray</first><last>Becker</last></author>
      <author><first>Richard</first><last>Herzog</last></author>
      <author><first>Benjamin</first><last>Odry</last></author>
      <author><first>Ron</first><last>Vianu</last></author>
      <pages>130-140</pages>
      <abstract>Pretrained Transformer based models finetuned on domain specific corpora have changed the landscape of NLP. However, training or fine-tuning these models for individual tasks can be time consuming and resource intensive. Thus, a lot of current research is focused on using transformers for multi-task learning (Raffel et al., 2020) and how to group the tasks to help a multi-task model to learn effective representations that can be shared across tasks (Standley et al., 2020; Fifty et al., 2021) . In this work, we show that a single multi-tasking model can match the performance of task specific model when the task specific models show similar representations across all of their hidden layers and their gradients are aligned, i.e. their gradients follow the same direction. We hypothesize that the above observations explain the effectiveness of multi-task learning. We validate our observations on our internal radiologist-annotated datasets on the cervical and lumbar spine. Our method is simple and intuitive, and can be used in a wide range of NLP problems.</abstract>
      <url hash="d2eea078">2022.naacl-industry.16</url>
      <bibkey>sehanobish-etal-2022-explaining</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.16</doi>
      <video href="2022.naacl-industry.16.mp4"/>
    </paper>
    <paper id="17">
      <title><fixed-case>FPI</fixed-case>: Failure Point Isolation in Large-scale Conversational Assistants</title>
      <author><first>Rinat</first><last>Khaziev</last></author>
      <author><first>Usman</first><last>Shahid</last></author>
      <author><first>Tobias</first><last>Röding</last></author>
      <author><first>Rakesh</first><last>Chada</last></author>
      <author><first>Emir</first><last>Kapanci</last></author>
      <author><first>Pradeep</first><last>Natarajan</last></author>
      <pages>141-148</pages>
      <abstract>Large-scale conversational assistants such as Cortana, Alexa, Google Assistant and Siri process requests through a series of modules for wake word detection, speech recognition, language understanding and response generation. An error in one of these modules can cascade through the system. Given the large traffic volumes in these assistants, it is infeasible to manually analyze the data, identify requests with processing errors and isolate the source of error. We present a machine learning system to address this challenge. First, we embed the incoming request and context, such as system response and subsequent turns, using pre-trained transformer models. Then, we combine these embeddings with encodings of additional metadata features (such as confidence scores from different modules in the online system) using a “mixing-encoder” to output the failure point predictions. Our system obtains 92.2% of human performance on this task while scaling to analyze the entire traffic in 8 different languages of a large-scale conversational assistant. We present detailed ablation studies analyzing the impact of different modeling choices.</abstract>
      <url hash="7fee073e">2022.naacl-industry.17</url>
      <bibkey>khaziev-etal-2022-fpi</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.17</doi>
      <video href="2022.naacl-industry.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Asynchronous Convergence in Multi-Task Learning via Knowledge Distillation from Converged Tasks</title>
      <author><first>Weiyi</first><last>Lu</last></author>
      <author><first>Sunny</first><last>Rajagopalan</last></author>
      <author><first>Priyanka</first><last>Nigam</last></author>
      <author><first>Jaspreet</first><last>Singh</last></author>
      <author><first>Xiaodi</first><last>Sun</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Belinda</first><last>Zeng</last></author>
      <author><first>Trishul</first><last>Chilimbi</last></author>
      <pages>149-159</pages>
      <abstract>Multi-task learning (MTL) aims to solve multiple tasks jointly by sharing a base representation among them. This can lead to more efficient learning and better generalization, as compared to learning each task individually. However, one issue that often arises in MTL is the convergence speed between tasks varies due to differences in task difficulty, so it can be a challenge to simultaneously achieve the best performance on all tasks with a single model checkpoint. Various techniques have been proposed to address discrepancies in task convergence rate, including weighting the per-task losses and modifying task gradients. In this work, we propose a novel approach that avoids the problem of requiring all tasks to converge at the same rate, but rather allows for “asynchronous” convergence among the tasks where each task can converge on its own schedule. As our main contribution, we monitor per-task validation metrics and switch to a knowledge distillation loss once a task has converged instead of continuing to train on the true labels. This prevents the model from overfitting on converged tasks while it learns the remaining tasks. We evaluate the proposed method in two 5-task MTL setups consisting of internal e-commerce datasets. The results show that our method consistently outperforms existing loss weighting and gradient balancing approaches, achieving average improvements of 0.9% and 1.5% over the best performing baseline model in the two setups, respectively.</abstract>
      <url hash="047bf17c">2022.naacl-industry.18</url>
      <bibkey>lu-etal-2022-asynchronous</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.18</doi>
      <video href="2022.naacl-industry.18.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="19">
      <title>Augmenting Training Data for Massive Semantic Matching Models in Low-Traffic <fixed-case>E</fixed-case>-commerce Stores</title>
      <author><first>Ashutosh</first><last>Joshi</last></author>
      <author><first>Shankar</first><last>Vishwanath</last></author>
      <author><first>Choon</first><last>Teo</last></author>
      <author><first>Vaclav</first><last>Petricek</last></author>
      <author><first>Vishy</first><last>Vishwanathan</last></author>
      <author><first>Rahul</first><last>Bhagat</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>160-167</pages>
      <abstract>Extreme multi-label classification (XMC) systems have been successfully applied in e-commerce (Shen et al., 2020; Dahiya et al., 2021) for retrieving products based on customer behavior. Such systems require large amounts of customer behavior data (e.g. queries, clicks, purchases) for training. However, behavioral data is limited in low-traffic e-commerce stores, impacting performance of these systems. In this paper, we present a technique that augments behavioral training data via query reformulation. We use the Aggregated Label eXtreme Multi-label Classification (AL-XMC) system (Shen et al., 2020) as an example semantic matching model and show via crowd-sourced human judgments that, when the training data is augmented through query reformulations, the quality of AL-XMC improves over a baseline that does not use query reformulation. We also show in online A/B tests that our method significantly improves business metrics for the AL-XMC model.</abstract>
      <url hash="bf36d620">2022.naacl-industry.19</url>
      <bibkey>joshi-etal-2022-augmenting</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.19</doi>
      <video href="2022.naacl-industry.19.mp4"/>
    </paper>
    <paper id="20">
      <title>Retrieval Based Response Letter Generation For a Customer Care Setting</title>
      <author><first>Biplob</first><last>Biswas</last></author>
      <author><first>Renhao</first><last>Cui</last></author>
      <author><first>Rajiv</first><last>Ramnath</last></author>
      <pages>168-175</pages>
      <abstract>Letter-like communications (such as email) are a major means of customer relationship management within customer-facing organizations. These communications are initiated on a channel by requests from customers and then responded to by the organization on the same channel. For decades, the job has almost entirely been conducted by human agents who attempt to provide the most appropriate reaction to the request. Rules have been made to standardize the overall customer service process and make sure the customers receive professional responses. Recent progress in natural language processing has made it possible to automate response generation. However, the diversity and open nature of customer queries and the lack of structured knowledge bases make this task even more challenging than typical task-oriented language generation tasks. Keeping those obstacles in mind, we propose a deep-learning based response letter generation framework that attempts to retrieve knowledge from historical responses and utilize it to generate an appropriate reply. Our model uses data augmentation to address the insufficiency of query-response pairs and employs a ranking mechanism to choose the best response from multiple potential options. We show that our technique outperforms the baselines by significant margins while producing consistent and informative responses.</abstract>
      <url hash="85819e7d">2022.naacl-industry.20</url>
      <bibkey>biswas-etal-2022-retrieval</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.20</doi>
      <video href="2022.naacl-industry.20.mp4"/>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>edical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning</title>
      <author><first>Angelo</first><last>Ziletti</last></author>
      <author><first>Alan</first><last>Akbik</last></author>
      <author><first>Christoph</first><last>Berns</last></author>
      <author><first>Thomas</first><last>Herold</last></author>
      <author><first>Marion</first><last>Legler</last></author>
      <author><first>Martina</first><last>Viell</last></author>
      <pages>176-187</pages>
      <abstract>Medical coding (MC) is an essential pre-requisite for reliable data retrieval and reporting. Given a free-text <i>reported term</i> (RT) such as “pain of right thigh to the knee”, the task is to identify the matching <i>lowest-level term</i> (LLT) –in this case “unilateral leg pain”– from a very large and continuously growing repository of standardized medical terms. However, automating this task is challenging due to a large number of LLT codes (as of writing over <tex-math>80\,000</tex-math>), limited availability of training data for long tail/emerging classes, and the general high accuracy demands of the medical domain.With this paper, we introduce the MC task, discuss its challenges, and present a novel approach called xTARS that combines traditional BERT-based classification with a recent zero/few-shot learning approach (TARS). We present extensive experiments that show that our combined approach outperforms strong baselines, especially in the few-shot regime. The approach is developed and deployed at Bayer, live since November 2021. As we believe our approach potentially promising beyond MC, and to ensure reproducibility, we release the code to the research community. </abstract>
      <url hash="2396e79a">2022.naacl-industry.21</url>
      <bibkey>ziletti-etal-2022-medical</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.21</doi>
      <video href="2022.naacl-industry.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Knowledge extraction from aeronautical messages (<fixed-case>NOTAM</fixed-case>s) with self-supervised language models for aircraft pilots</title>
      <author><first>Alexandre</first><last>Arnold</last></author>
      <author><first>Fares</first><last>Ernez</last></author>
      <author><first>Catherine</first><last>Kobus</last></author>
      <author><first>Marion-Cécile</first><last>Martin</last></author>
      <pages>188-196</pages>
      <abstract>During their pre-flight briefings, aircraft pilots must analyse a long list of NoTAMs (NOtice To AirMen) indicating potential hazards along the flight route, sometimes up to pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This self-supervised approach, where smaller amounts of labeled data are enough for task-specific fine-tuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models (by pilots, airlines or service providers), which is a first to the best of our knowledge.</abstract>
      <url hash="7367e44f">2022.naacl-industry.22</url>
      <bibkey>arnold-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.22</doi>
      <video href="2022.naacl-industry.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Intent Discovery for Enterprise Virtual Assistants: Applications of Utterance Embedding and Clustering to Intent Mining</title>
      <author><first>Minhua</first><last>Chen</last></author>
      <author><first>Badrinath</first><last>Jayakumar</last></author>
      <author><first>Michael</first><last>Johnston</last></author>
      <author><first>S. Eman</first><last>Mahmoodi</last></author>
      <author><first>Daniel</first><last>Pressel</last></author>
      <pages>197-208</pages>
      <abstract>A key challenge in the creation and refinement of virtual assistants is the ability to mine unlabeled utterance data to discover common intents. We develop an approach to this problem that combines large-scale pre-training and multi-task learning to derive a semantic embedding that can be leveraged to identify clusters of utterances that correspond to unhandled intents. An utterance encoder is first trained with a language modeling objective and subsequently adapted to predict intent labels from a large collection of cross-domain enterprise virtual assistant data using a multi-task cosine softmax loss. Experimental evaluation shows significant advantages for this multi-step pre-training approach, with large gains in downstream clustering accuracy on new applications compared to standard sentence embedding approaches. The approach has been incorporated into an interactive discovery tool that enables visualization and exploration of intents by system analysts and builders.</abstract>
      <url hash="eb8cf882">2022.naacl-industry.23</url>
      <bibkey>chen-etal-2022-intent</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.23</doi>
      <video href="2022.naacl-industry.23.mp4"/>
    </paper>
    <paper id="24">
      <title><fixed-case>R</fixed-case>e<fixed-case>F</fixed-case>in<fixed-case>ED</fixed-case>: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking</title>
      <author><first>Tom</first><last>Ayoola</last></author>
      <author><first>Shubhi</first><last>Tyagi</last></author>
      <author><first>Joseph</first><last>Fisher</last></author>
      <author><first>Christos</first><last>Christodoulopoulos</last></author>
      <author><first>Andrea</first><last>Pierleoni</last></author>
      <pages>209-220</pages>
      <abstract>We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed.</abstract>
      <url hash="e74f6131">2022.naacl-industry.24</url>
      <bibkey>ayoola-etal-2022-refined</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.24</doi>
      <video href="2022.naacl-industry.24.mp4"/>
      <pwccode url="https://github.com/amazon-research/ReFinED" additional="true">amazon-research/ReFinED</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aquaint">AQUAINT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ipm-nel">IPM NEL</pwcdataset>
    </paper>
    <paper id="25">
      <title>Lightweight Transformers for Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Daniel</first><last>Pressel</last></author>
      <author><first>Wenshuo</first><last>Liu</last></author>
      <author><first>Michael</first><last>Johnston</last></author>
      <author><first>Minhua</first><last>Chen</last></author>
      <pages>221-229</pages>
      <abstract>To understand how training on conversational language impacts performance of pre-trained models on downstream dialogue tasks, we build compact Transformer-based Language Models from scratch on several large corpora of conversational data. We compare the performance and characteristics of these models against BERT and other strong baselines on dialogue probing tasks. Commercial dialogue systems typically require a small footprint and fast execution time, but recent trends are in the other direction, with an ever-increasing number of parameters, resulting in difficulties in model deployment. We focus instead on training fast, lightweight models that excel at natural language understanding (NLU) and can replace existing lower-capacity conversational AI models with similar size and speed. In the process, we develop a simple but unique curriculum-based approach that moves from general-purpose to dialogue-targeted both in terms of data and objective. Our resultant models have around 1/3 the number of parameters of BERT-base and produce better representations for a wide array of intent detection datasets using linear and Mutual-Information probing techniques. Additionally, the models can be easily fine-tuned on a single consumer GPU card and deployed in near real-time production environments.</abstract>
      <url hash="003de177">2022.naacl-industry.25</url>
      <bibkey>pressel-etal-2022-lightweight</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.25</doi>
      <video href="2022.naacl-industry.25.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="26">
      <title><fixed-case>NER-MQMRC</fixed-case>: <fixed-case>F</fixed-case>ormulating Named Entity Recognition as Multi Question Machine Reading Comprehension</title>
      <author><first>Anubhav</first><last>Shrimal</last></author>
      <author><first>Avi</first><last>Jain</last></author>
      <author><first>Kartik</first><last>Mehta</last></author>
      <author><first>Promod</first><last>Yenigalla</last></author>
      <pages>230-238</pages>
      <abstract>NER has been traditionally formulated as a sequence labeling task. However, there has been recent trend in posing NER as a machine reading comprehension task (Wang et al., 2020; Mengge et al., 2020), where entity name (or other information) is considered as a question, text as the context and entity value in text as answer snippet. These works consider MRC based on a single question (entity) at a time. We propose posing NER as a multi-question MRC task, where multiple questions (one question per entity) are considered at the same time for a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC) architecture for this formulation. NER-MQMRC architecture considers all entities as input to BERT for learning token embeddings with self-attention and leverages BERT-based entity representation for further improving these token embeddings for NER task. Evaluation on three NER datasets show that our proposed architecture leads to average 2.5 times faster training and 2.3 times faster inference as compared to NER-SQMRC framework based models by considering all entities together in a single pass. Further, we show that our model performance does not degrade compared to single-question based MRC (NER-SQMRC) (Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for AE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this architecture primarily to solve large scale e-commerce attribute (or entity) extraction from unstructured text of a magnitude of 50k+ attributes to be extracted on a scalable production environment with high performance and optimised training and inference runtimes.</abstract>
      <url hash="dcd352b4">2022.naacl-industry.26</url>
      <bibkey>shrimal-etal-2022-ner</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.26</doi>
      <video href="2022.naacl-industry.26.mp4"/>
    </paper>
    <paper id="27">
      <title>What Do Users Care About? Detecting Actionable Insights from User Feedback</title>
      <author><first>Kasturi</first><last>Bhattacharjee</last></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>239-246</pages>
      <abstract>Users often leave feedback on a myriad of aspects of a product which, if leveraged successfully, can help yield useful insights that can lead to further improvements down the line. Detecting actionable insights can be challenging owing to large amounts of data as well as the absence of labels in real-world scenarios. In this work, we present an aggregation and graph-based ranking strategy for unsupervised detection of these insights from real-world, noisy, user-generated feedback. Our proposed approach significantly outperforms strong baselines on two real-world user feedback datasets and one academic dataset.</abstract>
      <url hash="4b9fadb9">2022.naacl-industry.27</url>
      <bibkey>bhattacharjee-etal-2022-users</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.27</doi>
      <video href="2022.naacl-industry.27.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
    </paper>
    <paper id="28">
      <title><fixed-case>CTM</fixed-case> - A Model for Large-Scale Multi-View Tweet Topic Classification</title>
      <author><first>Vivek</first><last>Kulkarni</last></author>
      <author><first>Kenny</first><last>Leung</last></author>
      <author><first>Aria</first><last>Haghighi</last></author>
      <pages>247-258</pages>
      <abstract>Automatically associating social media posts with topics is an important prerequisite for effective search and recommendation on many social media platforms. However, topic classification of such posts is quite challenging because of (a) a large topic space (b) short text with weak topical cues, and (c) multiple topic associations per post. In contrast to most prior work which only focuses on post-classification into a small number of topics (<tex-math>10-20</tex-math>), we consider the task of large-scale topic classification in the context of Twitter where the topic space is 10 times larger with potentially multiple topic associations per Tweet. We address the challenges above and propose a novel neural model,  that (a) supports a large topic space of 300 topics (b) takes a holistic approach to tweet content modeling – leveraging multi-modal content, author context, and deeper semantic cues in the Tweet. Our method offers an effective way to classify Tweets into topics at scale by yielding superior performance to other approaches (a relative lift of <tex-math>\mathbf{20}\%</tex-math> in median average precision score) and has been successfully deployed in production at Twitter.</abstract>
      <url hash="70a239fb">2022.naacl-industry.28</url>
      <bibkey>kulkarni-etal-2022-ctm</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.28</doi>
      <video href="2022.naacl-industry.28.mp4"/>
    </paper>
    <paper id="29">
      <title>Developing a Production System for <fixed-case>P</fixed-case>urpose of <fixed-case>C</fixed-case>all Detection in Business Phone Conversations</title>
      <author><first>Elena</first><last>Khasanova</last></author>
      <author><first>Pooja</first><last>Hiranandani</last></author>
      <author><first>Shayna</first><last>Gardiner</last></author>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Simon</first><last>Corston-Oliver</last></author>
      <author><first>Xue-Yong</first><last>Fu</last></author>
      <pages>259-267</pages>
      <abstract>For agents at a contact centre receiving calls, the most important piece of information is the reason for a given call. An agent cannot provide support on a call if they do not know why a customer is calling. In this paper we describe our implementation of a commercial system to detect Purpose of Call statements in English business call transcripts in real time. We present a detailed analysis of types of Purpose of Call statements and language patterns related to them, discuss an approach to collect rich training data by bootstrapping from a set of rules to a neural model, and describe a hybrid model which consists of a transformer-based classifier and a set of rules by leveraging insights from the analysis of call transcripts. The model achieved 88.6 F1 on average in various types of business calls when tested on real life data and has low inference time. We reflect on the challenges and design decisions when developing and deploying the system.</abstract>
      <url hash="1ce59f05">2022.naacl-industry.29</url>
      <bibkey>khasanova-etal-2022-developing</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.29</doi>
      <video href="2022.naacl-industry.29.mp4"/>
    </paper>
    <paper id="30">
      <title>Adversarial Text Normalization</title>
      <author><first>Joanna</first><last>Bitton</last></author>
      <author><first>Maya</first><last>Pavlova</last></author>
      <author><first>Ivan</first><last>Evtimov</last></author>
      <pages>268-279</pages>
      <abstract>Text-based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate, the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance, there remains an additional class of character-level attacks on which these models falter. Additionally, the process to retrain a model is time and resource intensive, creating a need for a lightweight, reusable defense. In this work, we propose the Adversarial Text Normalizer, a novel method that restores baseline performance on attacked content with low computational overhead. We evaluate the efficacy of the normalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech and Natural Language Inference. We find that text normalization provides a task-agnostic defense against character-level attacks that can be implemented supplementary to adversarial retraining solutions, which are more suited for semantic alterations.</abstract>
      <url hash="13d19c54">2022.naacl-industry.30</url>
      <bibkey>bitton-etal-2022-adversarial</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.30</doi>
      <video href="2022.naacl-industry.30.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
    </paper>
    <paper id="31">
      <title>Constraint-based Multi-hop Question Answering with Knowledge Graph</title>
      <author><first>Sayantan</first><last>Mitra</last></author>
      <author><first>Roshni</first><last>Ramnani</last></author>
      <author><first>Shubhashis</first><last>Sengupta</last></author>
      <pages>280-288</pages>
      <abstract>The objective of a Question-Answering system over Knowledge Graph (KGQA) is to respond to natural language queries presented over the KG. A complex question answering system typically addresses one of the two categories of complexity: questions with constraints and questions involving multiple hops of relations. Most of the previous works have addressed these complexities separately. Multi-hop KGQA necessitates reasoning across numerous edges of the KG in order to arrive at the correct answer. Because KGs are frequently sparse, multi-hop KGQA presents extra complications. Recent works have developed KG embedding approaches to reduce KG sparsity by performing missing link prediction. In this paper, we tried to address multi-hop constrained-based queries using KG embeddings to generate more flexible query graphs. Empirical results indicate that the proposed methodology produces state-of-the-art outcomes on three KGQA datasets.</abstract>
      <url hash="2b7ea111">2022.naacl-industry.31</url>
      <bibkey>mitra-etal-2022-constraint</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.31</doi>
      <video href="2022.naacl-industry.31.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
    </paper>
    <paper id="32">
      <title>Fast Bilingual Grapheme-To-Phoneme Conversion</title>
      <author><first>Hwa-Yeon</first><last>Kim</last></author>
      <author><first>Jong-Hwan</first><last>Kim</last></author>
      <author><first>Jae-Min</first><last>Kim</last></author>
      <pages>289-296</pages>
      <abstract>Autoregressive transformer (ART)-based grapheme-to-phoneme (G2P) models have been proposed for bi/multilingual text-to-speech systems. Although they have achieved great success, they suffer from high inference latency in real-time industrial applications, especially processing long sentence. In this paper, we propose a fast and high-performance bilingual G2P model. For fast and exact decoding, we used a non-autoregressive structured transformer-based architecture and data augmentation for predicting output length. Our model achieved better performance than that of the previous autoregressive model and about 2700% faster inference speed.</abstract>
      <url hash="bb1bdcea">2022.naacl-industry.32</url>
      <bibkey>kim-etal-2022-fast</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.32</doi>
      <revision id="1" href="2022.naacl-industry.32v1" hash="c9277315"/>
      <revision id="2" href="2022.naacl-industry.32v2" hash="bb1bdcea" date="2022-07-12">Changed wording, updated Figure 1 and corrected tables.</revision>
      <video href="2022.naacl-industry.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Knowledge Extraction From Texts Based on <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Anastasia</first><last>Shimorina</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Frédéric</first><last>Herledan</last></author>
      <pages>297-304</pages>
      <abstract>This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.</abstract>
      <url hash="a0b7870e">2022.naacl-industry.33</url>
      <bibkey>shimorina-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.33</doi>
      <video href="2022.naacl-industry.33.mp4"/>
      <pwccode url="https://github.com/shimorina/relation-extraction-db-wikidata" additional="false">shimorina/relation-extraction-db-wikidata</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>AIT-QA</fixed-case>: <fixed-case>Q</fixed-case>uestion Answering Dataset over Complex Tables in the Airline Industry</title>
      <author><first>Yannis</first><last>Katsis</last></author>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Mustafa</first><last>Canim</last></author>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Feifei</first><last>Pan</last></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <pages>305-314</pages>
      <abstract>Table Question Answering (Table QA) systems have been shown to be highly accurate when trained and tested on open-domain datasets built on top of Wikipedia tables. However, it is not clear whether their performance remains the same when applied to domain-specific scientific and business documents, encountered in industrial settings, which exhibit some unique characteristics: (a) they contain tables with a much more complex layout than Wikipedia tables (including hierarchical row and column headers), (b) they contain domain-specific terms, and (c) they are typically not accompanied by domain-specific labeled data that can be used to train Table QA models.To understand the performance of Table QA approaches in this setting, we introduce AIT-QA; a domain-specific Table QA test dataset. While focusing on the airline industry, AIT-QA reflects the challenges that domain-specific documents pose to Table QA, outlined above. In this work, we describe the creation of the dataset and report zero-shot experimental results of three SOTA Table QA methods. The results clearly expose the limitations of current methods with a best accuracy of just 51.8%. We also present pragmatic table pre-processing steps to pivot and project complex tables into a layout suitable for the SOTA Table QA models. Finally, we provide data-driven insights on how different aspects of this setting (including hierarchical headers, domain-specific terminology, and paraphrasing) affect Table QA methods, in order to help the community develop improved methods for domain-specific Table QA.</abstract>
      <url hash="b6d5b85b">2022.naacl-industry.34</url>
      <bibkey>katsis-etal-2022-ait</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.34</doi>
      <video href="2022.naacl-industry.34.mp4"/>
      <pwccode url="https://github.com/IBM/AITQA" additional="false">IBM/AITQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ait-qa">AIT-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ott-qa">OTT-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tat-qa">TAT-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="35">
      <title>Parameter-efficient Continual Learning Framework in Industrial Real-time Text Classification System</title>
      <author><first>Tao</first><last>Zhu</last></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Weijie</first><last>Liu</last></author>
      <author><first>Jiachi</first><last>Liu</last></author>
      <author><first>Yiren</first><last>Chen</last></author>
      <author><first>Weiquan</first><last>Mao</last></author>
      <author><first>Haoyan</first><last>Liu</last></author>
      <author><first>Kunbo</first><last>Ding</last></author>
      <author><first>Yudong</first><last>Li</last></author>
      <author><first>Xuefeng</first><last>Yang</last></author>
      <pages>315-323</pages>
      <abstract>Catastrophic forgetting is a challenge for model deployment in industrial real-time systems, which requires the model to quickly master a new task without forgetting the old one. Continual learning aims to solve this problem; however, it usually updates all the model parameters, resulting in extensive training times and the inability to deploy quickly. To address this challenge, we propose a parameter-efficient continual learning framework, in which efficient parameters are selected through an offline parameter selection strategy and then trained using an online regularization method. In our framework, only a few parameters need to be updated, which not only alleviates catastrophic forgetting, but also allows the model to be saved with the changed parameters instead of all parameters. Extensive experiments are conducted to examine the effectiveness of our proposal. We believe this paper will provide useful insights and experiences on developing deep learning-based online real-time systems.</abstract>
      <url hash="53bc3d4b">2022.naacl-industry.35</url>
      <bibkey>zhu-etal-2022-parameter</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.35</doi>
      <video href="2022.naacl-industry.35.mp4"/>
    </paper>
    <paper id="36">
      <title>Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Pragaash</first><last>Ponnusamy</last></author>
      <author><first>Clint Solomon</first><last>Mathialagan</last></author>
      <author><first>Gustavo</first><last>Aguilar</last></author>
      <author><first>Chengyuan</first><last>Ma</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <pages>324-333</pages>
      <abstract>Self-learning paradigms in large-scale conversational AI agents tend to leverage user feedback in bridging between what they say and what they mean. However, such learning, particularly in Markov-based query rewriting systems have far from addressed the impact of these models on future training where successive feedback is inevitably contingent on the rewrite itself, especially in a continually updating environment. In this paper, we explore the consequences of this inherent lack of self-awareness towards impairing the model performance, ultimately resulting in both Type I and II errors over time. To that end, we propose augmenting the Markov Graph construction with a superposition-based adjacency matrix. Here, our method leverages an induced stochasticity to reactively learn a locally-adaptive decision boundary based on the performance of the individual rewrites in a bi-variate beta setting. We also surface a data augmentation strategy that leverages template-based generation in abridging complex conversation hierarchies of dialogs so as to simplify the learning process. All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.</abstract>
      <url hash="617ae801">2022.naacl-industry.36</url>
      <bibkey>ponnusamy-etal-2022-self</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.36</doi>
      <video href="2022.naacl-industry.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Fast and Light-Weight Answer Text Retrieval in Dialogue Systems</title>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Siva Sankalp</first><last>Patel</last></author>
      <author><first>J William</first><last>Murdock</last></author>
      <author><first>Saloni</first><last>Potdar</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>334-343</pages>
      <abstract>Dialogue systems can benefit from being able to search through a corpus of text to find information relevant to user requests, especially when encountering a request for which no manually curated response is available. The state-of-the-art technology for neural dense retrieval or re-ranking involves deep learning models with hundreds of millions of parameters. However, it is difficult and expensive to get such models to operate at an industrial scale, especially for cloud services that often need to support a big number of individually customized dialogue systems, each with its own text corpus. We report our work on enabling advanced neural dense retrieval systems to operate effectively at scale on relatively inexpensive hardware. We compare with leading alternative industrial solutions and show that we can provide a solution that is effective, fast, and cost-efficient.</abstract>
      <url hash="e150f233">2022.naacl-industry.37</url>
      <bibkey>wan-etal-2022-fast</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.37</doi>
      <video href="2022.naacl-industry.37.mp4"/>
      <pwccode url="https://github.com/IBM/ColBERT-practical" additional="false">IBM/ColBERT-practical</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="38">
      <title><fixed-case>BLINK</fixed-case> with <fixed-case>E</fixed-case>lasticsearch for Efficient Entity Linking in Business Conversations</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last></author>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Aliaksandr</first><last>Martsinovich</last></author>
      <author><first>Jonathan</first><last>Johnston</last></author>
      <author><first>Xue-Yong</first><last>Fu</last></author>
      <author><first>Shashi Bhushan</first><last>Tn</last></author>
      <author><first>Simon</first><last>Corston-Oliver</last></author>
      <pages>344-352</pages>
      <abstract>An Entity Linking system aligns the textual mentions of entities in a text to their corresponding entries in a knowledge base. However, deploying a neural entity linking system for efficient real-time inference in production environments is a challenging task. In this work, we present a neural entity linking system that connects the product and organization type entities in business conversations to their corresponding Wikipedia and Wikidata entries. The proposed system leverages Elasticsearch to ensure inference efficiency when deployed in a resource limited cloud machine, and obtains significant improvements in terms of inference speed and memory consumption while retaining high accuracy.</abstract>
      <url hash="90745fc9">2022.naacl-industry.38</url>
      <bibkey>laskar-etal-2022-blink</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.38</doi>
      <video href="2022.naacl-industry.38.mp4"/>
    </paper>
    <paper id="39">
      <title><fixed-case>Q</fixed-case>2<fixed-case>R</fixed-case>: A Query-to-Resolution System for Natural-Language Queries</title>
      <author><first>Shiau Hong</first><last>Lim</last></author>
      <author><first>Laura</first><last>Wynter</last></author>
      <pages>353-361</pages>
      <abstract>We present a system for document retrieval that combines direct classification with standard content-based retrieval approaches to significantly improve the relevance of the retrieved documents. Our system exploits the availability of an imperfect but sizable amount of labeled data from past queries. For domains such as technical support, the proposed approach enhances the system’s ability to retrieve documents that are otherwise ranked very low based on content alone. The system is easy to implement and can make use of existing text ranking methods, augmenting them through the novel Q2R orchestration framework. Q2R has been extensively tested and is in use at IBM.</abstract>
      <url hash="d6b75b37">2022.naacl-industry.39</url>
      <bibkey>lim-wynter-2022-q2r</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.39</doi>
      <video href="2022.naacl-industry.39.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="40">
      <title><fixed-case>I</fixed-case>dentifying <fixed-case>C</fixed-case>orporate <fixed-case>C</fixed-case>redit <fixed-case>R</fixed-case>isk <fixed-case>S</fixed-case>entiments from <fixed-case>F</fixed-case>inancial <fixed-case>N</fixed-case>ews</title>
      <author><first>Noujoud</first><last>Ahbali</last></author>
      <author><first>Xinyuan</first><last>Liu</last></author>
      <author><first>Albert</first><last>Nanda</last></author>
      <author><first>Jamie</first><last>Stark</last></author>
      <author><first>Ashit</first><last>Talukder</last></author>
      <author><first>Rupinder Paul</first><last>Khandpur</last></author>
      <pages>362-370</pages>
      <abstract>Credit risk management is one central practice for financial institutions, and such practice helps them measure and understand the inherent risk within their portfolios. Historically, firms relied on the assessment of default probabilities and used the press as one tool to gather insights on the latest credit event developments of an entity. However, due to the deluge of the current news coverage for companies, analyzing news manually by financial experts is considered a highly laborious task. To this end, we propose a novel deep learning-powered approach to automate news analysis and credit adverse events detection to score the credit sentiment associated with a company. This paper showcases a complete system that leverages news extraction and data enrichment with targeted sentiment entity recognition to detect companies and text classification to identify credit events. We developed a custom scoring mechanism to provide the company’s credit sentiment score (<tex-math>CSS^{TM}</tex-math>) based on these detected events. Additionally, using case studies, we illustrate how this score helps understand the company’s credit profile and discriminates between defaulters and non-defaulters.</abstract>
      <url hash="89ad57e3">2022.naacl-industry.40</url>
      <bibkey>ahbali-etal-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.naacl-industry.40</doi>
      <video href="2022.naacl-industry.40.mp4"/>
    </paper>
  </volume>
  <event id="naacl-2022">
    <colocated>
      <volume-id>2022.findings-naacl</volume-id>
      <volume-id>2022.autosimtrans-1</volume-id>
      <volume-id>2022.bea-1</volume-id>
      <volume-id>2022.clinicalnlp-1</volume-id>
      <volume-id>2022.clpsych-1</volume-id>
      <volume-id>2022.dadc-1</volume-id>
      <volume-id>2022.deeplo-1</volume-id>
      <volume-id>2022.distcurate-1</volume-id>
      <volume-id>2022.dlg4nlp-1</volume-id>
      <volume-id>2022.emoji-1</volume-id>
      <volume-id>2022.gebnlp-1</volume-id>
      <volume-id>2022.hcinlp-1</volume-id>
      <volume-id>2022.mia-1</volume-id>
      <volume-id>2022.privatenlp-1</volume-id>
      <volume-id>2022.starsem-1</volume-id>
      <volume-id>2022.semeval-1</volume-id>
      <volume-id>2022.sigmorphon-1</volume-id>
      <volume-id>2022.sigtyp-1</volume-id>
      <volume-id>2022.socialnlp-1</volume-id>
      <volume-id>2022.suki-1</volume-id>
      <volume-id>2022.trustnlp-1</volume-id>
      <volume-id>2022.unimplicit-1</volume-id>
      <volume-id>2022.wnu-1</volume-id>
      <volume-id>2022.woah-1</volume-id>
      <volume-id>2022.wordplay-1</volume-id>
    </colocated>
  </event>
</collection>
