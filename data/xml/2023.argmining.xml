<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.argmining">
  <volume id="1" ingest-date="2023-11-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 10th Workshop on Argument Mining</booktitle>
      <editor><first>Milad</first><last>Alshomary</last></editor>
      <editor><first>Chung-Chi</first><last>Chen</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Joonsuk</first><last>Park</last></editor>
      <editor><first>Julia</first><last>Romberg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="d29b1b3a">2023.argmining-1</url>
      <venue>argmining</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="022ef60d">2023.argmining-1.0</url>
      <bibkey>argmining-ws-2023-argument</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Detecting Argumentative Fallacies in the Wild: Problems and Limitations of Large Language Models</title>
      <author><first>Ramon</first><last>Ruiz-Dolz</last></author>
      <author><first>John</first><last>Lawrence</last></author>
      <pages>1–10</pages>
      <abstract>Previous work on the automatic identification of fallacies in natural language text has typically approached the problem in constrained experimental setups that make it difficult to understand the applicability and usefulness of the proposals in the real world. In this paper, we present the first analysis of the limitations that these data-driven approaches could show in real situations. For that purpose, we first create a validation corpus consisting of natural language argumentation schemes. Second, we provide new empirical results to the emerging task of identifying fallacies in natural language text. Third, we analyse the errors observed outside of the testing data domains considering the new validation corpus. Finally, we point out some important limitations observed in our analysis that should be taken into account in future research in this topic. Specifically, if we want to deploy these systems in the Wild.</abstract>
      <url hash="42e1fd4b">2023.argmining-1.1</url>
      <bibkey>ruiz-dolz-lawrence-2023-detecting</bibkey>
      <doi>10.18653/v1/2023.argmining-1.1</doi>
    </paper>
    <paper id="2">
      <title>Using Masked Language Model Probabilities of Connectives for Stance Detection in <fixed-case>E</fixed-case>nglish Discourse</title>
      <author><first>Regina</first><last>Stodden</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Lea</first><last>Kawaletz</last></author>
      <author><first>Heidrun</first><last>Dorgeloh</last></author>
      <pages>11–18</pages>
      <abstract>This paper introduces an approach which operationalizes the role of discourse connectives for detecting argument stance. Specifically, the study investigates the utility of masked language model probabilities of discourse connectives inserted between a claim and a premise that supports or attacks it. The research focuses on a range of connectives known to signal support or attack, such as because, but, so, or although. By employing a LightGBM classifier, the study reveals promising results in stance detection in English discourse. While the proposed system does not aim to outperform state-of-the-art architectures, the classification accuracy is surprisingly high, highlighting the potential of these features to enhance argument mining tasks, including stance detection.</abstract>
      <url hash="70a6f694">2023.argmining-1.2</url>
      <bibkey>stodden-etal-2023-using</bibkey>
      <doi>10.18653/v1/2023.argmining-1.2</doi>
    </paper>
    <paper id="3">
      <title>Teach Me How to Argue: A Survey on <fixed-case>NLP</fixed-case> Feedback Systems in Argumentation</title>
      <author><first>Camelia</first><last>Guerraoui</last></author>
      <author><first>Paul</first><last>Reisert</last></author>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Farjana Sultana</first><last>Mim</last></author>
      <author><first>Keshav</first><last>Singh</last></author>
      <author><first>Jungmin</first><last>Choi</last></author>
      <author><first>Irfan</first><last>Robbani</last></author>
      <author><first>Shoichi</first><last>Naito</last></author>
      <author><first>Wenzhi</first><last>Wang</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>19–34</pages>
      <abstract>The use of argumentation in education has shown improvement in students’ critical thinking skills, and computational models for argumentation have been developed to further assist this process. Although these models are useful for evaluating the quality of an argument, they often cannot explain why a particular argument score was predicted, i.e., why the argument is good or bad, which makes it difficult to provide constructive feedback to users, e.g., students, so that they can strengthen their critical thinking skills. In this survey, we explore current NLP feedback systems by categorizing each into four important dimensions of feedback (Richness, Visualization, Interactivity and Personalization). We discuss limitations for each dimension and provide suggestions to enhance the power of feedback and explanations to ultimately improve user critical thinking skills.</abstract>
      <url hash="54cb7c3c">2023.argmining-1.3</url>
      <bibkey>guerraoui-etal-2023-teach</bibkey>
      <doi>10.18653/v1/2023.argmining-1.3</doi>
    </paper>
    <paper id="4">
      <title>Constituency Tree Representation for Argument Unit Recognition</title>
      <author><first>Samuel</first><last>Guilluy</last></author>
      <author><first>Florian</first><last>Mehats</last></author>
      <author><first>Billal</first><last>Chouli</last></author>
      <pages>35–44</pages>
      <abstract>The conventional method of extracting arguments from sentences solely relies on word proximity, disregarding the syntactic structure of the sentence. This approach often leads to inaccuracies, especially when identifying argumentative span boundaries. In this research, we investigate the benefits of utilizing a constituency tree representation of sentences to predict Argument Discourse Units (ADUs) at the token level. We first evaluate the effectiveness of utilizing the constituency tree representation for capturing the structural attributes of arguments within sentences. We demonstrate empirically that the constituency structure surpasses simple linear dependencies among neighboring words in terms of effectiveness. Our approach involves leveraging graph neural networks in conjunction with the constituency tree, adapting it specifically for argument unit recognition. Through extensive evaluation, our model outperforms existing approaches in recognizing argument units at the token level. Furthermore, we employ explainability methods to assess the suitability of our model architecture, providing insights into its performance.</abstract>
      <url hash="f691c8d4">2023.argmining-1.4</url>
      <bibkey>guilluy-etal-2023-constituency</bibkey>
      <doi>10.18653/v1/2023.argmining-1.4</doi>
    </paper>
    <paper id="5">
      <title>Stance-Aware Re-Ranking for Non-factual Comparative Queries</title>
      <author><first>Jan Heinrich</first><last>Reimer</last></author>
      <author><first>Alexander</first><last>Bondarenko</last></author>
      <author><first>Maik</first><last>Fröbe</last></author>
      <author><first>Matthias</first><last>Hagen</last></author>
      <pages>45–51</pages>
      <abstract>We propose a re-ranking approach to improve the retrieval effectiveness for non-factual comparative queries like ‘Which city is better, London or Paris?’ based on whether the results express a stance towards the comparison objects (London vs. Paris) or not. Applied to the 26 runs submitted to the Touché 2022 task on comparative argument retrieval, our stance-aware re-ranking significantly improves the retrieval effectiveness for all runs when perfect oracle-style stance labels are available. With our most effective practical stance detector based on GPT-3.5 (F₁ of 0.49 on four stance classes), our re-ranking still improves the effectiveness for all runs but only six improvements are significant. Artificially “deteriorating” the oracle-style labels, we further find that an F₁ of 0.90 for stance detection is necessary to significantly improve the retrieval effectiveness for the best run via stance-aware re-ranking.</abstract>
      <url hash="657aca0a">2023.argmining-1.5</url>
      <attachment type="Software" hash="55263778">2023.argmining-1.5.Software.zip</attachment>
      <bibkey>reimer-etal-2023-stance</bibkey>
      <doi>10.18653/v1/2023.argmining-1.5</doi>
    </paper>
    <paper id="6">
      <title>Legal Argument Extraction from Court Judgements using Integer Linear Programming</title>
      <author><first>Basit</first><last>Ali</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Anindita</first><last>Sinha Banerjee</last></author>
      <author><first>Dhirendra</first><last>Singh</last></author>
      <pages>52–63</pages>
      <abstract>Legal arguments are one of the key aspects of legal knowledge which are expressed in various ways in the unstructured text of court judgements. A large database of past legal arguments can be created by extracting arguments from court judgements, categorizing them, and storing them in a structured format. Such a database would be useful for suggesting suitable arguments for any new case. In this paper, we focus on extracting arguments from Indian Supreme Court judgements using minimal supervision. We first identify a set of certain sentence-level argument markers which are useful for argument extraction such as whether a sentence contains a claim or not, whether a sentence is argumentative in nature, whether two sentences are part of the same argument, etc. We then model the legal argument extraction problem as a text segmentation problem where we combine multiple weak evidences in the form of argument markers using Integer Linear Programming (ILP), finally arriving at a global document-level solution giving the most optimal legal arguments. We demonstrate the effectiveness of our technique by comparing it against several competent baselines.</abstract>
      <url hash="26d6eb21">2023.argmining-1.6</url>
      <bibkey>ali-etal-2023-legal</bibkey>
      <doi>10.18653/v1/2023.argmining-1.6</doi>
      <video href="2023.argmining-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Argument Detection in Student Essays under Resource Constraints</title>
      <author><first>Omid</first><last>Kashefi</last></author>
      <author><first>Sophia</first><last>Chan</last></author>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <pages>64–75</pages>
      <abstract>Learning to make effective arguments is vital for the development of critical-thinking in students and, hence, for their academic and career success. Detecting argument components is crucial for developing systems that assess students’ ability to develop arguments. Traditionally, supervised learning has been used for this task, but this requires a large corpus of reliable training examples which are often impractical to obtain for student writing. Large language models have also been shown to be effective few-shot learners, making them suitable for low-resource argument detection. However, concerns such as latency, service reliability, and data privacy might hinder their practical applicability. To address these challenges, we present a low-resource classification approach that combines the intrinsic entailment relationship among the argument elements with a parameter-efficient prompt-tuning strategy. Experimental results demonstrate the effectiveness of our method in reducing the data and computation requirements of training an argument detection model without compromising the prediction accuracy. This suggests the practical applicability of our model across a variety of real-world settings, facilitating broader access to argument classification for researchers spanning various domains and problem scenarios.</abstract>
      <url hash="2e0fb752">2023.argmining-1.7</url>
      <bibkey>kashefi-etal-2023-argument</bibkey>
      <doi>10.18653/v1/2023.argmining-1.7</doi>
    </paper>
    <paper id="8">
      <title>Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays</title>
      <author><first>Robin</first><last>Schaefer</last></author>
      <author><first>René</first><last>Knaebel</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>76–88</pages>
      <abstract>We define an argumentation strategy as the set of rhetorical and stylistic means that authors employ to produce an effective, and often persuasive, text. First computational accounts of such strategies have been relatively coarse-grained, while in our work we aim to move to a more detailed analysis. We extend the annotations of the Argument Annotated Essays corpus (Stab and Gurevych, 2017) with specific types of claims and premises, propose a model for their automatic identification and show first results, and then we discuss usage patterns that emerge with respect to the essay structure, the “flows” of argument component types, the claim-premise constellations, the role of the essay prompt type, and that of the individual author.</abstract>
      <url hash="6adc6603">2023.argmining-1.8</url>
      <bibkey>schaefer-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.argmining-1.8</doi>
    </paper>
    <paper id="9">
      <title>Dimensionality Reduction for Machine Learning-based Argument Mining</title>
      <author><first>Andrés</first><last>Segura-Tinoco</last></author>
      <author><first>Iván</first><last>Cantador</last></author>
      <pages>89–99</pages>
      <abstract>Recent approaches to argument mining have focused on training machine learning algorithms from annotated text corpora, utilizing as input high-dimensional linguistic feature vectors. Differently to previous work, in this paper, we preliminarily investigate the potential benefits of reducing the dimensionality of the input data. Through an empirical study, testing SVD, PCA and LDA techniques on a new argumentative corpus in Spanish for an underexplored domain (e-participation), and using a novel, rich argument model, we show positive results in terms of both computation efficiency and argumentative information extraction effectiveness, for the three major argument mining tasks: argumentative fragment detection, argument component classification, and argumentative relation recognition. On a space with dimension around 3-4% of the number of input features, the argument mining methods are able to reach 95-97% of the performance achieved by using the entire corpus, and even surpass it in some cases.</abstract>
      <url hash="dc64b17e">2023.argmining-1.9</url>
      <attachment type="Software" hash="5b391a68">2023.argmining-1.9.Software.zip</attachment>
      <bibkey>segura-tinoco-cantador-2023-dimensionality</bibkey>
      <doi>10.18653/v1/2023.argmining-1.9</doi>
    </paper>
    <paper id="10">
      <title>On the Impact of Reconstruction and Context for Argument Prediction in Natural Debate</title>
      <author><first>Zlata</first><last>Kikteva</last></author>
      <author><first>Alexander</first><last>Trautsch</last></author>
      <author><first>Patrick</first><last>Katzer</last></author>
      <author><first>Mirko</first><last>Oest</last></author>
      <author><first>Steffen</first><last>Herbold</last></author>
      <author><first>Annette</first><last>Hautli-Janisz</last></author>
      <pages>100–106</pages>
      <abstract>Debate naturalness ranges on a scale from small, highly structured, and topically focused settings to larger, more spontaneous and less constrained environments. The more unconstrained a debate, the more spontaneous speakers act: they build on contextual knowledge and use anaphora or ellipses to construct their arguments. They also use rhetorical devices such as questions and imperatives to support or attack claims. In this paper, we study how the reconstruction of the actual debate contributions, i.e., utterances which contain pronouns, ellipses and fuzzy language, into full-fledged propositions which are interpretable without context impacts the prediction of argument relations and investigate the effect of incorporating contextual information for the task. We work with highly complex spontaneous debates with more than 10 speakers on a wide variety of topics. We find that in contrast to our initial hypothesis, reconstruction does not improve predictions and context only improves them when used in combination with propositions.</abstract>
      <url hash="b40c3ea2">2023.argmining-1.10</url>
      <bibkey>kikteva-etal-2023-impact</bibkey>
      <doi>10.18653/v1/2023.argmining-1.10</doi>
    </paper>
    <paper id="11">
      <title>Unsupervised argument reframing with a counterfactual-based approach</title>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Dimitry</first><last>Mindlin</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>107–119</pages>
      <abstract>Framing is an important mechanism in argumentation, as participants in a debate tend to emphasize those aspects or dimensions of the issue under debate that support their standpoint. The task of reframing an argument, that is changing the underlying framing, has received increasing attention recently. We propose a novel unsupervised approach to argument reframing that takes inspiration from counterfactual explanation generation approaches in the field of eXplainable AI (XAI). We formalize the task as a mask-and-replace approach in which an LLM is tasked to replace masked tokens associated with a set of frames to be eliminated by other tokens related to a set of target frames to be added. Our method relies on two key mechanisms: framed decoding and reranking based on a number of metrics similar to those used in XAI to search for a suitable counterfactual. We evaluate our approach on three topics using the dataset by Ruckdeschel and Wiedemann (2022). We show that our two key mechanisms outperform an unguided LLM as a baseline by increasing the ratio of successfully reframed arguments by almost an order of magnitude.</abstract>
      <url hash="39833e08">2023.argmining-1.11</url>
      <bibkey>heinisch-etal-2023-unsupervised</bibkey>
      <doi>10.18653/v1/2023.argmining-1.11</doi>
    </paper>
    <paper id="12">
      <title>Overview of <fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg-2023: The First Shared Task in Multimodal Argument Mining</title>
      <author><first>Zhexiong</first><last>Liu</last></author>
      <author><first>Mohamed</first><last>Elaraby</last></author>
      <author><first>Yang</first><last>Zhong</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <pages>120–132</pages>
      <abstract>This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.</abstract>
      <url hash="01e9c480">2023.argmining-1.12</url>
      <bibkey>liu-etal-2023-overview</bibkey>
      <doi>10.18653/v1/2023.argmining-1.12</doi>
      <video href="2023.argmining-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>IUST</fixed-case> at <fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg: The First Shared Task in Multimodal Argument Mining</title>
      <author><first>Melika</first><last>Nobakhtian</last></author>
      <author><first>Ghazal</first><last>Zamaninejad</last></author>
      <author><first>Erfan</first><last>Moosavi Monazzah</last></author>
      <author><first>Sauleh</first><last>Eetemadi</last></author>
      <pages>133–138</pages>
      <abstract>ImageArg is a shared task at the 10th ArgMining Workshop at EMNLP 2023. It leverages the ImageArg dataset to advance multimodal persuasiveness techniques. This challenge comprises two distinct subtasks: 1) Argumentative Stance (AS) Classification: Assessing whether a given tweet adopts an argumentative stance. 2) Image Persuasiveness (IP) Classification: Determining if the tweet image enhances the persuasive quality of the tweet. We conducted various experiments on both subtasks and ranked sixth out of the nine participating teams.</abstract>
      <url hash="4e5e7c52">2023.argmining-1.13</url>
      <bibkey>nobakhtian-etal-2023-iust</bibkey>
      <doi>10.18653/v1/2023.argmining-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>TILFA</fixed-case>: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining</title>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Zhaowei</first><last>Wang</last></author>
      <author><first>Baixuan</first><last>Xu</last></author>
      <author><first>Tianshi</first><last>Zheng</last></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Ginny</first><last>Wong</last></author>
      <author><first>Simon</first><last>See</last></author>
      <pages>139–147</pages>
      <abstract>A main goal of Argument Mining (AM) is to analyze an author’s stance. Unlike previous AM datasets focusing only on text, the shared task at the 10th Workshop on Argument Mining introduces a dataset including both texts and images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework for Text, Image, and Layout Fusion in Argument Mining), is designed to handle this mixed data. It excels at not only understanding text but also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.</abstract>
      <url hash="13b3f3c7">2023.argmining-1.14</url>
      <bibkey>zong-etal-2023-tilfa</bibkey>
      <doi>10.18653/v1/2023.argmining-1.14</doi>
    </paper>
    <paper id="15">
      <title>A General Framework for Multimodal Argument Persuasiveness Classification of Tweets</title>
      <author><first>Mohammad</first><last>Soltani</last></author>
      <author><first>Julia</first><last>Romberg</last></author>
      <pages>148–156</pages>
      <abstract>An important property of argumentation concerns the degree of its persuasiveness, which can be influenced by various modalities. On social media platforms, individuals usually have the option of supporting their textual statements with images. The goals of the ImageArg shared task, held with ArgMining 2023, were therefore (A) to classify tweet stances considering both modalities and (B) to predict the influence of an image on the persuasiveness of a tweet text. In this paper, we present our proposed methodology that shows strong performance on both tasks, placing 3rd team on the leaderboard in each case with F1 scores of 0.8273 (A) and 0.5281 (B). The framework relies on pre-trained models to extract text and image features, which are then fed into a task-specific classification model. Our experiments highlighted that the multimodal vision and language model CLIP holds a specific importance in the extraction of features, in particular for task (A).</abstract>
      <url hash="045245e5">2023.argmining-1.15</url>
      <bibkey>soltani-romberg-2023-general</bibkey>
      <doi>10.18653/v1/2023.argmining-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>W</fixed-case>ebis @ <fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg 2023: Embedding-based Stance and Persuasiveness Classification</title>
      <author><first>Islam</first><last>Torky</last></author>
      <author><first>Simon</first><last>Ruth</last></author>
      <author><first>Shashi</first><last>Sharma</last></author>
      <author><first>Mohamed</first><last>Salama</last></author>
      <author><first>Krishna</first><last>Chaitanya</last></author>
      <author><first>Tim</first><last>Gollub</last></author>
      <author><first>Johannes</first><last>Kiesel</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <pages>157–161</pages>
      <abstract>This paper reports on the submissions of Webis to the two subtasks of ImageArg 2023. For the subtask of argumentative stance classification, we reached an F1 score of 0.84 using a BERT model for sequence classification. For the subtask of image persuasiveness classification, we reached an F1 score of 0.56 using CLIP embeddings and a neural network model, achieving the best performance for this subtask in the competition. Our analysis reveals that seemingly clear sentences (e.g., “I support gun control”) are still problematic for our otherwise competitive stance classifier and that ignoring the tweet text for image persuasiveness prediction leads to a model that is similarly effective to our top-performing model.</abstract>
      <url hash="ee6d8c78">2023.argmining-1.16</url>
      <bibkey>torky-etal-2023-webis</bibkey>
      <doi>10.18653/v1/2023.argmining-1.16</doi>
      <video href="2023.argmining-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title><fixed-case>GC</fixed-case>-Hunter at <fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg Shared Task: Multi-Modal Stance and Persuasiveness Learning</title>
      <author><first>Mohammad</first><last>Shokri</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <pages>162–166</pages>
      <abstract>With the rising prominence of social media, users frequently supplement their written content with images. This trend has brought about new challenges in automatic processing of social media messages. In order to fully understand the meaning of a post, it is necessary to capture the relationship between the image and the text. In this work we address the two main objectives of the ImageArg shared task. Firstly, we aim to determine the stance of a multi-modal tweet toward a particular issue. We propose a strong baseline, fine-tuning transformer based models on concatenation of tweet text and image text. The second goal is to predict the impact of an image on the persuasiveness of the text in a multi-modal tweet. To capture the persuasiveness of an image, we train vision and language models on the data and explore other sets of features merged with the model, to enhance prediction power. Ultimately, both of these goals contribute toward the broader aim of understanding multi-modal messages on social media and how images and texts relate to each other.</abstract>
      <url hash="3e847f8c">2023.argmining-1.17</url>
      <bibkey>shokri-levitan-2023-gc</bibkey>
      <doi>10.18653/v1/2023.argmining-1.17</doi>
    </paper>
    <paper id="18">
      <title>Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning</title>
      <author><first>Arushi</first><last>Sharma</last></author>
      <author><first>Abhibha</first><last>Gupta</last></author>
      <author><first>Maneesh</first><last>Bilalpur</last></author>
      <pages>167–174</pages>
      <abstract>To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal Argument Mining hosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-box text-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tuned text-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) and text-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot learning of LLMs performance.</abstract>
      <url hash="5c4ac461">2023.argmining-1.18</url>
      <bibkey>sharma-etal-2023-argumentative</bibkey>
      <doi>10.18653/v1/2023.argmining-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>SPLIT</fixed-case>: Stance and Persuasion Prediction with Multi-modal on Image and Textual Information</title>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Shaojun</first><last>Yu</last></author>
      <author><first>Xuan</first><last>Li</last></author>
      <author><first>Jia</first><last>Geng</last></author>
      <author><first>Zhiyuan</first><last>Zheng</last></author>
      <author><first>Joyce</first><last>Ho</last></author>
      <pages>175–180</pages>
      <abstract>Persuasiveness is a prominent personality trait that measures the extent to which a speaker can impact the beliefs, attitudes, intentions, motivations, and actions of their audience. The ImageArg task is a featured challenge at the 10th ArgMining Workshop during EMNLP 2023, focusing on harnessing the potential of the ImageArg dataset to advance techniques in multimodal persuasion. In this study, we investigate the utilization of dual-modality datasets and evaluate three distinct multi-modality models. By enhancing multi-modality datasets, we demonstrate both the advantages and constraints of cutting-edge models.</abstract>
      <url hash="7a0e0a76">2023.argmining-1.19</url>
      <bibkey>zhang-etal-2023-split</bibkey>
      <doi>10.18653/v1/2023.argmining-1.19</doi>
    </paper>
    <paper id="20">
      <title>Semantists at <fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg-2023: Exploring Cross-modal Contrastive and Ensemble Models for Multimodal Stance and Persuasiveness Classification</title>
      <author><first>Kanagasabai</first><last>Rajaraman</last></author>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Saravanan</first><last>Rajamanickam</last></author>
      <author><first>Adam Maciej</first><last>Westerski</last></author>
      <author><first>Jung-Jae</first><last>Kim</last></author>
      <pages>181–186</pages>
      <abstract>In this paper, we describe our system for ImageArg-2023 Shared Task that aims to identify an image’s stance towards a tweet and determine its persuasiveness score concerning a specific topic. In particular, the Shared Task proposes two subtasks viz. subtask (A) Multimodal Argument Stance (AS) Classification, and subtask (B) Multimodal Image Persuasiveness (IP) Classification, using a dataset composed of tweets (images and text) from controversial topics, namely gun control and abortion. For subtask A, we employ multiple transformer models using a text based approach to classify the argumentative stance of the tweet. For sub task B we adopted text based as well as multimodal learning methods to classify image persuasiveness of the tweet. Surprisingly, the text-based approach of the tweet overall performed better than the multimodal approaches considered. In summary, our best system achieved a F1 score of 0.85 for sub task (A) and 0.50 for subtask (B), and ranked 2nd in subtask (A) and 4th in subtask (B), among all teams submissions.</abstract>
      <url hash="116f57e8">2023.argmining-1.20</url>
      <bibkey>rajaraman-etal-2023-semantists</bibkey>
      <doi>10.18653/v1/2023.argmining-1.20</doi>
    </paper>
    <paper id="21">
      <title>Overview of <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews</title>
      <author><first>Nils</first><last>Dycke</last></author>
      <author><first>Ilia</first><last>Kuznetsov</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>187–196</pages>
      <abstract>Peer review is the key quality control mechanism in science. The core component of peer review are the review reports – argumentative texts where the reviewers evaluate the work and make suggestions to the authors. Reviewing is a demanding expert task prone to bias. An active line of research in NLP aims to support peer review via automatic analysis of review reports. This research meets two key challenges. First, NLP to date has focused on peer reviews from machine learning conferences. Yet, NLP models are prone to domain shift and might underperform when applied to reviews from a new research community. Second, while some venues make their reviewing processes public, peer reviewing data is generally hard to obtain and expensive to label. Approaches to low-data NLP processing for peer review remain under-investigated. Enabled by the recent release of open multi-domain corpora of peer reviews, the PragTag-2023 Shared Task explored the ways to increase domain robustness and address data scarcity in pragmatic tagging – a sentence tagging task where review statements are classified by their argumentative function. This paper describes the shared task, outlines the participating systems, and summarizes the results.</abstract>
      <url hash="27ac0e1c">2023.argmining-1.21</url>
      <bibkey>dycke-etal-2023-overview</bibkey>
      <doi>10.18653/v1/2023.argmining-1.21</doi>
      <video href="2023.argmining-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title><fixed-case>CATALPA</fixed-case>_<fixed-case>E</fixed-case>du<fixed-case>NLP</fixed-case> at <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag-2023</title>
      <author><first>Yuning</first><last>Ding</last></author>
      <author><first>Marie</first><last>Bexte</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <pages>197–201</pages>
      <abstract>This paper describes our contribution to the PragTag-2023 Shared Task. We describe and compare different approaches based on sentence classification, sentence similarity, and sequence tagging. We find that a BERT-based sentence labeling approach integrating positional information outperforms both sequence tagging and SBERT-based sentence classification. We further provide analyses highlighting the potential of combining different approaches.</abstract>
      <url hash="de7b16ff">2023.argmining-1.22</url>
      <bibkey>ding-etal-2023-catalpa</bibkey>
      <doi>10.18653/v1/2023.argmining-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>D</fixed-case>eep<fixed-case>B</fixed-case>lue<fixed-case>AI</fixed-case> at <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag-2023:Ensemble-based Text Classification Approaches under Limited Data Resources</title>
      <author><first>Zhipeng</first><last>Luo</last></author>
      <author><first>Jiahui</first><last>Wang</last></author>
      <author><first>Yihao</first><last>Guo</last></author>
      <pages>202–206</pages>
      <abstract>Due to the scarcity of review data and the high annotation cost, in this paper, we primarily delve into the fine-tuning of pretrained models using limited data. To enhance the robustness of the model, we employ adversarial training techniques. By introducing subtle perturbations, we compel the model to better cope with adversarial attacks, thereby increasing the stability of the model in input data. We utilize pooling techniques to aid the model in extracting critical information, reducing computational complexity, and improving the model’s generalization capability. Experimental results demonstrate the effectiveness of our proposed approach on a review paper dataset with limited data volume.</abstract>
      <url hash="de9b973f">2023.argmining-1.23</url>
      <bibkey>luo-etal-2023-deepblueai</bibkey>
      <doi>10.18653/v1/2023.argmining-1.23</doi>
      <video href="2023.argmining-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title><fixed-case>MILAB</fixed-case> at <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag-2023: Enhancing Cross-Domain Generalization through Data Augmentation with Reduced Uncertainty</title>
      <author><first>Yoonsang</first><last>Lee</last></author>
      <author><first>Dongryeol</first><last>Lee</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>207–211</pages>
      <abstract>This paper describes our submission to the PragTag task, which aims to categorize each sentence from peer reviews into one of the six distinct pragmatic tags. The task consists of three conditions: full, low, and zero, each distinguished by the number of training data and further categorized into five distinct domains. The main challenge of this task is the domain shift, which is exacerbated by non-uniform distribution and the limited availability of data across the six pragmatic tags and their respective domains. To address this issue, we predominantly employ two data augmentation techniques designed to mitigate data imbalance and scarcity: pseudo-labeling and synonym generation. We experimentally demonstrate the effectiveness of our approaches, achieving the first rank under the zero condition and the third in the full and low conditions.</abstract>
      <url hash="864c1794">2023.argmining-1.24</url>
      <bibkey>lee-etal-2023-milab</bibkey>
      <doi>10.18653/v1/2023.argmining-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>NUS</fixed-case>-<fixed-case>IDS</fixed-case> at <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag-2023: Improving Pragmatic Tagging of Peer Reviews through Unlabeled Data</title>
      <author><first>Sujatha Das</first><last>Gollapalli</last></author>
      <author><first>Yixin</first><last>Huang</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <pages>212–217</pages>
      <abstract>We describe our models for the Pragmatic Tagging of Peer Reviews Shared Task at the 10th Workshop on Argument Mining at EMNLP-2023. We trained multiple sentence classification models for the above competition task by employing various state-of-the-art transformer models that can be fine-tuned either in the traditional way or through instruction-based fine-tuning. Multiple model predictions on unlabeled data are combined to tentatively label unlabeled instances and augment the dataset to further improve performance on the prediction task. In particular, on the F1000RD corpus, we perform on-par with models trained on 100% of the training data while using only 10% of the data. Overall, on the competition datasets, we rank among the top-2 performers for the different data conditions.</abstract>
      <url hash="cf0cb459">2023.argmining-1.25</url>
      <bibkey>gollapalli-etal-2023-nus</bibkey>
      <doi>10.18653/v1/2023.argmining-1.25</doi>
      <video href="2023.argmining-1.25.mp4"/>
    </paper>
    <paper id="26">
      <title><fixed-case>S</fixed-case>urya<fixed-case>K</fixed-case>iran at <fixed-case>P</fixed-case>rag<fixed-case>T</fixed-case>ag 2023 - Benchmarking Domain Adaptation using Masked Language Modeling in Natural Language Processing For Specialized Data</title>
      <author><first>Kunal</first><last>Suri</last></author>
      <author><first>Prakhar</first><last>Mishra</last></author>
      <author><first>Albert</first><last>Nanda</last></author>
      <pages>218–222</pages>
      <abstract>Most transformer models are trained on English language corpus that contain text from forums like Wikipedia and Reddit. While these models are being used in many specialized domains such as scientific peer review, legal, and healthcare, their performance is subpar because they do not contain the information present in data relevant to such specialized domains. To help these models perform as well as possible on specialized domains, one of the approaches is to collect labeled data of that particular domain and fine-tune the transformer model of choice on such data. While a good approach, it suffers from the challenge of collecting a lot of labeled data which requires significant manual effort. Another way is to use unlabeled domain-specific data to pre-train these transformer model and then fine-tune this model on labeled data. We evaluate how transformer models perform when fine-tuned on labeled data after initial pre-training with unlabeled data. We compare their performance with a transformer model fine-tuned on labeled data without initial pre-training with unlabeled data. We perform this comparison on a dataset of Scientific Peer Reviews provided by organizers of PragTag-2023 Shared Task and observe that a transformer model fine-tuned on labeled data after initial pre-training on unlabeled data using Masked Language Modelling outperforms a transformer model fine-tuned only on labeled data without initial pre-training with unlabeled data using Masked Language Modelling.</abstract>
      <url hash="efdf78e2">2023.argmining-1.26</url>
      <bibkey>suri-etal-2023-suryakiran</bibkey>
      <doi>10.18653/v1/2023.argmining-1.26</doi>
    </paper>
  </volume>
</collection>
