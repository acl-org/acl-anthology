<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.bigscience">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of BigScience Episode #5 -- Workshop on Challenges &amp; Perspectives in Creating Large Language Models</booktitle>
      <editor><first>Angela</first><last>Fan</last></editor>
      <editor><first>Suzana</first><last>Ilic</last></editor>
      <editor><first>Thomas</first><last>Wolf</last></editor>
      <editor><first>Matthias</first><last>Gallé</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>virtual+Dublin</address>
      <month>May</month>
      <year>2022</year>
      <url hash="fa22c99a">2022.bigscience-1</url>
      <venue>bigscience</venue>
    </meta>
    <frontmatter>
      <url hash="4d2caefd">2022.bigscience-1.0</url>
      <bibkey>bigscience-2022-bigscience</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora</title>
      <author><first>Xisen</first><last>Jin</last></author>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Xiaokai</first><last>Wei</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>1-16</pages>
      <abstract>Pretrained language models (PTLMs) are typically learned over a large, static corpus and further fine-tuned for various downstream tasks. However, when deployed in the real world, a PTLM-based model must deal with data distributions that deviates from what the PTLM was initially trained on. In this paper, we study a lifelong language model pretraining challenge where a PTLM is continually updated so as to adapt to emerging data. Over a domain-incremental research paper stream and a chronologically-ordered tweet stream, we incrementally pretrain a PTLM with different continual learning algorithms, and keep track of the downstream task performance (after fine-tuning). We evaluate PTLM’s ability to adapt to new corpora while retaining learned knowledge in earlier corpora. Our experiments show distillation-based approaches to be most effective in retaining downstream performance in earlier domains. The algorithms also improve knowledge transfer, allowing models to achieve better downstream performance over latest data, and improve temporal generalization when distribution gaps exist between training and evaluation because of time. We believe our problem formulation, methods, and analysis will inspire future studies towards continual pretraining of language models.</abstract>
      <url hash="b618bf84">2022.bigscience-1.1</url>
      <bibkey>jin-etal-2022-lifelong</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.1</doi>
    </paper>
    <paper id="2">
      <title>Using <fixed-case>ASR</fixed-case>-Generated Text for Spoken Language Modeling</title>
      <author><first>Nicolas</first><last>Hervé</last></author>
      <author><first>Valentin</first><last>Pelloin</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Franck</first><last>Dary</last></author>
      <author><first>Antoine</first><last>Laurent</last></author>
      <author><first>Sylvain</first><last>Meignier</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>17-25</pages>
      <abstract>This papers aims at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. The new models (FlauBERT-Oral) will be shared with the community and are evaluated not only in terms of word prediction accuracy but also for two downstream tasks : classification of TV shows and syntactic parsing of speech. Experimental results show that FlauBERT-Oral is better than its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-Generated text can be useful to improve spoken language modeling.</abstract>
      <url hash="db785133">2022.bigscience-1.2</url>
      <bibkey>herve-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.2</doi>
      <video href="2022.bigscience-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings</title>
      <author><first>Zeerak</first><last>Talat</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Stella</first><last>Biderman</last></author>
      <author><first>Miruna</first><last>Clinciu</last></author>
      <author><first>Manan</first><last>Dey</last></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Sasha</first><last>Luccioni</last></author>
      <author><first>Maraim</first><last>Masoud</last></author>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Shanya</first><last>Sharma</last></author>
      <author><first>Arjun</first><last>Subramonian</last></author>
      <author><first>Jaesung</first><last>Tae</last></author>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Deepak</first><last>Tunuguntla</last></author>
      <author><first>Oskar</first><last>Van Der Wal</last></author>
      <pages>26-41</pages>
      <abstract>Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work. We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages. We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.</abstract>
      <url hash="eb2f21e0">2022.bigscience-1.3</url>
      <bibkey>talat-etal-2022-reap</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.3</doi>
      <video href="2022.bigscience-1.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/crows-pairs">CrowS-Pairs</pwcdataset>
    </paper>
    <paper id="4">
      <title>Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model</title>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>42-50</pages>
      <abstract>Ensembling is a popular method used to improve performance as a last resort. However, ensembling multiple models finetuned from a single pretrained model has been not very effective; this could be due to the lack of diversity among ensemble members. This paper proposes Multi-Ticket Ensemble, which finetunes different subnetworks of a single pretrained model and ensembles them. We empirically demonstrated that winning-ticket subnetworks produced more diverse predictions than dense networks and their ensemble outperformed the standard ensemble in some tasks when accurate lottery tickets are found on the tasks.</abstract>
      <url hash="74030f42">2022.bigscience-1.4</url>
      <bibkey>kobayashi-etal-2022-diverse</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.4</doi>
      <video href="2022.bigscience-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>UNIREX</fixed-case>: A Unified Learning Framework for Language Model Rationale Extraction</title>
      <author><first>Aaron</first><last>Chan</last></author>
      <author><first>Maziar</first><last>Sanjabi</last></author>
      <author><first>Lambert</first><last>Mathias</last></author>
      <author><first>Liang</first><last>Tan</last></author>
      <author><first>Shaoliang</first><last>Nie</last></author>
      <author><first>Xiaochang</first><last>Peng</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Hamed</first><last>Firooz</last></author>
      <pages>51-67</pages>
      <abstract>An extractive rationale explains a language model’s (LM’s) prediction on a given task instance by highlighting the text inputs that most influenced the prediction. Ideally, rationale extraction should be faithful (reflective of LM’s actual behavior) and plausible (convincing to humans), without compromising the LM’s (i.e., task model’s) task performance. Although attribution algorithms and select-predict pipelines are commonly used in rationale extraction, they both rely on certain heuristics that hinder them from satisfying all three desiderata. In light of this, we propose UNIREX, a flexible learning framework which generalizes rationale extractor optimization as follows: (1) specify architecture for a learned rationale extractor; (2) select explainability objectives (i.e., faithfulness and plausibility criteria); and (3) jointly the train task model and rationale extractor on the task using selected objectives. UNIREX enables replacing prior works’ heuristic design choices with a generic learned rationale extractor in (1) and optimizing it for all three desiderata in (2)-(3). To facilitate comparison between methods w.r.t. multiple desiderata, we introduce the Normalized Relative Gain (NRG) metric. Across five English text classification datasets, our best UNIREX configuration outperforms the strongest baselines by an average of 32.9% NRG. Plus, we find that UNIREX-trained rationale extractors’ faithfulness can even generalize to unseen datasets and tasks.</abstract>
      <url hash="108df412">2022.bigscience-1.5</url>
      <bibkey>chan-etal-2022-unirex</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.5</doi>
      <video href="2022.bigscience-1.5.mp4"/>
      <pwccode url="https://github.com/facebookresearch/unirex" additional="false">facebookresearch/unirex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cos-e">CoS-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="6">
      <title>Pipelines for Social Bias Testing of Large Language Models</title>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>68-74</pages>
      <abstract>The maturity level of language models is now at a stage in which many companies rely on them to solve various tasks. However, while research has shown how biased and harmful these models are, systematic ways of integrating social bias tests into development pipelines are still lacking. This short paper suggests how to use these verification techniques in development pipelines. We take inspiration from software testing and suggest addressing social bias evaluation as software testing. We hope to open a discussion on the best methodologies to handle social bias testing in language models.</abstract>
      <url hash="07ca0619">2022.bigscience-1.6</url>
      <bibkey>nozza-etal-2022-pipelines</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.6</doi>
      <video href="2022.bigscience-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0</title>
      <author><first>Francesco</first><last>De Toni</last></author>
      <author><first>Christopher</first><last>Akiki</last></author>
      <author><first>Javier</first><last>De La Rosa</last></author>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Enrique</first><last>Manjavacas</last></author>
      <author><first>Stefan</first><last>Schweter</last></author>
      <author><first>Daniel</first><last>Van Strien</last></author>
      <pages>75-83</pages>
      <abstract>In this work, we explore whether the recently demonstrated zero-shot abilities of the T0 model extend to Named Entity Recognition for out-of-distribution languages and time periods. Using a historical newspaper corpus in 3 languages as test-bed, we use prompts to extract possible named entities. Our results show that a naive approach for prompt-based zero-shot multilingual Named Entity Recognition is error-prone, but highlights the potential of such an approach for historical languages lacking labeled datasets. Moreover, we also find that T0-like models can be probed to predict the publication date and language of a document, which could be very relevant for the study of historical texts.</abstract>
      <url hash="72fb7cb8">2022.bigscience-1.7</url>
      <bibkey>de-toni-etal-2022-entities</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.7</doi>
      <video href="2022.bigscience-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>A Holistic Assessment of the Carbon Footprint of Noor, a Very Large <fixed-case>A</fixed-case>rabic Language Model</title>
      <author><first>Imad</first><last>Lakim</last></author>
      <author><first>Ebtesam</first><last>Almazrouei</last></author>
      <author><first>Ibrahim</first><last>Abualhaol</last></author>
      <author><first>Merouane</first><last>Debbah</last></author>
      <author><first>Julien</first><last>Launay</last></author>
      <pages>84-94</pages>
      <abstract>As ever larger language models grow more ubiquitous, it is crucial to consider their environmental impact. Characterised by extreme size and resource use, recent generations of models have been criticised for their voracious appetite for compute, and thus significant carbon footprint. Although reporting of carbon impact has grown more common in machine learning papers, this reporting is usually limited to compute resources used strictly for training. In this work, we propose a holistic assessment of the footprint of an extreme-scale language model, Noor. Noor is an ongoing project aiming to develop the largest multi-task Arabic language models–with up to 13B parameters–leveraging zero-shot generalisation to enable a wide range of downstream tasks via natural language instructions. We assess the total carbon bill of the entire project: starting with data collection and storage costs, including research and development budgets, pretraining costs, future serving estimates, and other exogenous costs necessary for this international cooperation. Notably, we find that inference costs and exogenous factors can have a significant impact on total budget. Finally, we discuss pathways to reduce the carbon footprint of extreme-scale models.</abstract>
      <url hash="857bbf5a">2022.bigscience-1.8</url>
      <bibkey>lakim-etal-2022-holistic</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.8</doi>
      <video href="2022.bigscience-1.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>GPT</fixed-case>-<fixed-case>N</fixed-case>eo<fixed-case>X</fixed-case>-20<fixed-case>B</fixed-case>: An Open-Source Autoregressive Language Model</title>
      <author><first>Sidney</first><last>Black</last></author>
      <author><first>Stella</first><last>Biderman</last></author>
      <author><first>Eric</first><last>Hallahan</last></author>
      <author><first>Quentin</first><last>Anthony</last></author>
      <author><first>Leo</first><last>Gao</last></author>
      <author><first>Laurence</first><last>Golding</last></author>
      <author><first>Horace</first><last>He</last></author>
      <author><first>Connor</first><last>Leahy</last></author>
      <author><first>Kyle</first><last>McDonell</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Michael</first><last>Pieler</last></author>
      <author><first>Usvsn Sai</first><last>Prashanth</last></author>
      <author><first>Shivanshu</first><last>Purohit</last></author>
      <author><first>Laria</first><last>Reynolds</last></author>
      <author><first>Jonathan</first><last>Tow</last></author>
      <author><first>Ben</first><last>Wang</last></author>
      <author><first>Samuel</first><last>Weinbach</last></author>
      <pages>95-136</pages>
      <abstract>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B’s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at <url>https://github.com/EleutherAI/gpt-neox</url>.</abstract>
      <url hash="dd5071ff">2022.bigscience-1.9</url>
      <bibkey>black-etal-2022-gpt</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.9</doi>
      <pwccode url="https://github.com/eleutherai/gpt-neox" additional="true">eleutherai/gpt-neox</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mmlu">MMLU</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/prost">PROST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
    </paper>
    <paper id="10">
      <title>Dataset Debt in Biomedical Language Modeling</title>
      <author><first>Jason</first><last>Fries</last></author>
      <author><first>Natasha</first><last>Seelam</last></author>
      <author><first>Gabriel</first><last>Altay</last></author>
      <author><first>Leon</first><last>Weber</last></author>
      <author><first>Myungsun</first><last>Kang</last></author>
      <author><first>Debajyoti</first><last>Datta</last></author>
      <author><first>Ruisi</first><last>Su</last></author>
      <author><first>Samuele</first><last>Garda</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Simon</first><last>Ott</last></author>
      <author><first>Matthias</first><last>Samwald</last></author>
      <author><first>Wojciech</first><last>Kusa</last></author>
      <pages>137-145</pages>
      <abstract>Large-scale language modeling and natural language prompting have demonstrated exciting capabilities for few and zero shot learning in NLP. However, translating these successes to specialized domains such as biomedicine remains challenging, due in part to biomedical NLP’s significant dataset debt – the technical costs associated with data that are not consistently documented or easily incorporated into popular machine learning frameworks at scale. To assess this debt, we crowdsourced curation of datasheets for 167 biomedical datasets. We find that only 13% of datasets are available via programmatic access and 30% lack any documentation on licensing and permitted reuse. Our dataset catalog is available at: <url>https://tinyurl.com/bigbio22</url>.</abstract>
      <url hash="6e7d01d4">2022.bigscience-1.10</url>
      <bibkey>fries-etal-2022-dataset</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.10</doi>
      <video href="2022.bigscience-1.10.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/blurb">BLURB</pwcdataset>
    </paper>
    <paper id="11">
      <title>Emergent Structures and Training Dynamics in Large Language Models</title>
      <author><first>Ryan</first><last>Teehan</last></author>
      <author><first>Miruna</first><last>Clinciu</last></author>
      <author><first>Oleg</first><last>Serikov</last></author>
      <author><first>Eliza</first><last>Szczechla</last></author>
      <author><first>Natasha</first><last>Seelam</last></author>
      <author><first>Shachar</first><last>Mirkin</last></author>
      <author><first>Aaron</first><last>Gokaslan</last></author>
      <pages>146-159</pages>
      <abstract>Large language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of information these networks learn and how such information can be encoded in the parameters of the model. We survey the literature on changes in the network during training, drawing from work outside of NLP when necessary, and on learned representations of linguistic features in large language models. We note in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivate future work that grounds the study of language models in an analysis of their changing internal structure during training time.</abstract>
      <url hash="1e301fe9">2022.bigscience-1.11</url>
      <bibkey>teehan-etal-2022-emergent</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.11</doi>
      <video href="2022.bigscience-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned</title>
      <author><first>Sameera</first><last>Horawalavithana</last></author>
      <author><first>Ellyn</first><last>Ayton</last></author>
      <author><first>Shivam</first><last>Sharma</last></author>
      <author><first>Scott</first><last>Howland</last></author>
      <author><first>Megha</first><last>Subramanian</last></author>
      <author><first>Scott</first><last>Vasquez</last></author>
      <author><first>Robin</first><last>Cosbey</last></author>
      <author><first>Maria</first><last>Glenski</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>160-172</pages>
      <abstract>Foundation models pre-trained on large corpora demonstrate significant gains across many natural language processing tasks and domains e.g., law, healthcare, education, etc. However, only limited efforts have investigated the opportunities and limitations of applying these powerful models to science and security applications. In this work, we develop foundation models of scientific knowledge for chemistry to augment scientists with the advanced ability to perceive and reason at scale previously unimagined. Specifically, we build large-scale (1.47B parameter) general-purpose models for chemistry that can be effectively used to perform a wide range of in-domain and out-of-domain tasks. Evaluating these models in a zero-shot setting, we analyze the effect of model and data scaling, knowledge depth, and temporality on model performance in context of model training efficiency. Our novel findings demonstrate that (1) model size significantly contributes to the task performance when evaluated in a zero-shot setting; (2) data quality (aka diversity) affects model performance more than data quantity; (3) similarly, unlike previous work, temporal order of the documents in the corpus boosts model performance only for specific tasks, e.g., SciQ; and (4) models pre-trained from scratch perform better on in-domain tasks than those tuned from general-purpose models like Open AI’s GPT-2.</abstract>
      <url hash="365545c7">2022.bigscience-1.12</url>
      <bibkey>horawalavithana-etal-2022-foundation</bibkey>
      <doi>10.18653/v1/2022.bigscience-1.12</doi>
      <video href="2022.bigscience-1.12.mp4"/>
      <pwccode url="https://github.com/eleutherai/gpt-neox" additional="false">eleutherai/gpt-neox</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sciq">SciQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
  </volume>
</collection>
