<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.sicon">
  <volume id="1" ingest-date="2023-07-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Social Influence in Conversations (SICon 2023)</booktitle>
      <editor><first>Kushal</first><last>Chawla</last></editor>
      <editor><first>Weiyan</first><last>Shi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="5bc6a441">2023.sicon-1</url>
      <venue>sicon</venue>
    </meta>
    <frontmatter>
      <url hash="c798e491">2023.sicon-1.0</url>
      <bibkey>sicon-2023-social</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Eliciting Rich Positive Emotions in Dialogue Generation</title>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Qingkai</first><last>Min</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>1-8</pages>
      <abstract>Positive emotion elicitation aims at evoking positive emotion states in human users in open-domain dialogue generation. However, most work focuses on inducing a single-dimension of positive sentiment using human annotated datasets, which limits the scale of the training dataset. In this paper, we propose to model various emotions in large unannotated conversations, such as joy, trust and anticipation, by leveraging a latent variable to control the emotional intention of the response. Our proposed emotion-eliciting-Conditional-Variational-AutoEncoder (EE-CVAE) model generates more diverse and emotionally-intelligent responses compared to single-dimension baseline models in human evaluation.</abstract>
      <url hash="d1aafcc7">2023.sicon-1.1</url>
      <bibkey>gong-etal-2023-eliciting</bibkey>
      <doi>10.18653/v1/2023.sicon-1.1</doi>
      <video href="2023.sicon-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Detoxifying Online Discourse: A Guided Response Generation Approach for Reducing Toxicity in User-Generated Text</title>
      <author><first>Ritwik</first><last>Bose</last><affiliation>Knox College</affiliation></author>
      <author><first>Ian</first><last>Perera</last><affiliation>The Institute for Human &amp; Machine Cognition</affiliation></author>
      <author><first>Bonnie</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>9-14</pages>
      <abstract>The expression of opinions, stances, and moral foundations on social media often coincide with toxic, divisive, or inflammatory language that can make constructive discourse across communities difficult. Natural language generation methods could provide a means to reframe or reword such expressions in a way that fosters more civil discourse, yet current Large Language Model (LLM) methods tend towards language that is too generic or formal to seem authentic for social media discussions. We present preliminary work on training LLMs to maintain authenticity while presenting a community’s ideas and values in a constructive, non-toxic manner.</abstract>
      <url hash="ee8d24da">2023.sicon-1.2</url>
      <bibkey>bose-etal-2023-detoxifying</bibkey>
      <doi>10.18653/v1/2023.sicon-1.2</doi>
      <video href="2023.sicon-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Large Language Models respond to Influence like Humans</title>
      <author><first>Lewis</first><last>Griffin</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Bennett</first><last>Kleinberg</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Maximilian</first><last>Mozes</last></author>
      <author><first>Kimberly</first><last>Mai</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Maria Do Mar</first><last>Vau</last></author>
      <author><first>Matthew</first><last>Caldwell</last><affiliation>NA</affiliation></author>
      <author><first>Augustine</first><last>Mavor-Parker</last></author>
      <pages>15-24</pages>
      <abstract>Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement boosts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same pattern of effects in both populations; although with greater per statement variability for the LLM. The second study concerns a specific mode of influence – populist framing of news to increase its persuasion and political mobilization. Newly collected data from simulated subjects was compared to previously published data from a 15 country experiment on 7286 human participants. Several effects from the human study were replicated by the simulated study, including ones that surprised the authors of the human study by contradicting their theoretical expectations; but some significant relationships found in human data were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.</abstract>
      <url hash="781b0e92">2023.sicon-1.3</url>
      <bibkey>griffin-etal-2023-large</bibkey>
      <doi>10.18653/v1/2023.sicon-1.3</doi>
    </paper>
    <paper id="4">
      <title>What Makes a Good Counter-Stereotype? Evaluating Strategies for Automated Responses to Stereotypical Text</title>
      <author><first>Kathleen</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Anna</first><last>Kerkhof</last></author>
      <pages>25-38</pages>
      <abstract>When harmful social stereotypes are expressed on a public platform, they must be addressed in a way that educates and informs both the original poster and other readers, without causing offence or perpetuating new stereotypes. In this paper, we synthesize findings from psychology and computer science to propose a set of potential counter-stereotype strategies. We then automatically generate such counter-stereotypes using ChatGPT, and analyze their correctness and expected effectiveness at reducing stereotypical associations. We identify the strategies of denouncing stereotypes, warning of consequences, and using an empathetic tone as three promising strategies to be further tested.</abstract>
      <url hash="10df2f89">2023.sicon-1.4</url>
      <bibkey>fraser-etal-2023-makes</bibkey>
      <doi>10.18653/v1/2023.sicon-1.4</doi>
      <video href="2023.sicon-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title><fixed-case>BC</fixed-case>ause: Reducing group bias and promoting cohesive discussion in online deliberation processes through a simple and engaging online deliberation tool</title>
      <author><first>Lucas</first><last>Anastasiou</last></author>
      <author><first>Anna</first><last>De Libbo</last><affiliation>NA</affiliation></author>
      <pages>39-49</pages>
      <abstract>Facilitating healthy online deliberation in terms of sensemaking and collaboration of discussion participants proves extremely challenging due to a number of known negative effects of online communication on social media platforms. We start from concerns and aspirations about the use of existing online discussion systems as distilled in previous literature, we then combine them with lessons learned on design and engineering practices from our research team, to inform the design of an easy-to-use tool (BCause.app) that enables higher quality discussions than traditional social media. We describe the design of this tool, highlighting the main interaction features that distinguish it from common social media, namely: i. the low-cost argumentation structuring of the conversations with direct replies; ii. and the distinctive use of reflective feedback rather than appreciative-only feedback. We then present the results of a controlled A/B experiment in which we show that the presence of argumentative and cognitive reflective discussion elements produces better social interaction with less polarization and promotes a more cohesive discussion than common social media-like interactions.</abstract>
      <url hash="920401d9">2023.sicon-1.5</url>
      <bibkey>anastasiou-de-libbo-2023-bcause</bibkey>
      <doi>10.18653/v1/2023.sicon-1.5</doi>
      <video href="2023.sicon-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Measuring Lexico-Semantic Alignment in Debates with Contextualized Word Representations</title>
      <author><first>Aina</first><last>Garí Soler</last><affiliation>Télécom-Paris</affiliation></author>
      <author><first>Matthieu</first><last>Labeau</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>Télécom ParisTech and Télécom Paris</affiliation></author>
      <pages>50-63</pages>
      <abstract>Dialog participants sometimes align their linguistic styles, e.g., they use the same words and syntactic constructions as their interlocutors. We propose to investigate the notion of lexico-semantic alignment: to what extent do speakers convey the same meaning when they use the same words? We design measures of lexico-semantic alignment relying on contextualized word representations. We show that they reflect interesting semantic differences between the two sides of a debate and that they can assist in the task of debate’s winner prediction.</abstract>
      <url hash="a55d1cb7">2023.sicon-1.6</url>
      <bibkey>gari-soler-etal-2023-measuring</bibkey>
      <doi>10.18653/v1/2023.sicon-1.6</doi>
      <video href="2023.sicon-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Exploring Linguistic Style Matching in Online Communities: The Role of Social Context and Conversation Dynamics</title>
      <author><first>Aparna</first><last>Ananthasubramaniam</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Jason</first><last>Yan</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Kenan</first><last>Alkiek</last></author>
      <author><first>Jiaxin</first><last>Pei</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Agrima</first><last>Seth</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Lavinia</first><last>Dunagan</last></author>
      <author><first>Minje</first><last>Choi</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Benjamin</first><last>Litterer</last><affiliation>NA</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan</affiliation></author>
      <pages>64-74</pages>
      <abstract>Linguistic style matching (LSM) in conversations can be reflective of several aspects of social influence such as power or persuasion. However, how LSM relates to the outcomes of online communication on platforms such as Reddit is an unknown question. In this study, we analyze a large corpus of two-party conversation threads in Reddit where we identify all occurrences of LSM using two types of style: the use of function words and formality. Using this framework, we examine how levels of LSM differ in conversations depending on several social factors within Reddit: post and subreddit features, conversation depth, user tenure, and the controversiality of a comment. Finally, we measure the change of LSM following loss of status after community banning. Our findings reveal the interplay of LSM in Reddit conversations with several community metrics, suggesting the importance of understanding conversation engagement when understanding community dynamics.</abstract>
      <url hash="b783b77b">2023.sicon-1.7</url>
      <bibkey>ananthasubramaniam-etal-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.sicon-1.7</doi>
      <revision id="1" href="2023.sicon-1.7v1" hash="a2075316"/>
      <revision id="2" href="2023.sicon-1.7v2" hash="b783b77b" date="2023-09-04">Updated the Acknowledgements.</revision>
      <video href="2023.sicon-1.7.mp4"/>
    </paper>
  </volume>
</collection>
