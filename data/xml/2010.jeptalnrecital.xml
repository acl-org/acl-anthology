<?xml version='1.0' encoding='UTF-8'?>
<collection id="2010.jeptalnrecital">
  <volume id="invite" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Philippe</first><last>Langlais</last></editor>
      <editor><first>Michel</first><last>Gagnon</last></editor>
      <publisher>ATALA</publisher>
      <address>Montréal, Canada</address>
      <month>July</month>
      <year>2010</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="5a4ea1cc">2010.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2010-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>La phraséologie en langue, en dictionnaire et en <fixed-case>TALN</fixed-case></title>
      <author><first>Igor</first><last>Mel’čuk</last></author>
      <pages>1–14</pages>
      <abstract/>
      <url hash="bccbbb5f">2010.jeptalnrecital-invite.1</url>
      <language>fra</language>
      <bibkey>melcuk-2010-la</bibkey>
    </paper>
    <paper id="2">
      <title>La montée en puissance des recherches en traduction automatique statistique</title>
      <author><first>Pierre</first><last>Isabelle</last></author>
      <pages>15–15</pages>
      <abstract/>
      <url hash="aedb2db0">2010.jeptalnrecital-invite.2</url>
      <language>fra</language>
      <bibkey>isabelle-2010-la</bibkey>
    </paper>
    <paper id="3">
      <title>The Quantitative Study of Writing Systems</title>
      <author><first>Gerald</first><last>Penn</last></author>
      <pages>16–16</pages>
      <abstract/>
      <url hash="aedb2db0">2010.jeptalnrecital-invite.3</url>
      <bibkey>penn-2010-quantitative</bibkey>
    </paper>
  </volume>
  <volume id="long" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Philippe</first><last>Langlais</last></editor>
      <editor><first>Michel</first><last>Gagnon</last></editor>
      <publisher>ATALA</publisher>
      <address>Montréal, Canada</address>
      <month>July</month>
      <year>2010</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="c678db68">2010.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2010-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Adaptation d’un Système de Traduction Automatique Statistique avec des Ressources monolingues</title>
      <author><first>Holger</first><last>Schwenk</last></author>
      <pages>1–10</pages>
      <abstract>Les performances d’un système de traduction statistique dépendent beaucoup de la qualité et de la quantité des données d’apprentissage disponibles. La plupart des textes parallèles librement disponibles proviennent d’organisations internationales. Le jargon observé dans ces textes n’est pas très adapté pour construire un système de traduction pour d’autres domaines. Nous présentons dans cet article une technique pour adapter le modèle de traduction à un domaine différent en utilisant des textes dans la langue source uniquement. Nous obtenons des améliorations significatives du score BLEU dans des systèmes de traduction de l’arabe vers le français et vers l’anglais.</abstract>
      <url hash="e94fb970">2010.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>schwenk-2010-adaptation</bibkey>
    </paper>
    <paper id="2">
      <title>Alignement de traductions rares à l’aide de paires de phrases non alignées</title>
      <author><first>Julien</first><last>Bourdaillet</last></author>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>11–20</pages>
      <abstract>Bien souvent, le sens d’un mot ou d’une expression peut être rendu dans une autre langue par plusieurs traductions. Parmi celles-ci, certaines se révèlent très fréquentes alors que d’autres le sont beaucoup moins, conformément à une loi zipfienne. La googlisation de notre monde n’échappe pas aux mémoires de traduction, qui mettent souvent à mal ou simplement ignorent ces traductions rares qui sont souvent de bonne qualité. Dans cet article, nous nous intéressons à ces traductions rares sous l’angle du repérage de traductions. Nous argumentons qu’elles sont plus difficiles à identifier que les traductions plus fréquentes. Nous décrivons une approche originale qui permet de mieux les identifier en tirant profit de l’alignement au niveau des mots de paires de phrases qui ne sont pas alignées. Nous montrons que cette approche permet d’améliorer l’identification de ces traductions rares.</abstract>
      <url hash="6f14ebda">2010.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>bourdaillet-etal-2010-alignement</bibkey>
    </paper>
    <paper id="3">
      <title>Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morpho-syntaxique état-de-l’art du français</title>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>21–30</pages>
      <abstract>Cet article présente MEltfr, un étiqueteur morpho-syntaxique automatique du français. Il repose sur un modèle probabiliste séquentiel qui bénéficie d’informations issues d’un lexique exogène, à savoir le Lefff. Evalué sur le FTB, MEltfr atteint un taux de précision de 97.75% (91.36% sur les mots inconnus) sur un jeu de 29 étiquettes. Ceci correspond à une diminution du taux d’erreur de 18% (36.1% sur les mots inconnus) par rapport au même modèle sans couplage avec le Lefff. Nous étudions plus en détail la contribution de cette ressource, au travers de deux séries d’expériences. Celles-ci font apparaître en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu’une modélisation plus fine du contexte droit des mots.</abstract>
      <url hash="8a133eb4">2010.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>denis-sagot-2010-exploitation</bibkey>
    </paper>
    <paper id="4">
      <title>Similarité sémantique et extraction de synonymes à partir de corpus</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>31–40</pages>
      <abstract>La définition de mesures sémantiques au niveau lexical a fait l’objet de nombreux travaux depuis plusieurs années. Dans cet article, nous nous focalisons plus spécifiquement sur les mesures de nature distributionnelle. Bien que différentes évaluations ont été réalisées les concernant, il reste difficile à établir si une mesure donnant de bons résultats dans un cadre d’évaluation peut être appliquée plus largement avec le même succès. Dans le travail présenté, nous commençons par sélectionner une mesure de similarité sur la base d’un test de type TOEFL étendu. Nous l’appliquons ensuite au problème de l’extraction de synonymes à partir de corpus en comparant nos résultats avec ceux de (Curran &amp; Moens, 2002). Enfin, nous testons l’intérêt pour cette tâche d’extraction de synonymes d’une méthode d’amélioration de la qualité des données distributionnelles proposée dans (Zhitomirsky-Geffet &amp; Dagan, 2009).</abstract>
      <url hash="74639068">2010.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>ferret-2010-similarite</bibkey>
    </paper>
    <paper id="5">
      <title>Au-delà de la paire de mots : extraction de cooccurrences syntaxiques multilexémiques</title>
      <author><first>Simon</first><last>Charest</last></author>
      <author><first>Éric</first><last>Brunelle</last></author>
      <author><first>Jean</first><last>Fontaine</last></author>
      <pages>41–50</pages>
      <abstract>Cet article décrit l’élaboration de la deuxième édition du dictionnaire de cooccurrences du logiciel d’aide à la rédaction Antidote. Cette nouvelle mouture est le résultat d’une refonte complète du processus d’extraction, ayant principalement pour but l’extraction de cooccurrences de plus de deux unités lexicales. La principale contribution de cet article est la description d’une technique originale pour l’extraction de cooccurrences de plus de deux mots conservant une structure syntaxique complète.</abstract>
      <url hash="4953d9e3">2010.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>charest-etal-2010-au</bibkey>
    </paper>
    <paper id="6">
      <title>Une approche cognitive de la fouille de grandes collections de documents</title>
      <author><first>Adil</first><last>El Ghali</last></author>
      <author><first>Yann</first><last>Vigile Hoareau</last></author>
      <pages>51–60</pages>
      <abstract>La récente éclosion du Web2.0 engendre un accroissement considérable de volumes textuels et intensifie ainsi l’importance d’une réflexion sur l’exploitation des connaissances à partir de grandes collections de documents. Dans cet article, nous présentons une approche de rechercher d’information qui s’inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requête permettant de récupérer des informations à partir d’une collection représentée dans un espace sémantique. Nous définissons les notions d’identité sémantique et de pollution sémantique dans un espace de documents. Nous illustrons notre approche par la description d’un système appelé BRAT (Blogosphere Random Analysis using Texts) basé sur les notions préalablement introduites d’identité et de pollution sématique appliquées à une tâche d’identification des actualités dans la blogosphère mondiale lors du concours TREC’09. Les premiers résultats produits sont tout à fait encourageant et indiquent les pistes des recherches à mettre en oeuvre afin d’améliorer les performances de BRAT.</abstract>
      <url hash="d57971f5">2010.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>el-ghali-vigile-hoareau-2010-une</bibkey>
    </paper>
    <paper id="7">
      <title>Motifs de graphe pour le calcul de dépendances syntaxiques complètes</title>
      <author><first>Jonathan</first><last>Marchand</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>61–70</pages>
      <abstract>Cet article propose une méthode pour calculer les dépendances syntaxiques d’un énoncé à partir du processus d’analyse en constituants. L’objectif est d’obtenir des dépendances complètes c’est-à-dire contenant toutes les informations nécessaires à la construction de la sémantique. Pour l’analyse en constituants, on utilise le formalisme des grammaires d’interaction : celui-ci place au cœur de la composition syntaxique un mécanisme de saturation de polarités qui peut s’interpréter comme la réalisation d’une relation de dépendance. Formellement, on utilise la notion de motifs de graphes au sens de la réécriture de graphes pour décrire les conditions nécessaires à la création d’une dépendance.</abstract>
      <url hash="8cb4df5c">2010.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>marchand-etal-2010-motifs</bibkey>
    </paper>
    <paper id="8">
      <title>Approche quantitative en syntaxe : l’exemple de l’alternance de position de l’adjectif épithète en français</title>
      <author><first>Juliette</first><last>Thuilier</last></author>
      <author><first>Gwendoline</first><last>Fox</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>71–80</pages>
      <abstract>Cet article présente une analyse statistique sur des données de syntaxe qui a pour but d’aider à mieux cerner le phénomène d’alternance de position de l’adjectif épithète par rapport au nom en français. Nous montrons comment nous avons utilisé les corpus dont nous disposons (French Treebank et le corpus de l’Est-Républicain) ainsi que les ressources issues du traitement automatique des langues, pour mener à bien notre étude. La modélisation à partir de 13 variables relevant principalement des propriétés du syntagme adjectival, de celles de l’item adjectival, ainsi que de contraintes basées sur la fréquence, permet de prédire à plus de 93% la position de l’adjectif. Nous insistons sur l’importance de contraintes relevant de l’usage pour le choix de la position de l’adjectif, notamment à travers la fréquence d’occurrence de l’adjectif, et la fréquence de contextes dans lesquels il apparaît.</abstract>
      <url hash="395d2967">2010.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>thuilier-etal-2010-approche</bibkey>
    </paper>
    <paper id="9">
      <title>Un modèle de caractérisation de la complexité syntaxique</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>81–90</pages>
      <abstract>Cet article présente un modèle de la complexité syntaxique. Il réunit un ensemble d’indices de complexité et les représente à l’aide d’un cadre formel homogène, offrant ainsi la possibilité d’une quantification automatique : le modèle proposé permet d’associer à chaque phrase un indice reflétant sa complexité.</abstract>
      <url hash="6d7915a6">2010.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>blache-2010-un</bibkey>
    </paper>
    <paper id="10">
      <title>Convertir des dérivations <fixed-case>TAG</fixed-case> en dépendances</title>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>91–100</pages>
      <abstract>Les structures de dépendances syntaxiques sont importantes et bien adaptées comme point de départ de diverses applications. Dans le cadre de l’analyseur TAG FRMG, nous présentons les détails d’un processus de conversion de forêts partagées de dérivations en forêts partagées de dépendances. Des éléments d’information sont fournis sur un algorithme de désambiguisation sur ces forêts de dépendances.</abstract>
      <url hash="36d2a404">2010.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>villemonte-de-la-clergerie-2010-convertir</bibkey>
    </paper>
    <paper id="11">
      <title>A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization</title>
      <author><first>Shamima</first><last>Mithun</last></author>
      <author><first>Leila</first><last>Kosseim</last></author>
      <pages>101–110</pages>
      <abstract>The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.</abstract>
      <url hash="dd7778f5">2010.jeptalnrecital-long.11</url>
      <bibkey>mithun-kosseim-2010-hybrid</bibkey>
    </paper>
    <paper id="12">
      <title>Une approche hybride traduction/correction pour la normalisation des <fixed-case>SMS</fixed-case></title>
      <author><first>Richard</first><last>Beaufort</last></author>
      <author><first>Sophie</first><last>Roekhaut</last></author>
      <author><first>Louise-Amélie</first><last>Cougnon</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <pages>111–120</pages>
      <abstract>Cet article présente une méthode hybride de normalisation des SMS, à mi-chemin entre correction orthographique et traduction automatique. La partie du système qui assure la normalisation utilise exclusivement des modèles entraînés sur corpus. Evalué en français par validation croisée, le système obtient un taux d’erreur au mot de 9.3% et un score BLEU de 0.83.</abstract>
      <url hash="68895ccf">2010.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>beaufort-etal-2010-une</bibkey>
    </paper>
    <paper id="13">
      <title>Recueil et analyse d’un corpus écologique de corrections orthographiques extrait des révisions de Wikipédia</title>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>121–130</pages>
      <abstract>Dans cet article, nous introduisons une méthode à base de règles permettant d’extraire automatiquement de l’historique des éditions de l’encyclopédie collaborative Wikipédia des corrections orthographiques. Cette méthode nous a permis de construire un corpus d’erreurs composé de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n’existe pas, à notre connaissance, de plus gros corpus d’erreurs écologiques librement disponible. En outre, les techniques mises en oeuvre peuvent être facilement transposées à de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l’étude des erreurs fréquentes ainsi que l’apprentissage et l’évaluation des correcteurs orthographiques automatiques. Plusieurs expériences illustrant son intérêt sont proposées.</abstract>
      <url hash="a9a5fbd1">2010.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>wisniewski-etal-2010-recueil</bibkey>
    </paper>
    <paper id="14">
      <title>Extension d’un système d’étiquetage d’entités nommées en étiqueteur sémantique</title>
      <author><first>Eric</first><last>Charton</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <author><first>Benoit</first><last>Ozell</last></author>
      <pages>131–140</pages>
      <abstract>L’étiquetage sémantique consiste à associer un ensemble de propriétés à une séquence de mots contenue dans un texte. Bien que proche de la tâche d’étiquetage par entités nommées, qui revient à attribuer une classe de sens à un mot, la tâche d’étiquetage ou d’annotation sémantique cherche à établir la relation entre l’entité dans son texte et sa représentation ontologique. Nous présentons un étiqueteur sémantique qui s’appuie sur un étiqueteur d’entités nommées pour mettre en relation un mot ou un groupe de mots avec sa représentation ontologique. Son originalité est d’utiliser une ontologie intermédiaire de nature statistique pour établir ce lien.</abstract>
      <url hash="4d0f415f">2010.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>charton-etal-2010-extension</bibkey>
    </paper>
    <paper id="15">
      <title>Extraction de paraphrases sémantiques et lexico-syntaxiques de corpus parallèles bilingues</title>
      <author><first>Jasmina</first><last>Milićević</last></author>
      <pages>141–150</pages>
      <abstract>Nous présentons le travail en cours effectué dans le cadre d’un projet d’extraction de paraphrases à partir de textes parallèles bilingues. Nous identifions des paraphrases sémantiques et lexico-syntaxiques, qui mettent en jeu des opérations relativement complexes sur les structures sémantiques et syntaxiques de phrases, et les décrivons au moyen de règles de paraphrasage de type Sens-Texte, utilisables dans diverses applications de TALN.</abstract>
      <url hash="e469ca68">2010.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>milicevic-2010-extraction</bibkey>
    </paper>
    <paper id="16">
      <title>Anatomie des structures énumératives</title>
      <author><first>Lydia-Mai</first><last>Ho-Dac</last></author>
      <author><first>Marie-Paule</first><last>Péry-Woodley</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>151–160</pages>
      <abstract>Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.</abstract>
      <url hash="88caa063">2010.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>ho-dac-etal-2010-anatomie</bibkey>
    </paper>
    <paper id="17">
      <title>Identification des actants et circonstants par apprentissage machine</title>
      <author><first>Fadila</first><last>Hadouche</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <pages>161–170</pages>
      <abstract>Dans cet article, nous traitons de l’identification automatique des participants actants et circonstants de lexies prédicatives verbales tirées d’un corpus spécialisé en langue française. Les actants contribuent à la réalisation du sens de la lexie alors que les circonstants sont optionnels : ils ajoutent une information supplémentaire qui ne fait pas partie intégrante du sémantisme de la lexie. Nous proposons une classification de ces participants par apprentissage machine basée sur un corpus de lexies verbales du domaine de l’informatique, lexies qui ont été annotées manuellement avec des rôles sémantiques. Nous présentons des features qui nous permettent d’identifier les participants et de distinguer les actants des circonstants.</abstract>
      <url hash="c7ccbe73">2010.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>hadouche-etal-2010-identification</bibkey>
    </paper>
    <paper id="18">
      <title>Extraction automatique d’un lexique à connotation géographique à des fins ontologiques dans un corpus de récits de voyage</title>
      <author><first>Marie-Noëlle</first><last>Bessagnet</last></author>
      <author><first>Mauro</first><last>Gaio</last></author>
      <author><first>Eric</first><last>Kergosien</last></author>
      <author><first>Christian</first><last>Sallaberry</last></author>
      <pages>171–180</pages>
      <abstract>Le but de ces travaux est d’extraire un lexique en analysant les relations entre des syntagmes nominaux et des syntagmes verbaux dans les textes de notre corpus, essentiellement des récits de voyage. L’hypothèse que nous émettons est de pouvoir établir une catégorisation des syntagmes nominaux associés à des Entités Nommées de type lieu à l’aide de l’analyse des relations verbales. En effet, nous disposons d’une chaine de traitement automatique qui extrait, interprète et valide des Entités Nommées de type lieu dans des documents textuels. Ce travail est complété par l’analyse des relations verbales associées à ces EN, candidates à l’enrichissement d’une ontologie.</abstract>
      <url hash="f4b99a09">2010.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>bessagnet-etal-2010-extraction</bibkey>
    </paper>
    <paper id="19">
      <title>Classification du genre vidéo reposant sur des transcriptions automatiques</title>
      <author><first>Stanislas</first><last>Oger</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Georges</first><last>Linarès</last></author>
      <pages>181–190</pages>
      <abstract>Dans cet article nous proposons une nouvelle méthode pour l’identification du genre vidéo qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots apparaissant dans les transcriptions des pistes audio des vidéos, obtenues à l’aide d’un système de reconnaissance automatique de la parole. Les expériences sont réalisées sur un corpus composé de dessins animés, de films, de journaux télévisés, de publicités, de documentaires, d’émissions de sport et de clips de musique. L’approche proposée permet d’obtenir un taux de bonne classification de 74% sur cette tâche. En combinant cette approche avec des méthodes reposant sur des paramètres acoustiques bas-niveau, nous obtenons un taux de bonne classification de 95%.</abstract>
      <url hash="03c00c53">2010.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>oger-etal-2010-classification</bibkey>
    </paper>
    <paper id="20">
      <title>Reconnaissance robuste d’entités nommées sur de la parole transcrite automatiquement</title>
      <author><first>Christian</first><last>Raymond</last></author>
      <author><first>Julien</first><last>Fayolle</last></author>
      <pages>191–200</pages>
      <abstract>Les transcriptions automatiques de parole constituent une ressource importante, mais souvent bruitée, pour décrire des documents multimédia contenant de la parole (e.g. journaux télévisés). En vue d’améliorer la recherche documentaire, une étape d’extraction d’information à caractère sémantique, précédant l’indexation, permet de faire face au problème des transcriptions imparfaites. Parmis ces contenus informatifs, on compte les entités nommées (e.g. noms de personnes) dont l’extraction est l’objet de ce travail. Les méthodes traditionnelles de reconnaissance basées sur une définition manuelle de grammaires formelles donnent de bons résultats sur du texte ou des transcriptions propres manuellement produites, mais leurs performances se trouvent fortement affectées lorsqu’elles sont appliquées sur des transcriptions automatiques. Nous présentons, ici, trois méthodes pour la reconnaissance d’entités nommées basées sur des algorithmes d’apprentissage automatique : les champs conditionnels aléatoires, les machines à de support, et les transducteurs à états finis. Nous présentons également une méthode pour rendre consistantes les données d’entrainement lorsqu’elles sont annotées suivant des conventions légèrement différentes. Les résultats montrent que les systèmes d’étiquetage obtenus sont parmi les plus robustes sur les données d’évaluation de la campagne ESTER 2 dans les conditions où la transcription automatique est particulièrement bruitée.</abstract>
      <url hash="fd099ad8">2010.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>raymond-fayolle-2010-reconnaissance</bibkey>
    </paper>
    <paper id="21">
      <title>Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané</title>
      <author><first>Younès</first><last>Bahou</last></author>
      <author><first>Abir</first><last>Masmoudi</last></author>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <pages>201–210</pages>
      <abstract>Les disfluences inhérents de toute parole spontanée sont un vrai défi pour les systèmes de compréhension de la parole. Ainsi, nous proposons dans cet article, une méthode originale pour le traitement des disfluences (plus précisément, les autocorrections, les répétitions, les hésitations et les amorces) dans le cadre de la compréhension automatique de l’oral arabe spontané. Notre méthode est basée sur une analyse à la fois robuste et partielle, des énoncés oraux arabes. L’idée consiste à combiner une technique de reconnaissance de patrons avec une analyse sémantique superficielle par segments conceptuels. Cette méthode a été testée à travers le module de compréhension du système SARF, un serveur vocal interactif offrant des renseignements sur le transport ferroviaire tunisien (Bahou et al., 2008). Les résultats d’évaluation de ce module montrent que la méthode proposée est très prometteuse. En effet, les mesures de rappel, de précision et de F-Measure sont respectivement de 79.23%, 74.09% et 76.57%.</abstract>
      <url hash="4764d8bf">2010.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>bahou-etal-2010-traitement</bibkey>
    </paper>
    <paper id="22">
      <title>Utilisation de relations sémantiques pour améliorer la segmentation thématique de documents télévisuels</title>
      <author><first>Camille</first><last>Guinaudeau</last></author>
      <author><first>Guillaume</first><last>Gravier</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>211–220</pages>
      <abstract>Les méthodes de segmentation thématique exploitant une mesure de la cohésion lexicale peuvent être appliquées telles quelles à des transcriptions automatiques de programmes télévisuels. Cependant, elles sont moins efficaces dans ce contexte, ne prenant en compte ni les particularités des émissions TV, ni celles des transcriptions. Nous étudions ici l’apport de relations sémantiques pour rendre les techniques de segmentation thématique plus robustes. Nous proposons une méthode pour exploiter ces relations dans une mesure de la cohésion lexicale et montrons qu’elles permettent d’augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus composés respectivement de 40h de journaux télévisés et de 40h d’émissions de reportage. Ces améliorations démontrent que les relations sémantiques peuvent rendre les méthodes de segmentation moins sensibles aux erreurs de transcription et au manque de répétitions constaté dans certaines émissions télévisées.</abstract>
      <url hash="60682288">2010.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>guinaudeau-etal-2010-utilisation</bibkey>
    </paper>
    <paper id="23">
      <title>Une évaluation de l’impact des types de textes sur la tâche de segmentation thématique</title>
      <author><first>Clémentine</first><last>Adam</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <pages>221–230</pages>
      <abstract>Cette étude a pour but de contribuer à la définition des objectifs de la segmentation thématique (ST), en incitant à prendre en considération le paramètre du type de textes dans cette tâche. Notre hypothèse est que, si la ST est certes pertinente pour traiter certains textes dont l’organisation est bien thématique, elle n’est pas adaptée à la prise en compte d’autres modes d’organisation (temporelle, rhétorique), et ne peut pas être appliquée sans précaution à des textes tout-venants. En comparant les performances d’un système de ST sur deux corpus, à organisation thématique “forte” et “faible”, nous montrons que cette tâche est effectivement sensible à la nature des textes.</abstract>
      <url hash="474b37e6">2010.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>adam-etal-2010-une</bibkey>
    </paper>
    <paper id="24">
      <title>Utilisation d’indices temporels pour la segmentation événementielle de textes</title>
      <author><first>Ludovic</first><last>Jean-Louis</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>231–240</pages>
      <abstract>Dans le domaine de l’Extraction d’Information, une place importante est faite à l’extraction d’événements dans des dépêches d’actualité, particulièrement justifiée dans le contexte d’applications de veille. Or il est fréquent qu’une dépêche d’actualité évoque plusieurs événements de même nature pour les comparer. Nous proposons dans cet article d’étudier des méthodes pour segmenter les textes en séparant les événements, dans le but de faciliter le rattachement des informations pertinentes à l’événement principal. L’idée est d’utiliser des modèles d’apprentissage statistique exploitant les marqueurs temporels présents dans les textes pour faire cette segmentation. Nous présentons plus précisément deux modèles (HMM et CRF) entraînés pour cette tâche et, en faisant une évaluation de ces modèles sur un corpus de dépêches traitant d’événements sismiques, nous montrons que les méthodes proposées permettent d’obtenir des résultats au moins aussi bons que ceux d’une approche ad hoc, avec une approche beaucoup plus générique.</abstract>
      <url hash="9b0a245f">2010.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>jean-louis-etal-2010-utilisation</bibkey>
    </paper>
    <paper id="25">
      <title>Évaluation automatique de résumés avec et sans référence</title>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <author><first>Iria</first><last>da Cunha</last></author>
      <author><first>Patricia</first><last>Velázquez-Morales</last></author>
      <author><first>Eric</first><last>Sanjuan</last></author>
      <pages>241–251</pages>
      <abstract>Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le contenu. Nous nous intéressons en particulier à la corrélation entre les mesures d’évaluation avec et sans référence humaine. Nous avons développé FRESA, un nouveau système d’évaluation fondé sur le contenu qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre système de comparaison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en étudiant leurs associations dans les tâches du résumé multi-document générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (français/espagnol).</abstract>
      <url hash="f49b841e">2010.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>torres-moreno-etal-2010-evaluation</bibkey>
    </paper>
    <paper id="26">
      <title>Jusqu’où peut-on aller avec les méthodes par extraction pour la rédaction de résumés?</title>
      <author><first>Pierre-Etienne</first><last>Genest</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <author><first>Mehdi</first><last>Yousfi-Monod</last></author>
      <pages>252–261</pages>
      <abstract>La majorité des systèmes de résumés automatiques sont basés sur l’extraction de phrases, or on les compare le plus souvent avec des résumés rédigés manuellement par abstraction. Nous avons mené une expérience dans le but d’établir une limite supérieure aux performances auxquelles nous pouvons nous attendre avec une approche par extraction. Cinq résumeurs humains ont composé 88 résumés de moins de 100 mots, en extrayant uniquement des phrases présentes intégralement dans les documents d’entrée. Les résumés ont été notés sur la base de leur contenu, de leur niveau linguistique et de leur qualité globale par les évaluateurs de NIST dans le cadre de la compétition TAC 2009. Ces résumés ont obtenus de meilleurs scores que l’ensemble des 52 systèmes automatiques participant à la compétition, mais de nettement moins bons que ceux obtenus par les résumeurs humains pouvant formuler les phrases de leur choix dans le résumé. Ce grand écart montre l’insuffisance des méthodes par extraction pure.</abstract>
      <url hash="548d6786">2010.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>genest-etal-2010-jusquou</bibkey>
    </paper>
    <paper id="27">
      <title>Comment formule-t-on une réponse en langue naturelle ?</title>
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>262–271</pages>
      <abstract>Cet article présente l’étude d’un corpus de réponses formulées par des humains à des questions factuelles. Des observations qualitatives et quantitatives sur la reprise d’éléments de la question dans les réponses sont exposées. La notion d’information-réponse est introduite et une étude de la présence de cet élément dans le corpus est proposée. Enfin, les formulations des réponses sont étudiées.</abstract>
      <url hash="907f36ec">2010.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>garcia-fernandez-etal-2010-comment</bibkey>
    </paper>
    <paper id="28">
      <title>Apprentissage non supervisé pour la traduction automatique : application à un couple de langues peu doté</title>
      <author><first>Thi</first><last>Ngoc Diep</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Eric</first><last>Castelli</last></author>
      <pages>272–281</pages>
      <abstract>Cet article présente une méthode non-supervisée pour extraire des paires de phrases parallèles à partir d’un corpus comparable. Un système de traduction automatique est utilisé pour exploiter le corpus comparable et détecter les paires de phrases parallèles. Un processus itératif est exécuté non seulement pour augmenter le nombre de paires de phrases parallèles extraites, mais aussi pour améliorer la qualité globale du système de traduction. Une comparaison avec une méthode semi-supervisée est présentée également. Les expériences montrent que la méthode non-supervisée peut être réellement appliquée dans le cas où on manque de données parallèles. Bien que les expériences préliminaires soient menées sur la traduction français-anglais, cette méthode non-supervisée est également appliquée avec succès à un couple de langues peu doté : vietnamien-français.</abstract>
      <url hash="03ab1394">2010.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>ngoc-diep-etal-2010-apprentissage</bibkey>
    </paper>
    <paper id="29">
      <title>Orthographic and Morphological Processing for <fixed-case>E</fixed-case>nglish-<fixed-case>A</fixed-case>rabic Statistical Machine Translation</title>
      <author><first>Ahmed</first><last>El Kholy</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>282–291</pages>
      <abstract>Much of the work on Statistical Machine Translation (SMT) from morphologically rich languages has shown that morphological tokenization and orthographic normalization help improve SMT quality because of the sparsity reduction they contribute. In this paper, we study the effect of these processes on SMT when translating into a morphologically rich language, namely Arabic. We explore a space of tokenization schemes and normalization options. We only evaluate on detokenized and orthographically correct (enriched) output. Our results show that the best performing tokenization scheme is that of the Penn Arabic Treebank. Additionally, training on orthographically normalized (reduced) text then jointly enriching and detokenizing the output outperforms training on enriched text.</abstract>
      <url hash="ef4d767d">2010.jeptalnrecital-long.29</url>
      <bibkey>el-kholy-habash-2010-orthographic</bibkey>
    </paper>
    <paper id="30">
      <title>Reordering Matrix Post-verbal Subjects for <fixed-case>A</fixed-case>rabic-to-<fixed-case>E</fixed-case>nglish <fixed-case>SMT</fixed-case></title>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Yuval</first><last>Marton</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>292–301</pages>
      <abstract>We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44% of the gain in BLEU and 51% of the gain in TER obtained with a word alignment training bitext that is 5 times larger.</abstract>
      <url hash="86eca585">2010.jeptalnrecital-long.30</url>
      <bibkey>carpuat-etal-2010-reordering</bibkey>
    </paper>
    <paper id="31">
      <title>Du <fixed-case>TAL</fixed-case> au <fixed-case>TIL</fixed-case></title>
      <author><first>Michael</first><last>Zock</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>302–310</pages>
      <abstract>Historiquement deux types de traitement de la langue ont été étudiés: le traitement par le cerveau (approche psycholinguistique) et le traitement par la machine (approche TAL). Nous pensons qu’il y a place pour un troisième type: le traitement interactif de la langue (TIL), l’ordinateur assistant le cerveau. Ceci correspond à un besoin réel dans la mesure où les gens n’ont souvent que des connaissances partielles par rapport au problème à résoudre. Le but du TIL est de construire des ponts entre ces connaissances momentanées d’un utilisateur et la solution recherchée. À l’aide de quelques exemples, nous essayons de montrer que ceci est non seulement faisable et souhaitable, mais également d’un coût très raisonnable.</abstract>
      <url hash="4093c413">2010.jeptalnrecital-long.31</url>
      <language>fra</language>
      <bibkey>zock-lapalme-2010-du</bibkey>
    </paper>
    <paper id="32">
      <title>Restrictions de sélection et réalisations syntagmatiques dans <fixed-case>DICOVALENCE</fixed-case> Conversion vers un format utilisable en <fixed-case>TAL</fixed-case></title>
      <author><first>Piet</first><last>Mertens</last></author>
      <pages>311–320</pages>
      <abstract>Cet article décrit des modifications du dictionnaire de valence des verbes du français DICOVALENCE qui visent à le rendre neutre par rapport aux modèles syntaxiques, à expliciter certaines informations sur le cadre de sous-catégorisation et à le rendre ainsi directement utilisable en TAL. Les informations explicitées sont les suivantes : (a) les fonctions syntaxiques des arguments verbaux, (b) les restrictions de sélection portant sur ces arguments et (c) leurs réalisations syntagmatiques possibles. Les restrictions sont exprimées à l’aide de traits sémantiques. L’article décrit aussi le calcul de ces traits sémantiques à partir des paradigmes des pronoms (et d’éléments similaires) associés aux arguments. On obtient un format indépendant du modèle syntaxique, dont l’interprétation est transparente.</abstract>
      <url hash="315e4fa7">2010.jeptalnrecital-long.32</url>
      <language>fra</language>
      <bibkey>mertens-2010-restrictions</bibkey>
    </paper>
    <paper id="33">
      <title>Recherche contextuelle d’équivalents en banque de terminologie</title>
      <author><first>Caroline</first><last>Barrière</last></author>
      <pages>321–330</pages>
      <abstract>Notre recherche démontre que l’utilisation du contenu d’un texte à traduire permet de mieux cibler dans une banque de terminologie les équivalents terminologiques pertinents à ce texte. Une banque de terminologie a comme particularité qu’elle catégorise ses entrées (fiches) en leur assignant un ou des domaines provenant d’une liste de domaines préétablie. La stratégie ici présentée repose sur l’utilisation de cette information sur les domaines. Un algorithme a été développé pour l’assignation automatique d’un profil de domaines à un texte. Celui-ci est combiné à un algorithme d’appariement entre les domaines d’un terme présent dans la banque de terminologie et le profil de domaines du texte. Pour notre expérimentation, des résumés bilingues (français et anglais) provenant de huit revues scientifiques nous fournissent un ensemble de 1130 paires d’équivalents terminologiques et le Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) nous sert de ressource terminologique. Sur notre ensemble, nous démontrons une réduction de 75% du rang moyen de l’équivalent correct en comparaison avec un choix au hasard.</abstract>
      <url hash="c37ca367">2010.jeptalnrecital-long.33</url>
      <language>fra</language>
      <bibkey>barriere-2010-recherche</bibkey>
    </paper>
    <paper id="34">
      <title>Réécriture de graphes de dépendances pour l’interface syntaxe-sémantique</title>
      <author><first>Guillaume</first><last>Bonfante</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Mathieu</first><last>Morey</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>331–340</pages>
      <abstract>Nous définissons le beta-calcul, un calcul de réécriture de graphes, que nous proposons d’utiliser pour étudier les liens entre différentes représentations linguistiques. Nous montrons comment transformer une analyse syntaxique en une représentation sémantique par la composition de deux jeux de règles de beta-calcul. Le premier souligne l’importance de certaines informations syntaxiques pour le calcul de la sémantique et explicite le lien entre syntaxe et sémantique sous-spécifiée. Le second décompose la recherche de modèles pour les représentations sémantiques sous-spécifiées.</abstract>
      <url hash="62b24c42">2010.jeptalnrecital-long.34</url>
      <language>fra</language>
      <bibkey>bonfante-etal-2010-reecriture</bibkey>
    </paper>
    <paper id="35">
      <title>Évaluer des annotations manuelles dispersées : les coefficients sont-ils suffisants pour estimer l’accord inter-annotateurs ?</title>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Claire</first><last>François</last></author>
      <author><first>Maha</first><last>Ghribi</last></author>
      <pages>341–350</pages>
      <abstract>L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus communément utilisés, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analysé dans quelle mesure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables à ces coefficients permettant de calculer le kappa pondéré (Cohen, 1968) et l’alpha (Krippendorff, 1980, 2004). Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul des distances entre catégories reposant sur les annotations réalisées.</abstract>
      <url hash="be7583c4">2010.jeptalnrecital-long.35</url>
      <language>fra</language>
      <bibkey>fort-etal-2010-evaluer</bibkey>
    </paper>
    <paper id="36">
      <title>An empirical study of maximum entropy approach for part-of-speech tagging of <fixed-case>V</fixed-case>ietnamese texts</title>
      <author><first>Phuong</first><last>Le-Hong</last></author>
      <author><first>Azim</first><last>Roussanaly</last></author>
      <author><first>Thi</first><last>Minh Huyen Nguyen</last></author>
      <author><first>Mathias</first><last>Rossignol</last></author>
      <pages>351–362</pages>
      <abstract>This paper presents an empirical study on the application of the maximum entropy approach for part-of-speech tagging of Vietnamese text, a language with special characteristics which largely distinguish it from occidental languages. Our best tagger explores and includes useful knowledge sources for tagging Vietnamese text and gives a 93.40%overall accuracy and a 80.69%unknown word accuracy on a test set of the Vietnamese treebank. Our tagger significantly outperforms the tagger that is being used for building the Vietnamese treebank, and as far as we are aware, this is the best tagging result ever published for the Vietnamese language.</abstract>
      <url hash="886d883b">2010.jeptalnrecital-long.36</url>
      <bibkey>le-hong-etal-2010-empirical</bibkey>
    </paper>
    <paper id="37">
      <title>Extraction semi-automatique d’un vocabulaire savant de base pour l’indexation automatique</title>
      <author><first>Lyne</first><last>Da Sylva</last></author>
      <pages>363–372</pages>
      <abstract>Le projet décrit vise à soutenir les efforts de constitution de ressources lexicales utiles à l’indexation automatique. Un type de vocabulaire utile à l’indexation est défini, le vocabulaire savant de base, qui peut s’articuler avec le vocabulaire spécialisé pour constituer des entrées d’index structurées. On présente les résultats d’ une expérimentation d’ extraction (semi-)automatique des mots du vocabulaire savant de base à partir d’un corpus ciblé, constitué de résumés d’articles scientifiques en français et en anglais. La tâche d’extraction a réussi à doubler une liste originale constituée manuellement pour le français. La comparaison est établie avec une expérimentation similaire effectuée pour l’anglais sur un corpus plus grand et contenant des résumés d’articles non seulement en sciences pures mais aussi en sciences humaines et sociales.</abstract>
      <url hash="7624dab0">2010.jeptalnrecital-long.37</url>
      <language>fra</language>
      <bibkey>da-sylva-2010-extraction</bibkey>
    </paper>
    <paper id="38">
      <title>Apprentissage non supervisé de la morphologie d’une langue par généralisation de relations analogiques</title>
      <author><first>Jean-François</first><last>Lavallée</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>373–382</pages>
      <abstract>Bien que les approches fondées sur la théorie de l’information sont prédominantes dans le domaine de l’analyse morphologique non supervisée, depuis quelques années, d’autres approches ont gagné en popularité, dont celles basées sur l’analogie formelle. Cette dernière reste tout de même marginale due notamment à son coût de calcul élevé. Dans cet article, nous proposons un algorithme basé sur l’analogie formelle capable de traiter les lexiques volumineux. Nous introduisons pour cela le concept de règle de cofacteur qui permet de généraliser l’information capturée par une analogie tout en contrôlant les temps de traitement. Nous comparons notre système à 2 systèmes : Morfessor (Creutz &amp; Lagus, 2005), un système de référence dans de nombreux travaux sur l’analyse morphologique et le système analogique décrit par Langlais (2009). Nous en montrons la supériorité pour 3 des 5 langues étudiées ici : le finnois, le turc, et l’allemand.</abstract>
      <url hash="ea3de068">2010.jeptalnrecital-long.38</url>
      <language>fra</language>
      <bibkey>lavallee-langlais-2010-apprentissage</bibkey>
    </paper>
    <paper id="39">
      <title>Analyse morphologique en terminologie biomédicale par alignement et apprentissage non-supervisé</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Ewa</first><last>Kijak</last></author>
      <pages>383–392</pages>
      <abstract>Dans le domaine biomédical, beaucoup de termes sont des composés savants (composés de plusieurs racines gréco-latines). L’étude de leur morphologie est importante pour de nombreuses applications puisqu’elle permet de structurer ces termes, de les rechercher efficacement, de les traduire... Dans cet article, nous proposons de suivre une démarche originale mais fructueuse pour mener cette analyse morphologique sur des termes simples en français, en nous appuyant sur une langue pivot, le japonais, et plus précisément sur les termes écrits en kanjis. Pour cela nous avons développé un algorithme d’alignement de termes spécialement adapté à cette tâche. C’est cet alignement d’un terme français avec sa traduction en kanjis qui fournit en même temps une décomposition en morphe et leur étiquetage par les kanjis correspondants. Évalué sur un jeu de données conséquent, notre approche obtient une précision supérieure à 70% et montrent son bien fondé en comparaison avec les techniques existantes. Nous illustrons également l’intérêt de notre démarche au travers de deux applications directes de ces alignements : la traduction de termes inconnus et la découverte de relations entre morphes pour la tructuration terminologique.</abstract>
      <url hash="d13ae822">2010.jeptalnrecital-long.39</url>
      <language>fra</language>
      <bibkey>claveau-kijak-2010-analyse</bibkey>
    </paper>
    <paper id="40">
      <title>Développement de ressources pour le persan: lexique morphologique et chaîne de traitements de surface</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Géraldine</first><last>Walther</last></author>
      <pages>393–402</pages>
      <abstract>Nous présentons PerLex, un lexique morphologique du persan à large couverture et librement disponible, accompagné d’une chaîne de traitements de surface pour cette langue. Nous décrivons quelques caractéristiques de la morphologie du persan, et la façon dont nous l’avons représentée dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la méthodologie que nous avons employée pour construire les entrées lexicales à partir de diverses sources, ainsi que sur les problèmes liés à la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de référence, et devrait donc constituer un bon point de départ pour le développement d’un lexique syntaxique du persan.</abstract>
      <url hash="c232d308">2010.jeptalnrecital-long.40</url>
      <language>fra</language>
      <bibkey>sagot-walther-2010-developpement</bibkey>
    </paper>
  </volume>
  <volume id="court" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Articles courts</booktitle>
      <editor><first>Philippe</first><last>Langlais</last></editor>
      <editor><first>Michel</first><last>Gagnon</last></editor>
      <publisher>ATALA</publisher>
      <address>Montréal, Canada</address>
      <month>July</month>
      <year>2010</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="e9c1f3b1">2010.jeptalnrecital-court.0</url>
      <bibkey>jep-taln-recital-2010-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Adjectifs relationnels et langue de spécialité : vérification d’une hypothèse linguistique en corpus comparable médical</title>
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <pages>1–6</pages>
      <abstract>Cet article présente une étude en corpus comparable médical pour confirmer la préférence d’utilisation des adjectifs relationnels dans les langues de spécialité et examiner plus finement l’alternance entre syntagmes nominaux avec adjectifs relationnels et syntagmes avec complément prépositionnel.</abstract>
      <url hash="670aaaf9">2010.jeptalnrecital-court.1</url>
      <language>fra</language>
      <bibkey>deleger-cartoni-2010-adjectifs</bibkey>
    </paper>
    <paper id="2">
      <title>Détermination et pondération des raffinements d’un terme à partir de son arbre des usages nommés</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Alain</first><last>Joubert</last></author>
      <pages>7–13</pages>
      <abstract>Grâce à la participation d’un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d’usage d’un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l’arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d’une descente en largeur. En simplifiant l’arbre des usages nommés, nous déterminons les différents sens d’un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.</abstract>
      <url hash="d719c9c0">2010.jeptalnrecital-court.2</url>
      <language>fra</language>
      <bibkey>lafourcade-joubert-2010-determination</bibkey>
    </paper>
    <paper id="3">
      <title>L’évaluation des paraphrases : pour une prise en compte de la tâche</title>
      <author><first>Jonathan</first><last>Chevelu</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <author><first>Thierry</first><last>Moudenc</last></author>
      <author><first>Ghislain</first><last>Putois</last></author>
      <pages>14–19</pages>
      <abstract>Les définitions des paraphrases privilégient généralement la conservation du sens. Cet article démontre par l’absurde qu’une évaluation uniquement basée sur la conservation du sens permet à un système inutile de production de paraphrase d’être jugé meilleur qu’un système au niveau de l’état de l’art. La conservation du sens n’est donc pas l’unique critère des paraphrases. Nous exhibons les trois objectifs des paraphrases : la conservation du sens, la naturalité et l’adaptation à la tâche. La production de paraphrase est alors un compromis dépendant de la tâche entre ces trois critères et ceux-ci doivent être pris en compte lors des évaluations.</abstract>
      <url hash="02085f17">2010.jeptalnrecital-court.3</url>
      <language>fra</language>
      <bibkey>chevelu-etal-2010-levaluation</bibkey>
    </paper>
    <paper id="4">
      <title>Constitution d’une ressource sémantique issue du treillis des catégories de <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Olivier</first><last>Collin</last></author>
      <author><first>Benoît</first><last>Gaillard</last></author>
      <author><first>Jean-Léon</first><last>Bouraoui</last></author>
      <pages>20–25</pages>
      <abstract>Le travail présenté dans cet article s’inscrit dans le thème de l’acquisition automatique de ressources sémantiques s’appuyant sur les données de Wikipedia. Nous exploitons le graphe des catégories associées aux pages de Wikipedia à partir duquel nous extrayons une hiérarchie de catégories parentes, sémantiquement et thématiquement liées. Cette extraction est le résultat d’une stratégie de plus court chemin appliquée au treillis global des catégories. Chaque page peut ainsi être représentée dans l’espace de ses catégories propres, ainsi que des catégories parentes. Nous montrons la possibilité d’utiliser cette ressource pour deux applications. La première concerne l’indexation et la classification des pages de Wikipedia. La seconde concerne la désambiguïsation dans le cadre d’un traducteur de requêtes français/anglais. Ce dernier travail a été réalisé en exploitant les catégories des pages anglaises.</abstract>
      <url hash="3e918181">2010.jeptalnrecital-court.4</url>
      <language>fra</language>
      <bibkey>collin-etal-2010-constitution</bibkey>
    </paper>
    <paper id="5">
      <title>Ponctuations fortes abusives</title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>26–31</pages>
      <abstract>Certaines ponctuations fortes sont « abusivement » utilisées à la place de ponctuations faibles, débouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article présente une étude sur corpus de ce phénomène et une ébauche d’outil pour repérer automatiquement les ponctuations fortes abusives.</abstract>
      <url hash="542ad90a">2010.jeptalnrecital-court.5</url>
      <language>fra</language>
      <bibkey>danlos-sagot-2010-ponctuations</bibkey>
    </paper>
    <paper id="6">
      <title>Une étude des questions “complexes” en question-réponse</title>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Mathieu</first><last>Falco</last></author>
      <pages>32–37</pages>
      <abstract>La plupart des systèmes de question-réponse ont été conçus pour répondre à des questions dites “factuelles” (réponses précises comme des dates, des lieux), et peu se sont intéressés au traitement des questions complexes. Cet article présente une typologie des questions en y incluant les questions complexes, ainsi qu’une typologie des formes de réponses attendues pour chaque type de questions. Nous présentons également des expériences préliminaires utilisant ces typologies pour les questions complexes, avec de bons résultats.</abstract>
      <url hash="21c4f665">2010.jeptalnrecital-court.6</url>
      <language>fra</language>
      <bibkey>moriceau-etal-2010-une</bibkey>
    </paper>
    <paper id="7">
      <title>Weak Translation Problems – a case study of Scriptural Translation</title>
      <author><first>Muhammad</first><last>Ghulam Abbas Malik</last></author>
      <author><first>Christian</first><last>Boitet</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>38–43</pages>
      <abstract>General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.</abstract>
      <url hash="687cfc0d">2010.jeptalnrecital-court.7</url>
      <bibkey>ghulam-abbas-malik-etal-2010-weak</bibkey>
    </paper>
    <paper id="8">
      <title>Pseudo-racinisation de la langue amazighe</title>
      <author><first>Fadoua</first><last>Ataa Allah</last></author>
      <author><first>Siham</first><last>Boulaknadel</last></author>
      <pages>44–49</pages>
      <abstract>Dans le cadre de la promotion de la langue amazighe, nous avons voulu lui apporter des ressources et outils linguistiques pour son traitement automatique et son intégration dans le domaine des nouvelles technologies de l’information et de la communication. Partant de ce principe, nous avons opté, au sein de l’Institut Royal de la Culture Amazighe, pour une démarche innovante de réalisations progressives de ressources linguistiques et d’outils de base de traitement automatique, qui permettront de préparer le terrain pour d’éventuelles recherches scientifiques. Dans cette perspective, nous avons entrepris de développer, dans un premier temps, un outil de pseudoracinisation basé sur une approche relevant du cas de la morphologie flexionnelle et reposant sur l’élimination d’une liste de suffixes et de préfixes de la langue amazighe. Cette approche permettra de regrouper les mots sémantiquement proches à partir de ressemblances afin d’être exploités dans des applications tel que la recherche d’information et la classification.</abstract>
      <url hash="969a45c5">2010.jeptalnrecital-court.8</url>
      <language>fra</language>
      <bibkey>ataa-allah-boulaknadel-2010-pseudo</bibkey>
    </paper>
    <paper id="9">
      <title>Une approche paresseuse de l’analyse sémantique ou comment construire une interface syntaxe-sémantique à partir d’exemples</title>
      <author><first>François-Régis</first><last>Chaumartin</last></author>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>50–55</pages>
      <abstract>Cet article montre comment calculer une interface syntaxe-sémantique à partir d’un analyseur en dépendance quelconque et interchangeable, de ressources lexicales variées et d’une base d’exemples associés à leur représentation sémantique. Chaque exemple permet de construire une règle d’interface. Nos représentations sémantiques sont des graphes hiérarchisés de relations prédicat-argument entre des acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée. Nous montrons comment obtenir un système très modulaire en calculant certaines règles par « soustraction » de règles moins modulaires.</abstract>
      <url hash="80b230cf">2010.jeptalnrecital-court.9</url>
      <language>fra</language>
      <bibkey>chaumartin-kahane-2010-une</bibkey>
    </paper>
    <paper id="10">
      <title>La traduction automatique des pronoms clitiques. Quelle approche pour quels résultats?</title>
      <author><first>Lorenza</first><last>Russo</last></author>
      <pages>56–61</pages>
      <abstract>Dans cet article, nous abordons la problématique de la traduction automatique des pronoms clitiques, en nous focalisant sur la traduction de l’italien vers le français et en comparant les résultats obtenus par trois systèmes : Its-2, développé au LATL (Laboratoire d’Analyse et de Technologie du Langage) et basé sur un analyseur syntaxique profond ; Babelfish, basé sur des règles linguistiques ; et Google Translate, caractérisé par une approche statistique.</abstract>
      <url hash="e9586bb2">2010.jeptalnrecital-court.10</url>
      <language>fra</language>
      <bibkey>russo-2010-la</bibkey>
    </paper>
    <paper id="11">
      <title>L’antonymie observée avec des méthodes de <fixed-case>TAL</fixed-case> : une relation à la fois syntagmatique et paradigmatique ?</title>
      <author><first>François</first><last>Morlane-Hondère</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <pages>62–67</pages>
      <abstract>Cette étude utilise des outils de TAL pour tester l’hypothèse avancée par plusieurs études linguistiques récentes selon laquelle la relation antonymique, classiquement décrite comme une relation paradigmatique, a la particularité de fonctionner également sur le plan syntagmatique, c’est-à-dire de réunir des mots qui sont non seulement substituables mais qui apparaissent également régulièrement dans des relations contextuelles. Nous utilisons deux méthodes – l’analyse distributionnelle pour le plan paradigmatique, la recherche par patrons antonymiques pour le plan syntagmatique. Les résultats montrent que le diagnostic d’antonymie n’est pas significativement meilleur lorsqu’on croise les deux méthodes, puisqu’une partie des antonymes identifiés ne répondent pas au test de substituabilité, ce qui semble confirmer la prépondérance du plan syntagmatique pour l’étude et l’acquisition de cette relation.</abstract>
      <url hash="890809c9">2010.jeptalnrecital-court.11</url>
      <language>fra</language>
      <bibkey>morlane-hondere-fabre-2010-lantonymie</bibkey>
    </paper>
    <paper id="12">
      <title>L’apport des concepts métiers pour la classification des questions ouvertes d’enquête</title>
      <author><first>Ludivine</first><last>Kuznik</last></author>
      <author><first>Anne-Laure</first><last>Guénet</last></author>
      <author><first>Anne</first><last>Peradotto</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>68–73</pages>
      <abstract>EDF utilise les techniques de Text Mining pour optimiser sa relation client, en analysant des réponses aux questions ouvertes d’enquête de satisfaction, et des retranscriptions de conversations issues des centres d’appels. Dans cet article, nous présentons les différentes contraintes applicatives liées à l’utilisation d’outils de text mining pour l’analyse de données clients. Après une analyse des différents outils présents sur le marché, nous avons identifié la technologie Skill CartridgeTM fournie par la société TEMIS comme la plus adaptée à nos besoins. Cette technologie nous permet une modélisation sémantique de concepts liés au motif d’insatisfaction. L’apport de cette modélisation est illustrée pour une tâche de classification de réponses d’enquêtes de satisfaction chargée d’évaluer la fidélité des clients EDF. La modélisation sémantique a permis une nette amélioration des scores de classification (F-mesure = 75,5%) notamment pour les catégories correspondant à la satisfaction et au mécontentement.</abstract>
      <url hash="e1242f37">2010.jeptalnrecital-court.12</url>
      <language>fra</language>
      <bibkey>kuznik-etal-2010-lapport</bibkey>
    </paper>
    <paper id="13">
      <title>Un étiqueteur de rôles grammaticaux libre pour le français intégré à <fixed-case>A</fixed-case>pache <fixed-case>UIMA</fixed-case></title>
      <author><first>Charles</first><last>Dejean</last></author>
      <author><first>Manoel</first><last>Fortun</last></author>
      <author><first>Clotilde</first><last>Massot</last></author>
      <author><first>Vincent</first><last>Pottier</last></author>
      <author><first>Fabien</first><last>Poulard</last></author>
      <author><first>Matthieu</first><last>Vernier</last></author>
      <pages>74–79</pages>
      <abstract>L’étiquetage des rôles grammaticaux est une tâche de pré-traitement récurrente. Pour le français, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche, ne nécessitant aucune ressource, pour la création d’un modèle de Markov caché (HMM) pour palier les problèmes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées à ce travail.</abstract>
      <url hash="bcc2b5c1">2010.jeptalnrecital-court.13</url>
      <language>fra</language>
      <bibkey>dejean-etal-2010-un</bibkey>
    </paper>
    <paper id="14">
      <title>Segmentation Automatique de Lettres Historiques</title>
      <author><first>Michel</first><last>Généreux</last></author>
      <author><first>Rita</first><last>Marquilhas</last></author>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <pages>80–85</pages>
      <abstract>Cet article présente une approche basée sur la comparaison fréquentielle de modèles lexicaux pour la segmentation automatique de textes historiques Portugais. Cette approche traite d’abord le problème de la segmentation comme un problème de classification, en attribuant à chaque élément lexical présent dans la phase d’apprentissage une valeur de saillance pour chaque type de segment. Ces modèles lexicaux permettent à la fois de produire une segmentation et de faire une analyse qualitative de textes historiques. Notre évaluation montre que l’approche adoptée permet de tirer de l’information sémantique que des approches se concentrant sur la détection des frontières séparant les segments ne peuvent acquérir.</abstract>
      <url hash="b120d46a">2010.jeptalnrecital-court.14</url>
      <language>fra</language>
      <bibkey>genereux-etal-2010-segmentation</bibkey>
    </paper>
    <paper id="15">
      <title>Traitement des inconnus : une approche systématique de l’incomplétude lexicale</title>
      <author><first>Helena</first><last>Blancafort</last></author>
      <author><first>Gaëlle</first><last>Recourcé</last></author>
      <author><first>Javier</first><last>Couto</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Rosa</first><last>Stern</last></author>
      <author><first>Denis</first><last>Teyssou</last></author>
      <pages>86–91</pages>
      <abstract>Cet article aborde le phénomène de l’incomplétude des ressources lexicales, c’est-à-dire la problématique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d’abord une définition opérationnelle de la notion d’inconnu. Nous décrivons ensuite une typologie des différentes classes d’inconnus, motivée par des considérations linguistiques et applicatives ainsi que par l’annotation des inconnus d’un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et validée par l’annotation d’un corpus important de l’Agence France-Presse dans le cadre du projet EDyLex.</abstract>
      <url hash="5296545e">2010.jeptalnrecital-court.15</url>
      <language>fra</language>
      <bibkey>blancafort-etal-2010-traitement</bibkey>
    </paper>
    <paper id="16">
      <title>Thésaurus et corpus de spécialité sciences du langage : approches lexicométriques appliquées à l’analyse de termes en corpus</title>
      <author><first>Évelyne</first><last>Jacquey</last></author>
      <author><first>Laurence</first><last>Kister</last></author>
      <author><first>Mick</first><last>Grzesitchak</last></author>
      <author><first>Bertrand</first><last>Gaiffe</last></author>
      <author><first>Coralie</first><last>Reutenauer</last></author>
      <author><first>Sandrine</first><last>Ollinger</last></author>
      <author><first>Mathieu</first><last>Valette</last></author>
      <pages>92–98</pages>
      <abstract>Cet article s’inscrit dans les recherches sur l’exploitation de ressources terminologiques pour l’analyse de textes de spécialité, leur annotation et leur indexation. Les ressources en présence sont, d’une part, un thesaurus des Sciences du Langage, le Thesaulangue et, d’autre part, un corpus d’échantillons issus de cinq ouvrages relevant du même domaine. L’article a deux objectifs. Le premier est de déterminer dans quelle mesure les termes de Thesaulangue sont représentés dans les textes. Le second est d’évaluer si les occurrences des unités lexicales correspondant aux termes de Thesaulangue relèvent majoritairement d’emplois terminologiques ou de langue courante. A cette fin, les travaux présentés utilisent une mesure de richesse lexicale telle qu’elle a été définie par Brunet (rapporté dans Muller, 1992) dans le domaine de la lexicométrie, l’indice W. Cette mesure est adaptée afin de mesurer la richesse terminologie (co-occurrents lexicaux et sémantiques qui apparaissent dans Thesaulangue).</abstract>
      <url hash="1dbf783a">2010.jeptalnrecital-court.16</url>
      <language>fra</language>
      <bibkey>jacquey-etal-2010-thesaurus</bibkey>
    </paper>
    <paper id="17">
      <title>Détection hors contexte des émotions à partir du contenu linguistique d’énoncés oraux : le système <fixed-case>E</fixed-case>mo<fixed-case>L</fixed-case>ogus</title>
      <author><first>Marc</first><last>Le Tallec</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Arielle</first><last>Syssau-Vaccarella</last></author>
      <pages>99–104</pages>
      <abstract>Le projet EmotiRob, soutenu par l’agence nationale de la recherche, s’est donné pour objectif de détecter des émotions dans un contexte d’application original : la réalisation d’un robot compagnon émotionnel pour des enfants fragilisés. Nous présentons dans cet article le système qui caractérise l’émotion induite par le contenu linguistique des propos de l’enfant. Il se base sur un principe de compositionnalité des émotions, avec une valeur émotionnelle fixe attribuée aux mots lexicaux, tandis que les verbes et les adjectifs agissent comme des fonctions dont le résultat dépend de la valeur émotionnelle de leurs arguments. L’article présente la méthode de calcul utilisée, ainsi que la norme lexicale émotionnelle correspondante. Une analyse quantitative et qualitative des premières expérimentations présente les différences entre les sorties du module de détection et l’annotation d’experts, montrant des résultats satisfaisants, avec la bonne détection de la valence émotionnelle dans plus de 90% des cas.</abstract>
      <url hash="8c6cf5ea">2010.jeptalnrecital-court.17</url>
      <language>fra</language>
      <bibkey>le-tallec-etal-2010-detection</bibkey>
    </paper>
    <paper id="18">
      <title>Acquisition de paraphrases sous-phrastiques depuis des paraphrases d’énoncés</title>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>105–110</pages>
      <abstract>Dans cet article, nous présentons la tâche d’acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et décrivons plusieurs techniques opérant à différents niveaux. Nous décrivons une évaluation visant à comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d’énoncés obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour améliorer des techniques existantes.</abstract>
      <url hash="7c2944b1">2010.jeptalnrecital-court.18</url>
      <language>fra</language>
      <bibkey>bouamor-etal-2010-acquisition</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>JAWS</fixed-case> : Just Another <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Subset</title>
      <author><first>Claire</first><last>Mouton</last></author>
      <author><first>Gaël</first><last>de Chalendar</last></author>
      <pages>111–116</pages>
      <abstract>WordNet, une des ressources lexicales les plus utilisées aujourd’hui a été constituée en anglais et les chercheurs travaillant sur d’autres langues souffrent du manque d’une telle ressource. Malgré les efforts fournis par la communauté française, les différents WordNets produits pour la langue française ne sont toujours pas aussi exhaustifs que le WordNet de Princeton. C’est pourquoi nous proposons une méthode novatrice dans la production de termes nominaux instanciant les différents synsets de WordNet en exploitant les propriétés syntaxiques distributionnelles du vocabulaire français. Nous comparons la ressource que nous obtenons avecWOLF et montrons que notre approche offre une couverture plus large.</abstract>
      <url hash="ec61e64a">2010.jeptalnrecital-court.19</url>
      <language>fra</language>
      <bibkey>mouton-de-chalendar-2010-jaws</bibkey>
    </paper>
    <paper id="20">
      <title>Un système de détection d’entités nommées adapté pour la campagne d’évaluation <fixed-case>ESTER</fixed-case> 2</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <pages>117–122</pages>
      <abstract>Dans cet article nous relatons notre participation à la campagne d’évaluation ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques). Après avoir décrit les objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.</abstract>
      <url hash="1d207bd3">2010.jeptalnrecital-court.20</url>
      <language>fra</language>
      <bibkey>brun-ehrmann-2010-un</bibkey>
    </paper>
    <paper id="21">
      <title>Comparaison de ressources lexicales pour l’extraction de synonymes</title>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>123–128</pages>
      <abstract/>
      <url hash="d381670b">2010.jeptalnrecital-court.21</url>
      <language>fra</language>
      <bibkey>muller-langlais-2010-comparaison</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>R</fixed-case>ef<fixed-case>G</fixed-case>en : un module d’identification des chaînes de référence dépendant du genre textuel</title>
      <author><first>Laurence</first><last>Longo</last></author>
      <author><first>Amalia</first><last>Todiraşcu</last></author>
      <pages>129–134</pages>
      <abstract>Dans cet article, nous présentons RefGen, un module d’identification des chaînes de référence pour le français. RefGen effectue une annotation automatique des expressions référentielles puis identifie les relations de coréférence établies entre ces expressions pour former des chaînes de référence. Le calcul de la référence utilise des propriétés des chaînes de référence dépendantes du genre textuel, l’échelle d’accessibilité d’(Ariel, 1990) et une série de filtres lexicaux, morphosyntaxiques et sémantiques. Nous évaluons les premiers résultats de RefGen sur un corpus issu de rapports publics.</abstract>
      <url hash="2ee50b0e">2010.jeptalnrecital-court.22</url>
      <language>fra</language>
      <bibkey>longo-todirascu-2010-refgen</bibkey>
    </paper>
    <paper id="23">
      <title>Détection et résolution d’entités nommées dans des dépêches d’agence</title>
      <author><first>Rosa</first><last>Stern</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>135–140</pages>
      <abstract>Nous présentons NP, un système de reconnaissance d’entités nommées. Comprenant un module de résolution, il permet d’associer à chaque occurrence d’entité le référent qu’elle désigne parmi les entrées d’un référentiel dédié. NP apporte ainsi des informations pertinentes pour l’exploitation de l’extraction d’entités nommées en contexte applicatif. Ce système fait l’objet d’une évaluation grâce au développement d’un corpus annoté manuellement et adapté aux tâches de détection et de résolution.</abstract>
      <url hash="719fc28d">2010.jeptalnrecital-court.23</url>
      <language>fra</language>
      <bibkey>stern-sagot-2010-detection</bibkey>
    </paper>
    <paper id="24">
      <title>Processus de décision à base de <fixed-case>SVM</fixed-case> pour la composition d’arbres de frames sémantiques</title>
      <author><first>Marie-Jean</first><last>Meurs</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>141–146</pages>
      <abstract>Cet article présente un processus de décision basé sur des classifieurs à vaste marge (SVMDP) pour extraire l’information sémantique dans un système de dialogue oral. Dans notre composant de compréhension, l’information est représentée par des arbres de frames sémantiques définies selon le paradigme FrameNet. Le processus d’interprétation est réalisé en deux étapes. D’abord, des réseaux bayésiens dynamiques (DBN) sont utilisés comme modèles de génération pour inférer des fragments d’arbres de la requête utilisateur. Ensuite, notre SVMDP dépendant du contexte compose ces fragments afin d’obtenir la représentation sémantique globale du message. Les expériences sont menées sur le corpus de dialogue MEDIA. Une procédure semi-automatique fournit une annotation de référence en frames sur laquelle les paramètres des DBN et SVMDP sont appris. Les résultats montrent que la méthode permet d’améliorer les performances d’identification de frames pour les exemples de test les plus complexes par rapport à un processus de décision déterministe ad hoc.</abstract>
      <url hash="c25a31ec">2010.jeptalnrecital-court.24</url>
      <language>fra</language>
      <bibkey>meurs-lefevre-2010-processus</bibkey>
    </paper>
    <paper id="25">
      <title>Les entités nommées événement et les verbes de cause-conséquence</title>
      <author><first>Béatrice</first><last>Arnulphy</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>147–152</pages>
      <abstract>L’extraction des événements désignés par des noms est peu étudiée dans des corpus généralistes. Si des lexiques de noms déclencheurs d’événements existent, les problèmes de polysémie sont nombreux et beaucoup d’événements ne sont pas introduits par des déclencheurs. Nous nous intéressons dans cet article à une hypothèse selon laquelle les verbes induisant la cause ou la conséquence sont de bons indices quant à la présence d’événements nominaux dans leur cotexte.</abstract>
      <url hash="7e70e244">2010.jeptalnrecital-court.25</url>
      <language>fra</language>
      <bibkey>arnulphy-etal-2010-les</bibkey>
    </paper>
    <paper id="26">
      <title>Construction d’un lexique affectif pour le français à partir de <fixed-case>T</fixed-case>witter</title>
      <author><first>Alexander</first><last>Pak</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>153–158</pages>
      <abstract>Un lexique affectif est un outil utile pour l’étude des émotions ainsi que pour la fouille d’opinion et l’analyse des sentiments. Un tel lexique contient des listes de mots annotés avec leurs évaluations émotionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais très peu pour le français. Un travail de longue haleine est nécessaire pour construire et enrichir un lexique affectif. Nous proposons d’utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes émotionnels en français. En utilisant l’ensemble des données recueillies, nous avons estimé les normes affectives de chaque mot. Nous utilisons les données de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en français afin de valider nos résultats. Les valeurs du coefficient tau de Kendall et du coefficient de corrélation de rang de Spearman montrent que nos scores estimés sont en accord avec les scores ANEW.</abstract>
      <url hash="9813f11b">2010.jeptalnrecital-court.26</url>
      <language>fra</language>
      <bibkey>pak-paroubek-2010-construction</bibkey>
    </paper>
    <paper id="27">
      <title>Analyse d’opinion : annotation sémantique de textes chinois</title>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Stéphane</first><last>Ferrari</last></author>
      <pages>159–164</pages>
      <abstract>Notre travail concerne l’analyse automatique des énoncés d’opinion en chinois. En nous inspirant de la théorie linguistique de l’Appraisal, nous proposons une méthode fondée sur l’usage de lexiques et de règles locales pour déterminer les caractéristiques telles que la Force (intensité), le Focus (prototypicalité) et la polarité de tels énoncés. Nous présentons le modèle et sa mise en oeuvre sur un corpus journalistique. Si pour la détection d’énoncés d’opinion, la précision est bonne (94 %), le taux de rappel (67 %) pose cependant des questions sur l’enrichissement des ressources actuelles.</abstract>
      <url hash="75ef5c83">2010.jeptalnrecital-court.27</url>
      <language>fra</language>
      <bibkey>zhang-ferrari-2010-analyse</bibkey>
    </paper>
    <paper id="28">
      <title>Fouille de données séquentielles d’itemsets pour l’apprentissage de patrons linguistiques</title>
      <author><first>Peggy</first><last>Cellier</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>165–170</pages>
      <abstract>Dans cet article nous présentons une méthode utilisant l’extraction de motifs séquentiels d’itemsets pour l’apprentissage automatique de patrons linguistiques. De plus, nous proposons de nous appuyer sur l’ordre partiel existant entre les motifs pour les énumérer de façon structurée et ainsi faciliter leur validation en tant que patrons linguistiques.</abstract>
      <url hash="a01866f8">2010.jeptalnrecital-court.28</url>
      <language>fra</language>
      <bibkey>cellier-charnois-2010-fouille</bibkey>
    </paper>
    <paper id="29">
      <title>Tree analogical learning. Application in <fixed-case>NLP</fixed-case></title>
      <author><first>Anouar</first><last>Ben Hassena</last></author>
      <author><first>Laurent</first><last>Miclet</last></author>
      <pages>171–176</pages>
      <abstract>In Artificial Intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. This paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and presents some of its uses in machine learning for NLP. The analogical proportion is a relation between four objects that expresses that the way to transform the first object into the second is the same as the way to transform the third in the fourth. We firstly give definitions about the general notion of analogical proportion between four objects. We give a special focus on objects structured as ordered and labeled trees, with an original definition of analogy based on optimal alignment. Secondly, we present two algorithms which deal with tree analogical matching and solving analogical equations between trees. We show their use in two applications : the learning of the syntactic tree (parsing) of a sentence and the generation of prosody for synthetic speech.</abstract>
      <url hash="2b94f07b">2010.jeptalnrecital-court.29</url>
      <bibkey>ben-hassena-miclet-2010-tree</bibkey>
    </paper>
    <paper id="30">
      <title>Adapter un système de question-réponse en domaine ouvert au domaine médical</title>
      <author><first>Mehdi</first><last>Embarek</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>177–182</pages>
      <abstract>Dans cet article, nous présentons Esculape, un système de question-réponse en français dédié aux médecins généralistes et élaboré à partir d’OEdipe, un système de question-réponse en domaine ouvert. Esculape ajoute à OEdipe la capacité d’exploiter la structure d’un modèle du domaine, le domaine médical dans le cas présent. Malgré l’existence d’un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n’est pas possible de se reposer entièrement sur ces ressources, et plus spécifiquement sur les relations qu’elles abritent, pour répondre aux questions. Nous montrons comment surmonter cette difficulté en apprenant de façon supervisée des patrons linguistiques d’extraction de relations et en les appliquant à l’extraction de réponses.</abstract>
      <url hash="233c8bfa">2010.jeptalnrecital-court.30</url>
      <language>fra</language>
      <bibkey>embarek-ferret-2010-adapter</bibkey>
    </paper>
    <paper id="31">
      <title>L’apport d’une approche hybride pour la reconnaissance des entités nommées en langue arabe</title>
      <author><first>Inès</first><last>Zribi</last></author>
      <author><first>Souha</first><last>Mezghani Hammami</last></author>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <pages>183–188</pages>
      <abstract>Dans cet article, nous proposons une méthode hybride pour la reconnaissance des entités nommées pour la langue arabe. Cette méthode profite, d’une part, des avantages de l’utilisation d’une méthode d’apprentissage pour extraire des règles permettant l’identification et la classification des entités nommées. D’autre part, elle repose sur un ensemble de règles extraites manuellement pour corriger et améliorer le résultat de la méthode d’apprentissage. Les résultats de l’évaluation de la méthode proposée sont encourageants. Nous avons obtenu un taux global de F-mesure égal à 79.24%.</abstract>
      <url hash="9974595c">2010.jeptalnrecital-court.31</url>
      <language>fra</language>
      <bibkey>zribi-etal-2010-lapport</bibkey>
    </paper>
    <paper id="32">
      <title>Semi-automated Extraction of a Wide-Coverage Type-Logical Grammar for <fixed-case>F</fixed-case>rench</title>
      <author><first>Richard</first><last>Moot</last></author>
      <pages>189–194</pages>
      <abstract>The paper describes the development of a wide-coverage type-logical grammar for French, which has been extracted from the Paris 7 treebank and received a significant amount of manual verification and cleanup. The resulting treebank is evaluated using a supertagger and performs at a level comparable to the best supertagging results for English.</abstract>
      <url hash="7ea9e4ef">2010.jeptalnrecital-court.32</url>
      <bibkey>moot-2010-semi</bibkey>
    </paper>
    <paper id="33">
      <title>Exploitation de résultats d’analyse syntaxique pour extraction semi-supervisée des chemins de relations</title>
      <author><first>Yayoi</first><last>Nakamura-Delloye</last></author>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>195–200</pages>
      <abstract>Le présent article décrit un travail en cours sur l’acquisition des patrons de relations entre entités nommées à partir de résultats d’analyse syntaxique. Sans aucun patron prédéfini, notre méthode fournit des chemins syntaxiques susceptibles de représenter une relation donnée à partir de quelques exemples de couples d’entités nommées entretenant la relation en question.</abstract>
      <url hash="c5143987">2010.jeptalnrecital-court.33</url>
      <language>fra</language>
      <bibkey>nakamura-delloye-villemonte-de-la-clergerie-2010-exploitation</bibkey>
    </paper>
    <paper id="34">
      <title>Reconnaissance d’entités nommées : enrichissement d’un système à base de connaissances à partir de techniques de fouille de textes</title>
      <author><first>Damien</first><last>Nouvel</last></author>
      <author><first>Arnaud</first><last>Soulet</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Nathalie</first><last>Friburger</last></author>
      <author><first>Denis</first><last>Maurel</last></author>
      <pages>201–206</pages>
      <abstract>Dans cet article, nous présentons et analysons les résultats du système de reconnaissance d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la métonymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les performances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la reconnaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les résultats obtenus à ce jour et en faisons une première analyse.</abstract>
      <url hash="1312ac54">2010.jeptalnrecital-court.34</url>
      <language>fra</language>
      <bibkey>nouvel-etal-2010-reconnaissance</bibkey>
    </paper>
    <paper id="35">
      <title>Traduction de requêtes basée sur Wikipédia</title>
      <author><first>Benoît</first><last>Gaillard</last></author>
      <author><first>Olivier</first><last>Collin</last></author>
      <author><first>Malek</first><last>Boualem</last></author>
      <pages>207–212</pages>
      <abstract>Cet article s’inscrit dans le domaine de la recherche d’information multilingue. Il propose une méthode de traduction automatique de requêtes basée sur Wikipédia. Une phase d’analyse permet de segmenter la requête en syntagmes ou unités lexicales à traduire en s’appuyant sur les liens multilingues entre les articles de Wikipédia. Une deuxième phase permet de choisir, parmi les traductions possibles, celle qui est la plus cohérente en s’appuyant sur les informations d’ordre sémantique fournies par les catégories associées à chacun des articles de Wikipédia. Cet article justifie que les données issues de Wikipédia sont particulièrement pertinentes pour la traduction de requêtes, détaille l’approche proposée et son implémentation, et en démontre le potentiel par la comparaison du taux d’erreur du prototype de traduction avec celui d’autres services de traduction automatique.</abstract>
      <url hash="efe04fdd">2010.jeptalnrecital-court.35</url>
      <language>fra</language>
      <bibkey>gaillard-etal-2010-traduction</bibkey>
    </paper>
    <paper id="36">
      <title>Automatic Question Generation from Sentences</title>
      <author><first>Husam</first><last>Ali</last></author>
      <author><first>Yllias</first><last>Chali</last></author>
      <author><first>Sadid</first><last>A. Hasan</last></author>
      <pages>213–218</pages>
      <abstract>Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information. We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.</abstract>
      <url hash="b23972d5">2010.jeptalnrecital-court.36</url>
      <bibkey>ali-etal-2010-automatic</bibkey>
    </paper>
  </volume>
  <volume id="demonstration" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. Démonstrations</booktitle>
      <editor><first>Philippe</first><last>Langlais</last></editor>
      <editor><first>Michel</first><last>Gagnon</last></editor>
      <publisher>ATALA</publisher>
      <address>Montréal, Canada</address>
      <month>July</month>
      <year>2010</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="f8d9d46c">2010.jeptalnrecital-demonstration.0</url>
      <bibkey>jep-taln-recital-2010-actes-de-la-17e</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Présentation du logiciel Antidote <fixed-case>HD</fixed-case></title>
      <author><first>Éric</first><last>Brunelle</last></author>
      <author><first>Simon</first><last>Charest</last></author>
      <pages>1–3</pages>
      <abstract/>
      <url hash="3281e19c">2010.jeptalnrecital-demonstration.1</url>
      <language>fra</language>
      <bibkey>brunelle-charest-2010-presentation</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>T</fixed-case>ermino<fixed-case>W</fixed-case>eb : recherche et analyse d’information thématique</title>
      <author><first>Caroline</first><last>Barrière</last></author>
      <pages>4–7</pages>
      <abstract>Notre démonstration porte sur le prototype TerminoWeb, une plateforme Web qui permet (1) la construction automatique d’un corpus thématique à partir d’une recherche de documents sur le Web, (2) l’extraction de termes du corpus, et (3) la recherche d’information définitionnelle sur ces termes en corpus. La plateforme intégrant les trois modules, elle aidera un langagier (terminologue, traducteur, rédacteur) à découvrir un nouveau domaine (thème) en facilitant la recherche et l’analyse de documents informatifs pertinents à ce domaine.</abstract>
      <url hash="8c2e9232">2010.jeptalnrecital-demonstration.2</url>
      <language>fra</language>
      <bibkey>barriere-2010-terminoweb</bibkey>
    </paper>
    <paper id="3">
      <title>The i<fixed-case>MAG</fixed-case> concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of <fixed-case>MT</fixed-case> pretranslations</title>
      <author><first>Christian</first><last>Boitet</last></author>
      <author><first>Cong</first><last>Phap Huynh</last></author>
      <author><first>Hong</first><last>Thai Nguyen</last></author>
      <author><first>Valérie</first><last>Bellynck</last></author>
      <pages>8–15</pages>
      <abstract>We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site.</abstract>
      <url hash="2515b875">2010.jeptalnrecital-demonstration.3</url>
      <bibkey>boitet-etal-2010-imag</bibkey>
    </paper>
    <paper id="4">
      <title>L’intégration d’un outil de repérage d’entités nommées pour la langue arabe dans un système de veille</title>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <pages>16–19</pages>
      <abstract>Dans cette démonstration, nous présentons l’implémentation d’un outil de repérage d’entités nommées à base de règle pour la langue arabe dans le système de veille médiatique EMM (Europe Media Monitor).</abstract>
      <url hash="cfa9eae0">2010.jeptalnrecital-demonstration.4</url>
      <language>fra</language>
      <bibkey>zaghouani-2010-lintegration</bibkey>
    </paper>
    <paper id="5">
      <title>Expressive : Génération automatique de parole expressive à partir de données non linguistiques</title>
      <author><first>Olivier</first><last>Blanc</last></author>
      <author><first>Noémi</first><last>Boubel</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Sophie</first><last>Roekhaut</last></author>
      <author><first>Anne</first><last>Catherine Simon</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <author><first>Richard</first><last>Beaufort</last></author>
      <pages>20–23</pages>
      <abstract>Nous présentons Expressive, un système de génération de parole expressive à partir de données non linguistiques. Ce système est composé de deux outils distincts : Taittingen, un générateur automatique de textes d’une grande variété lexico-syntaxique produits à partir d’une représentation conceptuelle du discours, et StyloPhone, un système de synthèse vocale multi-styles qui s’attache à rendre le discours produit attractif et naturel en proposant différents styles vocaux.</abstract>
      <url hash="0681f306">2010.jeptalnrecital-demonstration.5</url>
      <language>fra</language>
      <bibkey>blanc-etal-2010-expressive</bibkey>
    </paper>
    <paper id="6">
      <title>Exploitation de Wikipédia pour l’Enrichissement et la Construction des Ressources Linguistiques</title>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <author><first>Alexandre</first><last>Terrasa</last></author>
      <pages>24–27</pages>
      <abstract>Cet article présente une approche et des résultats utilisant l’encyclopédie en ligne Wikipédia comme ressource semi-structurée de connaissances linguistiques et en particulier comme un corpus comparable pour l’extraction de terminologie bilingue. Cette approche tend à extraire d’abord des paires de terme et traduction à partir de types des informations, liens et textes de Wikipédia. L’étape suivante consiste à l’utilisation de l’information linguistique afin de ré-ordonner les termes et leurs traductions pertinentes et ainsi éliminer les termes cibles inutiles. Les évaluations préliminaires utilisant les paires de langues français-anglais, japonais-français et japonais-anglais ont montré une bonne qualité des paires de termes extraits. Cette étude est très favorable pour la construction et l’enrichissement des ressources linguistiques tels que les dictionnaires et ontologies multilingues. Aussi, elle est très utile pour un système de recherche d’information translinguistique (RIT).</abstract>
      <url hash="97f17136">2010.jeptalnrecital-demonstration.6</url>
      <language>fra</language>
      <bibkey>sadat-terrasa-2010-exploitation</bibkey>
    </paper>
    <paper id="7">
      <title>Traitement automatique des langues des signes : le projet <fixed-case>D</fixed-case>icta-<fixed-case>S</fixed-case>ign, des corpus aux applications</title>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <author><first>Jérémie</first><last>Segouat</last></author>
      <pages>28–31</pages>
      <abstract>Cet article présente Dicta-Sign, un projet de recherche sur le traitement automatique des langues des signes (LS), qui aborde un grand nombre de questions de recherche : linguistique de corpus, modélisation linguistique, reconnaissance et génération automatique. L’objectif de ce projet est de réaliser trois applications prototypes destinées aux usagers sourds : un traducteur de termes de LS à LS, un outil de recherche par l’exemple et un Wiki en LS. Pour cela, quatre corpus comparables de cinq heures de dialogue seront produits et analysés. De plus, des avancées significatives sont attendues dans le domaine des outils d’annotation. Dans ce projet, le LIMSI est en charge de l’élaboration des modèles linguistiques et participe aux aspects corpus et génération automatique. Nous nous proposons d’illustrer l’état d’avancement de Dicta-Sign au travers de vidéos extraites du corpus et de démonstrations des outils de traitement et de génération d’animations de signeur virtuel.</abstract>
      <url hash="1d23400e">2010.jeptalnrecital-demonstration.7</url>
      <language>fra</language>
      <bibkey>braffort-etal-2010-traitement</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>F</fixed-case>ips<fixed-case>C</fixed-case>olor : grammaire en couleur interactive pour l’apprentissage du français</title>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Kamel</first><last>Nebhi</last></author>
      <author><first>Christopher</first><last>Laenzlinger</last></author>
      <pages>32–35</pages>
      <abstract>L’analyseur multilingue FiPS permet de transformer une phrase en une structure syntaxique riche et accompagnée d’informations lexicales, grammaticales et thématiques. On décrit ici une application qui adapte les structures en constituants de l’analyseur FiPS à une nomenclature grammaticale permettant la représentation en couleur. Cette application interactive et disponible en ligne (<url>http://latl.unige.ch/fipscolor</url>) peut être utilisée librement par les enseignants et élèves de primaire.</abstract>
      <url hash="76b73bd8">2010.jeptalnrecital-demonstration.8</url>
      <language>fra</language>
      <bibkey>goldman-etal-2010-fipscolor</bibkey>
    </paper>
    <paper id="9">
      <title>Des cartes dialectologiques numérisées pour le <fixed-case>TALN</fixed-case></title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <pages>36–39</pages>
      <abstract>Cette démonstration présente une interface web pour des données numérisées de l’atlas linguistique de la Suisse allemande. Nous présentons d’abord l’intégration des données brutes et des données interpolées de l’atlas dans une interface basée sur Google Maps. Ensuite, nous montrons des prototypes de systèmes de traduction automatique et d’identification de dialectes qui s’appuient sur ces données dialectologiques numérisées.</abstract>
      <url hash="41500279">2010.jeptalnrecital-demonstration.9</url>
      <language>fra</language>
      <bibkey>scherrer-2010-des</bibkey>
    </paper>
    <paper id="10">
      <title>Text-it /Voice-it Une application mobile de normalisation des <fixed-case>SMS</fixed-case></title>
      <author><first>Richard</first><last>Beaufort</last></author>
      <author><first>Kévin</first><last>Macé</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <pages>40–43</pages>
      <abstract>Cet article présente Text-it / Voice-it, une application de normalisation des SMS pour téléphone mobile. L’application permet d’envoyer et de recevoir des SMS normalisés, et offre le choix entre un résultat textuel (Text-it) et vocal (Voice-it).</abstract>
      <url hash="72bc59e4">2010.jeptalnrecital-demonstration.10</url>
      <language>fra</language>
      <bibkey>beaufort-etal-2010-text</bibkey>
    </paper>
    <paper id="11">
      <title>Wide-Coverage <fixed-case>F</fixed-case>rench Syntax and Semantics using Grail</title>
      <author><first>Richard</first><last>Moot</last></author>
      <pages>44–47</pages>
      <abstract>The system demo introduces Grail, a general-purpose parser for multimodal categorial grammars, with special emphasis on recent research which makes Grail suitable for wide-coverage French syntax and semantics. These developments have been possible thanks to a categorial grammar which has been extracted semi-automatically from the Paris 7 treebank and a semantic lexicon which maps word, part-of-speech tags and formulas combinations to Discourse Representation Structures.</abstract>
      <url hash="5dd9d5d2">2010.jeptalnrecital-demonstration.11</url>
      <bibkey>moot-2010-wide</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>M</fixed-case>e<fixed-case>TAE</fixed-case> : Plate-forme d’annotation automatique et d’exploration sémantiques pour le domaine médical</title>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>48–51</pages>
      <abstract>Nous présentons une plate-forme d’annotation sémantique et d’exploration de textes médicaux, appelée « MeTAE ». Le processus d’annotation automatique comporte une première étape de reconnaissance des entités médicales présentes dans les textes suivie d’une étape d’identification des relations sémantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE génère des annotations RDF à partir des informations extraites et offre une interface d’exploration des textes annotés avec des requêtes sous forme de formulaire. La plate-forme peut être utilisée pour analyser sémantiquement les textes médicaux ou interroger la base d’annotation disponible pour avoir une/des réponses à une requête donnée (e.g. « ?X prévient maladie d’Alzheimer », équivalent à la question « comment prévenir la maladie d’Alzheimer ? »). Cette application peut être la base d’un système de questions-réponses pour le domaine médical.</abstract>
      <url hash="377a963b">2010.jeptalnrecital-demonstration.12</url>
      <language>fra</language>
      <bibkey>ben-abacha-zweigenbaum-2010-metae</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>LEOPAR</fixed-case>, un analyseur syntaxique pour les grammaires d’interaction</title>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>52–55</pages>
      <abstract>Nous présentons ici l’analyseur syntaxique LEOPAR basé sur les grammaires d’interaction ainsi que d’autres outils utiles pour notre chaîne de traitement syntaxique.</abstract>
      <url hash="04021800">2010.jeptalnrecital-demonstration.13</url>
      <language>fra</language>
      <bibkey>guillaume-perrier-2010-leopar</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>T</fixed-case>rans<fixed-case>S</fixed-case>earch : un moteur de recherche de traductions</title>
      <author><first>Julien</first><last>Bourdaillet</last></author>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>56–59</pages>
      <abstract>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Cette démonstration vise à présenter le moteur de recherche de traductions TransSearch. Cetteapplication commerciale, accessible sur leWeb, repose d’une part sur l’exploitation d’un bitexte aligné au niveau des phrases, et d’autre part sur des modèles statistiques d’alignement de mots.</abstract>
      <url hash="3f6c9b2e">2010.jeptalnrecital-demonstration.14</url>
      <language>fra</language>
      <bibkey>bourdaillet-etal-2010-transsearch</bibkey>
    </paper>
    <paper id="15">
      <title>Moz: Translation of Structured Terminology-Rich Text</title>
      <author><first>Graham</first><last>Russell</last></author>
      <pages>60–63</pages>
      <abstract>Description of Moz, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.</abstract>
      <url hash="2024c154">2010.jeptalnrecital-demonstration.15</url>
      <bibkey>russell-2010-moz</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>MACAON</fixed-case> Une chaîne linguistique pour le traitement de graphes de mots</title>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Jean-François</first><last>Rey</last></author>
      <pages>64–67</pages>
      <abstract/>
      <url hash="e3678c0b">2010.jeptalnrecital-demonstration.16</url>
      <language>fra</language>
      <bibkey>nasr-etal-2010-macaon</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Alexandre</first><last>Patry</last></editor>
      <editor><first>Philippe</first><last>Langlais</last></editor>
      <editor><first>Aurélien</first><last>Max</last></editor>
      <publisher>ATALA</publisher>
      <address>Montréal, Canada</address>
      <month>July</month>
      <year>2010</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="03ea44c4">2010.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2010-actes-de-la-17e-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Attribution d’auteur au moyen de modèles de langue et de modèles stylométriques</title>
      <author><first>Audrey</first><last>Laroche</last></author>
      <pages>1–10</pages>
      <abstract>Dans une tâche consistant à trouver l’auteur (parmi 53) de chacun de 114 textes, nous analysons la performance de modèles de langue et de modèles stylométriques sous les angles du rappel et du nombre de paramètres. Le modèle de mots bigramme à lissage de Kneser-Ney modifié interpolé est le plus performant (75 % de bonnes réponses au premier rang). Parmi les modèles stylométriques, une combinaison de 7 paramètres liés aux parties du discours produit les meilleurs résultats (rappel de 25 % au premier rang). Dans les deux catégories de modèles, le rappel maximal n’est pas atteint lorsque le nombre de paramètres est le plus élevé.</abstract>
      <url hash="968e5dcb">2010.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>laroche-2010-attribution</bibkey>
    </paper>
    <paper id="2">
      <title>Densidées : calcul automatique de la densité des idées dans un corpus oral</title>
      <author><first>Hyeran</first><last>Lee</last></author>
      <author><first>Philippe</first><last>Gambette</last></author>
      <author><first>Elsa</first><last>Maillé</last></author>
      <author><first>Constance</first><last>Thuillier</last></author>
      <pages>11–20</pages>
      <abstract>La densité des idées, qui correspond au ratio entre le nombre de propositions sémantiques et le nombre de mots dans un texte reflète la qualité informative des propositions langagières d’un texte. L’apparition de la maladie d’Alzheimer a été reliée à une dégradation de la densité des idées, ce qui explique l’intérêt pour un calcul automatique de cette mesure. Nous proposons une méthode basée sur un étiquetage morphosyntaxique et des règles d’ajustement, inspirée du logiciel CPIDR. Cette méthode a été validée sur un corpus de quarante entretiens oraux transcrits et obtient de meilleurs résultats pour le français que CPIDR pour l’anglais. Elle est implémentée dans le logiciel libre Densidées disponible sur <url>http://code.google.com/p/densidees</url>.</abstract>
      <url hash="3dc32059">2010.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>lee-etal-2010-densidees</bibkey>
    </paper>
    <paper id="3">
      <title>Outils de segmentation du chinois et textométrie</title>
      <author><first>Li-Chi</first><last>Wu</last></author>
      <pages>21–33</pages>
      <abstract>La segmentation en mots est une première étape possible dans le traitement automatique de la langue chinoise. Les systèmes de segmentation se sont beaucoup développés depuis le premier apparu dans les années 1980. Il n’existe cependant aucun outil standard aujourd’hui. L’objectif de ce travail est de faire une comparaison des différents outils de segmentation en s’appuyant sur une analyse statistique. Le but est de définir pour quel type de texte chacun d’eux est le plus performant. Quatre outils de segmentation et deux corpus avec des thèmes distincts ont été choisis pour cette étude. À l’aide des outils textométriques Lexico3 et mkAlign, nous avons centré notre analyse sur le nombre de syllabes du chinois. Les données quantitatives ont permis d’objectiver des différences entre les outils. Le système Hylanda s’avère performant dans la segmentation des termes spécialisés et le système Stanford est plus indiqué pour les textes généraux. L’étude de la comparaison des outils de segmentation montre le statut incontournable de l’analyse textométrique aujourd’hui, celle-ci permettant d’avoir accès rapidement à la recherche d’information.</abstract>
      <url hash="ee84becd">2010.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>wu-2010-outils</bibkey>
    </paper>
    <paper id="4">
      <title>Acquisition de grammaires locales pour l’extraction de relations entre entités nommées</title>
      <author><first>Mani</first><last>Ezzat</last></author>
      <pages>34–43</pages>
      <abstract>La constitution de ressources linguistiques est une tâche cruciale pour les systèmes d’extraction d’information fondés sur une approche symbolique. Ces systèmes reposent en effet sur des grammaires utilisant des informations issues de dictionnaires électroniques ou de réseaux sémantiques afin de décrire un phénomène linguistique précis à rechercher dans les textes. La création et la révision manuelle de telles ressources sont des tâches longues et coûteuses en milieu industriel. Nous présentons ici un nouvel algorithme produisant une grammaire d’extraction de relations entre entités nommées, de manière semi-automatique à partir d’un petit ensemble de phrases représentatives. Dans un premier temps, le linguiste repère un jeu de phrases pertinentes à partir d’une analyse des cooccurrences d’entités repérées automatiquement. Cet échantillon n’a pas forcément une taille importante. Puis, un algorithme permet de produire une grammaire en généralisant progressivement les éléments lexicaux exprimant la relation entre entités. L’originalité de l’approche repose sur trois aspects : une représentation riche du document initial permettant des généralisations pertinentes, la collaboration étroite entre les aspects automatiques et l’apport du linguiste et sur la volonté de contrôler le processus en ayant toujours affaire à des données lisibles par un humain.</abstract>
      <url hash="d2dfbb3c">2010.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>ezzat-2010-acquisition</bibkey>
    </paper>
    <paper id="5">
      <title>Construction d’un corpus de paraphrases d’énoncés par traduction multiple multilingue</title>
      <author><first>Houda</first><last>Bouamor</last></author>
      <pages>44–53</pages>
      <abstract>Les corpus de paraphrases à large échelle sont importants dans de nombreuses applications de TAL. Dans cet article nous présentons une méthode visant à obtenir un corpus parallèle de paraphrases d’énoncés en français. Elle vise à collecter des traductions multiples proposées par des contributeurs volontaires francophones à partir de plusieurs langues européennes. Nous formulons l’hypothèse que deux traductions soumises indépendamment par deux participants conservent généralement le sens de la phrase d’origine, quelle que soit la langue à partir de laquelle la traduction est effectuée. L’analyse des résultats nous permet de discuter cette hypothèse.</abstract>
      <url hash="d3de4bee">2010.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>bouamor-2010-construction</bibkey>
    </paper>
    <paper id="6">
      <title>Ces noms qui cachent des événements : un premier repérage</title>
      <author><first>Amaria</first><last>Adila Bouabdallah</last></author>
      <pages>54–63</pages>
      <abstract>La détection des informations temporelles est cruciale pour le traitement automatique des textes, qu’il s’agisse de modélisation linguistique, d’applications en compréhension du langage ou encore de tâches de recherche documentaire ou d’extraction d’informations. De nombreux travaux ont été dédiés à l’analyse temporelle des textes, et plus précisément l’annotation des expressions temporelles ou des événements sous leurs différentes formes : verbales, adjectivales ou nominales. Dans cet article, nous décrivons une méthode pour la détection des syntagmes nominaux dénotant des événements. Notre approche est basée sur l’implémentation d’un test linguistique simple proposé par les linguistes pour cette tâche. Nous avons expérimenté notre méthode sur deux corpus différents ; le premier est composé d’articles de presse et le second est beaucoup plus grand, utilisant une interface pour interroger automatiquement le moteur de recherche Yahoo. Les résultats obtenus ont montré que cette méthode se révèle plus pertinente pour un plus large corpus.</abstract>
      <url hash="c4bc6e0e">2010.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>adila-bouabdallah-2010-ces</bibkey>
    </paper>
    <paper id="7">
      <title>Catégorisation automatique d’adjectifs d’opinion à partir d’une ressource linguistique générique</title>
      <author><first>Baptiste</first><last>Chardon</last></author>
      <pages>64–73</pages>
      <abstract>Cet article décrit un processus d’annotation manuelle de textes d’opinion, basé sur un schéma fin d’annotation indépendant de la langue et du corpus. Ensuite, à partir d’une partie de ce schéma, une méthode de construction automatique d’un lexique d’opinion à partir d’un analyseur syntaxique et d’une ressource linguistique est décrite. Cette méthode consiste à construire un arbre de décision basé sur les classes de concepts de la ressource utilisée. Dans un premier temps, nous avons étudié la couverture du lexique d’opinion obtenu par comparaison avec l’annotation manuelle effectuée sur un premier corpus de critiques de restaurants. La généricité de ce lexique a été mesurée en le comparant avec un second lexique, généré à partir d’un corpus de commentaires de films. Dans un second temps, nous avons évalué l’utilisabilité du lexique au travers d’une tâche extrinsèque, la reconnaissance de la polarité de commentaires d’internautes.</abstract>
      <url hash="7afb7c6a">2010.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>chardon-2010-categorisation</bibkey>
    </paper>
    <paper id="8">
      <title>Résumé automatique de documents arabes basé sur la technique <fixed-case>RST</fixed-case></title>
      <author><first>Mohamed</first><last>Hédi Maâloul</last></author>
      <author><first>Iskandar</first><last>Keskes</last></author>
      <pages>74–82</pages>
      <abstract>Dans cet article, nous nous intéressons au résumé automatique de textes arabes. Nous commençons par présenter une étude analytique réalisée sur un corpus de travail qui nous a permis de déduire, suite à des observations empiriques, un ensemble de relations et de frames (règles ou patrons) rhétoriques; ensuite nous présentons notre méthode de production de résumés pour les textes arabes. La méthode que nous proposons se base sur la Théorie de la Structure Rhétorique (RST) (Mann et al., 1988) et utilise des connaissances purement linguistiques. Le principe de notre proposition s’appuie sur trois piliers. Le premier pilier est le repérage des relations rhétoriques entres les différentes unités minimales du texte dont l’une possède le statut de noyau – segment de texte primordial pour la cohérence – et l’autre a le statut noyau ou satellite – segment optionnel. Le deuxième pilier est le dressage et la simplification de l’arbre RST. Le troisième pilier est la sélection des phrases noyaux formant le résumé final, qui tiennent en compte le type de relation rhétoriques choisi pour l’extrait.</abstract>
      <url hash="682f16b7">2010.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>hedi-maaloul-keskes-2010-resume</bibkey>
    </paper>
    <paper id="9">
      <title>Inférences aspecto-temporelles analysées avec la Logique Combinatoire</title>
      <author><first>Hee-Jin</first><last>Ro</last></author>
      <pages>83–92</pages>
      <abstract>Ce travail s’inscrit dans une recherche centrée sur une approche de l’Intelligence Artificielle (IA) et de la linguistique computationnelle. Il permet d’intégrer différentes techniques formelles de la Logique Combinatoire avec des types (Curry) et sa programmation fonctionnelle (Haskell) avec une théorie énonciative du temps et de l’aspect. Nous proposons des calculs formels de valeurs aspectotemporelles (processus inaccompli présent, processus inaccompli passé, événement passé et étatrésultant présent) associées à des représentations de significations verbales sous forme de schèmes applicatifs.</abstract>
      <url hash="466adc1c">2010.jeptalnrecital-recital.9</url>
      <language>fra</language>
      <bibkey>ro-2010-inferences</bibkey>
    </paper>
    <paper id="10">
      <title>Automatiser la rédaction de définitions terminographiques : questions et traitements</title>
      <author><first>Selja</first><last>Seppälä</last></author>
      <pages>93–102</pages>
      <abstract>Dans cet article, nous présentons une analyse manuelle de corpus de contextes conceptuels afin (i) de voir dans quelle mesure les méthodes de TALN existantes sont en principe adéquates pour automatiser la rédaction de définitions terminographiques, et (ii) de dégager des question précises dont la résolution permettrait d’automatiser davantage la production de définitions. Le but est de contribuer à la réflexion sur les enjeux de l’automatisation de cette tâche, en procédant à une série d’analyses qui nous mènent, étape par étape, à examiner l’adéquation des méthodes d’extraction de définitions et de contextes plus larges au travail terminographique de rédaction des définitions. De ces analyses émergent des questions précises relatives à la pertinence des informations extraites et à leur sélection. Des propositions de solutions et leurs implications pour le TALN sont examinées.</abstract>
      <url hash="e84d40b4">2010.jeptalnrecital-recital.10</url>
      <language>fra</language>
      <bibkey>seppala-2010-automatiser</bibkey>
    </paper>
    <paper id="11">
      <title>Représentation vectorielle de textes courts d’opinions, Analyse de traitements sémantiques pour la fouille d’opinions par clustering</title>
      <author><first>Benoît</first><last>Trouvilliez</last></author>
      <pages>103–112</pages>
      <abstract>Avec le développement d’internet et des sites d’échanges (forums, blogs, sondages en ligne, ...), l’exploitation de nouvelles sources d’informations dans le but d’en extraire des opinions sur des sujets précis (film, commerce,...) devient possible. Dans ce papier, nous présentons une approche de fouille d’opinions à partir de textes courts. Nous expliquons notamment en quoi notre choix d’utilisation de regroupements autour des idées exprimées nous a conduit à opter pour une représentation implicite telle que la représentation vectorielle. Nous voyons également les différents traitements sémantiques intégrés à notre chaîne de traitement (traitement de la négation, lemmatisation, stemmatisation, synonymie ou même polysémie des mots) et discutons leur impact sur la qualité des regroupements obtenus.</abstract>
      <url hash="99050821">2010.jeptalnrecital-recital.11</url>
      <language>fra</language>
      <bibkey>trouvilliez-2010-representation</bibkey>
    </paper>
  </volume>
</collection>
