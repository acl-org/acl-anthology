<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.readi">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)</booktitle>
      <editor><first>Núria</first><last>Gala</last></editor>
      <editor><first>Rodrigo</first><last>Wilkens</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-45-0</isbn>
    </meta>
    <frontmatter>
      <url hash="7f25a2c1">2020.readi-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Disambiguating Confusion Sets as an Aid for Dyslexic Spelling</title>
      <author><first>Steinunn Rut</first><last>Friðriksdóttir</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>1–5</pages>
      <abstract>Spell checkers and other proofreading software are crucial tools for people with dyslexia and other reading disabilities. Most spell checkers automatically detect spelling mistakes by looking up individual words and seeing if they exist in the vocabulary. However, one of the biggest challenges of automatic spelling correction is how to deal with real-word errors, i.e. spelling mistakes which lead to a real but unintended word, such as when then is written in place of than. These errors account for 20% of all spelling mistakes made by people with dyslexia. As both words exist in the vocabulary, a simple dictionary lookup will not detect the mistake. The only way to disambiguate which word was actually intended is to look at the context in which the word appears. This problem is particularly apparent in languages with rich morphology where there is often minimal orthographic difference between grammatical items. In this paper, we present our novel confusion set corpus for Icelandic and discuss how it could be used for context-sensitive spelling correction. We have collected word pairs from seven different categories, chosen for their homophonous properties, along with sentence examples and frequency information from said pairs. We present a small-scale machine learning experiment using a decision tree binary classification which results range from 73% to 86% average accuracy with 10-fold cross validation. While not intended as a finalized result, the method shows potential and will be improved in future research.</abstract>
      <url hash="c2ddf349">2020.readi-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>Is it simpler? An Evaluation of an Aligned Corpus of Standard-Simple Sentences</title>
      <author><first>Evelina</first><last>Rennes</last></author>
      <pages>6–13</pages>
      <abstract>Parallel monolingual resources are imperative for data-driven sentence simplification research. We present the work of aligning, at the sentence level, a corpus of all Swedish public authorities and municipalities web texts in standard and simple Swedish. We compare the performance of three alignment algorithms used for similar work in English (Average Alignment, Maximum Alignment, and Hungarian Alignment), and the best-performing algorithm is used to create a resource of 15,433 unique sentence pairs. We evaluate the resulting corpus using a set of features that has proven to predict text complexity of Swedish texts. The results show that the sentences of the simple sub-corpus are indeed less complex than the sentences of the standard part of the corpus, according to many of the text complexity measures.</abstract>
      <url hash="ca1871b5">2020.readi-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Incorporating Multiword Expressions in Phrase Complexity Estimation</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last></author>
      <pages>14–19</pages>
      <abstract>Multiword expressions (MWEs) were shown to be useful in a number of NLP tasks. However, research on the use of MWEs in lexical complexity assessment and simplification is still an under-explored area. In this paper, we propose a text complexity assessment system for English, which incorporates MWE identification. We show that detecting MWEs using state-of-the-art systems improves predicting complexity on an established lexical complexity dataset.</abstract>
      <url hash="c67f0831">2020.readi-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Automatically Assess Children’s Reading Skills</title>
      <author><first>Ornella</first><last>Mich</last></author>
      <author><first>Nadia</first><last>Mana</last></author>
      <author><first>Roberto</first><last>Gretter</last></author>
      <author><first>Marco</first><last>Matassoni</last></author>
      <author><first>Daniele</first><last>Falavigna</last></author>
      <pages>20–26</pages>
      <abstract>Assessing reading skills is an important task teachers have to perform at the beginning of a new scholastic year to evaluate the starting level of the class and properly plan next learning activities. Digital tools based on automatic speech recognition (ASR) may be really useful to support teachers in this task, currently very time consuming and prone to human errors. This paper presents a web application for automatically assessing fluency and accuracy of oral reading in children attending Italian primary and lower secondary schools. Our system, based on ASR technology, implements the Cornoldi’s MT battery, which is a well-known Italian test to assess reading skills. The front-end of the system has been designed following the participatory design approach by involving end users from the beginning of the creation process. Teachers may use our system to both test student’s reading skills and monitor their performance over time. In fact, the system offers an effective graphical visualization of the assessment results for both individual students and entire class. The paper also presents the results of a pilot study to evaluate the system usability with teachers.</abstract>
      <url hash="66aabc96">2020.readi-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Text Simplification to Help Individuals with Low Vision Read More Fluently</title>
      <author><first>Lauren</first><last>Sauvan</last></author>
      <author><first>Natacha</first><last>Stolowy</last></author>
      <author><first>Carlos</first><last>Aguilar</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Núria</first><last>Gala</last></author>
      <author><first>Frédéric</first><last>Matonti</last></author>
      <author><first>Eric</first><last>Castet</last></author>
      <author><first>Aurélie</first><last>Calabrèse</last></author>
      <pages>27–32</pages>
      <abstract>The objective of this work is to introduce text simplification as a potential reading aid to help improve the poor reading performance experienced by visually impaired individuals. As a first step, we explore what makes a text especially complex when read with low vision, by assessing the individual effect of three word properties (frequency, orthographic similarity and length) on reading speed in the presence of Central visual Field Loss (CFL). Individuals with bilateral CFL induced by macular diseases read pairs of French sentences displayed with the self-paced reading method. For each sentence pair, sentence n contained a target word matched with a synonym word of the same length included in sentence n+1. Reading time was recorded for each target word. Given the corpus we used, our results show that (1) word frequency has a significant effect on reading time (the more frequent the faster the reading speed) with larger amplitude (in the range of seconds) compared to normal vision; (2) word neighborhood size has a significant effect on reading time (the more neighbors the slower the reading speed), this effect being rather small in amplitude, but interestingly reversed compared to normal vision; (3) word length has no significant effect on reading time. Supporting the development of new and more effective assistive technology to help low vision is an important and timely issue, with massive potential implications for social and rehabilitation practices. The end goal of this project will be to use our findings to custom text simplification to this specific population and use it as an optimal and efficient reading aid.</abstract>
      <url hash="af173928">2020.readi-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Identifying Abstract and Concrete Words in <fixed-case>F</fixed-case>rench to Better Address Reading Difficulties</title>
      <author><first>Daria</first><last>Goriachun</last></author>
      <author><first>Núria</first><last>Gala</last></author>
      <pages>33–40</pages>
      <abstract>Literature in psycholinguistics and neurosciences has showed that abstract and concrete concepts are perceived differently by our brain, and that the abstractness of a word can cause difficulties in reading. In order to integrate this parameter into an automatic text simplification (ATS) system for French readers, an annotated list with 7,898 abstract and concrete nouns has been semi-automatically developed. Our aim was to obtain abstract and concrete nouns from an initial manually annotated short list by using two distributional approaches: nearest neighbors and syntactic co-occurrences. The results of this experience have enabled to shed light on the different behaviors of concrete and abstract nouns in context. Besides, the final list, a resource per se in French available on demand, provides a valuable contribution since annotated resources based on cognitive variables such as concreteness or abstractness are scarce and very difficult to obtain. In future work, the list will be enlarged and integrated into an existing lexicon with ranked synonyms for the identification of complex words in text simplification applications.</abstract>
      <url hash="89de065c">2020.readi-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>Benchmarking Data-driven Automatic Text Simplification for <fixed-case>G</fixed-case>erman</title>
      <author><first>Andreas</first><last>Säuberli</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>41–48</pages>
      <abstract>Automatic text simplification is an active research area, and there are first systems for English, Spanish, Portuguese, and Italian. For German, no data-driven approach exists to this date, due to a lack of training data. In this paper, we present a parallel corpus of news items in German with corresponding simplifications on two complexity levels. The simplifications have been produced according to a well-documented set of guidelines. We then report on experiments in automatically simplifying the German news items using state-of-the-art neural machine translation techniques. We demonstrate that despite our small parallel corpus, our neural models were able to learn essential features of simplified language, such as lexical substitutions, deletion of less relevant words and phrases, and sentence shortening.</abstract>
      <url hash="3c6516b6">2020.readi-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title>Visualizing Facets of Text Complexity across Registers</title>
      <author><first>Marina</first><last>Santini</last></author>
      <author><first>Arne</first><last>Jonsson</last></author>
      <author><first>Evelina</first><last>Rennes</last></author>
      <pages>49–56</pages>
      <abstract>In this paper, we propose visualizing results of a corpus-based study on text complexity using radar charts. We argue that the added value of this type of visualisation is the polygonal shape that provides an intuitive grasp of text complexity similarities across the registers of a corpus. The results that we visualize come from a study where we explored whether it is possible to automatically single out different facets of text complexity across the registers of a Swedish corpus. To this end, we used factor analysis as applied in Biber’s Multi-Dimensional Analysis framework. The visualization of text complexity facets with radar charts indicates that there is correspondence between linguistic similarity and similarity of shape across registers.</abstract>
      <url hash="84f47a68">2020.readi-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>omp<fixed-case>L</fixed-case>ex — A New Corpus for Lexical Complexity Prediction from <fixed-case>L</fixed-case>ikert <fixed-case>S</fixed-case>cale Data</title>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Michael</first><last>Cooper</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>57–62</pages>
      <abstract>Predicting which words are considered hard to understand for a given target population is a vital step in many NLP applications such astext simplification. This task is commonly referred to as Complex Word Identification (CWI). With a few exceptions, previous studieshave approached the task as a binary classification task in which systems predict a complexity value (complex vs. non-complex) fora set of target words in a text. This choice is motivated by the fact that all CWI datasets compiled so far have been annotated using abinary annotation scheme. Our paper addresses this limitation by presenting the first English dataset for continuous lexical complexityprediction. We use a 5-point Likert scale scheme to annotate complex words in texts from three sources/domains: the Bible, Europarl,and biomedical texts. This resulted in a corpus of 9,476 sentences each annotated by around 7 annotators.</abstract>
      <url hash="b5a1aba3">2020.readi-1.9</url>
      <language>eng</language>
      <revision id="1" href="2020.readi-1.9v1" hash="54aadc01"/>
      <revision id="2" href="2020.readi-1.9v2" hash="b5a1aba3" date="2020-07-08">Fixes typo in the title</revision>
    </paper>
    <paper id="10">
      <title><fixed-case>L</fixed-case>agun<fixed-case>T</fixed-case>est: A <fixed-case>NLP</fixed-case> Based Application to Enhance Reading Comprehension</title>
      <author><first>Kepa</first><last>Bengoetxea</last></author>
      <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
      <author><first>Amaia</first><last>Aguirregoitia</last></author>
      <pages>63–69</pages>
      <abstract>The ability to read and understand written texts plays an important role in education, above all in the last years of primary education. This is especially pertinent in language immersion educational programmes, where some students have low linguistic competence in the languages of instruction. In this context, adapting the texts to the individual needs of each student requires a considerable effort by education professionals. However, language technologies can facilitate the laborious adaptation of materials in order to enhance reading comprehension. In this paper, we present LagunTest, a NLP based application that takes as input a text in Basque or English, and offers synonyms, definitions, examples of the words in different contexts and presents some linguistic characteristics as well as visualizations. LagunTest is based on reusable and open multilingual and multimodal tools, and it is also distributed with an open license. LagunTest is intended to ease the burden of education professionals in the task of adapting materials, and the output should always be supervised by them.</abstract>
      <url hash="6cf9848c">2020.readi-1.10</url>
      <language>eng</language>
    </paper>
    <paper id="11">
      <title>A Lexical Simplification Tool for Promoting Health Literacy</title>
      <author><first>Leonardo</first><last>Zilio</last></author>
      <author><first>Liana</first><last>Braga Paraguassu</last></author>
      <author><first>Luis Antonio</first><last>Leiva Hercules</last></author>
      <author><first>Gabriel</first><last>Ponomarenko</last></author>
      <author><first>Laura</first><last>Berwanger</last></author>
      <author><first>Maria José</first><last>Bocorny Finatto</last></author>
      <pages>70–76</pages>
      <abstract>This paper presents MedSimples, an authoring tool that combines Natural Language Processing, Corpus Linguistics and Terminology to help writers to convert health-related information into a more accessible version for people with low literacy skills. MedSimples applies parsing methods associated with lexical resources to automatically evaluate a text and present simplification suggestions that are more suitable for the target audience. Using the suggestions provided by the tool, the author can adapt the original text and make it more accessible. The focus of MedSimples lies on texts for special purposes, so that it not only deals with general vocabulary, but also with specialized terms. The tool is currently under development, but an online working prototype exists and can be tested freely. An assessment of MedSimples was carried out aiming at evaluating its current performance with some promising results, especially for informing the future developments that are planned for the tool.</abstract>
      <url hash="68c9b2a5">2020.readi-1.11</url>
      <language>eng</language>
    </paper>
    <paper id="12">
      <title>A multi-lingual and cross-domain analysis of features for text simplification</title>
      <author><first>Regina</first><last>Stodden</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <pages>77–84</pages>
      <abstract>In text simplification and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, sentence length, or proportion of POS tags. These features are however mainly developed for English. In this paper, we investigate their relevance for Czech, German, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different features for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 features within the web domain corpora. Overall, the negative statistical tests regarding the other features across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.</abstract>
      <url hash="34148313">2020.readi-1.12</url>
      <language>eng</language>
    </paper>
    <paper id="13">
      <title>Combining Expert Knowledge with Frequency Information to Infer <fixed-case>CEFR</fixed-case> Levels for Words</title>
      <author><first>Alice</first><last>Pintard</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>85–92</pages>
      <abstract>Traditional approaches to set goals in second language (L2) vocabulary acquisition relied either on word lists that were obtained from large L1 corpora or on collective knowledge and experience of L2 experts, teachers, and examiners. Both approaches are known to offer some advantages, but also to have some limitations. In this paper, we try to combine both sources of information, namely the official reference level description for French language and the FLElex lexical database. Our aim is to train a statistical model on the French RLD that would be able to turn the distributional information from FLElex into one of the six levels of the Common European Framework of Reference for languages (CEFR). We show that such approach yields a gain of 29% in accuracy compared to the method currently used in the CEFRLex project. Besides, our experiments also offer deeper insights into the advantages and shortcomings of the two traditional sources of information (frequency vs. expert knowledge).</abstract>
      <url hash="722255bf">2020.readi-1.13</url>
      <language>eng</language>
    </paper>
    <paper id="14">
      <title>Coreference-Based Text Simplification</title>
      <author><first>Rodrigo</first><last>Wilkens</last></author>
      <author><first>Bruno</first><last>Oberle</last></author>
      <author><first>Amalia</first><last>Todirascu</last></author>
      <pages>93–100</pages>
      <abstract>Text simplification aims at adapting documents to make them easier to read by a given audience. Usually, simplification systems consider only lexical and syntactic levels, and, moreover, are often evaluated at the sentence level. Thus, studies on the impact of simplification in text cohesion are lacking. Some works add coreference resolution in their pipeline to address this issue. In this paper, we move forward in this direction and present a rule-based system for automatic text simplification, aiming at adapting French texts for dyslexic children. The architecture of our system takes into account not only lexical and syntactic but also discourse information, based on coreference chains. Our system has been manually evaluated in terms of grammaticality and cohesion. We have also built and used an evaluation corpus containing multiple simplification references for each sentence. It has been annotated by experts following a set of simplification guidelines, and can be used to run automatic evaluation of other simplification systems. Both the system and the evaluation corpus are freely available.</abstract>
      <url hash="b4fa1214">2020.readi-1.14</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
