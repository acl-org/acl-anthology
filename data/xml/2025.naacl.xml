<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.naacl">
  <volume id="long" ingest-date="2025-04-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</booktitle>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>April</month>
      <year>2025</year>
      <url hash="3d691192">2025.naacl-long</url>
      <venue>naacl</venue>
      <isbn>979-8-89176-189-6</isbn>
    </meta>
    <frontmatter>
      <url hash="43ffc4cf">2025.naacl-long.0</url>
      <bibkey>naacl-2025-long</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Understanding Figurative Meaning through Explainable Visual Entailment</title>
      <author><first>Arkadiy</first><last>Saakyan</last><affiliation>Columbia University</affiliation></author>
      <author><first>Shreyas</first><last>Kulkarni</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last><affiliation>SalesForce Research and State University of New York at Stony Brook</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <pages>1-23</pages>
      <abstract>Large Vision-Language Models (VLMs) have demonstrated strong capabilities in tasks requiring a fine-grained understanding of literal meaning in images and text, such as visual question-answering or visual entailment. However, there has been little exploration of the capabilities of these models when presented with images and captions containing figurative meaning, such as metaphors or humor. To close this gap, we propose a new task framing the figurative meaning understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a caption (hypothesis) and justify the predicted label with a textual explanation. The figurative phenomena can be present in the image, in the caption, or both. Using a human-AI collaboration approach, we build the accompanying expert-verified dataset V-FLUTE, containing 6,027 image, caption, label, explanation instances spanning five diverse figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. Through automatic evaluation, we find that VLMs struggle to generalize from literal to figurative meaning, particularly when it is present in images. Further, we identify common types of errors in VLM reasoning (hallucination and incomplete or unsound reasoning) across classes of models via human evaluation.</abstract>
      <url hash="04af6c18">2025.naacl-long.1</url>
      <bibkey>saakyan-etal-2025-understanding</bibkey>
    </paper>
    <paper id="2">
      <title>Benchmarking Distributional Alignment of Large Language Models</title>
      <author><first>Nicole</first><last>Meister</last></author>
      <author><first>Carlos</first><last>Guestrin</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tatsunori</first><last>Hashimoto</last><affiliation>Stanford University</affiliation></author>
      <pages>24-49</pages>
      <abstract>Language models (LMs) are increasingly used as simulacra for people, yet their ability to match the distribution of views of a specific demographic group and be <i>distributionally aligned</i> remains uncertain. This notion of distributional alignment is complex, as there is significant variation in the types of attributes that are simulated. Prior works have underexplored the role of three critical variables—the question domain, steering method, and distribution expression method—which motivates our contribution of a benchmark explicitly addressing these dimensions. We construct a dataset expanding beyond political values, create human baselines for this task, and evaluate the extent to which an LM can align with a particular group’s opinion distribution to inform design choices of such simulation systems. Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions.</abstract>
      <url hash="f0c95956">2025.naacl-long.2</url>
      <bibkey>meister-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="3">
      <title>World Models with Hints of Large Language Models for Goal Achieving</title>
      <author><first>Zeyuan</first><last>Liu</last></author>
      <author><first>Ziyu</first><last>Huan</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Xiyao</first><last>Wang</last></author>
      <author><first>Jiafei</first><last>Lyu</last></author>
      <author><first>Jian</first><last>Tao</last></author>
      <author><first>Xiu</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Huazhe</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>50-72</pages>
      <abstract>Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 41.8%, 21.1%, and 9.9%, respectively.</abstract>
      <url hash="6073bc60">2025.naacl-long.3</url>
      <bibkey>liu-etal-2025-world</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>C</fixed-case>og<fixed-case>LM</fixed-case>: Tracking Cognitive Development of Large Language Models</title>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Heda</first><last>Wang</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>73-87</pages>
      <abstract>Piaget’s Theory of Cognitive Development (PTC) posits that the development of cognitive levels forms the foundation for human learning across various abilities. As Large Language Models (LLMs) have recently shown remarkable abilities across a wide variety of tasks, we are curious about the cognitive levels of current LLMs: to what extent they have developed and how this development has been achieved. To this end, we construct a benchmark CogLM (Cognitive Ability Evaluation for Language Model) based on PTC to assess the cognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive abilities crafted by more than 20 human experts, providing a comprehensive testbed for the cognitive levels of LLMs. Through extensive experiments across multiple mainstream LLMs with CogLM, we find that: (1) In our testing framework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive abilities, comparable to those of a 20-year-old human. (2) The parameter size and optimization objective are two key factors affecting the cognitive levels of LLMs. (3) The performance on downstream tasks is positively correlated with the level of cognitive abilities. These findings fill the gap in research on the cognitive abilities of LLMs, tracing the development of LLMs from a cognitive perspective and guiding the future direction of their evolution.</abstract>
      <url hash="fb195c46">2025.naacl-long.4</url>
      <bibkey>wang-etal-2025-coglm</bibkey>
    </paper>
    <paper id="5">
      <title>Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities</title>
      <author><first>Minh Duc</first><last>Chu</last></author>
      <author><first>Zihao</first><last>He</last></author>
      <author><first>Rebecca</first><last>Dorn</last></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <pages>88-111</pages>
      <abstract>Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research.</abstract>
      <url hash="0ea6f675">2025.naacl-long.5</url>
      <bibkey>chu-etal-2025-improving</bibkey>
    </paper>
    <paper id="6">
      <title>Improving Retrospective Language Agents via Joint Policy Gradient Optimization</title>
      <author><first>Xueyang</first><last>Feng</last></author>
      <author><first>Bo</first><last>Lan</last></author>
      <author><first>Quanyu</first><last>Dai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Jiakai</first><last>Tang</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>112-141</pages>
      <abstract>In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates imitation learning and reinforcement learning, and design an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance the data efficiency and training stability in agent tasks. RetroAct significantly improves the performance of open-source models, reduces dependency on closed-source LLMs, and enables fine-tuned agents to learn and evolve continuously. We conduct extensive experiments across various testing environments, demonstrating RetroAct has substantial improvements in task performance and decision-making processes.</abstract>
      <url hash="08d0dfcc">2025.naacl-long.6</url>
      <bibkey>feng-etal-2025-improving</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>C</fixed-case>odex<fixed-case>G</fixed-case>raph: Bridging Large Language Models and Code Repositories via Code Graph Databases</title>
      <author><first>Xiangyan</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bo</first><last>Lan</last></author>
      <author><first>Zhiyuan</first><last>Hu</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Zhicheng</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Michael Qizhe</first><last>Shieh</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wenmeng</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>142-160</pages>
      <abstract>Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our code and demo will be released soon.</abstract>
      <url hash="9b3080a2">2025.naacl-long.7</url>
      <bibkey>liu-etal-2025-codexgraph</bibkey>
    </paper>
    <paper id="8">
      <title>Instantly Learning Preference Alignment via In-context <fixed-case>DPO</fixed-case></title>
      <author><first>Feifan</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxuan</first><last>Fan</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>161-178</pages>
      <abstract>Human Preference Alignment (HPA) can assist large language models (LLMs) to generate safe content. Due to the heavy cost of fine-tuning, tuning-free methods have emerged, typically modifying LLM decoding via post-processing. In this paper, we propose a novel and effective approach for HPA in a tuning-free way, named In-Context Direct Preference Optimization (ICDPO). We first rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after ICL. It enables LLMs to both generate and select the well-aligned response, which is precisely estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer. Extensive experiments show its effectiveness, particularly in outperforming multiple tuning-free baselines, even competitiveness with SFT and DPO. We also conduct detailed analyses to offer comprehensive insights into ICDPO.</abstract>
      <url hash="6fe7a291">2025.naacl-long.8</url>
      <bibkey>song-etal-2025-instantly</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>ALTER</fixed-case>: Augmentation for Large-Table-Based Reasoning</title>
      <author><first>Han</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuheng</first><last>Ma</last></author>
      <author><first>Hanfang</first><last>Yang</last><affiliation>Renmin University of China</affiliation></author>
      <pages>179-198</pages>
      <url hash="975b59b5">2025.naacl-long.9</url>
      <bibkey>zhang-etal-2025-alter</bibkey>
    </paper>
    <paper id="10">
      <title>What the #?*!: Disentangling Hate Across Target Identities</title>
      <author><first>Yiping</first><last>Jin</last><affiliation>Pompeu Fabra University</affiliation></author>
      <author><first>Leo</first><last>Wanner</last><affiliation>Catalan Institute for Research and Advanced Studies and Universitat Pompeu Fabra</affiliation></author>
      <author><first>Aneesh Moideen</first><last>Koya</last><affiliation>Knorex</affiliation></author>
      <pages>199-221</pages>
      <abstract>Hate speech (HS) classifiers do not perform equally well in detecting hateful expressions towards different target identities. They also demonstrate systematic biases in predicted hatefulness scores. Tapping on two recently proposed functionality test datasets for HS detection, we quantitatively analyze the impact of different factors on HS prediction. Experiments on popular industrial and academic models demonstrate that HS detectors assign a higher hatefulness score merely based on the mention of specific target identities. Besides, models often confuse hatefulness and the polarity of emotions. This result is worrisome as the effort to build HS detectors might harm the vulnerable identity groups we wish to protect: posts expressing anger or disapproval of hate expressions might be flagged as hateful themselves. We also carry out a study inspired by social psychology theory, which reveals that the accuracy of hatefulness prediction correlates strongly with the intensity of the stereotype.</abstract>
      <url hash="fd3dee16">2025.naacl-long.10</url>
      <bibkey>jin-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>MAD</fixed-case> Speech: Measures of Acoustic Diversity of Speech</title>
      <author><first>Matthieu</first><last>Futeral</last></author>
      <author><first>Andrea</first><last>Agostinelli</last><affiliation>Google</affiliation></author>
      <author><first>Marco</first><last>Tagliasacchi</last><affiliation>Google</affiliation></author>
      <author><first>Neil</first><last>Zeghidour</last><affiliation>Kyutai</affiliation></author>
      <author><first>Eugene</first><last>Kharitonov</last><affiliation>Google</affiliation></author>
      <pages>222-235</pages>
      <abstract>Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly available.</abstract>
      <url hash="14a96ab8">2025.naacl-long.11</url>
      <bibkey>futeral-etal-2025-mad</bibkey>
    </paper>
    <paper id="12">
      <title>The <fixed-case>R</fixed-case>ussian-focused embedders’ exploration: ru<fixed-case>MTEB</fixed-case> benchmark and <fixed-case>R</fixed-case>ussian embedding model design</title>
      <author><first>Artem</first><last>Snegirev</last><affiliation>SaluteDevices</affiliation></author>
      <author><first>Maria</first><last>Tikhonova</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Maksimova</first><last>Anna</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <author><first>Aleksandr</first><last>Abramov</last></author>
      <pages>236-254</pages>
      <abstract>Embedding models play a crucial role in Natural Language Processing (NLP) by creating text embeddings used in various tasks such as information retrieval and assessing semantic text similarity. This paper focuses on research related to embedding models in the Russian language. It introduces a new Russian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark, the Russian version extending the Massive Text Embedding Benchmark (MTEB). Our benchmark includes seven categories of tasks, such as semantic textual similarity, text classification, reranking, and retrieval.The research also assesses a representative set of Russian and multilingual models on the proposed benchmark. The findings indicate that the new model achieves results that are on par with state-of-the-art models in Russian. We release the model ru-en-RoSBERTa, and the ruMTEB framework comes with open-source code, integration into the original framework and a public leaderboard.</abstract>
      <url hash="c7819c3e">2025.naacl-long.12</url>
      <bibkey>snegirev-etal-2025-russian</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>PRACTIQ</fixed-case>: A Practical Conversational Text-to-<fixed-case>SQL</fixed-case> dataset with Ambiguous and Unanswerable Queries</title>
      <author><first>Mingwen</first><last>Dong</last></author>
      <author><first>Nischal</first><last>Ashok Kumar</last></author>
      <author><first>Yiqun</first><last>Hu</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Anuj</first><last>Chauhan</last><affiliation>Amazon</affiliation></author>
      <author><first>Chung-Wei</first><last>Hang</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuaichen</first><last>Chang</last></author>
      <author><first>Lin</first><last>Pan</last><affiliation>Amazon</affiliation></author>
      <author><first>Wuwei</first><last>Lan</last><affiliation>Amazon</affiliation></author>
      <author><first>Henghui</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiarong</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Patrick</first><last>Ng</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <pages>255-273</pages>
      <abstract>Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user’s clarification, and the assistant’s clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We release our code for data generation and experiments on GitHub.</abstract>
      <url hash="60a3b290">2025.naacl-long.13</url>
      <bibkey>dong-etal-2025-practiq</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MIRAGE</fixed-case>-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems</title>
      <author><first>Nandan</first><last>Thakur</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Suleman</first><last>Kazi</last><affiliation>Vectara</affiliation></author>
      <author><first>Ge</first><last>Luo</last><affiliation>Vectara Inc.</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Amin</first><last>Ahmad</last><affiliation>Vectara</affiliation></author>
      <pages>274-298</pages>
      <abstract>Traditional retrieval-augmented generation (RAG) benchmarks evaluate systems using heuristic-based metrics, but these require human preferences as the ground truth for reference. In contrast, arena-based benchmarks, where systems compete against each other, require an expensive large language model (LLM) as a judge for a reliable evaluation. We present a simple efficient technique to combine the best of both worlds. The idea is to train a surrogate judge using heuristic metrics as input, to output the LLM as a judge prediction.In our work, we develop MIRAGE-Bench, a synthetic arena-based RAG benchmark for 18 diverse languages on Wikipedia focused on multilingual answer generation evaluation. It extensively couples both heuristic features and LLM as a judge for evaluation. We benchmark 19 multilingual LLMs, and observe a high correlation (Kendall Tau (<tex-math>\tau</tex-math>) = 0.909) using our surrogate judge and between GPT-4o as a teacher using the Bradley-Terry framework. Our results show proprietary and large open-source LLMs currently dominate on MIRAGE-Bench. Our code and datasets are made publicly available here: https://github.com/vectara/mirage-bench.</abstract>
      <url hash="aa94555d">2025.naacl-long.14</url>
      <bibkey>thakur-etal-2025-mirage</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>LLM</fixed-case>s Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Do Xuan</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ngoc-Hai</first><last>Nguyen</last></author>
      <author><first>Tiviatis</first><last>Sim</last><affiliation>National University of Singaore, National University of Singapore</affiliation></author>
      <author><first>Hieu</first><last>Dao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>299-330</pages>
      <abstract>We present the first systematic evaluation examining format bias in performance of large language models (LLMs). Our approach distinguishes between two categories of an evaluation metric under format constraints to reliably and accurately assess performance: one measures performance when format constraints are adhered to, while the other evaluates performance regardless of constraint adherence. We then define a metric for measuring the format bias of LLMs and establish effective strategies to reduce it. Subsequently, we present our empirical format bias evaluation spanning four commonly used categories—multiple-choice question-answer, wrapping, list, and mapping—covering 15 widely-used formats. Our evaluation on eight generation tasks uncovers significant format bias across state-of-the-art LLMs. We further discover that improving the format-instruction following capabilities of LLMs across formats potentially reduces format bias. Based on our evaluation findings, we study prompting and fine-tuning with synthesized format data techniques to mitigate format bias. Our methods successfully reduce the variance in ChatGPT’s performance among wrapping formats from 235.33 to 0.71 (%^2)</abstract>
      <url hash="ea24f801">2025.naacl-long.15</url>
      <bibkey>long-etal-2025-llms</bibkey>
    </paper>
    <paper id="16">
      <title>The Impact of Visual Information in <fixed-case>C</fixed-case>hinese Characters: Evaluating Large Models’ Ability to Recognize and Utilize Radicals</title>
      <author><first>Xiaofeng</first><last>Wu</last></author>
      <author><first>Karl</first><last>Stratos</last><affiliation>Essential AI</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>331-350</pages>
      <abstract>The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs’ and VLMs’ understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models’ ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language processing (CLP) tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information.</abstract>
      <url hash="52eedca0">2025.naacl-long.16</url>
      <bibkey>wu-etal-2025-impact</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>R</fixed-case>efine: Enhancing Few-Shot Performance on Low-Resource <fixed-case>I</fixed-case>ndic Languages with Example Selection from related Example Banks</title>
      <author><first>Soumya Suvra</first><last>Ghosal</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Soumyabrata</first><last>Pal</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Koyel</first><last>Mukherjee</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>351-365</pages>
      <abstract>Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks—Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples.</abstract>
      <url hash="7f5c536c">2025.naacl-long.17</url>
      <bibkey>ghosal-etal-2025-promptrefine</bibkey>
    </paper>
    <paper id="18">
      <title>Unlocking Decoding-time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts</title>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Yupeng</first><last>Hou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>366-384</pages>
      <abstract>The task of multi-objective alignment aims at balancing and controlling the different alignment objectives, e.g., helpfulness, harmlessness and honesty) of large language models to meet the personalized requirements of different users. However, previous methods tend to train multiple models to deal with various user preferences, with the number of trained models growing linearly with the number of alignment objectives and the number of different preferences. Meanwhile, existing methods are generally poor in extensibility and require significant re-training for each new alignment objective considered. Considering the limitation of previous approaches, we propose MCA, which constructs an expert prompt and an adversarial prompt for each objective to contrast at the decoding time and balances the objectives through combining the contrast. Our approach is verified to be superior to previous methods in obtaining a well-distributed Pareto front among different alignment objectives.</abstract>
      <url hash="b612986e">2025.naacl-long.18</url>
      <bibkey>fu-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="19">
      <title>Fingerspelling within Sign Language Translation</title>
      <author><first>Garrett</first><last>Tanzer</last><affiliation>Google</affiliation></author>
      <pages>385-464</pages>
      <abstract>Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms. While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences—and improving this capability. We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling recognition data into the translation training mixture. We find that 1) substantially improves understanding of fingerspelling (and translation quality overall), but the effect of 2) is mixed.</abstract>
      <url hash="598d924b">2025.naacl-long.19</url>
      <bibkey>tanzer-2025-fingerspelling</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>M</fixed-case>o<fixed-case>DS</fixed-case>: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Alexa</first><last>Siu</last><affiliation>Adobe</affiliation></author>
      <author><first>Nedim</first><last>Lipka</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tong</first><last>Sun</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <pages>465-491</pages>
      <abstract>Query-focused summarization (QFS) gives a summary of documents to answer a query.Past QFS work assumes queries have one answer, ignoring debatable ones (*Is law school worth it?*).We introduce **Debatable QFS (DQFS)**, a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must *comprehensively cover* all sources and *balance perspectives*, favoring no side.These goals elude LLM QFS systems, which: 1) lack structured content plans, failing to guide LLMs to write balanced summaries, and 2) employ the same query to retrieve contexts across documents, failing to cover all perspectives specific to each document’s content.To overcome this, we design MoDS, a multi-LLM framework mirroring human panel discussions.MoDS treats documents as individual Speaker LLMs and has a Moderator LLM that picks speakers to respond to tailored queries for planned topics.Speakers use tailored queries to retrieve relevant contexts from their documents and supply perspectives, which are tracked in a rich outline, yielding a content plan to guide the final summary.Experiments on ConflictingQA with controversial web queries and DebateQFS, our new dataset of debate queries from Debatepedia, show MoDS beats SOTA by 38-59% in topic paragraph coverage and balance, based on new citation metrics. Users also find MoDS’s summaries to be readable and more balanced.</abstract>
      <url hash="9c6d6e19">2025.naacl-long.20</url>
      <bibkey>balepur-etal-2025-mods</bibkey>
    </paper>
    <paper id="21">
      <title>Aligning Sentence Simplification with <fixed-case>ESL</fixed-case> Learner’s Proficiency for Language Acquisition</title>
      <author><first>Guanlin</first><last>Li</last></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Noel</first><last>Crespi</last><affiliation>Telecom SudParis</affiliation></author>
      <pages>492-507</pages>
      <abstract>Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners’ language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than 20% compared to baseline models, while maintaining high simplification quality.</abstract>
      <url hash="6e16a88d">2025.naacl-long.21</url>
      <bibkey>li-etal-2025-aligning</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>P</fixed-case>eer<fixed-case>QA</fixed-case>: A Scientific Question Answering Dataset from Peer Reviews</title>
      <author><first>Tim</first><last>Baumgärtner</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Ted</first><last>Briscoe</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>508-544</pages>
      <abstract>We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health.PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens.</abstract>
      <url hash="65715106">2025.naacl-long.22</url>
      <bibkey>baumgartner-etal-2025-peerqa</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>AL</fixed-case>ii<fixed-case>CE</fixed-case>: Evaluating Positional Fine-grained Citation Generation</title>
      <author><first>Yilong</first><last>Xu</last></author>
      <author><first>Jinhua</first><last>Gao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaoming</first><last>Yu</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>545-561</pages>
      <abstract>Large Language Model (LLM) can enhance its credibility and verifiability by generating text with citations. However, existing research on citation generation is predominantly limited to sentence-level statements, neglecting the significance of positional fine-grained citations that can appear anywhere within sentences. To facilitate further exploration of the positional fine-grained citation generation, we propose ALiiCE, the first automatic evaluation framework for this task. Our method employs a dependency tree based approach to parse the sentence-level claim into atomic claims. Then ALiiCE evaluates citation quality using three metrics, including positional fine-grained citation recall, precision, and coefficient of variation of citation positions. We evaluate the positional fine-grained citation generation performance of several LLMs on long-form QA datasets. Our experiments and analyses demonstrate the effectiveness and reasonableness of ALiiCE. We offer our insights into the current advancements and future directions for the positional fine-grained citation generation task.</abstract>
      <url hash="7694d010">2025.naacl-long.23</url>
      <bibkey>xu-etal-2025-aliice</bibkey>
    </paper>
    <paper id="24">
      <title>An <fixed-case>LLM</fixed-case>-Based Approach for Insight Generation in Data Analysis</title>
      <author><first>Alberto Sánchez</first><last>Pérez</last><affiliation>Aily Labs and Eurecom</affiliation></author>
      <author><first>Alaa</first><last>Boukhary</last><affiliation>Ailylabs</affiliation></author>
      <author><first>Paolo</first><last>Papotti</last><affiliation>Eurecom</affiliation></author>
      <author><first>Luis Castejón</first><last>Lozano</last><affiliation>Aily Labs</affiliation></author>
      <author><first>Adam</first><last>Elwood</last><affiliation>Aily Labs</affiliation></author>
      <pages>562-582</pages>
      <abstract>Generating insightful and actionable information from databases is critical in data analysis. This paper introduces a novel approach using Large Language Models (LLMs) to automatically generate textual insights. Given a multi-table database as input, our method leverages LLMs to produce concise, text-based insights that reflect interesting patterns in the tables. Our framework includes a Hypothesis Generator to formulate domain-relevant questions, a Query Agent to answer such questions by generating SQL queries against a database, and a Summarization module to verbalize the insights. The insights are evaluated for both correctness and subjective insightfulness using a hybrid model of human judgment and automated metrics. Experimental results on public and enterprise databases demonstrate that our approach generates more insightful insights than other approaches while maintaining correctness.</abstract>
      <url hash="3ed144e4">2025.naacl-long.24</url>
      <bibkey>perez-etal-2025-llm</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>W</fixed-case>eb<fixed-case>Q</fixed-case>uality: A Large-scale Multi-modal Web Page Quality Assessment Dataset with Multiple Scoring Dimensions</title>
      <author><first>Tao</first><last>Zhang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Yige</first><last>Wang</last></author>
      <author><first>ZhuHangyu</first><last>ZhuHangyu</last></author>
      <author><first>Li</first><last>Xin</last><affiliation>Tencent QB search</affiliation></author>
      <author><first>Chen</first><last>Xiang</last><affiliation>Tencent QQBrowser LAB</affiliation></author>
      <author><first>Tian Hua</first><last>Zhou</last></author>
      <author><first>Jin</first><last>Ma</last><affiliation>Tencent Search</affiliation></author>
      <pages>583-596</pages>
      <abstract>The assessment of web page quality plays a critical role in a range of downstream applications, yet there is a notable absence of datasets for the evaluation of web page quality. This research presents the pioneering task of web page quality assessment and introduces the first comprehensive, multi-modal Chinese dataset named WebQuality specifically designed for this task. The dataset includes over 65,000 detailed an-notations spanning four sub-dimensions and incorporates elements such as HTML+CSS, text, and visual screenshot, facilitating in-depth modeling and assessment of web page quality. We performed evaluations using a variety of baseline models to demonstrate the complexity of the task. Additionally, we propose Hydra, an integrated multi-modal analysis model, and rigorously assess its performance and limitations through extensive ablation studies. To advance the field of web quality assessment, we offer unrestricted access to our dataset and codebase for the research community, available at https://github.com/incredible-smurf/WebQuality</abstract>
      <url hash="cc47ae32">2025.naacl-long.25</url>
      <bibkey>zhang-etal-2025-webquality</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>UFO</fixed-case>: A <fixed-case>UI</fixed-case>-Focused Agent for Windows <fixed-case>OS</fixed-case> Interaction</title>
      <author><first>Chaoyun</first><last>Zhang</last></author>
      <author><first>Liqun</first><last>Li</last></author>
      <author><first>Shilin</first><last>He</last></author>
      <author><first>Xu</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Qiao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Si</first><last>Qin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Minghua</first><last>Ma</last></author>
      <author><first>Yu</first><last>Kang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <pages>597-622</pages>
      <abstract>We introduce UFO, a UI-Fcused agent designed to fulfill user requests tailored to Windows OS applications by observing and analyzing the GUI and control information of these applications. UFO utilizes a hierarchical dual-agent framework that decomposes user requests using a divide-and-conquer approach, enabling seamless navigation and addressing sub-tasks across multiple applications. It also incorporates a control interaction module tailored for Windows OS, which detects control elements effectively and allows for fully automated execution. As a result, UFO simplifies complex and time-consuming processes into tasks that can be completed with natural language commands.We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios. The results derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFOin fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS.</abstract>
      <url hash="b798ba99">2025.naacl-long.26</url>
      <bibkey>zhang-etal-2025-ufo</bibkey>
    </paper>
    <paper id="27">
      <title>Is your benchmark truly adversarial? <fixed-case>A</fixed-case>dv<fixed-case>S</fixed-case>core: Evaluating Human-Grounded Adversarialness</title>
      <author><first>Yoo Yeon</first><last>Sung</last></author>
      <author><first>Maharshi</first><last>Gor</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Eve</first><last>Fleisig</last></author>
      <author><first>Ishani</first><last>Mondal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>623-642</pages>
      <abstract>Adversarial datasets should validate AI robustness by providing samples on which humans perform well, but models do not. However, as models evolve, datasets can become obsolete. Measuring whether a dataset remains adversarial is hindered by the lack of a standardized metric for measuring adversarialness. We propose ADVSCORE, a human-grounded evaluation metric that assesses a dataset’s adversarialness by capturing models’ and humans’ varying abilities, while also identifying poor examples. We then use ADVSCORE to motivate a new dataset creation pipeline for realistic and high-quality adversarial samples, enabling us to collect an adversarial question answering (QA) dataset, ADVQA. We apply ADVSCORE using 9,347 human responses and ten language models’ predictions to track model improvement over five years (2020–2024). ADVSCORE thus provides guidance for achieving robustness comparable with human capabilities. Furthermore, it helps determine to what extent adversarial datasets continue to pose challenges, ensuring that, rather than reflecting outdated or overly artificial difficulties, they effectively test model capabilities.</abstract>
      <url hash="29e81d37">2025.naacl-long.27</url>
      <bibkey>sung-etal-2025-benchmark</bibkey>
    </paper>
    <paper id="28">
      <title>Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation</title>
      <author><first>Liwen</first><last>Sun</last></author>
      <author><first>James Jialun</first><last>Zhao</last></author>
      <author><first>Wenjing</first><last>Han</last></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>643-655</pages>
      <abstract>Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets demonstrate that our multimodal retriever significantly outperforms other state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagate fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation.</abstract>
      <url hash="b797ee8b">2025.naacl-long.28</url>
      <bibkey>sun-etal-2025-fact</bibkey>
    </paper>
    <paper id="29">
      <title>On Behalf of the Stakeholders: Trends in <fixed-case>NLP</fixed-case> Model Interpretability in the Era of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nitay</first><last>Calderon</last><affiliation>Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Roi</first><last>Reichart</last><affiliation>Technion, Israel Institute of Technology</affiliation></author>
      <pages>656-693</pages>
      <abstract>Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their properties, and their relevance to different stakeholders. We further explore the practical implications of these paradigms by analyzing trends from the past decade across multiple research fields. To this end, we retrieved thousands of papers and employed an LLM to characterize them. Our analysis reveals significant disparities between NLP developers and non-developer users, as well as between research fields, underscoring the diverse needs of stakeholders. For example, explanations of internal model components are rarely used outside the NLP field. We hope this paper informs the future design, development, and application of methods that align with the objectives and requirements of various stakeholders.</abstract>
      <url hash="22d84f1e">2025.naacl-long.29</url>
      <bibkey>calderon-reichart-2025-behalf</bibkey>
    </paper>
    <paper id="30">
      <title>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</title>
      <author><first>Ruohong</first><last>Zhang</last></author>
      <author><first>Liangke</first><last>Gui</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhiqing</first><last>Sun</last><affiliation>OpenAI</affiliation></author>
      <author><first>Yihao</first><last>Feng</last><affiliation>Apple AI/ML</affiliation></author>
      <author><first>Keyang</first><last>Xu</last><affiliation>Google and Columbia University</affiliation></author>
      <author><first>Yuanhan</first><last>Zhang</last></author>
      <author><first>Di</first><last>Fu</last></author>
      <author><first>Chunyuan</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Alexander G</first><last>Hauptmann</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Yonatan</first><last>Bisk</last><affiliation>Meta and Carnegie Mellon University</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>694-717</pages>
      <abstract>Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for open-ended conversations, remains a significant challenge. While previous studies have explored using large multimodal models (LMMs) as reward models for guiding preference modeling, their ability to accurately assess the quality of generated responses and their alignment with video content has not been conclusively demonstrated. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model’s reward mechanism, which directly takes video frames as input. Furthermore, we show that applying our reward mechanism to DPO algorithm significantly improves model performance on open-ended video QA tasks.</abstract>
      <url hash="84501a19">2025.naacl-long.30</url>
      <bibkey>zhang-etal-2025-direct</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>F</fixed-case>lexi<fixed-case>GPT</fixed-case>: Pruning and Extending Large Language Models with Low-Rank Weight Sharing</title>
      <author><first>James Seale</first><last>Smith</last><affiliation>Samsung</affiliation></author>
      <author><first>Chi-Heng</first><last>Lin</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Shikhar</first><last>Tuli</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Haris</first><last>Jeelani</last></author>
      <author><first>Shangqian</first><last>Gao</last><affiliation>Florida State University</affiliation></author>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Head of AI center</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>Samsung Research America</affiliation></author>
      <pages>718-730</pages>
      <abstract>The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression rate of 40%. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended training with minimal additional parameter costs.</abstract>
      <url hash="7b8460ab">2025.naacl-long.31</url>
      <bibkey>smith-etal-2025-flexigpt</bibkey>
    </paper>
    <paper id="32">
      <title>Conformalized Answer Set Prediction for Knowledge Graph Embedding</title>
      <author><first>Yuqicheng</first><last>Zhu</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Nico</first><last>Potyka</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jiarong</first><last>Pan</last><affiliation>Eindhoven University of Technology</affiliation></author>
      <author><first>Bo</first><last>Xiong</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yunjie</first><last>He</last><affiliation>Universität Stuttgart; Bosch Center for Artificial Intelligence</affiliation></author>
      <author><first>Evgeny</first><last>Kharlamov</last><affiliation>Robert Bosch GmbH, Bosch and University of Oslo</affiliation></author>
      <author><first>Steffen</first><last>Staab</last><affiliation>University of Stuttgart and University of Southampton</affiliation></author>
      <pages>731-750</pages>
      <abstract>Knowledge graph embeddings (KGE) apply machine learning methods on knowledge graphs (KGs) to provide non-classical reasoning capabilities based on similarities and analogies. The learned KG embeddings are typically used to answer queries by ranking all potential answers, but rankings often lack a meaningful probabilistic interpretation - lower-ranked answers do not necessarily have a lower probability of being true. This limitation makes it difficult to quantify uncertainty of model’s predictions, posing challenges for the application of KGE methods in high-stakes domains like medicine. We address this issue by applying the theory of conformal prediction that allows generating answer sets, which contain the correct answer with probabilistic guarantees. We explain how conformal prediction can be used to generate such answer sets for link prediction tasks. Our empirical evaluation on four benchmark datasets using six representative KGE methods validates that the generated answer sets satisfy the probabilistic guarantees given by the theory of conformal prediction. We also demonstrate that the generated answer sets often have a sensible size and that the size adapts well with respect to the difficulty of the query.</abstract>
      <url hash="6810b824">2025.naacl-long.32</url>
      <bibkey>zhu-etal-2025-conformalized</bibkey>
    </paper>
    <paper id="33">
      <title>Parameter-free and Accessible Prompt Learning to Enhance Adversarial Robustness for Pre-trained Vision-Language Models</title>
      <author><first>Xingran</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Kun</first><last>Yang</last></author>
      <author><first>Changtao</first><last>Miao</last></author>
      <author><first>Bingyu</first><last>Hu</last></author>
      <author><first>Zhuoer</first><last>Xu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Shiwen</first><last>Cui</last><affiliation>ant group</affiliation></author>
      <author><first>Changhua</first><last>Meng</last><affiliation>Ant Group</affiliation></author>
      <author><first>Dan</first><last>Hong</last></author>
      <pages>751-761</pages>
      <abstract>Large pre-trained Vision-Language Models (VLMs) have revolutionized both computer vision and natural language processing. Despite their success, adversarial examples can still mislead VLMs into producing incorrect results. This work focuses on boosting the adversarial robustness of VLMs by searching for text prompts at the word level, rather than optimizing continuous textual embeddings. We introduce Parameter-Free Prompt Tuning (PFPT) to learn defense words that enhance resilience against adversarial attacks when appended to existing prompts, thereby offering ease of use due to the simplicity of this approach. These defense words are naturally present in the inherent vocabulary of VLMs, providing a human-readable property. PFPT employs a coarse-to-fine strategy with carefully designed optimization objectives to guide the word search. Extensive experiments demonstrate our method’s superiority over hand-engineered prompts and other state-of-the-art methods. PFPT significantly boosts accuracy and robustness, outperforming hand-engineered prompts with average gains of +4.9% and +5.8%, respectively (epsilon=1/255).</abstract>
      <url hash="c3847fe0">2025.naacl-long.33</url>
      <bibkey>zhou-etal-2025-parameter</bibkey>
    </paper>
    <paper id="34">
      <title>Fine-grained Fallacy Detection with Human Label Variation</title>
      <author><first>Alan</first><last>Ramponi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Agnese</first><last>Daffara</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>762-784</pages>
      <abstract>We introduce FAINA, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. FAINA includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond “single ground truth” evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span matches, overlaps, and the varying severity of labeling errors. Our experiments across four fallacy detection setups show that multi-task and multi-label transformer-based approaches are strong baselines across all settings. We release our data, code, and annotation guidelines to foster research on fallacy detection and human label variation more broadly.</abstract>
      <url hash="5400dae7">2025.naacl-long.34</url>
      <bibkey>ramponi-etal-2025-fine</bibkey>
    </paper>
    <paper id="35">
      <title>Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</title>
      <author><first>Hila</first><last>Gonen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Alisa</first><last>Liu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>785-798</pages>
      <abstract>Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior.</abstract>
      <url hash="3a1ffba3">2025.naacl-long.35</url>
      <bibkey>gonen-etal-2025-liking</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>SELFGOAL</fixed-case>: Your Language Agents Already Know How to Achieve High-level Goals</title>
      <author><first>Ruihan</first><last>Yang</last></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Aili</first><last>Chen</last></author>
      <author><first>Kyle</first><last>Richardson</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>799-819</pages>
      <abstract>Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SELFGOAL, a novel automatic approach designed to enhance agents’ capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SELFGOAL involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SELFGOAL significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments.</abstract>
      <url hash="ea52ada9">2025.naacl-long.36</url>
      <bibkey>yang-etal-2025-selfgoal</bibkey>
    </paper>
    <paper id="37">
      <title>Familarity: Better Evaluation of Zero-Shot Named Entity Recognition by Quantifying Label Shifts in Synthetic Training Data</title>
      <author><first>Jonas</first><last>Golde</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Patrick</first><last>Haller</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Fabio</first><last>Barth</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Nicolaas Paul</first><last>Jedema</last><affiliation>Amazon</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>820-834</pages>
      <abstract>Zero-shot named entity recognition (NER) is the task of detecting named entities of specific types (such as Person or Medicine) without any training examples. Current research increasingly relies on large synthetic datasets, automatically generated to cover tens of thousands of distinct entity types, to train zero-shot NER models. However, in this paper, we find that these synthetic datasets often contain entity types that are semantically highly similar to (or even the same as) those in standard evaluation benchmarks. Because of this overlap, we argue that reported F1 scores for zero-shot NER overestimate the true capabilities of these approaches. Further, we argue that current evaluation setups provide an incomplete picture of zero-shot abilities since they do not quantify the label shift (i.e., the similarity of labels) between training and evaluation datasets. To address these issues, we propose Familarity, a novel metric that captures both the semantic similarity between entity types in training and evaluation, as well as their frequency in the training data, to provide an estimate of label shift. It allows researchers to contextualize reported zero-shot NER scores when using custom synthetic training datasets. Further, it enables researchers to generate evaluation setups of various transfer difficulties for fine-grained analysis of zero-shot NER.</abstract>
      <url hash="590d3ef2">2025.naacl-long.37</url>
      <bibkey>golde-etal-2025-familarity</bibkey>
    </paper>
    <paper id="38">
      <title>Learning to Summarize from <fixed-case>LLM</fixed-case>-generated Feedback</title>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Taewon</first><last>Yun</last></author>
      <author><first>Yuho</first><last>Lee</last></author>
      <author><first>Jihwan</first><last>Oh</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Gihun</first><last>Lee</last></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <pages>835-857</pages>
      <abstract>Developing effective text summarizers remains a challenge due to issues like hallucinations, key information omissions, and verbosity in LLM-generated summaries. This work explores using LLM-generated feedback to improve summary quality by aligning the summaries with human preferences for faithfulness, completeness, and conciseness. We introduce FeedSum, a large-scale dataset containing multi-dimensional LLM feedback on summaries of varying quality across diverse domains. Our experiments show how feedback quality, dimensionality, and granularity influence preference learning, revealing that high-quality, multi-dimensional, fine-grained feedback significantly improves summary generation. We also compare two methods for using this feedback: supervised fine-tuning and direct preference optimization. Finally, we introduce SummLlama3-8b, a model that outperforms the nearly 10x larger Llama3-70b-instruct in generating human-preferred summaries, demonstrating that smaller models can achieve superior performance with appropriate training. The full dataset and SummLlama3-8B model are available at https://huggingface.co/datasets/DISLab/FeedSum and https://huggingface.co/DISLab/SummLlama3-8B.</abstract>
      <url hash="12697d20">2025.naacl-long.38</url>
      <bibkey>song-etal-2025-learning</bibkey>
    </paper>
    <paper id="39">
      <title>Hybrid Graphs for Table-and-Text based Question Answering using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ankush</first><last>Agarwal</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Chaitanya</first><last>Devaguptapu</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Ganesh</first><last>S</last></author>
      <pages>858-875</pages>
      <abstract>Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context.</abstract>
      <url hash="ad4e59f0">2025.naacl-long.39</url>
      <bibkey>agarwal-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>CF</fixed-case>in<fixed-case>B</fixed-case>ench: A Comprehensive <fixed-case>C</fixed-case>hinese Financial Benchmark for Large Language Models</title>
      <author><first>Ying</first><last>Nie</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Binwei</first><last>Yan</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Tianyu</first><last>Guo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Haoyu</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Wei</first><last>He</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Binfan</first><last>Zheng</last></author>
      <author><first>Weihao</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qiang</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Weijian</first><last>Sun</last></author>
      <author><first>Yunhe</first><last>Wang</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>876-891</pages>
      <abstract>Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging task like finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments on a wide spectrum of representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 66.02%, highlighting the challenge presented by CFinBench. All the data and evaluation code are open sourced at https://cfinbench.github.io/</abstract>
      <url hash="fda52af6">2025.naacl-long.40</url>
      <bibkey>nie-etal-2025-cfinbench</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>LLM</fixed-case>-Based Explicit Models of Opponents for Multi-Agent Games</title>
      <author><first>XiaoPeng</first><last>Yu</last></author>
      <author><first>Wanpeng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>892-911</pages>
      <abstract>In multi-agent scenarios, the ability to anticipate and respond to opponents is essential, particularly in environments involving adversarial and collaborative interactions. In this paper, we introduce Explicit Models of Opponents (EMO) based on Large Language Models (LLMs), enabling agents to better predict and adapt to diverse, dynamic multi-agent interactions. Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework. We test EMO alongside several reasoning methods in multi-player deduction games, where agents must infer hidden information about their opponents. The results show that EMO significantly enhances agents’ decision-making, outperforming traditional single-model approaches. Our findings demonstrate that EMO can be a powerful tool for enhancing LLM-based agents in complex multi-agent systems.</abstract>
      <url hash="397053de">2025.naacl-long.41</url>
      <bibkey>yu-etal-2025-llm</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>S</fixed-case>eq<fixed-case>AR</fixed-case>: Jailbreak <fixed-case>LLM</fixed-case>s with Sequential Auto-Generated Characters</title>
      <author><first>Yan</first><last>Yang</last></author>
      <author><first>Zeguan</first><last>Xiao</last></author>
      <author><first>Xin</first><last>Lu</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xuetao</first><last>Wei</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Hailiang</first><last>Huang</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <pages>912-931</pages>
      <abstract>The widespread applications of large language models (LLMs) have brought about concerns regarding their potential misuse. Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety and introduce SeqAR, a simple yet effective framework to design jailbreak prompts automatically. The SeqAR framework generates and optimizes multiple jailbreak characters and then applies sequential jailbreak characters in a single query to bypass the guardrails of the target LLM. Different from previous work which relies on proprietary LLMs or seed jailbreak templates crafted by human expertise, SeqAR can generate and optimize the jailbreak prompt in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SeqAR achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively. Furthermore, we extensively evaluate the transferability of the generated templates across different LLMs and held-out malicious requests, while also exploring defense strategies against the jailbreak attack designed by SeqAR.</abstract>
      <url hash="f54f477e">2025.naacl-long.42</url>
      <bibkey>yang-etal-2025-seqar</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>JMMMU</fixed-case>: A <fixed-case>J</fixed-case>apanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title>
      <author><first>Shota</first><last>Onohara</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Atsuyuki</first><last>Miyai</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yuki</first><last>Imajuku</last></author>
      <author><first>Kazuki</first><last>Egashira</last></author>
      <author><first>Jeonghun</first><last>Baek</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Kiyoharu</first><last>Aizawa</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>932-950</pages>
      <url hash="415c5e4b">2025.naacl-long.43</url>
      <bibkey>onohara-etal-2025-jmmmu</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>EASYTOOL</fixed-case>: Enhancing <fixed-case>LLM</fixed-case>-based Agents with Concise Tool Instruction</title>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Kaitao</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Yongliang</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kan</first><last>Ren</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>951-972</pages>
      <abstract>There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLMbased agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. Current LLMs exhibit satisfactory instruction-following capabilities based on instruction-following fine-tuning process. Motivated by this, in this paper, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction to fully leverage instruction-following capabilities of LLMs for easier tool usage. EASYTOOL purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EASYTOOL can significantly reduce token consumption and improve the performance of LLM-based agents on tool utilization in real-world scenarios. Our code is available in supplemental materials. Our code is available at https://github.com/microsoft/JARVIS/tree/main/easytool.</abstract>
      <url hash="57b3f3f1">2025.naacl-long.44</url>
      <bibkey>yuan-etal-2025-easytool</bibkey>
    </paper>
    <paper id="45">
      <title>Decoding Hate: Exploring Language Models’ Reactions to Hate Speech</title>
      <author><first>Paloma</first><last>Piot</last><affiliation>Universidade da Coruña</affiliation></author>
      <author><first>Javier</first><last>Parapar</last><affiliation>Universidad de La Coruña</affiliation></author>
      <pages>973-990</pages>
      <abstract>Hate speech is a harmful form of online expression, often manifesting as derogatory posts. It is a significant risk in digital environments. With the rise of Large Language Models (LLMs), there is concern about their potential to replicate hate speech patterns, given their training on vast amounts of unmoderated internet data. Understanding how LLMs respond to hate speech is crucial for their responsible deployment. However, the behaviour of LLMs towards hate speech has been limited compared. This paper investigates the reactions of seven state-of-the-art LLMs (LLaMA 2, Vicuna, LLaMA 3, Mistral, GPT-3.5, GPT-4, and Gemini Pro) to hate speech. Through qualitative analysis, we aim to reveal the spectrum of responses these models produce, highlighting their capacity to handle hate speech inputs. We also discuss strategies to mitigate hate speech generation by LLMs, particularly through fine-tuning and guideline guardrailing. Finally, we explore the models’ responses to hate speech framed in politically correct language.</abstract>
      <url hash="4b670791">2025.naacl-long.45</url>
      <bibkey>piot-parapar-2025-decoding</bibkey>
    </paper>
    <paper id="46">
      <title>Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</title>
      <author><first>Ziqiao</first><last>Ma</last></author>
      <author><first>Zekun</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Joyce</first><last>Chai</last><affiliation>University of Michigan</affiliation></author>
      <pages>991-1010</pages>
      <abstract>Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher’s choices of words influence students’ word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models.</abstract>
      <url hash="c30f1ee8">2025.naacl-long.46</url>
      <bibkey>ma-etal-2025-babysit</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>M</fixed-case>o<fixed-case>CE</fixed-case>: Adaptive Mixture of Contextualization Experts for Byte-based Neural Machine Translation</title>
      <author><first>Langlin</first><last>Huang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Mengyu</first><last>Bu</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>1011-1028</pages>
      <abstract>Byte-based machine translation systems have shown significant potential in massively multilingual settings. Unicode encoding, which maps each character to specific byte(s), eliminates the emergence of unknown words, even in new languages, enabling broad language scalability. However, byte-level tokenization results in sequences that are hard to interpret due to limited semantic information per byte. Local contextualization has proven effective in assigning initial semantics to tokens, improving sentence comprehension. Nevertheless, variations in encoding rules across languages necessitate an adaptive approach for effective contextualization. To this end, we propose Adaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and mixing attention heads, which are treated as contextualization experts. This enhances the flexibility of contextualization scales and improves the potential to discover a better strategy than previous methods. Experiment results show that our method outperforms existing methods without extensive manual adjustment of hyper-parameters and surpasses subword-based models with fewer parameters in Ted-59 dataset.</abstract>
      <url hash="f9345241">2025.naacl-long.47</url>
      <bibkey>huang-etal-2025-moce</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>LLM</fixed-case>-Human Pipeline for Cultural Grounding of Conversations</title>
      <author><first>Rajkumar</first><last>Pujari</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <pages>1029-1048</pages>
      <abstract>Conversations often adhere to well-understood social norms that vary across cultures. For example, while <i>addressing parents by name</i> is commonplace in the West, it is rare in most Asian cultures. Adherence or violation of such norms often dictates the tenor of conversations. Humans are able to navigate social situations requiring cultural awareness quite adeptly. However, it is a hard task for NLP models.In this paper, we tackle this problem by introducing a <i>Cultural Context Schema</i> for conversations. It comprises (1) conversational information such as emotions, dialogue acts, etc., and (2) cultural information such as social norms, violations, etc. We generate ~110k social norm and violation descriptions for ~23k conversations from Chinese culture using LLMs. We refine them using automated verification strategies which are evaluated against culturally aware human judgements. We organize these descriptions into meaningful structures we call Norm Concepts, using an interactive human-in-loop framework. We ground the norm concepts and the descriptions in conversations using symbolic annotation. Finally, we use the obtained dataset for downstream tasks such as emotion, sentiment, and dialogue act detection. We show that it significantly improves the empirical performance.</abstract>
      <url hash="4d33e3b2">2025.naacl-long.48</url>
      <bibkey>pujari-goldwasser-2025-llm</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>ACCESS</fixed-case> : A Benchmark for Abstract Causal Event Discovery and Reasoning</title>
      <author><first>Vy</first><last>Vo</last><affiliation>Monash University</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Tao</first><last>Feng</last></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Xiaoxi</first><last>Kang</last></author>
      <author><first>Songhai</first><last>Fan</last></author>
      <author><first>Tim</first><last>Dwyer</last><affiliation>Monash University</affiliation></author>
      <author><first>Lay-Ki</first><last>Soon</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>1049-1074</pages>
      <url hash="c6b66927">2025.naacl-long.49</url>
      <bibkey>vo-etal-2025-access</bibkey>
    </paper>
    <paper id="50">
      <title>Unmasking Implicit Bias: Evaluating Persona-Prompted <fixed-case>LLM</fixed-case> Responses in Power-Disparate Social Scenarios</title>
      <author><first>Bryan Chen Zhengyu</first><last>Tan</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>1075-1108</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities in simulating human behaviour and social intelligence. However, they risk perpetuating societal biases, especially when demographic information is involved. We introduce a novel framework using cosine distance to measure semantic shifts in responses and an LLM-judged Preference Win Rate (WR) to assess how demographic prompts affect response quality across power-disparate social scenarios. Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a “default persona” bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views. Moreover, interactions involving specific demographics are associated with lower-quality responses. Lastly, the presence of power disparities increases variability in response semantics and quality across demographic groups, suggesting that implicit biases may be heightened under power-imbalanced conditions. These insights expose the demographic biases inherent in LLMs and offer potential paths toward future bias mitigation efforts in LLMs.</abstract>
      <url hash="67f1f699">2025.naacl-long.50</url>
      <bibkey>tan-lee-2025-unmasking</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>G</fixed-case>lo<fixed-case>COM</fixed-case>: A Short Text Neural Topic Model via Global Clustering Context</title>
      <author><first>Quang Duc</first><last>Nguyen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Tung</first><last>Nguyen</last></author>
      <author><first>Duc Anh</first><last>Nguyen</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>1109-1124</pages>
      <abstract>Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model, **GloCOM** (**Glo**bal **C**lustering C**O**ntexts for Topic **M**odels), which addresses these challenges by constructing aggregated global clustering contexts for short documents, leveraging text embeddings from pre-trained language models. GloCOM can infer both global topic distributions for clustering contexts and local distributions for individual short texts. Additionally, the model incorporates these global contexts to augment the reconstruction loss, effectively handling the label sparsity issue. Extensive experiments on short text datasets show that our approach outperforms other state-of-the-art models in both topic quality and document representations.</abstract>
      <url hash="9ef663c3">2025.naacl-long.51</url>
      <bibkey>nguyen-etal-2025-glocom</bibkey>
    </paper>
    <paper id="52">
      <title>Reversed Attention: On The Gradient Descent Of Attention Layers In <fixed-case>GPT</fixed-case></title>
      <author><first>Shahar</first><last>Katz</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Lior</first><last>Wolf</last><affiliation>Tel Aviv University, Tel Aviv University and Tel Aviv University</affiliation></author>
      <pages>1125-1152</pages>
      <abstract>The success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as “Reversed Attention”.We visualized Reversed Attention and examine its properties, demonstrating its ability to elucidate the models’ behavior and edit dynamics.In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model’s weights, using a novel method called “attention patching”.In addition to enhancing the comprehension of how LMs configure attention layers during backpropagation, Reversed Attention maps contribute to a more interpretable backward pass.</abstract>
      <url hash="2e808648">2025.naacl-long.52</url>
      <bibkey>katz-wolf-2025-reversed</bibkey>
    </paper>
    <paper id="53">
      <title>Self-Harmonized Chain of Thought</title>
      <author><first>Ziqi</first><last>Jin</last></author>
      <author><first>Wei</first><last>Lu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>1153-1174</pages>
      <abstract>Chain-of-thought (CoT) prompting has demonstrated the capacity of large language models to perform complex reasoning through intermediate steps. While effective, current CoT methods face challenges: Zero-shot-CoT can lead to reasoning errors, and Few-shot-CoT requires labor-intensive manual demonstrations. Auto-CoT attempts to address these issues by automatically generating diverse demonstrations, but this diversity can lead to inconsistent reasoning patterns. We propose ECHO (Self-Harmonized Chain of Thought), a novel method that unifies diverse solution paths into a consistent and effective reasoning pattern. ECHO employs an iterative process to refine and harmonize automatically generated demonstrations, mitigating the limitations of existing approaches. Our comprehensive experiments across arithmetic, commonsense, and symbolic reasoning tasks demonstrate that ECHO outperforms Auto-CoT by an average of 2.8%. These findings suggest that ECHO represents a significant step towards more robust and generalizable automated reasoning in large language models.</abstract>
      <url hash="d72d753f">2025.naacl-long.53</url>
      <bibkey>jin-lu-2025-self</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>A</fixed-case>na<fixed-case>S</fixed-case>core: Understanding Semantic Parallelism in Proportional Analogies</title>
      <author><first>Liyan</first><last>Wang</last><affiliation>Waseda University</affiliation></author>
      <author><first>Haotong</first><last>Wang</last></author>
      <author><first>Yves</first><last>Lepage</last><affiliation>Waseda University</affiliation></author>
      <pages>1175-1188</pages>
      <abstract>Formulaic criteria for proportional analogies, which capture relational mappings between two ratios of terms, are mainly confined to the formal level. As analogy datasets grow more complex, especially in evaluating the cognitive abilities of Large Language Models (LLMs), assessing parallelism in them becomes increasingly challenging and often requires human annotation. In this work, we propose AnaScore, an automatic metric for evaluating the strength of semantic parallelism in sentence analogies. AnaScore systematically provides formalized explanations for shared relational patterns at the level of conceptual knowledge. We apply AnaScore to annotate several existing datasets, considering different directions of the relations, and uncover artifacts in data construction. Our experiments with various LLMs demonstrate the efficacy of the AnaScore metric in capturing the inherent quality of analogical relationships, showing a positive correlation between analogy quality and model performance. Thanks to this metric, we clearly demonstrate that formally explainable examples are more beneficial for analogical reasoning, while ambiguous analogies with no clear criterion tend to hinder inference.</abstract>
      <url hash="a910d767">2025.naacl-long.54</url>
      <bibkey>wang-etal-2025-anascore</bibkey>
    </paper>
    <paper id="55">
      <title>Generating Complex Question Decompositions in the Face of Distribution Shifts</title>
      <author><first>Kelvin</first><last>Han</last><affiliation>LORIA/CNRS, Université of Lorraine</affiliation></author>
      <author><first>Claire</first><last>Gardent</last><affiliation>CNRS</affiliation></author>
      <pages>1189-1211</pages>
      <abstract>Question decomposition has been found to help large language models’ (LLMs) performance on complex question answering (QA) by breaking these questions into simpler sub-questions for answering. Nonetheless, performance on the task remains dominated by supervised approaches, suggesting room for making LLMs better decomposers. One way of improving LLM training and fine-tuning is to leverage synthetic training data, but the superior performance of supervised approaches collapses in the face of distribution shifts, making them unsuitable for generating synthetic data across new domains and at scale. To address this, we propose an approach to generate synthetic decomposition data with only five annotated examples; we do this by (i) extending recent advancements in using LLM-as-judge and for reranking in novel ways, as well as (ii) using a panel of smaller-sized LLMs for data generation instead of resource-intensive larger models. Through careful validation of our approach over two benchmark datasets, we show that our data generation and modelling approaches bring consistent improvements over using few-shot prompting with LLMs for the task. Our code and models can be found at https://github.com/hankelvin/complex_question_decomposition.</abstract>
      <url hash="59faf0de">2025.naacl-long.55</url>
      <bibkey>han-gardent-2025-generating</bibkey>
    </paper>
    <paper id="56">
      <title>Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering</title>
      <author><first>Yeonjun</first><last>In</last></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Mehrab</first><last>Tanjim</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ritwik</first><last>Sinha</last></author>
      <author><first>Chanyoung</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>1212-1233</pages>
      <abstract>The retrieval augmented generation (RAG) framework addresses an ambiguity in user queries in QA systems by retrieving passages that cover all plausible interpretations and generating comprehensive responses based on the passages. However, our preliminary studies reveal that a single retrieval process often suffers from low-quality results, as the retrieved passages frequently fail to capture all plausible interpretations. Although the iterative RAG approach has been proposed to address this problem, it comes at the cost of significantly reduced efficiency. To address these issues, we propose the diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved passages to encompass diverse interpretations. Subsequently, DIVA verifies the quality of the passages and adapts the most suitable approach tailored to their quality. This approach improves the QA systems’ accuracy and robustness by handling low quality retrieval issue in ambiguous questions, while enhancing efficiency.</abstract>
      <url hash="7ac3f4a5">2025.naacl-long.56</url>
      <bibkey>in-etal-2025-diversify</bibkey>
    </paper>
    <paper id="57">
      <title>Unifying <fixed-case>AI</fixed-case> Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of <fixed-case>LLM</fixed-case>-Powered <fixed-case>AI</fixed-case> Tutors</title>
      <author><first>Kaushal Kumar</first><last>Maurya</last></author>
      <author><first>Kv Aditya</first><last>Srivatsa</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Kseniia</first><last>Petukhova</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1234-1251</pages>
      <abstract>In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues. Previous efforts towards evaluation have beenlimited to subjective protocols and benchmarks. To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain. We release MRBench – a new evaluation benchmark containing 192 conversations and 1,596 responses from seven state-of-the-art LLM-based and human tutors, providing gold annotations for eight pedagogical dimensions. We assess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as evaluators and analyze each tutor’s pedagogical abilities, highlighting which LLMs are good tutors and which ones are more suitable as question-answering systems. We believe that the presented taxonomy, benchmark, and human-annotated labels will streamline the evaluation process and help track the progress in AI tutors’ development.</abstract>
      <url hash="2b6147d2">2025.naacl-long.57</url>
      <bibkey>maurya-etal-2025-unifying</bibkey>
    </paper>
    <paper id="58">
      <title>Where is the answer? An empirical study of positional bias for parametric knowledge extraction in language model</title>
      <author><first>Kuniaki</first><last>Saito</last><affiliation>OMRON SINICX</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Kihyuk</first><last>Sohn</last><affiliation>Facebook</affiliation></author>
      <author><first>Yoshitaka</first><last>Ushiku</last><affiliation>OMRON SINIC X, NexaScience, RIKEN, Ridge-i and National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>1252-1269</pages>
      <abstract>Language model (LM) stores diverse factual knowledge in their parameters, which is learned during self-supervised training on unlabeled documents and is made extractable by instruction-tuning. For knowledge-intensive tasks, it is essential to memorize information in a way that makes it extractable from LM’s parameters with diverse queries. However, LMs suffer from a phenomenon called “perplexity curse”; despite minimizing document perplexity during training, LMs struggle to extract information via a question prompt. In this paper, we study the problem by fine-tuning LMs for new data and find a very intriguing fact that all studied LMs suffer from positional bias in the training document, i.e., they struggle to answer questions about the information described in the middle or at the end of the training document. Our study indicates that this problem stems from the auto-regressive training, ie., predicting the next token given all previous tokens, thus adding regularization mitigates the issue. Our discoveries supported by extensive analysis will be an important key to extracting knowledge from the parameters of LMs. We will publish our code and dataset upon acceptance.</abstract>
      <url hash="8cbd8aca">2025.naacl-long.58</url>
      <bibkey>saito-etal-2025-answer</bibkey>
    </paper>
    <paper id="59">
      <title>Evaluating Morphological Compositional Generalization in Large Language Models</title>
      <author><first>Mete</first><last>Ismayilzada</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Defne</first><last>Circi</last></author>
      <author><first>Jonne</first><last>Sälevä</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Hale</first><last>Sirin</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Abdullatif</first><last>Köksal</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Duygu</first><last>Ataman</last><affiliation>New York University</affiliation></author>
      <author><first>Lonneke Van Der</first><last>Plas</last><affiliation>Idiap Research Institute</affiliation></author>
      <pages>1270-1305</pages>
      <abstract>Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative and discriminative tasks to assess morphological productivity and systematicity. Focusing on agglutinative languages such as Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases. While models can identify individual morphological combinations better than chance, their performance lacks systematicity, leading to significant accuracy gaps compared to humans.</abstract>
      <url hash="4d05e75b">2025.naacl-long.59</url>
      <bibkey>ismayilzada-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="60">
      <title>Balancing Forget Quality and Model Utility: A Reverse <fixed-case>KL</fixed-case>-Divergence Knowledge Distillation Approach for Better Unlearning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Bichen</first><last>Wang</last></author>
      <author><first>Yuzhe</first><last>Zi</last></author>
      <author><first>Yixin</first><last>Sun</last></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1306-1321</pages>
      <abstract>As concern for privacy rights has grown and the size of language model training datasets has expanded, research into machine unlearning for large language models (LLMs) has become crucial. Before the era of LLMs, research on machine unlearning mainly focused on classification tasks in small parameter models. However, as parameter sizes have grown and unlearning targets have become more complex, unlearning has become more challenging, especially in scenarios involving generation instead of classification, as the output space of such models is significantly larger and more diverse. Existing methods based on gradient ascent and its variants often struggle with balancing forget quality and model utility, leading to either over unlearning or partial unlearning. To address this challenge, we propose Reverse KL-Divergence based Knowledge Distillation for Unlearning (RKLU), a novel unlearning method for LLMs. RKLU focuses on precisely unlearning the components of the token distribution related to the unlearning target, allowing us to achieve significant forget quality while maintaining model utility in our experiments.</abstract>
      <url hash="f18b1029">2025.naacl-long.60</url>
      <bibkey>wang-etal-2025-balancing</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>A</fixed-case>gent<fixed-case>M</fixed-case>ove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction</title>
      <author><first>Jie</first><last>Feng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuwei</first><last>Du</last></author>
      <author><first>Jie</first><last>Zhao</last></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>1322-1338</pages>
      <abstract>Next location prediction plays a crucial role in various real-world applications. Recently, due to the limitation of existing deep learning methods, attempts have been made to apply large language models (LLMs) to zero-shot next location prediction task. However, they directly generate the final output using LLMs without systematic design, which limits the potential of LLMs to uncover complex mobility patterns and underestimates their extensive reserve of global geospatial knowledge. In this paper, we introduce AgentMove, a systematic agentic prediction framework to achieve generalized next location prediction. In AgentMove, we first decompose the mobility prediction task and design specific modules to complete them, including spatial-temporal memory for individual mobility pattern mining, world knowledge generator for modeling the effects of urban structure and collective knowledge extractor for capturing the shared patterns among population. Finally, we combine the results of three modules and conduct a reasoning step to generate the final predictions. Extensive experiments utilizing mobility data from two distinct sources reveal that AgentMove surpasses the leading baseline by 3.33% to 8.57% across 8 out of 12 metrics and it shows robust predictions with various LLMs as base and also less geographical bias across cities. Our codes are available via https://github.com/tsinghua-fib-lab/AgentMove.</abstract>
      <url hash="e0c0ccf0">2025.naacl-long.61</url>
      <bibkey>feng-etal-2025-agentmove</bibkey>
    </paper>
    <paper id="62">
      <title>Embedding derived animacy rankings offer insights into the sources of grammatical animacy</title>
      <author><first>Vivian G.</first><last>Li</last></author>
      <pages>1339-1351</pages>
      <abstract>In this study, we applied the semantic projection approach to animacy, a feature that has not been previously explored using this method. We compared the relative animacy rankings of nouns denoting animals, humans, objects, and first-, second-, and third-person pronouns, as derived from word embeddings, with rankings derived from human behavioral ratings of animacy and from grammatical patterns. Our results support the semantic projection approach as an effective method for deriving proxies of human perception from word embeddings and offer insights into the sources of grammatical animacy.</abstract>
      <url hash="301914d9">2025.naacl-long.62</url>
      <bibkey>li-2025-embedding</bibkey>
    </paper>
    <paper id="63">
      <title>Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement</title>
      <author><first>Qianyue</first><last>Wang</last></author>
      <author><first>Jinwu</first><last>Hu</last></author>
      <author><first>Zhengping</first><last>Li</last></author>
      <author><first>Yufeng</first><last>Wang</last></author>
      <author><first>Daiyuan</first><last>Li</last></author>
      <author><first>Yu</first><last>Hu</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Mingkui</first><last>Tan</last></author>
      <pages>1352-1391</pages>
      <abstract>Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writingand interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, making it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address this issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation method, named DOME, to generate the long-form story with coherent content and plot. Specifically, the Dynamic Hierarchical Outline(DHO) mechanism incorporates the novel writing theory into outline planning and fuses the plan and writing stages together, improving the coherence of the plot by ensuring the plot completeness and adapting to the uncertainty during story generation. A Memory-Enhancement Module (MEM) based on temporal knowledge graphs is introduced to store and access the generated content, reducing contextual conflicts and improving story coherence. Finally, we propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to automatically evaluate the contextual consistency of long-form story. Experiments demonstrate that DOME significantly improves the fluency, coherence, and overall quality of generated long stories compared to state-of-the-art methods.</abstract>
      <url hash="0edf1d08">2025.naacl-long.63</url>
      <bibkey>wang-etal-2025-generating</bibkey>
    </paper>
    <paper id="64">
      <title>Little Giants: Synthesizing High-Quality Embedding Data at Scale</title>
      <author><first>Haonan</first><last>Chen</last></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nan</first><last>Yang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1392-1411</pages>
      <abstract>Synthetic data generation has become an increasingly popular way of training models without the need for large, manually labeled datasets. For tasks like text embedding, synthetic data offers diverse and scalable training examples, significantly reducing the cost of human annotation. However, most current approaches rely heavily on proprietary models like GPT-4, which are expensive and inefficient for generating large-scale embedding data. In this paper, we introduce SPEED, a framework that aligns open-source small models (8B) to efficiently generate large-scale synthetic embedding data. Through supervised fine-tuning, preference optimization, and self-improvement, SPEED enables small open-source models to produce high-quality data. Remarkably, SPEED uses only less than 1/10 of the GPT API calls, outperforming the state-of-the-art embedding model E5_mistral when both are trained solely on their synthetic data. Using this efficient generator, we conduct a comprehensive study on how various factors within the alignment pipeline impact data quality and reveal the scaling law for synthetic embedding data. Our codes and models are released in https://github.com/haon-chen/SPEED.</abstract>
      <url hash="8d2e185b">2025.naacl-long.64</url>
      <bibkey>chen-etal-2025-little</bibkey>
    </paper>
    <paper id="65">
      <title>Can <fixed-case>LLM</fixed-case>s Convert Graphs to Text-Attributed Graphs?</title>
      <author><first>Zehong</first><last>Wang</last></author>
      <author><first>Sidney</first><last>Liu</last></author>
      <author><first>Zheyuan</first><last>Zhang</last></author>
      <author><first>Tianyi</first><last>Ma</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Chuxu</first><last>Zhang</last><affiliation>University of Connecticut</affiliation></author>
      <author><first>Yanfang</first><last>Ye</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>1412-1432</pages>
      <abstract>Graphs are ubiquitous structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. To model graph-structured data, graph neural networks (GNNs) have become a popular tool. However, existing GNN architectures encounter challenges in cross-graph learning where multiple graphs have different feature spaces. To address this, recent approaches introduce text-attributed graphs (TAGs), where each node is associated with a textual description, which can be projected into a unified feature space using textual encoders. While promising, this method relies heavily on the availability of text-attributed graph data, which is difficult to obtain in practice. To bridge this gap, we propose a novel method named Topology-Aware Node description Synthesis (TANS), leveraging large language models (LLMs) to convert existing graphs into text-attributed graphs. The key idea is to integrate topological information into LLMs to explain how graph topology influences node semantics. We evaluate our TANS on text-rich, text-limited, and text-free graphs, demonstrating its applicability. Notably, on text-free graphs, our method significantly outperforms existing approaches that manually design node features, showcasing the potential of LLMs for preprocessing graph-structured data in the absence of textual information. The code and data are available at https://github.com/Zehong-Wang/TANS.</abstract>
      <url hash="8e2ba33f">2025.naacl-long.65</url>
      <bibkey>wang-etal-2025-llms-convert</bibkey>
    </paper>
    <paper id="66">
      <title>Forest for the Trees: Overarching Prompting Evokes High-Level Reasoning in Large Language Models</title>
      <author><first>Haoran</first><last>Liao</last></author>
      <author><first>Shaohua</first><last>Hu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhihao</first><last>Zhu</last></author>
      <author><first>Hao</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yaohui</first><last>Jin</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>1433-1453</pages>
      <abstract>Chain-of-thought (CoT) and subsequent methods adopted a deductive paradigm that decomposes the reasoning process, demonstrating remarkable performances across NLP tasks. However, such a paradigm faces the challenge of getting bogged down in low-level semantic details, hindering large language models (LLMs) from correctly understanding, selecting, and compositing conditions. In this work, we present Overarching Prompting (OaP), a simple prompting method that elicits the high-level thinking of LLMs. Specifically, OaP first abstracts the whole problem into a simplified archetype and formulates strategies grounded in concepts and principles, establishing an overarching perspective for guiding reasoning. We conducted experiments with SoTA models, including ChatGPT, InstructGPT, and Llama3-70B-instruct, and received promising performances across tasks including Knowledge QA, Mathematical, and Open-Domain Reasoning. For instance, OaP improved ChatGPT and CoT by 19.0% and 3.1% on MMLU’s College Physics, 8.8% and 2.3% on GSM8k, and 10.3% and 2.5% on StrategyQA, respectively.</abstract>
      <url hash="1fd1984e">2025.naacl-long.66</url>
      <bibkey>liao-etal-2025-forest</bibkey>
    </paper>
    <paper id="67">
      <title>On the Role of Speech Data in Reducing Toxicity Detection Bias</title>
      <author><first>Samuel</first><last>Bell</last><affiliation>Facebook</affiliation></author>
      <author><first>Mariano Coria</first><last>Meglioli</last><affiliation>Meta</affiliation></author>
      <author><first>Megan</first><last>Richards</last><affiliation>New York University</affiliation></author>
      <author><first>Eduardo</first><last>Sánchez</last><affiliation>University College London, University of London and Meta</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta and Syntexys Inc</affiliation></author>
      <author><first>Skyler</first><last>Wang</last></author>
      <author><first>Adina</first><last>Williams</last><affiliation>FAIR (Meta Platforms Inc.)</affiliation></author>
      <author><first>Levent</first><last>Sagun</last><affiliation>Meta</affiliation></author>
      <author><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <pages>1454-1468</pages>
      <abstract>Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTOX dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction.</abstract>
      <url hash="7fec88f0">2025.naacl-long.67</url>
      <bibkey>bell-etal-2025-role</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>ITALIC</fixed-case>: An <fixed-case>I</fixed-case>talian Culture-Aware Natural Language Benchmark</title>
      <author><first>Andrea</first><last>Seveso</last></author>
      <author><first>Daniele</first><last>Potertì</last></author>
      <author><first>Edoardo</first><last>Federici</last><affiliation>University of Milan</affiliation></author>
      <author><first>Mario</first><last>Mezzanzanica</last></author>
      <author><first>Fabio</first><last>Mercorio</last></author>
      <pages>1469-1478</pages>
      <abstract>We present ITALIC, a large-scale benchmark dataset of 10,000 multiple-choice questions designed to evaluate the natural language understanding of the Italian language and culture. ITALIC spans 12 domains, exploiting public tests to score domain experts in real-world scenarios. We detail our data collection process, stratification techniques, and selection strategies. ITALIC provides a comprehensive assessment suite that captures commonsense reasoning and linguistic proficiency in a morphologically rich language. We establish baseline performances using 17 state-of-the-art LLMs, revealing current limitations in Italian language understanding and highlighting significant linguistic complexity and cultural specificity challenges. ITALIC serves as a benchmark for evaluating existing models and as a roadmap for future research, encouraging the development of more sophisticated and culturally aware natural language systems.</abstract>
      <url hash="c35f5413">2025.naacl-long.68</url>
      <bibkey>seveso-etal-2025-italic</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>RAP</fixed-case>: A Metric for Balancing Repetition and Performance in Open-Source Large Language Models</title>
      <author><first>Donghao</first><last>Huang</last></author>
      <author><first>Thanh-Son</first><last>Nguyen</last><affiliation>Agency for Science, Technology and Research</affiliation></author>
      <author><first>Fiona</first><last>Liausvia</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Zhaoxia</first><last>Wang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>1479-1496</pages>
      <abstract>Large Language Models (LLMs) have significantly advanced natural language processing, but content repetition in open-source LLMs remains a critical challenge that adversely affects user experience. The repetition penalty parameter (RPP) aims to mitigate this issue by preventing repeated content generation, but excessive use of RPP can compromise the overall quality. In this paper, we propose Repetition-Aware Performance (RAP), a novel evaluation metric that quantifies and integrates repetition penalty into the assessment of model performance, enabling tuning of RPP. We evaluate our approach using twelve open-source LLMs, ranging from 2 billion to 70 billion parameters, tested on question answering and machine translation tasks across three datasets with varying prompting techniques. Experimental results show that RAP effectively tunes RPP, helping to identify a trade-off value that significantly reduces repetition while minimizing performance loss. Upon acceptance, we will release the code and the dataset of generated text, providing a valuable resource for further research on repetition detection and LLMs evaluation.</abstract>
      <url hash="01e3e243">2025.naacl-long.69</url>
      <bibkey>huang-etal-2025-rap</bibkey>
    </paper>
    <paper id="70">
      <title>Improving Data Annotation for Low-Resource Relation Extraction with Logical Rule-Augmented Collaborative Language Models</title>
      <author><first>Xiyang</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Chunming</first><last>Hu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junfan</first><last>Chen</last><affiliation>Beihang University</affiliation></author>
      <author><first>Baowen</first><last>Xu</last><affiliation>Avic Digital Corporation</affiliation></author>
      <pages>1497-1510</pages>
      <abstract>Low-resource relation extraction aims to identify semantic relationships between entities using scarce labeled data. Recent studies exploit large language models to recognize relations based on retrieved examplars, yielding promising results. However, the reliability of predictions from these methods is constrained by the presence of irrelevant context within demonstrations and the inherent flaws of large language models in producing undesired outputs. Inspired by the precision and generalization of abstract logic, in this paper, we propose distilling logical rules to uniformly represent task knowledge sourced from distinct origins and facilitate deductive reasoning. We develop a collaborative annotating framework that iteratively integrates high-confidence predictions of rule-enhanced relation extractors with varying scales, efficiently obtaining reliable pseudo annotations from massive unlabeled samples without human supervision. Experiments under two inference settings show that our approach achieves new state-of-the-art performance on benchmark datasets in few-shot scenarios.</abstract>
      <url hash="5b1de0f7">2025.naacl-long.70</url>
      <bibkey>liu-etal-2025-improving</bibkey>
    </paper>
    <paper id="71">
      <title><fixed-case>C</fixed-case>omp<fixed-case>A</fixed-case>ct: Compressed Activations for Memory-Efficient <fixed-case>LLM</fixed-case> Training</title>
      <author><first>Yara</first><last>Shamshoum</last><affiliation>Computer Science Department, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Nitzan</first><last>Hodos</last></author>
      <author><first>Yuval</first><last>Sieradzki</last><affiliation>Computer Science Department, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Assaf</first><last>Schuster</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <pages>1511-1524</pages>
      <abstract>We introduce CompAct, a technique that reduces peak memory utilization on GPU by 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory is a major limiting factor in training LLMs, with various recent works aiming to reduce model memory. However most works don’t target the largest component of allocated memory during training: the model’s compute graph, which is stored for the backward pass. By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters. Our compression uses random projection matrices, thus avoiding additional memory overheads. Comparisons with previous techniques for either pretraining or fine-tuning show that CompAct substantially improves existing compute-performance tradeoffs. We expect CompAct’s savings to scale even higher for larger models.</abstract>
      <url hash="d3d442a2">2025.naacl-long.71</url>
      <bibkey>shamshoum-etal-2025-compact</bibkey>
    </paper>
    <paper id="72">
      <title>Large Language Models Are Cross-Lingual Knowledge-Free Reasoners</title>
      <author><first>Peng</first><last>Hu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Sizhe</first><last>Liu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Changjiang</first><last>Gao</last></author>
      <author><first>Xin</first><last>Huang</last><affiliation>China Mobile Communications Company Limited Research Institute</affiliation></author>
      <author><first>Xue</first><last>Han</last></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Chao</first><last>Deng</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>1525-1542</pages>
      <abstract>Large Language Models have demonstrated impressive reasoning capabilities across multiple languages. However, the relationship between capabilities in different languages is less explored. In this work, we decompose the process of reasoning tasks into two separated components: knowledge retrieval and knowledge-free reasoning, and analyze the relationship between cross-lingual transferability and these two components. With adapted commonsense reasoning datasets and constructed knowledge-free reasoning datasets, we show that the knowledge-free reasoning capability can be nearly perfectly transferred across various source-target language directions despite the secondary impact of resource in some specific target languages, while cross-lingual knowledge retrieval significantly hinders the transfer. Moreover, by analyzing the hidden states and feed-forward network neuron activation during the reasoning, we show that higher similarity of hidden representations and larger overlap of activated neurons could explain the better cross-lingual transferability of knowledge-free reasoning than knowledge retrieval. Thus, we hypothesize that knowledge-free reasoning shares similar neurons in different languages for reasoning, while knowledge is stored separately in different languages.</abstract>
      <url hash="798a1bf9">2025.naacl-long.72</url>
      <bibkey>hu-etal-2025-large-language</bibkey>
    </paper>
    <paper id="73">
      <title>What Did <fixed-case>I</fixed-case> Do Wrong? Quantifying <fixed-case>LLM</fixed-case>s’ Sensitivity and Consistency to Prompt Engineering</title>
      <author><first>Federico</first><last>Errica</last><affiliation>NEC</affiliation></author>
      <author><first>Davide</first><last>Sanvito</last></author>
      <author><first>Giuseppe</first><last>Siracusano</last><affiliation>NEC</affiliation></author>
      <author><first>Roberto</first><last>Bifulco</last><affiliation>NEC</affiliation></author>
      <pages>1543-1558</pages>
      <abstract>Large Language Models (LLMs) changed the way we design and interact with software systems. Their ability to process and extract information from text has drastically improved productivity in a number of routine tasks. Developers that want to include these models in their software stack, however, face a dreadful challenge: debugging LLMs’ inconsistent behavior across minor variations of the prompt. We therefore introduce two metrics for classification tasks, namely *sensitivity* and *consistency*, which are complementary to task performance. First, sensitivity measures changes of predictions across rephrasings of the prompt, and does not require access to ground truth labels. Instead, consistency measures how predictions vary across rephrasings for elements of the same class. We perform an empirical comparison of these metrics on text classification tasks, using them as guideline for understanding failure modes of the LLM. Our hope is that sensitivity and consistency will be helpful to guide prompt engineering and obtain LLMs that balance robustness with performance.</abstract>
      <url hash="bfec669e">2025.naacl-long.73</url>
      <bibkey>errica-etal-2025-wrong</bibkey>
    </paper>
    <paper id="74">
      <title>Detect, Disambiguate, and Translate: On-Demand Visual Reasoning for Multimodal Machine Translation with Large Vision-Language Models</title>
      <author><first>Danyang</first><last>Liu</last></author>
      <author><first>Fanjie</first><last>Kong</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaohang</first><last>Sun</last></author>
      <author><first>Dhruva</first><last>Patil</last><affiliation>Amazon</affiliation></author>
      <author><first>Avijit</first><last>Vajpayee</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhu</first><last>Liu</last><affiliation>Amazon Prime Video</affiliation></author>
      <author><first>Vimal</first><last>Bhat</last><affiliation>Amazon</affiliation></author>
      <author><first>Najmeh</first><last>Sadoughi</last><affiliation>Amazon</affiliation></author>
      <pages>1559-1570</pages>
      <abstract>Multimodal machine translation (MMT) aims to leverage additional modalities to assist in language translation. With limited parallel data, current MMT systems rely heavily on monolingual English captioning data. These systems face three key issues: they often overlook that visual signals are unnecessary in many cases, they lack transparency in how visual information is used for disambiguation when needed, and they have yet to fully explore the potential of large-scale vision-language models (LVLMs) for MMT tasks. To address these issues, we propose the Detect, Disambiguate, and Translate (DeDiT) framework, the first reasoning-based framework for MMT leveraging LVLMs. DeDiT detects ambiguity in the input sentence, performs visual reasoning only when ambiguity is found, and generates the final translation.We implemented two versions of DeDiT: a prompting method for large proprietary LVLMs and a fine-tuning method for smaller LVLMs using synthetic data. Experiments on the Multi30K and CoMMuTE benchmarks show that DeDiT outperforms state-of-the-art models in disambiguation accuracy and translation quality. We also introduce an improved evaluation metric for disambiguation accuracy that enhances performance assessment and can be applied to proprietary models accessed via APIs.</abstract>
      <url hash="76d53117">2025.naacl-long.74</url>
      <bibkey>liu-etal-2025-detect</bibkey>
    </paper>
    <paper id="75">
      <title>Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding</title>
      <author><first>Xinhao</first><last>Xu</last></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Mengyao</first><last>Lyu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Sicheng</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yizhe</first><last>Xiong</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <author><first>Zijia</first><last>Lin</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Jungong</first><last>Han</last><affiliation>The University of Sheffield and University of Sheffield</affiliation></author>
      <author><first>Guiguang</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <pages>1571-1590</pages>
      <abstract>Multi-modal large language models (MLLMs) integrate the inherent text generation capabilities of large language models with an understanding of other modalities, promising wide applications in open-ended tasks. Despite their success, they often generate plausible but incorrect content. This phenomenon, known as hallucination, significantly impacts their practical deployment. In this paper, we delve into the intrinsic characteristics of hallucination from the perspective of interaction between input and output tokens. We find that the hallucination typically occurs with attention reduction of output tokens to image tokens. Based on this observation, we introduce image Token attention-guided Decoding (iTaD), a plug-and-play method which leverages MLLMs’ internal representations to mitigate their hallucinations. We first define an image token attention vector to measure the inter-layer differences in attention of output tokens to image tokens across different layers. Based on the vector, we design a novel layer selection strategy and conduct inter-layer contrastive decoding to highlight the progression in image understanding, thereby exploiting attention to image tokens to mitigate hallucinations. Extensive experiments well demonstrate iTaD’s effectiveness across different MLLMs and benchmarks.</abstract>
      <url hash="f44ad284">2025.naacl-long.75</url>
      <bibkey>xu-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="76">
      <title>A Multi-modal Large Language Model with Graph-of-Thought for Effective Recommendation</title>
      <author><first>Zixuan</first><last>Yi</last></author>
      <author><first>Iadh</first><last>Ounis</last><affiliation>University of Glasgow</affiliation></author>
      <pages>1591-1606</pages>
      <abstract>Chain-of-Thought (CoT) prompting has been shown to be effective in guiding Large Language Models (LLMs) to decompose complex tasks into multiple intermediate steps, and constructing a rational reasoning chain for inferring answers. However, the linear nature of CoT falls short from enabling LLMs to effectively handle graph structures, which are essential for personalized recommendation tasks that rely on user-item interaction graphs. To bridge this gap, we introduce GollaRec, which leverages a Graph-of-Thought (GoT) prompting technique in a Multi-modal LLM, namely LLaVA, to effectively exploit the complex structure of the interaction graphs. GollaRec enhances the recommendation effectiveness by integrating both visual and textual “thoughts” into a graph-structured prompt, using both item images and descriptions to produce richer multi-modal user/item representations. In our proposed approach, GollaRec leverages text-graph alignment and graph instruction tuning to allow the Multi-modal LLM to capture complex graph structures. In addition, GollaRec leverages a graph adaptor to integrate user-item interactions into the resulting user/item embeddings, therefore effectively adapting the model to the recommendation task. Our extensive experiments on 6 benchmark datasets demonstrate the superiority of our proposed GollaRec model over 12 existing state-of-the-art models in various multi-modal recommendation tasks, including general and multi-domain recommendation tasks.</abstract>
      <url hash="463be3a7">2025.naacl-long.76</url>
      <bibkey>yi-ounis-2025-multi</bibkey>
    </paper>
    <paper id="77">
      <title>Investigating Human Values in Online Communities</title>
      <author><first>Nadav</first><last>Borenstein</last></author>
      <author><first>Arnav</first><last>Arora</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Lucie-Aimée</first><last>Kaffee</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>1607-1627</pages>
      <abstract>Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over nine million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, we discover a very negative stance towards conformity in the Vegan and AbolishTheMonarchy subreddits. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states. Through our work, we demonstrate how our dataset and method can be used as a complementary tool for qualitative study of online communication.</abstract>
      <url hash="f1023824">2025.naacl-long.77</url>
      <bibkey>borenstein-etal-2025-investigating</bibkey>
    </paper>
    <paper id="78">
      <title>Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation</title>
      <author><first>Tianyu</first><last>Liu</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Jirui</first><last>Qi</last></author>
      <author><first>Paul</first><last>He</last></author>
      <author><first>Arianna</first><last>Bisazza</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>1628-1647</pages>
      <abstract>Recent work suggests that large language models enhanced with retrieval-augmented generation are easily influenced by the order in which the retrieved documents are presented to the model when solving tasks such as question answering (QA).However, there is no method to date that exploits this phenomenon to improve generation.To fill this gap, in this study, we show that the pointwise mutual information between a context and a question is an effective gauge for language model performance.Importantly, this gauge does not depend on knowing the answer to the question <i>a priori</i>.Through experiments on two question-answering datasets using a variety of large language models, we find evidence for an empirical correlation between answer accuracy and pointwise mutual information.Additionally, we propose two methods that use the pointwise mutual information between a document and a question as a gauge for selecting and constructing prompts that lead to better performance, whose effectiveness we demonstrate through experimentation.</abstract>
      <url hash="f94bec14">2025.naacl-long.78</url>
      <bibkey>liu-etal-2025-pointwise</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>MATO</fixed-case>: A Model-Agnostic Training Optimization for Aspect Sentiment Triplet Extraction</title>
      <author><first>Shaopeng</first><last>Tang</last><affiliation>Wuhan University of Technology</affiliation></author>
      <author><first>Lin</first><last>Li</last><affiliation>Wuhan University of Technology</affiliation></author>
      <author><first>Xiaohui</first><last>Tao</last><affiliation>University of Southern Queensland</affiliation></author>
      <author><first>Leqi</first><last>Zhong</last><affiliation>Wuhan University of Technology</affiliation></author>
      <author><first>Qing</first><last>Xie</last><affiliation>Wuhan University of Technology</affiliation></author>
      <pages>1648-1662</pages>
      <abstract>As an important fine-grained sentiment analysis task, aspect sentiment triplet extraction (ASTE) aims to identify three elements, i.e., aspect, opinion and sentiment polarity as a triplet. Advanced ASTE researches have mostly explored triplet-wise ability to achieve superior improvement. However, existing models with strong in-house performances may struggle to generalize to the challenging cases with the diverse expression of inter-triplet and intra-triplet elements. To this end, we propose a **M**odel-**A**gnostic **T**raining **O**ptimization (**MATO**) to improve ASTE model inference consistent with expected results facing triplet element diversity. Specifically, we design inter-triplet and intra-triplet metamorphic relations (MRs), and calculate the violation rate (VR) on each element of one triplet through metamorphic testing (MT), indicating the capacity to accommodate the diverse elements. Moreover, we propose an element-wise diversity-aware loss based on the VRs of aspect, opinion and sentiment, which can be jointly trained with existed ASTE models via uncertainty weighing. Conducted on four benchmark datasets and seven ASTE models, experimental results show that our MATO can enhance their diversity capacity, decreasing the average element-wise VRs by 3.28% to 15.36%. Meanwhile, our MATO is comparable to or better than those in terms of F1-score.</abstract>
      <url hash="f8d42639">2025.naacl-long.79</url>
      <bibkey>tang-etal-2025-mato</bibkey>
    </paper>
    <paper id="80">
      <title>Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts</title>
      <author><first>Tong</first><last>Zhu</last></author>
      <author><first>Daize</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jiacheng</first><last>Ruan</last></author>
      <author><first>Wenliang</first><last>Chen</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1663-1677</pages>
      <abstract>Mixture-of-Experts (MoE) models have shown remarkable capability in instruction tuning, especially when the number of tasks scales. However, previous methods simply merge all training tasks (e.g. creative writing, coding, and mathematics) and apply fixed sampling weights, without considering the importance of different tasks as the model training state changes. In this way, the most helpful data cannot be effectively distinguished, leading to suboptimal model performance. To reduce the potential redundancies of datasets, we make the first attempt and propose a novel dynamic data mixture for MoE instruction tuning. Specifically, inspired by MoE’s token routing preference, we build dataset-level representations and then capture the subtle differences among datasets. Finally, we propose to dynamically adjust the sampling weight of datasets by their inter-redundancies, thus maximizing global performance under a limited training budget. The experimental results on two MoE models demonstrate the effectiveness of our approach on both downstream knowledge &amp; reasoning tasks and open-ended queries.</abstract>
      <url hash="bc715e37">2025.naacl-long.80</url>
      <bibkey>zhu-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="81">
      <title><fixed-case>E</fixed-case>mo<fixed-case>D</fixed-case>ynami<fixed-case>X</fixed-case>: Emotional Support Dialogue Strategy Prediction by Modelling <fixed-case>M</fixed-case>i<fixed-case>X</fixed-case>ed Emotions and Discourse Dynamics</title>
      <author><first>Chenwei</first><last>Wan</last></author>
      <author><first>Matthieu</first><last>Labeau</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA</affiliation></author>
      <pages>1678-1695</pages>
      <abstract>Designing emotionally intelligent conversational systems to provide comfort and advice to people experiencing distress is a compelling area of research. Recently, with advancements in large language models (LLMs), end-to-end dialogue agents without explicit strategy prediction steps have become prevalent. However, implicit strategy planning lacks transparency, and recent studies show that LLMs’ inherent preference bias towards certain socio-emotional strategies hinders the delivery of high-quality emotional support. To address this challenge, we propose decoupling strategy prediction from language generation, and introduce a novel dialogue strategy prediction framework, EmoDynamiX, which models the discourse dynamics between user fine-grained emotions and system strategies using a heterogeneous graph for better performance and transparency. Experimental results on two ESC datasets show EmoDynamiX outperforms previous state-of-the-art methods with a significant margin (better proficiency and lower preference bias). Our approach also exhibits better transparency by allowing backtracing of decision making.</abstract>
      <url hash="27629d30">2025.naacl-long.81</url>
      <bibkey>wan-etal-2025-emodynamix</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>R</fixed-case>eas<fixed-case>VQA</fixed-case>: Advancing <fixed-case>V</fixed-case>ideo<fixed-case>QA</fixed-case> with Imperfect Reasoning Process</title>
      <author><first>Jianxin</first><last>Liang</last></author>
      <author><first>Xiaojun</first><last>Meng</last><affiliation>Noah’s Ark Lab, Huawei Technologies Ltd.</affiliation></author>
      <author><first>Huishuai</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yueqian</first><last>Wang</last></author>
      <author><first>Jiansheng</first><last>Wei</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>1696-1709</pages>
      <url hash="117c5dcb">2025.naacl-long.82</url>
      <bibkey>liang-etal-2025-reasvqa</bibkey>
    </paper>
    <paper id="83">
      <title>Divergent Thoughts toward One Goal: <fixed-case>LLM</fixed-case>-based Multi-Agent Collaboration System for Electronic Design Automation</title>
      <author><first>Haoyuan</first><last>Wu</last></author>
      <author><first>Haisheng</first><last>Zheng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhuolun</first><last>He</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Bei</first><last>Yu</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <pages>1710-1721</pages>
      <abstract>Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts.However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms.Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps.Any errors will lead to the instability and failure of EDA flow automation.To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation.Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.</abstract>
      <url hash="efd4dc9d">2025.naacl-long.83</url>
      <bibkey>wu-etal-2025-divergent</bibkey>
    </paper>
    <paper id="84">
      <title>A Survey of <fixed-case>QUD</fixed-case> Models for Discourse Processing</title>
      <author><first>Yingxue</first><last>Fu</last><affiliation>Centre Inria d’Université Côte d’Azur</affiliation></author>
      <pages>1722-1732</pages>
      <abstract>Question Under Discussion (QUD), which is originally a linguistic analytic framework, gains increasing attention in the community of natural language processing over the years. Various models have been proposed for implementing QUD for discourse processing. This survey summarizes these models, with a focus on application to written texts, and examines studies that explore the relationship between QUD and mainstream discourse frameworks, including RST, PDTB and SDRT. Some questions that may require further study are suggested.</abstract>
      <url hash="f6c41774">2025.naacl-long.84</url>
      <bibkey>fu-2025-survey</bibkey>
    </paper>
    <paper id="85">
      <title><fixed-case>S</fixed-case>afety<fixed-case>Q</fixed-case>uizzer: Timely and Dynamic Evaluation on the Safety of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhichao</first><last>Shi</last><affiliation>institute of computing technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaoling</first><last>Jing</last></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Yuanzhuo</first><last>Wang</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Jie</first><last>Zhang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>1733-1747</pages>
      <abstract>With the expansion of the application of Large Language Models (LLMs), concerns about their safety have grown among researchers. Numerous studies have demonstrated the potential risks of LLMs generating harmful content and have proposed various safety assessment benchmarks to evaluate these risks. However, the evaluation questions in current benchmarks, especially for Chinese, are too straightforward, making them easily rejected by target LLMs, and difficult to update with practical relevance due to their lack of correlation with real-world events. This hinders the effective application of these benchmarks in continuous evaluation tasks. To address these limitations, we propose SafetyQuizzer, a question-generation framework designed to evaluate the safety of LLMs more sustainably in the Chinese context. SafetyQuizzer leverages a finetuned LLM and jailbreaking attack templates to generate subtly offensive questions, which reduces the decline rate. Additionally, by utilizing retrieval-augmented generation, SafetyQuizzer incorporates the latest real-world events into evaluation questions, improving the adaptability of the benchmarks. Our experiments demonstrate that evaluation questions generated by SafetyQuizzer significantly reduce the decline rate compared to other benchmarks while maintaining a comparable attack success rate. Our code is available at https://github.com/zhichao-stone/SafetyQuizzer. Warning: this paper contains examples that may be offensive or upsetting.</abstract>
      <url hash="76bd15e8">2025.naacl-long.85</url>
      <bibkey>shi-etal-2025-safetyquizzer</bibkey>
    </paper>
    <paper id="86">
      <title>Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Wei</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yulin</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Cheng</first><last>Jiayang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianshu</first><last>Chu</last></author>
      <author><first>Xuebing</first><last>Zhou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Peizhao</first><last>Hu</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>1748-1766</pages>
      <abstract>Privacy research has attracted wide attention as individuals worry that their private data can be easily leaked during interactions with smart devices, social platforms, and AI applications. Existing works mostly consider privacy attacks and defenses on various sub-fields. Within each field, various privacy attacks and defenses are studied to address patterns of personally identifiable information (PII). In this paper, we argue that privacy is not solely about PII patterns. We ground on the Contextual Integrity (CI) theory which posits that people’s perceptions of privacy are highly correlated with the corresponding social context. Based on such an assumption, we formulate privacy as a reasoning problem rather than naive PII matching. We develop the first comprehensive checklist that covers social identities, private attributes, and existing privacy regulations. Unlike prior works on CI that either cover limited expert annotated norms or model incomplete social context, our proposed privacy checklist uses the whole Health Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to show that we can resort to large language models (LLMs) to completely cover the HIPAA’s regulations. Additionally, our checklist also gathers expert annotations across multiple ontologies to determine private information including but not limited to PII. We use our preliminary results on the HIPAA to shed light on future context-centric privacy research to cover more privacy regulations, social norms and standards. We will release the reproducible code and data.</abstract>
      <url hash="cfa6b7fd">2025.naacl-long.86</url>
      <bibkey>li-etal-2025-privacy</bibkey>
    </paper>
    <paper id="87">
      <title>Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion</title>
      <author><first>Ziyao</first><last>Xu</last><affiliation>Peking University</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>1767-1783</pages>
      <abstract>Humans have strong capabilities of decomposition and composition in natural-to-formal language conversion (N2F) when faced with an unfamiliar formal language, and can easily cope with compositional gaps and counter-intuitive symbolic names. To investigate whether large language models (LLMs) have this set of basic capabilities in N2F, we propose the STD framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition and composition capabilities of LLMs in N2F. Based on this framework, we evaluate and analyze the most advanced LLMs, and the main findings include that: (1) the LLMs are deficient in both decomposition and composition; (2) the LLMs show a wide coverage of error types that can be attributed to deficiencies in natural language understanding and the learning and use of symbolic systems; (3) compositional gaps and counter-intuitive symbolic names both affect the decomposition and composition of the LLMs. Our work provides a new perspective for investigating the basic capabilities of decomposition and composition of LLMs in N2F. The detailed analysis of deficiencies and attributions can help subsequent improvements of LLMs.</abstract>
      <url hash="32e2ec60">2025.naacl-long.87</url>
      <bibkey>xu-wang-2025-investigating</bibkey>
    </paper>
    <paper id="88">
      <title>Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring</title>
      <author><first>Honglin</first><last>Mu</last><affiliation>Harbin Institute Of Technology</affiliation></author>
      <author><first>Han</first><last>He</last></author>
      <author><first>Yuxin</first><last>Zhou</last></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Yang</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Qi</first><last>Shi</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1784-1799</pages>
      <abstract>Large language model (LLM) safety is a critical issue, with numerous studies employing red team testing to enhance model security. Among these, jailbreak methods explore potential vulnerabilities by crafting malicious prompts that induce model outputs contrary to safety alignments. Existing black-box jailbreak methods often rely on model feedback, repeatedly submitting queries with detectable malicious instructions during the attack search process. Although these approaches are effective, the attacks may be intercepted by content moderators during the search process. We propose an improved transfer attack method that guides malicious prompt construction by locally training a mirror model of the target black-box model through benign data distillation. This method offers enhanced stealth, as it does not involve submitting identifiable malicious instructions to the target model during the search phase. Our approach achieved a maximum attack success rate of 92%, or a balanced value of 80% with an average of 1.5 detectable jailbreak queries per sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore the need for more robust defense mechanisms.</abstract>
      <url hash="1be53841">2025.naacl-long.88</url>
      <bibkey>mu-etal-2025-stealthy</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>V</fixed-case>ivid<fixed-case>M</fixed-case>ed: Vision Language Model with Versatile Visual Grounding for Medicine</title>
      <author><first>Lingxiao</first><last>Luo</last><affiliation>Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Bingda</first><last>Tang</last></author>
      <author><first>Xuanzhong</first><last>Chen</last><affiliation>, Tsinghua University</affiliation></author>
      <author><first>Rong</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Ting</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <pages>1800-1821</pages>
      <abstract>Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable promise in generating visually grounded responses. However, their application in the medical domain is hindered by unique challenges. For instance, most VLMs rely on a single method of visual grounding, whereas complex medical tasks demand more versatile approaches. Additionally, while most VLMs process only 2D images, a large portion of medical images are 3D. The lack of medical data further compounds these obstacles. To address these challenges, we present VividMed, a vision language model with versatile visual grounding for medicine. Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data. We design a three-stage training procedure and an automatic data synthesis pipeline based on open datasets and models. Besides visual grounding tasks, VividMed also excels in other common downstream tasks, including Visual Question Answering (VQA) and report generation. Ablation studies empirically show that the integration of visual grounding ability leads to improved performance on these tasks. Our code is publicly available at https://github.com/function2-llx/MMMM.</abstract>
      <url hash="128b9c79">2025.naacl-long.89</url>
      <bibkey>luo-etal-2025-vividmed</bibkey>
    </paper>
    <paper id="90">
      <title>Mixture of Multimodal Adapters for Sentiment Analysis</title>
      <author><first>Kezhou</first><last>Chen</last></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Huixia</first><last>Ben</last><affiliation>Anhui University of Science and Technology</affiliation></author>
      <author><first>Shengeng</first><last>Tang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Yanbin</first><last>Hao</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>1822-1833</pages>
      <abstract>Pre-trained language model (PLM) have achieved great success in text sentiment analysis. However, in practical applications, sentiment is not only conveyed through language but also hidden in other modalities. Therefore, multimodal sentiment analysis (MSA) has attracted increasing research interest. Compared to text sentiment analysis, MSA is challenging since (1) emotions hidden in body movements or vocal timbres eclipse traditional analytical methods, and (2) transferring PLM to MSA task requires huge training parameters. Thus, to solve these issues, we introduce the Mixture of Multimodal Adapters (MMA) into the PLM. Specifically, we first design a mixture-of-multimodal-experts module to capture and fuse emotional movements from different data. Meanwhile, we use a compression parameter for each expert to reduce the training burden. We apply our method to two benchmark datasets and achieve state-of-the-art performance with a tiny trainable parameter count. For example, compared to the current state-of-the-art method, AcFormer, we only need 1/22 of its training parameters amount (130M<tex-math>\rightarrow</tex-math>6M) to achieve better results.</abstract>
      <url hash="01b06b99">2025.naacl-long.90</url>
      <bibkey>chen-etal-2025-mixture</bibkey>
    </paper>
    <paper id="91">
      <title>The Impact of Inference Acceleration on Bias of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Elisabeth</first><last>Kirsten</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <author><first>Vedant</first><last>Nanda</last><affiliation>University of Maryland, College Park, MPI-SWS and University of Maryland, College Park &amp; MPI-SWS</affiliation></author>
      <author><first>Muhammad Bilal</first><last>Zafar</last><affiliation>Ruhr-Universität Bochum and Research Center for Trustworthy Data Science and Security</affiliation></author>
      <pages>1834-1853</pages>
      <abstract>Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.This paper contains prompts and outputs which may be deemed offensive.</abstract>
      <url hash="096b81e9">2025.naacl-long.91</url>
      <bibkey>kirsten-etal-2025-impact</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>A</fixed-case>fri<fixed-case>H</fixed-case>ate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last><affiliation>Imperial College London and Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Idris</first><last>Abdulmumin</last><affiliation>Ahmadu Bello University</affiliation></author>
      <author><first>Abinew Ali</first><last>Ayele</last><affiliation>Bahir Dar University, Universität Hamburg</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>Ibrahim Said</first><last>Ahmad</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Saminu Mohammad</first><last>Aliyu</last></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Abigail</first><last>Oppong</last></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Chiamaka Ijeoma</first><last>Chukwuneke</last><affiliation>Nnamdi Azikiwe University</affiliation></author>
      <author><first>Ebrahim Chekol</first><last>Jibril</last></author>
      <author><first>Elyas Abdi</first><last>Ismail</last></author>
      <author><first>Esubalew</first><last>Alemneh</last><affiliation>Bahir Dar University</affiliation></author>
      <author><first>Hagos Tesfahun</first><last>Gebremichael</last></author>
      <author><first>Lukman Jibril</first><last>Aliyu</last></author>
      <author><first>Meriem</first><last>Beloucif</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Oumaima</first><last>Hourrane</last><affiliation>Al Akhawayn University</affiliation></author>
      <author><first>Rooweither</first><last>Mabuya</last><affiliation>North-West University</affiliation></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Samuel</first><last>Rutunda</last></author>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Tadesse Kebede</first><last>Guge</last><affiliation>Haramaya University</affiliation></author>
      <author><first>Tesfa Tegegne</first><last>Asfaw</last></author>
      <author><first>Lilian Diana Awuor</first><last>Wanzare</last><affiliation>Maseno University</affiliation></author>
      <author><first>Nelson Odhiambo</first><last>Onyango</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <pages>1854-1871</pages>
      <abstract>Hate speech and abusive language are global phenomena that need socio-cultural background knowledge to be understood, identified, and moderated. However, in many regions of the Global South, there have been several documented occurrences of (1) absence of moderation and (2) censorship due to the reliance on keyword spotting out of context. Further, high-profile individuals have frequently been at the center of the moderation process, while large and targeted hate speech campaigns against minorities have been overlooked.These limitations are mainly due to the lack of high-quality data in the local languages and the failure to include local communities in the collection, annotation, and moderation processes. To address this issue, we present AfriHate: a multilingual collection of hate speech and abusive language datasets in 15 African languages. Each instance in <b>AfriHate</b> is a tweet annotated by native speakers familiar with the regional culture. We report the challenges related to the construction of the datasets and present various classification baseline results with and without using LLMs. We find that model performance highly depends on the language and that multilingual models can help boost performance in low-resource settings.</abstract>
      <url hash="bc3688f0">2025.naacl-long.92</url>
      <bibkey>muhammad-etal-2025-afrihate</bibkey>
    </paper>
    <paper id="93">
      <title>Revealing the Barriers of Language Agents in Planning</title>
      <author><first>Jian</first><last>Xie</last></author>
      <author><first>Kexun</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>1872-1888</pages>
      <abstract>Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.</abstract>
      <url hash="c471a101">2025.naacl-long.93</url>
      <bibkey>xie-etal-2025-revealing</bibkey>
    </paper>
    <paper id="94">
      <title>You Only Read Once (<fixed-case>YORO</fixed-case>): Learning to Internalize Database Knowledge for Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Hideo</first><last>Kobayashi</last><affiliation>Amazon</affiliation></author>
      <author><first>Wuwei</first><last>Lan</last><affiliation>Amazon</affiliation></author>
      <author><first>Peng</first><last>Shi</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Shuaichen</first><last>Chang</last></author>
      <author><first>Jiang</first><last>Guo</last><affiliation>Amazon</affiliation></author>
      <author><first>Henghui</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Ng</last><affiliation>Amazon</affiliation></author>
      <pages>1889-1901</pages>
      <abstract>While significant progress has been made on the text-to-SQL task, recent solutions repeatedly encode the same database schema for every question, resulting in unnecessary high inference cost and often overlooking crucial database knowledge. To address these issues, we propose You Only Read Once (YORO), a novel paradigm that directly internalizes database knowledge into the parametric knowledge of a text-to-SQL model during training and eliminates the need for schema encoding during inference. YORO significantly reduces the input token length by 66%-98%. Despite its shorter inputs, our empirical results demonstrate YORO’s competitive performances with traditional systems on three benchmarks as well as its significant outperformance on large databases. Furthermore, YORO excels in handling questions with challenging value retrievals such as abbreviation.</abstract>
      <url hash="e8eaae38">2025.naacl-long.94</url>
      <bibkey>kobayashi-etal-2025-read</bibkey>
    </paper>
    <paper id="95">
      <title>Option Symbol Matters: Investigating and Mitigating Multiple-Choice Option Symbol Bias of Large Language Models</title>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Ping</first><last>Jian</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Chengzhi</first><last>Li</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>1902-1917</pages>
      <abstract>Multiple-Choice Question Answering (MCQA) is a widely used task in the evaluation of Large Language Models (LLMs). In this work, we reveal that current LLMs’ performance in MCQA could be heavily influenced by the choice of option symbol sets, due to the option symbol bias. That is, when altering only the option symbols (e.g., A/B/C/D<tex-math>\rightarrow</tex-math>i/ii/iii/iv), the results could vary sharply, leading to a margin of approximately 10% in accuracy. To uncover the mechanisms behind this, we investigate the internal components of LLMs from a causal perspective. By measuring the causal effects, we identify a small subset of attention heads responsible for the symbol bias. Subsequently, we interpret these key components in a human-understandable way, showing that attention heads with higher causal effects are more likely to focus on only option symbols, while those with lower causal effects tend to distribute their attention across the content of questions and options. It also motivates us to pursue debiasing based on the causal effects. Specifically, to mitigate such bias, we propose a tuning-free, causal effect driven debiasing method which intervenes the activations of identified components according to their causal effects, with stronger interventions corresponding to higher causal effects. Experimental results demonstrate that the proposed method not only alleviates aforementioned bias, but also improves the MCQA performance of LLMs.</abstract>
      <url hash="33497dea">2025.naacl-long.95</url>
      <bibkey>yang-etal-2025-option</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>DAWN</fixed-case>-<fixed-case>ICL</fixed-case>: Strategic Planning of Problem-solving Trajectories for Zero-Shot In-Context Learning</title>
      <author><first>Xinyu</first><last>Tang</last><affiliation>Renmin University of China</affiliation></author>
      <author id="xiaolei-wang-fudan"><first>Xiaolei</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1918-1934</pages>
      <abstract>Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning (ICL) without using human-annotated demonstrations.Existing ZS-ICL methods either use large language models (LLMs) to generate (input, label) pairs as pseudo-demonstrations or leverage historical pseudo-demonstrations to help solve the current problem.They assume that all problems are from the same task and traverse them in a random order.However, in real-world scenarios, problems usually come from diverse tasks, and only a few belong to the same task.The random traversing order may generate unreliable pseudo-demonstrations and lead to error accumulation.To address this problem, we reformulate ZS-**ICL** as a planning problem and propose a **D**emonstration-**AW**are Mo**N**te Carlo Tree Search (MCTS) approach (DAWN-ICL), which leverages MCTS to strategically plan the problem-solving trajectories for ZS-ICL.In addition, to achieve effective and efficient <tex-math>Q</tex-math> value estimation, we propose a demonstration-aware <tex-math>Q</tex-math>-value function and use it to enhance the selection phase and accelerate the expansion and simulation phases in MCTS.Extensive experiments demonstrate the effectiveness and efficiency of DAWN-ICL on in-domain and cross-domain scenarios, and it even outperforms ICL using human-annotated demonstrations.The code is available at https://github.com/txy77/MCTS4ZSICL.</abstract>
      <url hash="b786597c">2025.naacl-long.96</url>
      <bibkey>tang-etal-2025-dawn</bibkey>
    </paper>
    <paper id="97">
      <title><fixed-case>LL</fixed-case>a<fixed-case>SA</fixed-case>: Large Language and Structured Data Assistant</title>
      <author><first>Yao</first><last>Xu</last></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiabei</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>ZengXiangrong</first><last>ZengXiangrong</last></author>
      <author><first>Bingning</first><last>Wang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Guang</first><last>Liu</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>1935-1946</pages>
      <url hash="2383f9f5">2025.naacl-long.97</url>
      <bibkey>xu-etal-2025-llasa</bibkey>
    </paper>
    <paper id="98">
      <title>Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss</title>
      <author><first>Fu-An</first><last>Chao</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Berlin</first><last>Chen</last><affiliation>National Taiwan Normal University</affiliation></author>
      <pages>1947-1961</pages>
      <abstract>Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts: the former aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while the latter focuses instead on pinpointing the precise phonetic pronunciation errors made by non-native language learners. However, it is generally expected that a full-fledged CAPT system should perform both functionalities simultaneously and efficiently. In response to this surging demand, we in this work first propose HMamba, a novel CAPT approach that seamlessly integrates APA and MDD tasks in parallel. In addition, we introduce a novel loss function, decoupled cross-entropy loss (deXent), specifically tailored for MDD to facilitate better-supervised learning for detecting mispronounced phones, thereby enhancing overall performance. A comprehensive set of empirical results on the speechocean762 benchmark dataset demonstrates the effectiveness of our approach on APA. Notably, our proposed approach also yields a considerable improvement in MDD performance over a strong baseline, achieving an F1-score of 63.85%. Our codes are made available at https://github.com/Fuann/hmamba</abstract>
      <url hash="ee0a6540">2025.naacl-long.98</url>
      <bibkey>chao-chen-2025-towards</bibkey>
    </paper>
    <paper id="99">
      <title>Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models</title>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>University of Washington and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Jillian</first><last>Fisher</last><affiliation>University of Washington</affiliation></author>
      <author><first>Taylor</first><last>Sorensen</last><affiliation>University of Washington and Brigham Young University</affiliation></author>
      <author><first>Ximing</first><last>Lu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Maria</first><last>Antoniak</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Niloofar</first><last>Mireshghallah</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <pages>1962-1978</pages>
      <abstract>High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we recover what training data is known to LLMs? In this work we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights or token probabilities, by using information-guided probes. Our work builds on a key observation: text passages with high surprisal are good search material for memorization probes. By evaluating a model’s ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.</abstract>
      <url hash="1fe9ee22">2025.naacl-long.99</url>
      <bibkey>ravichander-etal-2025-information</bibkey>
    </paper>
    <paper id="100">
      <title>An Interpretable and Crosslingual Method for Evaluating Second-Language Dialogues</title>
      <author><first>Rena Wei</first><last>Gao</last></author>
      <author><first>Xuetong</first><last>Wu</last></author>
      <author><first>Carsten</first><last>Roever</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Jing</first><last>Wu</last><affiliation>Tiangong University</affiliation></author>
      <author><first>Long</first><last>Lv</last></author>
      <author><first>Jingxuan</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>1979-2008</pages>
      <abstract>We analyse the cross-lingual transferability of a dialogue evaluation framework that assesses the relationships between micro-level linguistic features (e.g. backchannels) and macro-level interactivity labels (e.g. topic management), originally designed for English-as-a-second-language dialogues. To this end, we develop CNIMA (**C**hinese **N**on-Native **I**nteractivity **M**easurement and **A**utomation), a Chinese-as-a-second-language labelled dataset with 10K dialogues. We found the evaluation framework to be robust across languages, revealing language-specific and language-universal relationships between micro-level and macro-level features. Next, we propose an automated, interpretable approach with low data requirements that scores the overall quality of a second-language dialogue based on the framework. Our approach is interpretable in that it reveals the key linguistic and interactivity features that contributed to the overall quality score. As our approach does not require labelled data, it can also be adapted to other languages for second-language dialogue evaluation.</abstract>
      <url hash="46bbea22">2025.naacl-long.100</url>
      <bibkey>gao-etal-2025-interpretable</bibkey>
    </paper>
    <paper id="101">
      <title>From Allies to Adversaries: Manipulating <fixed-case>LLM</fixed-case> Tool-Calling through Adversarial Injection</title>
      <author><first>Rupeng</first><last>Zhang</last></author>
      <author><first>Haowei</first><last>Wang</last></author>
      <author><first>Junjie</first><last>Wang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mingyang</first><last>Li</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuekai</first><last>Huang</last></author>
      <author><first>Dandan</first><last>Wang</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Qing</first><last>Wang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>2009-2028</pages>
      <abstract>Tool-calling has changed Large Language Model (LLM) applications by integrating external tools, significantly enhancing their functionality across diverse tasks. However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied. To fill this gap, we present ToolCommander, a novel framework designed to exploit vulnerabilities in LLM tool-calling systems through adversarial tool injection. Our framework employs a well-designed two-stage attack strategy. Firstly, it injects malicious tools to collect user queries, then dynamically updates the injected tools based on the stolen information to enhance subsequent attacks. These stages enable ToolCommander to execute privacy theft, launch denial-of-service attacks, and even manipulate business competition by triggering unscheduled tool-calling. Notably, the ASR reaches 91.67% for privacy theft and hits 100% for denial-of-service and unscheduled tool calling in certain cases. Our work demonstrates that these vulnerabilities can lead to severe consequences beyond simple misuse of tool-calling systems, underscoring the urgent need for robust defensive strategies to secure LLM Tool-calling systems.</abstract>
      <url hash="38701871">2025.naacl-long.101</url>
      <bibkey>zhang-etal-2025-allies</bibkey>
    </paper>
    <paper id="102">
      <title><fixed-case>COVE</fixed-case>: <fixed-case>CO</fixed-case>ntext and <fixed-case>VE</fixed-case>racity prediction for out-of-context images</title>
      <author><first>Jonathan</first><last>Tonglet</last></author>
      <author><first>Gabriel</first><last>Thiem</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>2029-2049</pages>
      <abstract>Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image’s caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction model on all context items, often by more than five percentage points. It is competitive with the best veracity prediction models on synthetic data and outperforms them on real-world data, showing that it is beneficial to combine the two tasks sequentially. Finally, we conduct a human study that reveals that the predicted context is a reusable and interpretable artifact to verify new out-of-context captions for the same image. Our code and data are made available.</abstract>
      <url hash="f68c2385">2025.naacl-long.102</url>
      <bibkey>tonglet-etal-2025-cove</bibkey>
    </paper>
    <paper id="103">
      <title>Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization</title>
      <author><first>Yang</first><last>Zhong</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh, University of Pittsburgh and University of Pittsburgh</affiliation></author>
      <pages>2050-2073</pages>
      <abstract>Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better aggregate sentence-level scores predicted by NLI models. Our approach shows improved performance on top of different model baselines over several evaluation benchmarks, covering rich domains of texts, focusing on long document summarization. This underscores the significance of incorporating discourse features in developing models for scoring summaries for long document factual inconsistency.</abstract>
      <url hash="2768b311">2025.naacl-long.103</url>
      <bibkey>zhong-litman-2025-discourse</bibkey>
    </paper>
    <paper id="104">
      <title>Language Models are Crossword Solvers</title>
      <author><first>Soumadeep</first><last>Saha</last><affiliation>Indian Statistical Institute, Kolkata</affiliation></author>
      <author><first>Sutanoya</first><last>Chakraborty</last><affiliation>Indian Statistical Institute</affiliation></author>
      <author><first>Saptarshi</first><last>Saha</last><affiliation>Indian Statistical Institute</affiliation></author>
      <author><first>Utpal</first><last>Garain</last><affiliation>Indian Statistical Institute</affiliation></author>
      <pages>2074-2090</pages>
      <abstract>Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with large language models (LLMs). We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles. Additionally, we demonstrate that LLMs generalize well and are capable of supporting answers with sound rationale.</abstract>
      <url hash="820481a8">2025.naacl-long.104</url>
      <bibkey>saha-etal-2025-language</bibkey>
    </paper>
    <paper id="105">
      <title><fixed-case>WH</fixed-case>o<fixed-case>W</fixed-case>: A Cross-domain Approach for Analysing Conversation Moderation</title>
      <author><first>Ming-Bin</first><last>Chen</last></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>2091-2126</pages>
      <abstract>We propose WHoW, an evaluation framework for analyzing the facilitation strategies of moderators across different domains/scenarios by examining their motives (Why), dialogue acts (How) and target speaker (Who). Using this framework, we annotated 5,657 moderation sentences with human judges and 15,494 sentences with GPT-4o from two domains: TV debates and radio panel discussions. Comparative analysis demonstrates the framework’s cross-domain generalisability and reveals distinct moderation strategies: debate moderators emphasise coordination and facilitate interaction through questions and instructions, while panel discussion moderators prioritize information provision and actively participate in discussions. Our analytical framework works for different moderation scenarios, enhances our understanding of moderation behaviour through automatic large-scale analysis, and facilitates the development of moderator agents.</abstract>
      <url hash="81e7fc07">2025.naacl-long.105</url>
      <bibkey>chen-etal-2025-whow</bibkey>
    </paper>
    <paper id="106">
      <title>Uplifting Lower-Income Data: Strategies for Socioeconomic Perspective Shifts in Large Multi-modal Models</title>
      <author><first>Joan</first><last>Nwatu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>2127-2144</pages>
      <abstract>Recent work has demonstrated that the unequal representation of cultures and socioeconomic groups in training data leads to biased Large Multi-modal (LMM) models. To improve LMM model performance on underrepresented data, we propose and evaluate several prompting strategies using non-English, geographic, and socioeconomic attributes. We show that these geographic and socioeconomic integrated prompts favor retrieving topic appearances commonly found in data from low-income households across different countries leading to improved LMM model performance on lower-income data. Our analyses identify and highlight contexts where these strategies yield the most improvements.</abstract>
      <url hash="d32b8108">2025.naacl-long.106</url>
      <bibkey>nwatu-etal-2025-uplifting</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>MS</fixed-case>c-<fixed-case>SQL</fixed-case>: Multi-Sample Critiquing Small Language Models For Text-To-<fixed-case>SQL</fixed-case> Translation</title>
      <author><first>Satya Krishna</first><last>Gorti</last><affiliation>Layer6 AI</affiliation></author>
      <author><first>Ilan</first><last>Gofman</last></author>
      <author><first>Zhaoyan</first><last>Liu</last><affiliation>Layer6 AI</affiliation></author>
      <author><first>Jiapeng</first><last>Wu</last><affiliation>Layer 6</affiliation></author>
      <author><first>Noël</first><last>Vouitsis</last><affiliation>Layer 6 AI</affiliation></author>
      <author><first>Guangwei</first><last>Yu</last><affiliation>Layer6 AI</affiliation></author>
      <author><first>Jesse C.</first><last>Cresswell</last><affiliation>Layer 6 AI</affiliation></author>
      <author><first>Rasa</first><last>Hosseinzadeh</last><affiliation>Layer6</affiliation></author>
      <pages>2145-2160</pages>
      <abstract>Text-to-SQL generation enables non-experts to interact with databases via natural language. Recent advances rely on large closed-source models like GPT-4 that present challenges in accessibility, privacy, and latency. To address these issues, we focus on developing small, efficient, and open-source text-to-SQL models. We demonstrate the benefits of sampling multiple candidate SQL generations and propose our method, MSc-SQL, to critique them using associated metadata. Our sample critiquing model evaluates multiple outputs simultaneously, achieving state-of-the-art performance compared to other open-source models while remaining competitive with larger models at a much lower cost. Full code can be found at github.com/layer6ai-labs/msc-sql.</abstract>
      <url hash="1e306971">2025.naacl-long.107</url>
      <bibkey>gorti-etal-2025-msc</bibkey>
    </paper>
    <paper id="108">
      <title>Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding</title>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Xiangdong</first><last>Su</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Guanglai</first><last>Gao</last><affiliation>Inner Mongolia University</affiliation></author>
      <pages>2161-2172</pages>
      <abstract>Recent studies have highlighted the effectiveness of tensor decomposition methods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we found that inherent heterogeneity among factor tensors in tensor decomposition significantly hinders the tensor fusion process and further limits the performance of link prediction. To overcome this limitation, we introduce a novel method that maps factor tensors onto a unified smooth Lie group manifold to make the distribution of factor tensors approximating homogeneous in tensor decomposition. We provide the theoretical proof of our motivation that homogeneous tensors are more effective than heterogeneous tensors in tensor fusion and approximating the target for tensor decomposition based TKGE methods. The proposed method can be directly integrated into existing tensor decomposition based TKGE methods without introducing extra parameters. Extensive experiments demonstrate the effectiveness of our method in mitigating the heterogeneity and in enhancing the tensor decomposition based TKGE models.</abstract>
      <url hash="6b7b9790">2025.naacl-long.108</url>
      <bibkey>li-etal-2025-mitigating-heterogeneity</bibkey>
    </paper>
    <paper id="109">
      <title>What Goes Into a <fixed-case>LM</fixed-case> Acceptability Judgment? Rethinking the Impact of Frequency and Length</title>
      <author><first>Lindia</first><last>Tjuatja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <author><first>Sophie</first><last>Hao</last></author>
      <pages>2173-2186</pages>
      <abstract>When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability—SLOR (Pauls and Klein, 2012; Lau et al., 2017)—across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs’ lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context.</abstract>
      <url hash="04d75478">2025.naacl-long.109</url>
      <bibkey>tjuatja-etal-2025-goes</bibkey>
    </paper>
    <paper id="110">
      <title><fixed-case>W</fixed-case>ave<fixed-case>FM</fixed-case>: A High-Fidelity and Efficient Vocoder Based on Flow Matching</title>
      <author><first>Tianze</first><last>Luo</last></author>
      <author><first>Xingchen</first><last>Miao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Wenbo</first><last>Duan</last></author>
      <pages>2187-2198</pages>
      <abstract>Flow matching offers a robust and stable approach to training diffusion models. However, directly applying flow matching to neural vocoders can result in subpar audio quality. In this work, we present WaveFM, a reparameterized flow matching model for mel-spectrogram conditioned speech synthesis, designed to enhance both sample quality and generation speed for diffusion vocoders. Since mel-spectrograms represent the energy distribution of waveforms, WaveFM adopts a mel-conditioned prior distribution instead of a standard Gaussian prior to minimize unnecessary transportation costs during synthesis. Moreover, while most diffusion vocoders rely on a single loss function, we argue that incorporating auxiliary losses, including a refined multi-resolution STFT loss, can further improve audio quality. To speed up inference without degrading sample quality significantly, we introduce a tailored consistency distillation method for WaveFM. Experiment results demonstrate that our model achieves superior performance in both quality and efficiency compared to previous diffusion vocoders, while enabling waveform generation in a single inference step.</abstract>
      <url hash="46104566">2025.naacl-long.110</url>
      <bibkey>luo-etal-2025-wavefm</bibkey>
    </paper>
    <paper id="111">
      <title>Analyzing and Evaluating Correlation Measures in <fixed-case>NLG</fixed-case> Meta-Evaluation</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Li</first><last>Lin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>2199-2222</pages>
      <abstract>The correlation between NLG automatic evaluation metrics and human evaluation is often regarded as a critical criterion for assessing the capability of an evaluation metric. However, different grouping methods and correlation coefficients result in various types of correlation measures used in meta-evaluation. In specific evaluation scenarios, prior work often directly follows conventional measure settings, but the characteristics and differences between these measures have not gotten sufficient attention. Therefore, this paper analyzes 12 common correlation measures using a large amount of real-world data from six widely-used NLG evaluation datasets and 32 evaluation metrics, revealing that different measures indeed impact the meta-evaluation results. Furthermore, we propose three perspectives that reflect the capability of meta-evaluation: discriminative power, ranking consistency, and sensitivity to score granularity. We find that the measure using global grouping and Pearson correlation coefficient exhibits the best performance in both discriminative power and ranking consistency. Besides, the measures using system-level grouping or Kendall correlation are the least sensitive to score granularity.</abstract>
      <url hash="f0e4b90c">2025.naacl-long.111</url>
      <bibkey>gao-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="112">
      <title>Cascading Large Language Models for Salient Event Graph Generation</title>
      <author><first>Xingwei</first><last>Tan</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Yuxiang</first><last>Zhou</last><affiliation>King’s College London</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>2223-2245</pages>
      <abstract>Generating event graphs from long documents is challenging due to the inherent complexity of multiple tasks involved such as detecting events, identifying their relationships, and reconciling unstructured input with structured graphs. Recent studies typically consider all events with equal importance, failing to distinguish salient events crucial for understanding narratives. This paper presents CALLMSAE, a CAscading Large Language Model framework for SAlient Event graph generation, which leverages the capabilities of LLMs and eliminates the need for costly human annotations. We first identify salient events by prompting LLMs to generate summaries, from which salient events are identified. Next, we develop an iterative code refinement prompting strategy to generate event relation graphs, removing hallucinated relations and recovering missing edges. Powered by CALLMSAE, we present <i>NYT-SEG</i>, a large-scale automatically annotated event graph dataset which can serve as distant supervision signals. Fine-tuning contextualised graph generation models on <i>NYT-SEG</i> outperforms the models trained on CAEVO data. Results on a human-annotated test set show that the proposed method generates salient and more accurate graphs, outperforming competitive baselines.</abstract>
      <url hash="cc24ba15">2025.naacl-long.112</url>
      <bibkey>tan-etal-2025-cascading</bibkey>
    </paper>
    <paper id="113">
      <title>Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models</title>
      <author><first>Artem</first><last>Vazhentsev</last><affiliation>Skolkovo Institute of Science and Technology and Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Lyudmila</first><last>Rvanova</last><affiliation>ITMO University</affiliation></author>
      <author><first>Ivan</first><last>Lazichny</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Maxim</first><last>Panov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>2246-2262</pages>
      <abstract>Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) – a well-established UQ technique in classification tasks – for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.</abstract>
      <url hash="c74392eb">2025.naacl-long.113</url>
      <bibkey>vazhentsev-etal-2025-token</bibkey>
    </paper>
    <paper id="114">
      <title>How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?</title>
      <author><first>Kenza</first><last>Benkirane</last></author>
      <author><first>Jackie</first><last>Kay</last><affiliation>University College London, University of London and DeepMind</affiliation></author>
      <author><first>Maria</first><last>Perez-Ortiz</last><affiliation>University College London, University of London</affiliation></author>
      <pages>2263-2288</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have positioned them as powerful tools for clinical decision-making, with rapidly expanding applications in healthcare. However, concerns about bias remain a significant challenge in the clinical implementation of LLMs, particularly regarding gender and ethnicity. This research investigates the evaluation and mitigation of bias in LLMs applied to complex clinical cases, focusing on gender and ethnicity biases. We introduce a novel Counterfactual Patient Variations (CPV) dataset derived from the JAMA Clinical ChallengeUsing this dataset, we built a framework for bias evaluation, employing both Multiple Choice Questions (MCQs) and corresponding explanations. We explore prompting with eight LLMs and fine-tuning as debiasing methods. Our findings reveal that addressing social biases in LLMs requires a multidimensional approach as mitigating gender bias can occur while introducing ethnicity biases, and that gender bias in LLM embeddings varies significantly across medical specialities. We demonstrate that evaluating both MCQ response and explanation processes is crucial, as correct responses can be based on biased reasoning. We provide a framework for evaluating LLM bias in real-world clinical cases, offer insights into the complex nature of bias in these models, and present strategies for bias mitigation.</abstract>
      <url hash="a6deb30c">2025.naacl-long.114</url>
      <bibkey>benkirane-etal-2025-diagnose</bibkey>
    </paper>
    <paper id="115">
      <title>From Redundancy to Relevance: Information Flow in <fixed-case>LVLM</fixed-case>s Across Reasoning Tasks</title>
      <author><first>Xiaofeng</first><last>Zhang</last></author>
      <author><first>Yihao</first><last>Quan</last></author>
      <author><first>Chen</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaosong</first><last>Yuan</last></author>
      <author><first>Shaotian</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Liang</first><last>Xie</last></author>
      <author><first>Wenxiao</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chaochen</first><last>Gu</last></author>
      <author><first>Hao</first><last>Tang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>2289-2299</pages>
      <abstract>Large Vision Language Models (LVLMs) achieve great performance on visual-language reasoning tasks, however, the black-box nature of LVLMs hinders in-depth research on the reasoning mechanism. As all images need to be converted into image tokens to fit the input format of large language models (LLMs) along with natural language prompts, sequential visual representation is essential to the performance of LVLMs, and the information flow analysis approach can be an effective tool for determining interactions between these representations. In this paper, we propose integrating attention analysis with LLaVA-CAM, concretely, attention scores highlight relevant regions during forward propagation, while LLaVA-CAM captures gradient changes through backward propagation, revealing key image features. By exploring the information flow from the perspective of visual representation contribution, we observe that it tends to converge in shallow layers but diversify in deeper layers. To validate our analysis, we conduct comprehensive experiments with truncation strategies across various LVLMs for visual question answering and image captioning tasks, and experimental results not only verify our hypothesis but also reveal a consistent pattern of information flow convergence in the corresponding layers, and the information flow cliff layer will be different due to different contexts.</abstract>
      <url hash="a3d10dee">2025.naacl-long.115</url>
      <bibkey>zhang-etal-2025-redundancy</bibkey>
    </paper>
    <paper id="116">
      <title>Patent-<fixed-case>CR</fixed-case>: A Dataset for Patent Claim Revision</title>
      <author><first>Lekang</first><last>Jiang</last></author>
      <author><first>Pascal A.</first><last>Scherz</last></author>
      <author><first>Stefan</first><last>Goetz</last><affiliation>University of Cambridge and Duke University</affiliation></author>
      <pages>2300-2314</pages>
      <abstract>This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.</abstract>
      <url hash="44162801">2025.naacl-long.116</url>
      <bibkey>jiang-etal-2025-patent</bibkey>
    </paper>
    <paper id="117">
      <title><fixed-case>M</fixed-case>erge<fixed-case>ME</fixed-case>: Model Merging Techniques for Homogeneous and Heterogeneous <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>s</title>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Giannis</first><last>Karamanolakis</last><affiliation>Amazon</affiliation></author>
      <author><first>Victor</first><last>Soto</last><affiliation>Amazon</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>Amazon and University of Massachusetts, Lowell</affiliation></author>
      <author><first>Mayank</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jianhua</first><last>Lu</last><affiliation>The Queen’s University Belfast</affiliation></author>
      <pages>2315-2328</pages>
      <abstract>The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.</abstract>
      <url hash="25c9d0b8">2025.naacl-long.117</url>
      <bibkey>zhou-etal-2025-mergeme</bibkey>
    </paper>
    <paper id="118">
      <title>Fine-Tuned <fixed-case>LLM</fixed-case>s are “Time Capsules” for Tracking Societal Bias Through Books</title>
      <author><first>Sangmitra</first><last>Madhusudan</last></author>
      <author><first>Robert</first><last>Morabito</last></author>
      <author><first>Skye</first><last>Reid</last></author>
      <author><first>Nikta Gohari</first><last>Sadr</last></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>2329-2358</pages>
      <abstract>Books, while often rich in cultural insights, can also mirror societal biases of their eras—biases that Large Language Models (LLMs) may learn and perpetuate during training. We introduce a novel method to trace and quantify these biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising 593 fictional books across seven decades (1950-2019), to track bias evolution. By fine-tuning LLMs on books from each decade and using targeted prompts, we examine shifts in biases related to gender, sexual orientation, race, and religion. Our findings indicate that LLMs trained on decade-specific books manifest biases reflective of their times, with both gradual trends and notable shifts. For example, model responses showed a progressive increase in the portrayal of women in leadership roles (from 8% to 22%) from the 1950s to 2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly aligning with third-wave feminism. Same-sex relationship references increased markedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+ visibility. Concerningly, negative portrayals of Islam rose sharply in the 2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we demonstrate that these biases stem mainly from the books’ content and not the models’ architecture or initial training. Our study offers a new perspective on societal bias trends by bridging AI, literary studies, and social science research.</abstract>
      <url hash="8939b93c">2025.naacl-long.118</url>
      <bibkey>madhusudan-etal-2025-fine</bibkey>
    </paper>
    <paper id="119">
      <title>Exploring the Cost-Effectiveness of Perspective Taking in Crowdsourcing Subjective Assessment: A Case Study of Toxicity Detection</title>
      <author><first>Xiaoni</first><last>Duan</last><affiliation>Purdue University</affiliation></author>
      <author><first>Zhuoyan</first><last>Li</last><affiliation>Purdue University</affiliation></author>
      <author><first>Chien-Ju</first><last>Ho</last><affiliation>Washington University in St. Louis</affiliation></author>
      <author><first>Ming</first><last>Yin</last><affiliation>Purdue University</affiliation></author>
      <pages>2359-2372</pages>
      <abstract>Crowdsourcing has been increasingly utilized to gather subjective assessment, such as evaluating the toxicity of texts. Since there doesnot exist a single “ground truth” answer for subjective annotations, obtaining annotations to accurately reflect the opinions of differentsubgroups becomes a key objective for these subjective assessment tasks. Traditionally, this objective is accomplished by directly soliciting a large number of annotations from each subgroup, which can be costly especially when annotators of certain subgroups are hard to access. In this paper, using toxicity evaluation as an example, we explore the feasibility of using perspective taking—that is, asking annotators to take the point of views of a certain subgroup and estimate opinions within that subgroup—as a way to achieve this objective cost-efficiently. Our results show that compared to the baseline approach of directly soliciting annotations from the target subgroup, perspective taking could lead to better estimates of the subgroup-level opinion when annotations from the target subgroup is costly while the budget is limited. Moreover, prompting annotators to take the perspectives of contrasting subgroups simultaneously can further improve the quality of the estimates. Finally, we find that aggregating multiple perspective-taking annotations while soliciting a small number of annotations directly from the target subgroup for calibration leads to the highest-quality estimates under limited budget.</abstract>
      <url hash="5552704f">2025.naacl-long.119</url>
      <bibkey>duan-etal-2025-exploring</bibkey>
    </paper>
    <paper id="120">
      <title><fixed-case>N</fixed-case>orm<fixed-case>A</fixed-case>d: A Framework for Measuring the Cultural Adaptability of Large Language Models</title>
      <author><first>Abhinav Sukumar</first><last>Rao</last></author>
      <author><first>Akhila</first><last>Yerukola</last></author>
      <author><first>Vishwa</first><last>Shah</last></author>
      <author><first>Katharina</first><last>Reinecke</last><affiliation>Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2373-2403</pages>
      <abstract>To be effectively and safely deployed to global user populations, large language models (LLMs) may need to adapt outputs to user values and cultures, not just know about them. We introduce NormAd, an evaluation framework to assess LLMs’ cultural adaptability, specifically measuring their ability to judge social acceptability across varying levels of cultural norm specificity, from abstract values to explicit social norms. As an instantiation of our framework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions representing social-etiquette related cultural norms from 75 countries. Through comprehensive experiments on NormAd-Eti, we find that LLMs struggle to accurately judge social acceptability across these varying degrees of cultural contexts and show stronger adaptability to English-centric cultures over those from the Global South. Even in the simplest setting where the relevant social norms are provided, the best LLMs’ performance (<tex-math>\textless</tex-math> 82%) lags behind humans (<tex-math>\textgreater</tex-math> 95%). In settings with abstract values and country information, model performance drops substantially (<tex-math>\textless</tex-math> 60%), while human accuracy remains high (<tex-math>\textgreater</tex-math>90%). Furthermore, we find that models are better at recognizing socially acceptable versus unacceptable situations. Our findings showcase the current pitfalls in socio-cultural reasoning of LLMs which hinder their adaptability for global audiences.</abstract>
      <url hash="10e16596">2025.naacl-long.120</url>
      <bibkey>rao-etal-2025-normad</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>L</fixed-case>i<fixed-case>PO</fixed-case>: Listwise Preference Optimization through Learning-to-Rank</title>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Junru</first><last>Wu</last></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Misha</first><last>Khalman</last><affiliation>Google</affiliation></author>
      <author><first>Rishabh</first><last>Joshi</last><affiliation>Google</affiliation></author>
      <author><first>Yao</first><last>Zhao</last><affiliation>Google</affiliation></author>
      <author><first>Mohammad</first><last>Saleh</last></author>
      <author><first>Simon</first><last>Baumgartner</last><affiliation>Google</affiliation></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Peter J</first><last>Liu</last><affiliation>Google Brain</affiliation></author>
      <author><first>Xuanhui</first><last>Wang</last><affiliation>Google</affiliation></author>
      <pages>2404-2420</pages>
      <abstract>Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach.In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a thorough study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the LiPO framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives. Following this connection, we provide an examination of ranking objectives that are not well studied for LM alignment, with DPO and SLiC as special cases when list size is two. In particular, we highlight a specific method, LiPO-<tex-math>\lambda</tex-math>, which leverages a state-of-the-art listwise ranking objective and weights each preference pair in a more advanced manner. We show that LiPO-<tex-math>\lambda</tex-math> can outperform DPO variants and SLiC by a clear margin on several preference alignment tasks with both curated and real rankwise preference data.</abstract>
      <url hash="a7eb55d0">2025.naacl-long.121</url>
      <bibkey>liu-etal-2025-lipo</bibkey>
    </paper>
    <paper id="122">
      <title>Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection</title>
      <author><first>Maximilian</first><last>Spliethöver</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Tim</first><last>Knebler</last></author>
      <author><first>Fabian</first><last>Fumagalli</last></author>
      <author><first>Maximilian</first><last>Muschalik</last><affiliation>Institute of Computer Science, Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Barbara</first><last>Hammer</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Eyke</first><last>Hüllermeier</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>2421-2449</pages>
      <abstract>Recent advances on instruction fine-tuning have led to the development of various prompting techniques for large language models, such as explicit reasoning steps. However, the success of techniques depends on various parameters, such as the task, language model, and context provided. Finding an effective prompt is, therefore, often a trial-and-error process. Most existing approaches to automatic prompting aim to optimize individual techniques instead of compositions of techniques and their dependence on the input. To fill this gap, we propose an adaptive prompting approach that predicts the optimal prompt composition ad-hoc for a given input. We apply our approach to social bias detection, a highly context-dependent task that requires semantic understanding. We evaluate it with three large language models on three datasets, comparing compositions to individual techniques and other baselines. The results underline the importance of finding an effective prompt composition. Our approach robustly ensures high detection performance, and is best in several settings. Moreover, first experiments on other tasks support its generalizability.</abstract>
      <url hash="b56e551b">2025.naacl-long.122</url>
      <bibkey>spliethover-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="123">
      <title>Enhancing Discriminative Representation in Similar Relation Clusters for Few-Shot Continual Relation Extraction</title>
      <author><first>Anh Duc</first><last>Le</last></author>
      <author><first>Nam Le</first><last>Hai</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thanh Xuan</first><last>Nguyen</last></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Nguyen Thi Ngoc</first><last>Diep</last></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>2450-2467</pages>
      <abstract>Few-shot Continual Relation Extraction (FCRE) has emerged as a significant challenge in information extraction, necessitating that relation extraction (RE) systems can sequentially identify new relations with limited labeled samples. While existing studies have demonstrated promising results in FCRE, they often overlook the issue of similar relations, which is a critical factor contributing to catastrophic forgetting. In this work, we propose Sirus–a novel method that utilizes relation descriptions and dynamic clustering on these descriptions to identify similar relations. Leveraging this information, we introduce innovative loss functions specifically designed to enhance the distinction between relations, with a focus on learning to differentiate similar ones. Experimental results show that our approach can effectively mitigate the problem of catastrophic forgetting and outperforms state-of-the-art methods by a large margin. Additionally, we explore the potential of Large Language Model Embeddings (LLMEs) with representation learning and embedding capabilities, demonstrating their promise for advancing FCRE systems.</abstract>
      <url hash="036a6bde">2025.naacl-long.123</url>
      <bibkey>le-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="124">
      <title><fixed-case>S</fixed-case>ym<fixed-case>B</fixed-case>a: Symbolic Backward Chaining for Structured Natural Language Reasoning</title>
      <author><first>Jinu</first><last>Lee</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Wonseok</first><last>Hwang</last><affiliation>University of Seoul and LBox Co., Ltd.</affiliation></author>
      <pages>2468-2484</pages>
      <abstract>To improve the performance and explainability of LLM-based natural language reasoning, structured reasoning can be applied to generate explicitly structured proofs. Among different methods for structured reasoning, we specifically focus on backward chaining, where the proof goal is recursively decomposed to subgoals by searching and applying rules. We argue that current LLM-based backward chaining systems (e.g. Least-to-most prompting and LAMBADA) are incomplete, as they omit crucial algorithmic components identified from the classic backward chaining algorithm in computational logic (SLD Resolution). To this end, we propose a novel backward chaining system, SymBa (Symbolic Backward Chaining), which integrates a symbolic solver and an LLM. In SymBa, the solver controls the proof process, and the LLM is only called when the solver requires new information to complete the proof. Empowered by completeness, SymBa achieves a significant improvement in seven deductive, relational, and arithmetic reasoning benchmarks compared to the baselines.</abstract>
      <url hash="d9472ca0">2025.naacl-long.124</url>
      <bibkey>lee-hwang-2025-symba</bibkey>
    </paper>
    <paper id="125">
      <title><fixed-case>MEDA</fixed-case>: Dynamic <fixed-case>KV</fixed-case> Cache Allocation for Efficient Multimodal Long-Context Inference</title>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Hui</first><last>Shen</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Zheda</first><last>Mai</last></author>
      <author><first>Mi</first><last>Zhang</last><affiliation>The Ohio State University</affiliation></author>
      <pages>2485-2497</pages>
      <abstract>Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial computational resources as their multimodal Key-Value (KV) cache grows with increasing input lengths, challenging memory and time efficiency. For multimodal scenarios, the cross-modal interactions inevitablely increase complexity, and prior methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, often adopting uniform or progressive reduction strategis for layer-wise cache allocation. This results in precision loss and suboptimal performance. We propose <b>MEDA</b>, a novel approach specifically designed for the complexities of multimodal settings, dynamically allocating KV cache sizes based on attention entropy to better adapt to multimodal interactions.Through a dynamic multimodal KV cache allocation strategy, MEDA compresses the KV cache, adaptively retains sufficient multimodal information at each layer. Meanwhile, to mitigate the degradation of contextual information due to cache compression, we also integrate KV pairs merging techniques to maintain coherence. MEDA achieves up to <b>72%</b> KV cache memory reduction and <b>2.82</b> faster decoding speeds in some cases, while maintaining or enhancing performance on various multimodal tasks in a long context, including multi-image and long video scenarios.</abstract>
      <url hash="6e12e6d0">2025.naacl-long.125</url>
      <bibkey>wan-etal-2025-meda</bibkey>
    </paper>
    <paper id="126">
      <title>Language Models Largely Exhibit Human-like Constituent Ordering Preferences</title>
      <author><first>Ada</first><last>Tur</last></author>
      <author><first>Gaurav</first><last>Kamath</last><affiliation>McGill University</affiliation></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>ServiceNow Inc, Mila, McGill University and Mila, McGill University</affiliation></author>
      <pages>2498-2521</pages>
      <abstract>Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent’s length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with constituent movement, and may provide insights into existing theories on when and how the shift occurs in human language. We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs. Despite performing unexpectedly around particle movement, LLMs generally align with human preferences around constituent ordering.</abstract>
      <url hash="8adcb17c">2025.naacl-long.126</url>
      <bibkey>tur-etal-2025-language</bibkey>
    </paper>
    <paper id="127">
      <title><fixed-case>S</fixed-case>afe<fixed-case>Q</fixed-case>uant: <fixed-case>LLM</fixed-case> Safety Analysis via Quantized Gradient Inspection</title>
      <author><first>Sindhu</first><last>Padakandla</last><affiliation>Fujitsu Research India Pvt Ltd</affiliation></author>
      <author><first>Sadbhavana</first><last>Babar</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Rathod Darshan</first><last>D</last></author>
      <author><first>Manohar</first><last>Kaul</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <pages>2522-2536</pages>
      <abstract>Contemporary jailbreak attacks on Large Language Models (LLMs) employ sophisticated techniques with obfuscated content to bypass safety guardrails. Existing defenses either use computationally intensive LLM verification or require adversarial fine-tuning, leaving models vulnerable to advanced attacks. We introduce SafeQuant, a novel defense framework that leverages quantized gradient patterns to identify harmful prompts efficiently. Our key insight is that when generating identical responses like “Sure”, LLMs exhibit distinctly different internal gradient patterns for safe versus harmful prompts, reflecting conflicts with safety training. By capturing these patterns through selective gradient masking and quantization, SafeQuant significantly outperforms existing defenses across multiple benchmarks while maintaining model utility. The method demonstrates particular effectiveness against sophisticated attacks like WordGame prompts and persuasive adversarial attacks, achieving an F1-score of 0.80 on WordGame dataset and outperforming state-of-the-art (SoTA) methods like GradSafe by an absolute margin of 57%.</abstract>
      <url hash="d2755a7f">2025.naacl-long.127</url>
      <bibkey>padakandla-etal-2025-safequant</bibkey>
    </paper>
    <paper id="128">
      <title>Exploring Large Language Models for Effective Rumor Detection on Social Media</title>
      <author><first>Yirong</first><last>Zeng</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Bibo</first><last>Cai</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2537-2552</pages>
      <abstract>In this paper, we explore using Large Language Models (LLMs) for rumor detection on social media. It involves assessing the veracity of claims on social media based on social context (e.g., comments, propagation patterns). LLMs, despite their impressive capabilities in text-based reasoning tasks, struggle to achieve promising rumor detection performance when facing long structured social contexts. Our preliminary analysis shows that large-scale contexts hinder LLMs’ reasoning abilities, while moderate contexts perform better for LLMs, highlighting the need for refined contexts. Accordingly, we propose a semantic-propagation collaboration-base framework that integrates small language models (e.g., graph attention network) with LLMs for effective rumor detection. It models contexts by enabling text semantic and propagation patterns to collaborate through graph attention mechanisms, and reconstruct the context by aggregating attention values during inference. Also, a cluster-based unsupervised method to refine context is proposed for generalization. Extensive experiments demonstrate the effectiveness of proposed methods in rumor detection. This work bridges the gap for LLMs in facing long, structured data and offers a novel solution for rumor detection on social media.</abstract>
      <url hash="a9ba21af">2025.naacl-long.128</url>
      <bibkey>zeng-etal-2025-exploring</bibkey>
    </paper>
    <paper id="129">
      <title>No Simple Answer to Data Complexity: An Examination of Instance-Level Complexity Metrics for Classification Tasks</title>
      <author><first>Ryan A.</first><last>Cook</last></author>
      <author><first>John P.</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Ahmed</first><last>Abbasi</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>2553-2573</pages>
      <abstract>Natural Language Processing research has become increasingly concerned with understanding data quality and complexity at the instance level. Instance-level complexity scores can be used for tasks such as filtering out noisy observations and subsampling informative examples. However, there exists a diverse taxonomy of complexity metrics that can be used for a classification task, making metric selection itself a difficult task. We empirically examine the relationship between these metrics and find that simply storing training loss provides similar complexity rankings as other more computationally intensive techniques. Metric similarity allows us to subsample data with higher aggregate complexity along several metrics using a single a priori available meta-feature. Further, this choice of complexity metric does not impact demographic fairness, even in downstream predictions. Researchers should consider metric availability and similarity, as using the wrong metric or sampling strategy may hurt performance.</abstract>
      <url hash="17d233ea">2025.naacl-long.129</url>
      <bibkey>cook-etal-2025-simple</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>NLI</fixed-case> under the Microscope: What Atomic Hypothesis Decomposition Reveals</title>
      <author><first>Neha</first><last>Srikanth</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>2574-2589</pages>
      <abstract>Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model’s consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify critical atomic sub-problems of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the inferential consistency of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts.</abstract>
      <url hash="7307eaa2">2025.naacl-long.130</url>
      <bibkey>srikanth-rudinger-2025-nli</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>HISTOIRESMORALES</fixed-case>: A <fixed-case>F</fixed-case>rench Dataset for Assessing Moral Alignment</title>
      <author><first>Thibaud</first><last>Leteno</last><affiliation>Université Jean Monnet</affiliation></author>
      <author><first>Irina</first><last>Proskurina</last></author>
      <author><first>Antoine</first><last>Gourru</last><affiliation>Université Jean Monnet</affiliation></author>
      <author><first>Julien</first><last>Velcin</last><affiliation>ERIC</affiliation></author>
      <author><first>Charlotte</first><last>Laclau</last><affiliation>Télecom Paris</affiliation></author>
      <author><first>Guillaume</first><last>Metzler</last><affiliation>Université Lumiére (Lyon II)</affiliation></author>
      <author><first>Christophe</first><last>Gravier</last><affiliation>University Jean Monnet</affiliation></author>
      <pages>2590-2612</pages>
      <abstract>Aligning language models with human values is crucial, especially as they become more integrated into everyday life. While models are often adapted to user preferences, it is equally important to ensure they align with moral norms and behaviours in real-world social situations. Despite significant progress in languages like English and Chinese, French has seen little attention in this area, leaving a gap in understanding how LLMs handle moral reasoning in this language. To address this gap, we introduce HistoiresMorales, a French dataset derived from MoralStories, created through translation and subsequently refined with the assistance of native speakers to guarantee grammatical accuracy and adaptation to the French cultural context. We also rely on annotations of the moral values within the dataset to ensure their alignment with French norms. HistoiresMorales covers a wide range of social situations, including differences in tipping practices, expressions of honesty in relationships, and responsibilities toward animals. To foster future research, we also conduct preliminary experiments on the alignment of multilingual models on French and English data and the robustness of the alignment. We find that while LLMs are generally aligned with human moral norms by default, they can be easily influenced with user-preference optimization for both moral and immoral data.</abstract>
      <url hash="04bbef2f">2025.naacl-long.131</url>
      <bibkey>leteno-etal-2025-histoiresmorales</bibkey>
    </paper>
    <paper id="132">
      <title>Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment</title>
      <author><first>Kwanghee</first><last>Choi</last></author>
      <author><first>Eunjung</first><last>Yeo</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Kalvin</first><last>Chang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>David R</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2613-2628</pages>
      <abstract>Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features.</abstract>
      <url hash="5aa105fd">2025.naacl-long.132</url>
      <bibkey>choi-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="133">
      <title><fixed-case>SAPIENT</fixed-case>: Mastering Multi-turn Conversational Recommendation with Strategic Planning and <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search</title>
      <author><first>Hanwen</first><last>Du</last></author>
      <author><first>Bo</first><last>Peng</last><affiliation>Google</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>2629-2648</pages>
      <abstract>Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines. Our code and data are accessible through https://github.com/ninglab/SAPIENT.</abstract>
      <url hash="a9d70d38">2025.naacl-long.133</url>
      <bibkey>du-etal-2025-sapient</bibkey>
    </paper>
    <paper id="134">
      <title>Reliability of Topic Modeling</title>
      <author><first>Kayla</first><last>Schroeder</last></author>
      <author><first>Zach</first><last>Wood-Doughty</last><affiliation>Northwestern University</affiliation></author>
      <pages>2649-2662</pages>
      <abstract>Topic models allow researchers to extract latent factors from text data and use those variables in downstream statistical analyses. However, these methodologies can vary significantly due to initialization differences, randomness in sampling procedures, or noisy data. Reliability of these methods is of particular concern as many researchers treat learned topic models as ground truth for subsequent analyses. In this work, we show that the standard practice for quantifying topic model reliability fails to capture essential aspects of the variation in two widely-used topic models. Drawing from a extensive literature on measurement theory, we provide empirical and theoretical analyses of three other metrics for evaluating the reliability of topic models. On synthetic and real-world data, we show that McDonald’s <tex-math>\omega</tex-math> provides the best encapsulation of reliability. This metric provides an essential tool for validation of topic model methodologies that should be a standard component of any topic model-based research.</abstract>
      <url hash="388d5d62">2025.naacl-long.134</url>
      <bibkey>schroeder-wood-doughty-2025-reliability</bibkey>
    </paper>
    <paper id="135">
      <title>Style Transfer with Multi-iteration Preference Optimization</title>
      <author><first>Shuai</first><last>Liu</last><affiliation>University of Southern California, Information Sciences Institute</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>2663-2681</pages>
      <abstract>Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as ‘tuning’. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of exploration and optimization, and choosing contrastive examples by following a ‘hope’ vs ‘fear’ sampling strategy. Cognizant of the difference between machine translation and style transfer, however, we further tailor our framework with a new pseudo-parallel data generation method and a dynamic weighted reward aggregation method to tackle the lack of parallel data and the need for a multi-objective reward. We evaluate our model on two commonly used text style transfer datasets. Through automatic and human evaluation results we show the effectiveness and the superiority of our model compared to state-of-the-art baselines.</abstract>
      <url hash="a03017d6">2025.naacl-long.135</url>
      <bibkey>liu-may-2025-style</bibkey>
    </paper>
    <paper id="136">
      <title><fixed-case>DTELS</fixed-case>: Towards Dynamic Granularity of Timeline Summarization</title>
      <author><first>Chenlong</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhou</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>2682-2703</pages>
      <url hash="81758bc2">2025.naacl-long.136</url>
      <bibkey>zhang-etal-2025-dtels</bibkey>
    </paper>
    <paper id="137">
      <title><fixed-case>ALERT</fixed-case>: An <fixed-case>LLM</fixed-case>-powered Benchmark for Automatic Evaluation of Recommendation Explanations</title>
      <author><first>Yichuan</first><last>Li</last></author>
      <author><first>Xinyang</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Chenwei</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Mao</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Tianyi</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Kyumin</first><last>Lee</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Zhengyang</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Trishul</first><last>Chilimbi</last><affiliation>Amazon</affiliation></author>
      <pages>2704-2719</pages>
      <abstract>Recommendation explanation systems have become increasingly vital with the widespread adoption of recommender systems. However, existing recommendation explanation evaluation benchmarks suffer from limited item diversity, impractical user profiling requirements, and unreliable and unscalable evaluation protocols. We present ALERT, a model-agnostic recommendation explanation evaluation benchmark. The benchmark comprises three main contributions: 1) a diverse dataset encompassing 15 Amazon e-commerce categories with 2,761 user-item interactions, incorporating implicit preferences through purchase histories;2) two novel LLM-powered automatic evaluators that enable scalable and human-preference aligned evaluation of explanations; and 3) a robust divide-and-aggregate approach that synthesizes multiple LLM judgments, achieving 70% concordance with expert human evaluation and substantially outperforming existing methods.ALERT facilitates comprehensive evaluation of recommendation explanations across diverse domains, advancing the development of more effective explanation systems.</abstract>
      <url hash="c9e70503">2025.naacl-long.137</url>
      <bibkey>li-etal-2025-alert</bibkey>
    </paper>
    <paper id="138">
      <title><fixed-case>DETQUS</fixed-case>: Decomposition-Enhanced Transformers for <fixed-case>QU</fixed-case>ery-focused Summarization</title>
      <author><first>Yasir</first><last>Khan</last></author>
      <author><first>Xinlei</first><last>Wu</last></author>
      <author><first>Sangpil</first><last>Youm</last><affiliation>University of Florida</affiliation></author>
      <author><first>Justin</first><last>Ho</last></author>
      <author><first>Aryaan Mehboob</first><last>Shaikh</last></author>
      <author><first>Jairo</first><last>Garciga</last></author>
      <author><first>Rohan</first><last>Sharma</last><affiliation>University of Florida</affiliation></author>
      <author><first>Bonnie J</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>2720-2731</pages>
      <abstract>Query-focused tabular summarization is an emerging task in table-to-text generation that synthesizes a summary response from tabular data based on user queries. Traditional transformer-based approaches face challenges due to token limitations and the complexity of reasoning over large tables. To address these challenges, we introduce DETQUS (Decomposition-Enhanced Transformers for QUery-focused Summarization), a system designed to improve summarization accuracy by leveraging tabular decomposition alongside a fine-tuned encoder-decoder model. DETQUS employs a large language model to selectively reduce table size, retaining only query-relevant columns while preserving essential information. This strategy enables more efficient processing of large tables and enhances summary quality. Our approach, equipped with table-based QA model Omnitab, achieves a ROUGE-L score of 0.4437, outperforming the previous state-ofthe- art REFACTOR model (ROUGE-L: 0.422). These results highlight DETQUS as a scalable and effective solution for query-focused tabular summarization, offering a structured alternative to more complex architectures.</abstract>
      <url hash="a64ef86f">2025.naacl-long.138</url>
      <bibkey>khan-etal-2025-detqus</bibkey>
    </paper>
    <paper id="139">
      <title><fixed-case>I</fixed-case>roko<fixed-case>B</fixed-case>ench: A New Benchmark for <fixed-case>A</fixed-case>frican Languages in the Age of Large Language Models</title>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>Jessica</first><last>Ojo</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Jian Yun</first><last>Zhuang</last></author>
      <author><first>Jesujoba Oluwadara</first><last>Alabi</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Millicent</first><last>Ochieng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <author><first>Chiamaka Ijeoma</first><last>Chukwuneke</last><affiliation>Nnamdi Azikiwe University</affiliation></author>
      <author><first>Happy</first><last>Buzaaba</last><affiliation>Princeton University</affiliation></author>
      <author><first>Blessing Kudzaishe</first><last>Sibanda</last></author>
      <author><first>Godson Koffi</first><last>Kalipe</last></author>
      <author><first>Jonathan</first><last>Mukiibi</last></author>
      <author><first>Salomon</first><last>Kabongo Kabenamualu</last><affiliation>TIB/L3S</affiliation></author>
      <author><first>Foutse</first><last>Yuehgoh</last></author>
      <author><first>Mmasibidi</first><last>Setaka</last></author>
      <author><first>Lolwethu</first><last>Ndolela</last></author>
      <author><first>Nkiruka</first><last>Odu</last><affiliation>African University of Science and Technology</affiliation></author>
      <author><first>Rooweither</first><last>Mabuya</last><affiliation>North-West University</affiliation></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last><affiliation>Imperial College London and Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Sokhar</first><last>Samb</last></author>
      <author><first>Tadesse Kebede</first><last>Guge</last><affiliation>Haramaya University</affiliation></author>
      <author><first>Tombekai Vangoni</first><last>Sherman</last></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <pages>2732-2757</pages>
      <abstract>Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (e.g. African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench—a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference(AfriXNLI), mathematical reasoning(AfriMGSM), and multi-choice knowledge-based QA(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings(where test sets are translated into English) across 10 open and four proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages (such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Gemma 2 27B only at 63% of the best-performing proprietary model GPT-4o performance. Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, like Gemma 2 27B and LLaMa 3.1 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages.</abstract>
      <url hash="e56bc265">2025.naacl-long.139</url>
      <bibkey>adelani-etal-2025-irokobench</bibkey>
    </paper>
    <paper id="140">
      <title>The Impact of Domain-Specific Terminology on Machine Translation for Finance in <fixed-case>E</fixed-case>uropean Languages</title>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Xiaomo</first><last>Liu</last><affiliation>JP Morgan AI Research</affiliation></author>
      <pages>2758-2775</pages>
      <abstract>Domain-specific machine translation (MT) poses significant challenges due to specialized terminology, particularly when translating across multiple languages with scarce resources. In this study, we present the first impact analysis of domain-specific terminology on multilingual MT for finance, focusing on European languages within the subdomain of macroeconomics. To this end, we construct a multi-parallel corpus from the European Central Bank, aligned across 22 languages. Using this resource, we compare open-source multilingual MT systems with large language models (LLMs) that possess multilingual capabilities. Furthermore, by developing and curating an English financial glossary, we propose a methodology to analyze the relationship between translation performance (into English) and the accuracy of financial term matching, obtaining significant correlation results. Finally, using the multi-parallel corpus and the English glossary, we automatically align a multilingual financial terminology, validating the English-Spanish alignments and incorporating them into our discussion. Our findings provide valuable insights into the current state of financial MT for European languages and offer resources for future research and system improvements.</abstract>
      <url hash="10f7437e">2025.naacl-long.140</url>
      <bibkey>oncevay-etal-2025-impact</bibkey>
    </paper>
    <paper id="141">
      <title>Benchmarking Language Model Creativity: A Case Study on Code Generation</title>
      <author><first>Yining</first><last>Lu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Dixuan</first><last>Wang</last></author>
      <author><first>Tianjian</first><last>Li</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dongwei</first><last>Jiang</last></author>
      <author><first>Sanjeev</first><last>Khudanpur</last><affiliation>Whiting School of Engineering</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>2776-2794</pages>
      <abstract>As LLMs become increasingly prevalent, it is interesting to consider how “creative” these models can be. From cognitive science, creativity consists of at least two key characteristics: <i>convergent</i> thinking (purposefulness to achieve a given goal) and <i>divergent</i> thinking (adaptability to explore new environments or constraints) (CITATION). In this work, we introduce a framework for quantifying LLM creativity that incorporates the two design ingredients: (1) We introduce DENIAL PROMPTING which pushes LLMs to develop more creative solutions to a given problem by incrementally imposing new constraints on the previous solution, compelling LLMs to adopt new strategies. (2) We define NEOGAUGE, a metric that quantifies both convergent and divergent thinking in the generated creative responses by LLMs. We test the proposed framework on Codeforces problems, which serve as both a natural dataset for coding tasks and a collection of prior human solutions. We quantify NEOGAUGE for various proprietary and open-source models and find that even the most creative model, GPT-4, still falls short of demonstrating human-like creativity. We also experiment with advanced reasoning strategies (MCTS, self-correction, etc.) and observe no significant improvement in creativity. As a by-product of our analysis, we release NEOCODER dataset for reproducing our results on future models.</abstract>
      <url hash="058345f0">2025.naacl-long.141</url>
      <bibkey>lu-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="142">
      <title>Have <fixed-case>LLM</fixed-case>s Reopened the Pandora’s Box of <fixed-case>AI</fixed-case>-Generated Fake News?</title>
      <author><first>Xinyu</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Wenbo</first><last>Zhang</last></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Hangzhi</first><last>Guo</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bonam</first><last>Mingole</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>S. Shyam</first><last>Sundar</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Sarah</first><last>Rajtmajer</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Amulya</first><last>Yadav</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>2795-2811</pages>
      <abstract>With the rise of AI-generated content spewed at scale from large language models (LLMs), genuine concerns about the spread of fake news have intensified. The perceived ability of LLMs to produce convincing fake news at scale poses new challenges for both human and automated fake news detection systems. To address this gap, this paper presents the findings from a university-level competition that aimed to explore how LLMs can be used by humans to create fake news, and to assess the ability of human annotators and AI models to detect it. A total of 110 participants used LLMs to create 252 unique fake news stories, and 84 annotators participated in the detection tasks. Our findings indicate that LLMs are ~68% more effective at detecting real news than humans. However, for fake news detection, the performance of LLMs and humans remains comparable (~60% accuracy). Additionally, we examine the impact of visual elements (e.g., pictures) in news on the accuracy of detecting fake news stories. Finally, we also examine various strategies used by fake news creators to enhance the credibility of their AI-generated content. This work highlights the increasing complexity of detecting AI-generated fake news, particularly in collaborative human-AI settings.</abstract>
      <url hash="fb495b4c">2025.naacl-long.142</url>
      <bibkey>wang-etal-2025-llms-reopened</bibkey>
    </paper>
    <paper id="143">
      <title>Probe-Free Low-Rank Activation Intervention</title>
      <author><first>Chonghe</first><last>Jiang</last></author>
      <author><first>Bao</first><last>Nguyen</last></author>
      <author><first>Anthony Man-Cho</first><last>So</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Viet Anh</first><last>Nguyen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2812-2824</pages>
      <abstract>Language models (LMs) can produce texts that appear accurate and coherent but contain untruthful or toxic content. Inference-time interventions that edit the hidden activations have shown promising results in steering the LMs towards desirable generations. Existing activation intervention methods often comprise an activation probe to detect undesirable generation, triggering the activation modification to steer subsequent generation. This paper proposes a probe-free intervention method <b>FLORAIN</b> for all attention heads in a specific activation layer. It eliminates the need to train classifiers for probing purposes. The intervention function is parametrized by a sample-wise nonlinear low-rank mapping, which is trained by minimizing the distance between the modified activations and their projection onto the manifold of desirable content. Under specific constructions of the manifold and projection distance, we show that the intervention strategy can be computed efficiently by solving a smooth optimization problem. The empirical results, benchmarked on multiple base models, demonstrate that FLORAIN consistently outperforms several baseline methods in enhancing model truthfulness and quality across generation and multiple-choice tasks. Our implementation can be found at https://github.com/nguyenngocbaocmt02/EFI.</abstract>
      <url hash="66b10e69">2025.naacl-long.143</url>
      <bibkey>jiang-etal-2025-probe</bibkey>
    </paper>
    <paper id="144">
      <title><fixed-case>F</fixed-case>act<fixed-case>T</fixed-case>rack: Time-Aware World State Tracking in Story Outlines</title>
      <author><first>Zhiheng</first><last>Lyu</last></author>
      <author><first>Kevin</first><last>Yang</last><affiliation>Scaled Cognition</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>2825-2848</pages>
      <abstract>While accurately detecting and correcting factual contradictions in language model outputs has become increasingly important as their capabilities improve, doing so is highly challenging. We propose a novel method, FactTrack, for tracking atomic facts and addressing factual contradictions. Crucially, FactTrack also maintains time-aware validity intervals for each fact, allowing for change over time. At a high level, FactTrack consists of a four-step pipeline to update a world state data structure for each new event: (1) decompose the event into directional atomic facts; (2) determine the validity interval of each atomic fact using the world state; (3) detect contradictions with existing facts in the world state; and finally (4) add new facts to the world state and update existing atomic facts. When we apply FactTrack to contradiction detection on structured story outlines, we find that FactTrack using LLaMA2-7B-Chat substantially outperforms a fair baseline using LLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline. Moreover, when using GPT4, FactTrack significantly outperforms the GPT4 baseline.</abstract>
      <url hash="2385f082">2025.naacl-long.144</url>
      <bibkey>lyu-etal-2025-facttrack</bibkey>
    </paper>
    <paper id="145">
      <title>A <fixed-case>B</fixed-case>ayesian Optimization Approach to Machine Translation Reranking</title>
      <author><first>Julius</first><last>Cheng</last></author>
      <author><first>Maike</first><last>Züfle</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Vilém</first><last>Zouhar</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>2849-2862</pages>
      <abstract>Reranking, or scoring a list of prediction candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate, remains a simple and effective method for improving prediction quality. However, reranking with high quality scoring models can add substantial computational cost to the translation pipeline, which we address in this work by framing list reranking as a Bayesian optimization (BayesOpt) problem over the candidate list, where unknown scores are modeled with a Gaussian process. This algorithm scores candidates iteratively, choosing next candidates by balancing between exploration, choosing to score those that differ from candidates already scored, and exploitation, choosing to score those that resemble high-scoring candidates.This procedure finds high-scoring candidates while scoring only a fraction of the candidates list; given candidate lists of 200 random samples (before deduplication), our method achieves the same CometKiwi score using only 70 scoring evaluations on average compared to scoring a random subset of 180 candidates. We also propose multi-fidelity BayesOpt for list reranking, where scores obtained from a noisier but cheaper proxy scoring model are incorporated into the search process. We show that well-trained distilled proxy scorers can further improve the performance of BayesOpt.</abstract>
      <url hash="95f995ce">2025.naacl-long.145</url>
      <bibkey>cheng-etal-2025-bayesian</bibkey>
    </paper>
    <paper id="146">
      <title>Multi-Conditional Ranking with Large Language Models</title>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>2863-2883</pages>
      <abstract>Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iteratively Ranking the items (EXSIR). Our extensive experiments show that this decomposed reasoning method enhances LLMs’ performance significantly, achieving up to a 14.4% improvement over existing LLMs. We also provide a detailed analysis of LLMs performance across various condition categories, and examine the effectiveness of decomposition step. Furthermore, we compare our method with existing approaches such as Chain-of-Thought and existing ranking models, demonstrating the superiority of our approach and complexity of MCR task. We will make our dataset and code publicly available.</abstract>
      <url hash="905e3790">2025.naacl-long.146</url>
      <bibkey>pezeshkpour-hruschka-2025-multi</bibkey>
    </paper>
    <paper id="147">
      <title><fixed-case>R</fixed-case>e<fixed-case>GLA</fixed-case>: Refining Gated Linear Attention</title>
      <author><first>Peng</first><last>Lu</last></author>
      <author><first>Ivan</first><last>Kobyzev</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Philippe</first><last>Langlais</last><affiliation>Université de Montréal</affiliation></author>
      <pages>2884-2898</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module: feature maps, normalization, and the gating mechanism. We developed a feature mapping function to address some crucial issues that previous suggestions overlooked. Then we offered further rationale for the integration of normalization layers to stabilize the training process. Moreover, we explored the saturation phenomenon of the gating mechanism and augmented it with a refining module. We conducted extensive experiments and showed our architecture outperforms previous Gated Linear Attention mechanisms in extensive tasks including training from scratch and post-linearization with continual pre-training.</abstract>
      <url hash="b240385c">2025.naacl-long.147</url>
      <bibkey>lu-etal-2025-regla</bibkey>
    </paper>
    <paper id="148">
      <title>Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders</title>
      <author><first>Kshitish</first><last>Ghate</last></author>
      <author><first>Isaac</first><last>Slaughter</last><affiliation>University of Washington</affiliation></author>
      <author><first>Kyra</first><last>Wilson</last><affiliation>University of Washington</affiliation></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Aylin</first><last>Caliskan</last><affiliation>University of Washington</affiliation></author>
      <pages>2899-2915</pages>
      <abstract>While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance (<tex-math>0.3 \leq r \leq 0.8</tex-math>), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline.</abstract>
      <url hash="5f6dba2f">2025.naacl-long.148</url>
      <bibkey>ghate-etal-2025-intrinsic</bibkey>
    </paper>
    <paper id="149">
      <title>Benchmarking Failures in Tool-Augmented Language Models</title>
      <author><first>Eduardo</first><last>Treviño</last></author>
      <author><first>Hugo</first><last>Contant</last></author>
      <author><first>James</first><last>Ngai</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zora Zhiruo</first><last>Wang</last></author>
      <pages>2916-2934</pages>
      <abstract>The integration of tools has extended the capabilities of language models (LMs) beyond vanilla text generation to versatile scenarios. However, tool-augmented language models (TaLMs) often assume ‘perfect’ information access and tool availability, which may not hold in the real world. To systematically study TaLMs imperfections, we introduce the FAIL-TaLMs benchmark, featuring two major failures: under-specified user queries and non-available tools. FAIL-TaLMS contains 1,749 examples using 906 tools across 21 categories, including single- and multi-tool usage. We evaluate top-performing proprietary and open-source models, and find all current models except for Claude struggle to recognize missing tools or information. Further, to study possible mitigation of the failures, we enable real-time human interaction, named the Ask-and-Help method, to provide missing information or replace non-functional tools. While Ask-and-Help can help models solve tasks more correctly when queries are under-specified, it brings minimal benefit when complex tools are broken.</abstract>
      <url hash="386d5ab9">2025.naacl-long.149</url>
      <bibkey>trevino-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="150">
      <title>Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework</title>
      <author><first>Reza</first><last>Averly</last><affiliation>, Ohio State University, Columbus</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>2935-2951</pages>
      <abstract>Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. Our initial experiment reveals significant contrast in performance for some clinical entities and how a simple exploitment on entity types can alleviate this issue. In this paper, we introduce a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of entity sub-types and then filter them. Our experimental results demonstrate the efficacies of our framework and the improvements across all metrics, models, datasets, and entity types. Our analysis also reveals substantial improvement in recognizing previously missed entities using entity decomposition. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.</abstract>
      <url hash="d886b5fb">2025.naacl-long.150</url>
      <bibkey>averly-ning-2025-entity</bibkey>
    </paper>
    <paper id="151">
      <title>Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective</title>
      <author><first>Shenglai</first><last>Zeng</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiankun</first><last>Zhang</last></author>
      <author><first>Bingheng</first><last>Li</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yuping</first><last>Lin</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Tianqi</first><last>Zheng</last><affiliation>Amazon</affiliation></author>
      <author><first>Dante</first><last>Everaert</last><affiliation>Amazon</affiliation></author>
      <author><first>Hanqing</first><last>Lu</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Monica Xiao</first><last>Cheng</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>2952-2969</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing the performance of Large Language Models (LLMs). However, these systems face challenges in effectively integrating external knowledge with the LLM’s internal knowledge, often leading to issues with misleading or unhelpful information. This work aims to provide a systematic study on knowledge checking in RAG systems. We conduct a comprehensive analysis of LLM representation behaviors and demonstrate the significance of using representations in knowledge checking. Motivated by the findings, we further develop representation-based classifiers for knowledge filtering. We show substantial improvements in RAG performance, even when dealing with noisy knowledge databases. Our study provides new insights into leveraging LLM representations for enhancing the reliability and effectiveness of RAG systems.</abstract>
      <url hash="eb35cff5">2025.naacl-long.151</url>
      <bibkey>zeng-etal-2025-towards</bibkey>
    </paper>
    <paper id="152">
      <title>The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning</title>
      <author><first>Longju</first><last>Bai</last></author>
      <author><first>Angana</first><last>Borah</last></author>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>2970-2993</pages>
      <abstract>Large Multimodal Models (LMMs) exhibit impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of most data and models. Conversely, multi-agent models have shown significant capability in solving complex tasks. Our study evaluates the collective performance of LMMs in a multi-agent interaction setting for the novel task of cultural image captioning. Our contributions are as follows: (1) We introduce MosAIC, a Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs with distinct cultural personas; (2) We provide a dataset of culturally enriched image captions in English for images from China, India, and Romania across three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable metric for evaluating cultural information within image captions; and (4) We show that the multi-agent interaction outperforms single-agent models across different metrics, and offer valuable insights for future research.</abstract>
      <url hash="f7fb79f6">2025.naacl-long.152</url>
      <bibkey>bai-etal-2025-power</bibkey>
    </paper>
    <paper id="153">
      <title>Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison</title>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Sara</first><last>Papi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>2994-3006</pages>
      <abstract>Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech—the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.</abstract>
      <url hash="cae85f43">2025.naacl-long.153</url>
      <bibkey>lam-etal-2025-prepending</bibkey>
    </paper>
    <paper id="154">
      <title><fixed-case>CORRECT</fixed-case>: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking</title>
      <author><first>Delvin Ce</first><last>Zhang</last><affiliation>The Pennsylvania State University</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>3007-3019</pages>
      <abstract>Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.</abstract>
      <url hash="2479719f">2025.naacl-long.154</url>
      <bibkey>zhang-lee-2025-correct</bibkey>
    </paper>
    <paper id="155">
      <title>Racing Thoughts: Explaining Contextualization Errors in Large Language Models</title>
      <author><first>Michael A.</first><last>Lepori</last><affiliation>Brown University</affiliation></author>
      <author><first>Michael Curtis</first><last>Mozer</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Asma</first><last>Ghandeharioun</last><affiliation>Google</affiliation></author>
      <pages>3020-3036</pages>
      <abstract>The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt “John is going fishing, so he walks over to the bank. Can he make an ATM transaction?”, a model may incorrectly respond “Yes” if it has not properly contextualized “bank” as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., “bank” must be properly contextualized before the final token, "?", integrates information from “bank”), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic interpretability, we provide correlational and causal evidence in support of the hypothesis and suggest inference-time interventions to address it.</abstract>
      <url hash="1ed1ca1a">2025.naacl-long.155</url>
      <bibkey>lepori-etal-2025-racing</bibkey>
    </paper>
    <paper id="156">
      <title><fixed-case>DREAM</fixed-case>: Improving Video-Text Retrieval Through Relevance-Based Augmentation Using Large Foundation Models</title>
      <author><first>Yimu</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Shuai</first><last>Yuan</last><affiliation>TikTok</affiliation></author>
      <author><first>Bo</first><last>Xue</last></author>
      <author><first>Xiangru</first><last>Jian</last><affiliation>ServiceNow Research and University of Waterloo</affiliation></author>
      <author><first>Wei</first><last>Pang</last><affiliation>Vector Institute</affiliation></author>
      <author><first>Mushi</first><last>Wang</last></author>
      <author><first>Ning</first><last>Yu</last><affiliation>Netflix Eyeline Studios</affiliation></author>
      <pages>3037-3056</pages>
      <abstract>Recent progress in video-text retrieval has been driven largely by advancements in model architectures and training strategies. However, the representation learning capabilities of video-text retrieval models remain constrained by low-quality and limited training data annotations. To address this issue, we present a novel Vi<b>d</b>eo-Text <b>R</b>etrieval Paradigm with R<b>e</b>levance-based <b>A</b>ug<b>m</b>entation, namely dReAm, which enhances video and text data using large foundation models to learn more generalized features. Specifically, we first adopt a simple augmentation method, which generates self-similar data by randomly duplicating or dropping subwords and frames. In addition, inspired by the recent advancement in visual and language generative models, we propose a more robust augmentation method through textual paraphrasing and video stylization using large language models (LLMs) and visual generative models (VGMs). To further enrich video and text information, we propose a relevance-based augmentation method, where LLMs and VGMs generate and integrate new relevant information into the original data. Leveraging this enriched data, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of dReAm over existing methods. Code will be available upon acceptance.</abstract>
      <url hash="a524adad">2025.naacl-long.156</url>
      <bibkey>wang-etal-2025-dream</bibkey>
    </paper>
    <paper id="157">
      <title><fixed-case>T</fixed-case>o<fixed-case>W</fixed-case>: Thoughts of Words Improve Reasoning in Large Language Models</title>
      <author><first>Zhikun</first><last>Xu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ming</first><last>Shen</last></author>
      <author><first>Jacob</first><last>Dineen</last></author>
      <author><first>Zhaonan</first><last>Li</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Xiao</first><last>Ye</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Shijie</first><last>Lu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Aswin</first><last>Rrv</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>Arizona State University</affiliation></author>
      <pages>3057-3075</pages>
      <abstract>We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction. ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts. Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the implicit reasoning processes in raw texts. While there are many ways to acquire such thoughts of words, we explore the first step of acquiring ToW annotations through distilling from larger models. After continual pre-training with only 70K ToW annotations, we effectively improve models’ reasoning performances by 7% to 9% on average and reduce model hallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks and applications, introducing no additional biases on labels or semantics.</abstract>
      <url hash="51bb05f5">2025.naacl-long.157</url>
      <bibkey>xu-etal-2025-tow</bibkey>
    </paper>
    <paper id="158">
      <title>A Probabilistic Framework for <fixed-case>LLM</fixed-case> Hallucination Detection via Belief Tree Propagation</title>
      <author><first>Bairu</first><last>Hou</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Shiyu</first><last>Chang</last></author>
      <pages>3076-3099</pages>
      <abstract>We describe Belief Tree Propagation (BTProp), a probabilistic framework for LLM hallucination detection. To judge the truth of a statement, BTProp generates a belief tree by recursively expanding the initial statement into a set of logically related claims, then reasoning globally about the relationships between these claims. BTProp works by constructing a probabilistic model of the LM itself: it reasons jointly about logical relationships between claims and relationships between claim probabilities and LM factuality judgments via probabilistic inference in a “hidden Markov tree”. This method improves over state-of-the-art baselines by 3%-9% (evaluated by AUROC and AUC-PR) on multiple hallucination detection benchmarks.</abstract>
      <url hash="502cc1f1">2025.naacl-long.158</url>
      <bibkey>hou-etal-2025-probabilistic</bibkey>
    </paper>
    <paper id="159">
      <title><fixed-case>ERAS</fixed-case>: Evaluating the Robustness of <fixed-case>C</fixed-case>hinese <fixed-case>NLP</fixed-case> Models to Morphological Garden Path Errors</title>
      <author><first>Qinchan</first><last>Li</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Sophie</first><last>Hao</last></author>
      <pages>3100-3111</pages>
      <abstract>In languages without orthographic word boundaries, NLP models perform _word segmentation_, either as an explicit preprocessing step or as an implicit step in an end-to-end computation. This paper shows that Chinese NLP models are vulnerable to _morphological garden path errors_—errors caused by a failure to resolve local word segmentation ambiguities using sentence-level morphosyntactic context. We propose a benchmark, _ERAS_, that tests a model’s vulnerability to morphological garden path errors by comparing its behavior on sentences with and without local segmentation ambiguities. Using ERAS, we show that word segmentation models make morphological garden path errors on locally ambiguous sentences, but do not make equivalent errors on unambiguous sentences. We further show that sentiment analysis models with character-level tokenization make implicit garden path errors, even without an explicit word segmentation step in the pipeline. Our results indicate that models’ segmentation of Chinese text often fails to account for morphosyntactic context.</abstract>
      <url hash="12902cb8">2025.naacl-long.159</url>
      <bibkey>li-hao-2025-eras</bibkey>
    </paper>
    <paper id="160">
      <title>Superlatives in Context: Modeling the Implicit Semantics of Superlatives</title>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Bonnie</first><last>Webber</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>3112-3126</pages>
      <abstract>Superlatives are used to single out elements with a maximal/minimal property. Semantically, superlatives perform a set comparison: something (or some things) has the min/max property out of a set. As such, superlatives provide an ideal phenomenon for studying implicit phenomena and discourse restrictions. While this comparison set is often not explicitly defined, its (implicit) restrictions can be inferred from the discourse context the expression appears in. In this work we provide an extensive computational study on the semantics of superlatives. We propose a unified account of superlative semantics which allows us to derive a broad-coverage annotation schema. Using this unified schema we annotated a multi-domain dataset of superlatives and their semantic interpretations. We specifically focus on interpreting implicit or ambiguous superlative expressions, by analyzing how the discourse context restricts the set of interpretations. In a set of experiments we then analyze how well models perform at variations of predicting superlative semantics, with and without context. We show that the fine-grained semantics of superlatives in context can be challenging for contemporary models, including GPT-4.</abstract>
      <url hash="ebe245cf">2025.naacl-long.160</url>
      <bibkey>pyatkin-etal-2025-superlatives</bibkey>
    </paper>
    <paper id="161">
      <title><fixed-case>LLM</fixed-case>s Are Not Intelligent Thinkers: Introducing Mathematical Topic Tree Benchmark for Comprehensive Evaluation of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Arash Gholami</first><last>Davoodi</last><affiliation>Apple</affiliation></author>
      <author><first>Seyed Pouyan Mousavi</first><last>Davoudi</last></author>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <pages>3127-3140</pages>
      <abstract>Large language models (LLMs) demonstrate impressive capabilities in mathematical reasoning. However, despite these achievements, current evaluations are mostly limited to specific mathematical topics, and it remains unclear whether LLMs are genuinely engaging in reasoning. To address these gaps, we present the Mathematical Topics Tree (MaTT) benchmark, a challenging and structured benchmark that offers 1,958 questions across a wide array of mathematical subjects, each paired with a detailed hierarchical chain of topics. Upon assessing different LLMs using the MaTT benchmark, we find that GPT-4 achieved a mere 54% accuracy in a multiple-choice scenario. Interestingly, even when employing Chain-of-Thought prompting, we observe mostly no notable improvement. Moreover, LLMs accuracy dramatically reduced by up to 24.2 percentage point when the questions were presented without providing choices. Further detailed analysis of the LLMs’ performance across a range of topics showed significant discrepancy even for closely related subtopics within the same general mathematical area. In an effort to pinpoint the reasons behind LLMs performances, we conducted a manual evaluation of the completeness and correctness of the explanations generated by GPT-4 when choices were available. Surprisingly, we find that in only 53.3% of the instances where the model provided a correct answer, the accompanying explanations were deemed complete and accurate, i.e., the model engaged in genuine reasoning.</abstract>
      <url hash="bee08b28">2025.naacl-long.161</url>
      <bibkey>davoodi-etal-2025-llms</bibkey>
    </paper>
    <paper id="162">
      <title>Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Haijiang</first><last>Liu</last></author>
      <author><first>Arnav</first><last>Arora</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>3141-3154</pages>
      <abstract>Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.</abstract>
      <url hash="1d433987">2025.naacl-long.162</url>
      <bibkey>cao-etal-2025-specializing</bibkey>
    </paper>
    <paper id="163">
      <title>Representing Rule-based Chatbots with Transformers</title>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Abhishek</first><last>Panigrahi</last></author>
      <author><first>Danqi</first><last>Chen</last><affiliation>Princeton University</affiliation></author>
      <pages>3155-3180</pages>
      <abstract>What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer—for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought.Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.</abstract>
      <url hash="9fe9dc5b">2025.naacl-long.163</url>
      <bibkey>friedman-etal-2025-representing</bibkey>
    </paper>
    <paper id="164">
      <title>Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models</title>
      <author><first>Michael</first><last>Hanna</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology</affiliation></author>
      <pages>3181-3203</pages>
      <abstract>Autoregressive transformer language models (LMs) possess strong syntactic abilities, often successfully handling phenomena from agreement to NPI licensing. However, the features they use to incrementally process their linguistic input are not well understood. In this paper, we fill this gap by studying the mechanisms underlying garden path sentence processing in LMs. Specifically, we ask: (1) Do LMs use syntactic features or shallow heuristics to perform incremental sentence processing? (2) Do LMs represent only one potential interpretation, or multiple? and (3) Do LMs reanalyze or repair their initial incorrect representations? To address these questions, we use sparse autoencoders to identify interpretable features that determine which continuation—and thus which reading—of a garden path sentence the LM prefers. We find that while many important features relate to syntactic structure, some reflect syntactically irrelevant heuristics. Moreover, though most active features correspond to one reading of the sentence, some features correspond to the other, suggesting that LMs assign weight to both possibilities. Finally, LMs fail to re-use features to answer follow-up questions.</abstract>
      <url hash="c70f1265">2025.naacl-long.164</url>
      <bibkey>hanna-mueller-2025-incremental</bibkey>
    </paper>
    <paper id="165">
      <title>Entangled Relations: Leveraging <fixed-case>NLI</fixed-case> and Meta-analysis to Enhance Biomedical Relation Extraction</title>
      <author><first>William P</first><last>Hogan</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>3204-3220</pages>
      <abstract>Recent research efforts have explored the potential of leveraging natural language inference (NLI) techniques to enhance relation extraction (RE). In this vein, we introduce MetaEntail-RE, a novel adaptation method that harnesses NLI principles to enhance RE performance. Our approach follows past works by verbalizing relation classes into class-indicative hypotheses, aligning a traditionally multi-class classification task to one of textual entailment. We introduce three key enhancements: (1) Meta-class analysis which, instead of labeling non-entailed premise-hypothesis pairs with the less informative “neutral” entailment label, provides additional context by analyzing overarching meta-relationships between classes; (2) Feasible hypothesis filtering, which removes unlikely hypotheses from consideration based on domain knowledge derived from data; and (3) Group-based prediction selection, which further improves performance by selecting highly confident predictions. MetaEntail-RE is conceptually simple and empirically powerful, yielding significant improvements over conventional relation extraction techniques and other NLI formulations. We observe surprisingly large F1 gains of 17.6 points on BioRED and 13.4 points on ReTACRED compared to conventional methods, underscoring the versatility of MetaEntail-RE across both biomedical and general domains.</abstract>
      <url hash="4e21cc93">2025.naacl-long.165</url>
      <bibkey>hogan-shang-2025-entangled</bibkey>
    </paper>
    <paper id="166">
      <title>Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models</title>
      <author><first>Hengyi</first><last>Wang</last></author>
      <author><first>Haizhou</first><last>Shi</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Shiwei</first><last>Tan</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Weiyi</first><last>Qin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Wenyuan</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Tunyu</first><last>Zhang</last></author>
      <author><first>Akshay</first><last>Nambi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tanuja</first><last>Ganu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <pages>3221-3241</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have shown significant promise in various applications, leading to broad interest from researchers and practitioners alike. However, a comprehensive evaluation of their long-context capabilities remains underexplored. To address these gaps, we introduce the MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to assess the long-context capabilities of MLLMs. Besides multi-image input, we employ image stitching to further increase the input context length, and develop a protocol to automatically generate labels for sub-image level retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their capability to locate a target sub-image (needle) within a set of images (haystack) based on textual instructions and descriptions of image contents. This setup necessitates an advanced understanding of extensive visual contexts and effective information retrieval within long-context image inputs. With this benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and open-source models. The findings reveal that GPT-4o consistently surpasses other models in long-context scenarios, but suffers from hallucination problems in negative samples, i.e., when needles are not in the haystacks. Our comprehensive long-context evaluation of MLLMs also sheds lights on the considerable performance gap between API-based and open-source models. All the code, data, and instructions required to reproduce the main results are available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.</abstract>
      <url hash="861b4fae">2025.naacl-long.166</url>
      <bibkey>wang-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="167">
      <title><fixed-case>W</fixed-case>orld<fixed-case>C</fixed-case>uisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <author><first>Frederikus</first><last>Hudi</last></author>
      <author><first>Patrick Amadeus</first><last>Irawan</last></author>
      <author><first>David</first><last>Anugraha</last></author>
      <author><first>Rifki Afina</first><last>Putri</last><affiliation>Universitas Gadjah Mada</affiliation></author>
      <author><first>Wang</first><last>Yutong</last></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Ubaidillah Ariq</first><last>Prathama</last></author>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Afifa</first><last>Amriani</last><affiliation>No Institution</affiliation></author>
      <author><first>Anar</first><last>Rzayev</last></author>
      <author><first>Anirban</first><last>Das</last><affiliation>Capital One</affiliation></author>
      <author><first>Ashmari</first><last>Pramodya</last></author>
      <author><first>Aulia</first><last>Adila</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Candy Olivia</first><last>Mawalim</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Cheng Ching</first><last>Lam</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Daud</first><last>Abolade</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Fariz</first><last>Ikhwantri</last></author>
      <author><first>Garry</first><last>Kuwanto</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Hanyang</first><last>Zhao</last><affiliation>Columbia University</affiliation></author>
      <author><first>Haryo Akbarianto</first><last>Wibowo</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jan Wira Gotama</first><last>Putra</last></author>
      <author><first>Junho</first><last>Myung</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Lucky</first><last>Susanto</last></author>
      <author><first>Maria Angelica Riera</first><last>Machin</last></author>
      <author><first>Marina</first><last>Zhukova</last></author>
      <author><first>Michael</first><last>Anugraha</last></author>
      <author><first>Muhammad Farid</first><last>Adilazuarda</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Natasha Christabelle</first><last>Santosa</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Rio Alexander</first><last>Audino</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Shi-Xiong</first><last>Zhang</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Stephanie Yulia</first><last>Salim</last></author>
      <author><first>Yi</first><last>Zhou</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Yinxuan</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <author><first>Shogo</first><last>Okada</last></author>
      <author><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Derry Tanti</first><last>Wijaya</last><affiliation>Monash University and Boston University</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Chong-Wah</first><last>Ngo</last><affiliation>Singapore Management University</affiliation></author>
      <pages>3242-3264</pages>
      <abstract>Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.</abstract>
      <url hash="fa943506">2025.naacl-long.167</url>
      <bibkey>winata-etal-2025-worldcuisines</bibkey>
    </paper>
    <paper id="168">
      <title>Extracting and Understanding the Superficial Knowledge in Alignment</title>
      <author><first>Runjin</first><last>Chen</last></author>
      <author><first>Gabriel Jacob</first><last>Perin</last></author>
      <author><first>Xuxi</first><last>Chen</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Xilun</first><last>Chen</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Yan</first><last>Han</last><affiliation>Amazon</affiliation></author>
      <author><first>Nina S. T.</first><last>Hirata</last><affiliation>Universidade de São Paulo</affiliation></author>
      <author><first>Junyuan</first><last>Hong</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <pages>3265-3280</pages>
      <abstract>Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model’s ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate those superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.</abstract>
      <url hash="e0a830ee">2025.naacl-long.168</url>
      <bibkey>chen-etal-2025-extracting</bibkey>
    </paper>
    <paper id="169">
      <title>Smurfs: Multi-Agent System using Context-Efficient <fixed-case>DFSDT</fixed-case> for Tool Planning</title>
      <author><first>Junzhi</first><last>Chen</last></author>
      <author><first>Juhao</first><last>Liang</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>3281-3298</pages>
      <abstract>Teaching large language models (LLMs) to use tools for solving complex problems can grant them human-like reasoning abilities. ReAct and its variants are popular frameworks for tool use in both single-agent and multi-agent systems. To address issues like error propagation and limited exploration in ReAct, the Deep First Search Decision Tree (DFSDT) was proposed, but it faces challenges such as rollback instability, redundant context, and premature termination in single-agent settings. We introduce “Smurfs,” a novel multi-agent system (MAS) that enhances DFSDT with a modular, context-efficient, and training-free design. Smurfs surpasses baseline methods in both the open-ended StableToolBench and the closed-ended HotpotQA tasks, reducing token usage by 60.9% compared to DFSDT and enabling Mistral-7b to perform on par with GPT-4-DFSDT. Extensive ablation studies confirm the effectiveness of Smurfs’ core components, offering valuable insights for the construction and interpretation of MAS, and paving the way for future exploration. We release the code at https://github.com/FreedomIntelligence/Smurfs.</abstract>
      <url hash="745d6c62">2025.naacl-long.169</url>
      <bibkey>chen-etal-2025-smurfs</bibkey>
    </paper>
    <paper id="170">
      <title>From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Sheng</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hoifung</first><last>Poon</last><affiliation>Microsoft</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3299-3324</pages>
      <abstract>Motivated by in-context learning (ICL) capabilities of Large Language Models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. Through perturbations over different modality information, we show that modalities matter differently across tasks in multimodal ICL. Guided by task-specific modality impact, we recommend modality-driven demonstration strategies to boost ICL performance. We also find that models may follow inductive biases from multimodal ICL even if they are rarely seen in or contradict semantic priors from pretraining data. Our principled analysis provides a comprehensive way of understanding the role of demonstrations in multimodal in-context learning, and sheds light on effectively improving multimodal ICL on a wide range of tasks.</abstract>
      <url hash="64a77df2">2025.naacl-long.170</url>
      <bibkey>xu-etal-2025-introspection</bibkey>
    </paper>
    <paper id="171">
      <title>Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets</title>
      <author><first>Tianjian</first><last>Li</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Weiting</first><last>Tan</last></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3325-3343</pages>
      <abstract>Data abundance across different domains exhibits a long-tailed distribution: few domains have abundant data, while most face data scarcity. Our work focuses on a multilingual setting, where available data is heavily skewed toward high-resource languages, creating significant imbalances in training data sizes across languages. This disparity challenges training language models to perform uniformly well in all languages. Two common strategies to address this issue are upsampling low-resource languages (Temperature Sampling) and upweighting their loss functions (Scalarization). These methods are often assumed to be equivalent, but this equivalence has not been rigorously established, prompting our investigation.Through theoretical and empirical analysis, we identify when these two methods are equivalent and when they diverge. We prove that they are equivalent under full gradient descent but differ under stochastic gradient descent due to differences in gradient variance. Specifically, Temperature Sampling exhibits lower variance in gradient estimation, leading to faster convergence but a higher risk of overfitting. Based on these insights, we propose Cooldown, a strategy that starts by heavily upsampling low-resource languages to accelerate convergence and gradually reduces the upsampling to prevent overfitting—achieving the best of both worlds. Our method competes effectively with existing data re-weighting techniques while offering computational efficiency.</abstract>
      <url hash="5e87bc0a">2025.naacl-long.171</url>
      <bibkey>li-etal-2025-upsample</bibkey>
    </paper>
    <paper id="172">
      <title><fixed-case>LLM</fixed-case> The Genius Paradox: A Linguistic and Math Expert’s Struggle with Simple Word-based Counting Problems</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xuezhe</first><last>Ma</last><affiliation>USC/ISI</affiliation></author>
      <pages>3344-3370</pages>
      <abstract>Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r’s in the word “strawberry”. There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple evaluation settings to investigate validity of prevalent conjectures. Meanwhile, we measure transferability of advanced mathematical and coding reasoning capabilities from specialized LLMs to simple counting tasks. Although specialized LLMs suffer from counting problems as well, we find conjectures about inherent deficiency of LLMs invalid and further seek opportunities to elicit knowledge and capabilities from LLMs which are beneficial to counting tasks. Compared with strategies such as finetuning and in-context learning that are commonly adopted to enhance performance on new or challenging tasks, we show that engaging reasoning is the most robust and efficient way to help LLMs better perceive tasks with more accurate responses.We hope our conjecture validation design could provide insights to study future critical failure modes of LLMs. Based on challenges in transferring advanced capabilities to much simpler tasks, we call for more attention to model capability acquisition and evaluation. We also highlight the importance of cultivating consciousness of “reasoning before responding” during model pretraining.</abstract>
      <url hash="e71598e9">2025.naacl-long.172</url>
      <bibkey>xu-ma-2025-llm</bibkey>
    </paper>
    <paper id="173">
      <title><fixed-case>PAPILLON</fixed-case>: Privacy Preservation from <fixed-case>I</fixed-case>nternet-based and Local Language Model Ensembles</title>
      <author><first>Siyan</first><last>Li</last></author>
      <author><first>Vethavikashini Chithrra</first><last>Raghuram</last><affiliation>CCC Intelligent Solutions</affiliation></author>
      <author><first>Omar</first><last>Khattab</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>3371-3390</pages>
      <abstract>Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user’s machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM interactions to construct a natural benchmark called PUPA, which contains personally identifiable information (PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM pipeline that uses prompt optimization to address a simpler version of our task. Our best pipeline maintains high response quality for 85.5% of user queries while restricting privacy leakage to only 7.5%. We still leave a large margin to the generation quality of proprietary LLMs for future work.</abstract>
      <url hash="8a173c82">2025.naacl-long.173</url>
      <bibkey>li-etal-2025-papillon</bibkey>
    </paper>
    <paper id="174">
      <title><fixed-case>W</fixed-case>hen2<fixed-case>C</fixed-case>all: When (not) to Call Tools</title>
      <author><first>Hayley</first><last>Ross</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Ameya Sunil</first><last>Mahabaleshwarkar</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yoshi</first><last>Suhara</last><affiliation>NVIDIA</affiliation></author>
      <pages>3391-3409</pages>
      <abstract>Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling—whether the correct tool is called with the correct parameters—and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can’t be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this benchmark. We also develop a training set for When2Call and leverage the multiple-choice nature of the benchmark to develop a preference optimization training regime, which shows considerably more improvement than traditional fine-tuning. We release the benchmark and training data as well as evaluation scripts.</abstract>
      <url hash="7d33d93a">2025.naacl-long.174</url>
      <bibkey>ross-etal-2025-when2call</bibkey>
    </paper>
    <paper id="175">
      <title>Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization</title>
      <author><first>Zilu</first><last>Tang</last></author>
      <author><first>Rajen</first><last>Chatterjee</last><affiliation>Apple</affiliation></author>
      <author><first>Sarthak</first><last>Garg</last><affiliation>Apple</affiliation></author>
      <pages>3410-3433</pages>
      <abstract>Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user’s trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve *post-hoc* mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency.To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.</abstract>
      <url hash="755e83fc">2025.naacl-long.175</url>
      <bibkey>tang-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="176">
      <title>Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools</title>
      <author><first>Yilun</first><last>Hao</last></author>
      <author><first>Yongchao</first><last>Chen</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Chuchu</first><last>Fan</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>3434-3483</pages>
      <abstract>Large Language Models (LLMs) struggle to directly generate correct plans for complex multi-constraint planning problems, even with self-verification and self-critique. For example, a U.S. domestic travel planning benchmark TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information. In this work, we tackle this by proposing an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems, which are further consumed by sound and complete satisfiability solvers. We start with TravelPlanner as the primary use case and show that our framework achieves a success rate of 93.9% and is effective with diverse paraphrased prompts. More importantly, our framework has strong zero-shot generalizability, successfully handling unseen constraints in our newly created unseen international travel dataset and generalizing well to new fundamentally different domains. Moreover, when user input queries are infeasible, our framework can identify the unsatisfiable core, provide failure reasons, and offers personalized modification suggestions. We show that our framework can modify and solve for an average of 81.6% and 91.7% unsatisfiable queries from two datasets and prove with ablations that all key components of our framework are effective and necessary.</abstract>
      <url hash="69b12588">2025.naacl-long.176</url>
      <bibkey>hao-etal-2025-large</bibkey>
    </paper>
    <paper id="177">
      <title>Who Relies More on World Knowledge and Bias for Syntactic Ambiguity Resolution: Humans or <fixed-case>LLM</fixed-case>s?</title>
      <author><first>So Young</first><last>Lee</last><affiliation>Miami University</affiliation></author>
      <author><first>Russell</first><last>Scheinberg</last></author>
      <author><first>Amber</first><last>Shore</last><affiliation>Portland State University</affiliation></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <pages>3484-3498</pages>
      <abstract>This study explores how recent large language models (LLMs) navigate relative clause attachment ambiguity and use world knowledge biases for disambiguation in six typologically diverse languages: English, Chinese, Japanese, Korean, Russian, and Spanish. We describe the process of creating a novel dataset – MultiWho – for fine-grained evaluation of relative clause attachment preferences in ambiguous and unambiguous contexts. Our experiments with three LLMs indicate that, contrary to humans, LLMs consistently exhibit a preference for local attachment, displaying limited responsiveness to syntactic variations or language-specific attachment patterns.Although LLMs performed well in unambiguous cases, they rigidly prioritized world knowledge biases, lacking the flexibility of human language processing. These findings highlight the need for more diverse, pragmatically nuanced multilingual training to improve LLMs’ handling of complex structures and human-like comprehension.</abstract>
      <url hash="7c9d2c2d">2025.naacl-long.177</url>
      <bibkey>lee-etal-2025-relies</bibkey>
    </paper>
    <paper id="178">
      <title>Beyond Benchmarks: Building a Richer Cross-Document Event Coreference Dataset with Decontextualization</title>
      <author><first>Jin</first><last>Zhao</last></author>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Bingyang</first><last>Ye</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Xinrui</first><last>Hu</last></author>
      <author><first>Nianwen</first><last>Xue</last><affiliation>Brandeis University</affiliation></author>
      <author><first>James</first><last>Pustejovsky</last><affiliation>Brandeis University</affiliation></author>
      <pages>3499-3513</pages>
      <abstract>Cross-Document Event Coreference (CDEC) annotation is challenging and difficult to scale, resulting in existing datasets being small and lacking diversity. We introduce a new approach leveraging large language models (LLMs) to decontextualize event mentions, by simplifying the document-level annotation task to sentence pairs with enriched context, enabling the creation of Richer EventCorefBank (RECB), a denser and more expressive dataset annotated at faster speed. Decontextualization has been shown to improve annotation speed without compromising quality and to enhance model performance. Our baseline experiment indicates that systems trained on RECB achieve comparable results on the EventCorefBank(ECB+) test set, showing the high quality of our dataset and its generalizability on other CDEC datasets. In addition, our evaluation shows that the strong baseline models are still struggling with RECB comparing to other CDEC datasets, suggesting that the richness and diversity of RECB present significant challenges to current CDEC systems.</abstract>
      <url hash="e67899c9">2025.naacl-long.178</url>
      <bibkey>zhao-etal-2025-beyond</bibkey>
    </paper>
    <paper id="179">
      <title>Can Unconfident <fixed-case>LLM</fixed-case> Annotations Be Used for Confident Conclusions?</title>
      <author><first>Kristina</first><last>Gligoric</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tijana</first><last>Zrnic</last><affiliation>Stanford University</affiliation></author>
      <author><first>Cinoo</first><last>Lee</last><affiliation>Stanford University</affiliation></author>
      <author><first>Emmanuel</first><last>Candes</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>3514-3533</pages>
      <abstract>Large language models (LLMs) have shown high agreement with human raters across a variety of tasks, demonstrating potential to ease the challenges of human data collection. In computational social science (CSS), researchers are increasingly leveraging LLM annotations to complement slow and expensive human annotations. Still, guidelines for collecting and using LLM annotations, without compromising the validity of downstream conclusions, remain limited. We introduce Confidence-driven inference: a method that combines LLM annotations and LLM confidence indicators to strategically select which human annotations should be collected, with the goal of producing accurate statistical estimates and provably valid confidence intervals while reducing the number of human annotations needed. Our approach comes with safeguards against LLM annotations of poor quality, guaranteeing that the conclusions will be both valid and no less accurate than if we only relied on human annotations. We demonstrate the effectiveness of Confidence-driven inference over baselines in statistical estimation tasks across three CSS settings—text politeness, stance, and bias—reducing the needed number of human annotations by over 25% in each. Although we use CSS settings for demonstration, Confidence-driven inference can be used to estimate most standard quantities across a broad range of NLP problems.</abstract>
      <url hash="7e7410b7">2025.naacl-long.179</url>
      <bibkey>gligoric-etal-2025-unconfident</bibkey>
    </paper>
    <paper id="180">
      <title>Beyond End-to-End <fixed-case>VLM</fixed-case>s: Leveraging Intermediate Text Representations for Superior Flowchart Understanding</title>
      <author><first>Junyi</first><last>Ye</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Ankan</first><last>Dash</last></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Guiling</first><last>Wang</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <pages>3534-3548</pages>
      <abstract>Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability—users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability—it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or reasoning. We propose TextFlow, addressing aforementioned issues with two stages: (i) Vision Textualizer—which generates textual representations from flowchart images; and (ii) Textual Reasoner—which performs question-answering based on the text representations. TextFlow offers three key advantages: (i) users can select the type of text representations (e.g., Graphviz, Mermaid, PlantUML), or further convert them into executable graph object to call tools, enhancing performance and controllability; (ii) it improves explainability by helping to attribute errors more clearly to visual or textual processing components; and (iii) it promotes the modularization of the solution, such as allowing advanced LLMs to be used in the reasoner stage when VLMs underperform in end-to-end fashion. Experiments on the FlowVQA and FlowLearn benchmarks demonstrate TextFlow’s state-of-the-art performance as well as its robustness. All code and data are publicly available.</abstract>
      <url hash="91ba3239">2025.naacl-long.180</url>
      <bibkey>ye-etal-2025-beyond</bibkey>
    </paper>
    <paper id="181">
      <title>Ihquin tlahtouah in Tetelahtzincocah: An annotated, multi-purpose audio and text corpus of Western Sierra <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl</title>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Cheyenne</first><last>Wing</last></author>
      <author><first>María Ximena Juárez</first><last>Huerta</last></author>
      <author><first>Angeles Márquez</first><last>Hernandez</last></author>
      <author><first>Francis M.</first><last>Tyers</last><affiliation>Indiana University, Bloomington</affiliation></author>
      <pages>3549-3562</pages>
      <abstract>The development of digital linguistic resources is essential for enhancing the inclusion of indigenous and marginalized languages in the digital domain. Indigenous languages of Mexico, despite representing vast typological diversity and millions of speakers, have largely been overlooked in NLP until recently. In this paper, we present a corpus of audio and annotated transcriptions of Western Sierra Puebla Nahuatl, an endangered variety of Nahuatl spoken in Puebla, Mexico. The data made available in this corpus are useful for ASR, spelling normalization, and word-level language identification. We detail the corpus-creation process, and describe experiments to report benchmark results for each of these important NLP tasks. The corpus audio and text is made freely available.</abstract>
      <url hash="29bb88f8">2025.naacl-long.181</url>
      <bibkey>pugh-etal-2025-ihquin</bibkey>
    </paper>
    <paper id="182">
      <title>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</title>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <author><first>Zhouxiang</first><last>Fang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Yash</first><last>Singla</last><affiliation>Johns Hopkins University and Johns Hopkins University</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering and Bloomberg</affiliation></author>
      <pages>3563-3599</pages>
      <abstract>LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.</abstract>
      <url hash="c2f8c004">2025.naacl-long.182</url>
      <bibkey>chen-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="183">
      <title>Unfamiliar Finetuning Examples Control How Language Models Hallucinate</title>
      <author><first>Katie</first><last>Kang</last></author>
      <author><first>Eric</first><last>Wallace</last><affiliation>OpenAI and University of California Berkeley</affiliation></author>
      <author><first>Claire</first><last>Tomlin</last><affiliation>UC Berkeley</affiliation></author>
      <author><first>Aviral</first><last>Kumar</last><affiliation>Carnegie Mellon University and Google DeepMind</affiliation></author>
      <author><first>Sergey</first><last>Levine</last><affiliation>University of California Berkeley</affiliation></author>
      <pages>3600-3612</pages>
      <abstract>Large language models are known to hallucinate, but the underlying mechanism that govern how models hallucinate are not yet fully understood. In this work, we find that unfamiliar examples in the models’ finetuning data – those that introduce concepts beyond the base model’s scope of knowledge – are crucial in shaping these errors. In particular, we find that an LLM’s hallucinated predictions tend to mirror the responses associated with its unfamiliar finetuning examples. This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model’s responses to unfamiliar queries (e.g., say “I don’t know”). We empirically validate this observation in a series of controlled experiments involving SFT, RL, and reward model finetuning on TriviaQA and MMLU. Our work further investigates RL finetuning strategies for improving the factuality of long-form model generations. We find that, while hallucinations from the reward model can significantly undermine the effectiveness of RL factuality finetuning, strategically controlling how reward models hallucinate can minimize these negative effects. Leveraging our previous observations on controlling hallucinations, we propose an approach for learning more reliable reward models, and show that they improve the efficacy of RL factuality finetuning in long-form biography and book/movie plot generation tasks.</abstract>
      <url hash="9e7612c4">2025.naacl-long.183</url>
      <bibkey>kang-etal-2025-unfamiliar</bibkey>
    </paper>
    <paper id="184">
      <title>Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient <fixed-case>LLM</fixed-case> Sampling</title>
      <author><first>Guangya</first><last>Wan</last></author>
      <author><first>Yuqi</first><last>Wu</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Sheng</first><last>Li</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>3613-3635</pages>
      <abstract>Self-consistency mitigates hallucinations in Large Language Models (LLMs) by sampling multiple reasoning paths, but it lacks a systematic approach to determine the optimal number of samples or select the most faithful rationale. To address this limitation, we introduce Reasoning-Aware Self-Consistency (RASC), a novel framework that enhances sampling efficiency and reasoning faithfulness by dynamically evaluating both outputs and rationales. RASC assesses the quality of reasoning and the consistency of answers for each generated sample, using these assessments to guide early stopping decisions and rationale selection. The framework employs criteria-based stopping and weighted majority voting, enabling more informed choices on when to halt sampling and which rationale to select. Our comprehensive experiments across diverse question-answering datasets demonstrate that RASC outperforms existing methods, reducing sample usage by approximately 70% while maintaining accuracy. Moreover, RASC facilitates the selection of high-fidelity rationales, thereby improving the faithfulness of LLM outputs. Our approach effectively addresses the efficiency-accuracy trade-off in LLM reasoning tasks, offering a new perspective for more nuanced, faithful, and effective utilization of LLMs in resource-constrained environments.</abstract>
      <url hash="cd3c6805">2025.naacl-long.184</url>
      <bibkey>wan-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="185">
      <title><fixed-case>M</fixed-case>at<fixed-case>V</fixed-case>i<fixed-case>X</fixed-case>: Multimodal Information Extraction from Visually Rich Articles</title>
      <author><first>Ghazal</first><last>Khalighinejad</last><affiliation>Department of Computer Science, Duke University</affiliation></author>
      <author><first>Sharon</first><last>Scott</last><affiliation>P&amp;G</affiliation></author>
      <author><first>Ollie</first><last>Liu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kelly L.</first><last>Anderson</last></author>
      <author><first>Rickard</first><last>Stureborg</last><affiliation>Duke University</affiliation></author>
      <author><first>Aman</first><last>Tyagi</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>3636-3655</pages>
      <abstract>Multimodal information extraction (MIE) is crucial for scientific literature, where valuable data is often spread across text, figures, and tables. In materials science, extracting structured information from research articles can accelerate the discovery of new materials. However, the multimodal nature and complex interconnections of scientific content present challenges for traditional text-based methods. We introduce MatViX, a benchmark consisting of 324 full-length research articles and 1,688 complex structured JSON files, carefully curated by domain experts in polymer nanocomposites and biodegradation. These JSON files are extracted from text, tables, and figures in full-length documents, providing a comprehensive challenge for MIE. We introduce a novel evaluation method to assess the accuracy of curve similarity and the alignment of hierarchical structures. Additionally, we benchmark vision-language models (VLMs) in a zero-shot manner, capable of processing long contexts and multimodal inputs. Our results demonstrate significant room for improvement in current models.</abstract>
      <url hash="66acaaf9">2025.naacl-long.185</url>
      <bibkey>khalighinejad-etal-2025-matvix</bibkey>
    </paper>
    <paper id="186">
      <title>Towards Rationality in Language and Multimodal Agents: A Survey</title>
      <author><first>Bowen</first><last>Jiang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Yangxinyu</first><last>Xie</last></author>
      <author><first>Xiaomeng</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Yuan</last></author>
      <author><first>Zhuoqun</first><last>Hao</last></author>
      <author><first>Xinyi</first><last>Bai</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Weijie J</first><last>Su</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Camillo Jose</first><last>Taylor</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Tanwi</first><last>Mallick</last><affiliation>Argonne National Laboratory</affiliation></author>
      <pages>3656-3675</pages>
      <abstract>This work discusses how to build more rational language and multimodal agents and what criteria define rationality in intelligent systems.Rationality is the quality of being guided by reason, characterized by decision-making that aligns with evidence and logical principles. It plays a crucial role in reliable problem-solving by ensuring well-grounded and consistent solutions. Despite their progress, large language models (LLMs) often fall short of rationality due to their bounded knowledge space and inconsistent outputs. In response, recent efforts have shifted toward developing multimodal and multi-agent systems, as well as integrating modules like external tools, programming codes, symbolic reasoners, utility function, and conformal risk controls rather than relying solely on a single LLM for decision-making. This paper surveys state-of-the-art advancements in language and multimodal agents, assesses their role in enhancing rationality, and outlines open challenges and future research directions. We maintain an open repository at https://github.com/bowen-upenn/Agent_Rationality.</abstract>
      <url hash="75becc60">2025.naacl-long.186</url>
      <bibkey>jiang-etal-2025-towards</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>C</fixed-case>lu<fixed-case>S</fixed-case>an<fixed-case>T</fixed-case>: Differentially Private and Semantically Coherent Text Sanitization</title>
      <author><first>Ahmed Musa</first><last>Awon</last></author>
      <author><first>Yun</first><last>Lu</last><affiliation>University of Victoria</affiliation></author>
      <author><first>Shera</first><last>Potka</last><affiliation>University of Victoria</affiliation></author>
      <author><first>Alex</first><last>Thomo</last><affiliation>University of Victoria</affiliation></author>
      <pages>3676-3693</pages>
      <abstract>We introduce CluSanT, a novel text sanitization framework based on Metric Local Differential Privacy (MLDP). Our framework consists of three components: token clustering, cluster embedding, and token sanitization. For the first, CluSanT employs Large Language Models (LLMs) to create—a set of potential substitute tokens which we meaningfully cluster. Then, we develop a parameterized cluster embedding that balances the trade-off between privacy and utility. Lastly, we propose a MLDP algorithm which sanitizes/substitutes sensitive tokens in a text with the help of our embedding. Notably, our MLDP-based framework can be tuned with parameters such that (1) existing state-of-the-art (SOTA) token sanitization algorithms can be described—and improved—via our framework with extremal values of our parameters, and (2) by varying our parameters, we allow for a whole spectrum of privacy-utility tradeoffs between the two SOTA. Our experiments demonstrate CluSanT’s balance between privacy and semantic coherence, highlighting its capability as a valuable framework for privacy-preserving text sanitization.</abstract>
      <url hash="7c040721">2025.naacl-long.187</url>
      <bibkey>awon-etal-2025-clusant</bibkey>
    </paper>
    <paper id="188">
      <title><fixed-case>T</fixed-case>urking<fixed-case>B</fixed-case>ench: A Challenge Benchmark for Web Agents</title>
      <author><first>Kevin</first><last>Xu</last></author>
      <author><first>Yeganeh</first><last>Kordi</last></author>
      <author><first>Tanay</first><last>Nayak</last></author>
      <author><first>Adi</first><last>Asija</last></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Kate</first><last>Sanders</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Adam</first><last>Byerly</last><affiliation>Johns Hopkins University and Johns Hopkins University Applied Physics Laboratory</affiliation></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3694-3710</pages>
      <abstract>Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments.Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally designed for crowdsourcing workers to perform various annotation tasks. Each task’s HTML instructions are instantiated with different values derived from crowdsourcing tasks, creating diverse instances. This benchmark includes 32.2K instances spread across 158 tasks.To support the evaluation of TurkingBench, we have developed a framework that links chatbot responses to actions on web pages (e.g., modifying a text box, selecting a radio button). We assess the performance of cutting-edge private and open-source models, including language-only and vision-language models (such as GPT4 and InternVL), on this benchmark. Our results show that while these models outperform random chance, there is still significant room for improvement. We hope that this benchmark will drive progress in the evaluation and development of web-based agents.</abstract>
      <url hash="1c557b56">2025.naacl-long.188</url>
      <bibkey>xu-etal-2025-turkingbench</bibkey>
    </paper>
    <paper id="189">
      <title><fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>ree: Agent-guided Tree Search for Code Generation with Large Language Models</title>
      <author><first>Jierui</first><last>Li</last></author>
      <author><first>Hung</first><last>Le</last><affiliation>Amazon</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Doyen</first><last>Sahoo</last><affiliation>SalesForce.com</affiliation></author>
      <pages>3711-3726</pages>
      <abstract>Pretrained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1% on HumanEval, 98.7% on MBPP, and 43.0% on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains, achieving a 31.9% solving rate.</abstract>
      <url hash="0bedbddc">2025.naacl-long.189</url>
      <bibkey>li-etal-2025-codetree</bibkey>
    </paper>
    <paper id="190">
      <title><fixed-case>DPL</fixed-case>: Diverse Preference Learning Without A Reference Model</title>
      <author><first>Abhijnan</first><last>Nath</last></author>
      <author><first>Andrey</first><last>Volozin</last><affiliation>Optum</affiliation></author>
      <author><first>Saumajit</first><last>Saha</last></author>
      <author><first>Albert Aristotle</first><last>Nanda</last></author>
      <author><first>Galina</first><last>Grunin</last><affiliation>Optum</affiliation></author>
      <author><first>Rahul</first><last>Bhotika</last><affiliation>Optum Labs</affiliation></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last><affiliation>Colorado State University</affiliation></author>
      <pages>3727-3747</pages>
      <abstract>In direct preference alignment in LLMs, most existing methods seek to retrieve the reward function directly from preference data. However, real-world preference data often contains diversity in preference annotations reflective of true human preferences. Existing algorithms, including KTO, do not directly utilize such nuances in the annotations which limits their applicability. In this work, we propose Diverse Preference Learning (DPL), a reference model-free method that simultaneously learns a baseline desirability in LLM responses while being robust to the diversity of preference annotations. Our experiments for instruction-following on Ultrafeedback and AlpacaEval 2.0 and for text-summarization on Reddit TL;DR suggest that DPL is consistently better at learning the diversity of preferences compared to existing methods, including those that require a reference model in memory. Apart from overall quality, we find that DPL’s completions, on average, are more honest, helpful, truthful and safe compared to existing methods.</abstract>
      <url hash="ac8a76d4">2025.naacl-long.190</url>
      <bibkey>nath-etal-2025-dpl</bibkey>
    </paper>
    <paper id="191">
      <title>Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data</title>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Marc</first><last>Marone</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tianjian</first><last>Li</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3748-3768</pages>
      <abstract>To trust the fluent generations of large language models (LLMs), humans must be able to _verify_ their correctness against trusted, external sources. Recent efforts, such as providing citations via retrieved documents or post-hoc provenance, enhance verifiability but provide no guarantees on their correctness. To address these limitations, we tackle the verifiability goal with a different philosophy: _trivializing the verification process by developing models that quote verbatim statements from trusted sources in their pre-training data._We propose Quote-Tuning, which demonstrates the feasibility of aligning models to quote. The core of Quote-Tuning is a fast membership inference function that efficiently verifies text against trusted corpora. We leverage this tool to design a reward function to quantify quotes in model responses, and curate datasets for preference learning. Experiments show that Quote-Tuning significantly increases verbatim quotes from high-quality documents by up to 130% relative to base models while maintaining response quality. Quote-Tuning is applicable in different tasks, generalizes to out-of-domain data and diverse model families, and provides additional benefits to truthfulness. Our method not only serves as a hassle-free method to increase quoting but also opens up avenues for improving LLM trustworthiness through better verifiability.</abstract>
      <url hash="5f993c9f">2025.naacl-long.191</url>
      <bibkey>zhang-etal-2025-verifiable</bibkey>
    </paper>
    <paper id="192">
      <title><fixed-case>V</fixed-case>o<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models</title>
      <author><first>Zejun</first><last>Li</last></author>
      <author><first>Ruipu</first><last>Luo</last></author>
      <author><first>Jiwen</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>3769-3798</pages>
      <url hash="e512a222">2025.naacl-long.192</url>
      <bibkey>li-etal-2025-vocot</bibkey>
    </paper>
    <paper id="193">
      <title><fixed-case>ACCORD</fixed-case>: Closing the Commonsense Measurability Gap</title>
      <author><first>François</first><last>Roewer-Després</last></author>
      <author><first>Jinyue</first><last>Feng</last></author>
      <author><first>Zining</first><last>Zhu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Frank</first><last>Rudzicz</last><affiliation>Dalhousie University</affiliation></author>
      <pages>3799-3829</pages>
      <abstract>We present ACCORD, a framework and benchmark suite for disentangling the commonsense grounding and reasoning abilities of large language models (LLMs) through controlled, multi-hop counterfactuals. ACCORD introduces formal elements to commonsense reasoning to explicitly control and quantify reasoning complexity beyond the typical 1 or 2 hops. Uniquely, ACCORD can automatically generate benchmarks of arbitrary reasoning complexity, so it scales with future LLM improvements. Indeed, our experiments on state-of-the-art LLMs show performance degrading to below random chance with only moderate scaling, leaving substantial headroom for improvement. We release a leaderboard of the benchmark suite tested in this work, as well as code for automatically generating more complex benchmarks.</abstract>
      <url hash="4fffb5cb">2025.naacl-long.193</url>
      <bibkey>roewer-despres-etal-2025-accord</bibkey>
    </paper>
    <paper id="194">
      <title><fixed-case>CRMA</fixed-case>rena: Understanding the Capacity of <fixed-case>LLM</fixed-case> Agents to Perform Professional <fixed-case>CRM</fixed-case> Tasks in Realistic Environments</title>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Akshara</first><last>Prabhakar</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Sidharth</first><last>Dhawan</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Yixin</first><last>Mao</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Huan</first><last>Wang</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>3830-3850</pages>
      <abstract>Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. Following guidance from CRM experts and industry best practices, we designed CRMArena with nine customer service tasks distributed across three personas: service agent, analyst, and manager. The benchmark includes 16 commonly used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, along with latent variables (e.g., complaint habits, policy violations) to simulate realistic data distributions. Experimental results reveal that state-of-the-art LLM agents succeed in less than 58% of the tasks with ReAct prompting, and less than 65% even with function-calling abilities. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environments.</abstract>
      <url hash="b4117a1c">2025.naacl-long.194</url>
      <bibkey>huang-etal-2025-crmarena</bibkey>
    </paper>
    <paper id="195">
      <title>Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models</title>
      <author><first>Juan Pablo</first><last>Munoz</last><affiliation>Intel</affiliation></author>
      <author><first>Jinjie</first><last>Yuan</last><affiliation>Intel</affiliation></author>
      <author><first>Nilesh</first><last>Jain</last><affiliation>Intel Corp</affiliation></author>
      <pages>3851-3863</pages>
      <abstract>Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal of selected components at different granularities to reduce the model size and computational overhead, thus improving their efficiency while maintaining accuracy. The proposed solutions, collectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x during inference, demonstrating that model efficiency can be improved by eliminating several redundancies with minimal impact on the overall model performance. The code is available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.</abstract>
      <url hash="50acd952">2025.naacl-long.195</url>
      <bibkey>munoz-etal-2025-mamba</bibkey>
    </paper>
    <paper id="196">
      <title><fixed-case>CBT</fixed-case>-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title>
      <author><first>Mian</first><last>Zhang</last></author>
      <author><first>Xianjun</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Xinlu</first><last>Zhang</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Travis</first><last>Labrum</last></author>
      <author><first>Jamie C.</first><last>Chiu</last><affiliation>Princeton University</affiliation></author>
      <author><first>Shaun M.</first><last>Eack</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Fei</first><last>Fang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <pages>3864-3900</pages>
      <abstract>There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-Bench, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-Bench: **I: Basic CBT knowledge acquisition**, with the task of multiple-choice questions; **II: Cognitive model understanding**, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; **III: Therapeutic response generation**, with the task of generating responses to patient speech in CBT therapy sessions.These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients’ cognitive structures and generating effective responses, suggesting potential future work.</abstract>
      <url hash="c45fed60">2025.naacl-long.196</url>
      <bibkey>zhang-etal-2025-cbt</bibkey>
    </paper>
    <paper id="197">
      <title>An Efficient Gloss-Free Sign Language Translation Using Spatial Configurations and Motion Dynamics with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Eui Jun</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Junmyeong</first><last>Lee</last></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>3901-3920</pages>
      <abstract>Gloss-free Sign Language Translation (SLT) converts sign videos into spoken language sentences without relying on glosses, which are the written representations of signs. Recently, Large Language Models (LLMs) have shown remarkable translation performance in gloss-free methods by harnessing their powerful natural language generation capabilities. However, these methods often rely on domain-specific fine-tuning of visual encoders to achieve optimal results. By contrast, we emphasize the importance of capturing the spatial configurations and motion dynamics in sign language. With this in mind, we introduce Spatial and Motion-based Sign Language Translation (SpaMo), a novel LLM-based SLT framework. The core idea of SpaMo is simple yet effective: instead of domain-specific tuning, we use off-the-shelf visual encoders to extract spatial and motion features, which are then input into an LLM along with a language prompt. Additionally, we employ a visual-text alignment process as a lightweight warm-up step before applying SLT supervision. Our experiments demonstrate that SpaMo achieves state-of-the-art performance on three popular datasets—PHOENIX14T, CSL-Daily, and How2Sign—without visual fine-tuning.</abstract>
      <url hash="713e6bf6">2025.naacl-long.197</url>
      <bibkey>hwang-etal-2025-efficient</bibkey>
    </paper>
    <paper id="198">
      <title><fixed-case>S</fixed-case>ketch2<fixed-case>C</fixed-case>ode: Evaluating Vision-Language Models for Interactive Web Design Prototyping</title>
      <author><first>Ryan</first><last>Li</last></author>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>3921-3955</pages>
      <abstract>Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas. However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration. To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code supports interactive agent evaluation that mimics real-world design workflows, where a VLM-based agent iteratively refines its generations by communicating with a simulated user, either passively receiving feedback instructions or proactively asking clarification questions. We comprehensively analyze ten commercial and open-source models, showing that Sketch2Code is challenging for existing VLMs; even the most capable models struggle to accurately interpret sketches and formulate effective questions that lead to steady improvement. Nevertheless, a user study with UI/UX experts reveals a significant preference for proactive question-asking over passive feedback reception, highlighting the need to develop more effective paradigms for multi-turn conversational assistants.</abstract>
      <url hash="6a8dc771">2025.naacl-long.198</url>
      <bibkey>li-etal-2025-sketch2code</bibkey>
    </paper>
    <paper id="199">
      <title><fixed-case>D</fixed-case>esign2<fixed-case>C</fixed-case>ode: Benchmarking Multimodal Code Generation for Automated Front-End Engineering</title>
      <author><first>Chenglei</first><last>Si</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Li</last></author>
      <author><first>Zhengyuan</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>3956-3974</pages>
      <abstract>Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development in which multimodal large language models (MLLMs) directly convert visual designs into code implementations. In this work, we construct Design2Code – the first real-world benchmark for this task. Specifically, we manually curate 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations to validate the performance ranking. To rigorously benchmark MLLMs, we test various multimodal prompting methods on frontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained break-down metrics indicate that models mostly lag in recalling visual elements from the input webpages and generating correct layout designs.</abstract>
      <url hash="c3d2a300">2025.naacl-long.199</url>
      <bibkey>si-etal-2025-design2code</bibkey>
    </paper>
    <paper id="200">
      <title>Temporal-Aware Soft Prompt Tuning for Automatic Text Dating</title>
      <author><first>Hai</first><last>Wang</last></author>
      <author><first>Yuzhi</first><last>Liang</last></author>
      <author><first>Han</first><last>Ren</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <pages>3975-3987</pages>
      <abstract>This paper presents Temporal-aware Soft Prompt Tuning (TASPT), a novel approach for automatic text dating. Unlike existing methods, which often overlook the evolution of word meanings in texts spanning long periods, TASPT incorporates the unique characteristics of historical texts. It introduces a temporal-aware text representation that dynamically captures both semantic variance and invariance. This representation is combined with a soft prompt, enabling efficient parameter tuning for automatic text dating. Experiments show that TASPT outperforms all existing methods on two diachronic datasets: the Twenty-Four Histories and the Royal Society Corpus.</abstract>
      <url hash="a0c70587">2025.naacl-long.200</url>
      <bibkey>wang-etal-2025-temporal</bibkey>
    </paper>
    <paper id="201">
      <title>Sparser Mixture-of-Adapters with Cross-Layer Generalization</title>
      <author><first>Ziyue</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3988-4002</pages>
      <url hash="a3536642">2025.naacl-long.201</url>
      <bibkey>li-zhou-2025-sparser</bibkey>
    </paper>
    <paper id="202">
      <title>How to Align Multiple Signed Language Corpora for Better Sign-to-Sign Translations?</title>
      <author><first>Mert</first><last>Inan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yang</first><last>Zhong</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Vidya</first><last>Ganesh</last></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>4003-4016</pages>
      <abstract>There are more than 300 documented signed languages worldwide, which are indispensable avenues for computational linguists to study cross-cultural and cross-linguistic factors that affect automatic sign understanding and generation. Yet, these are studied under critically low-resource settings, especially when examining multiple signed languages simultaneously. In this work, we hypothesize that a linguistically informed alignment algorithm can improve the results of sign-to-sign translation models. To this end, we first conduct a qualitative analysis of similarities and differences across three signed languages: American Sign Language (ASL), Chinese Sign Language (CSL), and German Sign Language (DGS). We then introduce a novel generation and alignment algorithm for translating one sign language to another, exploring Large Language Models (LLMs) as intermediary translators and paraphrasers. We also compile a dataset of sign-to-sign translation pairs between these signed languages. Our model trained on this dataset performs well on automatic metrics for sign-to-sign translation and generation. Our code and data will be available for the camera-ready version of the paper.</abstract>
      <url hash="2df825f3">2025.naacl-long.202</url>
      <bibkey>inan-etal-2025-align</bibkey>
    </paper>
    <paper id="203">
      <title>Communication Makes Perfect: Persuasion Dataset Construction via Multi-<fixed-case>LLM</fixed-case> Communication</title>
      <author><first>Weicheng</first><last>Ma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hefan</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Ivory</first><last>Yang</last></author>
      <author><first>Shiyu</first><last>Ji</last></author>
      <author><first>Joice</first><last>Chen</last></author>
      <author><first>Farnoosh</first><last>Hashemi</last></author>
      <author><first>Shubham</first><last>Mohole</last></author>
      <author><first>Ethan</first><last>Gearey</last></author>
      <author><first>Michael</first><last>Macy</last></author>
      <author><first>Saeed</first><last>Hassanpour</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>4017-4045</pages>
      <abstract>Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework’s potential to significantly advance research in both computational and social science domains concerning persuasive communication.</abstract>
      <url hash="ba296021">2025.naacl-long.203</url>
      <bibkey>ma-etal-2025-communication</bibkey>
    </paper>
    <paper id="204">
      <title>Soft Prompting for Unlearning in Large Language Models</title>
      <author><first>Karuna</first><last>Bhaila</last></author>
      <author><first>Minh-Hao</first><last>Van</last></author>
      <author><first>Xintao</first><last>Wu</last></author>
      <pages>4046-4056</pages>
      <abstract>The widespread popularity of Large Language Models (LLMs), partly due to their emerging in-context learning ability, has highlighted the importance of ethical and safety considerations for deployment. Motivated by corresponding data protection guidelines, we investigate machine unlearning for LLMs. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize unlearning in LLMs. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that are prepended to a query to induce unlearning of specific training examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method, and results indicate that SPUL can significantly improve the trade-off between utility and forgetting for text classification and question-answering. We further validate our method with LLMs of varying parameter sizes to highlight its flexibility and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data.</abstract>
      <url hash="8636dc84">2025.naacl-long.204</url>
      <bibkey>bhaila-etal-2025-soft</bibkey>
    </paper>
    <paper id="205">
      <title>Mutual-pairing Data Augmentation for Fewshot Continual Relation Extraction</title>
      <author><first>Nguyen Hoang</first><last>Anh</last></author>
      <author><first>Quyen</first><last>Tran</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Thanh Xuan</first><last>Nguyen</last></author>
      <author><first>Nguyen Thi Ngoc</first><last>Diep</last></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Trung</first><last>Le</last><affiliation>Monash University</affiliation></author>
      <pages>4057-4075</pages>
      <abstract>Data scarcity is a major challenge in Few-shot Continual Relation Extraction (FCRE), where models must learn new relations from limited data while retaining past knowledge. Current methods, restricted by minimal data streams, struggle with catastrophic forgetting and overfitting. To overcome this, we introduce a novel *data augmentation strategy* that transforms single input sentences into complex texts by integrating both old and new data. Our approach sharpens model focus, enabling precise identification of word relationships based on specified relation types. By embedding adversarial training effects and leveraging new training perspectives through special objective functions, our method enhances model performance significantly. Additionally, we explore Sharpness-Aware Minimization (SAM) in Few-shot Continual Learning. Our extensive experiments uncover fascinating behaviors of SAM across tasks and offer valuable insights for future research in this dynamic field.</abstract>
      <url hash="522ec61a">2025.naacl-long.205</url>
      <bibkey>anh-etal-2025-mutual</bibkey>
    </paper>
    <paper id="206">
      <title><fixed-case>KMMLU</fixed-case>: Measuring Massive Multitask Language Understanding in <fixed-case>K</fixed-case>orean</title>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>Hanwool</first><last>Lee</last><affiliation>Shinhan Securities</affiliation></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI</affiliation></author>
      <author><first>Seungone</first><last>Kim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Stanford University, Contextual AI and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Taekyoon</first><last>Choi</last><affiliation>NAVER</affiliation></author>
      <author><first>Cheonbok</first><last>Park</last><affiliation>NAVER</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <author><first>Stella</first><last>Biderman</last><affiliation>EleutherAI and Booz Allen Hamilton</affiliation></author>
      <pages>4076-4104</pages>
      <abstract>We propose KMMLU, a Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. While prior Korean evaluation tools heavily rely on translated versions of existing English benchmarks, KMMLU is collected from original Korean exams, thereby capturing linguistic and cultural aspects of the Korean language. Recent models struggle to show performance over 60%, significantly below the pass mark of the source exams (80%), highlighting the room for improvement. Notably, one-fifth of the questions in KMMLU require knowledge of Korean culture for accurate resolution. KMMLU thus provides a more accurate reflection of human preferences compared to translated versions of MMLU and offers deeper insights into LLMs’ shortcomings in Korean knowledge. The dataset and codes are made publicly available for future research.</abstract>
      <url hash="3878fe99">2025.naacl-long.206</url>
      <bibkey>son-etal-2025-kmmlu</bibkey>
    </paper>
    <paper id="207">
      <title>Protecting Privacy in Multimodal Large Language Models with <fixed-case>MLLMU</fixed-case>-Bench</title>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Guangyao</first><last>Dou</last></author>
      <author><first>Mengzhao</first><last>Jia</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Qingkai</first><last>Zeng</last><affiliation>Amazon</affiliation></author>
      <author><first>Yongle</first><last>Yuan</last></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>4105-4135</pages>
      <abstract>Generative models such as Large Language Models (LLM) and Multimodal Large Language models (MLLMs) trained on massive web corpora can memorize and disclose individuals’ confidential and private data, raising legal and ethical concerns. While many previous works have addressed this issue in LLM via machine unlearning, it remains largely unexplored for MLLMs. To tackle this challenge, we introduce Multimodal Large Language Model Unlearning Benchmark (MLLMU-Bench), a novel benchmark aimed at advancing the understanding of multimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives. The benchmark is divided into four sets to assess unlearning algorithms in terms of efficacy, generalizability, and model utility. Finally, we provide baseline results using existing generative model unlearning algorithms. Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation tasks, while multimodal unlearning approaches perform better in classification with multimodal inputs.</abstract>
      <url hash="e6bfd31b">2025.naacl-long.207</url>
      <bibkey>liu-etal-2025-protecting</bibkey>
    </paper>
    <paper id="208">
      <title><fixed-case>LLM</fixed-case>4<fixed-case>D</fixed-case>ist<fixed-case>R</fixed-case>econfig: A Fine-tuned Large Language Model for Power Distribution Network Reconfiguration</title>
      <author><first>Panayiotis</first><last>Christou</last></author>
      <author><first>Md. Zahidul</first><last>Islam</last></author>
      <author><first>Yuzhang</first><last>Lin</last><affiliation>New York University</affiliation></author>
      <author><first>Jingwei</first><last>Xiong</last></author>
      <pages>4136-4155</pages>
      <abstract>Power distribution networks are evolving due to the integration of distributed energy resources (DERs) and increased customer participation. To maintain optimal operation, minimize losses, and meet varying load demands, frequent network reconfiguration is necessary. Traditionally, the reconfiguration task relies on optimization software and expert operators, but as systems grow more complex, faster and more adaptive solutions are required without expert intervention. Data-driven reconfiguration is gaining traction for its accuracy, speed, and robustness against incomplete network data. Large language models (LLMs), with their ability to capture complex patterns, offer a promising approach for efficient and responsive network reconfiguration in evolving complex power networks.In this work, we introduce LLM4DistReconfig, a deep learning-based approach utilizing a fine-tuned LLM to solve the distribution network reconfiguration problem. By carefully crafting prompts and designing a custom loss function, we train the LLM with inputs representing network parameters such as buses, available lines, open lines, node voltages, and system loss. The model then predicts optimal reconfigurations by outputting updated network configurations that minimize system loss while meeting operational constraints. Our approach significantly reduces inference time compared to classical algorithms, allowing for near real-time optimal reconfiguration after training. Experimental results show that our method generates optimal configurations minimizing system loss for five individual and a combined test dataset. It also produces minimal invalid edges, no cycles, or subgraphs across all datasets, fulfilling domain-specific needs. Additionally, the generated responses contain less than 5% improper outputs on seen networks and satisfactory results on unseen networks, demonstrating its effectiveness and reliability for the reconfiguration task.</abstract>
      <url hash="6abb9291">2025.naacl-long.208</url>
      <bibkey>christou-etal-2025-llm4distreconfig</bibkey>
    </paper>
    <paper id="209">
      <title><fixed-case>W</fixed-case>ater<fixed-case>P</fixed-case>ool: A Language Model Watermark Mitigating Trade-Offs among Imperceptibility, Efficacy and Robustness</title>
      <author><first>Baizhou</first><last>Huang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>4156-4182</pages>
      <abstract>Watermarking is a prominent technique to trace the usage of specific large language models (LLMs) by injecting patterns into model-generated content. An ideal watermark should be imperceptible, easily detectable, and robust to text alterations, yet existing methods typically face trade-offs among these properties. This paper utilizes a key-centered scheme to unify existing methods by decomposing a watermark into two components: a key module and a mark module. We show that the trade-off issue is the reflection of the conflict between the scale of the key sampling space during generation and the complexity of key restoration during detection within the key module. To this end, we introduce WaterPool, a simple yet effective key module that preserves a complete key sampling space for imperceptibility while utilizing semantics-based search to improve the key restoration process. WaterPool can integrate seamlessly with existing watermarking techniques, significantly enhancing their performance, achieving near-optimal imperceptibility, and markedly improving their detection efficacy and robustness (+12.73% for KGW, +20.27% for EXP, +7.27% for ITS).</abstract>
      <url hash="47e884e0">2025.naacl-long.209</url>
      <bibkey>huang-wan-2025-waterpool</bibkey>
    </paper>
    <paper id="210">
      <title>Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack</title>
      <author><first>Cheng</first><last>Wang</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Yujun</first><last>Cai</last><affiliation>The University of Queensland</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4183-4194</pages>
      <abstract>Retrieval-augmented generation (RAG) systems enhance large language models by incorporating external knowledge, addressing issues like outdated internal knowledge and hallucination. However, their reliance on external knowledge bases makes them vulnerable to corpus poisoning attacks, where adversarial passages can be injected to manipulate retrieval results. Existing methods for crafting such passages, such as random token replacement or training inversion models, are often slow and computationally expensive, requiring either access to retriever’s gradients or large computational resources. To address these limitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an efficient black-box method that leverages two key properties of retrievers: insensitivity to token order and bias towards influential tokens. By focusing on these characteristics, DIGA dynamically adjusts its genetic operations to generate effective adversarial passages with significantly reduced time and memory usage. Our experimental evaluation shows that DIGA achieves superior efficiency and scalability compared to existing methods, while maintaining comparable or better attack success rates across multiple datasets.</abstract>
      <url hash="2dfab2ac">2025.naacl-long.210</url>
      <bibkey>wang-etal-2025-tricking</bibkey>
    </paper>
    <paper id="211">
      <title>The Good, The Bad, and The Greedy: Evaluation of <fixed-case>LLM</fixed-case>s Should Not Ignore Non-Determinism</title>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <pages>4195-4206</pages>
      <abstract>Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks’ consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation.</abstract>
      <url hash="596adcd2">2025.naacl-long.211</url>
      <bibkey>song-etal-2025-good</bibkey>
    </paper>
    <paper id="212">
      <title><fixed-case>CVE</fixed-case>-Bench: Benchmarking <fixed-case>LLM</fixed-case>-based Software Engineering Agent’s Ability to Repair Real-World <fixed-case>CVE</fixed-case> Vulnerabilities</title>
      <author><first>Peiran</first><last>Wang</last></author>
      <author><first>Xiaogeng</first><last>Liu</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>4207-4224</pages>
      <abstract>Automated vulnerability repair is a crucial field within software engineering and security research. Large Language Models (LLMs) and LLM agents have demonstrated significant potential in this domain by understanding descriptions in natural language and generating corresponding formal code. Although the coding capabilities of LLMs have advanced rapidly, evaluation benchmarks for real-world programming setups are still lagging, preventing the development of LLM and LLM agents in real-world vulnerability repair. To this end, we introduce CVE-Bench, an evaluation framework consisting of 509 Common Vulnerabilities and Exposures (CVEs) from four programming languages and 120 popular open-source repositories. Unlike previous vulnerability repair benchmarks, which only involve the code input and output, we provide LLM agents with a test environment that simulates the real-world vulnerability repair process. This environment provides multiple levels of CVE information modeling, such as black-box testing and white-box testing. It enables the agents to use static analysis tools to assist their repair process. Our evaluation reveals that the SWE-agent can only repair 21% of vulnerabilities at its best. Furthermore, they lack expert knowledge about how to use the analysis tool to assist in vulnerability repair.</abstract>
      <url hash="061b2d92">2025.naacl-long.212</url>
      <bibkey>wang-etal-2025-cve</bibkey>
    </paper>
    <paper id="213">
      <title><fixed-case>PROMPTEVALS</fixed-case>: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines</title>
      <author><first>Reya</first><last>Vir</last><affiliation>UC Berkeley, University of California, Berkeley</affiliation></author>
      <author><first>Shreya</first><last>Shankar</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Harrison</first><last>Chase</last><affiliation>LangChain</affiliation></author>
      <author><first>William</first><last>Hinthorn</last></author>
      <author><first>Aditya</first><last>Parameswaran</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>4225-4245</pages>
      <abstract>Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains—such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering.</abstract>
      <url hash="d498e030">2025.naacl-long.213</url>
      <bibkey>vir-etal-2025-promptevals</bibkey>
    </paper>
    <paper id="214">
      <title><fixed-case>T</fixed-case>ool<fixed-case>F</fixed-case>low: Boosting <fixed-case>LLM</fixed-case> Tool-Calling Through Natural and Coherent Dialogue Synthesis</title>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Weiwen</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>4246-4263</pages>
      <abstract>Supervised fine-tuning (SFT) is a common method to enhance the tool calling capabilities of Large Language Models (LLMs), with the training data often being synthesized. The current data synthesis process generally involves sampling a set of tools, formulating a requirement based on these tools, and generating the call statements. However, tools sampled randomly lack relevance, making them difficult to combine and thus reducing the diversity of the data. Additionally, current work overlooks the coherence between turns of dialogues, leading to a gap between the synthesized data and real-world scenarios. To address these issues, we propose a Graph-based Sampling strategy to sample more relevant tool combinations, and a Planned-generation strategy to create plans that guide the synthesis of coherent dialogues. We integrate these two strategies and enable multiple agents to synthesize the dialogue data interactively, resulting in our tool-calling data synthesis pipeline ToolFlow. Data quality assessments demonstrate improvements in the naturalness and coherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B using 8,000 synthetic dialogues generated with ToolFlow. Results show that the model achieves tool-calling performance comparable to or even surpassing GPT-4, while maintaining strong general capabilities.</abstract>
      <url hash="6d1ea147">2025.naacl-long.214</url>
      <bibkey>wang-etal-2025-toolflow</bibkey>
    </paper>
    <paper id="215">
      <title>Fighting Spurious Correlations in Text Classification via a Causal Learning Perspective</title>
      <author><first>Yuqing</first><last>Zhou</last><affiliation>George Mason University</affiliation></author>
      <author><first>Ziwei</first><last>Zhu</last><affiliation>George Mason University</affiliation></author>
      <pages>4264-4274</pages>
      <abstract>In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models’ reliance on spurious correlations and improve model robustness. Our approach integrates a causal feature selection method based on counterfactual reasoning, along with an unbiased inverse propensity weighting (IPW) loss function. By focusing on selecting causal features, we ensure that the model relies less on spurious features during prediction. We theoretically justify our approach and empirically show that CCR achieves state-of-the-art performance among methods without group labels, and in some cases, it can compete with the models that utilize group labels. Our code can be found at: https://github.com/yuqing-zhou/Causal-Learning-For-Robust-Classifier.</abstract>
      <url hash="b0a2bf26">2025.naacl-long.215</url>
      <bibkey>zhou-zhu-2025-fighting</bibkey>
    </paper>
    <paper id="216">
      <title>Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval</title>
      <author><first>Yu</first><last>Xia</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Haoliang</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>4275-4286</pages>
      <abstract>Large language models (LLMs) have been used to generate query expansions augmenting original queries for improving information search. Recent studies also explore providing LLMs with initial retrieval results to generate query expansions more grounded to document corpus. However, these methods mostly focus on enhancing textual similarities between search queries and target documents, overlooking document relations. For queries like “Find me a highly rated camera for wildlife photography compatible with my Nikon F-Mount lenses”, existing methods may generate expansions that are semantically similar but structurally unrelated to user intents. To handle such semi-structured queries with both textual and relational requirements, in this paper we propose a knowledge-aware query expansion framework, augmenting LLMs with structured document relations from knowledge graph (KG). To further address the limitation of entity-based scoring in existing KG-based methods, we leverage document texts as rich KG node representations and use document-based relation filtering for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three datasets of diverse domains show the advantages of our method compared against state-of-the-art baselines on textual and relational semi-structured retrieval.</abstract>
      <url hash="87b42eb4">2025.naacl-long.216</url>
      <bibkey>xia-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="217">
      <title><fixed-case>SVD</fixed-case>-<fixed-case>LLM</fixed-case> V2: Optimizing Singular Value Truncation for Large Language Model Compression</title>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Samiul</first><last>Alam</last></author>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Hui</first><last>Shen</last></author>
      <author><first>Mi</first><last>Zhang</last><affiliation>The Ohio State University</affiliation></author>
      <pages>4287-4296</pages>
      <abstract>Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) emerges as a promising method for compressing LLMs. However, existing SVD-based compression approaches suffer from substantial truncation losses, leading to severe performance degradation in compressed models. In this work, we introduce , a novel SVD-based LLM compression method that optimizes singular value truncation in SVD compression with two key strategies. First, employs dynamic compression ratio allocation to effectively balance the extremely large truncation loss across different layers. Second, it implements loss-optimized weight truncation to ensure that the truncated singular values result in a lower and more stable truncation loss in practice. We evaluate on ten datasets and five models on various scales and demonstrated that outperforms current state-of-the-art methods. The source code is available at <url>https://github.com/AIoT-MLSys-Lab/SVD-LLM</url>.</abstract>
      <url hash="a9fda3d4">2025.naacl-long.217</url>
      <bibkey>wang-etal-2025-svd-llm</bibkey>
    </paper>
    <paper id="218">
      <title><fixed-case>A</fixed-case>udio<fixed-case>B</fixed-case>ench: A Universal Benchmark for Audio Large Language Models</title>
      <author><first>Bin</first><last>Wang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Xunlong</first><last>Zou</last><affiliation>A*STAR</affiliation></author>
      <author><first>Geyu</first><last>Lin</last><affiliation>Institute of Infocomm Research, A*STAR</affiliation></author>
      <author><first>Shuo</first><last>Sun</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Zhuohan</first><last>Liu</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Wenyu</first><last>Zhang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>I2R</affiliation></author>
      <author><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <pages>4297-4316</pages>
      <abstract>We introduce AudioBench, a universal benchmark designed to evaluate Audio Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26 datasets, among which, 7 are newly proposed datasets. The evaluation targets three main aspects: speech understanding, audio scene understanding, and voice understanding (paralinguistic). Despite recent advancements, there lacks a comprehensive benchmark for AudioLLMs on instruction following capabilities conditioned on audio signals. AudioBench addresses this gap by setting up datasets as well as desired evaluation metrics. Besides, we also evaluated the capabilities of five popular models and found that no single model excels consistently across all tasks. We outline the research outlook for AudioLLMs and anticipate that our open-sourced evaluation toolkit, data, and leaderboard will offer a robust testbed for future model developments.</abstract>
      <url hash="6ccb7fae">2025.naacl-long.218</url>
      <bibkey>wang-etal-2025-audiobench</bibkey>
    </paper>
    <paper id="219">
      <title>Efficient Prompting for Continual Adaptation to Missing Modalities</title>
      <author><first>Zirun</first><last>Guo</last></author>
      <author><first>Shulei</first><last>Wang</last></author>
      <author><first>Wang</first><last>Lin</last></author>
      <author><first>Weicai</first><last>Yan</last></author>
      <author><first>Yangyang</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4317-4327</pages>
      <abstract>Missing modality issues are common in real-world applications, arising from factors such as equipment failures and privacy concerns. When fine-tuning pre-trained models on downstream datasets with missing modalities, performance can degrade significantly. Current methods often aggregate various missing cases to train recovery modules or align multimodal features, resulting in suboptimal performance, high computational costs, and the risk of catastrophic forgetting in continual environments where data arrives sequentially. In this paper, we formulate the dynamic missing modality problem as a continual learning task and introduce the continual multimodal missing modality task. To address this challenge efficiently, we introduce three types of prompts: modality-specific, task-aware, and task-specific prompts. These prompts enable the model to learn intra-modality, inter-modality, intra-task, and inter-task features. Furthermore, we propose a contrastive task interaction strategy to explicitly learn prompts correlating different modalities. We conduct extensive experiments on three public datasets, where our method consistently outperforms state-of-the-art approaches.</abstract>
      <url hash="29cf6e45">2025.naacl-long.219</url>
      <bibkey>guo-etal-2025-efficient</bibkey>
    </paper>
    <paper id="220">
      <title>Benchmarking and Building Zero-Shot <fixed-case>H</fixed-case>indi Retrieval Model with <fixed-case>H</fixed-case>indi-<fixed-case>BEIR</fixed-case> and <fixed-case>NLLB</fixed-case>-E5</title>
      <author><first>Arkadeep</first><last>Acharya</last></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <pages>4328-4348</pages>
      <abstract>Given the large number of Hindi speakers worldwide, there is a pressing need for robust and efficient information retrieval systems for Hindi. Despite ongoing research, comprehensive benchmarks for evaluating retrieval models in Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark, comprising 15 datasets across seven distinct tasks. We evaluate state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark, identifying task and domain-specific challenges that impact Hindi retrieval performance. Building on the insights from these results, we introduce NLLB-E5, a multilingual retrieval model that leverages a zero-shot approach to support Hindi without the need for Hindi training data. We believe our contributions, including the release of the Hindi-BEIR benchmark and the NLLB-E5 model, will be a valuable resource for researchers and promote advancements in multilingual retrieval models.</abstract>
      <url hash="f301f476">2025.naacl-long.220</url>
      <bibkey>acharya-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="221">
      <title>Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion</title>
      <author><first>Muzhi</first><last>Li</last></author>
      <author><first>Cehao</first><last>Yang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chengjin</first><last>Xu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Xuhui</first><last>Jiang</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Yiyan</first><last>Qi</last><affiliation>IDEA</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Ho-fung</first><last>Leung</last><affiliation>The Chinese University of Hong Kong and</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <pages>4349-4363</pages>
      <abstract>The Knowledge Graph Completion (KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that KGR3 consistently improves various KGC methods. Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of 12.3% and 5.6% on the FB15k237 and WN18RR datasets.</abstract>
      <url hash="75987c34">2025.naacl-long.221</url>
      <bibkey>li-etal-2025-retrieval</bibkey>
    </paper>
    <paper id="222">
      <title>See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias</title>
      <author><first>Junehyoung</first><last>Kwon</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>MiHyeon</first><last>Kim</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Eunju</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Juhwan</first><last>Choi</last><affiliation>AITRICS</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>4364-4378</pages>
      <abstract>Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to “dominant modality bias.” This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, **BalGrad** to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality’s contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that **BalGrad** effectively alleviates over-reliance on specific modalities when making predictions.</abstract>
      <url hash="785619f5">2025.naacl-long.222</url>
      <bibkey>kwon-etal-2025-see</bibkey>
    </paper>
    <paper id="223">
      <title>Harnessing and Evaluating the Intrinsic Extrapolation Ability of Large Language Models for Vehicle Trajectory Prediction</title>
      <author><first>Jiawei</first><last>Liu</last></author>
      <author><first>Yanjiao</first><last>Liu</last></author>
      <author><first>Xun</first><last>Gong</last></author>
      <author><first>Tingting</first><last>Wang</last><affiliation>Jilin University</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yunfeng</first><last>Hu</last><affiliation>Jilin University</affiliation></author>
      <pages>4379-4391</pages>
      <abstract>Emergent abilities of large language models (LLMs) have significantly advanced their application in autonomous vehicle (AV) research. Safe integration of LLMs into vehicles, however, necessitates their thorough understanding of dynamic traffic environments. Towards this end, this study introduces a framework leveraging LLMs’ built-in extrapolation capabilities for vehicle trajectory prediction, thereby evaluating their comprehension of the evolution of traffic agents’ behaviors and interactions over time. The framework employs a traffic encoder to extract spatial-level scene features from agents’ observed trajectories to facilitate efficient scene representation. To focus on LLM’s innate capabilities, scene features are then converted into LLM-compatible tokens through a reprogramming adapter and finally decoded into predicted trajectories with a linear decoder. Experimental results quantitatively demonstrate the framework’s efficacy in enabling off-the-shelf, frozen LLMs to achieve competitive trajectory prediction performance, with qualitative analyses revealing their enhanced understanding of complex, multi-agent traffic scenarios.</abstract>
      <url hash="19e4bb23">2025.naacl-long.223</url>
      <bibkey>liu-etal-2025-harnessing</bibkey>
    </paper>
    <paper id="224">
      <title>Stronger Models are Not Always Stronger Teachers for Instruction Tuning</title>
      <author><first>Zhangchen</first><last>Xu</last></author>
      <author><first>Fengqing</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luyao</first><last>Niu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>4392-4405</pages>
      <abstract>Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions and engage with users meaningfully. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt larger models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models’ Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.</abstract>
      <url hash="5eb1b8e0">2025.naacl-long.224</url>
      <bibkey>xu-etal-2025-stronger</bibkey>
    </paper>
    <paper id="225">
      <title>Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product</title>
      <author><first>Pengxiang</first><last>Lan</last></author>
      <author><first>Haoyu</first><last>Xu</last></author>
      <author><first>Enneng</first><last>Yang</last></author>
      <author><first>Yuliang</first><last>Liang</last></author>
      <author><first>Guibing</first><last>Guo</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jianzhe</first><last>Zhao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xingwei</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <pages>4406-4421</pages>
      <abstract>Prompt tuning (PT) offers a cost-effective alternative to fine-tuning large-scale pre-trained language models (PLMs), requiring only a few parameters in soft prompt tokens added before the input text. However, existing PT approaches face two significant issues: i They overlook intrinsic semantic associations between soft prompt tokens, leading to high discreteness and limited interactions, thus reducing the model’s comprehension and effectiveness in complex tasks. ii Due to the complexity of downstream tasks, long soft prompt is necessitated to improve performance, but prompt length correlates positively with memory usage and computational costs. Achieving high efficiency and performance remains an ongoing challenge. To address these issues, we propose a novel Low-parameters Prompt Tuning (LAMP) method, which leverages prompt decomposition and compressed outer product. Specifically, the prompt decomposition module employs Truncated SVD to reduce training parameters and significantly lower the dimensionality of the soft prompt parameter space. It then utilizes a compressed outer product module to facilitate multiple interactions among prompt tokens, exploring their intrinsic associations to enhance knowledge representation. Finally, LAMP uses average pooling to reduce memory usage and training/inference time. Extensive experiments across six architectures and eight datasets demonstrate that LAMP outperforms state-of-the-art PT-based and LoRA-based methods in performance and efficiency.</abstract>
      <url hash="f37d6d00">2025.naacl-long.225</url>
      <bibkey>lan-etal-2025-efficient</bibkey>
    </paper>
    <paper id="226">
      <title>Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs</title>
      <author><first>Jiancheng</first><last>Dong</last></author>
      <author><first>Lei</first><last>Jiang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Wei</first><last>Jin</last><affiliation>Emory University</affiliation></author>
      <author><first>Lu</first><last>Cheng</last></author>
      <pages>4422-4435</pages>
      <abstract>Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves concatenating data points of varying lengths until reaching the designed maximum length to facilitate GPU processing. However, randomly concatenating data points can lead to cross-contamination of sequences due to the significant difference in their subject matter. The mainstream approaches in SFT ensure that each token in the attention calculation phase only focuses on tokens within its own short sequence, without providing additional learning signals for the preceding context. To address these challenges, we introduce Threshold Filtering Packing (TFP), a method that selects samples with related context while maintaining sufficient diversity within the same pack. Our experiments show that TFP offers a simple-to-implement and scalable approach that significantly enhances SFT performance, with observed improvements of up to 7% on GSM8K, 4% on HumanEval. Furthermore, results from bias benchmark datasets highlight TFP’s promising performance in improving fairness while also boosting prediction accuracy by 15%.</abstract>
      <url hash="64827132">2025.naacl-long.226</url>
      <bibkey>dong-etal-2025-threshold</bibkey>
    </paper>
    <paper id="227">
      <title>Transferable Post-training via Inverse Value Learning</title>
      <author><first>Xinyu</first><last>Lu</last></author>
      <author><first>Xueru</first><last>Wen</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>4436-4447</pages>
      <abstract>As post-training processes utilize increasingly large datasets and base models continue to grow in size, the computational demands and implementation challenges of existing algorithms are escalating significantly. In this paper, we propose modeling the changes at the logits level during post-training using a separate neural network (i.e., the value network). After training this network on a small base model using demonstrations, this network can be seamlessly integrated with another pre-trained models during inference, enabling them to achieve similar capability enhancements. We systematically investigate the best practices for this paradigm in terms of pre-training weights and connection schemes. We demonstrate that the resulting value network has broad transferability across pre-trained models of different parameter sizes within the same family, models undergoing continuous pre-training within the same family, and models with different vocabularies across families. In certain cases, it can achieve performance comparable to full-parameter fine-tuning. Furthermore, we explore training methods to enhance transferability, which effectively improve the transfer performance of the value model across models of various parameter scales and prevent overfitting to the base model used during training.</abstract>
      <url hash="59c10205">2025.naacl-long.227</url>
      <bibkey>lu-etal-2025-transferable</bibkey>
    </paper>
    <paper id="228">
      <title><fixed-case>FLEX</fixed-case>: Expert-level False-Less <fixed-case>EX</fixed-case>ecution Metric for Text-to-<fixed-case>SQL</fixed-case> Benchmark</title>
      <author><first>Heegyu</first><last>Kim</last><affiliation>Ajou University</affiliation></author>
      <author><first>Jeon</first><last>Taeyang</last><affiliation>Ajou University</affiliation></author>
      <author><first>SeungHwan</first><last>Choi</last></author>
      <author><first>Seungtaek</first><last>Choi</last><affiliation>Yanolja</affiliation></author>
      <author><first>Hyunsouk</first><last>Cho</last><affiliation>Ajou University</affiliation></author>
      <pages>4448-4475</pages>
      <abstract>Text-to-SQL systems have become crucial for translating natural language into SQL queries in various industries, enabling non-technical users to perform complex data operations. The need for accurate evaluation methods has increased as these systems have grown more sophisticated. However, the Execution Accuracy (EX), the most prevalent evaluation metric, still shows many false positives and negatives. Thus, this paper introduces **FLEX(False-Less EXecution)**, a novel approach to evaluating text-to-SQL systems using large language models (LLMs) to emulate human expert-level evaluation of SQL queries. Our metric improves agreement with human experts (from 62 to 87.04 in Cohen’s kappa) with comprehensive context and sophisticated criteria. Our extensive experiments yield several key insights: (1) Models’ performance increases by over 2.6 points on average, substantially affecting rankings on Spider and BIRD benchmarks; (2) The underestimation of models in EX primarily stems from annotation quality issues; and (3) Model performance on particularly challenging questions tends to be overestimated. This work contributes to a more accurate and nuanced evaluation of text-to-SQL systems, potentially reshaping our understanding of state-of-the-art performance in this field.</abstract>
      <url hash="3dfb40d2">2025.naacl-long.228</url>
      <bibkey>kim-etal-2025-flex</bibkey>
    </paper>
    <paper id="229">
      <title><fixed-case>AID</fixed-case>: Adaptive Integration of Detectors for Safe <fixed-case>AI</fixed-case> with Language Models</title>
      <author><first>Xinran</first><last>Wang</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Enmao</first><last>Diao</last><affiliation>ColAI</affiliation></author>
      <author><first>Qi</first><last>Le</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Jie</first><last>Ding</last></author>
      <author><first>Ali</first><last>Anwar</last><affiliation>University of Minnesota</affiliation></author>
      <pages>4476-4492</pages>
      <abstract>As Large Language Models (LLMs) increasingly influence content generation across diverse platforms, there is a heightened urgency to regulate their outputs to ensure safe usage. However, defining safety is complex, given that entities across domains may interpret it through varied lenses and develop safety detectors—models trained to identify specific unsafe content based on predefined criteria. To address this complexity, we introduce the approach of Adaptive Integration of Detectors (AID) to orchestrate the strengths of multiple pretrained detectors to ensure comprehensive effectiveness in diverse scenarios. AID employs a Mixture-of-Experts (MoE) framework, wherein it dynamically assigns and learns data-adaptive weights for each detector using domain-specific annotated data and LLM-extracted features. We provide theoretical insights into why MoE can be effective by showing its optimality in a Neyman-Pearson setting. Our experimental studies using various detection tasks curated from benchmark datasets demonstrate AID’s ability to synergistically combine the unique capabilities of individual detectors. For example, it is observed that AID can improve the area under the curve (AUC) by an absolute value of 0.07 to 0.21, with a median of 0.12, compared to the best individual detectors developed for specific safety aspects. The improvement is particularly significant for complex detection tasks that mix different unsafe data sources.</abstract>
      <url hash="c279182a">2025.naacl-long.229</url>
      <bibkey>wang-etal-2025-aid</bibkey>
    </paper>
    <paper id="230">
      <title><fixed-case>SSML</fixed-case>o<fixed-case>RA</fixed-case>: Enhancing Low-Rank Adaptation with State Space Model</title>
      <author><first>Jiayang</first><last>Yu</last></author>
      <author><first>Yihang</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>YongKang</first><last>Liu</last><affiliation>Northeast University at Qinhuangdao Campus</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>4493-4506</pages>
      <abstract>Fine-tuning is a key approach for adapting language models to specific downstream tasks, but updating all model parameters becomes impractical as model sizes increase.Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), address this challenge by introducing additional adaptation parameters into pre-trained weight matrices.However, LoRA’s performance varies across different insertion points within the model, highlighting potential parameter inefficiency due to unnecessary insertions. To this end, we propose SSMLoRA (**S**tate **S**pace **M**odel **L**ow-**R**ank **A**daptation), an extension of LoRA that incorporates a State Space Model (SSM) to interconnect low-rank matrices. SSMLoRA ensures that performance is maintained even with sparser insertions. SSMLoRA allows the model to not only map inputs to a low-rank space for better feature extraction but also leverage the computations from the previous low-rank space. Our method achieves comparable performance to LoRA on the General Language Understanding Evaluation (GLUE) benchmark while using only half the parameters. Additionally, due to its structure, SSMLoRA shows promise in handling tasks with longer input sequences.</abstract>
      <url hash="f37a6db6">2025.naacl-long.230</url>
      <bibkey>yu-etal-2025-ssmlora</bibkey>
    </paper>
    <paper id="231">
      <title>Sharpness-Aware Minimization for Topic Models with High-Quality Document Representations</title>
      <author><first>Tung</first><last>Nguyen</last></author>
      <author><first>Tue</first><last>Le</last></author>
      <author><first>Hoang Tran</first><last>Vuong</last></author>
      <author><first>Quang Duc</first><last>Nguyen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Duc Anh</first><last>Nguyen</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>4507-4524</pages>
      <abstract>Recent advanced frameworks in topic models have significantly enhanced the performance compared to conventional probabilistic approaches. Such models, mostly constructed from neural network architecture together with other advanced techniques such as contextual embedding, optimal transport distance and pre-trained language model, etc. have effectively improved the topic quality and document topic distribution. Despite the improvements, these methods lack considerations of effective optimization for complex objective functions that contain log-likelihood and additional regularization terms. In this study, we propose to apply an efficient optimization method to improve the generalization and performance of topic models. Our approach explicitly considers the sharpness of the loss landscape during optimization, which forces the optimizer to choose directions in the parameter space that lead to flatter minima, in which the models are typically more stable and robust to small perturbations in the data. Additionally, we propose an effective strategy to select the flatness region for parameter optimization by leveraging the optimal transport distance between doc-topic distributions and doc-cluster proportions, which can effectively enhance document representation. Experimental results on popular benchmark datasets demonstrate that our method effectively improves the performance of baseline topic models.</abstract>
      <url hash="3b387bf8">2025.naacl-long.231</url>
      <bibkey>nguyen-etal-2025-sharpness</bibkey>
    </paper>
    <paper id="232">
      <title><tex-math>C^2</tex-math>: Scalable Auto-Feedback for <fixed-case>LLM</fixed-case>-based Chart Generation</title>
      <author><first>Woosung</first><last>Koh</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Yonsei University</affiliation></author>
      <author><first>Janghan</first><last>Yoon</last></author>
      <author><first>MinHyung</first><last>Lee</last></author>
      <author><first>Youngjin</first><last>Song</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jaegwan</first><last>Cho</last></author>
      <author><first>Jaehyun</first><last>Kang</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Taehyeon</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Se-Young</first><last>Yun</last><affiliation>KAIST</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Bongshin</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <pages>4525-4566</pages>
      <url hash="c0aa9c0f">2025.naacl-long.232</url>
      <bibkey>koh-etal-2025-c2</bibkey>
    </paper>
    <paper id="233">
      <title>A Top-down Graph-based Tool for Modeling Classical Semantic Maps: A Case Study of Supplementary Adverbs</title>
      <author><first>Zhu</first><last>Liu</last></author>
      <author><first>Cunliang</first><last>Kong</last></author>
      <author><first>Ying</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>4567-4576</pages>
      <abstract>Semantic map models (SMMs) construct a network-like conceptual space from cross-linguistic instances or forms, based on the connectivity hypothesis. This approach has been widely used to represent similarity and entailment relationships in cross-linguistic concept comparisons. However, most SMMs are manually built by human experts using bottom-up procedures, which are often labor-intensive and time-consuming. In this paper, we propose a novel graph-based algorithm that automatically generates conceptual spaces and SMMs in a top-down manner. The algorithm begins by creating a dense graph, which is subsequently pruned into minimal spanning trees, selected according to metrics we propose. These evaluation metrics include both intrinsic and extrinsic measures, considering factors such as network structure and the trade-off between precision and coverage. A case study on cross-linguistic supplementary adverbs demonstrates the effectiveness and efficiency of our model compared to human annotations and other automated methods. The tool is available at https://github.com/RyanLiut/SemanticMapModel.</abstract>
      <url hash="be56a5cb">2025.naacl-long.233</url>
      <bibkey>liu-etal-2025-top</bibkey>
    </paper>
    <paper id="234">
      <title><fixed-case>U</fixed-case>ni<fixed-case>HGKR</fixed-case>: Unified Instruction-aware Heterogeneous Knowledge Retrievers</title>
      <author><first>Dehai</first><last>Min</last></author>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Chenyu</first><last>You</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>4577-4594</pages>
      <abstract>Existing information retrieval (IR) models often assume a homogeneous structure for knowledge sources and user queries, limiting their applicability in real-world settings where retrieval is inherently heterogeneous and diverse. In this paper, we introduce UniHGKR, a unified instruction-aware heterogeneous knowledge retriever that (1) builds a unified retrieval space for heterogeneous knowledge and (2) follows diverse user instructions to retrieve knowledge in specified types. UniHGKR consists of three principal stages, including heterogeneous self-supervised pretraining, text-anchored embedding alignment, and instruction-aware retriever fine-tuning, enabling it to generalize across varied retrieval contexts. This framework is highly scalable, with a BERT-based version and a UniHGKR-7B version trained on large language models. Also, we introduce CompMix-IR, the first native heterogeneous knowledge retrieval benchmark. It includes two retrieval scenarios with various instructions, over 9,400 question answer (QA) pairs, and a corpus of 10 million entries, covering four different types of data. Extensive experiments show that UniHGKR consistently outperform state-of-the-art methods on CompMix-IR, achieving up to 6.36% and 54.23% relative improvements in two scenarios, respectively. Finally, by equipping our retriever for open-domain heterogeneous QA systems, we achieve a new state-of-the-art result on the popular ConvMix task, with an absolute improvement of up to 5.90 points.</abstract>
      <url hash="8398d7d9">2025.naacl-long.234</url>
      <bibkey>min-etal-2025-unihgkr</bibkey>
    </paper>
    <paper id="235">
      <title>Improving Model Evaluation using <fixed-case>SMART</fixed-case> Filtering of Benchmark Datasets</title>
      <author><first>Vipul</first><last>Gupta</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Candace</first><last>Ross</last><affiliation>Meta</affiliation></author>
      <author><first>David</first><last>Pantoja</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Megan</first><last>Ung</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Adina</first><last>Williams</last><affiliation>FAIR (Meta Platforms Inc.)</affiliation></author>
      <pages>4595-4615</pages>
      <abstract>One of the most challenging problems facing NLP today is evaluation. Some of the most pressing issues pertain to benchmark saturation, data contamination, and diversity in the quality of test examples. To address these concerns, we propose Selection Methodology for Accurate, Reduced, and Targeted (SMART) filtering, a novel approach to select a high-quality subset of examples from existing benchmark datasets by systematically removing less informative and lower quality examples. Our approach applies three filtering criteria, removing (i) easy examples, (ii) data-contaminated examples, and (iii) examples that are similar to each other based on distance in an embedding space. We demonstrate the effectiveness of SMART Filtering on three multiple choice QA datasets, where our methodology increases efficiency by reducing dataset size by 48% on average, while increasing Pearson correlation with rankings from ChatBot Arena, a more open-ended human evaluation setting. Our method enables us to be more efficient, whether we are using SMART Filtering to make new benchmarks more challenging, or to revitalize older, human generated datasets, while still preserving the relative model rankings.</abstract>
      <url hash="e3058946">2025.naacl-long.235</url>
      <bibkey>gupta-etal-2025-improving</bibkey>
    </paper>
    <paper id="236">
      <title>Entropy-Based Decoding for Retrieval-Augmented Large Language Models</title>
      <author><first>Zexuan</first><last>Qiu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zijing</first><last>Ou</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Bin</first><last>Wu</last></author>
      <author><first>Jingjing</first><last>Li</last></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <pages>4616-4627</pages>
      <abstract>Augmenting Large Language Models (LLMs) with retrieved external knowledge has proven effective in improving the factual accuracy of generated responses. Despite their success, retrieval-augmented LLMs still face the distractibility issue, where the generated responses are negatively influenced by noise from both external and internal knowledge sources. In this paper, we introduce a novel, training-free decoding method guided by entropy considerations to mitigate this issue. Our approach utilizes entropy-based document-parallel ensemble decoding to prioritize low-entropy distributions from retrieved documents, thereby enhancing the extraction of relevant information of context. Additionally, it incorporates a contrastive decoding mechanism that contrasts the obtained low-entropy ensemble distribution with the high-entropy distribution derived from the model’s internal knowledge across layers, which ensures a greater emphasis on reliable external information. Extensive experiments on open-domain question answering datasets demonstrate the superiority of our method.</abstract>
      <url hash="0c7c963b">2025.naacl-long.236</url>
      <bibkey>qiu-etal-2025-entropy</bibkey>
    </paper>
    <paper id="237">
      <title>What We Talk About When We Talk About <fixed-case>LM</fixed-case>s: Implicit Paradigm Shifts and the Ship of Language Models</title>
      <author><first>Shengqi</first><last>Zhu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Jeffrey</first><last>Rzeszotarski</last><affiliation>Cornell University</affiliation></author>
      <pages>4628-4646</pages>
      <abstract>The term Language Models (LMs) as a time-specific collection of models of interest is constantly reinvented, with its referents updated much like the *Ship of Theseus* replaces its parts but remains the same ship in essence. In this paper, we investigate this *Ship of Language Models* problem, wherein scientific evolution takes the form of continuous, implicit retrofits of key *existing* terms. We seek to initiate a novel perspective of scientific progress, in addition to the more well-studied emergence of *new* terms. To this end, we construct the data infrastructure based on recent NLP publications. Then, we perform a series of text-based analyses toward a detailed, quantitative understanding of the use of Language Models as a term of art. Our work highlights how systems and theories influence each other in scientific discourse, and we call for attention to the transformation of this Ship that we all are contributing to.</abstract>
      <url hash="5c6cf8a3">2025.naacl-long.237</url>
      <bibkey>zhu-rzeszotarski-2025-talk</bibkey>
    </paper>
    <paper id="238">
      <title>Diversity Helps Jailbreak Large Language Models</title>
      <author><first>Weiliang</first><last>Zhao</last></author>
      <author><first>Daniel</first><last>Ben-Levi</last></author>
      <author><first>Wei</first><last>Hao</last></author>
      <author><first>Junfeng</first><last>Yang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chengzhi</first><last>Mao</last><affiliation>Google</affiliation></author>
      <pages>4647-4680</pages>
      <abstract>We have uncovered a powerful jailbreak technique that leverages large language models’ ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.</abstract>
      <url hash="c69bd006">2025.naacl-long.238</url>
      <bibkey>zhao-etal-2025-diversity</bibkey>
    </paper>
    <paper id="239">
      <title>Constrained Decoding with Speculative Lookaheads</title>
      <author><first>Nishanth Sridhar</first><last>Nakshatri</last><affiliation>Purdue University</affiliation></author>
      <author><first>Shamik</first><last>Roy</last><affiliation>Amazon</affiliation></author>
      <author><first>Rajarshi</first><last>Das</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Suthee</first><last>Chaidaroon</last><affiliation>Amazon</affiliation></author>
      <author><first>Leonid</first><last>Boytsov</last><affiliation>Amazon</affiliation></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last><affiliation>Amazon</affiliation></author>
      <pages>4681-4700</pages>
      <abstract>Constrained decoding with lookahead heuristics (CDLH) is a highly effective method for aligning LLM generations to human preferences. However, the extensive lookahead roll-out operations for each generated token makes CDLH prohibitively expensive, resulting in low adoption in practice. In contrast, common decoding strategies such as greedy decoding are extremely efficient, but achieve very low constraint satisfaction. We propose constrained decoding with speculative lookaheads (CDSL), a technique that significantly improves upon the inference efficiency of CDLH without experiencing the drastic performance reduction seen with greedy decoding. CDSL is motivated by the recently proposed idea of speculative decoding that uses a much smaller draft LLM for generation and a larger target LLM for verification. In CDSL, the draft model is used to generate lookaheads which is verified by a combination of target LLM and task-specific reward functions. This process accelerates decoding by reducing the computational burden while maintaining strong performance. We evaluate CDSL in two constraint decoding tasks with three LLM families and achieve 2.2x to 12.15x speedup over CDLH without significant performance reduction.</abstract>
      <url hash="305a87d7">2025.naacl-long.239</url>
      <bibkey>nakshatri-etal-2025-constrained</bibkey>
    </paper>
    <paper id="240">
      <title><fixed-case>D</fixed-case>y<fixed-case>PCL</fixed-case>: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition</title>
      <author><first>Wonjun</first><last>Lee</last></author>
      <author><first>Solee</first><last>Im</last></author>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>4701-4712</pages>
      <abstract>Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification alignment. Unlike prior studies focusing on utterance-level embeddings, our granular learning allows discrimination of subtle parts of speech. In addition, we introduce dynamic curriculum learning, which progressively transitions from easy negative samples to difficult-to-distinguishable negative samples based on phonetic similarity of phoneme. Our approach to training by difficulty levels alleviates the inherent variability of speakers, better identifying challenging speeches. Evaluated on the UASpeech dataset, DyPCL outperforms baseline models, achieving an average 22.10% relative reduction in word error rate (WER) across the overall dysarthria group.</abstract>
      <url hash="7623043a">2025.naacl-long.240</url>
      <bibkey>lee-etal-2025-dypcl</bibkey>
    </paper>
    <paper id="241">
      <title>Revisiting Early Detection of Sexual Predators via Turn-level Optimization</title>
      <author><first>JinMyeong</first><last>An</last></author>
      <author><first>Sangwon</first><last>Ryu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>4713-4724</pages>
      <abstract>Online grooming is a severe social threat where sexual predators gradually entrap child victims with subtle and gradual manipulation. Therefore, timely intervention for online grooming is critical for proactive protection. However, previous methods fail to determine the optimal intervention points (i.e., jump to conclusions) as they rely on chat-level risk labels by causing weak supervision of risky utterances. For timely detection, we propose speed control reinforcement learning (SCoRL), incorporating a practical strategy derived from luring communication theory (LCT). To capture the predator’s turn-level entrapment, we use a turn-level risk label based on the LCT. Then, we design a novel speed control reward function that balances the trade-off between speed and accuracy based on turn-level risk label; thus, SCoRL can identify the optimal intervention moment. In addition, we introduce a turn-level metric for precise evaluation, identifying limitations in previously used chat-level metrics. Experimental results show that SCoRL effectively preempted online grooming, offering a more proactive and timely solution. Further analysis reveals that our method enhances performance while intuitively identifying optimal early intervention points.</abstract>
      <url hash="198cc74b">2025.naacl-long.241</url>
      <bibkey>an-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="242">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>TTS</fixed-case>-<fixed-case>ZS</fixed-case>: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion</title>
      <author><first>Yinghao Aaron</first><last>Li</last></author>
      <author><first>Xilin</first><last>Jiang</last></author>
      <author><first>Cong</first><last>Han</last><affiliation>Columbia University</affiliation></author>
      <author><first>Nima</first><last>Mesgarani</last><affiliation>Columbia University</affiliation></author>
      <pages>4725-4744</pages>
      <abstract>The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to capture diverse prosodic variations, trained adversarially with multi-modal discriminators. A diffusion model is then built to sample this time-varying style code for efficient latent diffusion. Using classifier-free guidance, StyleTTS-ZS achieves high similarity to the reference speaker in the style diffusion process. Furthermore, to expedite sampling, the style diffusion model is distilled with perceptual loss using only 10k samples, maintaining speech quality and similarity while reducing inference speed by 90%. Our model surpasses previous state-of-the-art large-scale zero-shot TTS models in both naturalness and similarity, offering a 10-20× faster sampling speed, making it an attractive alternative for efficient large-scale zero-shot TTS systems. The audio demo, code and models are available at https://styletts-zs.github.io/.</abstract>
      <url hash="9c443891">2025.naacl-long.242</url>
      <bibkey>li-etal-2025-styletts</bibkey>
    </paper>
    <paper id="243">
      <title>Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation</title>
      <author><first>Satyapriya</first><last>Krishna</last></author>
      <author><first>Kalpesh</first><last>Krishna</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Anhad</first><last>Mohananey</last><affiliation>Google</affiliation></author>
      <author><first>Steven</first><last>Schwarcz</last><affiliation>University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park</affiliation></author>
      <author><first>Adam</first><last>Stambler</last><affiliation>Google</affiliation></author>
      <author><first>Shyam</first><last>Upadhyay</last><affiliation>Google</affiliation></author>
      <author><first>Manaal</first><last>Faruqui</last><affiliation>Google</affiliation></author>
      <pages>4745-4759</pages>
      <abstract>Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs’ ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (&gt;50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.</abstract>
      <url hash="b69c0ed6">2025.naacl-long.243</url>
      <bibkey>krishna-etal-2025-fact</bibkey>
    </paper>
    <paper id="244">
      <title><fixed-case>R</fixed-case>each<fixed-case>A</fixed-case>gent: Enhancing Mobile Agent via Page Reaching and Operation</title>
      <author><first>Qinzhuo</first><last>Wu</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <pages>4760-4775</pages>
      <abstract>Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the Intersection over Union (IoU) Accuracy and Text Accuracy by 7.12% and 7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the SOTA agent. Our data and code will be released upon acceptance.</abstract>
      <url hash="3e38e990">2025.naacl-long.244</url>
      <bibkey>wu-etal-2025-reachagent</bibkey>
    </paper>
    <paper id="245">
      <title>Learning to Solve Domain-Specific Calculation Problems with Knowledge-Intensive Programs Generator</title>
      <author><first>Chengyuan</first><last>Liu</last></author>
      <author><first>Shihang</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Lizhi</first><last>Qing</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jun</first><last>Lin</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4776-4791</pages>
      <abstract>Domain Large Language Models (LLMs) are developed for domain-specific tasks based on general LLMs. But it still requires professional knowledge to facilitate the expertise for some domain-specific tasks. In this paper, we investigate into knowledge-intensive calculation problems. We find that the math problems to be challenging for LLMs, when involving complex domain-specific rules and knowledge documents, rather than simple formulations of terminologies. Therefore, we propose a pipeline to solve the domain-specific calculation problems with Knowledge-Intensive Programs Generator more effectively, named as KIPG. It generates knowledge-intensive programs according to the domain-specific documents. For each query, key variables are extracted, then outcomes which are dependent on domain knowledge are calculated with the programs. By iterative preference alignment, the code generator learns to improve the logic consistency with the domain knowledge. Taking legal domain as an example, we have conducted experiments to prove the effectiveness of our pipeline, and extensive analysis on the modules. We also find that the code generator is also adaptable to other domains, without training on the new knowledge.</abstract>
      <url hash="f02a4331">2025.naacl-long.245</url>
      <bibkey>liu-etal-2025-learning-solve</bibkey>
    </paper>
    <paper id="246">
      <title><fixed-case>SLIM</fixed-case>: Let <fixed-case>LLM</fixed-case> Learn More and Forget Less with Soft <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> and Identity Mixture</title>
      <author><first>Jiayi</first><last>Han</last><affiliation>Inspur Group Co, Ltd</affiliation></author>
      <author><first>Liang</first><last>Du</last><affiliation>Tencent</affiliation></author>
      <author><first>Hongwei</first><last>Du</last></author>
      <author><first>Xiangguo</first><last>Zhou</last></author>
      <author><first>Yiwen</first><last>Wu</last></author>
      <author><first>Yuanfang</first><last>Zhang</last></author>
      <author><first>Weibo</first><last>Zheng</last><affiliation>Inspur Genersoft Co., Ltd.</affiliation></author>
      <author><first>Donghong</first><last>Han</last><affiliation>Northeastern University</affiliation></author>
      <pages>4792-4804</pages>
      <abstract>Despite the recent efforts from the NLP community, balancing the training budget, downstream performance, and general capabilities of large language models (LLM) remains a challenge in many applications. Training the entire model for downstream tasks is expensive, and could easily result in catastrophic forgetting. Parameter-efficient fine-tuning (PEFT) could reduce the training cost, but it still suffers from forgetting, and limits the learning on the downstream tasks. To address the aforementioned issues, we propose a novel mixture of expert (MoE) framework based on Soft LoRA and Identity Mixture (SLIM). SLIM allows dynamic routing between LoRA adapters and identity layers, thus enabling the bypass of LoRA adapters to suppress forgetting of general capacity. We adopt weight yielding with sliding clustering for better out-of-domain distinguish to enhance the routing. We also convert the mixture of LoRA adapters to the model merging formulation and introduce dynamic merging with its fast implementation for LoRA adapters to keep the general capabilities. Extensive experiments demonstrate that the proposed SLIM is comparable to the state-of-the-art PEFT approaches on the downstream tasks while achieving the leading performance in mitigating catastrophic forgetting. We plan to open-source the code upon publication.</abstract>
      <url hash="40ee3394">2025.naacl-long.246</url>
      <bibkey>han-etal-2025-slim</bibkey>
    </paper>
    <paper id="247">
      <title><fixed-case>MME</fixed-case>val<fixed-case>P</fixed-case>ro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation</title>
      <author><first>Jinsheng</first><last>Huang</last><affiliation>Peking University</affiliation></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Taian</first><last>Guo</last></author>
      <author><first>Fu</first><last>Zeng</last></author>
      <author><first>Yusheng</first><last>Zhao</last></author>
      <author><first>Bohan</first><last>Wu</last></author>
      <author><first>Ye</first><last>Yuan</last></author>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Zhihui</first><last>Guo</last></author>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Jingyang</first><last>Yuan</last></author>
      <author><first>Wei</first><last>Ju</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Luchen</first><last>Liu</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>4805-4822</pages>
      <abstract>Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding and reasoning abilities, often assessed through multiple-choice questions (MCQs) that include an image, a question, and several options. However, many benchmarks used for such evaluations suffer from systematic biases. Remarkably, Large Language Models (LLMs) without any visual perception capabilities achieve non-trivial performance, undermining the credibility of these evaluations. To address this issue while maintaining the efficiency of MCQ evaluations, we propose MMEVALPRO, a benchmark designed to avoid Type-I errors through a trilogy evaluation pipeline and more rigorous metrics. For each original question from existing benchmarks, human annotators augment it by creating one perception question and one knowledge anchor question through a meticulous annotation process. MMEVALPRO comprises 2,138 question triplets, totaling 6,414 distinct questions. Two-thirds of these questions are manually labeled by human experts, while the rest are sourced from existing benchmarks (MMMU, ScienceQA, and MathVista). Compared with the existing benchmarks, our experiments with the latest LLMs and LMMs demonstrate that MMEVALPRO is **more challenging** (the best LMM lags behind human performance by 31.73%, compared to an average gap of 8.03% in previous benchmarks) and **more trustworthy** (the best LLM trails the best LMM by 23.09%, whereas the gap for previous benchmarks is just 14.64%). Our in-depth analysis explains the reason for the large performance gap and justifies the trustworthiness of evaluation, underscoring its significant potential for advancing future research.</abstract>
      <url hash="5002afd4">2025.naacl-long.247</url>
      <bibkey>huang-etal-2025-mmevalpro</bibkey>
    </paper>
    <paper id="248">
      <title><fixed-case>M</fixed-case>i<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Harnessing Minor Singular Components for Parameter-Efficient <fixed-case>LLM</fixed-case> Finetuning</title>
      <author><first>Hanqing</first><last>Wang</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <pages>4823-4836</pages>
      <abstract>Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computational and memory costs. Previous LoRA-based approaches initialize the low-rank matrices with Gaussian distribution and zero values while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might interfere with the well-learned subspace of the pretrained weight matrices. In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principal singular components frozen. It is observed that the minor matrix corresponds to the noisy or long-tail information, while the principal matrix contains important knowledge. The MiLoRA initializes the low-rank matrices within a subspace that is orthogonal to the principal matrix, thus the pretrained knowledge is expected to be well preserved. During finetuning, MiLoRA makes the most use of the less-optimized subspace for learning the labeled dataset. Extensive experiments on commonsense reasoning, math reasoning, instruction following and visual instruction following benchmarks present the superior performance of our method.</abstract>
      <url hash="78bfdcdb">2025.naacl-long.248</url>
      <bibkey>wang-etal-2025-milora</bibkey>
    </paper>
    <paper id="249">
      <title>Analyzing (In)Abilities of <fixed-case>SAE</fixed-case>s via Formal Languages</title>
      <author><first>Abhinav</first><last>Menon</last></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <author><first>David</first><last>Krueger</last></author>
      <author><first>Ekdeep Singh</first><last>Lubana</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <pages>4837-4862</pages>
      <abstract>Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of models trained on formal languages (Dyck-2, Expr, and English PCFG) under a wide variety of hyperparameter settings, finding interpretable latents often emerge in the features learned by our SAEs. However, similar to vision, we find performance turns out to be highly sensitive to inductive biases of the training pipeline. Moreover, we show latents correlating to certain features of the input do not always induce a causal impact on model’s computation. We thus argue that causality has to become a central target in SAE training: learning of causal features should be incentivized from the ground-up. Motivated by this, we propose and perform preliminary investigations for an approach that promotes learning of causally relevant features in our formal language setting.</abstract>
      <url hash="d3c9d7f3">2025.naacl-long.249</url>
      <bibkey>menon-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="250">
      <title>Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning</title>
      <author><first>Subin</first><last>Kim</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Hoonrae</first><last>Kim</last></author>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>4863-4880</pages>
      <abstract>Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client’s facial expressions.To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs’ performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.</abstract>
      <url hash="842d0f5f">2025.naacl-long.250</url>
      <bibkey>kim-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="251">
      <title>Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction</title>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Guangyue</first><last>Peng</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>4881-4897</pages>
      <abstract>Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. Our code and the constructed database will be publicly available after the paper is published.</abstract>
      <url hash="a82127cf">2025.naacl-long.251</url>
      <bibkey>li-etal-2025-explanation</bibkey>
    </paper>
    <paper id="252">
      <title>A Unified Supervised and Unsupervised Dialogue Topic Segmentation Framework Based on Utterance Pair Modeling</title>
      <author><first>Shihao</first><last>Yang</last></author>
      <author><first>Ziyi</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Jiang</last></author>
      <author><first>Chunsheng</first><last>Qin</last></author>
      <author><first>Shuhua</first><last>Liu</last></author>
      <pages>4898-4908</pages>
      <abstract>The Dialogue Topic Segmentation task aims to divide a dialogue into different topic paragraphs in order to better understand the structure and content of the dialogue. Due to the short sentences, serious references and non-standard language in the dialogue, it is difficult to determine the boundaries of the topic. Although the unsupervised approaches based on LLMs performs well, it is still difficult to surpass the supervised methods based on classical models in specific domains. To this end, this paper proposes UPS (Utterance Pair Segment), a dialogue topic segmentation method based on utterance pair relationship modeling, unifying the supervised and unsupervised network architectures. For supervised pre-training, the model predicts the adjacency and topic affiliation of utterances in dialogues. For unsupervised pre-training, the dialogue-level and utterance-level relationship prediction tasks are used to train the model. The pre-training and fine-tuning strategies are carried out in different scenarios, such as supervised, few-shot, and unsupervised data. By adding a domain adapter and a task adapter to the Transformer, the model learns in the pre-training and fine-tuning stages, respectively, which significantly improves the segmentation effect. As the result, the proposed method has achieved the best results on multiple benchmark datasets across various scenarios.</abstract>
      <url hash="ff97739c">2025.naacl-long.252</url>
      <bibkey>yang-etal-2025-unified</bibkey>
    </paper>
    <paper id="253">
      <title>Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance</title>
      <author><first>Borui</first><last>Xu</last><affiliation>Shandong University</affiliation></author>
      <author><first>Yao</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zeyi</first><last>Wen</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Weiguo</first><last>Liu</last></author>
      <author><first>Bingsheng</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4909-4922</pages>
      <abstract>The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence, factual consistency, and summary length. Our findings reveal significant variations in SLM performance, with top-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results comparable to those of 70B LLMs while generating more concise summaries. Notably, SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in summary quality. Additionally, our analysis indicates that instruction tuning does not consistently enhance the news summarization capabilities of SLMs. This research not only contributes to the understanding of SLMs but also provides practical insights for researchers seeking efficient summarization solutions that balance performance and resource use.</abstract>
      <url hash="95a7aa6d">2025.naacl-long.253</url>
      <bibkey>xu-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="254">
      <title>Dynamic Fisher-weighted Model Merging via <fixed-case>B</fixed-case>ayesian Optimization</title>
      <author><first>Sanwoo</first><last>Lee</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiahao</first><last>Liu</last><affiliation>Meituan</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>4923-4935</pages>
      <abstract>The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic Fisher-weighted Merging (DF-Merge). Specifically, candidate models are associated with a set of coefficients that linearly scale their fine-tuned parameters. Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets. Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients. Experimental results show that DF-Merge outperforms strong baselines across models of different sizes and a variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises from the unified view of merging and that near-optimal performance is achievable in a few iterations, even with minimal validation data.</abstract>
      <url hash="713ef77a">2025.naacl-long.254</url>
      <bibkey>lee-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="255">
      <title><fixed-case>AI</fixed-case>-Assisted Human Evaluation of Machine Translation</title>
      <author><first>Vilém</first><last>Zouhar</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Cohere</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>4936-4950</pages>
      <url hash="8c40a768">2025.naacl-long.255</url>
      <bibkey>zouhar-etal-2025-ai</bibkey>
    </paper>
    <paper id="256">
      <title><fixed-case>MLLM</fixed-case>-Bench: Evaluating Multimodal <fixed-case>LLM</fixed-case>s with Per-sample Criteria</title>
      <author><first>Wentao</first><last>Ge</last></author>
      <author><first>Shunian</first><last>Chen</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Hardy</first><last>Chen</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Nuo</first><last>Chen</last><affiliation>National University of Singapore and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Wenya</first><last>Xie</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Shuo</first><last>Yan</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>ChenghaoZhu</first><last>ChenghaoZhu</last></author>
      <author><first>Ziyue</first><last>Lin</last></author>
      <author><first>Dingjie</first><last>Song</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhang</first><last>Zhiyi</last></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>4951-4974</pages>
      <abstract>Multimodal large language models (MLLMs) have broadened the scope of AI applications. Existing automatic evaluation methodologies for MLLMs are mainly limited in evaluating <i>objective</i> queries without considering <i>real-world</i> user experiences, inadequately addressing the nuances of creative and associative multimodal tasks. However, the <i>open-ended</i> and <i>subjective</i> nature of such tasks poses a significant challenge to the evaluation methodology, where it is difficult to define the ground-truth answers for them. To this end, in our paper, we propose a new evaluation paradigm for MLLMs, which is evaluating MLLMs with <i>per-sample criteria</i> using potent MLLM as the judge. To validate the feasibility and effectiveness of this paradigm, we design a benchmark, dubbed <b>MLLM-Bench</b>, by curating the evaluation samples across six comprehensive cognitive levels. We benchmark 26 popular MLLMs in a pairwise-comparison fashion, showing diverse performance across models. Moreover, the validity of our benchmark manifests itself in reaching 88.02% agreement with human evaluation. We contend that the proposed paradigm explores the potential of MLLMs as effective evaluation tools with the help of per-sample criteria.</abstract>
      <url hash="6b8756ef">2025.naacl-long.256</url>
      <bibkey>ge-etal-2025-mllm</bibkey>
    </paper>
    <paper id="257">
      <title><fixed-case>A</fixed-case>gent<fixed-case>S</fixed-case>ense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios</title>
      <author><first>Xinyi</first><last>Mou</last></author>
      <author><first>Jingcong</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiayu</first><last>Lin</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xinnong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiawei</first><last>Liu</last></author>
      <author><first>Shiyue</first><last>Yang</last></author>
      <author><first>Rong</first><last>Ye</last><affiliation>ByteDance</affiliation></author>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Haoyu</first><last>Kuang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>4975-5001</pages>
      <abstract>Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts. We evaluate LLM-driven agents through multi-turn interactions, emphasizing both goal completion and implicit reasoning. We analyze goals using ERG theory and conduct comprehensive experiments. Our findings highlight that LLMs struggle with goals in complex social scenarios, especially high-level growth needs, and even GPT-4o requires improvement in private information reasoning.</abstract>
      <url hash="2e28c7d1">2025.naacl-long.257</url>
      <bibkey>mou-etal-2025-agentsense</bibkey>
    </paper>
    <paper id="258">
      <title><fixed-case>F</fixed-case>act<fixed-case>CG</fixed-case>: Enhancing Fact Checkers with Graph-Based Multi-Hop Data</title>
      <author><first>Deren</first><last>Lei</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yaxi</first><last>Li</last></author>
      <author><first>Siyao</first><last>Li</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mengya</first><last>Hu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rui</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ken</first><last>Archer</last></author>
      <author><first>Mingyu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Emily</first><last>Ching</last></author>
      <author><first>Alex</first><last>Deng</last></author>
      <pages>5002-5020</pages>
      <abstract>Prior research on training grounded factuality classification models to detect hallucinations in large language models (LLMs) has relied on public natural language inference (NLI) data and synthetic data. However, conventional NLI datasets are not well-suited for document-level reasoning, which is critical for detecting LLM hallucinations. Recent approaches to document-level synthetic data generation involve iteratively removing sentences from documents and annotating factuality using LLM-based prompts. While effective, this method is computationally expensive for long documents and limited by the LLM’s capabilities. In this work, we analyze the differences between existing synthetic training data used in state-of-the-art models and real LLM output claims. Based on our findings, we propose a novel approach for synthetic data generation, CG2C, that leverages multi-hop reasoning on context graphs extracted from documents. Our fact checker model, FactCG, demonstrates improved performance with more connected reasoning, using the same backbone models. Experiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark with much smaller model size.</abstract>
      <url hash="fabc6307">2025.naacl-long.258</url>
      <bibkey>lei-etal-2025-factcg</bibkey>
    </paper>
    <paper id="259">
      <title>Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction</title>
      <author><first>Lu</first><last>Yang</last></author>
      <author><first>Jiajia</first><last>Li</last></author>
      <author><first>En</first><last>Ci</last></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <pages>5021-5040</pages>
      <abstract>Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations simultaneously, the inclusion of irrelevant relations introduces decision complexity and impacts extraction accuracy. Therefore, for multi-relation extraction, we propose LDNet, which incorporates multi-aspect relation modeling and a label drop mechanism. By assigning different relations to different levels for understanding and decision-making, we reduce decision confusion. Additionally, the label drop mechanism effectively mitigates the impact of irrelevant relations. Experiments show that LDNet outperforms or achieves competitive performance with state-of-the-art systems on 9 tasks, 33 datasets, in both single-modal and multi-modal, few-shot and zero-shot settings.</abstract>
      <url hash="4f760ec1">2025.naacl-long.259</url>
      <bibkey>yang-etal-2025-label</bibkey>
    </paper>
    <paper id="260">
      <title>Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction</title>
      <author><first>Dongming</first><last>Sheng</last><affiliation>Interactive Entertainment Group, Tencent Inc.</affiliation></author>
      <author><first>Kexin</first><last>Han</last></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Yucheng</first><last>Huang</last></author>
      <author><first>Jun</first><last>Lang</last></author>
      <author><first>Wenqiang</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <pages>5041-5053</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with impressive outcomes being achieved on high-resource languages. However, the application of cross-lingual transfer to the ASTE task has been relatively unexplored, and current code-switching methods still suffer from term boundary detection issues and out-of-dictionary problems. In this study, we introduce a novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap between the bilingual training phase and the monolingual test-time prediction. During training, a generative model is developed based on bilingual code-switched training data and can produce bilingual ASTE triplets for bilingual inputs. In the testing stage, we employ an alignment-based code-switching technique for test-time augmentation. Extensive experiments on cross-lingual ASTE datasets validate the effectiveness of our proposed method. We achieve an average improvement of 3.7% in terms of weighted-averaged F1 in four datasets with different languages. Additionally, we set a benchmark using ChatGPT and GPT-4, and demonstrate that even smaller generative models fine-tuned with our proposed TT-CSW framework surpass ChatGPT and GPT-4 by 14.2% and 5.0% respectively.</abstract>
      <url hash="fe033977">2025.naacl-long.260</url>
      <bibkey>sheng-etal-2025-test</bibkey>
    </paper>
    <paper id="261">
      <title><fixed-case>V</fixed-case>is<fixed-case>CGEC</fixed-case>: Benchmarking the Visual <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Xiaoman</first><last>Wang</last></author>
      <author><first>Dan</first><last>Yuan</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yike</first><last>Zhao</last></author>
      <author><first>Xiaoxiao</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xizhi</first><last>Chen</last></author>
      <author><first>Yunshi</first><last>Lan</last><affiliation>East China Normal University</affiliation></author>
      <pages>5054-5068</pages>
      <url hash="1bcd8381">2025.naacl-long.261</url>
      <bibkey>wang-etal-2025-viscgec</bibkey>
    </paper>
    <paper id="262">
      <title>Are We Done with <fixed-case>MMLU</fixed-case>?</title>
      <author><first>Aryo Pradipta</first><last>Gema</last><affiliation>Anthropic and University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Joshua Ong Jun</first><last>Leang</last></author>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alessio</first><last>Devoto</last></author>
      <author><first>Alberto Carlo Maria</first><last>Mancino</last></author>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Yu</first><last>Zhao</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Xiaotang</first><last>Du</last></author>
      <author><first>Mohammad Reza</first><last>Ghasemi Madani</last><affiliation>University of Trento</affiliation></author>
      <author><first>Claire</first><last>Barale</last></author>
      <author><first>Robert</first><last>McHardy</last><affiliation>AssemblyAI</affiliation></author>
      <author><first>Joshua</first><last>Harris</last><affiliation>UK Health Security Agency</affiliation></author>
      <author><first>Jean</first><last>Kaddour</last></author>
      <author><first>Emile</first><last>Van Krieken</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>5069-5096</pages>
      <abstract>Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU’s error-ridden questions to enhance its future utility and reliability as a benchmark. Therefore, we open up MMLU-Redux for additional annotation.</abstract>
      <url hash="77de4465">2025.naacl-long.262</url>
      <bibkey>gema-etal-2025-done</bibkey>
    </paper>
    <paper id="263">
      <title><fixed-case>M</fixed-case>e<fixed-case>NT</fixed-case>i: Bridging Medical Calculator and <fixed-case>LLM</fixed-case> Agent with Nested Tool Calling</title>
      <author><first>Yakun</first><last>Zhu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shaohang</first><last>Wei</last><affiliation>Peking University</affiliation></author>
      <author><first>Xu</first><last>Wang</last></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>5097-5116</pages>
      <abstract>Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual’s health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.</abstract>
      <url hash="a8831b4c">2025.naacl-long.263</url>
      <bibkey>zhu-etal-2025-menti</bibkey>
    </paper>
    <paper id="264">
      <title>Steering Knowledge Selection Behaviours in <fixed-case>LLM</fixed-case>s via <fixed-case>SAE</fixed-case>-Based Representation Engineering</title>
      <author><first>Yu</first><last>Zhao</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alessio</first><last>Devoto</last></author>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xiaotang</first><last>Du</last></author>
      <author><first>Aryo Pradipta</first><last>Gema</last><affiliation>Anthropic and University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>5117-5136</pages>
      <abstract>Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context—this phenomenon, known as <i>context-memory knowledge conflicts</i>, can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers. Such signals allow us to detect whether a knowledge conflict occurs and use <i>inference-time</i> intervention strategies to resolve it. In this work, we propose SpARE, a <i>training-free</i> representation engineering method that uses pre-trained sparse auto-encoders (SAEs) to control the knowledge selection behaviour of LLMs. SpARE identifies the functional features that control the knowledge selection behaviours and applies them to edit the internal activations of LLMs at inference time. Our experimental results show that SpARE can effectively control the usage of either knowledge source to resolve knowledge conflict in open-domain question-answering tasks, surpassing existing representation engineering methods (+10%) as well as contrastive decoding methods (+15%).</abstract>
      <url hash="f246dc71">2025.naacl-long.264</url>
      <bibkey>zhao-etal-2025-steering</bibkey>
    </paper>
    <paper id="265">
      <title><fixed-case>M</fixed-case>o<fixed-case>D</fixed-case>ification: Mixture of Depths Made Easy</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Meizhi</first><last>Zhong</last></author>
      <author><first>Qimeng</first><last>Wang</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Xuantao</first><last>Lu</last></author>
      <author><first>Zheyu</first><last>Ye</last><affiliation>Xiaohongshu Inc</affiliation></author>
      <author><first>Chengqiang</first><last>Lu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Dawei</first><last>Song</last><affiliation>Beijing Institute of Technology and Open University</affiliation></author>
      <pages>5137-5149</pages>
      <abstract>Long-context efficiency has recently become a trending topic in serving large language models (LLMs). And mixture of depths (MoD) is proposed as a perfect fit to bring down both latency and memory. In this paper, however, we discover that MoD can barely transform existing LLMs without costly training over an extensive number of tokens. To enable the transformations from any LLMs to MoD ones, we showcase top-k operator in MoD should be promoted to threshold-p operator, and refinement to architecture and data should also be crafted along. All these designs form our method termed MoDification. Through a comprehensive set of experiments covering model scales from 3B to 70B, we exhibit MoDification strikes an excellent balance between efficiency and effectiveness. MoDification can achieve up to ~1.2<tex-math>\times</tex-math> speedup in latency and ~1.8<tex-math>\times</tex-math> reduction in memory compared to original LLMs especially in long-context applications.</abstract>
      <url hash="edb3cdad">2025.naacl-long.265</url>
      <bibkey>zhang-etal-2025-modification</bibkey>
    </paper>
    <paper id="266">
      <title>On the Vulnerability of Text Sanitization</title>
      <author><first>Meng</first><last>Tong</last></author>
      <author><first>Kejiang</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiaojian</first><last>Yuan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiayang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jie</first><last>Zhang</last><affiliation>A*STAR</affiliation></author>
      <pages>5150-5164</pages>
      <abstract>Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess the effectiveness of sanitization. In this paper, we aim to provide a more accurate evaluation of sanitization effectiveness. Inspired by the works of Palamidessi et al., we implement theoretically optimal reconstruction attacks targeting text sanitization. We derive their bounds on ASR as benchmarks for evaluating sanitization performance. For real-world applications, we propose two practical reconstruction attacks based on these theoretical findings. Our experimental results underscore the necessity of reassessing these overlooked risks. Notably, one of our attacks achieves a 46.4% improvement in ASR over the state-of-the-art baseline, with a privacy budget of <tex-math>\epsilon=4.0</tex-math> on the SST-2 dataset. Our code is available at: https://github.com/mengtong0110/On-the-Vulnerability-of-Text-Sanitization.</abstract>
      <url hash="c6d0663f">2025.naacl-long.266</url>
      <bibkey>tong-etal-2025-vulnerability</bibkey>
    </paper>
    <paper id="267">
      <title>Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models</title>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Prasoon</first><last>Bajpai</last></author>
      <author><first>Soham</first><last>Dan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>5165-5180</pages>
      <abstract>While recent large language models (LLMs) demonstrate remarkable abilities in responding to queries in diverse languages, their ability to handle long multilingual contexts is unexplored. As such, a systematic evaluation of the long-context capabilities of LLMs in multilingual settings is crucial, specifically in the context of information retrieval. To address this gap, we introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to assess a model’s ability to retrieve relevant information (the needle) from a collection of multilingual distractor texts (the haystack). This test serves as an extension of the multilingual question-answering task, encompassing both monolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs on MLNeedle. Our findings reveal that model performance can vary significantly with language and needle position. Specifically, we observe that model performance is the lowest when the needle is (i) in a language outside the English language family, and (ii) located in the middle of the input context. Furthermore, although some models claim a context size of 8k tokens or greater, none demonstrate satisfactory cross-lingual retrieval performance as the context length increases. Our analysis provides key insights into the long-context behavior of LLMs in multilingual settings to guide future evaluation protocols. To our knowledge, this is the first study to investigate the multilingual long-context behavior of LLMs.</abstract>
      <url hash="081c260f">2025.naacl-long.267</url>
      <bibkey>hengle-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="268">
      <title>Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation</title>
      <author><first>Hoang</first><last>Pham</last><affiliation>Viettel AI &amp; Data Service Center</affiliation></author>
      <author><first>Thanh-Do</first><last>Nguyen</last></author>
      <author><first>Khac-Hoai Nam</first><last>Bui</last><affiliation>Viettel Group</affiliation></author>
      <pages>5181-5197</pages>
      <abstract>Claim verification is a long-standing and challenging task that demands not only high accuracy but also explainability and thoroughness of the verification process. This task becomes an emerging research issue in the era of large language models (LLMs) since real-world claims are often complex, featuring intricate semantic structures or obfuscated entities. Traditional approaches typically address this by decomposing claims into sub-claims and querying a knowledge base to resolve hidden or ambiguous entities. However, the absence of effective disambiguation strategies for these entities can compromise the entire verification process. To address these challenges, we propose Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and comprehension abilities of LLM agents. VeGraph operates in three phases: (1) Graph Representation - an input claim is decomposed into structured triplets, forming a graph-based representation that integrates both structured and unstructured information; (2) Entity Disambiguation -VeGraph iteratively interacts with the knowledge base to resolve ambiguous entities within the graph for deeper sub-claim verification; and (3) Verification - remaining triplets are verified to complete the fact-checking process. Experiments using Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive performance compared to baselines across benchmarks (HoVer and FEVEROUS), effectively addressing claim verification challenges. Our source code and data are available for further exploitation.</abstract>
      <url hash="b33bb5a2">2025.naacl-long.268</url>
      <bibkey>pham-etal-2025-verify</bibkey>
    </paper>
    <paper id="269">
      <title>Exploring the Potential of Large Language Models for Heterophilic Graphs</title>
      <author><first>Yuxia</first><last>Wu</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Shujie</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yuan</first><last>Fang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Chuan</first><last>Shi</last><affiliation>Beijing University of Post and Telecommunication, Tsinghua University</affiliation></author>
      <pages>5198-5211</pages>
      <abstract>Large language models (LLMs) have presented significant opportunities to enhance various machine learning applications, including graph neural networks (GNNs). By leveraging the vast open-world knowledge within LLMs, we can more effectively interpret and utilize textual data to better characterize heterophilic graphs, where neighboring nodes often have different labels. However, existing approaches for heterophilic graphs overlook the rich textual data associated with nodes, which could unlock deeper insights into their heterophilic contexts. In this work, we explore the potential of LLMs for modeling heterophilic graphs and propose a novel two-stage framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first stage, we fine-tune the LLM to better identify homophilic and heterophilic edges based on the textual content of their nodes. In the second stage, we adaptively manage message propagation in GNNs for different edge types based on node features, structures, and heterophilic or homophilic characteristics. To cope with the computational demands when deploying LLMs in practical scenarios, we further explore model distillation techniques to fine-tune smaller, more efficient models that maintain competitive performance. Extensive experiments validate the effectiveness of our framework, demonstrating the feasibility of using LLMs to enhance node classification on heterophilic graphs.</abstract>
      <url hash="e510652d">2025.naacl-long.269</url>
      <bibkey>wu-etal-2025-exploring</bibkey>
    </paper>
    <paper id="270">
      <title>Exploiting Edited Large Language Models as General Scientific Optimizers</title>
      <author><first>Qitan</first><last>Lv</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Hong</first><last>Wang</last></author>
      <pages>5212-5237</pages>
      <abstract>Large language models (LLMs) have been widely adopted in mathematical optimization in scientific scenarios for their extensive knowledge and advanced reasoning capabilities. Existing methods mainly focus on utilizing LLMs to solve optimization problems in a prompt-based manner, which takes observational feedback as additional textual descriptions. However, due to LLM’s **high sensitivity to the prompts** and **tendency to get lost in lengthy prompts**, these methods struggle to effectively utilize the observational feedback from each optimization step, which severely hinders the applications for real-world scenarios. To address these challenges, we propose a conceptually simple and general bi-level optimization method, namely **G**eneral **S**cientific **O**ptimizers (GSO).Specifically, GSO first utilizes inner-level simulators as experimental platforms to evaluate the current solution and provide observational feedback. Then, LLMs serve as knowledgeable and versatile scientists, generating new solutions by refining potential errors from the feedback as the outer-level optimization.Finally, simulations together with the expert knowledge in LLMs are jointly updated with bi-level interactions via model editing.Extensive experiments show that GSO consistently outperforms existing state-of-the-art methods using *six* different LLM backbone on *seven* different tasks, demonstrating the effectiveness and a wide range of applications.</abstract>
      <url hash="da75a772">2025.naacl-long.270</url>
      <bibkey>lv-etal-2025-exploiting</bibkey>
    </paper>
    <paper id="271">
      <title><fixed-case>DIRAS</fixed-case>: Efficient <fixed-case>LLM</fixed-case> Annotation of Document Relevance for Retrieval Augmented Generation</title>
      <author><first>Jingwei</first><last>Ni</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tobias</first><last>Schimanski</last></author>
      <author><first>Meihong</first><last>Lin</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Elliott</first><last>Ash</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Markus</first><last>Leippold</last><affiliation>University of Zurich</affiliation></author>
      <pages>5238-5258</pages>
      <abstract>Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information when answering queries that need an integrated analysis of information (e.g., Tell me good news in the stock market today.)? To address these concerns, RAG developers need to annotate information retrieval (IR) data for their domain of interest, which is challenging because (1) domain-specific queries usually need nuanced definitions of relevance beyond shallow semantic relevance; and (2) human or GPT-4 annotation is costly and cannot cover all (query, document) pairs (i.e., annotation selection bias), thus harming the effectiveness in evaluating IR recall. To address these challenges, we propose DIRAS (**D**omain-specific **I**nformation **R**etrieval **A**nnotation with **S**calability), a manual-annotation-free schema that fine-tunes open-sourced LLMs to consider nuanced relevance definition and annotate (partial) relevance labels with calibrated relevance scores. Extensive evaluation shows that DIRAS enables smaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking unseen (query, document) pairs, and is helpful for real-world RAG development.</abstract>
      <url hash="949029c8">2025.naacl-long.271</url>
      <bibkey>ni-etal-2025-diras</bibkey>
    </paper>
    <paper id="272">
      <title>Hello Again! <fixed-case>LLM</fixed-case>-powered Personalized Agent for Long-term Dialogue</title>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Chenghao</first><last>Yang</last></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>5259-5276</pages>
      <abstract>Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation. For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval. Furthermore, the persona module conducts dynamic persona modeling for both users and agents. The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses. The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks. The code is released at https://github.com/leolee99/LD-Agent.</abstract>
      <url hash="752537ca">2025.naacl-long.272</url>
      <bibkey>li-etal-2025-hello</bibkey>
    </paper>
    <paper id="273">
      <title>My <fixed-case>LLM</fixed-case> might Mimic <fixed-case>AAE</fixed-case> - But When Should It?</title>
      <author><first>Sandra Camille</first><last>Sandoval</last></author>
      <author><first>Christabel</first><last>Acquaye</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Kwesi Adu</first><last>Cobbina</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Mohammad Nayeem</first><last>Teli</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Hal Daumé</first><last>Iii</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>5277-5302</pages>
      <abstract>We examine the representation of African American English (AAE) in large language models (LLMs), exploring (a) the perceptions Black Americans have of how effective these technologies are at producing authentic AAE, and (b) in what contexts Black Americans find this desirable. Through both a survey of Black Americans (<tex-math>n=</tex-math> 104) and annotation of LLM-produced AAE by Black Americans (<tex-math>n=</tex-math> 228), we find that Black Americans favor choice and autonomy in determining when AAE is appropriate in LLM output. They tend to prefer that LLMs default to communicating in Mainstream U.S. English in formal settings, with greater interest in AAE production in less formal settings. When LLMs were appropriately prompted and provided in context examples, our participants found their outputs to have a level of AAE authenticity on par with transcripts of Black American speech. Select code and data for our project can be found here: <url>https://github.com/smelliecat/AAEMime.git</url></abstract>
      <url hash="0ba85535">2025.naacl-long.273</url>
      <bibkey>sandoval-etal-2025-llm</bibkey>
    </paper>
    <paper id="274">
      <title>High-Dimension Human Value Representation in Large Language Models</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Delong</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Leila</first><last>Khalatbari</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Ziwei</first><last>Ji</last></author>
      <author><first>Etsuko</first><last>Ishii</last><affiliation>Amazon</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>5303-5330</pages>
      <abstract>The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, such as Reinforcement Learning with Human Feedback (RLHF), constitutional learning, and safety fine-tuning etc., there is an urgent need to understand the scope and nature of human values injected into these LLMs before their deployment and adoption. We propose UniVar, a high-dimensional neural representation of symbolic human value distributions in LLMs, orthogonal to model architecture and training data. This is a continuous and scalable representation, self-supervised from the value-relevant output of 8 LLMs and evaluated on 15 open-source and commercial LLMs. Through UniVar, we visualize and explore how LLMs prioritize different values in 25 languages and cultures, shedding light on the complex interplay between human values and language modeling.</abstract>
      <url hash="8e8b6489">2025.naacl-long.274</url>
      <bibkey>cahyawijaya-etal-2025-high</bibkey>
    </paper>
    <paper id="275">
      <title>Not all Hallucinations are Good to Throw Away When it Comes to Legal Abstractive Summarization</title>
      <author><first>Nihed</first><last>Bendahman</last></author>
      <author><first>Karen</first><last>Pinel-Sauvagnat</last><affiliation>IRIT - Université Paul Sabatier</affiliation></author>
      <author><first>Gilles</first><last>Hubert</last><affiliation>IRIT, University of Toulouse</affiliation></author>
      <author><first>Mokhtar Boumedyen</first><last>Billami</last></author>
      <pages>5331-5344</pages>
      <abstract>Automatic summarization of legal documents requires a thorough understanding of their specificities, mainly with respect to the vocabulary used by legal experts. Indeed, the latter rely heavily on their external knowledge when writing summaries, in order to contextualize the main entities of the source document. This leads to reference summaries containing many abstractions, that sota models struggle to generate. In this paper, we propose an entity-driven approach aiming at learning the model to generate factual hallucinations, as close as possible to the abstractions of the reference summaries. We evaluated our approach on two different datasets, with legal documents in English and French. Results show that our approach allows to reduce non-factual hallucinations and maximize both summary coverage and factual hallucinations at entity-level. Moreover, the overall quality of summaries is also improved, showing that guiding summarization with entities is a valuable solution for legal documents summarization.</abstract>
      <url hash="ef4c63d5">2025.naacl-long.275</url>
      <bibkey>bendahman-etal-2025-hallucinations</bibkey>
    </paper>
    <paper id="276">
      <title>Query-focused Referentiability Learning for Zero-shot Retrieval</title>
      <author><first>Jaeyoung</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>5345-5358</pages>
      <abstract>Dense passage retrieval enhances Information Retrieval (IR) by encoding queries and passages into representation space. However, passage representations often fail to be referenced by their gold queries under domain shifts, revealing a weakness in representation space. One desirable concept for representations is ”argmaxable”. Being argmaxable ensures that no representations are theoretically excluded from selection due to geometric constraints. To be argmaxable, a notable approach is to increase isotropy, where representations are evenly spread out in all directions. These findings, while desirable also for IR, focus on passage representation and not on query, making it challenging to directly apply their findings to IR. In contrast, we introduce a novel query-focused concept of ”referentiable” tailored for IR tasks, which ensures that passage representations are referenced by their gold queries. Building on this, we propose Learning Referentiable Representation (LRR), and two strategic metrics, Self-P and Self-Q, quantifying how the representations are referentiable. Our experiments compare three dense model versions: Naive, Isotropic, and Referentiable, demonstrating that LRR leads to enhanced zero-shot performance, surpassing existing naive and isotropic versions.</abstract>
      <url hash="dbaf9be8">2025.naacl-long.276</url>
      <bibkey>kim-etal-2025-query</bibkey>
    </paper>
    <paper id="277">
      <title>A Novel Computational Modeling Foundation for Automatic Coherence Assessment</title>
      <author><first>Aviya</first><last>Maimon</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>5359-5377</pages>
      <abstract>Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks such as summarization, long-form question-answering, and more.Current NLP approaches for modeling coherence often rely on a proxy task, specifically, <i>sentence reordering</i>. However, such an approach may not capture the full range of factors contributing to coherence.To remedy this, in this work we employ the formal linguistic definition by Reinhart:1980 of what makes a discourse coherent, consisting of three conditions, <i>cohesion, consistency</i> and <i>relevance</i>, and formalize these conditions as respective computational tasks, which are in turn jointly trained. We evaluate this modeling approach on two human-rated coherence benchmarks: one of automatically-generated stories and one of real-world texts.Our experiments show that jointly training on the proposed tasks leads to better performance on each task compared with task-specific models, and to better performance on assessing coherence overall.Our proposed computational framework thus paves the way for a more advanced, broad-coverage coherence assessment.</abstract>
      <url hash="d4bd9465">2025.naacl-long.277</url>
      <bibkey>maimon-2025-novel</bibkey>
    </paper>
    <paper id="278">
      <title>Token-based Decision Criteria Are Suboptimal in In-context Learning</title>
      <author><first>Hakaze</first><last>Cho</last></author>
      <author><first>Yoshihiro</first><last>Sakai</last><affiliation>Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Mariko</first><last>Kato</last></author>
      <author><first>Kenshiro</first><last>Tanaka</last></author>
      <author><first>Akira</first><last>Ishii</last></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <pages>5378-5401</pages>
      <abstract>In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM’s last hidden states. In detail, we assign the label of the nearest centroid previously estimated from a calibration set to the test sample as the predicted label. Our experiments on 6 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based baselines by about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-class overlap, and LMs provide linearly separable intra-class clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the principle of ICL. Our official code implementation can be found at https://github.com/hc495/Hidden_Calibration.</abstract>
      <url hash="734192de">2025.naacl-long.278</url>
      <bibkey>cho-etal-2025-token</bibkey>
    </paper>
    <paper id="279">
      <title><fixed-case>CSE</fixed-case>val: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated <fixed-case>LLM</fixed-case>s</title>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Aswini Kumar</first><last>Padhi</last></author>
      <author><first>Anil</first><last>Bandhakavi</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>5402-5419</pages>
      <abstract>Counterspeech has emerged as a popular and effective strategy for combating online hate speech, sparking growing research interest in automating its generation using language models. However, the field still lacks standardised evaluation protocols and reliable automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce ‘CSEval‘, a novel dataset and framework for evaluating counterspeech quality across four dimensions: *contextual-relevance*, *aggressiveness*, *argument-coherence*, and *suitableness*. Furthermore, we propose *Auto-Calibrated COT for Counterspeech Evaluation* (‘Auto-CSEval‘), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that ‘Auto-CSEval‘ outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant improvement in automated counterspeech evaluation.</abstract>
      <url hash="a35a3e4c">2025.naacl-long.279</url>
      <bibkey>hengle-etal-2025-cseval</bibkey>
    </paper>
    <paper id="280">
      <title>Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study</title>
      <author><first>Menglong</first><last>Cui</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Pengzhi</first><last>Gao</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <pages>5420-5443</pages>
      <abstract>Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and X-ALMA and achieves competitive performance with Google Translate and GPT-4-turbo.</abstract>
      <url hash="9fae0fce">2025.naacl-long.280</url>
      <bibkey>cui-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="281">
      <title><fixed-case>RAG</fixed-case> <fixed-case>LLM</fixed-case>s are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models</title>
      <author><first>Bang</first><last>An</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Shiyue</first><last>Zhang</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering and Bloomberg</affiliation></author>
      <pages>5444-5474</pages>
      <abstract>Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming.However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model’s safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.</abstract>
      <url hash="0604cfb9">2025.naacl-long.281</url>
      <bibkey>an-etal-2025-rag</bibkey>
    </paper>
    <paper id="282">
      <title>Evaluating Evidence Attribution in Generated Fact Checking Explanations</title>
      <author><first>Rui</first><last>Xing</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Melbourne</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>5475-5496</pages>
      <abstract>Automated fact-checking systems often struggle with trustworthiness, as their generated explanations can include hallucinations. In this work, we explore evidence attribution for fact-checking explanation generation. We introduce a novel evaluation protocol, citation masking and recovery, to assess attribution quality in generated explanations. We implement our protocol using both human annotators and automatic annotators and found that LLM annotation correlates with human annotation, suggesting that attribution assessment can be automated. Finally, our experiments reveal that: (1) the best-performing LLMs still generate explanations that are not always accurate in their attribution; and (2) human-curated evidence is essential for generating better explanations.</abstract>
      <url hash="dd6ab27a">2025.naacl-long.282</url>
      <bibkey>xing-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="283">
      <title><fixed-case>ETHIC</fixed-case>: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage</title>
      <author><first>Taewhoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Kyochul</first><last>Jang</last><affiliation>Korea University</affiliation></author>
      <author><first>Donghyeon</first><last>Lee</last></author>
      <author><first>Minju</first><last>Song</last></author>
      <author><first>Hyunjae</first><last>Kim</last><affiliation>Yale University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>5497-5512</pages>
      <abstract>Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new metric called information coverage (IC), which quantifies the proportion of the input context necessary for answering queries. Our findings indicate that current benchmarks exhibit low IC; although the input context may be extensive, the actual usable context is often limited. To address this, we present ETHIC, a novel benchmark designed to assess LLMs’ ability to leverage the entire context. Our benchmark comprises 1,986 test instances spanning four long-context tasks with high IC scores in the domains of books, debates, medicine, and law. Our evaluations reveal significant performance drops in contemporary LLMs, highlighting a critical challenge in managing long contexts. Our benchmark is available at https://github.com/dmis-lab/ETHIC.</abstract>
      <url hash="374c2387">2025.naacl-long.283</url>
      <bibkey>lee-etal-2025-ethic</bibkey>
    </paper>
    <paper id="284">
      <title>Aggregation Artifacts in Subjective Tasks Collapse Large Language Models’ Posteriors</title>
      <author><first>Georgios</first><last>Chochlakis</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Alexandros</first><last>Potamianos</last><affiliation>Amazon, University of Southern California and National Technical University of Athens</affiliation></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <author><first>Shrikanth</first><last>Narayanan</last><affiliation>University of Southern California</affiliation></author>
      <pages>5513-5528</pages>
      <abstract>In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than “learning” to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified.</abstract>
      <url hash="073a7724">2025.naacl-long.284</url>
      <bibkey>chochlakis-etal-2025-aggregation</bibkey>
    </paper>
    <paper id="285">
      <title><fixed-case>A</fixed-case>rabic Dataset for <fixed-case>LLM</fixed-case> Safeguard Evaluation</title>
      <author><first>Yasser</first><last>Ashraf</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Bin</first><last>Gu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>5529-5546</pages>
      <abstract>The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the socio-cultural context of the Arab world. To uncover the impact of different stances in handling sensitive and controversial topics, we propose a dual-perspective evaluation framework. It assesses the LLM responses from both governmental and opposition viewpoints. Experiments over five leading Arabic-centric and multilingual LLMs reveal substantial disparities in their safety performance. This reinforces the need for culturally specific datasets to ensure the responsible deployment of LLMs.</abstract>
      <url hash="2eec3236">2025.naacl-long.285</url>
      <bibkey>ashraf-etal-2025-arabic</bibkey>
    </paper>
    <paper id="286">
      <title>Anticipating Future with Large Language Model for Simultaneous Machine Translation</title>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Oleksii</first><last>Hrinchuk</last><affiliation>Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Zhehuai</first><last>Chen</last></author>
      <author><first>Vitaly</first><last>Lavrukhin</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Jagadeesh</first><last>Balam</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Boris</first><last>Ginsburg</last><affiliation>NVIDIA</affiliation></author>
      <pages>5547-5557</pages>
      <abstract>Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods only use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters’ technique to forecast future words before hearing them, we propose Translation by Anticipating Future (TAF), a method to improve translation quality while retaining low latency. Its core idea is to use a large language model (LLM) to predict future source words and opportunistically translate without introducing too much risk. We evaluate our TAF and multiple baselines of SMT on four language directions. Experiments show that TAF achieves the best translation quality-latency trade-off and outperforms the baselines by up to 5 BLEU points at the same latency (three words).</abstract>
      <url hash="62ae0709">2025.naacl-long.286</url>
      <bibkey>ouyang-etal-2025-anticipating</bibkey>
    </paper>
    <paper id="287">
      <title><fixed-case>G</fixed-case>uide<fixed-case>LLM</fixed-case>: Exploring <fixed-case>LLM</fixed-case>-Guided Conversation with Applications in Autobiography Interviewing</title>
      <author><first>Jinhao</first><last>Duan</last></author>
      <author><first>Xinyu</first><last>Zhao</last></author>
      <author><first>Zhuoxuan</first><last>Zhang</last><affiliation>Columbia University and Brown University</affiliation></author>
      <author><first>Eunhye Grace</first><last>Ko</last></author>
      <author><first>Lily</first><last>Boddy</last></author>
      <author><first>Chenan</first><last>Wang</last></author>
      <author><first>Tianhao</first><last>Li</last></author>
      <author><first>Alexander</first><last>Rasgon</last></author>
      <author><first>Junyuan</first><last>Hong</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Min Kyung</first><last>Lee</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Chenxi</first><last>Yuan</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Qi</first><last>Long</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Ying</first><last>Ding</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>5558-5588</pages>
      <abstract>Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations—where LLMs direct the discourse and steer the conversation’s objectives—remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation. Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over 200 events mentioned during the interviewing for each chatbot evaluation. We compare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and Llama-3-70b-Instruct, from the perspective of interviewing quality, and autobiography generation quality. For automatic evaluation, we derive user proxies from multiple autobiographies and employ LLM-as-a-judge to score LLM behaviors. We further conduct a human-involved experiment by employing 45 human participants to chat with GuideLLM and baselines. We then collect human feedback, preferences, and ratings regarding the qualities of conversation and autobiography. Experimental results indicate that GuideLLM significantly outperforms baseline LLMs in automatic evaluation and achieves consistent leading performances in human ratings.</abstract>
      <url hash="5bea5328">2025.naacl-long.287</url>
      <bibkey>duan-etal-2025-guidellm</bibkey>
    </paper>
    <paper id="288">
      <title>Fine-Tuning Large Language Models with Sequential Instructions</title>
      <author><first>Hanxu</first><last>Hu</last></author>
      <author><first>Simon</first><last>Yu</last></author>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>5589-5610</pages>
      <abstract>We find that existing instruction-tuned models usually struggle to adhere to a query with multiple intentions, which impairs their performance when the completion of several tasks is demanded by a single command. Hence, this paper teaches models to respond to sequential instructions. Our first attempt stems from a task-driven perspective, manually creating additional intermediate tasks to train multilingual and visual question answering. Next, we develop an automatic and generic process that turns instructions in existing data into diverse and complex task chains. Models that underwent sequential instruction tuning follow a list of instructions better and deliver higher results in coding, maths, and open-ended generation. Moreover, we put forward a new benchmark named SeqEval to evaluate a model’s ability to follow all the instructions in a sequence, which further corroborates the benefits of our sequential instruction tuning method.</abstract>
      <url hash="2e9571f1">2025.naacl-long.288</url>
      <bibkey>hu-etal-2025-fine-tuning</bibkey>
    </paper>
    <paper id="289">
      <title>Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing</title>
      <author><first>Mayank</first><last>Kothyari</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Sunita</first><last>Sarawagi</last><affiliation>IIT Bombay</affiliation></author>
      <author><first>Soumen</first><last>Chakrabarti</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Gaurav</first><last>Arora</last><affiliation>Amazon</affiliation></author>
      <author><first>Srujana</first><last>Merugu</last></author>
      <pages>5611-5629</pages>
      <abstract>LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances. Finally, we adapt and extend a recent method for diverse ICE selection to work with whole and fragmented ICE instances. We evaluate our system, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing visible accuracy gains from our proposed decomposed diverse demonstration method. Benefits are particularly notable for smaller LLMs, ICE pools having larger labeled trees, and programs in lower resource languages.</abstract>
      <url hash="cb613d8e">2025.naacl-long.289</url>
      <bibkey>kothyari-etal-2025-diverse</bibkey>
    </paper>
    <paper id="290">
      <title>Elevating Legal <fixed-case>LLM</fixed-case> Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning</title>
      <author><first>Rujing</first><last>Yao</last></author>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Chenghao</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jingwei</first><last>Xiong</last></author>
      <author><first>Fang</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaozhong</first><last>Liu</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <pages>5630-5642</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods.</abstract>
      <url hash="a633fc2b">2025.naacl-long.290</url>
      <bibkey>yao-etal-2025-elevating</bibkey>
    </paper>
    <paper id="291">
      <title>Efficient One-shot Compression via Low-Rank Local Feature Distillation</title>
      <author><first>Yaya</first><last>Sy</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>Christophe</first><last>Cerisara</last><affiliation>University of Lorraine</affiliation></author>
      <author><first>Irina</first><last>Illina</last></author>
      <pages>5643-5661</pages>
      <abstract>Current structured pruning approaches for large language models typically involve two steps: (1) compression using calibration data and (2) costly continued pretraining on billions of tokens to recover lost performance. This second step is necessary as the first significantly impacts model accuracy. Moreover, prior research suggests that pretrained Transformer weights are not necessarily low-rank, unlike their activations, making one-shot structured pruning challenging. Based on this observation, we propose Lillama, a compression method that locally distills activations with low-rank weights. Using SVD for initialization and a joint loss combining teacher and student activations, we accelerate convergence and reduce memory use with local gradient updates. Lillama compresses Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while retaining over 95% of its original performance. Phi-2 3B can be compressed by 40% with just 13 million calibration tokens, resulting in a small model that competes with recent models of similar size. The method generalizes well to non-transformer architectures, compressing Mamba-3B by 20% while maintaining 99% performance.</abstract>
      <url hash="9017ed61">2025.naacl-long.291</url>
      <bibkey>sy-etal-2025-efficient</bibkey>
    </paper>
    <paper id="292">
      <title>Waste Not, Want Not; Recycled <fixed-case>G</fixed-case>umbel Noise Improves Consistency in Natural Language Generation</title>
      <author><first>Damien</first><last>De Mijolla</last></author>
      <author><first>Hannan</first><last>Saddiq</last></author>
      <author><first>Kim</first><last>Moore</last></author>
      <pages>5662-5686</pages>
      <abstract>Consistency in the output of language models is critical for their reliability and practical utility. Due to their training objective, language models learn to model the full space of possible continuations, leading to outputs that can vary significantly in style, content, and tone, even for similar inputs. To address this, we propose a novel decoding algorithm that enhances response consistency across different prompts with no degradation in response quality. By incorporating a latent variable into the next-token sampling process based on the Gumbel reparametrisation trick, our method outperforms standard sampling by up to 10% across semantic and stylistic consistency benchmarks. Additionally, our approach integrates seamlessly with existing sampling methods with negligible computational overhead, providing a practical solution for improving the reliability of language model outputs.</abstract>
      <url hash="464ab052">2025.naacl-long.292</url>
      <bibkey>de-mijolla-etal-2025-waste</bibkey>
    </paper>
    <paper id="293">
      <title><fixed-case>C</fixed-case>on<fixed-case>QR</fixed-case>et: A New Benchmark for Fine-Grained Automatic Evaluation of Retrieval Augmented Computational Argumentation</title>
      <author><first>Kaustubh</first><last>Dhole</last><affiliation>Emory University</affiliation></author>
      <author><first>Kai</first><last>Shu</last><affiliation>Emory University</affiliation></author>
      <author><first>Eugene</first><last>Agichtein</last><affiliation>Emory University</affiliation></author>
      <pages>5687-5713</pages>
      <abstract>Computational argumentation, which involves generating answers or summaries for controversial topics like abortion bans and vaccination, has become increasingly important in today’s polarized environment. Sophisticated LLM capabilities offer the potential to provide nuanced, evidence-based answers to such questions through Retrieval-Augmented Argumentation (RAArg), leveraging real-world evidence for high-quality, grounded arguments. However, evaluating RAArg remains challenging, as human evaluation is costly and difficult for complex, lengthy answers on complicated topics. At the same time, re-using existing argumentation datasets is no longer sufficient, as they lack long, complex arguments and realistic evidence from potentially misleading sources, limiting holistic evaluation of retrieval effectiveness and argument quality. To address these gaps, we investigate automated evaluation methods using multiple fine-grained LLM judges, providing better and more interpretable assessments than traditional single-score metrics and even previously reported human crowdsourcing. To validate the proposed techniques, we introduce ConQRet, a new benchmark featuring long and complex human-authored arguments on debated topics, grounded in real-world websites, allowing an exhaustive evaluation across retrieval effectiveness, argument quality, and groundedness. We validate our LLM Judges on a prior dataset and the new ConQRet benchmark. Our proposed LLM Judges and the ConQRet benchmark can enable rapid progress in computational argumentation and can be naturally extended to other complex retrieval-augmented generation tasks.</abstract>
      <url hash="90eefa56">2025.naacl-long.293</url>
      <bibkey>dhole-etal-2025-conqret</bibkey>
    </paper>
    <paper id="294">
      <title><fixed-case>S</fixed-case>ynth<fixed-case>D</fixed-case>etox<fixed-case>M</fixed-case>: <fixed-case>M</fixed-case>odern <fixed-case>LLM</fixed-case>s are Few-Shot Parallel Detoxification Data Annotators</title>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Nikita</first><last>Sushko</last></author>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <pages>5714-5733</pages>
      <abstract>Existing approaches to multilingual text detoxification are hampered by the scarcity of parallel multilingual datasets. In this work, we introduce a pipeline for the generation of multilingual parallel detoxification data. We also introduce SynthDetoxM, a manually collected and synthetically generated multilingual parallel text detoxification dataset comprising 16,000 high-quality detoxification sentence pairs across German, French, Spanish and Russian. The data was sourced from different toxicity evaluation datasets and then rewritten with nine modern open-source LLMs in few-shot setting. Our experiments demonstrate that models trained on the produced synthetic datasets have superior performance to those trained on the human-annotated MultiParaDetox dataset even in data limited setting. Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting. We release our dataset and code to help further research in multilingual text detoxification.</abstract>
      <url hash="4892f636">2025.naacl-long.294</url>
      <bibkey>moskovskiy-etal-2025-synthdetoxm</bibkey>
    </paper>
    <paper id="295">
      <title><fixed-case>BEMEAE</fixed-case>: Moving Beyond Exact Span Match for Event Argument Extraction</title>
      <author><first>Enfa</first><last>Fane</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Md Nayem</first><last>Uddin</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Oghenevovwe</first><last>Ikumariegbe</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Daniyal</first><last>Kashif</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Steven</first><last>Corman</last></author>
      <pages>5734-5749</pages>
      <abstract>Event Argument Extraction (EAE) is a key task in natural language processing, focusing on identifying and classifying event arguments in text. However, the widely adopted exact span match (ESM) evaluation metric has notable limitations due to its rigid span constraints, often misidentifying valid predictions as errors and underestimating system performance. In this paper, we evaluate nine state-of-the-art EAE models on the RAMS and GENEVA datasets, highlighting ESM’s limitations. To address these issues, we introduce BEMEAE (Beyond Exact Span Match for Event Argument Extraction), a novel evaluation metric that recognizes predictions that are semantically equivalent to or improve upon the reference. BEMEAE integrates deterministic components with a semantic matching component for more accurate assessment. Our experiments demonstrate that BEMEAE aligns more closely with human judgments. We show that BEMEAE not only leads to higher F1 scores compared to ESM but also results in significant changes in model rankings, underscoring ESM’s inadequacy for comprehensive evaluation of EAE.</abstract>
      <url hash="e2cd205a">2025.naacl-long.295</url>
      <bibkey>fane-etal-2025-bemeae</bibkey>
    </paper>
    <paper id="296">
      <title>u<fixed-case>D</fixed-case>istil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes</title>
      <author><first>Abdul</first><last>Waheed</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Karima</first><last>Kadaoui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>5750-5767</pages>
      <abstract>Recent work on distilling Whisper’s knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally, the distillation process requires a large amount of data thereby limiting its applicability in low-resource settings. To address this, we propose a distillation framework that does not require <i>any</i> labeled data. Through experimentation, we show that our best-distilled models outperform the teacher model by 5-7 WER points and are on par with or outperform similar supervised data filtering setups. When scaling the data, our models significantly outperform all zero-shot and supervised models. Our models are also 25-50% more compute- and memory-efficient while maintaining performance equal to or better than that of the teacher model. For more details about our models, dataset, and other resources, please visit our GitHub page: <url>https://github.com/UBC-NLP/uDistilWhisper</url>.</abstract>
      <url hash="0f9e64bc">2025.naacl-long.296</url>
      <bibkey>waheed-etal-2025-udistil</bibkey>
    </paper>
    <paper id="297">
      <title>Iterative Self-Tuning <fixed-case>LLM</fixed-case>s for Enhanced Jailbreaking Capabilities</title>
      <author><first>Chung-En</first><last>Sun</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xiaodong</first><last>Liu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Weiwei</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tsui-Wei</first><last>Weng</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Aidan</first><last>San</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Michel</first><last>Galley</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>5768-5786</pages>
      <abstract>Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce **ADV-LLM**, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety.</abstract>
      <url hash="362918f3">2025.naacl-long.297</url>
      <bibkey>sun-etal-2025-iterative</bibkey>
    </paper>
    <paper id="298">
      <title><fixed-case>V</fixed-case>oice<fixed-case>T</fixed-case>ext<fixed-case>B</fixed-case>lender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning</title>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Krishna C</first><last>Puvvada</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Zhehuai</first><last>Chen</last></author>
      <author><first>Piotr</first><last>Zelasko</last><affiliation>NVIDIA</affiliation></author>
      <author><first>He</first><last>Huang</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Kunal</first><last>Dhawan</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Ke</first><last>Hu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jagadeesh</first><last>Balam</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Boris</first><last>Ginsburg</last><affiliation>NVIDIA</affiliation></author>
      <pages>5787-5802</pages>
      <abstract>Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting, where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone. Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT. Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks. Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs.</abstract>
      <url hash="80d98df4">2025.naacl-long.298</url>
      <bibkey>peng-etal-2025-voicetextblender</bibkey>
    </paper>
    <paper id="299">
      <title>Rethinking Word Similarity: Semantic Similarity through Classification Confusion</title>
      <author><first>Kaitlyn</first><last>Zhou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Haishan</first><last>Gao</last></author>
      <author><first>Sarah Li</first><last>Chen</last></author>
      <author><first>Dan</first><last>Edelstein</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chen</first><last>Shani</last></author>
      <pages>5803-5817</pages>
      <abstract>Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion is inspired by Tversky (1977)’s suggestion that similarity features be chosen dynamically. Here we train a classifier to map contextual embeddings to word identities and use the classifier confusion (the probability of choosing a confounding word c instead of the correct target word t) as a measure of the similarity of c and t. The set of potential confounding words acts as the chosen features. Our method is comparable to cosine similarity in matching human similarity judgments across several datasets (MEN, WirdSim353, and SimLex), and can measure similarity using predetermined features of interest. We demonstrate our model’s ability to make use of dynamic features by applying it to test a hypothesis about changes in the 18th C. meaning of the French word “révolution” from popular to state action during the French Revolution. We hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.</abstract>
      <url hash="f7685c8b">2025.naacl-long.299</url>
      <bibkey>zhou-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="300">
      <title><fixed-case>SUNAR</fixed-case>: Semantic Uncertainty based Neighborhood Aware Retrieval for Complex <fixed-case>QA</fixed-case></title>
      <author><first>Venktesh</first><last>V</last></author>
      <author><first>Mandeep</first><last>Rathee</last></author>
      <author><first>Avishek</first><last>Anand</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>5818-5835</pages>
      <abstract>Complex question-answering (QA) systems face significant challenges in retrieving and reasoning over information that addresses multifaceted queries. While large language models (LLMs) have advanced the reasoning capabilities of these systems, the bounded-recall problem persists, where procuring all relevant documents in first-stage retrieval remains a challenge. Missing pertinent documents at this stage leads to performance degradation that cannot be remedied in later stages, especially given the limited context windows of LLMs which necessitate high recall at smaller retrieval depths. In this paper, we introduce SUNAR, a novel approach that leverages LLMs to guide a Neighborhood Aware Retrieval process. SUNAR iteratively explores a neighborhood graph of documents, dynamically promoting or penalizing documents based on uncertainty estimates from interim LLM-generated answer candidates. We validate our approach through extensive experiments on two complex QA datasets. Our results show that SUNAR significantly outperforms existing retrieve-and-reason baselines, achieving up to a 31.84% improvement in performance over existing state-of-the-art methods for complex QA. Our code and data are anonymously available at https://anonymous.4open.science/r/SUNAR-8D36/.</abstract>
      <url hash="ea31eed8">2025.naacl-long.300</url>
      <bibkey>v-etal-2025-sunar</bibkey>
    </paper>
    <paper id="301">
      <title>Do <fixed-case>RAG</fixed-case> Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage</title>
      <author><first>Kaige</first><last>Xie</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft</affiliation></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>5836-5849</pages>
      <abstract>Evaluating retrieval-augmented generation (RAG) systems remains challenging, particularly for open-ended questions that lack definitive answers and require coverage of multiple sub-topics. In this paper, we introduce a novel evaluation framework based on sub-question coverage, which measures how well a RAG system addresses different facets of a question. We propose decomposing questions into sub-questions and classifying them into three types—core, background, and follow-up—to reflect their roles and importance. Using this categorization, we introduce a fine-grained evaluation protocol that provides insights into the retrieval and generation characteristics of RAG systems, including three commercial generative answer engines: You.com, Perplexity AI, and Bing Chat. Interestingly, we find that while all answer engines cover core sub-questions more often than background or follow-up ones, they still miss around 50% of core sub-questions, revealing clear opportunities for improvement. Further, sub-question coverage metrics prove effective for ranking responses, achieving 82% accuracy compared to human preference annotations. Lastly, we also demonstrate that leveraging core sub-questions enhances both retrieval and answer generation in a RAG system, resulting in a 74% win rate over the baseline that lacks sub-questions.</abstract>
      <url hash="30de07ee">2025.naacl-long.301</url>
      <bibkey>xie-etal-2025-rag</bibkey>
    </paper>
    <paper id="302">
      <title>Stronger Universal and Transferable Attacks by Suppressing Refusals</title>
      <author><first>David</first><last>Huang</last></author>
      <author><first>Avidan</first><last>Shah</last></author>
      <author><first>Alexandre</first><last>Araujo</last><affiliation>New York University</affiliation></author>
      <author><first>David</first><last>Wagner</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Chawin</first><last>Sitawarin</last></author>
      <pages>5850-5876</pages>
      <abstract>Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF), essentially embedding a “safety feature” into the model’s parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that such optimization-based attacks (unlike hand-crafted ones) are sample-specific. To make them universal and transferable, one has to incorporate multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial prompts discovered by such optimizers are inherently prompt-universal and transferable, even when optimized on a single model and a single harmful request. To further exploit this phenomenon, we introduce IRIS, a new objective to these optimizers to explicitly deactivate the safety feature to create an even stronger universal and transferable attack. Without requiring a large number of queries or accessing output token probabilities, our universal and transferable attack achieves a 25% success rate against the state-of-the-art Circuit Breaker defense (Zou et al., 2024), compared to 2.5% by white-box GCG. Crucially, IRIS also attains state-of-the-art transfer rates on frontier models: GPT-3.5-Turbo (90%), GPT-4o-mini (86%), GPT-4o (76%), o1-mini (54%), o1-preview (48%), o3-mini (66%), and deepseek-reasoner (90%).</abstract>
      <url hash="21c5f033">2025.naacl-long.302</url>
      <bibkey>huang-etal-2025-stronger</bibkey>
    </paper>
    <paper id="303">
      <title>The <fixed-case>B</fixed-case>i<fixed-case>GG</fixed-case>en Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models</title>
      <author><first>Seungone</first><last>Kim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Juyoung</first><last>Suk</last></author>
      <author><first>Ji Yong</first><last>Cho</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Chaeeun</first><last>Kim</last></author>
      <author><first>Dongkeun</first><last>Yoon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Guijin</first><last>Son</last></author>
      <author><first>Yejin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sheikh</first><last>Shafayat</last><affiliation>KAIST</affiliation></author>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sue Hyun</first><last>Park</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hyeonbin</first><last>Hwang</last></author>
      <author><first>Jinkyung</first><last>Jo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hyowon</first><last>Cho</last><affiliation>KAIST</affiliation></author>
      <author><first>Haebin</first><last>Shin</last></author>
      <author><first>Seongyun</first><last>Lee</last></author>
      <author><first>Hanseok</first><last>Oh</last></author>
      <author><first>Noah</first><last>Lee</last></author>
      <author><first>Namgyu</first><last>Ho</last></author>
      <author><first>Se June</first><last>Joo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Miyoung</first><last>Ko</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Yoonjoo</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Jamin</first><last>Shin</last><affiliation>NAVER</affiliation></author>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Sean</first><last>Welleck</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>LG Corporation and University of Illinois, Chicago</affiliation></author>
      <author><first>Kyungjae</first><last>Lee</last><affiliation>University of Seoul</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Config Intelligence</affiliation></author>
      <pages>5877-5919</pages>
      <abstract>As language models (LMs) become capable of handling a wide range of tasks, their evaluation is becoming as challenging as their development. Most generation benchmarks currently assess LMs using abstract evaluation criteria-like helpfulness and harmlessness-which often lack the flexibility and granularity of human assessment. Additionally, these benchmarks tend to focus disproportionately on specific capabilities such as instruction following, leading to coverage bias. To overcome these limitations, we introduce the BiGGen Bench, a principled generation benchmark designed to thoroughly evaluate nine distinct capabilities of LMs across 77 diverse tasks. A key feature of the BiGGen Bench is its use of instance-specific evaluation criteria, closely mirroring the nuanced discernment of human evaluation. We apply this benchmark to assess 100 frontier LMs using five evaluator LMs. Our code, data, and evaluation results are all publicly available at https://github.com/prometheus-eval/prometheus-eval.</abstract>
      <url hash="d926643b">2025.naacl-long.303</url>
      <bibkey>kim-etal-2025-biggen</bibkey>
    </paper>
    <paper id="304">
      <title><fixed-case>D</fixed-case>ream<fixed-case>S</fixed-case>ync: Aligning Text-to-Image Generation with Image Understanding Feedback</title>
      <author><first>Jiao</first><last>Sun</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Deqing</first><last>Fu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Yushi</first><last>Hu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Su</first><last>Wang</last><affiliation>Balyasny Asset Management</affiliation></author>
      <author><first>Royi</first><last>Rassin</last><affiliation>Research, Google and Bar-Ilan University</affiliation></author>
      <author><first>Da-Cheng</first><last>Juan</last><affiliation>Google Research</affiliation></author>
      <author><first>Dana</first><last>Alon</last><affiliation>Research, Google</affiliation></author>
      <author><first>Charles</first><last>Herrmann</last><affiliation>Google</affiliation></author>
      <author><first>Sjoerd Van</first><last>Steenkiste</last><affiliation>Google</affiliation></author>
      <author><first>Ranjay</first><last>Krishna</last><affiliation>University of Washington</affiliation></author>
      <author><first>Cyrus</first><last>Rashtchian</last><affiliation>Google Research</affiliation></author>
      <pages>5920-5945</pages>
      <abstract>Despite their widespread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user’s input text. We introduce DreamSync, a simple yet effective training algorithm that improves T2I models to be faithful to the text input. DreamSync utilizes large vision-language models (VLMs) to effectively identify the fine-grained discrepancies between generated images and the text inputs and enable T2I models to self-improve without labeled data. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation’s aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation, model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA aesthetic) and human evaluation shows that DreamSync improves text rendering compared to SDXL by 18.5% on DSG1K benchmark.</abstract>
      <url hash="8e01a5d3">2025.naacl-long.304</url>
      <bibkey>sun-etal-2025-dreamsync</bibkey>
    </paper>
    <paper id="305">
      <title>Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals</title>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Anahita</first><last>Bhiwandiwalla</last></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>5946-5991</pages>
      <abstract>With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images, producing over 57 million responses from popular models. Our multi-dimensional bias evaluation framework reveals that social attributes such as perceived race, gender, and physical characteristics depicted in images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of individuals.</abstract>
      <url hash="2dfef0bd">2025.naacl-long.305</url>
      <bibkey>howard-etal-2025-uncovering</bibkey>
    </paper>
    <paper id="306">
      <title><fixed-case>AEGIS</fixed-case>2.0: A Diverse <fixed-case>AI</fixed-case> Safety Dataset and Risks Taxonomy for Alignment of <fixed-case>LLM</fixed-case> Guardrails</title>
      <author><first>Shaona</first><last>Ghosh</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Prasoon</first><last>Varshney</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Makesh Narsimhan</first><last>Sreedhar</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Aishwarya</first><last>Padmakumar</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Traian</first><last>Rebedea</last><affiliation>NVIDIA and University Politehnica of Bucharest</affiliation></author>
      <author><first>Jibin Rajan</first><last>Varghese</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Christopher</first><last>Parisien</last></author>
      <pages>5992-6026</pages>
      <abstract>As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM “jury” system to assess the safety of responses we obtain Aegis2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets generated leveraging GPT-4. Additionally, we introduce a novel training blend that combines topic following data with safety data. This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis2.0 data and models to the research community to aid in safety guardrailing of LLMs.</abstract>
      <url hash="c34a2378">2025.naacl-long.306</url>
      <bibkey>ghosh-etal-2025-aegis2</bibkey>
    </paper>
    <paper id="307">
      <title><fixed-case>UOREX</fixed-case>: Towards Uncertainty-Aware Open Relation Extraction</title>
      <author><first>Rebii</first><last>Jamal</last></author>
      <author><first>Mounir</first><last>Ourekouch</last></author>
      <author><first>Mohammed</first><last>Erradi</last></author>
      <pages>6027-6040</pages>
      <abstract>Open relation extraction (OpenRE) aims to identify relational facts within open-domain corpora without relying on predefined relation types. A significant limitation of current state-of-the-art OpenRE approaches is their inability to accurately self-assess their performance. Which is caused by the reliance on pseudo-labels, that treats all points within a cluster equally, regardless of their actual relative position according to the cluster center. This leads to models that are often overconfident in their incorrect predictions , significantly undermining their reliability. In this paper, we introduce an approach that addresses this challenge by effectively modeling a part of the epistemic uncertainty within OpenRE. Instead of using pseudo labels that mask uncertainty, our approach is built to train a classifier directly with the clustering distribution. Our experimental results across various datasets demonstrate that the suggested approach improves reliability of OpenRE by preventing overconfident errors. Furthermore we show that by improving the reliability of the predictions, UOREX operates more efficiently in a generative active learning context where an LLM is the oracle, doubling the performance gain compared to the state-of-the-art.</abstract>
      <url hash="a21cffa3">2025.naacl-long.307</url>
      <bibkey>jamal-etal-2025-uorex</bibkey>
    </paper>
    <paper id="308">
      <title>Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training</title>
      <author><first>Yuchen</first><last>Zhuang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jingfeng</first><last>Yang</last><affiliation>Amazon</affiliation></author>
      <author><first>Haoming</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Kewei</first><last>Cheng</last></author>
      <author><first>Sanket</first><last>Lokegaonkar</last><affiliation>Amazon</affiliation></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Qing</first><last>Ping</last><affiliation>Amazon</affiliation></author>
      <author><first>Tianyi</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Binxuan</first><last>Huang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhengyang</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ruijie</first><last>Wang</last></author>
      <author><first>Rongzhi</first><last>Zhang</last><affiliation>Georgia Institute of Technology and Zhejiang University</affiliation></author>
      <author><first>Nasser</first><last>Zalmout</last><affiliation>Amazon</affiliation></author>
      <author><first>Priyanka</first><last>Nigam</last></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>6041-6068</pages>
      <abstract>Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.</abstract>
      <url hash="18e90239">2025.naacl-long.308</url>
      <bibkey>zhuang-etal-2025-hephaestus</bibkey>
    </paper>
    <paper id="309">
      <title><fixed-case>T</fixed-case>iny<fixed-case>T</fixed-case>hinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection</title>
      <author><first>Shengmin</first><last>Piao</last></author>
      <author><first>Sanghyun</first><last>Park</last><affiliation>Yonsei University</affiliation></author>
      <pages>6069-6087</pages>
      <abstract>Large Language Models exhibit impressive reasoning capabilities across diverse tasks, motivating efforts to distill these capabilities into smaller models through generated reasoning data. However, direct training on such synthesized reasoning data may lead to superficial imitation of reasoning process, rather than fostering a genuine integration of reasoning capabilities with underlying knowledge. To address this, we propose TinyThinker, a framework introducing two novel approaches. First, we introduce a three-stage process that incrementally guides the student model through the reasoning process, progressively refining knowledge from coarse to fine granularity. Second, we develop a two-phase training framework comprising an initial reasoning acquisition phase followed by a self-reflection phase utilizing self-generated data. Experiments on commonsense reasoning benchmarks demonstrate that TinyThinker achieves superior performance compared to baselines. Ablation studies further validate the effectiveness of each component in our framework. We expect that TinyThinker can be extended to other knowledge-intensive reasoning tasks, offering an alternative strategy for developing effective reasoning capabilities in smaller language models. Codes are available at https://github.com/shengminp/TinyThinker.</abstract>
      <url hash="0443a833">2025.naacl-long.309</url>
      <bibkey>piao-park-2025-tinythinker</bibkey>
    </paper>
    <paper id="310">
      <title><fixed-case>V</fixed-case>is<fixed-case>D</fixed-case>o<fixed-case>M</fixed-case>: Multi-Document <fixed-case>QA</fixed-case> with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation</title>
      <author><first>Manan</first><last>Suri</last></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Kanika</first><last>Goswami</last><affiliation>Indira Gandhi Delhi Technical University for Women</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>6088-6109</pages>
      <abstract>Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. A key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce a coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, we benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and long-context LLM baselines for end-to-end multimodal document QA by 12-20%.</abstract>
      <url hash="fb62f8a8">2025.naacl-long.310</url>
      <bibkey>suri-etal-2025-visdom</bibkey>
    </paper>
    <paper id="311">
      <title><fixed-case>VT</fixed-case>ech<fixed-case>AGP</fixed-case>: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models</title>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Jiaying</first><last>Gong</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Chenhan</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>William A</first><last>Ingram</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Edward</first><last>Fox</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Hoda</first><last>Eldardiry</last><affiliation>, Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>6110-6130</pages>
      <abstract>Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt generative language model, DSPT5. For training, we leverage a contrastive-generative loss function to learn the keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling decoding strategy at both semantic and structural levels to further select the best output candidate. We evaluate DSPT5 and various state-of-the-art large language models (LLMs) from multiple perspectives. Results demonstrate that the SOTA LLMs do not provide satisfactory outcomes, while the lightweight DSPT5 can achieve competitive results. To the best of our knowledge, we are the first to build a benchmark dataset and solutions for academic-to-general-audience text paraphrase dataset. Models will be public after acceptance.</abstract>
      <url hash="15eba61e">2025.naacl-long.311</url>
      <bibkey>cheng-etal-2025-vtechagp</bibkey>
    </paper>
    <paper id="312">
      <title>Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages</title>
      <author><first>Jannik</first><last>Brinkmann</last><affiliation>New York University and University of Mannheim</affiliation></author>
      <author><first>Chris</first><last>Wendler</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Christian</first><last>Bartelt</last><affiliation>Technische Universität Clausthal</affiliation></author>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology</affiliation></author>
      <pages>6131-6150</pages>
      <abstract>Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that ablating only multilingual features decreases classifier performance to near-chance across languages. We then use these features to precisely modify model behavior in a machine translation task; this demonstrates both the generality and selectivity of these feature’s roles in the network. Our findings suggest that even models trained predominantly on English data can develop robust, cross-lingual abstractions of morphosyntactic concepts.</abstract>
      <url hash="a806885d">2025.naacl-long.312</url>
      <bibkey>brinkmann-etal-2025-large</bibkey>
    </paper>
    <paper id="313">
      <title>Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts</title>
      <author><first>Weisi</first><last>Liu</last><affiliation>University of Memphis</affiliation></author>
      <author><first>Guangzeng</first><last>Han</last><affiliation>University of Memphis</affiliation></author>
      <author><first>Xiaolei</first><last>Huang</last><affiliation>University of Memphis</affiliation></author>
      <pages>6151-6166</pages>
      <abstract>Time is implicitly embedded in classification process: classifiers are usually built on existing data while to be applied on future data whose distributions (e.g., label and token) may change. However, existing state-of-the-art classification models merely consider the temporal variations and primarily focus on English corpora, which leaves temporal studies less explored, let alone under multilingual settings. In this study, we fill the gap by treating time as domains (e.g., 2024 vs. 2025), examining temporal effects, and developing a domain adaptation framework to generalize classifiers over time on four languages, English, Danish, French, and German. Our framework proposes Mixture of Temporal Experts (MoTE) to leverage both semantic and data distributional shifts to learn and adapt temporal trends into classification models. Our analysis shows classification performance varies over time across different languages, and we experimentally demonstrate that MoTE can enhance classifier generalizability over temporal data shifts. Our study provides analytic insights and addresses the need for time-aware models that perform robustly in multilingual scenarios.</abstract>
      <url hash="251adaf0">2025.naacl-long.313</url>
      <bibkey>liu-etal-2025-examining</bibkey>
    </paper>
    <paper id="314">
      <title><fixed-case>FLEURS</fixed-case>-<fixed-case>ASL</fixed-case>: Including <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage in Massively Multilingual Multitask Evaluation</title>
      <author><first>Garrett</first><last>Tanzer</last><affiliation>Google</affiliation></author>
      <pages>6167-6191</pages>
      <abstract>Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES (for text) and FLEURS (for speech) to support their first sign language (as video), American Sign Language, translated by 5 Certified Deaf Interpreters. FLEURS-ASL can be used to evaluate a variety of tasks—primarily sentence- and discourse-level translation—between ASL and 200 other languages as text, or 102 languages as speech. We provide baselines for tasks from ASL to English text using a unified modeling approach that incorporates timestamp tokens and previous text tokens in a 34-second context window, trained on random video clips from YouTube-ASL. This model meets or exceeds the performance of phrase-level baselines while supporting a multitude of new tasks. We also use FLEURS-ASL to show that multimodal frontier models have virtually no understanding of ASL, underscoring the importance of including sign languages in standard evaluation suites.</abstract>
      <url hash="f202766a">2025.naacl-long.314</url>
      <bibkey>tanzer-2025-fleurs</bibkey>
    </paper>
    <paper id="315">
      <title><fixed-case>E</fixed-case>vo<fixed-case>A</fixed-case>gent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms</title>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Kaitao</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiangjie</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>6192-6217</pages>
      <abstract>The rise of powerful large language models (LLMs) has spurred a new trend in building LLM-based autonomous agents for solving complex tasks, especially multi-agent systems. Despite the remarkable progress, we notice that existing works are heavily dependent on human-designed frameworks, which greatly limits the functional scope and scalability of agent systems. How to automatically extend the specialized agent to multi-agent systems to improve task-solving capability still remains a significant challenge. In this paper, we introduce EVOAGENT, a generic method to automatically extend specialized agents to multi-agent systems via the evolutionary algorithm, thereby improving the effectiveness of LLM-based agents in solving tasks. Specifically, we consider the existing agent frameworks as the initial individual and then apply a series of evolutionary operators (e.g., mutation, crossover, selection, etc.) to generate multiple agents with diverse settings. Experimental results across various tasks show that EVOAGENT can significantly enhance the tasksolving capability of LLM-based agents, and can be generalized to any LLM-based agent framework to extend them into multi-agent systems. Resources are available at https://evo-agent.github.io/.</abstract>
      <url hash="895652df">2025.naacl-long.315</url>
      <bibkey>yuan-etal-2025-evoagent</bibkey>
    </paper>
    <paper id="316">
      <title><fixed-case>E</fixed-case>mo<fixed-case>C</fixed-case>haracter: Evaluating the Emotional Fidelity of Role-Playing Agents in Dialogues</title>
      <author><first>Qiming</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qiujie</first><last>Xie</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaolong</first><last>Wang</last></author>
      <author><first>Qingqiu</first><last>Li</last></author>
      <author><first>Yuejie</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Shang</first><last>Gao</last><affiliation>Deakin University</affiliation></author>
      <pages>6218-6240</pages>
      <abstract>Role-playing agents (RPAs) powered by large language models (LLMs) have been widely utilized in dialogue systems for their capability to deliver personalized interactions. Current evaluations of RPAs mainly focus on personality fidelity, tone imitation, and knowledge consistency, while overlooking emotional fidelity, a key factor that affects user experience. To this end, we propose a benchmark called EmoCharacter to assess emotional fidelity of RPAs in dialogues. EmoCharacter includes two benchmark datasets (single-turn and multi-turn dialogues), three evaluation settings, and six metrics to measure the emotional fidelity between RPAs and the characters they portray. Based on EmoCharacter, we conduct extensive evaluations on RPAs powered by seven widely used LLMs with representative role-playing methods. Our empirical findings reveal that: (1) Contrary to intuition, current role-playing methods often reduce the emotional fidelity of LLMs in dialogues; (2) Enhancing the general capabilities of LLMs does not necessarily improve the emotional fidelity of RPAs; (3) Fine-tuning or In-Context Learning based on real dialogue data can enhance emotional fidelity.</abstract>
      <url hash="c1576a3a">2025.naacl-long.316</url>
      <bibkey>feng-etal-2025-emocharacter</bibkey>
    </paper>
    <paper id="317">
      <title>Language Models can Categorize System Inputs for Performance Analysis</title>
      <author><first>Dominic</first><last>Sobhani</last></author>
      <author><first>Ruiqi</first><last>Zhong</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last><affiliation>The Univesity of Tokyo and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>6241-6257</pages>
      <abstract>Language model systems are used to process diverse categories of input requests, ranging from improving creative writing to solving programming challenges. It would be useful to know which categories they are good at. However, existing evaluations compare model performance on pre-defined categories, failing to reflect a system’s performance on finer-grained or novel ones. We propose to automatically search for finer-grained categories based on inputs where a system performs well or poorly, and describe them in natural language. To search for these categories, we propose a large number of candidate category descriptions, e.g. “Communication Improvement”, find the subset of inputs that match the category descriptions, and calculate the performance on these categories; then we sort these categories based on their performance, thereby highlighting those that score high or low. As one application, we apply our method to compare LLaMA 3-70B and Claude 3 Opus, which have similar Elo-ratings on Chatbot Arena; our method finds the former is weaker at making text more professional and humorous while better at providing psychological insights, depicting a more nuanced picture of model performance.</abstract>
      <url hash="3e7cff7f">2025.naacl-long.317</url>
      <bibkey>sobhani-etal-2025-language</bibkey>
    </paper>
    <paper id="318">
      <title><fixed-case>F</fixed-case>in<fixed-case>E</fixed-case>val: A <fixed-case>C</fixed-case>hinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models</title>
      <author><first>Xin</first><last>Guo</last></author>
      <author><first>Haotian</first><last>Xia</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Zhaowei</first><last>Liu</last></author>
      <author><first>Hanyang</first><last>Cao</last></author>
      <author><first>Zhi</first><last>Yang</last></author>
      <author><first>Zhiqiang</first><last>Liu</last></author>
      <author><first>Sizhe</first><last>Wang</last></author>
      <author><first>Jinyi</first><last>Niu</last></author>
      <author><first>Chuqi</first><last>Wang</last></author>
      <author><first>Yanhui</first><last>Wang</last></author>
      <author><first>Xiaolong</first><last>Liang</last></author>
      <author><first>Xiaoming</first><last>Huang</last></author>
      <author><first>Bing</first><last>Zhu</last><affiliation>HSBC Lab</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Weining</first><last>Shen</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Liwen</first><last>Zhang</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <pages>6258-6292</pages>
      <abstract>Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs’ financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics. Financial Industry Knowledge contains 1,434 questions covering practical scenarios like investment research. Financial Security Knowledge assesses models through 1,640 questions on topics like application security and cryptography. Financial Agent evaluates tool usage and complex reasoning with 616 questions. FinEval has multiple evaluation settings, including zero-shot, five-shot with chain-of-thought, and assesses model performance using objective and subjective criteria. Our results show that Claude 3.5-Sonnet achieves the highest weighted average score of 72.9 across all financial domain categories under zero-shot setting. Our work provides a comprehensive benchmark closely aligned with Chinese financial domain. The data and the code are available at https://github.com/SUFE-AIFLMLab/FinEval.</abstract>
      <url hash="05ec9793">2025.naacl-long.318</url>
      <bibkey>guo-etal-2025-fineval</bibkey>
    </paper>
    <paper id="319">
      <title>Rethinking the Role of <fixed-case>LLM</fixed-case>s for Document-level Relation Extraction: a Refiner with Task Distribution and Probability Fusion</title>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xinlong</first><last>Jin</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Hongsen</first><last>Yu</last></author>
      <author><first>Huangming</first><last>Xu</last></author>
      <pages>6293-6312</pages>
      <abstract>Document-level relation extraction (DocRE) provides a broad context for extracting one or more relations for each entity pair. Large language models (LLMs) have made great progress in relation extraction tasks. However, one of the main challenges we face is that LLMs have difficulty in multi-label relation prediction tasks. Additionally, another noteworthy challenge and discovery we reveal: the small language models (SLMs) for DocRE tend to classify existing relations as ”no relation” (NA), while LLMs tend to predict existing relations for all entity pairs. To address these challenges, we propose a novel method that utilizes LLMs as a refiner, employing task distribution and probability fusion. The task distribution we carefully designed aims to distinguish hard and easy tasks, and feed hard tasks to our LLMs-based framework to reevaluate and refine. Further, in order to effectively solve the multi-label relation prediction problem in the refinement process, we propose a probability fusion method, ensuring and enhancing fusion predictions by maintaining a balance between SLMs and LLMs. Extensive experiments on widely-used datasets demonstrate that our method outperforms existing LLMbased methods without fine-tuning by an average of 25.2% F1. Refining SLMs using our method consistently boosts the performance of the SLMs, achieving new state-of-the-art results compared to existing SLMs and LLMs. Our code: https://github.com/Drasick/Drell.</abstract>
      <url hash="3dde5f76">2025.naacl-long.319</url>
      <bibkey>zhang-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="320">
      <title>Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?</title>
      <author><first>Qisheng</first><last>Hu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Quanyu</first><last>Long</last></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>6313-6336</pages>
      <abstract>Fact-checking pipelines increasingly adopt the Decompose-Then-Verify paradigm, where texts are broken down into smaller claims for individual verification and subsequently combined for a veracity decision. While decomposition is widely-adopted in such pipelines, its effects on final fact-checking performance remain underexplored. Some studies have reported improvements from decompostition, while others have observed performance declines, indicating its inconsistent impact. To date, no comprehensive analysis has been conducted to understand this variability. To address this gap, we present an in-depth analysis that explicitly examines the impact of decomposition on downstream verification performance. Through error case inspection and experiments, we introduce a categorization of decomposition errors and reveal a trade-off between accuracy gains and the noise introduced through decomposition. Our analysis provides new insights into understanding current system’s instability and offers guidance for future studies toward improving claim decomposition in fact-checking pipelines.</abstract>
      <url hash="40cf5511">2025.naacl-long.320</url>
      <bibkey>hu-etal-2025-decomposition</bibkey>
    </paper>
    <paper id="321">
      <title>Model Surgery: Modulating <fixed-case>LLM</fixed-case>’s Behavior Via Simple Parameter Editing</title>
      <author><first>Huanqian</first><last>Wang</last></author>
      <author><first>Yang</first><last>Yue</last></author>
      <author><first>Rui</first><last>Lu</last><affiliation>Department of Automation, Tsinghua University</affiliation></author>
      <author><first>Jingxin</first><last>Shi</last></author>
      <author><first>Andrew</first><last>Zhao</last></author>
      <author><first>Shenzhi</first><last>Wang</last></author>
      <author><first>Shiji</first><last>Song</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Gao</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6337-6357</pages>
      <abstract>Large Language Models (LLMs) have demonstrated great potential as generalist assistants, showcasing powerful task understanding and problem-solving capabilities. To deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts. Current approaches for detoxification or preventing jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), which requires finetuning billions of parameters through gradient descent with substantial computational cost. Furthermore, models modified through SFT and RLHF may deviate from the pretrained models, potentially leading to a degradation in foundational LLM capabilities. In this paper, we observe that surprisingly, directly editing a small subset of parameters can effectively modulate specific behaviors of LLMs, such as detoxification and resistance to jailbreaking, with only inference-level computational resources. Experiments demonstrate that in the detoxification task, our approach achieves reductions of up to 90.0% in toxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while maintaining the LLM’s general capabilities in areas such as common sense, question answering, and mathematics.</abstract>
      <url hash="ae5abe73">2025.naacl-long.321</url>
      <bibkey>wang-etal-2025-model</bibkey>
    </paper>
    <paper id="322">
      <title>Effective Skill Unlearning through Intervention and Abstention</title>
      <author><first>Yongce</first><last>Li</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chung-En</first><last>Sun</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tsui-Wei</first><last>Weng</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>6358-6371</pages>
      <url hash="ba429ee0">2025.naacl-long.322</url>
      <bibkey>li-etal-2025-effective</bibkey>
    </paper>
    <paper id="323">
      <title><fixed-case>C</fixed-case>haracter<fixed-case>B</fixed-case>ox: Evaluating the Role-Playing Capabilities of <fixed-case>LLM</fixed-case>s in Text-Based Virtual Worlds</title>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Yi</first><last>Huang</last></author>
      <author><first>Yanqi</first><last>Dai</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Haoxuan</first><last>Li</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>6372-6391</pages>
      <abstract>Role-playing is a crucial capability of Large Language Models (LLMs), enabling a wide range of practical applications, including intelligent non-player characters, digital twins, and emotional companions. Evaluating this capability in LLMs is challenging due to the complex dynamics involved in role-playing, such as maintaining character fidelity throughout a storyline and navigating open-ended narratives without a definitive ground truth. Current evaluation methods, which primarily focus on question-answering or conversational snapshots, fall short of adequately capturing the nuanced character traits and behaviors essential for authentic role-playing. In this paper, we propose CharacterBox, which is a simulation sandbox designed to generate situational fine-grained character behavior trajectories. These behavior trajectories enable a more comprehensive and in-depth evaluation of role-playing capabilities. CharacterBox consists of two main components: the character agent and the narrator agent. The character agent, grounded in psychological and behavioral science, exhibits human-like behaviors, while the narrator agent coordinates interactions between character agents and environmental changes. Additionally, we introduce two trajectory-based methods that leverage CharacterBox to enhance LLM performance. To reduce costs and facilitate the adoption of CharacterBox by public communities, we fine-tune two smaller models, CharacterNR and CharacterRM, as substitutes for GPT API calls, and demonstrate their competitive performance compared to advanced GPT APIs. The code is available at https://github.com/Paitesanshi/CharacterBox.</abstract>
      <url hash="71d43acc">2025.naacl-long.323</url>
      <bibkey>wang-etal-2025-characterbox</bibkey>
    </paper>
    <paper id="324">
      <title>A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models</title>
      <author><first>Xiujie</first><last>Song</last></author>
      <author><first>Mengyue</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Kenny Q.</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Chunhao</first><last>Zhang</last></author>
      <author><first>Yanyi</first><last>Chen</last></author>
      <pages>6392-6409</pages>
      <abstract>Large Vision-Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the Cookie Theft task in human cognitive tests, we propose a novel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs using images with rich semantics. The benchmark consists of 251 images along with comprehensive annotations. It defines eight reasoning capabilities and comprises an image description task and a visual question answering task. Our evaluation of well-known LVLMs shows that there is still a significant gap in cognitive abilities between LVLMs and humans.</abstract>
      <url hash="a979d6b8">2025.naacl-long.324</url>
      <bibkey>song-etal-2025-cognitive</bibkey>
    </paper>
    <paper id="325">
      <title><fixed-case>C</fixed-case>o<fixed-case>ME</fixed-case>: An Unlearning-based Approach to Conflict-free Model Editing</title>
      <author><first>Dahyun</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>6410-6422</pages>
      <abstract>Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model’s generative performance.</abstract>
      <url hash="420e7e50">2025.naacl-long.325</url>
      <bibkey>jung-etal-2025-come</bibkey>
    </paper>
    <paper id="326">
      <title>On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena</title>
      <author><first>Tarek</first><last>Naous</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>6423-6443</pages>
      <abstract>Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English compared to Arabic. We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses. This also extends to entities that exhibit high lexical overlap with languages that are not Arabic but use the Arabic script. Further, we show how frequency-based tokenization leads to this issue in LMs, which gets worse with larger Arabic vocabularies. We will make CAMeL-2 available at: https://github.com/tareknaous/camel2</abstract>
      <url hash="542864b8">2025.naacl-long.326</url>
      <bibkey>naous-xu-2025-origin</bibkey>
    </paper>
    <paper id="327">
      <title>Adapting Sentence-level Automatic Metrics for Document-level Simplification Evaluation</title>
      <author><first>Mounica</first><last>Maddela</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Fernando</first><last>Alva-Manchego</last><affiliation>Cardiff University</affiliation></author>
      <pages>6444-6459</pages>
      <abstract>Text simplification aims to enhance the clarity and comprehensibility of a complex text while preserving its original meaning. Previous research on the automatic evaluation of text simplification has primarily focused on sentence simplification, with commonly used metrics such as SARI and advanced metrics such as LENS being trained and evaluated at the sentence level. However, these metrics often underperform on longer texts. In our study, we propose a novel approach to adapt existing sentence-level metrics for paragraph- or document-level simplification. We benchmark our approach against a wide variety of existing reference-based and reference-less metrics across multiple domains. Empirical results demonstrate that our approach outperforms traditional sentence-level metrics in terms of correlation with human judgment. Furthermore, we evaluate the sensitivity and robustness of various metrics to different types of errors produced by existing text simplification systems.</abstract>
      <url hash="763293f5">2025.naacl-long.327</url>
      <bibkey>maddela-alva-manchego-2025-adapting</bibkey>
    </paper>
    <paper id="328">
      <title>Decoding Speculative Decoding</title>
      <author><first>Minghao</first><last>Yan</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Saurabh</first><last>Agarwal</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Shivaram</first><last>Venkataraman</last><affiliation>Microsoft and University of Wisconsin, Madison</affiliation></author>
      <pages>6460-6473</pages>
      <abstract>Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model’s capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models and our approach generalizes further to all LLaMA models (1/2/3.1) and supervised fine-tuned models.</abstract>
      <url hash="6723554e">2025.naacl-long.328</url>
      <bibkey>yan-etal-2025-decoding</bibkey>
    </paper>
    <paper id="329">
      <title>Leveraging <fixed-case>LLM</fixed-case> For Synchronizing Information Across Multilingual Tables</title>
      <author><first>Siddharth</first><last>Khincha</last></author>
      <author><first>Tushar</first><last>Kataria</last><affiliation>University of Utah</affiliation></author>
      <author><first>Ankita</first><last>Anand</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>6474-6492</pages>
      <abstract>The vast amount of online information today poses challenges for non-English speakers, as much of it is concentrated in high-resource languages such as English and French. Wikipedia reflects this imbalance, with content in low-resource languages frequently outdated or incomplete. Recent research has sought to improve cross-language synchronization of Wikipedia tables using rule-based methods. These approaches can be effective, but they struggle with complexity and generalization. This paper explores large language models (LLMs) for multilingual information synchronization, using zero-shot prompting as a scalable solution. We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance. Our findings reveal that single-prompt approaches often produce suboptimal results, prompting us to introduce a task decomposition strategy that enhances coherence and accuracy. Our proposed method outperforms existing baselines, particularly in Information Updation (1.79%) and Information Addition (20.58%), highlighting the model’s strength in dynamically updating and enriching data across architectures.</abstract>
      <url hash="078cf280">2025.naacl-long.329</url>
      <bibkey>khincha-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="330">
      <title><fixed-case>C</fixed-case>on<fixed-case>M</fixed-case>e<fixed-case>C</fixed-case>: A Dataset for Metonymy Resolution with Common Nouns</title>
      <author><first>Saptarshi</first><last>Ghosh</last></author>
      <author><first>Tianyu</first><last>Jiang</last><affiliation>University of Cincinnati</affiliation></author>
      <pages>6493-6509</pages>
      <abstract>Metonymy plays an important role in our daily communication. People naturally think about things using their most salient properties or commonly related concepts. For example, by saying “The bus decided to skip our stop today,” we actually mean that the bus driver made the decision, not the bus. Prior work on metonymy resolution has mainly focused on named entities. However, metonymy involving common nouns (such as desk, baby, and school) is also a frequent and challenging phenomenon. We argue that NLP systems should be capable of identifying the metonymic use of common nouns in context. We create a new metonymy dataset ConMeC, which consists of 6,000 sentences, where each sentence is paired with a target common noun and annotated by humans to indicate whether that common noun is used metonymically or not in that context. We also introduce a chain-of-thought based prompting method for detecting metonymy using large language models (LLMs). We evaluate our LLM-based pipeline, as well as a supervised BERT model on our dataset and three other metonymy datasets. Our experimental results demonstrate that LLMs could achieve performance comparable to the supervised BERT model on well-defined metonymy categories, while still struggling with instances requiring nuanced semantic understanding. Our dataset is publicly available at: https://github.com/SaptGhosh/ConMeC.</abstract>
      <url hash="607bb494">2025.naacl-long.330</url>
      <bibkey>ghosh-jiang-2025-conmec</bibkey>
    </paper>
    <paper id="331">
      <title>Self-<fixed-case>DC</fixed-case>: When to Reason and When to Act? Self Divide-and-Conquer for Compositional Unknown Questions</title>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Baohang</first><last>Zhou</last><affiliation>Nankai University</affiliation></author>
      <author><first>Tianhua</first><last>Zhang</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Huimin</first><last>Wang</last><affiliation>Jarvis Research Center, Tencent YouTu Lab</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6510-6525</pages>
      <abstract>Previous research has typically concentrated on leveraging the internal knowledge of Large Language Models (LLMs) to answer known questions (i.e., internal reasoning such as generate-then-read). In contrast, for questions that fall outside their known scope, these models rely on external knowledge retrieval to provide accurate responses (i.e., external acting such as retrieve-then-read). However, few previous works consider the <i>compositional questions</i>, which consist of several known and unknown sub-questions, necessitating the dynamic combination of previous two methods (i.e., <i>internal reasoning and external acting</i>) to achieve a better trade-off between effectiveness and efficiency. To this end, we introduce a <b>Self</b> <b>D</b>ivide-and-<b>C</b>onquer (<i>Self-DC</i>) framework, accompanying with the first <b>C</b>ompositional <b>u</b>nknown <b>Q</b>uestion-<b>A</b>nswering dataset (CuQA). This framework enables LLMs to adaptively choose between using internal knowledge and retrieving external knowledge as needed, resulting in a better trade-off between effectiveness and efficiency. Experimental results on two datasets demonstrate that <i>Self-DC</i> can achieve comparable or even better performance with much fewer external calls compared with several strong baselines.</abstract>
      <url hash="b75ae84c">2025.naacl-long.331</url>
      <bibkey>wang-etal-2025-self</bibkey>
    </paper>
    <paper id="332">
      <title><fixed-case>TRANSIENTTABLES</fixed-case>: Evaluating <fixed-case>LLM</fixed-case>s’ Reasoning on Temporally Evolving Semi-structured Tables</title>
      <author><first>Abhilash</first><last>Shankarampeta</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Harsh</first><last>Mahajan</last></author>
      <author><first>Tushar</first><last>Kataria</last><affiliation>University of Utah</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>6526-6544</pages>
      <abstract>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance.</abstract>
      <url hash="978b1007">2025.naacl-long.332</url>
      <bibkey>shankarampeta-etal-2025-transienttables</bibkey>
    </paper>
    <paper id="333">
      <title><fixed-case>A</fixed-case>dvisor<fixed-case>QA</fixed-case>: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence</title>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>Sogang University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>6545-6565</pages>
      <abstract>As the integration of large language models into daily life is on the rise, there is still a lack of dataset for *advising on subjective and personal dilemmas*. To address this gap, we introduce AdvisorQA, which aims to improve LLMs’ capability to offer advice for deeply subjective concerns, utilizing the LifeProTips Reddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a *collective intelligence*. Therefore, we’ve completed a dataset encompassing daily life questions, diverse corresponding responses, and majority vote ranking, which we use to train a helpfulness metric. In baseline experiments, models aligned with AdvisorQA dataset demonstrated improved helpfulness through our automatic metric, as well as GPT-4 and human evaluations. Additionally, we expanded the independent evaluation axis to include harmlessness. AdvisorQA marks a significant leap in enhancing QA systems to provide subjective, helpful, and harmless advice, showcasing LLMs’ improved understanding of human subjectivity.</abstract>
      <url hash="cb04fef1">2025.naacl-long.333</url>
      <bibkey>kim-etal-2025-advisorqa</bibkey>
    </paper>
    <paper id="334">
      <title>t<fixed-case>RAG</fixed-case>: Term-level Retrieval-Augmented Generation for Domain-Adaptive Retrieval</title>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jongyoon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jihyuk</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <pages>6566-6578</pages>
      <abstract>Neural retrieval models have emerged as an effective tool for information retrieval, but their performance suffers when there is a domain shift between training and test data distributions. Recent work aims to construct pseudo-training data for the target domain by generating domain-adapted pseudo-queries using large language models (LLMs). However, we identifies that LLMs exhibit a “seen term bias” where the generated pseudo-queries fail to include relevant “unseen” terms as expected for domain adaptation purposes. To address this limitation, we propose to improve the term recall of unseen query terms, by using term-level Retrieval-Augmented Generation (tRAG). Specifically, unlike existing document-level RAG, we propose to generate domain-specific keywords from all documents in the corpus, including those unseen in any individual document. To filter hallucination, generated keywords are retrieved and reranked, leveraging relevance feedback from both retrievers and LLMs. Experiments on the BEIR benchmark show tRAG significantly improves recall for unseen terms by 10.6% and outperforms LLM and retrieval-augmented generation baselines on overall retrieval performance.</abstract>
      <url hash="97438fc7">2025.naacl-long.334</url>
      <bibkey>lee-etal-2025-trag</bibkey>
    </paper>
    <paper id="335">
      <title><fixed-case>JRE</fixed-case>-<fixed-case>L</fixed-case>: Journalist, Reader, and Editor <fixed-case>LLM</fixed-case>s in the Loop for Science Journalism for the General Audience</title>
      <author><first>Gongyao</first><last>Jiang</last></author>
      <author><first>Xinran</first><last>Shi</last></author>
      <author><first>Qiong</first><last>Luo</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>6579-6594</pages>
      <abstract>Science journalism reports current scientific discoveries to non-specialists, aiming to enable public comprehension of the state of the art. This task is challenging as the audience often lacks specific knowledge about the presented research. We propose JRE-L, a framework that integrates three LLMs mimicking the writing-reading-feedback-revision loop. In JRE-L, one LLM acts as the journalist, another LLM as the general public reader, and the third LLM as an editor. The journalist’s writing is iteratively refined by feedback from the reader and suggestions from the editor. Our experiments demonstrate that by leveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can generate articles that are more accessible than those generated by existing methods, including prompting single advanced models such as GPT-4 and other LLM-collaboration strategies. Our code is publicly available at github.com/Zzoay/JRE-L.</abstract>
      <url hash="95232693">2025.naacl-long.335</url>
      <bibkey>jiang-etal-2025-jre</bibkey>
    </paper>
    <paper id="336">
      <title>Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models</title>
      <author><first>Ziche</first><last>Liu</last></author>
      <author><first>Rui</first><last>Ke</last></author>
      <author><first>Yajiao</first><last>Liu</last></author>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>6595-6611</pages>
      <abstract>Data selection for fine-tuning large language models (LLMs) aims to choose a high-quality subset from existing datasets, allowing the trained model to outperform baselines trained on the full dataset. However, the expanding body of research lacks a clear, unified framework, and the variability in experimental settings complicates systematic comparisons.While existing surveys comprehensively overview the stages and methods of data selection, they often overlook an in-depth exploration of the fine-tuning phase. In this paper, we conduct a focused review of recent data selection techniques for fine-tuning LLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme—comprising feature extraction, criteria design, and selector evaluation—to systematically categorize and evaluate these methods. Additionally, we propose a unified comparison approach that incorporates ratio-based efficiency and ranking-based feasibility metrics to address inconsistencies across experiments. Our findings reveal that methods emphasizing more targeted quality measurement achieve higher efficiency but at the cost of feasibility. Finally, we discuss trends and highlight four key challenges in fine-tuning data selection, offering potential directions for future research.</abstract>
      <url hash="9f2c0f02">2025.naacl-long.336</url>
      <bibkey>liu-etal-2025-take</bibkey>
    </paper>
    <paper id="337">
      <title>Graph Neural Network Enhanced Retrieval for Question Answering of Large Language Models</title>
      <author><first>Zijian</first><last>Li</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qingyan</first><last>Guo</last></author>
      <author><first>Jiawei</first><last>Shao</last><affiliation>China Telecom</affiliation></author>
      <author><first>Lei</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jun</first><last>Zhang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <pages>6612-6633</pages>
      <abstract>Retrieval augmented generation has revolutionized large language model (LLM) outputs by providing factual supports. Nevertheless, it struggles to capture all the necessary knowledge for complex reasoning questions. Existing retrieval methods typically divide reference documents into passages, treating them in isolation. These passages, however, are often interrelated, such as passages that are contiguous or share the same keywords. Therefore, it is crucial to recognize such relatedness for enhancing the retrieval process. In this paper, we propose a novel retrieval method, called GNN-Ret, which leverages graph neural networks (GNNs) to enhance retrieval by exploiting the relatedness between passages. Specifically, we first construct a graph of passages by connecting passages that are structure-related or keyword-related. A graph neural network (GNN) is then leveraged to exploit the relationships between passages and improve the retrieval of supporting passages. Furthermore, we extend our method to handle multi-hop reasoning questions using a recurrent graph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates the graphs of passages from previous steps, thereby enhancing the retrieval of supporting passages. Extensive experiments on benchmark datasets demonstrate that GNN-Ret achieves higher accuracy for question answering with a single query of LLMs than strong baselines that require multiple queries, and RGNN-Ret further improves accuracy and achieves state-of-the-art performance, with up to 10.4 accuracy improvement on the 2WikiMQA dataset.</abstract>
      <url hash="475030fe">2025.naacl-long.337</url>
      <bibkey>li-etal-2025-graph</bibkey>
    </paper>
    <paper id="338">
      <title>Pula: Training Large Language Models for Setswana</title>
      <author><first>Nathan</first><last>Brown</last></author>
      <author><first>Vukosi</first><last>Marivate</last><affiliation>University of Pretoria</affiliation></author>
      <pages>6634-6656</pages>
      <abstract>In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic LLM-generated text. To accompany this data, we release the code used for dataset construction, formatting, filtering, and scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.</abstract>
      <url hash="47eb7882">2025.naacl-long.338</url>
      <bibkey>brown-marivate-2025-pula</bibkey>
    </paper>
    <paper id="339">
      <title><fixed-case>L</fixed-case>egal<fixed-case>V</fixed-case>iz: Legal Text Visualization by Text To Diagram Generation</title>
      <author><first>Eri</first><last>Onami</last></author>
      <author><first>Taiki</first><last>Miyanishi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Koki</first><last>Maeda</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Shuhei</first><last>Kurita</last><affiliation>National Institute of Informatics and New York University</affiliation></author>
      <pages>6657-6676</pages>
      <abstract>Legal documents including judgments and court orders require highly sophisticated legal knowledge for understanding. To disclose expert knowledge for non-experts, we explore the problem of visualizing legal texts with easy-to-understand diagrams and propose a novel dataset of LegalViz with 23 languages and 7,010 cases of legal document and visualization pairs, using the DOT graph description language of Graphviz. LegalViz provides a simple diagram from a complicated legal corpus identifying legal entities, transactions, legal sources, and statements at a glance, that are essential in each judgment. In addition, we provide new evaluation metrics for the legal diagram visualization by considering graph structures, textual similarities, and legal contents. We conducted empirical studies on few-shot and finetuning large language models for generating legal diagrams and evaluated them with these metrics, including legal content-based evaluation within 23 languages. Models trained with LegalViz outperform existing models including GPTs, confirming the effectiveness of our dataset.</abstract>
      <url hash="f7cf6e68">2025.naacl-long.339</url>
      <bibkey>onami-etal-2025-legalviz</bibkey>
    </paper>
    <paper id="340">
      <title>Active Few-Shot Learning for Text Classification</title>
      <author><first>Saeed</first><last>Ahmadnia</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Arash</first><last>Yousefi Jordehi</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Mahsa</first><last>Hosseini Khasheh Heyran</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Seyed Abolghasem</first><last>Mirroshandel</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>6677-6694</pages>
      <abstract>The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers consistent performance improvement even when more support samples are added. To address this challenge, we propose an active learning-based instance selection mechanism that identifies effective support instances from the unlabeled pool and can work with different LLMs. Our experiments on five tasks show that our method frequently improves the performance of FSL. We make our implementation available on GitHub.</abstract>
      <url hash="0484bc3a">2025.naacl-long.340</url>
      <bibkey>ahmadnia-etal-2025-active</bibkey>
    </paper>
    <paper id="341">
      <title>Enhancing Multimodal Entity Linking with <fixed-case>J</fixed-case>accard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation</title>
      <author><first>Cong-Duy T</first><last>Nguyen</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Thong Thanh</first><last>Nguyen</last></author>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Khoi M.</first><last>Le</last></author>
      <author><first>Nguyen Viet</first><last>Anh</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Feng</first><last>Yichao</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>6695-6708</pages>
      <abstract>Previous research on multimodal entity linking (MEL) has primarily employed contrastive learning as the primary objective. However, using the rest of the batch as negative samples without careful consideration, these studies risk leveraging easy features and potentially overlook essential details that make entities unique. In this work, we propose JD-CCL (Jaccard Distance-based Conditional Contrastive Learning), a novel approach designed to enhance the ability to match multimodal entity linking models. JD-CCL leverages meta-information to select negative samples with similar attributes, making the linking task more challenging and robust. Additionally, to address the limitations caused by the variations within the visual modality among mentions and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid Controllable Patch Transform). It enhances visual representations by incorporating multi-view synthetic images and contextual textual representations to scale and shift patch representations. Experimental results on benchmark MEL datasets demonstrate the strong effectiveness of our approach.</abstract>
      <url hash="a22c4bda">2025.naacl-long.341</url>
      <bibkey>nguyen-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="342">
      <title><fixed-case>R</fixed-case>esearch<fixed-case>A</fixed-case>gent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</title>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sujay Kumar</first><last>Jauhar</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Silviu</first><last>Cucerzan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <pages>6709-6738</pages>
      <abstract>The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.</abstract>
      <url hash="b104c399">2025.naacl-long.342</url>
      <bibkey>baek-etal-2025-researchagent</bibkey>
    </paper>
    <paper id="343">
      <title>Logit Separability-Driven Samples and Multiple Class-Related Words Selection for Advancing In-Context Learning</title>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Junlang</first><last>Qian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>6739-6759</pages>
      <abstract>Effective organization of in-context learning (ICL) demonstrations is key to improving the quality of large language model (LLM) responses. To create better sample-label pairs that instruct LLM understanding, we introduce logit separability, a criterion to assess the clarity of both samples and class-related words at the logit level. This facilitates the optimization of sample and label selection, enhancing the precision of information provided in ICL demonstrations. Additionally, we find that incorporating multiple class-related words for each sample, rather than relying on a single class name, improves performance by offering a broader range of label information. Building on these insights, we propose LICL, a logit separability-based method that jointly organizes samples and integrates multiple class-related words into each sample-label pair. Evaluations across seven classification datasets show that this approach significantly improves ICL performance by providing clearer instructions and richer label information.</abstract>
      <url hash="8bc3945c">2025.naacl-long.343</url>
      <bibkey>zhu-etal-2025-logit</bibkey>
    </paper>
    <paper id="344">
      <title>Identifying Emerging Concepts in Large Corpora</title>
      <author><first>Sibo</first><last>Ma</last><affiliation>Stanford University</affiliation></author>
      <author><first>Julian</first><last>Nyarko</last><affiliation>Stanford University</affiliation></author>
      <pages>6760-6778</pages>
      <abstract>We introduce a new method to identify emerging concepts in large text corpora. By analyzing changes in the heatmaps of the underlying embedding space, we are able to detect these concepts with high accuracy shortly after they originate, in turn outperforming common alternatives. We further demonstrate the utility of our approach by analyzing speeches in the U.S. Senate from 1941 to 2015. Our results suggest that the minority party is more active in introducing new concepts into the Senate discourse. We also identify specific concepts that closely correlate with the Senators’ racial, ethnic, and gender identities. An implementation of our method is publicly available.</abstract>
      <url hash="ea3331f4">2025.naacl-long.344</url>
      <bibkey>ma-nyarko-2025-identifying</bibkey>
    </paper>
    <paper id="345">
      <title><fixed-case>C</fixed-case>ode<fixed-case>SCM</fixed-case>: Causal Analysis for Multi-Modal Code Generation</title>
      <author><first>Mukur</first><last>Gupta</last></author>
      <author><first>Noopur</first><last>Bhatt</last></author>
      <author><first>Suman</first><last>Jana</last><affiliation>Columbia University</affiliation></author>
      <pages>6779-6793</pages>
      <abstract>In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model’s spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly influence code generation.</abstract>
      <url hash="ca322fe1">2025.naacl-long.345</url>
      <bibkey>gupta-etal-2025-codescm</bibkey>
    </paper>
    <paper id="346">
      <title>From Distributional to Overton Pluralism: Investigating Large Language Model Alignment</title>
      <author><first>Thom</first><last>Lake</last><affiliation>University of Texas at Austin and Indeed</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <author><first>Greg</first><last>Durrett</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>6794-6814</pages>
      <abstract>The alignment process changes several properties of a large language model’s (LLM’s) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at [github.com/thomlake/investigating-alignment](https://github.com/thomlake/investigating-alignment).</abstract>
      <url hash="7d0bbfee">2025.naacl-long.346</url>
      <bibkey>lake-etal-2025-distributional</bibkey>
    </paper>
    <paper id="347">
      <title>Advancing <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> Efficiency: A Collaboration-Constrained Routing (<tex-math>\texttt{C2R}</tex-math>) Strategy for Better Expert Parallelism Design</title>
      <author><first>Mohan</first><last>Zhang</last></author>
      <author><first>Pingzhi</first><last>Li</last></author>
      <author><first>Jie</first><last>Peng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Mufan</first><last>Qiu</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>6815-6825</pages>
      <url hash="7e0cd1ea">2025.naacl-long.347</url>
      <bibkey>zhang-etal-2025-advancing</bibkey>
    </paper>
    <paper id="348">
      <title><fixed-case>L</fixed-case>ib<fixed-case>E</fixed-case>volution<fixed-case>E</fixed-case>val: A Benchmark and Study for Version-Specific Code Generation</title>
      <author><first>Sachit</first><last>Kuhar</last><affiliation>Amazon</affiliation></author>
      <author><first>Wasi Uddin</first><last>Ahmad</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Zijian</first><last>Wang</last><affiliation>Amazon AWS AI Labs</affiliation></author>
      <author><first>Nihal</first><last>Jain</last><affiliation>Amazon</affiliation></author>
      <author><first>Haifeng</first><last>Qian</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Baishakhi</first><last>Ray</last><affiliation>Columbia University</affiliation></author>
      <author><first>Murali Krishna</first><last>Ramanathan</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaofei</first><last>Ma</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Anoop</first><last>Deoras</last></author>
      <pages>6826-6840</pages>
      <abstract>Recent advancements in code completion models have primarily focused on local file contexts. However, these studies do not fully capture the complexity of real-world software development, which often requires the use of rapidly-evolving public libraries. To address this gap, we introduce LibEvolutionEval, a comprehensive study that emphasizes the need to understand library evolution to perform accurate in-line code completions. LibEvolutionEvaloffers a version-specific code-completion task across eight libraries as they evolve over the years, along with an in-depth analysis of the evolution of two widely used and well-maintained public libraries: PyTorch and Matplotlib. We evaluate several popular models and find that public library evolution significantly affects their performance. To mitigate this, we explored how retrieving version-specific library documentation and prompt-based techniques can enhance model capability in dealing with these fast-evolving packages. This suggests a promising path forward for better handling fast-evolving libraries. Our tasks will be made publicly available upon acceptance.</abstract>
      <url hash="d59a27db">2025.naacl-long.348</url>
      <bibkey>kuhar-etal-2025-libevolutioneval</bibkey>
    </paper>
    <paper id="349">
      <title>Evaluating and Mitigating Object Hallucination in Large Vision-Language Models: Can They Still See Removed Objects?</title>
      <author><first>Yixiao</first><last>He</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Pengfei</first><last>Ren</last></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <author><first>Huazheng</first><last>Wang</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zirui</first><last>Zhuang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jing</first><last>Wang</last></author>
      <pages>6841-6858</pages>
      <abstract>Large Vision-Language Models (LVLMs) have a significant issue with object hallucinations, where researchers have noted that LVLMs often mistakenly determine objects as present in images where they do not actually exist. Some recent studies evaluate the occurrence of object hallucinations by asking LVLMs whether they see objects that do not exist in input images. However, we observe that these evaluation methods have some limitations, such as the objects being questioned potentially having little relevance to the image. In this paper, we introduce a more challenging benchmark for evaluating object hallucinations by removing objects from images and then asking the model whether it can still see the removed objects. Our evaluation result reveals that LVLMs suffer from severe hallucinations, as they often still claim to see the removed objects. Through our analysis, we find that biases in training result in LVLMs lacking guidance on learning about the absence of objects, which in turn leads to a lack of ability to determine that objects do not exist in images. To address this issue, we further propose oDPO, a direct preference optimization objective based on visual objects. By guiding LVLMs to learn to determine the existence of objects, oDPO effectively alleviates object hallucinations. It achieves more competitive results than other hallucination mitigation approaches across multiple object hallucination benchmarks and enhances the performance of LVLMs in various vision-language tasks.</abstract>
      <url hash="a4fa4593">2025.naacl-long.349</url>
      <bibkey>he-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="350">
      <title>Self-Pluralising Culture Alignment for Large Language Models</title>
      <author><first>Shaoyang</first><last>Xu</last></author>
      <author><first>Yongqi</first><last>Leng</last></author>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>6859-6877</pages>
      <abstract>As large language models (LLMs) become increasingly accessible in many countries, it is essential to align them to serve pluralistic human values across cultures. However, pluralistic culture alignment in LLMs remain an open problem. In this paper, we propose CultureSPA, a Self-Pluralising Culture Alignment framework that allows LLMs to simultaneously align to pluralistic cultures. The framework first generates questions on various culture topics, then yields LLM outputs in response to these generated questions under both culture-aware and culture-unaware settings. By comparing culture-aware/unaware outputs, we are able to detect and collect culture-related instances. These instances are employed to fine-tune LLMs to serve pluralistic cultures in either a culture-joint or culture-specific way. Extensive experiments demonstrate that CultureSPA significantly improves the alignment of LLMs to diverse cultures without compromising general abilities. And further improvements can be achieved if CultureSPA is combined with advanced prompt engineering techniques. Comparisons between culture-joint and culture-specific tuning strategies, along with variations in data quality and quantity, illustrate the robustness of our method. We also explore the mechanisms underlying CultureSPA and the relations between different cultures it reflects.</abstract>
      <url hash="8165a539">2025.naacl-long.350</url>
      <bibkey>xu-etal-2025-self</bibkey>
    </paper>
    <paper id="351">
      <title>K-<fixed-case>COMP</fixed-case>: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor</title>
      <author><first>Jeonghun</first><last>Cho</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>6878-6901</pages>
      <url hash="80af280b">2025.naacl-long.351</url>
      <bibkey>cho-lee-2025-k</bibkey>
    </paper>
    <paper id="352">
      <title><fixed-case>D</fixed-case>raw<fixed-case>E</fixed-case>du<fixed-case>M</fixed-case>ath: Evaluating Vision Language Models with Expert-Annotated Students’ Hand-Drawn Math Images</title>
      <author><first>Sami</first><last>Baral</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>Li</first><last>Lucy</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Ryan</first><last>Knight</last><affiliation>Insource Services, Inc</affiliation></author>
      <author><first>Alice</first><last>Ng</last></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Neil</first><last>Heffernan</last></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>6902-6920</pages>
      <abstract>In real-world settings, vision language models (VLMs) should robustly handle naturalistic, noisy visual content as well as domain-specific language and concepts. For example, K-12 educators using digital learning platforms may need to examine and provide feedback across many images of students’ math work. To assess the potential of VLMs to support educators in settings like this one, we introduce DrawEduMath, an English-language dataset of 2,030 images of students’ handwritten responses to K-12 math problems. Teachers provided detailed annotations, including free-form descriptions of each image and 11,661 question-answer (QA) pairs. These annotations capture a wealth of pedagogical insights, ranging from students’ problem-solving strategies to the composition of their drawings, diagrams, and writing. We evaluate VLMs on teachers’ QA pairs, as well as 44,362 synthetic QA pairs derived from teachers’ descriptions using language models (LMs). We show that even state-of-the-art VLMs leave much room for improvement on DrawEduMath questions. We also find that synthetic QAs, though imperfect, can yield similar model rankings as teacher-written QAs. We release DrawEduMath to support the evaluation of VLMs’ abilities to reason mathematically over images gathered with educational contexts in mind.</abstract>
      <url hash="c1593a6b">2025.naacl-long.352</url>
      <bibkey>baral-etal-2025-drawedumath</bibkey>
    </paper>
    <paper id="353">
      <title>Knowledge Graph Guided Evaluation of Abstention Techniques</title>
      <author><first>Kinshuk</first><last>Vasisht</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Navreet</first><last>Kaur</last><affiliation>University of Washington</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <pages>6921-6939</pages>
      <abstract>To deploy language models safely, it is crucial that they abstain from responding to inappropriate requests. Several prior studies test the safety promises of models based on their effectiveness in blocking malicious requests. In this work, we focus on evaluating the underlying techniques that cause models to abstain. We create ‘SELECT‘, a benchmark derived from a set of benign concepts (e.g., “rivers”) from a knowledge graph. Focusing on benign concepts isolates the effect of safety training, and grounding these concepts in a knowledge graph allows us to study the *generalization* and *specificity* of abstention techniques. Using ‘SELECT‘, we benchmark different abstention techniques over six open-weight and closed-source models. We find that the examined techniques indeed cause models to abstain with over 80% abstention rates. However, these techniques are not as effective for descendants of the target concepts, where abstention rates drop by 19%. We also characterize the generalization-specificity trade-offs for different techniques. Overall, no single technique is invariably better than others, and our findings inform practitioners of the various trade-offs involved.</abstract>
      <url hash="d4258e4d">2025.naacl-long.353</url>
      <bibkey>vasisht-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="354">
      <title><fixed-case>W</fixed-case>av2<fixed-case>P</fixed-case>rompt: End-to-End Speech Prompt Learning and Task-based Fine-tuning for Text-based <fixed-case>LLM</fixed-case>s</title>
      <author><first>Keqi</first><last>Deng</last></author>
      <author><first>Guangzhi</first><last>Sun</last></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>6940-6956</pages>
      <abstract>Wav2Prompt is proposed which allows integrating spoken input with a text-based large language model (LLM). Wav2Prompt uses a straightforward training process with only the same data used to train an automatic speech recognition (ASR) model. After training, Wav2Prompt learns continuous representations from speech and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training targets and utilises a continuous integrate-and-fire mechanism for explicit speech-text alignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot spoken language tasks such as speech translation (ST), speech understanding (SLU), and spoken-query-based question answering (SQQA). It is shown that for these zero-shot tasks, Wav2Prompt performs similarly to an ASR-LLM cascade and better than recent prior work. If relatively small amounts of task-specific paired data are available, the Wav2Prompt-LLM combination can be end-to-end (E2E) fine-tuned and then yields greatly improved results relative to an ASR-LLM cascade for the above tasks. For instance, for English-French ST, a Wav2Prompt-LLM combination gave a 5 BLEU point increase over an ASR-LLM cascade.</abstract>
      <url hash="d7a2c76b">2025.naacl-long.354</url>
      <bibkey>deng-etal-2025-wav2prompt</bibkey>
    </paper>
    <paper id="355">
      <title>Legal Judgment Prediction based on Knowledge-enhanced Multi-Task and Multi-Label Text Classification</title>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ming</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Changlong</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>6957-6970</pages>
      <abstract>Legal judgment prediction (LJP) is an essential task for legal AI, aiming at predicting judgments based on the facts of a case. Legal judgments can involve multiple law articles and charges. Although recent methods in LJP have made notable progress, most are constrained to single-task settings (e.g., only predicting charges) or single-label settings (e.g., not accommodating cases with multiple charges), diverging from the complexities of real-world scenarios. In this paper, we address the challenge of predicting relevant law articles and charges within the framework of legal judgment prediction, treating it as a multi-task and multi-label text classification problem. We introduce a knowledge-enhanced approach, called K-LJP, that incorporates (I) ”label-level knowledge” (such as definitions and relationships among labels) to enhance the representation of case facts for each task, and (ii) ”task-level knowledge” (such as the alignment between law articles and corresponding charges) to improve task synergy. Comprehensive experiments demonstrate our method’s effectiveness in comparison to state-of-the-art (SOTA) baselines.</abstract>
      <url hash="f9a1a9f5">2025.naacl-long.355</url>
      <bibkey>li-etal-2025-legal</bibkey>
    </paper>
    <paper id="356">
      <title><fixed-case>SP</fixed-case>e<fixed-case>C</fixed-case>trum: A Grounded Framework for Multidimensional Identity Representation in <fixed-case>LLM</fixed-case>-Based Agent</title>
      <author><first>Keyeun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seo Hyeong</first><last>Kim</last></author>
      <author><first>Seolhee</first><last>Lee</last></author>
      <author><first>Jinsu</first><last>Eun</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yena</first><last>Ko</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hayeon</first><last>Jeon</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Esther Hehsun</first><last>Kim</last></author>
      <author><first>Seonghye</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Soeun</first><last>Yang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Eun-mee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hajin</first><last>Lim</last><affiliation>Seoul National University</affiliation></author>
      <pages>6971-6991</pages>
      <abstract>Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual’s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum’s effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)—derived from short essays on preferences and daily routines—modeled characters’ identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.</abstract>
      <url hash="572b4a5c">2025.naacl-long.356</url>
      <bibkey>lee-etal-2025-spectrum</bibkey>
    </paper>
    <paper id="357">
      <title>Beemo: Benchmark of Expert-edited Machine-generated Outputs</title>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>Toloka AI</affiliation></author>
      <author><first>Jason S</first><last>Lucas</last></author>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Amazon</affiliation></author>
      <author><first>Jooyoung</first><last>Lee</last><affiliation>Amazon</affiliation></author>
      <author><first>Sergei</first><last>Tilga</last><affiliation>Toloka AI</affiliation></author>
      <author><first>Adaku</first><last>Uchendu</last><affiliation>MIT Lincoln Laboratory</affiliation></author>
      <author><first>Vladislav</first><last>Mikhailov</last><affiliation>University of Oslo</affiliation></author>
      <pages>6992-7018</pages>
      <abstract>The rapid proliferation of large language models (LLMs) has increased the volume of machine-generated texts (MGTs) and blurred text authorship in various domains. However, most existing MGT benchmarks include single-author texts (human-written and machine-generated). This conventional design fails to capture more practical multi-author scenarios, where the user refines the LLM response for natural flow, coherence, and factual correctness. Our paper introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo), which includes 6.5k texts written by humans, generated by ten instruction-finetuned LLMs, and edited by experts for various use cases, ranging from creative writing to summarization. Beemo additionally comprises 13.1k machine-generated and LLM-edited texts, allowing for diverse MGT detection evaluation across various edit types. We document Beemo’s creation protocol and present the results of benchmarking 33 configurations of MGT detectors in different experimental setups. We find that expert-based editing evades MGT detection, while LLM-edited texts are unlikely to be recognized as human-written. Beemo and all materials are publicly available.</abstract>
      <url hash="418aca47">2025.naacl-long.357</url>
      <bibkey>artemova-etal-2025-beemo</bibkey>
    </paper>
    <paper id="358">
      <title><fixed-case>SANDW</fixed-case>i<fixed-case>CH</fixed-case>: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc</title>
      <author><first>Daniel</first><last>Guzman Olivares</last></author>
      <author><first>Lara</first><last>Quijano</last><affiliation>Universidad Autónoma de Madrid</affiliation></author>
      <author><first>Federico</first><last>Liberatore</last></author>
      <pages>7019-7033</pages>
      <abstract>The rise of generative chat-based Large Language Models (LLMs) over the past two years has spurred a race to develop systems that promise near-human conversational and reasoning experiences. However, recent studies indicate that the language understanding offered by these models remains limited and far from human-like performance, particularly in grasping the contextual meanings of words—an essential aspect of reasoning. In this paper, we present a simple yet computationally efficient framework for multilingual Word Sense Disambiguation (WSD). Our approach reframes the WSD task as a cluster discrimination analysis over a semantic network refined from BabelNet using group algebra. We validate our methodology across multiple WSD benchmarks, achieving a new state of the art for all languages and tasks, as well as in individual assessments by part of speech. Notably, our model significantly surpasses the performance of current alternatives, even in low-resource languages, while reducing the parameter count by 72%.</abstract>
      <url hash="339e998d">2025.naacl-long.358</url>
      <bibkey>guzman-olivares-etal-2025-sandwich</bibkey>
    </paper>
    <paper id="359">
      <title>Towards Automatic Evaluation for Image Transcreation</title>
      <author><first>Simran</first><last>Khanuja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Vivek</first><last>Iyer</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xiaoyu</first><last>He</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>7034-7047</pages>
      <abstract>Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we identify three critical dimensions of image transcreation: cultural relevance, semantic equivalence and visual similarity, and design our metrics to evaluate systems along these axes. Our results show that proprietary VLMs best identify cultural relevance and semantic equivalence, while vision-encoder representations are adept at measuring visual similarity. Meta-evaluation across 7 countries shows our metrics agree strongly with human ratings, with average segment-level correlations ranging from 0.55-0.87. Finally, through a discussion of the merits and demerits of each metric, we offer a robust framework for automated image transcreation evaluation, grounded in both theoretical foundations and practical application. Our code can be found here: https://github.com/simran-khanuja/automatic-eval-transcreation</abstract>
      <url hash="fb949fd0">2025.naacl-long.359</url>
      <bibkey>khanuja-etal-2025-towards</bibkey>
    </paper>
    <paper id="360">
      <title><fixed-case>I</fixed-case>mg<fixed-case>T</fixed-case>rojan: Jailbreaking Vision-Language Models with <fixed-case>ONE</fixed-case> Image</title>
      <author><first>Xijia</first><last>Tao</last></author>
      <author><first>Shuai</first><last>Zhong</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <pages>7048-7063</pages>
      <abstract>There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack’s success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.</abstract>
      <url hash="b05e8cd3">2025.naacl-long.360</url>
      <bibkey>tao-etal-2025-imgtrojan</bibkey>
    </paper>
    <paper id="361">
      <title><fixed-case>RAG</fixed-case>-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement</title>
      <author><first>Jinhao</first><last>Jiang</last></author>
      <author><first>Jiayi</first><last>Chen</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Ruiyang</first><last>Ren</last></author>
      <author><first>Shijie</first><last>Wang</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yang</first><last>Song</last><affiliation>BOSS Zhipin</affiliation></author>
      <author><first>Tao</first><last>Zhang</last></author>
      <pages>7064-7074</pages>
      <abstract>Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose <b>RAG-Star</b>, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose a retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods. Our codes and data are publicly available at https://github.com/RUCAIBox/RAG-Star.</abstract>
      <url hash="3521a1a4">2025.naacl-long.361</url>
      <bibkey>jiang-etal-2025-rag</bibkey>
    </paper>
    <paper id="362">
      <title>Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration</title>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Jingqian</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Xi</first><last>Zeng</last></author>
      <author><first>Xingwei</first><last>Liang</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>7075-7092</pages>
      <abstract>Stance detection is critical for understanding the underlying position or attitude expressed toward a topic. Large language models (LLMs) have demonstrated significant advancements across various natural language processing tasks including stance detection, however, their performance in stance detection is limited by biases and spurious correlations inherent due to their data-driven nature. Our statistical experiment reveals that LLMs are prone to generate biased stances due to sentiment-stance spurious correlations and preference towards certain individuals and topics. Furthermore, the results demonstrate a strong negative correlation between stance bias and stance detection performance, underscoring the importance of mitigating bias to enhance the utility of LLMs in stance detection. Therefore, in this paper, we propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs. Further, to address the challenge of effectively learning bias representations and the difficulty in the generalizability of debiasing, we construct counterfactual augmented data. This approach enhances the calibration network, facilitating the debiasing and out-of-domain generalization. Experimental results on in-target and zero-shot stance detection tasks show that the proposed FACTUAL can effectively mitigate biases of LLMs, achieving state-of-the-art results.</abstract>
      <url hash="7d7e1d60">2025.naacl-long.362</url>
      <bibkey>li-etal-2025-mitigating-biases</bibkey>
    </paper>
    <paper id="363">
      <title>Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction</title>
      <author><first>Junlang</first><last>Qian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Zepeng</first><last>Zhai</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>7093-7115</pages>
      <url hash="82188186">2025.naacl-long.363</url>
      <bibkey>qian-etal-2025-beyond</bibkey>
    </paper>
    <paper id="364">
      <title>Investigating Hallucinations in Simultaneous Machine Translation: Knowledge Distillation Solution and Components Analysis</title>
      <author><first>Donglei</first><last>Yu</last></author>
      <author><first>Xiaomian</first><last>Kang</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Feifei</first><last>Zhai</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Nanchang</first><last>Cheng</last></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7116-7131</pages>
      <abstract>Simultaneous Machine Translation (SiMT) generates target translation before receiving the whole source sentence and faces a serious hallucination problem. In contrast, traditional offline machine translation (OMT) models exhibit significantly fewer hallucinations. Motivated by this disparity, we propose Knowledge Distillation for SiMT (KD-SiMT), a simple yet effective method that utilizes the OMT model to mitigate hallucinations in SiMT. Experiments on Zh<tex-math>\rightarrow</tex-math>En and De<tex-math>\rightarrow</tex-math>En tasks demonstrate that KD-SiMT effectively reduces hallucinations and enhances the SiMT performance. Furthermore, we systematically investigate the deficiencies in SiMT models related to serious hallucinations and the effect of KD-SiMT. Specifically, we design targeted tasks and metrics to quantitatively evaluate the components in SiMT models from the perspectives of model structure and knowledge acquisition. Our analyses reveal that inaccurate source representations and imbalanced cross-attention are more likely to occur in SiMT models when generating hallucinations, while KD-SiMT alleviates these issues. Besides, we find that KD-SiMT equips SiMT models with sufficient faithfulness knowledge in training, thus reducing hallucinations.</abstract>
      <url hash="2d3c1d7c">2025.naacl-long.364</url>
      <bibkey>yu-etal-2025-investigating</bibkey>
    </paper>
    <paper id="365">
      <title><fixed-case>M</fixed-case>arkov Chain of Thought for Efficient Mathematical Reasoning</title>
      <author><first>Wen</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Minpeng</first><last>Liao</last></author>
      <author><first>Kai</first><last>Fan</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7132-7157</pages>
      <url hash="c9cf69d9">2025.naacl-long.365</url>
      <bibkey>yang-etal-2025-markov</bibkey>
    </paper>
    <paper id="366">
      <title>Towards Inducing Long-Context Abilities in Multilingual Neural Machine Translation Models</title>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Pranjal A</first><last>Chitale</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <pages>7158-7170</pages>
      <abstract>Neural Machine Translation (NMT) models have traditionally used Sinusoidal Positional Embeddings (PEs), which often struggle to capture long-range dependencies and are inefficient for handling extended context or document-level translation tasks. This work addresses the challenge of transitioning pre-trained NMT models from absolute Sinusoidal PEs to Relative PEs, such as RoPE and ALiBi, without compromising performance. We demonstrate that parameter-efficient fine-tuning, using only a small amount of high-quality data, can successfully facilitate this transition. Experimental results indicate that switching from Sinusoidal to Relative PEs results in competitive translation quality on sentence-level evaluation benchmarks. Additionally, models trained with RoPE consistently outperform those using ALiBi and Sinusoidal PEs on document-level benchmarks across both string-based metrics and qualitative evaluations. Moreover, we find that a small amount of long-context data in a few languages is sufficient for cross-lingual length generalization, thereby inducing long-context capabilities.</abstract>
      <url hash="0aed3868">2025.naacl-long.366</url>
      <bibkey>gumma-etal-2025-towards</bibkey>
    </paper>
    <paper id="367">
      <title>Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection</title>
      <author><first>Koji</first><last>Inoue</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Divesh</first><last>Lala</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Gabriel</first><last>Skantze</last><affiliation>KTH Royal Institute of Technology, Stockholm, Sweden</affiliation></author>
      <author><first>Tatsuya</first><last>Kawahara</last><affiliation>Kyoto University, Tokyo Institute of Technology</affiliation></author>
      <pages>7171-7181</pages>
      <abstract>In human conversations, short backchannel utterances such as “yeah” and “oh” play a crucial role in facilitating smooth and engaging dialogue.These backchannels signal attentiveness and understanding without interrupting the speaker, making their accurate prediction essential for creating more natural conversational agents.This paper proposes a novel method for real-time, continuous backchannel prediction using a fine-tuned Voice Activity Projection (VAP) model.While existing approaches have relied on turn-based or artificially balanced datasets, our approach predicts both the timing and type of backchannels in a continuous and frame-wise manner on unbalanced, real-world datasets.We first pre-train the VAP model on a general dialogue corpus to capture conversational dynamics and then fine-tune it on a specialized dataset focused on backchannel behavior.Experimental results demonstrate that our model outperforms baseline methods in both timing and type prediction tasks, achieving robust performance in real-time environments.This research offers a promising step toward more responsive and human-like dialogue systems, with implications for interactive spoken dialogue applications such as virtual assistants and robots.</abstract>
      <url hash="5658487f">2025.naacl-long.367</url>
      <bibkey>inoue-etal-2025-yeah</bibkey>
    </paper>
    <paper id="368">
      <title>Prompt Compression for Large Language Models: A Survey</title>
      <author><first>Zongqian</first><last>Li</last></author>
      <author><first>Yinhong</first><last>Liu</last></author>
      <author><first>Yixuan</first><last>Su</last><affiliation>Cohere</affiliation></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>7182-7195</pages>
      <abstract>Leveraging large language models (LLMs) for complex natural language tasks typically requires long-form prompts to convey detailed requirements and information, which results in increased memory usage and inference costs. To mitigate these challenges, multiple efficient methods have been proposed, with prompt compression gaining significant research interest. This survey provides an overview of prompt compression techniques, categorized into hard prompt methods and soft prompt methods. First, the technical approaches of these methods are compared, followed by an exploration of various ways to understand their mechanisms, including the perspectives of attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic language. We also examine the downstream adaptations of various prompt compression techniques. Finally, the limitations of current prompt compression methods are analyzed, and several future directions are outlined, such as optimizing the compression encoder, combining hard and soft prompts methods, and leveraging insights from multimodality.</abstract>
      <url hash="f441ec14">2025.naacl-long.368</url>
      <bibkey>li-etal-2025-prompt</bibkey>
    </paper>
    <paper id="369">
      <title>Goal-Conditioned <fixed-case>DPO</fixed-case>: Prioritizing Safety in Misaligned Instructions</title>
      <author><first>Joo Bon</first><last>Maeng</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seongmin</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seokin</first><last>Seo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Kee-Eung</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>7196-7211</pages>
      <abstract>Large language models (LLMs) undergo extensive safety training to maximize both helpfulness and harmlessness in their responses. However, various jailbreak attacks jeopardize model safety, allowing malicious actors to bypass safety guidelines. Existing defense methods primarily focus on aligning the model’s output towards less harmful responses through post-processing or input perturbation. Consequently, these approaches are prone to general performance degradation and lack the ability to defend against a wide variety of attacks. In this paper, we propose goal-conditioned direct preference optimization (GC-DPO), which is trained to prioritize the system prompt over the user prompt through goal-conditioning, and thus enables a good balance between safety and performance. Empirically, we show that our approach significantly reduces the average Attack Success Rate (ASR) on a wide variety of jailbreak attacks. In particular, GC-DPO achieves a reduction of 67.1% to 5.0% in ASR for Vicuna-7B, a state-of-the-art result, without compromising the model’s general performance.</abstract>
      <url hash="a2ae7cde">2025.naacl-long.369</url>
      <bibkey>maeng-etal-2025-goal</bibkey>
    </paper>
    <paper id="370">
      <title>K-Level Reasoning: Establishing Higher Order Beliefs in Large Language Models for Strategic Reasoning</title>
      <author><first>Yadong</first><last>Zhang</last></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tao</first><last>Ge</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xun</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yan</first><last>Xia</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Man</first><last>Lan</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>7212-7234</pages>
      <abstract>Strategic reasoning is a complex yet essential capability for intelligent agents. It requires Large Language Model (LLM) agents to adapt their strategies dynamically in multi-agent environments. Unlike static reasoning tasks, success in these contexts depends on anticipating other agents’ beliefs and actions while continuously adjusting strategies to achieve individual goals. LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others’ perspectives and adapt to changing environments. Inspired by the Level-K framework from game theory and behavioral economics, which extends reasoning from simple reactions to structured strategic depth, we propose a novel framework: “K-Level Reasoning with Large Language Models (K-R).” This framework employs recursive mechanisms to enable LLMs to achieve varying levels of strategic depth, allowing agents to form higher order beliefs—beliefs about others’ beliefs. We validate this framework through rigorous testing on four testbeds: two classical game theory problems and two social intelligence tasks. The results demonstrate the advantages of K-R in strategic reasoning. Our work presents the first recursive implementation of strategic depth in large language models (LLMs). It establishes a foundation for future research into theory of mind and strategic reasoning in LLMs.</abstract>
      <url hash="dcedf07f">2025.naacl-long.370</url>
      <bibkey>zhang-etal-2025-k</bibkey>
    </paper>
    <paper id="371">
      <title><fixed-case>S</fixed-case>yllo<fixed-case>B</fixed-case>io-<fixed-case>NLI</fixed-case>: Evaluating Large Language Models on Biomedical Syllogistic Reasoning</title>
      <author><first>Magdalena</first><last>Wysocka</last><affiliation>CRUK NBC Manchester Institute and Technical University of Gdansk</affiliation></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Oskar</first><last>Wysocki</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>7235-7258</pages>
      <abstract>Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid conclusions and extracting supporting evidence across 28 syllogistic schemes instantiated with human genome pathways. Extensive experiments reveal that biomedical syllogistic reasoning is particularly challenging for zero-shot LLMs, which achieve an average accuracy between 70% on generalized modus ponens and 23% on disjunctive syllogism. At the same time, we found that few-shot prompting can boost the performance of different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper analysis shows that both techniques exhibit high sensitivity to superficial lexical variations, highlighting a dependency between reliability, models’ architecture, and pre-training regime. Overall, our results indicate that, while in-context examples have the potential to elicit syllogistic reasoning in LLMs, existing models are still far from achieving the robustness and consistency required for safe biomedical NLI applications.</abstract>
      <url hash="ac2c3a97">2025.naacl-long.371</url>
      <bibkey>wysocka-etal-2025-syllobio</bibkey>
    </paper>
    <paper id="372">
      <title>The State and Fate of Summarization Datasets: A Survey</title>
      <author><first>Noam</first><last>Dahan</last><affiliation>Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>7259-7278</pages>
      <abstract>Automatic summarization has consistently attracted attention due to its versatility and wide application in various downstream tasks. Despite its popularity, we find that annotation efforts have largely been disjointed, and have lacked common terminology. Consequently, it is challenging to discover existing resources or identify coherent research directions. To address this, we survey a large body of work spanning 133 datasets in over 100 languages, creating a novel ontology covering sample properties, collection methods and distribution. With this ontology we make key observations, including the lack of accessible high-quality datasets for low-resource languages, and the field’s overreliance on the news domain and on automatically collected distant supervision. Finally, we make available a web interface that allows users to interact and explore our ontology and dataset collection, as well as a template for a summarization data card, which can be used to streamline future research into a more coherent body of work.</abstract>
      <url hash="e2d067b0">2025.naacl-long.372</url>
      <bibkey>dahan-stanovsky-2025-state</bibkey>
    </paper>
    <paper id="373">
      <title><fixed-case>MGM</fixed-case>: Global Understanding of Audience Overlap Graphs for Predicting the Factuality and the Bias of News Media</title>
      <author><first>Muhammad Arslan</first><last>Manzoor</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ruihong</first><last>Zeng</last></author>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shangsong</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>7279-7295</pages>
      <abstract>In the current era of rapidly growing digital data, evaluating the political bias and factuality of news outlets has become more important for seeking reliable information online. In this work, we study the classification problem of profiling news media from the lens of political bias and factuality. Traditional profiling methods, such as Pre-trained Language Models (PLMs) and Graph Neural Networks (GNNs) have shown promising results, but they face notable challenges. PLMs focus solely on textual features, causing them to overlook the complex relationships between entities, while GNNs often struggle with media graphs containing disconnected components and insufficient labels. To address these limitations, we propose MediaGraphMind (MGM), an effective solution within a variational Expectation-Maximization (EM) framework. Instead of relying on limited neighboring nodes, MGM leverages features, structural patterns, and label information from globally similar nodes. Such a framework not only enables GNNs to capture long-range dependencies for learning expressive node representations but also enhances PLMs by integrating structural information and therefore improving the performance of both models. The extensive experiments demonstrate the effectiveness of the proposed framework and achieve new state-of-the-art results. Further, we share our repository which contains the dataset, code, and documentation.</abstract>
      <url hash="2507c1dd">2025.naacl-long.373</url>
      <bibkey>manzoor-etal-2025-mgm</bibkey>
    </paper>
    <paper id="374">
      <title>A Logical Fallacy-Informed Framework for Argument Generation</title>
      <author><first>Luca</first><last>Mouchel</last></author>
      <author><first>Debjit</first><last>Paul</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Shaobo</first><last>Cui</last></author>
      <author><first>Robert</first><last>West</last><affiliation>EPFL - EPF Lausanne and Microsoft</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>7296-7314</pages>
      <abstract>Despite the remarkable performance of large language models (LLMs), they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. An important factor contributing to LLMs’ suboptimal performance in generating coherent arguments is their oversight of logical fallacies. To address this issue, we introduce fallacy-informed preference optimization (FIPO) that helps steer LLMs toward generating logically sound arguments. FIPO includes a classification loss to capture the fine-grained information on fallacy types. Our results on argument generation tasks show that FIPO reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results reveal that the quality of the arguments generated by our method significantly outperforms the fine-tuned baselines and other preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation.</abstract>
      <url hash="11fb5920">2025.naacl-long.374</url>
      <bibkey>mouchel-etal-2025-logical</bibkey>
    </paper>
    <paper id="375">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search</title>
      <author><first>Di</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jianbo</first><last>Wu</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Jingdi</first><last>Lei</last><affiliation>Shanghai Artificial Intelligence Laboratory and Beijing Institute of Technology</affiliation></author>
      <author><first>Tong</first><last>Che</last><affiliation>NVIDIA</affiliation></author>
      <author id="jiatong-li-hk"><first>Jiatong</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tong</first><last>Xie</last><affiliation>University of New South Wales and GreenDynamics</affiliation></author>
      <author><first>Xiaoshui</first><last>Huang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shufei</first><last>Zhang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Marco</first><last>Pavone</last><affiliation>NVIDIA and Stanford University</affiliation></author>
      <author><first>Yuqiang</first><last>Li</last></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Dongzhan</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>7315-7337</pages>
      <abstract>This paper presents LLaMA-Berry, an advanced mathematical reasoning framework to enhance the problem-solving ability of large language models (LLMs). The framework combines Monte Carlo Tree Search with Self-Refine (SR-MCTS) to optimize the reasoning paths and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critique and rewriting capabilities of LLMs, our SR-MCTS overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms, enabling a more efficient exploration of solution spaces. To guide the search process, we propose the Pairwise Preference Reward Model (PPRM), which predicts pairwise preferences between solutions through instruction-following capabilities trained by Reinforcement Learning from Human Feedback (RLHF). Finally, the Enhanced Borda Count (EBC) method is adopted to synthesize pairwise preferences into global quantile scores for evaluations. This approach mitigates the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior search efficiency and performance compared to existing open-source and closed-source methods, particularly in complex Olympiad-level benchmarks, including AIME24 and AMC23.</abstract>
      <url hash="685cb26e">2025.naacl-long.375</url>
      <bibkey>zhang-etal-2025-llama</bibkey>
    </paper>
    <paper id="376">
      <title>Generative Prompt Internalization</title>
      <author><first>Haebin</first><last>Shin</last></author>
      <author><first>Lei</first><last>Ji</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI</affiliation></author>
      <author><first>Eunbi</first><last>Choi</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Config Intelligence</affiliation></author>
      <pages>7338-7363</pages>
      <abstract>Prompts used in recent large language model based applications are often fixed and lengthy, leading to significant computational overhead. To address this challenge, we propose Generative Prompt Internalization (GenPI), a lightweight method that employs a joint training approach. GenPI not only replicates the behavior of models with prompt inputs but also generates the content of the prompt along with reasons for why the model’s behavior should change accordingly. We demonstrate that our approach effectively internalizes complex prompts across various agent-based application scenarios. For effective training without interactions with the dedicated environments, we introduce a data synthesis technique that autonomously collects conversational datasets by swapping the roles of the agent and environment. This method is especially useful in scenarios where only a predefined prompt is available without a corresponding training dataset. By internalizing complex prompts, Generative Prompt Internalization enables high performance and efficient inference without the need for explicit prompts.</abstract>
      <url hash="117e63ed">2025.naacl-long.376</url>
      <bibkey>shin-etal-2025-generative</bibkey>
    </paper>
    <paper id="377">
      <title>Script-Agnosticism and its Impact on Language Identification for <fixed-case>D</fixed-case>ravidian Languages</title>
      <author><first>Milind</first><last>Agarwal</last></author>
      <author><first>Joshua</first><last>Otten</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>7364-7384</pages>
      <abstract>Language identification is used as the first step in many data collection and crawling efforts because it allows us to sort online text into language-specific buckets. However, many modern languages, such as Konkani, Kashmiri, Punjabi etc., are synchronically written in several scripts. Moreover, languages with different writing systems do not share significant lexical, semantic, and syntactic properties in neural representation spaces, which is a disadvantage for closely related languages and low-resource languages, especially those from the Indian Subcontinent. To counter this, we propose learning script-agnostic representations using several different experimental strategies (upscaling, flattening, and script mixing) focusing on four major Dravidian languages (Tamil, Telugu, Kannada, and Malayalam). We find that word-level script randomization and exposure to a language written in multiple scripts is extremely valuable for downstream script-agnostic language identification, while also maintaining competitive performance on naturally occurring text.</abstract>
      <url hash="7a7a37e8">2025.naacl-long.377</url>
      <bibkey>agarwal-etal-2025-script</bibkey>
    </paper>
    <paper id="378">
      <title><fixed-case>NAT</fixed-case>: Enhancing Agent Tuning with Negative Samples</title>
      <author><first>Renxi</first><last>Wang</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yixuan</first><last>Zhang</last></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Haonan</first><last>Li</last></author>
      <pages>7385-7398</pages>
      <abstract>Interaction trajectories between agents and environments have proven effective in tuning LLMs into task-specific agents. However, constructing these trajectories, especially successful trajectories, is often computationally and time intensive due to the relatively low success rates of even the most advanced LLMs, such as GPT-4 and Claude. Additionally, common training paradigms like supervised fine-tuning (SFT) and reinforcement learning (RL) not only require large volumes of data but also have specific demands regarding the trajectories used. For instance, existing SFT approaches typically utilize only positive examples, limiting their efficiency in low-resource scenarios. To address this, we introduce Negative-Aware Training (NAT), a straightforward yet effective method that leverages both successful and failed trajectories for fine-tuning, maximizing the utility of limited resources. Experimental results demonstrate that NAT consistently surpasses existing methods, including SFT, DPO, and PPO, across various tasks.</abstract>
      <url hash="1a1408d5">2025.naacl-long.378</url>
      <bibkey>wang-etal-2025-nat</bibkey>
    </paper>
    <paper id="379">
      <title>Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies</title>
      <author><first>Zirui</first><last>Song</last></author>
      <author><first>Guangxian</first><last>Ouyang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Hongbin</first><last>Na</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Zijing</first><last>Shi</last></author>
      <author><first>Zhenhao</first><last>Chen</last></author>
      <author><first>Fu</first><last>Yujie</last></author>
      <author><first>Zeyu</first><last>Zhang</last><affiliation>The Australian National University</affiliation></author>
      <author><first>Shiyu</first><last>Jiang</last></author>
      <author><first>Miao</first><last>Fang</last><affiliation>Northeastern University at Qinhuangdao</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>7399-7415</pages>
      <abstract>Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. To accomplish this task, we leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety. These textual task descriptions are then integrated with designed 3D assets to simulate realistic environments. Within these constructed environments, our LLM-based robotic agent learns the necessary skills to proactively discover and handle the proposed anomalies through task decomposition, optimal learning approach selection. We demonstrate that our generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.</abstract>
      <url hash="99ed39fb">2025.naacl-long.379</url>
      <bibkey>song-etal-2025-hazards</bibkey>
    </paper>
    <paper id="380">
      <title>How to Make the Most of <fixed-case>LLM</fixed-case>s’ Grammatical Knowledge for Acceptability Judgments</title>
      <author><first>Yusuke</first><last>Ide</last></author>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Miyu</first><last>Oba</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>7416-7432</pages>
      <abstract>The grammatical knowledge of language models (LMs) is often measured using a benchmark of linguistic minimal pairs, where LMs are presented with a pair of acceptable and unacceptable sentences and required to judge which is more acceptable. Conventional approaches compare sentence probabilities directly, but large language models (LLMs) provide nuanced evaluation methods using prompts and templates. We therefore investigate how to derive the most accurate acceptability judgments from LLMs to comprehensively evaluate their grammatical knowledge. Through extensive experiments in both English and Chinese, we compare nine judgment methods and demonstrate that two of them, in-template LP (a probability readout method) and Yes/No probability computing (a prompting-based method), achieve higher accuracy than the conventional approach. Our analysis reveals that the top two methods excel in different linguistic phenomena, suggesting they access different aspects of the LLMs’ grammatical knowledge. We find that ensembling the two methods achieves even higher accuracy. Consequently, we recommend these techniques, either individually or ensembled, as more effective alternatives to conventional approaches for assessing grammatical knowledge in LLMs.</abstract>
      <url hash="3a150127">2025.naacl-long.380</url>
      <bibkey>ide-etal-2025-make</bibkey>
    </paper>
    <paper id="381">
      <title>Is Your <fixed-case>LLM</fixed-case> Outdated? A Deep Look at Temporal Generalization</title>
      <author><first>ChenghaoZhu</first><last>ChenghaoZhu</last></author>
      <author><first>Nuo</first><last>Chen</last><affiliation>National University of Singapore and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yufei</first><last>Gao</last></author>
      <author><first>Yunyi</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Prayag</first><last>Tiwari</last><affiliation>Halmstad University</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>7433-7457</pages>
      <abstract>The rapid advancement of Large Language Models (LLMs) has led to the development of benchmarks that consider temporal dynamics, however, there remains a gap in understanding how well these models can generalize across temporal contexts due to the inherent dynamic nature of language and information. This paper introduces the concept of temporal generalization in LLMs, including bias in past and future generalizations. Then we introduce FreshBench, a new evaluation framework that employs fresh text and event prediction for assessing LLMs’ temporal adaptability, ensuring the evaluation process free from data leakage and subjective bias. The experiment shows significant temporal biases and a decline in performance over time.</abstract>
      <url hash="f6bdc053">2025.naacl-long.381</url>
      <bibkey>chenghaozhu-etal-2025-llm</bibkey>
    </paper>
    <paper id="382">
      <title>Towards a Perspectivist Turn in Argument Quality Assessment</title>
      <author><first>Julia</first><last>Romberg</last><affiliation>GESIS Leibniz Institute for the Social Sciences</affiliation></author>
      <author><first>Maximilian</first><last>Maurer</last><affiliation>GESIS Leibniz Institute for the Social Sciences</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>GESIS – Leibniz Institute for the Social Sciences and Heinrich-Heine University Düsseldorf</affiliation></author>
      <pages>7458-7485</pages>
      <abstract>The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable datasets. We fill this gap by conducting a systematic review of argument quality datasets. We assign them to a multi-layered categorization targeting two aspects: (a) What has been annotated: we collect the quality dimensions covered in datasets and consolidate them in an overarching taxonomy, increasing dataset comparability and interoperability. (b) Who annotated: we survey what information is given about annotators, enabling perspectivist research and grounding our recommendations for future actions. To this end, we discuss datasets suitable for developing perspectivist models (i.e., those containing individual, non-aggregated annotations), and we showcase the importance of a controlled selection of annotators in a pilot study.</abstract>
      <url hash="cedcb884">2025.naacl-long.382</url>
      <bibkey>romberg-etal-2025-towards</bibkey>
    </paper>
    <paper id="383">
      <title>A Picture is Worth A Thousand Numbers: Enabling <fixed-case>LLM</fixed-case>s Reason about Time Series via Visualization</title>
      <author><first>Haoxin</first><last>Liu</last></author>
      <author><first>Chenghao</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>B. Aditya</first><last>Prakash</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7486-7518</pages>
      <abstract>Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, have been largely underexplored fortime-series reasoning (TsR), which is ubiquitous in the real world. In this work, wepropose TimerBed, the first comprehensivetestbed for evaluating LLMs’ TsR performance.Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, diversecombinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and observe the initial failuresof LLMs in TsR, as evidenced by the ineffectiveness of zero shot (ZST) and performancedegradation of few shot in-context learning(ICL). Further, we identify one possible rootcause: the numerical modeling of data. Toaddress this, we propose a prompt-based solution VL-Time, with visualization-modeled dataand language-guided reasoning. Experimental results demonstrate that VL-Time enablesmultimodal LLMs to be non-trivial ZST andpowerful ICL reasoners for time series, achieving about 140% average performance improvement and 99% average token costs reduction.TimerBed and VL-Time are available at https://github.com/AdityaLab/DeepTime/.</abstract>
      <url hash="d57434a4">2025.naacl-long.383</url>
      <bibkey>liu-etal-2025-picture</bibkey>
    </paper>
    <paper id="384">
      <title><fixed-case>P</fixed-case>lag<fixed-case>B</fixed-case>ench: Exploring the Duality of Large Language Models in Plagiarism Generation and Detection</title>
      <author><first>Jooyoung</first><last>Lee</last><affiliation>Amazon</affiliation></author>
      <author><first>Toshini</first><last>Agrawal</last></author>
      <author><first>Adaku</first><last>Uchendu</last><affiliation>MIT Lincoln Laboratory</affiliation></author>
      <author><first>Thai</first><last>Le</last><affiliation>Indiana University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>7519-7534</pages>
      <abstract>Recent studies have raised concerns about the potential threats large language models (LLMs) pose to academic integrity and copyright protection. Yet, their investigation is predominantly focused on literal copies of original texts. Also, how LLMs can facilitate the detection of LLM-generated plagiarism remains largely unexplored. To address these gaps, we introduce PlagBench, a dataset of 46.5K synthetic text pairs that represent three major types of plagiarism: verbatim copying, paraphrasing, and summarization. These samples are generated by three advanced LLMs. We rigorously validate the quality of PlagBench through a combination of fine-grained automatic evaluation and human annotation. We then utilize this dataset for two purposes: (1) to examine LLMs’ ability to transform original content into accurate paraphrases and summaries, and (2) to evaluate the plagiarism detection performance of five modern LLMs alongside three specialized plagiarism checkers. Our results show that GPT-3.5 Turbo can produce high-quality paraphrases and summaries without significantly increasing text complexity compared to GPT-4 Turbo. However, in terms of detection, GPT-4 outperforms other LLMs and commercial detection tools by 20%, highlights the evolving capabilities of LLMs not only in content generation but also in plagiarism detection. Data and source code are available at https://github.com/Brit7777/plagbench.</abstract>
      <url hash="bc6f2942">2025.naacl-long.384</url>
      <bibkey>lee-etal-2025-plagbench</bibkey>
    </paper>
    <paper id="385">
      <title>Commonality and Individuality! Integrating Humor Commonality with Speaker Individuality for Humor Recognition</title>
      <author><first>Haohao</first><last>Zhu</last></author>
      <author><first>Xiaokun</first><last>Zhang</last><affiliation>City University of HongKong</affiliation></author>
      <author><first>Zeyuan</first><last>Zeng</last></author>
      <author><first>Junyu</first><last>Lu</last></author>
      <author><first>Zewen</first><last>Bai</last></author>
      <author><first>Liang</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>7535-7547</pages>
      <abstract>Humor recognition aims to identify whether a specific speaker’s text is humorous. Current methods for humor recognition mainly suffer from two limitations: (1) they solely focus on one aspect of humor commonalities, ignoring the multifaceted nature of humor; and (2) they typically overlook the critical role of speaker individuality, which is essential for a comprehensive understanding of humor expressions. To bridge these gaps, we introduce the Commonality and Individuality Incorporated Network for Humor Recognition (CIHR), a novel model designed to enhance humor recognition by integrating multifaceted humor commonalities with the distinctive individuality of speakers. The CIHR features a Humor Commonality Analysis module that explores various perspectives of multifaceted humor commonality within user texts, and a Speaker Individuality Extraction module that captures both static and dynamic aspects of a speaker’s profile to accurately model their distinctive individuality. Additionally, Static and Dynamic Fusion modules are introduced to effectively incorporate the humor commonality with speaker’s individuality in the humor recognition process. Extensive experiments demonstrate the effectiveness of CIHR, underscoring the importance of concurrently addressing both multifaceted humor commonality and distinctive speaker individuality in humor recognition.</abstract>
      <url hash="53b731bd">2025.naacl-long.385</url>
      <bibkey>zhu-etal-2025-commonality</bibkey>
    </paper>
    <paper id="386">
      <title><fixed-case>CAST</fixed-case>: Corpus-Aware Self-similarity Enhanced Topic modelling</title>
      <author><first>Yanan</first><last>Ma</last></author>
      <author><first>Chenghao</first><last>Xiao</last><affiliation>Durham University</affiliation></author>
      <author><first>Chenhan</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Sabine N Van Der</first><last>Veer</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Lamiece</first><last>Hassan</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>7548-7561</pages>
      <abstract>Topic modelling is a pivotal unsupervised machine learning technique for extracting valuable insights from large document collections. Existing neural topic modelling methods often encode contextual information of documents, while ignoring contextual details of candidate centroid words, leading to the inaccurate selection of topic words due to the *contextualization gap*. In parallel, it is found that functional words are frequently selected over topical words. To address these limitations, we introduce **CAST**: **C**orpus-**A**ware **S**elf-similarity Enhanced **T**opic modelling, a novel topic modelling method that builds upon candidate centroid word embeddings contextualized on the dataset, and a novel self-similarity-based method to filter out less meaningful tokens. Inspired by findings in contrastive learning that self-similarities of functional token embeddings in different contexts are much lower than topical tokens, we find self-similarity to be an effective metric to prevent functional words from acting as candidate topic words. Our approach significantly enhances the coherence and diversity of generated topics, as well as the topic model’s ability to handle noisy data. Experiments on news benchmark datasets and one Twitter dataset demonstrate the method’s superiority in generating coherent, diverse topics, and handling noisy data, outperforming strong baselines.</abstract>
      <url hash="093c7016">2025.naacl-long.386</url>
      <bibkey>ma-etal-2025-cast</bibkey>
    </paper>
    <paper id="387">
      <title>A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding</title>
      <author><first>Abdulfattah</first><last>Safa</last></author>
      <author><first>Gözde Gül</first><last>Şahin</last><affiliation>Koç University</affiliation></author>
      <pages>7562-7579</pages>
      <abstract>Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like MultiWOZ 2.1, with up to 90% fewer requests to the LLM API.</abstract>
      <url hash="9badc399">2025.naacl-long.387</url>
      <bibkey>safa-sahin-2025-zero</bibkey>
    </paper>
    <paper id="388">
      <title>Navigating the Cultural Kaleidoscope: A Hitchhiker’s Guide to Sensitivity in Large Language Models</title>
      <author><first>Somnath</first><last>Banerjee</last><affiliation>Cisco and IIT Kharagpur</affiliation></author>
      <author><first>Sayan</first><last>Layek</last></author>
      <author><first>Hari</first><last>Shrawgi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rajarshi</first><last>Mandal</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Avik</first><last>Halder</last></author>
      <author><first>Shanu</first><last>Kumar</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sagnik</first><last>Basu</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Parag</first><last>Agrawal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rima</first><last>Hazra</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>7580-7617</pages>
      <abstract>Cultural harm stems in LLMs whereby these models fail to align with specific cultural norms, resulting in misrepresentations or violations of cultural values. This work addresses the challenges of ensuring cultural sensitivity in LLMs, especially in small-parameter models that often lack the extensive training data needed to capture global cultural nuances. We present two key contributions: (1) A cultural harm test dataset, created to assess model outputs across different cultural contexts through scenarios that expose potential cultural insensitivities, and (2) A culturally aligned preference dataset, aimed at restoring cultural sensitivity through fine-tuning based on feedback from diverse annotators. These datasets facilitate the evaluation and enhancement of LLMs, ensuring their ethical and safe deployment across different cultural landscapes. Our results show that integrating culturally aligned feedback leads to a marked improvement in model behavior, significantly reducing the likelihood of generating culturally insensitive or harmful content.</abstract>
      <url hash="9aee01ea">2025.naacl-long.388</url>
      <bibkey>banerjee-etal-2025-navigating</bibkey>
    </paper>
    <paper id="389">
      <title>Padding Tone: A Mechanistic Analysis of Padding Tokens in <fixed-case>T</fixed-case>2<fixed-case>I</fixed-case> Models</title>
      <author><first>Michael</first><last>Toker</last></author>
      <author><first>Ido</first><last>Galil</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Hadas</first><last>Orgad</last><affiliation>Computer Science Department, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Rinon</first><last>Gal</last><affiliation>NVIDIA and Tel Aviv University, Tel Aviv</affiliation></author>
      <author><first>Yoad</first><last>Tewel</last></author>
      <author><first>Gal</first><last>Chechik</last><affiliation>Bar Ilan University and NVIDIA</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>7618-7632</pages>
      <abstract>Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by appending padding tokens to the input. Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal three distinct scenarios: padding tokens may affect the model’s output during text encoding, during the diffusion process, or be effectively ignored. Moreover, we identify key relationships between these scenarios and the model’s architecture (cross or self-attention) and its training process (frozen or trained text encoder). These insights contribute to a deeper understanding of the mechanisms of padding tokens, potentially informing future model design and training practices in T2I systems.</abstract>
      <url hash="8bd6013a">2025.naacl-long.389</url>
      <bibkey>toker-etal-2025-padding</bibkey>
    </paper>
    <paper id="390">
      <title>In-Context Learning (and Unlearning) of Length Biases</title>
      <author><first>Stephanie</first><last>Schoch</last></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <pages>7633-7671</pages>
      <abstract>Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning). This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.</abstract>
      <url hash="e5e970a6">2025.naacl-long.390</url>
      <bibkey>schoch-ji-2025-context</bibkey>
    </paper>
    <paper id="391">
      <title><fixed-case>A</fixed-case>d<fixed-case>TEC</fixed-case>: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising</title>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>7672-7691</pages>
      <abstract>As the fluency of ad texts automatically generated by natural language generation technologies continues to improve, there is an increasing demand to assess the quality of these creatives in real-world setting.We propose **AdTEC**, the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations.Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as constructing a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically maintained in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on this dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark.Our results show that while PLMs have a practical level of performance in several tasks, humans continue to outperform them in certain domains, indicating that there remains significant potential for further improvement in this area.</abstract>
      <url hash="89798b1f">2025.naacl-long.391</url>
      <bibkey>zhang-etal-2025-adtec</bibkey>
    </paper>
    <paper id="392">
      <title>Empowering Retrieval-based Conversational Recommendation with Contrasting User Preferences</title>
      <author><first>Heejin</first><last>Kook</last></author>
      <author><first>Junyoung</first><last>Kim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Seongmin</first><last>Park</last></author>
      <author><first>Jongwuk</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>7692-7707</pages>
      <abstract>Conversational recommender systems (CRSs) are designed to suggest the target item that the user is likely to prefer through multi-turn conversations. Recent studies stress that capturing sentiments in user conversations improves recommendation accuracy. However, they employ a single user representation, which may fail to distinguish between contrasting user intentions, such as likes and dislikes, potentially leading to suboptimal performance. To this end, we propose a novel conversational recommender model, called COntrasting user pReference expAnsion and Learning (CORAL). Firstly, CORAL extracts the user’s hidden pref- erences through contrasting preference expansion using the reasoning capacity of the LLMs. Based on the potential preference, CORAL explicitly differentiates the contrasting preferences and leverages them into the recommendation process via preference-aware learning. Extensive experiments show that CORAL significantly outperforms existing methods in three benchmark datasets, improving up to 99.72% in Recall@10. The code and datasets are available at https://github.com/kookeej/CORAL.</abstract>
      <url hash="86455f50">2025.naacl-long.392</url>
      <bibkey>kook-etal-2025-empowering</bibkey>
    </paper>
    <paper id="393">
      <title><fixed-case>LRQ</fixed-case>: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices</title>
      <author><first>Jung Hyun</first><last>Lee</last><affiliation>NAVER CLOVA</affiliation></author>
      <author><first>Jeonghoon</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology and NAVER</affiliation></author>
      <author><first>June Yong</first><last>Yang</last></author>
      <author><first>Se Jung</first><last>Kwon</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Eunho</first><last>Yang</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <author><first>Dongsoo</first><last>Lee</last><affiliation>NAVER CLOVA</affiliation></author>
      <pages>7708-7743</pages>
      <abstract>With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing weights and activations of LLMs still suffer from non-negligible accuracy drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) - a simple yet effective post-training weight quantization method for LLMs that reconstructs the outputs of an intermediate Transformer block by leveraging low-rank weight-scaling matrices, replacing the conventional full weight-scaling matrices that entail as many learnable scales as their associated weights. Thanks to parameter sharing via low-rank structure, LRQ only needs to learn significantly fewer parameters while enabling the individual scaling of weights, thus boosting the generalization capability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ works under (i) 8-bit weight and per-tensor activation quantization, (ii) 4-bit weight and 8-bit per-token activation quantization, and (iii) low-bit weight-only quantization schemes. Our code is available at Software.</abstract>
      <url hash="102186ec">2025.naacl-long.393</url>
      <bibkey>lee-etal-2025-lrq</bibkey>
    </paper>
    <paper id="394">
      <title>Towards Robust Knowledge Representations in Multilingual <fixed-case>LLM</fixed-case>s for Equivalence and Inheritance based Consistent Reasoning</title>
      <author><first>Gaurav</first><last>Arora</last><affiliation>Amazon</affiliation></author>
      <author><first>Srujana</first><last>Merugu</last></author>
      <author><first>Shreya</first><last>Jain</last></author>
      <author><first>Vaibhav</first><last>Saxena</last><affiliation>Amazon</affiliation></author>
      <pages>7744-7762</pages>
      <abstract>Reasoning and linguistic skills form the cornerstone of human intelligence, facilitating problem-solving and decision-making. Recent advances in Large Language Models (LLMs) have led to impressive linguistic capabilities and emergent reasoning behaviors, fueling widespread adoption across application domains. However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations. In this work, we focus on evaluating whether LLMs have the requisite representations to reason using two foundational relationships: “equivalence” and “inheritance”. We introduce novel tasks and benchmarks spanning six languages and observe that current SOTA LLMs often produce conflicting answers to the same questions across languages in 17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases. To enhance consistency across languages, we propose novel “Compositional Representations” where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.</abstract>
      <url hash="7c392312">2025.naacl-long.394</url>
      <bibkey>arora-etal-2025-towards</bibkey>
    </paper>
    <paper id="395">
      <title><fixed-case>LLM</fixed-case>s as Meta-Reviewers’ Assistants: A Case Study</title>
      <author><first>Eftekhar</first><last>Hossain</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Sanjeev Kumar</first><last>Sinha</last><affiliation>Auburn University</affiliation></author>
      <author><first>Naman</first><last>Bansal</last><affiliation>Auburn University</affiliation></author>
      <author><first>R. Alexander</first><last>Knipper</last><affiliation>Auburn University</affiliation></author>
      <author><first>Souvika</first><last>Sarkar</last><affiliation>Wichita State University</affiliation></author>
      <author><first>John</first><last>Salvador</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Yash</first><last>Mahajan</last></author>
      <author><first>Sri Ram Pavan Kumar</first><last>Guttikonda</last></author>
      <author><first>Mousumi</first><last>Akter</last><affiliation>Technische Universität Dortmund</affiliation></author>
      <author><first>Md. Mahadi</first><last>Hassan</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Matthew</first><last>Freestone</last></author>
      <author><first>Matthew C. Williams</first><last>Jr.</last></author>
      <author><first>Dongji</first><last>Feng</last><affiliation>Gustavus Adolphus College</affiliation></author>
      <author><first>Santu</first><last>Karmaker</last><affiliation>University of Central Florida</affiliation></author>
      <pages>7763-7803</pages>
      <abstract>One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves assimilating diverse opinions from multiple expert peers, formulating one’s self-judgment as a senior expert, and then summarizing all these perspectives into a concise holistic overview to make an overall recommendation. This process is time-consuming and can be compromised by human factors like fatigue, inconsistency, missing tiny details, etc. Given the latest major developments in Large Language Models (LLMs), it is very compelling to rigorously study whether LLMs can help meta-reviewers perform this important task better. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in better comprehending multiple experts’ perspectives by generating a controlled multi-perspective-summary (MPS) of their opinions. To achieve this, we prompt three LLMs with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the MPSs generated by the LLMs and report our findings.</abstract>
      <url hash="ad2263d8">2025.naacl-long.395</url>
      <bibkey>hossain-etal-2025-llms</bibkey>
    </paper>
    <paper id="396">
      <title>A Survey of <fixed-case>NLP</fixed-case> Progress in <fixed-case>S</fixed-case>ino-<fixed-case>T</fixed-case>ibetan Low-Resource Languages</title>
      <author><first>Shuheng</first><last>Liu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Michael</first><last>Best</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7804-7825</pages>
      <abstract>Despite the increasing effort in including more low-resource languages in NLP/CL development, most of the world’s languages are still absent. In this paper, we take the example of the Sino-Tibetan language family which consists of hundreds of low-resource languages, and we look at the representation of these low-resource languages in papers archived on ACL Anthology. Our findings indicate that while more techniques and discussions on more languages are present in more publication venues over the years, the overall focus on this language family has been minimal. The lack of attention might be owing to the small number of native speakers and governmental support of these languages. The current development of large language models, albeit successful in a few quintessential rich-resource languages, are still trailing when tackling these low-resource languages. Our paper calls for the attention in NLP/CL research on the inclusion of low-resource languages, especially as increasing resources are poured into the development of data-driven language models.</abstract>
      <url hash="23c09153">2025.naacl-long.396</url>
      <bibkey>liu-best-2025-survey</bibkey>
    </paper>
    <paper id="397">
      <title>Enhancing Language Model Hypernetworks with Restart: A Study on Optimization</title>
      <author><first>Yihan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Rongrong</first><last>Ji</last></author>
      <author><first>Jie</first><last>Chen</last><affiliation>Peking University</affiliation></author>
      <pages>7826-7838</pages>
      <abstract>Hypernetworks are a class of meta-networks that generate weights for main neural networks. Their unique parameter spaces necessitate exploring suitable optimization strategies to enhance performance, especially for language models. However, a comprehensive investigation into optimization strategies for hypernetworks remains absent. To address this gap, we analyze the loss landscape of hypernetworks and propose that restart optimization strategies can improve their performance for language models. We find that hypernetworks have inherently more complicated loss landscapes compared to conventional networks due to their distinct parameter spaces. Consequently, a restart strategy that periodically resets the learning rate can facilitate better convergence for hypernetworks. Through experiments on instruction tuning and multi-task training, we demonstrate that the restart strategy consistently enhances the performance of hypernetworks for language models, often more effectively than for conventional deep neural networks. Our findings highlight the importance of tailored optimization techniques to unlock the full potential of hypernetworks in natural language processing tasks.</abstract>
      <url hash="121d9d0b">2025.naacl-long.397</url>
      <bibkey>zhang-etal-2025-enhancing-language</bibkey>
    </paper>
    <paper id="398">
      <title>Functional Lexicon in Subword Tokenization</title>
      <author><first>Zachary William</first><last>Hopton</last><affiliation>University of Zurich and University of Zurich</affiliation></author>
      <author><first>Yves</first><last>Scherrer</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Tanja</first><last>Samardzic</last><affiliation>University of Zurich</affiliation></author>
      <pages>7839-7853</pages>
      <abstract>The distinction between function and content units of the lexicon has been somewhat neglected in recent NLP work, but it could still be useful when working with low-resource languages, and, in particular, to improve cross-lingual transfer. In this paper, we investigate to what extent BPE subword tokenization can be used to identify units of the functional lexicon in a language without any annotated data. We analyze subword tokens in terms of their productivity and attempt to find thresholds that best distinguish function from content tokens. On a sample of seven diverse languages, we find that the best results are obtained with 50 BPE merges. We also show that this subword tokenization setting can be beneficial for the interlinear glossing task.</abstract>
      <url hash="80038ae3">2025.naacl-long.398</url>
      <bibkey>hopton-etal-2025-functional</bibkey>
    </paper>
    <paper id="399">
      <title>Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data</title>
      <author><first>Haonan</first><last>Wang</last></author>
      <author><first>Minbin</first><last>Huang</last></author>
      <author><first>Runhui</first><last>Huang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Lanqing</first><last>Hong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hang</first><last>Xu</last><affiliation>Huawei Noah‘s Ark Lab</affiliation></author>
      <author><first>Tianyang</first><last>Hu</last></author>
      <author><first>Xiaodan</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>7854-7873</pages>
      <abstract>Contrastive Language-Image Pre-training (CLIP) has become the standard for cross- modal image-text representation learning. Improving CLIP typically requires additional data and retraining with new loss functions, but these demands raise resource and time costs, limiting practical use. In this work, we introduce HELIP, a cost-effective strategy that improves CLIP models by exploiting challenging text-image pairs within existing datasets in continuous training. This eliminates the need for additional data or extensive retraining. Moreover, HELIP integrates effortlessly into current training pipelines with minimal code modifications, allowing for quick and seamless implementation. On comprehensive benchmarks, HELIP consistently boosts existing models. In particular, within just two epochs of training, it improves zero-shot classification accuracy on ImageNet for SLIP models pre-trained on CC3M, CC12M, and YFCC15M datasets by 3.05%, 4.47%, and 10.1% , respectively. In addition, on fine-grained classification datasets, HELIP improves the zero-shot performance of CLIP and SLIP by an average of 8.4% and 18.6%, and their linear probe performance by an average of 9.5% and 3.0%.</abstract>
      <url hash="3f475045">2025.naacl-long.399</url>
      <bibkey>wang-etal-2025-getting</bibkey>
    </paper>
    <paper id="400">
      <title>Evaluating the Prompt Steerability of Large Language Models</title>
      <author><first>Erik</first><last>Miehling</last><affiliation>IBM Research</affiliation></author>
      <author><first>Michael</first><last>Desmond</last></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elizabeth M.</first><last>Daly</last><affiliation>IBM Research</affiliation></author>
      <author><first>Kush R.</first><last>Varshney</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Eitan</first><last>Farchi</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Pierre</first><last>Dognin</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jesus</first><last>Rios</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Djallel</first><last>Bouneffouf</last></author>
      <author><first>Miao</first><last>Liu</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Prasanna</first><last>Sattigeri</last><affiliation>IBM Research</affiliation></author>
      <pages>7874-7900</pages>
      <abstract>Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerability of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model’s joint behavioral distribution can be shifted from its baseline. By defining steerability indices and inspecting how these indices change as a function of steering effort, we can estimate the steerability of a model across various persona dimensions and directions. Our benchmark reveals that the steerability of many current models is limited — due to both a skew in their baseline behavior and an asymmetry in their steerability across many persona dimensions. We release an implementation of our benchmark at https://github.com/IBM/prompt-steering.</abstract>
      <url hash="5a265f43">2025.naacl-long.400</url>
      <bibkey>miehling-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="401">
      <title>A Data-Driven Method for Analyzing and Quantifying Lyrics-Dance Motion Relationships</title>
      <author><first>Kento</first><last>Watanabe</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Masataka</first><last>Goto</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>7901-7916</pages>
      <abstract>Dancing to music with lyrics is a popular form of expression. While it is generally accepted that there are relationships between lyrics and dance motions, previous studies have not explored these relationships. A major challenge is that the relationships between lyrics and dance motions are not constant throughout a song but are instead localized to specific parts. To address this challenge, we hypothesize that lyrics and dance motions that co-occur across multiple songs are related. Based on this hypothesis, we propose a novel data-driven method to detect the parts of songs where meaningful relationships between lyrics and dance motions exist. We use clustering to transform lyrics and dance motions into symbols, enabling the calculation of co-occurrence frequencies and detection of significant correlations. The effectiveness of our method is validated by a dataset of time-synchronized lyrics and dance motions, which showed high correlation values for emotionally salient lyrics such as “love”, which is expressed in heart-shaped motions. Furthermore, using our relationship detection method, we propose a method for retrieving dance motions from lyrics that outperforms previous text-to-motion retrieval methods, which focus on prose and non-dance motions.</abstract>
      <url hash="295c9ddd">2025.naacl-long.401</url>
      <bibkey>watanabe-goto-2025-data</bibkey>
    </paper>
    <paper id="402">
      <title><fixed-case>CROPE</fixed-case>: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts</title>
      <author><first>Malvina</first><last>Nikandrou</last></author>
      <author><first>Georgios</first><last>Pantazopoulos</last></author>
      <author><first>Nikolas</first><last>Vitsakis</last></author>
      <author><first>Ioannis</first><last>Konstas</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Alessandro</first><last>Suglia</last><affiliation>Heriot-Watt University</affiliation></author>
      <pages>7917-7936</pages>
      <abstract>As Vision and Language models (VLMs) become accessible across the globe, it is important that they demonstrate cultural knowledge. In his paper, we introduce CROPE, a visual question answering benchmark designed to probe the knowledge of culture-specific concepts and evaluate the capacity for cultural adaptation through contextual information. This allows us to distinguish between parametric knowledge acquired during training and contextual knowledge provided during inference via visual and textual descriptions. Our evaluation of several state-of-the-art open VLMs shows large performance disparities between culture-specific and common concepts in the parametric setting. Moreover, experiments with contextual knowledge indicate that models struggle to effectively utilize multimodal information and bind culture specific concepts to their depictions. Our findings reveal limitations in the cultural understanding and adaptability of current VLMs that need to be addressed toward more culturally inclusive models.</abstract>
      <url hash="849fb816">2025.naacl-long.402</url>
      <bibkey>nikandrou-etal-2025-crope</bibkey>
    </paper>
    <paper id="403">
      <title><fixed-case>P</fixed-case>ic<fixed-case>P</fixed-case>ersona-<fixed-case>TOD</fixed-case> : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona</title>
      <author><first>Jihyun</first><last>Lee</last></author>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Seungyeon</first><last>Seo</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>7937-7958</pages>
      <abstract>Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users’ personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains.</abstract>
      <url hash="79adbd28">2025.naacl-long.403</url>
      <bibkey>lee-etal-2025-picpersona</bibkey>
    </paper>
    <paper id="404">
      <title>Scaling <fixed-case>LLM</fixed-case> Inference Efficiently with Optimized Sample Compute Allocation</title>
      <author><first>Kexun</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shang</first><last>Zhou</last></author>
      <author><first>Danqing</first><last>Wang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>7959-7973</pages>
      <abstract>Sampling is a basic operation for large language models (LLMs). In reinforcement learning rollouts and meta generation algorithms such as Best-of-N, it is essential to sample correct trajectories within a given compute budget. To find an optimal allocation for sample compute budgets, several choices need to be made:Which sampling configurations (model, temperature, language, etc.) to use?How many samples to generate in each configuration?We formulate these choices as a learning problem and propose OSCA, an algorithm that Optimizes Sample Compute Allocation by finding an optimal mix of different inference configurations.Our experiments show that with our learned mixed allocation, we can achieve accuracy better than the best single configuration with 128x less compute on code generation and 25x less compute on 4 reasoning tasks.is also shown to be effective in agentic workflows beyond single-turn tasks, achieving a better accuracy on SWE-Bench with 3x less compute than the default configuration.Our code and generations are released at https://github.com/LeiLiLab/OSCA.</abstract>
      <url hash="e548881c">2025.naacl-long.404</url>
      <bibkey>zhang-etal-2025-scaling</bibkey>
    </paper>
    <paper id="405">
      <title>Large Language Models for <fixed-case>P</fixed-case>ersian-<fixed-case>E</fixed-case>nglish Idiom Translation</title>
      <author><first>Sara</first><last>Rezaeimanesh</last></author>
      <author><first>Faezeh</first><last>Hosseini</last><affiliation>Teias</affiliation></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <pages>7974-7985</pages>
      <abstract>Large language models (LLMs) have shown superior capabilities in translating figurative language compared to neural machine translation (NMT) systems. However, the impact of different prompting methods and LLM-NMT combinations on idiom translation has yet to be thoroughly investigated. This paper introduces two parallel datasets of sentences containing idiomatic expressions for Persian<tex-math>\rightarrow</tex-math>English and English<tex-math>\rightarrow</tex-math>Persian translations, with Persian idioms sampled from our PersianIdioms resource, a collection of 2,200 idioms and their meanings, with 700 including usage examples.Using these datasets, we evaluate various open- and closed-source LLMs, NMT models, and their combinations. Translation quality is assessed through idiom translation accuracy and fluency. We also find that automatic evaluation methods like LLM-as-a-judge, BLEU, and BERTScore are effective for comparing different aspects of model performance. Our experiments reveal that Claude-3.5-Sonnet delivers outstanding results in both translation directions. For English<tex-math>\rightarrow</tex-math>Persian, combining weaker LLMs with Google Translate improves results, while Persian<tex-math>\rightarrow</tex-math>English translations benefit from single prompts for simpler models and complex prompts for advanced ones.</abstract>
      <url hash="e89c7d4f">2025.naacl-long.405</url>
      <bibkey>rezaeimanesh-etal-2025-large</bibkey>
    </paper>
    <paper id="406">
      <title>Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities</title>
      <author><first>Kourosh T</first><last>Baghaei</last><affiliation>George Mason University</affiliation></author>
      <author><first>Dieter</first><last>Pfoser</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>7986-8005</pages>
      <abstract>Vision and language navigation (VLN) is a challenging task towards the creation of embodied agents that requires spatial and temporal reasoning over the instructions provided in natural language and aligning them with the visual perception of an environment. Although a number of methods and approaches have been developed, none achieves human level performance in outdoor settings (by up to 75 percent). The contributions of visual and language modalities to the success of VLN have been studied, however here we focus on an overlooked property of routes and show that navigational instructions can be represented as patterns of actions that also describe trajectory shapes. Through carefully crafted experiments, we show that agents generalization to unseen environments depends not only on visual and linguistic features, but also on the shape of trajectories presented to the model during the fine-tuning. Our experiments show that the diversity of patterns of actions during training is a key contributor to high success rates for agents. Last, we propose a solution based on data augmentation that fills the gap in missing patterns of training data. Our findings will guide researchers towards improved practices in the development and evaluation of VLN datasets and agents.</abstract>
      <url hash="91728419">2025.naacl-long.406</url>
      <bibkey>baghaei-etal-2025-follow</bibkey>
    </paper>
    <paper id="407">
      <title>Sneaking Syntax into Transformer Language Models with Tree Regularization</title>
      <author><first>Ananjan</first><last>Nandi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher D</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Shikhar</first><last>Murty</last><affiliation>Stanford University</affiliation></author>
      <pages>8006-8024</pages>
      <abstract>While compositional accounts of human language understanding are based on a hierarchical tree-like process, neural models like transformers lack a direct inductive bias for such tree structures. Introducing syntactic inductive biases could unlock more robust and data-efficient learning in transformer language models (LMs), but existing methods for incorporating such structure greatly restrict models, either limiting their expressivity or increasing inference complexity. This work instead aims to softly inject syntactic inductive biases into given transformer circuits, through a structured regularizer. We introduce TreeReg, an auxiliary loss function that converts bracketing decisions from silver parses into a set of differentiable orthogonality constraints on vector hidden states. TreeReg integrates seamlessly with the standard LM objective, requiring no architectural changes. LMs pre-trained with TreeReg on natural language corpora such as WikiText-103 achieve up to 10% lower perplexities on out-of-distribution data and up to 9.5 point improvements in syntactic generalization, requiring less than half the training data to outperform standard LMs. TreeReg still provides gains for pre-trained LLMs: Continued pre-training of Sheared Llama with TreeReg results in improved syntactic generalization, and fine-tuning on MultiNLI with TreeReg mitigates degradation of performance on adversarial NLI benchmarks by 41.2 points. We release all code to guide future research.</abstract>
      <url hash="0ff1be3a">2025.naacl-long.407</url>
      <bibkey>nandi-etal-2025-sneaking</bibkey>
    </paper>
    <paper id="408">
      <title>Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness</title>
      <author><first>Sougata</first><last>Saha</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Saurabh Kumar</first><last>Pandey</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8025-8042</pages>
      <abstract>Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, “culture” is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess “cultural awareness”, and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.</abstract>
      <url hash="08067898">2025.naacl-long.408</url>
      <bibkey>saha-etal-2025-meta</bibkey>
    </paper>
    <paper id="409">
      <title>Reading between the Lines: Can <fixed-case>LLM</fixed-case>s Identify Cross-Cultural Communication Gaps?</title>
      <author><first>Sougata</first><last>Saha</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Saurabh Kumar</first><last>Pandey</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Harshit</first><last>Gupta</last></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8043-8067</pages>
      <abstract>In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines.</abstract>
      <url hash="1d73d8c0">2025.naacl-long.409</url>
      <bibkey>saha-etal-2025-reading</bibkey>
    </paper>
    <paper id="410">
      <title><fixed-case>HMT</fixed-case>: Hierarchical Memory Transformer for Efficient Long Context Language Processing</title>
      <author><first>Zifan</first><last>He</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yingqi</first><last>Cao</last></author>
      <author><first>Zongyue</first><last>Qin</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Neha</first><last>Prakriya</last></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jason</first><last>Cong</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>8068-8089</pages>
      <abstract>Transformer-based large language models (LLM) have been widely used in language processing applications. However, due to the memory constraints of the devices, most of them restrict the context window. Even though recurrent models in previous works can memorize past tokens to enable unlimited context and maintain effectiveness, they have “flat” memory architectures. Such architectures have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we believe that imitating brain memory hierarchy is beneficial for model memorization. Thus, we propose the Hierarchical Memory Transformer (HMT), a novel framework that facilitates a model’s long-context processing ability by imitating human memorization behavior. Leveraging memory-augmented segment-level recurrence, we organize the memory hierarchy by preserving tokens from early input segments, passing memory embeddings along the sequence, and recalling relevant information from history. Evaluating general language modeling, question-answering tasks, and the summarization task, we show that HMT consistently improves the long-context processing ability of existing models. Furthermore, HMT achieves a comparable or superior generation quality to long-context LLMs with <tex-math>2 \sim 57\times</tex-math> fewer parameters and <tex-math>2.5 \sim 116\times</tex-math> less inference memory, significantly outperforming previous memory-augmented models.</abstract>
      <url hash="1848bdb7">2025.naacl-long.410</url>
      <bibkey>he-etal-2025-hmt</bibkey>
    </paper>
    <paper id="411">
      <title>Faux Polyglot: A Study on Information Disparity in Multilingual Large Language Models</title>
      <author><first>Nikhil</first><last>Sharma</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ziang</first><last>Xiao</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>8090-8107</pages>
      <abstract>Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences? In this paper, we studied LLM’s linguistic preference in a cross-language RAG-based information search setting. We found that LLMs displayed systemic bias towards information in the same language as the query language in both document retrieval and answer generation. Furthermore, in scenarios where no information is in the language of the query, LLMs prefer documents in high-resource languages during generation, potentially reinforcing the dominant views. Such bias exists for both factual and opinion-based queries. Our results highlight the linguistic divide within multilingual LLMs in information search systems. The seemingly beneficial multilingual capability of LLMs may backfire on information parity by reinforcing language-specific filter bubbles further marginalizing low-resource views.</abstract>
      <url hash="6b452cb9">2025.naacl-long.411</url>
      <bibkey>sharma-etal-2025-faux</bibkey>
    </paper>
    <paper id="412">
      <title>Teaching Models to Balance Resisting and Accepting Persuasion</title>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Peter</first><last>Hase</last><affiliation>Anthropic</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>8108-8122</pages>
      <abstract>Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. *negative*) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. *positive*) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce **P**ersuasion-**B**alanced **T**raining (or **PBT**), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion *when appropriate*. PBT allows us to use data generated from dialogues between smaller 7-8B models for training much larger 70B models. Moreover, PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates across two domains (trivia and commonsense QA). We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model’s performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.</abstract>
      <url hash="168cb1c9">2025.naacl-long.412</url>
      <bibkey>stengel-eskin-etal-2025-teaching</bibkey>
    </paper>
    <paper id="413">
      <title>Making Language Models Robust Against Negation</title>
      <author><first>MohammadHossein</first><last>Rezaei</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <pages>8123-8142</pages>
      <abstract>Negation has been a long-standing challenge for language models.Previous studies have shown that they struggle with negation in many natural language understanding tasks.In this work, we propose a self-supervised method to make language models more robust against negation.We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task.We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf versions on nine negation-related benchmarks.Most notably, our pre-training tasks yield between 1.8% and 9.1% improvement on CondaQA, a large question-answering corpus requiring reasoning over negation.</abstract>
      <url hash="f619bb63">2025.naacl-long.413</url>
      <bibkey>rezaei-blanco-2025-making</bibkey>
    </paper>
    <paper id="414">
      <title>Through the Lens of History: Methods for Analyzing Temporal Variation in Content and Framing of State-run <fixed-case>C</fixed-case>hinese Newspapers</title>
      <author><first>Shijia</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>David A.</first><last>Smith</last><affiliation>Northeastern University</affiliation></author>
      <pages>8143-8172</pages>
      <abstract>State-run Chinese newspapers are believed to strategically select and frame news articles to align with the shifting political tides of the country. This paper describes methods to quantify these changes in content and framing over time. Looking at more than 50 years of articles from the People’s Daily and Reference News, we analyze differences in name mentions and sentiment in news articles for politicians before and after their deaths, as well as during and not during certain political events. We find significant estimates of difference, reflecting the changes in various aspects of the political environment in China during different time periods. We also apply change point detection methods to identify turning points in time series data of name mentions and sentiment. The identified turning points show a high co-occurrence with crucial political events and deaths of politicians. Furthermore, we utilize topic modeling to analyze the framing choices for articles written in different decades. The changes in frequent topic words are more significant in People’s Daily than in Reference News, which is consistent with the focus shifts of the Chinese central government in history. Finally, by using pre-trained language models to predict masked names in news articles, we analyze the distinctiveness of the language used to report individuals.</abstract>
      <url hash="34a230df">2025.naacl-long.414</url>
      <bibkey>liu-smith-2025-lens</bibkey>
    </paper>
    <paper id="415">
      <title><fixed-case>P</fixed-case>oisoned<fixed-case>P</fixed-case>arrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models</title>
      <author><first>Michael-Andrei</first><last>Panaitescu-Liess</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Pankayaraj</first><last>Pathmanathan</last></author>
      <author><first>Yigitcan</first><last>Kaya</last></author>
      <author><first>Zora</first><last>Che</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Bang</first><last>An</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Sicheng</first><last>Zhu</last></author>
      <author><first>Aakriti</first><last>Agrawal</last></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <pages>8173-8190</pages>
      <abstract>As the capabilities of large language models (LLMs) continue to expand, their usage has become increasingly prevalent. However, as reflected in numerous ongoing lawsuits regarding LLM-generated content, addressing copyright infringement remains a significant challenge. In this paper, we introduce PoisonedParrot: the first <i>stealthy</i> data poisoning attack that induces an LLM to generate copyrighted content even when the model has not been directly trained on the specific copyrighted material. PoisonedParrot integrates small fragments of copyrighted text into the poison samples using an off-the-shelf LLM. Despite its simplicity, evaluated in a wide range of experiments, PoisonedParrot is surprisingly effective at priming the model to generate copyrighted content with no discernible side effects. Moreover, we discover that existing defenses are largely ineffective against our attack. Finally, we make the first attempt at mitigating copyright-infringement poisoning attacks by proposing a defense: ParrotTrap. We encourage the community to explore this emerging threat model further.</abstract>
      <url hash="a622223f">2025.naacl-long.415</url>
      <bibkey>panaitescu-liess-etal-2025-poisonedparrot</bibkey>
    </paper>
    <paper id="416">
      <title>Towards Operationalizing Right to Data Protection</title>
      <author><first>Abhinav</first><last>Java</last><affiliation>Microsoft</affiliation></author>
      <author><first>Simra</first><last>Shahid</last><affiliation>University of Virginia, Charlottesville and Adobe Systems</affiliation></author>
      <author><first>Chirag</first><last>Agarwal</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>8191-8205</pages>
      <abstract>The widespread practice of indiscriminate data scraping to fine-tune language models (LMs) raises significant legal and ethical concerns, particularly regarding compliance with data protection laws such as the General Data Protection Regulation (GDPR). This practice often results in the unauthorized use of personal information, prompting growing debate within the academic and regulatory communities. Recent works have introduced the concept of generating unlearnable datasets (by adding imperceptible noise to the clean data), such that the underlying model achieves lower loss during training but fails to generalize to the unseen test setting. Though somewhat effective, these approaches are predominantly designed for images and are limited by several practical constraints like requiring knowledge of the target model. To this end, we introduce **RegText**, a framework that injects imperceptible spurious correlations into natural language datasets, effectively rendering them unlearnable without affecting semantic content. We demonstrate RegText’s utility through rigorous empirical analysis of small and large LMs. Notably, RegText can restrict newer models like GPT-4o and Llama from learning on our generated data, resulting in a drop in their test accuracy compared to their zero-shot performance and paving the way for generating unlearnable text to protect public data.</abstract>
      <url hash="d0267822">2025.naacl-long.416</url>
      <bibkey>java-etal-2025-towards</bibkey>
    </paper>
    <paper id="417">
      <title>Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models</title>
      <author><first>Aliakbar</first><last>Nafar</last></author>
      <author><first>K. Brent</first><last>Venable</last><affiliation>University of West Florida and Florida Institute of Human and Machine Cognition</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>8206-8229</pages>
      <abstract>Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can solve real-world regression problems and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.</abstract>
      <url hash="11cdc928">2025.naacl-long.417</url>
      <bibkey>nafar-etal-2025-learning</bibkey>
    </paper>
    <paper id="418">
      <title><fixed-case>GL</fixed-case>i<fixed-case>REL</fixed-case> - Generalist Model for Zero-Shot Relation Extraction</title>
      <author><first>Jack</first><last>Boylan</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chris</first><last>Hokamp</last><affiliation>Quantexa</affiliation></author>
      <author><first>Demian Gholipour</first><last>Ghalandari</last><affiliation>Quantexa</affiliation></author>
      <pages>8230-8245</pages>
      <abstract>We introduce GLiREL, an efficient architecture and training paradigm for zero-shot relation classification. Identifying relationships between entities is a key task in information extraction pipelines. The zero-shot setting for relation extraction, where a taxonomy of relations is not pre-specified, has proven to be particularly challenging because of the computational complexity of inference, and because of the lack of labeled training data with sufficient coverage. Existing approaches rely upon distant supervision using auxiliary models to generate training data for unseen labels, upon very large general-purpose large language models (LLMs), or upon complex pipelines models with multiple inference stages. Inspired by the recent advancements in zero-shot named entity recognition, this paper introduces an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels.</abstract>
      <url hash="d6d94b19">2025.naacl-long.418</url>
      <bibkey>boylan-etal-2025-glirel</bibkey>
    </paper>
    <paper id="419">
      <title><fixed-case>C</fixed-case>om<fixed-case>PO</fixed-case>: Community Preferences for Language Model Personalization</title>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>8246-8279</pages>
      <abstract>Conventional algorithms for training language models (LMs) with human feedback rely on preferences that are assumed to account for an “average” user, disregarding subjectivity and finer-grained variations. Recent studies have raised concerns that aggregating such diverse and often contradictory human feedback to finetune models results in generic models that generate outputs not preferred by many user groups, as they tend to average out styles and norms. To address this issue, we draw inspiration from recommendation systems and propose ComPO, a method to personalize preference optimization in LMs by contextualizing the probability distribution of model outputs with the preference provider. Focusing on group-level preferences rather than individuals, we collect and release ComPRed, a question answering dataset with community-level preferences from Reddit. This dataset facilitates studying diversity in preferences without incurring privacy concerns associated with individual feedback. Our experiments reveal that conditioning language models on a community identifier (i.e., subreddit name) during preference tuning substantially enhances model performance. Conversely, replacing this context with random subreddit identifiers significantly diminishes performance, highlighting the effectiveness of our approach in tailoring responses to communities’ preferences.</abstract>
      <url hash="56dc6634">2025.naacl-long.419</url>
      <bibkey>kumar-etal-2025-compo</bibkey>
    </paper>
    <paper id="420">
      <title><fixed-case>G</fixed-case>round<fixed-case>C</fixed-case>ocoa: A Benchmark for Evaluating Compositional &amp; Conditional Reasoning in Language Models</title>
      <author><first>Harsh</first><last>Kohli</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <pages>8280-8295</pages>
      <abstract>The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two aspects that are central to human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.</abstract>
      <url hash="954069c5">2025.naacl-long.420</url>
      <bibkey>kohli-etal-2025-groundcocoa</bibkey>
    </paper>
    <paper id="421">
      <title><fixed-case>ALPACA</fixed-case> <fixed-case>AGAINST</fixed-case> <fixed-case>VICUNA</fixed-case>: Using <fixed-case>LLM</fixed-case>s to Uncover Memorization of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aly M.</first><last>Kassem</last></author>
      <author><first>Omar</first><last>Mahmoud</last></author>
      <author><first>Niloofar</first><last>Mireshghallah</last></author>
      <author><first>Hyunwoo</first><last>Kim</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Sherif</first><last>Saad</last><affiliation>University of Windsor</affiliation></author>
      <author><first>Santu</first><last>Rana</last><affiliation>Deakin University</affiliation></author>
      <pages>8296-8321</pages>
      <abstract>In this paper, we investigate the overlooked impact of instruction-tuning on memorization in large language models (LLMs), which has largely been studied in base, pre-trained models. We propose a black-box prompt optimization method where an attacker LLM agent uncovers higher levels of memorization in a victim agent, surpassing traditional approaches that prompt the model directly with training data. Using an iterative rejection-sampling process, we design instruction-based prompts that minimize overlap with training data to avoid providing direct solutions while maximizing overlap between the victim’s output and the training data to induce memorization. Our method shows 23.7% more overlap with training data compared to state-of-the-art baselines. We explore two attack settings: an analytical approach that determines the empirical upper bound of the attack, both with and without access to responses for prompt initialization, and a practical classifier-based method for assessing memorization without access to memorized data. Our findings reveal that instruction-tuned models can expose pre-training data as much as, or more than, base models; contexts beyond the original training data can lead to leakage; and instructions generated by other LLMs open new avenues for automated attacks, which we believe require further exploration.</abstract>
      <url hash="ba3af19f">2025.naacl-long.421</url>
      <bibkey>kassem-etal-2025-alpaca</bibkey>
    </paper>
    <paper id="422">
      <title>Evaluating Contextualized Representations of (<fixed-case>S</fixed-case>panish) Ambiguous Words: A New Lexical Resource and Empirical Analysis</title>
      <author><first>Pamela D</first><last>Riviere</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Anne L.</first><last>Beatty-Martínez</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Sean</first><last>Trott</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>8322-8338</pages>
      <abstract>Lexical ambiguity—where a single wordform takes on distinct, context-dependent meanings–serves as a useful tool to compare across different language models’ (LMs’) ability to form distinct, contextualized representations of the same stimulus. Few studies have systematically compared LMs’ contextualized word embeddings for languages beyond English. Here, we evaluate semantic representations of Spanish ambiguous nouns in context in a suite of Spanish-language monolingual and multilingual BERT-based models. We develop a novel dataset of minimal-pair sentences evoking the same or different sense for a target ambiguous noun. In a pre-registered study, we collect contextualized human relatedness judgments for each sentence pair. We find that various BERT-based LMs’ contextualized semantic representations capture some variance in human judgments but fall short of the human benchmark. In exploratory work, we find that performance scales with model size. We also identify stereotyped trajectories of target noun disambiguation as a proportion of traversal through a given LM family’s architecture, which we partially replicate in English. We contribute (1) a dataset of controlled, Spanish sentence stimuli with human relatedness norms, and (2) to our evolving understanding of the impact that LM specification (architectures, training protocols) exerts on contextualized embeddings.</abstract>
      <url hash="3991ff33">2025.naacl-long.422</url>
      <bibkey>riviere-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="423">
      <title>Understanding <fixed-case>LLM</fixed-case>s’ Fluid Intelligence Deficiency: An Analysis of the <fixed-case>ARC</fixed-case> Task</title>
      <author><first>Junjie</first><last>Wu</last><affiliation>HKUST</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Dit-Yan</first><last>Yeung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>8339-8360</pages>
      <abstract>While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs’ parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs’ abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding. Our data and code will be publicly released, and the data is also attached in the submission.</abstract>
      <url hash="0b1161c0">2025.naacl-long.423</url>
      <bibkey>wu-etal-2025-understanding</bibkey>
    </paper>
    <paper id="424">
      <title><fixed-case>F</fixed-case>ed<fixed-case>S</fixed-case>pa<fixed-case>LLM</fixed-case>: Federated Pruning of Large Language Models</title>
      <author><first>Guangji</first><last>Bai</last></author>
      <author><first>Yijiang</first><last>Li</last><affiliation>Argonne National Laboratory</affiliation></author>
      <author><first>Zilinghan</first><last>Li</last><affiliation>Argonne National Laboratory</affiliation></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>Emory University</affiliation></author>
      <author><first>Kibaek</first><last>Kim</last><affiliation>Argonne National Laboratory</affiliation></author>
      <pages>8361-8373</pages>
      <abstract>Large Language Models (LLMs) achieve state-of-the-art performance but are challenging to deploy due to their high computational and storage demands. Pruning can reduce model size, yet existing methods assume public access to calibration data, which is impractical for privacy-sensitive applications. To address the challenge of pruning LLMs in privacy-preserving settings, we propose FedSpaLLM, the first federated learning framework designed specifically for pruning LLMs. FedSpaLLM enables clients to locally prune their models based on private data while accounting for system heterogeneity and maintaining communication efficiency. Our framework introduces several key innovations: (1) a novel <tex-math>\ell_0</tex-math>-norm aggregation function that ensures only non-zero weights are averaged across clients, preserving important model parameters; (2) an adaptive mask expansion technique that meets global sparsity targets while accommodating client-specific pruning decisions; and (3) a layer sampling strategy that reduces communication overhead and personalizes the pruning process based on client resources. Extensive experiments show that FedSpaLLM improves pruning performance in diverse federated settings.</abstract>
      <url hash="12f6c1c9">2025.naacl-long.424</url>
      <bibkey>bai-etal-2025-fedspallm</bibkey>
    </paper>
    <paper id="425">
      <title><fixed-case>IHE</fixed-case>val: Evaluating Language Models on Following the Instruction Hierarchy</title>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Shiyang</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Haoming</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yifan</first><last>Gao</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Haodong</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yichuan</first><last>Li</last></author>
      <author><first>Qingyu</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>8374-8398</pages>
      <abstract>The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models’ ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.</abstract>
      <url hash="ccc31857">2025.naacl-long.425</url>
      <bibkey>zhang-etal-2025-iheval</bibkey>
    </paper>
    <paper id="426">
      <title>Afrispeech-Dialog: A Benchmark Dataset for Spontaneous <fixed-case>E</fixed-case>nglish Conversations in Healthcare and Beyond</title>
      <author><first>Mardhiyah</first><last>Sanni</last><affiliation>Intron Health</affiliation></author>
      <author><first>Tassallah</first><last>Abdullahi</last></author>
      <author><first>Devendra Deepak</first><last>Kayande</last></author>
      <author><first>Emmanuel</first><last>Ayodele</last><affiliation>Intron Health</affiliation></author>
      <author><first>Naome A</first><last>Etori</last></author>
      <author><first>Michael Samwel</first><last>Mollel</last></author>
      <author><first>Moshood O.</first><last>Yekini</last><affiliation>Masakhane</affiliation></author>
      <author><first>Chibuzor</first><last>Okocha</last></author>
      <author><first>Lukman Enegi</first><last>Ismaila</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Folafunmi</first><last>Omofoye</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Boluwatife A.</first><last>Adewale</last></author>
      <author><first>Tobi</first><last>Olatunji</last></author>
      <pages>8399-8417</pages>
      <abstract>Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form, accented speech, comparing their performance with native accents and discover a 10%+ performance degradation. Additionally, we explore medical conversation summarization capabilities of large language models (LLMs) to demonstrate the impact of ASR errors on downstream medical summaries, providing insights into the challenges and opportunities for speech technologies in the Global South. Our work highlights the need for more inclusive datasets to advance conversational AI in low-resource settings.</abstract>
      <url hash="0199c71c">2025.naacl-long.426</url>
      <bibkey>sanni-etal-2025-afrispeech</bibkey>
    </paper>
    <paper id="427">
      <title><fixed-case>THREAD</fixed-case>: Thinking Deeper with Recursive Spawning</title>
      <author><first>Philip</first><last>Schroeder</last></author>
      <author><first>Nathaniel W.</first><last>Morgan</last></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>James R.</first><last>Glass</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>8418-8442</pages>
      <abstract>Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.</abstract>
      <url hash="0720d613">2025.naacl-long.427</url>
      <bibkey>schroeder-etal-2025-thread</bibkey>
    </paper>
    <paper id="428">
      <title><fixed-case>CORG</fixed-case>: Generating Answers from Complex, Interrelated Contexts</title>
      <author><first>Hyunji</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <pages>8443-8460</pages>
      <abstract>In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (COrg), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. COrg consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that COrg balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.</abstract>
      <url hash="85675151">2025.naacl-long.428</url>
      <bibkey>lee-etal-2025-corg</bibkey>
    </paper>
    <paper id="429">
      <title>Generating Diverse Hypotheses for Inductive Reasoning</title>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyukhun</first><last>Koh</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Minsung</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>8461-8474</pages>
      <abstract>Inductive reasoning — the process of inferring general rules from a small number of observations — is a fundamental aspect of human intelligence. Recent works suggest that large language models (LLMs) can engage in inductive reasoning by sampling multiple hypotheses about the rules and selecting the one that best explains the observations. However, due to the IID sampling, semantically redundant hypotheses are frequently generated, leading to significant wastage of compute. In this paper, we 1) demonstrate that increasing the temperature to enhance the diversity is limited due to text degeneration issue, and 2) propose a novel method to improve the diversity while maintaining text quality. We first analyze the effect of increasing the temperature parameter, which is regarded as the LLM’s diversity control, on IID hypotheses. Our analysis shows that as temperature rises, diversity and accuracy of hypotheses increase up to a certain point, but this trend saturates due to text degeneration. To generate hypotheses that are more semantically diverse and of higher quality, we propose a novel approach inspired by human inductive reasoning, which we call Mixture of Concepts (MoC). When applied to several inductive reasoning benchmarks, MoC demonstrated significant performance improvements compared to standard IID sampling and other approaches.</abstract>
      <url hash="7c637def">2025.naacl-long.429</url>
      <bibkey>lee-etal-2025-generating</bibkey>
    </paper>
    <paper id="430">
      <title>On the Analysis and Distillation of Emergent Outlier Properties in Pre-trained Language Models</title>
      <author><first>Tianyang</first><last>Zhao</last><affiliation>Amazon</affiliation></author>
      <author><first>Kunwar Yashraj</first><last>Singh</last><affiliation>Amazon</affiliation></author>
      <author><first>Srikar</first><last>Appalaraju</last><affiliation>Amazon</affiliation></author>
      <author><first>Peng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Ying Nian</first><last>Wu</last><affiliation>UCLA</affiliation></author>
      <author><first>Li Erran</first><last>Li</last><affiliation>Amazon and Columbia University</affiliation></author>
      <pages>8475-8507</pages>
      <abstract>A small subset of dimensions within language Transformers’ representation spaces emerge as “outliers” during pretraining, encoding critical knowledge sparsely. We extend previous findings on emergent outliers to Encoder-Decoder Transformers and instruction-finetuned models, and tackle the problem of distilling a student Transformer from a larger teacher Transformer. Knowledge distillation reduces model size and cost by transferring knowledge from a larger teacher to a smaller student, necessitating a trade-off among representation dimensions. We show that emergent outlier dimensions contribute significantly more to zero-shot performance than non-outlier dimensions. Based on this, we propose the Emergent Outlier Focused Distillation (EOFD) method, which prioritizes critical outlier dimensions in distillation using a weighted MSE loss. We empirically demonstrate that EOFD outperforms state-of-the-art distillation methods and generalizes well across Encoder-only BERT, Decoder-only GPT-2, and Encoder-Decoder T5 architectures.</abstract>
      <url hash="9d2c6777">2025.naacl-long.430</url>
      <bibkey>zhao-etal-2025-analysis</bibkey>
    </paper>
    <paper id="431">
      <title>Open-World Evaluation for Retrieving Diverse Perspectives</title>
      <author><first>Hung-Ting</first><last>Chen</last><affiliation>New York University</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <pages>8508-8528</pages>
      <abstract>We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model-based automatic evaluator that decides whether each retrieved document contains a perspective. This allows us to evaluate the performance of three different types of corpus (Wikipedia, web snapshot, and corpus constructed on the fly with retrieved pages from the search engine) paired with retrievers. Retrieving diverse documents remains challenging, with the outputs from existing retrievers covering all perspectives on only 33.74% of the examples. We further study the impact of query expansion and diversity-focused reranking approaches and analyze retriever sycophancy. Together, we lay the foundation for future studies in retrieval diversity handling complex queries.</abstract>
      <url hash="c92ee72e">2025.naacl-long.431</url>
      <bibkey>chen-choi-2025-open</bibkey>
    </paper>
    <paper id="432">
      <title>Analyzing the Inner Workings of Transformers in Compositional Generalization</title>
      <author><first>Ryoma</first><last>Kumon</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>8529-8540</pages>
      <abstract>The compositional generalization abilities of neural models have been sought after for human-like linguistic competence.The popular method to evaluate such abilities is to assess the models’ input-output behavior.However, that does not reveal the internal mechanisms, and the underlying competence of such models in compositional generalization remains unclear.To address this problem, we explore the inner workings of a Transformer model byfinding an existing subnetwork that contributes to the generalization performance and by performing causal analyses on how the model utilizes syntactic features.We find that the model depends on syntactic features to output the correct answer, but that the subnetwork with much better generalization performance than the whole model relies on a non-compositional algorithm in addition to the syntactic features.We also show that the subnetwork improves its generalization performance relatively slowly during the training compared to the in-distribution one, and the non-compositional solution is acquired in the early stages of the training.</abstract>
      <url hash="5cb8c0da">2025.naacl-long.432</url>
      <bibkey>kumon-yanaka-2025-analyzing</bibkey>
    </paper>
    <paper id="433">
      <title>Substance Beats Style: Why Beginning Students Fail to Code with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Francesca</first><last>Lucchetti</last></author>
      <author><first>Zixuan</first><last>Wu</last></author>
      <author><first>Arjun</first><last>Guha</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Molly Q</first><last>Feldman</last><affiliation>Oberlin College</affiliation></author>
      <author><first>Carolyn Jane</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <pages>8541-8610</pages>
      <abstract>Although LLMs are increasing the productivity of professional programmers, existing work shows that beginners struggle to prompt LLMs to solve text-to-code tasks (Nguyen et al., 2024; Prather et al., 2024b; Mordechai et al., 2024). Why is this the case? This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs need to solve code generation tasks. We study (1) with a causal intervention experiment on technical vocabulary and (2) by analyzing graphs that abstract how students edit prompts and the different failures that they encounter. We find that substance beats style: a poor grasp of technical vocabulary is merely correlated with prompt failure; that the information content of prompts predicts success; that students get stuck making trivial edits; and more. Our findings have implications for the use of LLMs in programming education, and for efforts to make computing more accessible with LLMs.</abstract>
      <url hash="0bc15cd2">2025.naacl-long.433</url>
      <bibkey>lucchetti-etal-2025-substance</bibkey>
    </paper>
    <paper id="434">
      <title>Reverse Thinking Makes <fixed-case>LLM</fixed-case>s Stronger Reasoners</title>
      <author><first>Justin</first><last>Chen</last></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Hamid</first><last>Palangi</last><affiliation>Google</affiliation></author>
      <author><first>Rujun</first><last>Han</last><affiliation>Google</affiliation></author>
      <author><first>Sayna</first><last>Ebrahimi</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Swaroop</first><last>Mishra</last><affiliation>Google</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>8611-8630</pages>
      <abstract>Reverse thinking plays a crucial role in human reasoning. Humans can reason not only from a problem to a solution but also in reverse, i.e., start from the solution and reason towards the problem. This often enhances overall reasoning performance as it enables consistency checks between their forward and backward thinking. To enable Large Language Models (LLMs) to perform reverse thinking, we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data augmentation and learning objectives. In RevThink, we augment the dataset by collecting structured forward-backward reasoning from a teacher model, consisting of: (1) the original question, (2) forward reasoning, (3) backward question, and (4) backward reasoning. We then employ three objectives to train a smaller student model in a multi-task learning fashion: (a) generate forward reasoning from a question, (b) generate a backward question from a question, and (c) generate backward reasoning from the backward question. Experiments across 12 datasets covering commonsense, math, and logical reasoning show an average 13.53% improvement over the student model’s zero-shot performance and a 6.84% improvement over the strongest knowledge distillation baselines. Moreover, our method demonstrates sample efficiency – using only 10% of the correct forward reasoning from the training data, it outperforms a standard fine-tuning method trained on 10x more forward reasoning. RevThink also exhibits strong generalization to out-of-distribution held-out datasets.</abstract>
      <url hash="112567e9">2025.naacl-long.434</url>
      <bibkey>chen-etal-2025-reverse</bibkey>
    </paper>
    <paper id="435">
      <title>Towards Lifelong Dialogue Agents via Timeline-based Memory Management</title>
      <author><first>Kai Tzu-iunn</first><last>Ong</last></author>
      <author><first>Namyoung</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Minju</first><last>Gwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Taeyoon</first><last>Kwon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>8631-8661</pages>
      <abstract>To achieve lifelong human-agent interaction, dialogue agents need to constantly memorize perceived information and properly retrieve it for response generation (RG). While prior studies focus on getting rid of outdated memories to improve retrieval quality, we argue that such memories provide rich, important contextual cues for RG (e.g., changes in user behaviors) in long-term conversations. We present THEANINE, a framework for LLM-based lifelong dialogue agents. THEANINE discards memory removal and manages large-scale memories by linking them based on their temporal and cause-effect relation. Enabled by this linking structure, THEANINE augments RG with memory timelines - series of memories representing the evolution or causality of relevant past events. Along with THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme, addressing the limitation of G-Eval and human efforts when assessing agent performance in integrating past memories into RG. A supplementary video for THEANINE and data for TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.</abstract>
      <url hash="c6922802">2025.naacl-long.435</url>
      <bibkey>ong-etal-2025-towards</bibkey>
    </paper>
    <paper id="436">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>D</fixed-case>istance: Stronger Content-Independent Style Embeddings with Synthetic Parallel Examples</title>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Jiacheng</first><last>Zhu</last></author>
      <author><first>Justin</first><last>Qiu</last></author>
      <author><first>Zachary</first><last>Horvitz</last></author>
      <author><first>Marianna</first><last>Apidianaki</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <pages>8662-8685</pages>
      <abstract>Style representations aim to embed texts with similar writing styles closely and texts with different styles far apart, regardless of content. However, the contrastive triplets often used for training these representations may vary in both style and content, leading to potential content leakage in the representations. We introduce StyleDistance, a novel approach to training stronger content-independent style embeddings. We use a large language model to create a synthetic dataset of near-exact paraphrases with controlled style variations, and produce positive and negative examples across 40 distinct style features for precise contrastive learning. We assess the quality of our synthetic data and embeddings through human and automatic evaluations. StyleDistance enhances the content-independence of style embeddings, which generalize to real-world benchmarks and outperform leading style representations in downstream applications.</abstract>
      <url hash="5f521884">2025.naacl-long.436</url>
      <bibkey>patel-etal-2025-styledistance</bibkey>
    </paper>
    <paper id="437">
      <title><fixed-case>F</fixed-case>i<fixed-case>NE</fixed-case>: Filtering and Improving Noisy Data Elaborately with Large Language Models</title>
      <author><first>Junliang</first><last>He</last></author>
      <author><first>Ziyue</first><last>Fan</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shaohui</first><last>Kuang</last></author>
      <author><first>Li</first><last>Xiaoqing</last></author>
      <author><first>Kai</first><last>Song</last></author>
      <author><first>Yaqian</first><last>Zhou</last><affiliation>Fudan University, Tsinghua University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>8686-8707</pages>
      <url hash="63e02e5d">2025.naacl-long.437</url>
      <bibkey>he-etal-2025-fine</bibkey>
    </paper>
    <paper id="438">
      <title><fixed-case>CAMIE</fixed-case>val: Enhancing <fixed-case>NLG</fixed-case> Evaluation through Multidimensional Comparative Instruction-Following Analysis</title>
      <author><first>Ziyue</first><last>Fan</last><affiliation>Fudan University</affiliation></author>
      <author><first>Junliang</first><last>He</last></author>
      <author><first>Li</first><last>Xiaoqing</last></author>
      <author><first>Shaohui</first><last>Kuang</last></author>
      <author><first>Kai</first><last>Song</last></author>
      <author><first>Yaqian</first><last>Zhou</last><affiliation>Fudan University, Tsinghua University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>8708-8733</pages>
      <abstract>With the rapid development of large language models (LLMs), due to their strong performance across various fields, LLM-based evaluation methods (LLM-as-a-Judge) have become widely used in natural language generation (NLG) evaluation. However, these methods encounter the following challenges: (1) distinguishing instruction-following ability, (2) being applicable across diverse NLG tasks, and (3) identifying low-quality outputs. To address these issues, we propose CAMIEval, a multidimensional comparative evaluation method based on instruction-following. Specifically, we define three fundamental dimensions of instruction-following: relevance, factuality, and adherence. Subsequently, we introduce a concrete Chain-of-Thoughts (ConcreteCoT) process to enhance the accuracy of evaluations. In addition, we trained a “regrettable model” RegretLM to generate low-quality outputs, which helps the evaluator better identify the potential shortcomings of the candidate output by comparing low-quality outputs with reference outputs. Through this comparison, the evaluator can generate instruction-specific dimensions that complement the fundamental dimensions, forming a more comprehensive evaluation metric system. Experiments on two NLG evaluation benchmarks demonstrate that CAMIEval consistently outperforms existing methods in terms of correlation with human evaluations, providing a general and accurate framework for evaluating the outputs of LLMs.</abstract>
      <url hash="17b005b4">2025.naacl-long.438</url>
      <bibkey>fan-etal-2025-camieval</bibkey>
    </paper>
    <paper id="439">
      <title><fixed-case>L</fixed-case>ong<fixed-case>L</fixed-case>eader: A Comprehensive Leaderboard for Large Language Models in Long-context Scenarios</title>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Hongye</first><last>Jin</last><affiliation>Texas A&amp;M</affiliation></author>
      <author><first>Cheng-Che</first><last>Lee</last></author>
      <author><first>Rulin</first><last>Shao</last></author>
      <author><first>Jingfeng</first><last>Yang</last><affiliation>Amazon</affiliation></author>
      <author><first>Mingyu</first><last>Zhao</last></author>
      <author><first>Zhaoyu</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Qin</first><last>Lu</last><affiliation>Amazon</affiliation></author>
      <author><first>Kaiwen</first><last>Men</last></author>
      <author><first>Ning</first><last>Xie</last><affiliation>Amazon</affiliation></author>
      <author><first>Huasheng</first><last>Li</last></author>
      <author><first>Bing</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Han</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Lingyun</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <pages>8734-8750</pages>
      <abstract>Large Language Models (LLMs), exemplified by Claude and LLama, have exhibited impressive proficiency in tackling a myriad of Natural Language Processing (NLP) tasks. Yet, in pursuit of the ambitious goal of attaining Artificial General Intelligence (AGI), there remains ample room for enhancing LLM capabilities. Chief among these is the pressing need to bolster long-context comprehension. Numerous real-world scenarios demand LLMs to adeptly reason across extended contexts, such as multi-turn dialogues or agent workflow. Hence, recent advancements have been dedicated to stretching the upper bounds of long-context comprehension, with models like Claude 3 accommodating up to 200k tokens, employing various techniques to achieve this feat. Aligned with this progression, we propose a leaderboard LongLeader that seeks to comprehensively assess different long-context comprehension abilities of diverse LLMs and context length extension strategies across meticulously selected benchmarks. Specifically, we aim to address the following questions: 1) Do LLMs genuinely deliver the long-context proficiency they purport? 2) Which benchmarks offer reliable metrics for evaluating long-context comprehension? 3) What technical strategies prove effective in extending the understanding of longer contexts? We streamline the evaluation process for LLMs on the benchmarks, offering open-source access to the benchmarks and maintaining a dedicated website for leaderboards. We will continuously curate new datasets and update models to the leaderboards.</abstract>
      <url hash="8cf0d037">2025.naacl-long.439</url>
      <bibkey>chen-etal-2025-longleader</bibkey>
    </paper>
    <paper id="440">
      <title>Language Models Can Infer Action Semantics for Symbolic Planners from Environment Feedback</title>
      <author><first>Wang Bill</first><last>Zhu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ishika</first><last>Singh</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <pages>8751-8773</pages>
      <abstract>Symbolic planners can discover a sequence of actions from initial to goal states given expert-defined, domain-specific logical action semantics. Large Language Models (LLMs) can directly generate such sequences, but limitations in reasoning and state-tracking often result in plans that are insufficient or unexecutable. We propose Predicting Semantics of Actions with Language Models (PSALM), which automatically learns action semantics by leveraging the strengths of both symbolic planners and LLMs. PSALM repeatedly proposes and executes plans, using the LLM to partially generate plans and to infer domain-specific action semantics based on execution outcomes. PSALM maintains a belief over possible action semantics that is iteratively updated until a goal state is reached. Experiments on 7 environments show that when learning just from one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to 100%, and explores the environment more efficiently than prior work to infer ground truth domain action semantics.</abstract>
      <url hash="bec6ba63">2025.naacl-long.440</url>
      <bibkey>zhu-etal-2025-language-models</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>SLM</fixed-case>-Mod: Small Language Models Surpass <fixed-case>LLM</fixed-case>s at Content Moderation</title>
      <author><first>Xianyang</first><last>Zhan</last></author>
      <author><first>Agam</first><last>Goyal</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Yilun</first><last>Chen</last></author>
      <author><first>Eshwar</first><last>Chandrasekharan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Koustuv</first><last>Saha</last><affiliation>Department of Computer Science</affiliation></author>
      <pages>8774-8790</pages>
      <abstract>Large language models (LLMs) have shown promise in many natural language understanding tasks, including content moderation. However, these models can be expensive to query in real-time and do not allow for a community-specific approach to content moderation. To address these challenges, we explore the use of open-source small language models (SLMs) for community-specific content moderation tasks. We fine-tune and evaluate SLMs (less than 15B parameters) by comparing their performance against much larger open- and closed-sourced models in both a zero-shot and few-shot setting. Using 150K comments from 15 popular Reddit communities, we find that SLMs outperform zero-shot LLMs at content moderation<tex-math>-11.5</tex-math>% higher accuracy and 25.7% higher recall on average across all communities. Moreover, few-shot in-context learning leads to only a marginal increase in the performance of LLMs, still lacking compared to SLMs. We further show the promise of cross-community content moderation, which has implications for new communities and the development of cross-platform moderation techniques. Finally, we outline directions for future work on language model based content moderation.</abstract>
      <url hash="0b33cbdf">2025.naacl-long.441</url>
      <bibkey>zhan-etal-2025-slm</bibkey>
    </paper>
    <paper id="442">
      <title>On Positional Bias of Faithfulness for Long-form Summarization</title>
      <author><first>David</first><last>Wan</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Jesse</first><last>Vig</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>8791-8810</pages>
      <abstract>Large Language Models (LLMs) often exhibit positional bias in long-context settings, under-attending to information in the middle of inputs. We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias. To consistently evaluate faithfulness, we first compile a benchmark of eight human-annotated long-form summarization datasets and perform a meta-evaluation of faithfulness metrics. We show that LLM-based faithfulness metrics, though effective with full-context inputs, remain sensitive to document order, indicating positional bias. Analyzing LLM-generated summaries across six datasets, we find a “U-shaped” trend in faithfulness, where LLMs faithfully summarize the beginning and end of documents but neglect middle content. Perturbing document order similarly reveals models are less faithful when important documents are placed in the middle of the input. We find that this behavior is partly due to shifting focus with context length: as context increases, summaries become less faithful, but beyond a certain length, faithfulness improves as the model focuses on the end. Finally, we experiment with different generation techniques to reduce positional bias and find that prompting techniques effectively direct model attention to specific positions, whereas more sophisticated approaches offer limited improvements. Our data and code will be publicly available.</abstract>
      <url hash="2143347a">2025.naacl-long.442</url>
      <bibkey>wan-etal-2025-positional</bibkey>
    </paper>
    <paper id="443">
      <title><fixed-case>BPO</fixed-case>: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment</title>
      <author><first>Sizhe</first><last>Wang</last></author>
      <author><first>Yongqi</first><last>Tong</last></author>
      <author><first>Hengyuan</first><last>Zhang</last></author>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>8811-8826</pages>
      <abstract>Reinforcement Learning with Human Feedback (RLHF) is the key to the success of large language models (LLMs) in recent years. In this work, we first introduce the concepts of knowledge breadth and knowledge depth, which measure the comprehensiveness and depth of an LLM or knowledge source respectively. We reveal that the imbalance in the number of prompts and responses can lead to a potential disparity in breadth and depth learning within alignment tuning datasets by showing that even a simple uniform method for balancing the number of instructions and responses can lead to significant improvements. Building on this, we further propose Balanced Preference Optimization (BPO), designed to dynamically augment the knowledge depth of each sample. BPO is motivated by the observation that the usefulness of knowledge varies across samples, necessitating tailored learning of knowledge depth. To achieve this, we introduce gradient-based clustering, estimating the knowledge informativeness and usefulness of each augmented sample based on the model’s optimization direction. Our experimental results across various benchmarks demonstrate that BPO outperforms other baseline methods in alignment tuning while maintaining training efficiency. Furthermore, we conduct a detailed analysis of each component of BPO, providing guidelines for future research in preference data optimization.</abstract>
      <url hash="384c2afd">2025.naacl-long.443</url>
      <bibkey>wang-etal-2025-bpo</bibkey>
    </paper>
    <paper id="444">
      <title><fixed-case>UNDIAL</fixed-case>: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models</title>
      <author><first>Yijiang River</first><last>Dong</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Hongzhou</first><last>Lin</last><affiliation>Amazon</affiliation></author>
      <author><first>Mikhail</first><last>Belkin</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ramon</first><last>Huerta</last></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>Google DeepMind and University of Cambridge</affiliation></author>
      <pages>8827-8840</pages>
      <abstract>Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL is the first direct tuning method to achieve both robustness in unlearning and scalability, while maintaining stable training dynamics and resilience to hyperparameter tuning.</abstract>
      <url hash="6ec6faf3">2025.naacl-long.444</url>
      <bibkey>dong-etal-2025-undial</bibkey>
    </paper>
    <paper id="445">
      <title><fixed-case>H</fixed-case>-<fixed-case>STAR</fixed-case>: <fixed-case>LLM</fixed-case>-driven Hybrid <fixed-case>SQL</fixed-case>-Text Adaptive Reasoning on Tables</title>
      <author><first>Nikhil</first><last>Abhyankar</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Chandan K.</first><last>Reddy</last><affiliation>Virginia Tech and Amazon</affiliation></author>
      <pages>8841-8863</pages>
      <abstract>Tabular reasoning involves interpreting natural language queries about tabular data, which presents a unique challenge of combining language understanding with structured data analysis. Existing methods employ either textual reasoning, which excels in semantic interpretation but struggles with mathematical operations, or symbolic reasoning, which handles computations well but lacks semantic understanding. This paper introduces a novel algorithm H-STAR that integrates both symbolic and semantic (textual) approaches in a two-stage process to address these limitations. H-STAR employs: (1) step-wise table extraction using ‘multi-view’ column retrieval followed by row extraction, and (2) adaptive reasoning that adapts reasoning strategies based on question types, utilizing semantic reasoning for direct lookup and complex lexical queries while augmenting textual reasoning with symbolic reasoning support for quantitative and logical tasks. Our extensive experiments demonstrate that H-STAR significantly outperforms state-of-the-art methods across three tabular question-answering (QA) and fact-verification datasets, underscoring its effectiveness and efficiency.</abstract>
      <url hash="cae9d018">2025.naacl-long.445</url>
      <bibkey>abhyankar-etal-2025-h</bibkey>
    </paper>
    <paper id="446">
      <title>Kill two birds with one stone: generalized and robust <fixed-case>AI</fixed-case>-generated text detection via dynamic perturbations</title>
      <author><first>Yinghan</first><last>Zhou</last></author>
      <author><first>Juan</first><last>Wen</last><affiliation>China Agricultural University</affiliation></author>
      <author><first>Wanli</first><last>Peng</last><affiliation>China Agricultural University</affiliation></author>
      <author><first>Xue</first><last>Yiming</last><affiliation>China Agricultural University</affiliation></author>
      <author><first>ZiWei</first><last>Zhang</last></author>
      <author><first>Wu</first><last>Zhengxian</last><affiliation>China Agricultural University</affiliation></author>
      <pages>8864-8875</pages>
      <abstract>The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness.While, existing methods either focus on model generalization or concentrate on robustness.The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we first empirically reveal an intrinsic mechanism for model generalization and robustness of AIGT detection task.Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action.Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios.Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks.</abstract>
      <url hash="2b99fb37">2025.naacl-long.446</url>
      <bibkey>zhou-etal-2025-kill</bibkey>
    </paper>
    <paper id="447">
      <title>Vision-Language Models Can Self-Improve Reasoning via Reflection</title>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Li</first><last>YanTao</last></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Jianbing</first><last>Zhang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>8876-8892</pages>
      <abstract>Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, <tex-math>R^3V</tex-math>, which iteratively enhances the model’s Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that <tex-math>R^3V</tex-math> consistently improves multimodal LLM reasoning, achieving a relative improvement of 23% to 60% over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation. Our code is available at https://github.com/njucckevin/MM-Self-Improve.</abstract>
      <url hash="a15b7011">2025.naacl-long.447</url>
      <bibkey>cheng-etal-2025-vision</bibkey>
    </paper>
    <paper id="448">
      <title>Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training</title>
      <author><first>Deven Mahesh</first><last>Mistry</last></author>
      <author><first>Anooshka</first><last>Bajaj</last></author>
      <author><first>Yash</first><last>Aggarwal</last></author>
      <author><first>Sahaj Singh</first><last>Maini</last></author>
      <author><first>Zoran</first><last>Tiganj</last><affiliation>Indiana University, Bloomington</affiliation></author>
      <pages>8893-8911</pages>
      <abstract>We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their similarities and differences with human memory and learning.</abstract>
      <url hash="4d208a1f">2025.naacl-long.448</url>
      <bibkey>mistry-etal-2025-emergence</bibkey>
    </paper>
    <paper id="449">
      <title>Knowledge Graph-Guided Retrieval Augmented Generation</title>
      <author><first>Xiangrong</first><last>Zhu</last></author>
      <author><first>Yuexiang</first><last>Xie</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Yaliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <pages>8912-8924</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG<tex-math>^2</tex-math>RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG<tex-math>^2</tex-math>RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG<tex-math>^2</tex-math>RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality.</abstract>
      <url hash="d229c513">2025.naacl-long.449</url>
      <bibkey>zhu-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="450">
      <title>Amphista: Bi-directional Multi-head Decoding for Accelerating <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>Zeping</first><last>Li</last><affiliation>AMD</affiliation></author>
      <author><first>Xinlong</first><last>Yang</last></author>
      <author><first>Ziheng</first><last>Gao</last></author>
      <author><first>Ji</first><last>Liu</last><affiliation>AMD</affiliation></author>
      <author><first>Guanchen</first><last>Li</last></author>
      <author><first>Zhuang</first><last>Liu</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Dong</first><last>Li</last></author>
      <author><first>Jinzhang</first><last>Peng</last></author>
      <author><first>Lu</first><last>Tian</last></author>
      <author><first>Emad</first><last>Barsoum</last><affiliation>AMD</affiliation></author>
      <pages>8925-8938</pages>
      <abstract>Large Language Models (LLMs) inherently use autoregressive decoding, which lacks parallelism in inference and results in significantly slow inference speed. While methods such as Medusa constructs parallelized heads, they lack adequate information interaction across different prediction positions. To overcome this limitation, we introduce Amphista, an enhanced speculative decoding framework that builds upon Medusa. Specifically, Amphista models an *Auto-embedding Block* capable of parallel inference, incorporating bi-directional attention to enable interaction between different drafting heads. Additionally, Amphista integrates *Staged Adaptation Layers*, which ensure a seamless transition of semantic information from the target model’s autoregressive inference to the drafting heads’ non-autoregressive inference, effectively achieving paradigm shift and feature fusion. Experimental results on Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista achieves substantial acceleration while maintaining generation quality. On MT-Bench, Amphista delivers up to **2.75×** speedup over vanilla autoregressive decoding and **1.40×** over Medusa on Vicuna 33B in wall-clock time.</abstract>
      <url hash="93e2d527">2025.naacl-long.450</url>
      <bibkey>li-etal-2025-amphista</bibkey>
    </paper>
    <paper id="451">
      <title><fixed-case>CAVE</fixed-case>: Controllable Authorship Verification Explanations</title>
      <author><first>Sahana</first><last>Ramnath</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kartik</first><last>Pandey</last></author>
      <author><first>Elizabeth</first><last>Boschee</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California</affiliation></author>
      <pages>8939-8961</pages>
      <abstract>Authorship Verification (AV) (do two documents have the same author?) is essential in many real-life applications. AV is often used in privacy-sensitive domains that require an offline proprietary model that is deployed on premises, making publicly served online models (APIs) a suboptimal choice. Current offline AV models however have lower downstream utility due to limited accuracy (eg: traditional stylometry AV systems) and lack of accessible post-hoc explanations. In this work, we address the above challenges by developing a trained, offline model CAVE (Controllable Authorship Verification Explanations). CAVE generates free-text AV explanations that are controlled to be (1) accessible (uniform structure that can be decomposed into sub-explanations grounded to relevant linguistic features), and (2) easily verified for explanation-label consistency. We generate silver-standard training data grounded to the desirable linguistic features by a prompt-based method Prompt-CAVE. We then filter the data based on rationale-label consistency using a novel metric Cons-R-L. Finally, we fine-tune a small, offline model (Llama-3-8B) with this data to create our model CAVE. Results on three difficult AV datasets show that CAVE generates high quality explanations (as measured by automatic and human evaluation) as well as competitive task accuracy. We have submitted our code and datasets as supplementary material.</abstract>
      <url hash="c88f2167">2025.naacl-long.451</url>
      <bibkey>ramnath-etal-2025-cave</bibkey>
    </paper>
    <paper id="452">
      <title>Are <fixed-case>LLM</fixed-case>-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on <fixed-case>LLM</fixed-case>-based Evaluation</title>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yerin</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yongil</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>8962-8984</pages>
      <abstract>In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using **EMBER**, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.</abstract>
      <url hash="490d83fa">2025.naacl-long.452</url>
      <bibkey>lee-etal-2025-llm</bibkey>
    </paper>
    <paper id="453">
      <title>Dynamic Uncertainty Ranking: Enhancing Retrieval-Augmented In-Context Learning for Long-Tail Knowledge in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shuyang</first><last>Yu</last></author>
      <author><first>Runxue</first><last>Bao</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Taha</first><last>Kass-Hout</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Jiayu</first><last>Zhou</last><affiliation>University of Michigan - Ann Arbor and Michigan State University</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <pages>8985-8997</pages>
      <abstract>Large language models (LLMs) can learn vast amounts of knowledge from diverse domains during pre-training. However, long-tail knowledge from specialized domains is often scarce and underrepresented, rarely appearing in the models’ memorization. Prior work has shown that in-context learning (ICL) with retriever augmentation can help LLMs better capture long-tail knowledge, reducing their reliance on pre-trained data. Despite these advances, we observe that LLM predictions for long-tail questions remain uncertain to variations in retrieved samples. To take advantage of the uncertainty in ICL for guiding LLM predictions toward correct answers on long-tail samples, we propose a reinforcement learning-based dynamic uncertainty ranking method for retrieval-augmented ICL that accounts for the varying impact of each retrieved sample on LLM predictions. Our approach prioritizes more informative and stable samples while demoting misleading ones, updating rankings based on the feedback from the LLM w.r.t. each retrieved sample. To enhance training efficiency and reduce query costs, we introduce a learnable dynamic ranking threshold, adjusted when the model encounters negative prediction shifts. Experimental results on various question-answering datasets from different domains show that our method outperforms the best baseline by 2.76%, with a notable 5.96% boost in accuracy on long-tail questions that elude zero-shot inference. Our code is available at <url>https://github.com/Yu-shuyan/uncertian_ranker</url>.</abstract>
      <url hash="d09b9244">2025.naacl-long.453</url>
      <bibkey>yu-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="454">
      <title><fixed-case>S</fixed-case>eq1<fixed-case>F</fixed-case>1<fixed-case>B</fixed-case>: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training</title>
      <author><first>Sun</first><last>Ao</last></author>
      <author><first>Weilin</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xinrong</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chuan</first><last>Shi</last><affiliation>Beijing University of Post and Telecommunication, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>8998-9008</pages>
      <abstract>Training large language models (LLMs) heavily relies on distributed training strategies, among which pipeline parallelism (PP) plays a crucial role. As training sequences extend to 32k or even 128k tokens, current PP methods face severe bottlenecks, including substantial pipeline bubbles and high memory footprint, greatly hindering training throughput and model scalability. This paper introduces a sequence-level one-forward-one-backward (1F1B) PP method, named Seq1F1B, tailored for training LLMs on long sequences with high training throughput and memory efficiency. Unlike typical PP methods, which adopt batch-level pipeline schedule, Seq1F1B schedules the pipeline of training LLMs at the sequence level. It uses a computational strategy to partition sequences appropriately, significantly reducing pipeline bubbles and memory footprint. Compared to competitive PP baselines such as Megatron 1F1B PP, Seq1F1B achieves 1.14X training throughput with half memory footprint.Notably, Seq1F1B trains an LLM with 30B parameters on sequences up to 64k tokens using 64X NVIDIA A100 GPUs without using recomputation strategies, a feat unachievable with existing methods.We have released our code on GitHub to facilitate further research and development in LLM training on long sequences: https://github.com/thunlp/Seq1F1B.</abstract>
      <url hash="39a94508">2025.naacl-long.454</url>
      <bibkey>ao-etal-2025-seq1f1b</bibkey>
    </paper>
    <paper id="455">
      <title>Differentially Private Learning Needs Better Model Initialization and Self-Distillation</title>
      <author><first>Ivoline C.</first><last>Ngong</last><affiliation>University of Vermont</affiliation></author>
      <author><first>Joseph</first><last>Near</last><affiliation>University of Vermont</affiliation></author>
      <author><first>Niloofar</first><last>Mireshghallah</last></author>
      <pages>9009-9027</pages>
      <abstract>Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine’s generations in 78.38% of cases across all datasets and metrics, while also demonstrating substantial improvements in lexical diversity, achieving 85.31% in MSTTR and 86.82% in Jaccard similarity. Our fine-grained analysis reveals that DPRefine reduces linguistic errors in generated text by 84%, mitigating grammar errors, spelling mistakes, and missing punctuation commonly associated with DPSGD. It also reduces inconsistencies present in non-private models, such as fabricated details and misattributed quotes. We find that small models like GPT-2 and T5 are effective for initialization and distillation, highlighting their potential in enabling scalable and efficient deployment of high-performing, privacy-preserving language models with improved linguistic quality and consistency.</abstract>
      <url hash="bcc310c6">2025.naacl-long.455</url>
      <bibkey>ngong-etal-2025-differentially</bibkey>
    </paper>
    <paper id="456">
      <title>Is a Peeled Apple Still Red? Evaluating <fixed-case>LLM</fixed-case>s’ Ability for Conceptual Combination with Property Type</title>
      <author><first>Seokwon</first><last>Song</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taehyun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewoo</first><last>Ahn</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jae Hyuk</first><last>Sung</last></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>9028-9048</pages>
      <abstract>Conceptual combination is a cognitive process that merges basic concepts, enabling the creation of complex expressions. During this process, the properties of combination (e.g., the whiteness of a peeled apple) can be inherited from basic concepts, newly emerge, or be canceled. However, previous studies have evaluated a limited set of properties and have not examined the generative process.To address this gap, we introduce the Conceptual Combination with Property Type dataset (CCPT), which consists of 12.3K annotated triplets of noun phrases, properties, and property types. Using CCPT, we establish three types of tasks to evaluate LLMs for conceptual combination thoroughly.Our key findings are threefold:(1) Our automatic metric grading property emergence and cancellation closely corresponds with human judgments.(2) LLMs, including OpenAI’s o1, struggle to generate noun phrases which possess given emergent properties.(3) Our proposed method, inspired by cognitive psychology model that explains how relationships between concepts are formed, improves performances in all generative tasks.The dataset and experimental code are available at https://github.com/seokwon99/CCPT.git.</abstract>
      <url hash="40226d5e">2025.naacl-long.456</url>
      <bibkey>song-etal-2025-peeled</bibkey>
    </paper>
    <paper id="457">
      <title><fixed-case>CRS</fixed-case>core: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells</title>
      <author><first>Atharva</first><last>Naik</last></author>
      <author><first>Marcus</first><last>Alenius</last></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Meta AI and Carnegie Mellon University</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>9049-9076</pages>
      <abstract>The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff ). Furthermore, code review is a one-to-many problem, like generation and summarization, with many “valid reviews” for a diff. Thus, we develop CRScore — a reference-free metric to measure dimensions of review quality like conciseness, comprehensiveness, and relevance. We design CRScore to evaluate reviews in a way that is grounded in claims and potential issues detected in the code by LLMs and static analyzers. We demonstrate that CRScore can produce valid, fine-grained scores of review quality that have the greatest alignment with human judgment among open-source metrics (0.54 Spearman correlation) and are more sensitive than reference-based metrics. We also release a corpus of 2.9k human-annotated review quality scores for machine-generated and GitHub review comments to support the development of automated metrics.</abstract>
      <url hash="258ec8fb">2025.naacl-long.457</url>
      <bibkey>naik-etal-2025-crscore</bibkey>
    </paper>
    <paper id="458">
      <title><fixed-case>KS</fixed-case>-Lottery: Finding Certified Lottery Tickets for Multilingual Transfer in Large Language Models</title>
      <author><first>Fei</first><last>Yuan</last></author>
      <author><first>Chang</first><last>Ma</last></author>
      <author><first>Shuai</first><last>Yuan</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>9077-9090</pages>
      <abstract>The lottery ticket hypothesis posits the existence of “winning tickets” within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens’ embedding of LLaMA suffices to reach the fine-tuning translation performance .</abstract>
      <url hash="49956c8f">2025.naacl-long.458</url>
      <bibkey>yuan-etal-2025-ks</bibkey>
    </paper>
    <paper id="459">
      <title><fixed-case>PA</fixed-case>-<fixed-case>RAG</fixed-case>: <fixed-case>RAG</fixed-case> Alignment via Multi-Perspective Preference Optimization</title>
      <author><first>Jiayi</first><last>Wu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hengyi</first><last>Cai</last></author>
      <author><first>Lingyong</first><last>Yan</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>9091-9112</pages>
      <abstract>The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.</abstract>
      <url hash="a06fa393">2025.naacl-long.459</url>
      <bibkey>wu-etal-2025-pa</bibkey>
    </paper>
    <paper id="460">
      <title><tex-math>B^4</tex-math>: A Black-Box Scrubbing Attack on <fixed-case>LLM</fixed-case> Watermarks</title>
      <author><first>Baizhou</first><last>Huang</last></author>
      <author><first>Xiao</first><last>Pu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>9113-9126</pages>
      <abstract>Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some even necessitates knowledge about hyperparameters of the watermarking method. Such prerequisites are unattainable in real-world scenarios. Targeting at a more realistic black-box threat model with fewer assumptions, we here propose <tex-math>B^4</tex-math>, a black-box scrubbing attack on watermarks. Specifically, we formulate the watermark scrubbing attack as a constrained optimization problem by capturing its objectives with two distributions, a Watermark Distribution and a Fidelity Distribution. This optimization problem can be approximately solved using two proxy distributions. Experimental results across 12 different settings demonstrate the superior performance of <tex-math>B^4</tex-math> compared with other baselines.</abstract>
      <url hash="8d0f4b7b">2025.naacl-long.460</url>
      <bibkey>huang-etal-2025-b4</bibkey>
    </paper>
    <paper id="461">
      <title><fixed-case>IMRRF</fixed-case>: Integrating Multi-Source Retrieval and Redundancy Filtering for <fixed-case>LLM</fixed-case>-based Fake News Detection</title>
      <author><first>Dayang</first><last>Li</last></author>
      <author><first>Fanxiao</first><last>Li</last></author>
      <author><first>Bingbing</first><last>Song</last></author>
      <author><first>Li</first><last>Tang</last></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Yunnan University</affiliation></author>
      <pages>9127-9142</pages>
      <abstract>The widespread use of social networks has significantly accelerated the dissemination of information but has also facilitated the rapid spread of fake news, leading to various negative consequences. Recently, with the emergence of large language models (LLMs), researchers have focused on leveraging LLMs for automated fake news detection. Unfortunately, many issues remain to be addressed. First, the evidence retrieved to verify given fake news is often insufficient, limiting the performance of LLMs when reasoning directly from this evidence. Additionally, the retrieved evidence frequently contains substantial redundant information, which can interfere with the LLMs’ judgment. To address these limitations, we propose a Multiple Knowledge Sources Retrieval and LLM Knowledge Conversion framework, which enriches the evidence available for claim verification. We also introduce a Redundant Information Filtering Strategy, which minimizes the influence of irrelevant information on the LLM reasoning process. Extensive experiments conducted on two challenging fact-checking datasets demonstrate that our proposed method outperforms state-of-the-art fact-checking baselines. Our code is available at https://github.com/quark233/IMRRF/tree/main.</abstract>
      <url hash="4909f78f">2025.naacl-long.461</url>
      <bibkey>li-etal-2025-imrrf</bibkey>
    </paper>
    <paper id="462">
      <title>Matina: A Large-Scale 73<fixed-case>B</fixed-case> Token <fixed-case>P</fixed-case>ersian Text Corpus</title>
      <author><first>Sara Bourbour</first><last>Hosseinbeigi</last><affiliation>Tarbiat Modares University</affiliation></author>
      <author><first>Fatemeh</first><last>Taherinezhad</last></author>
      <author><first>Heshaam</first><last>Faili</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Hamed</first><last>Baghbani</last></author>
      <author><first>Fatemeh</first><last>Nadi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mostafa</first><last>Amiri</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <pages>9143-9157</pages>
      <abstract>Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality. We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks. Both the dataset and preprocessing codes are publicly available, enabling researchers to build on and improve this resource for future Persian NLP advancements.</abstract>
      <url hash="0cb8996e">2025.naacl-long.462</url>
      <bibkey>hosseinbeigi-etal-2025-matina</bibkey>
    </paper>
    <paper id="463">
      <title><fixed-case>SMAB</fixed-case>: <fixed-case>MAB</fixed-case> based word Sensitivity Estimation Framework and its Applications in Adversarial Text Generation</title>
      <author><first>Saurabh Kumar</first><last>Pandey</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sachin</first><last>Vashistha</last></author>
      <author><first>Debrup</first><last>Das</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Somak</first><last>Aditya</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>9158-9176</pages>
      <abstract>To understand the complexity of sequence classification tasks, Hahn et al. (2021) proposed sensitivity as the number of disjoint subsets of the input sequence that can each be individually changed to change the output. Though effective, calculating sensitivity at scale using this framework is costly because of exponential time complexity. Therefore, we introduce a Sensitivity-based Multi-Armed Bandit framework (SMAB), which provides a scalable approach for calculating word-level local (sentence-level) and global (aggregated) sensitivities concerning an underlying text classifier for any dataset. We establish the effectiveness of our approach through various applications. We perform a case study on CHECKLIST generated sentiment analysis dataset where we show that our algorithm indeed captures intuitively high and low-sensitive words. Through experiments on multiple tasks and languages, we show that sensitivity can serve as a proxy for accuracy in the absence of gold data. Lastly, we show that guiding perturbation prompts using sensitivity values in adversarial example generation improves attack success rate by 15.58%, whereas using sensitivity as an additional reward in adversarial paraphrase generation gives a 12.00% improvement over SOTA approaches. Warning: Contains potentially offensive content.</abstract>
      <url hash="2561cecf">2025.naacl-long.463</url>
      <bibkey>pandey-etal-2025-smab</bibkey>
    </paper>
    <paper id="464">
      <title><fixed-case>M</fixed-case>ana<fixed-case>TTS</fixed-case> <fixed-case>P</fixed-case>ersian: a recipe for creating <fixed-case>TTS</fixed-case> datasets for lower resource languages</title>
      <author><first>Mahta Fetrat</first><last>Qharabagh</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Zahra</first><last>Dehghanian</last></author>
      <author><first>Hamid R.</first><last>Rabiee</last></author>
      <pages>9177-9206</pages>
      <abstract>In this study, we introduce ManaTTS, the most extensive publicly accessible single-speaker Persian corpus, and a comprehensive framework for collecting transcribed speech datasets for the Persian language. ManaTTS, released under the open CC-0 license, comprises approximately 86 hours of audio with a sampling rate of 44.1 kHz. The dataset is supported by a fully transparent, MIT-licensed pipeline, a testament to innovation in the field. It includes unique tools for sentence tokenization, bounded audio segmentation, and a novel forced alignment method. This alignment technique is specifically designed for low-resource languages, addressing a crucial need in the field. With this dataset, we trained a Tacotron2-based TTS model, achieving a Mean Opinion Score (MOS) of 3.76, which is remarkably close to the MOS of 3.86 for the utterances generated by the same vocoder and natural spectrogram, and the MOS of 4.01 for the natural waveform, demonstrating the exceptional quality and effectiveness of the corpus.</abstract>
      <url hash="9e7364b5">2025.naacl-long.464</url>
      <bibkey>qharabagh-etal-2025-manatts</bibkey>
    </paper>
    <paper id="465">
      <title><fixed-case>C</fixed-case>ulture<fixed-case>I</fixed-case>nstruct: Curating Multi-Cultural Instructions at Scale</title>
      <author><first>Viet Thanh</first><last>Pham</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>9207-9228</pages>
      <abstract>Large language models, despite their remarkable success in recent years, still exhibit severe cultural bias. Therefore, in this paper, we introduce CultureInstruct, a large-scale instruction-tuning dataset designed to reduce cultural bias in LLMs. CultureInstruct is constructed with an automatic pipeline, utilizing public web sources and a specialized LLM to generate instruction. Our data comprises 430K instructions, ranging from classic NLP tasks to complex reasoning. CultureInstruct also covers 11 most relevant topics to cultural knowledge, making it highly diverse. Our experiments show that fine-tuning LLMs with CultureInstruct results in consistent improvements across three types of cultural benchmarks, including (i) general cultural knowledge, (ii) human opinions and values, and (iii) linguistic cultural bias. Our best model, Qwen2-Instruct 72B + CultureInstruct, outperforms GPT-4o Mini and GPT-4o with 18.47% and 13.07% average relative improvements on cultural benchmarks.</abstract>
      <url hash="7ab968c8">2025.naacl-long.465</url>
      <bibkey>pham-etal-2025-cultureinstruct</bibkey>
    </paper>
    <paper id="466">
      <title>Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models</title>
      <author><first>Lovish</first><last>Madaan</last><affiliation>Meta and University College London, University of London</affiliation></author>
      <author><first>David</first><last>Esiobu</last><affiliation>Facebook</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Dieuwke</first><last>Hupkes</last><affiliation>Facebook</affiliation></author>
      <pages>9229-9242</pages>
      <abstract>In the recent past, a popular way of evaluating natural language understanding (NLU), was to consider a model’s ability to perform natural language inference (NLI) tasks. In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs. Focusing on five different NLI benchmarks across six models of different scales, we investigate if they are able to discriminate models of different size and quality and how their accuracies develop during training. Furthermore, we investigate the extent to which the softmax distributions of models align with human distributions in cases where statements are ambiguous or vague. Overall, our results paint a positive picture for the NLI tasks: we find that they are able to discriminate well between models at various stages of training, yet are not (all) saturated. Furthermore, we find that while the similarity of model distributions with human label distributions increases with scale, it is still much higher than the similarity between two populations of humans, making it a potentially interesting statistic to consider.</abstract>
      <url hash="4c01e91d">2025.naacl-long.466</url>
      <bibkey>madaan-etal-2025-lost</bibkey>
    </paper>
    <paper id="467">
      <title><fixed-case>D</fixed-case>ense<fixed-case>SSM</fixed-case>: State Space Models with Dense Hidden Connection for Efficient Large Language Models</title>
      <author><first>Wei</first><last>He</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kai</first><last>Han</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yehui</first><last>Tang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Chengcheng</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yujie</first><last>Yang</last></author>
      <author><first>Tianyu</first><last>Guo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yunhe</first><last>Wang</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>9243-9254</pages>
      <abstract>Large language models (LLMs) face a significant challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. This incremental improvement maintains the training parallelizability and inference efficiency of SSMs while significantly boosting performance. The proposed method is broadly applicable to various SSM types, including RetNet and Mamba, and DenseSSM achieves significant performance improvements on public benchmarks, demonstrating its effectiveness and versatility.</abstract>
      <url hash="983ad5db">2025.naacl-long.467</url>
      <bibkey>he-etal-2025-densessm</bibkey>
    </paper>
    <paper id="468">
      <title>A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model</title>
      <author><first>Shengxiang</first><last>Gao</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <author><first>Fang</first><last>Nan</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Yongbing</first><last>Zhang</last></author>
      <author><first>Yuxin</first><last>Huang</last></author>
      <author><first>Kaiwen</first><last>Tan</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Zhengtao</first><last>Yu</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <pages>9255-9265</pages>
      <abstract>Existing research on news summarization primarily focuses on single-language single-document (SLSD), single-language multi-document (SLMD) or cross-language single-document (CLSD). However, in real-world scenarios, news about an international event often involves multiple documents in different languages, i.e., mixed-language multi-document (MLMD). Therefore, summarizing MLMD news is of great significance. However, the lack of datasets for MLMD news summarization has constrained the development of research in this area. To fill this gap, we construct a mixed-language multi-document news summarization dataset (MLMD-news), which contains four different languages and 10,992 source document cluster and target summary pairs. Additionally, we propose a graph-based extract-generate model and benchmark various methods on the MLMD-news dataset and publicly release our dataset and code, aiming to advance research in summarization within MLMD scenarios.</abstract>
      <url hash="527b739a">2025.naacl-long.468</url>
      <bibkey>gao-etal-2025-mixed</bibkey>
    </paper>
    <paper id="469">
      <title>Measuring memorization in language models via probabilistic extraction</title>
      <author><first>Jamie</first><last>Hayes</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Marika</first><last>Swanberg</last><affiliation>Google</affiliation></author>
      <author><first>Harsh</first><last>Chaudhari</last></author>
      <author><first>Itay</first><last>Yona</last><affiliation>Google</affiliation></author>
      <author><first>Ilia</first><last>Shumailov</last></author>
      <author><first>Milad</first><last>Nasr</last><affiliation>Google</affiliation></author>
      <author><first>Christopher A.</first><last>Choquette-Choo</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Katherine</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>A. Feder</first><last>Cooper</last><affiliation>Research, Microsoft and Computer Science Department, Stanford University</affiliation></author>
      <pages>9266-9291</pages>
      <abstract>Large language models (LLMs) are susceptible to memorizing training data, raising concerns about the potential extraction of sensitive information at generation time. Discoverable extraction is the most common method for measuring this issue: split a training example into a prefix and suffix, then prompt the LLM with the prefix, and deem the example extractable if the LLM generates the matching suffix using greedy sampling. This definition yields a yes-or-no determination of whether extraction was successful with respect to a single query. Though efficient to compute, we show that this definition is unreliable because it does not account for non-determinism present in more realistic (non-greedy) sampling schemes, for which LLMs produce a range of outputs for the same prompt. We introduce probabilistic discoverable extraction, which, without additional cost, relaxes discoverable extraction by considering multiple queries to quantify the probability of extracting a target sequence. We evaluate our probabilistic measure across different models, sampling schemes, and training-data repetitions, and find that this measure provides more nuanced information about extraction risk compared to traditional discoverable extraction.</abstract>
      <url hash="ceab559b">2025.naacl-long.469</url>
      <bibkey>hayes-etal-2025-measuring</bibkey>
    </paper>
    <paper id="470">
      <title>Audio Is the Achilles’ Heel: Red Teaming Audio Large Multimodal Models</title>
      <author><first>Hao</first><last>Yang</last><affiliation>Monash University</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <pages>9292-9306</pages>
      <abstract>Large Multimodal Models (LMMs) have demonstrated the ability to interact with humans under real-world conditions by combining Large Language Models (LLMs) and modality encoders to align multimodal information (visual and auditory) with text. However, such models raise new safety challenges of whether models that are safety-aligned on text also exhibit consistent safeguards for multimodal inputs. Despite recent safety-alignment research on vision LMMs, the safety of audio LMMs remains under-explored. In this work, we comprehensively red team the safety of five advanced audio LMMs under three settings: (i) harmful questions in both audio and text formats, (ii) harmful questions in text format accompanied by distracting non-speech audio, and (iii) speech-specific jailbreaks. Our results under these settings demonstrate that open-source audio LMMs suffer an average attack success rate of 69.14% on harmful audio questions, and exhibit safety vulnerabilities when distracted with non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro achieve an attack success rate of 70.67% on the harmful query benchmark. We provide insights on what could cause these reported safety-misalignments. Warning: this paper contains offensive examples.</abstract>
      <url hash="02455026">2025.naacl-long.470</url>
      <bibkey>yang-etal-2025-audio</bibkey>
    </paper>
    <paper id="471">
      <title><fixed-case>EMS</fixed-case>-<fixed-case>SD</fixed-case>: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models</title>
      <author><first>Yunsheng</first><last>Ni</last></author>
      <author><first>Chuanjian</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yehui</first><last>Tang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Kai</first><last>Han</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yunhe</first><last>Wang</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>9307-9320</pages>
      <abstract>Speculative decoding emerges as a pivotal technique for enhancing the inference speed of Large Language Models (LLMs). Despite recent research aiming to improve prediction efficiency, multi-sample speculative decoding has been overlooked due to varying numbers of accepted tokens within a batch in the verification phase. Vanilla method adds padding tokens in order to ensure that the number of new tokens remains consistent across samples. However, this increases the computational and memory access overhead, thereby reducing the speedup ratio. We propose a novel method that can resolve the issue of inconsistent tokens accepted by different samples without necessitating an increase in memory or computing overhead. Furthermore, our proposed method can handle the situation where the prediction tokens of different samples are inconsistent without the need to add padding tokens. Sufficient experiments demonstrate the efficacy of our method. Our code will be released later.</abstract>
      <url hash="c8651e87">2025.naacl-long.471</url>
      <bibkey>ni-etal-2025-ems</bibkey>
    </paper>
    <paper id="472">
      <title>Regularized Best-of-N Sampling with Minimum <fixed-case>B</fixed-case>ayes Risk Objective for Language Model Alignment</title>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Tetsuro</first><last>Morimura</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Kaito</first><last>Ariu</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Kenshi</first><last>Abe</last><affiliation>University of Electro-Communications and CyberAgent, Inc.</affiliation></author>
      <pages>9321-9347</pages>
      <abstract>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking when the accuracy of the reward model is not high enough. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization, which ensures that the language model remains close to the reference model. In this research, we propose MBR-BoN, a variant of BoN that aims to mitigate reward hacking at inference time by incorporating the Minimum Bayes Risk (MBR) objective as a proximity regularization term. We show empirically and analytically that the MBR objective quantifies the proximity of the response to the reference policy, serving as a proximity regularizer for BoN sampling. We evaluate MBR-BoN on the AlpacaFarm and Anthropic’s hh-rlhf datasets and show that it outperforms both BoN sampling and MBR decoding. As an application of MBR-BoN, we use it to generate a pairwise preference learning dataset. Experimental results show that DPO models trained on a dataset generated with MBR-BoN outperform a DPO model generated with vanilla BoN.</abstract>
      <url hash="4031edac">2025.naacl-long.472</url>
      <bibkey>jinnai-etal-2025-regularized</bibkey>
    </paper>
    <paper id="473">
      <title><fixed-case>MAPW</fixed-case>ise: Evaluating Vision-Language Models for Advanced Map Queries</title>
      <author><first>Srija</first><last>Mukhopadhyay</last></author>
      <author><first>Abhishek</first><last>Rajgaria</last></author>
      <author><first>Prerana</first><last>Khatiwada</last></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>9348-9378</pages>
      <abstract>Vision-language models (VLMs) excel at tasks requiring joint understanding of visual and linguistic information. A particularly promising yet under-explored application for these models lies in answering questions based on various kinds of maps. This study investigates the efficacy of VLMs in answering questions based on choropleth maps, which are widely used for data analysis and representation. To facilitate and encourage research in this area, we introduce a novel map-based question-answering benchmark, consisting of maps from three geographical regions (United States, India, China), each containing around 1000 questions. Our benchmark incorporates 43 diverse question templates, requiring nuanced understanding of relative spatial relationships, intricate map features, and complex reasoning. It also includes maps with discrete and continuous values, covering variations in color mapping, category ordering, and stylistic patterns, enabling a comprehensive analysis. We evaluated the performance of multiple VLMs on this benchmark, highlighting gaps in their abilities, and providing insights for improving such models. Our dataset, along with all necessary code scripts, is available at map-wise.github.io.</abstract>
      <url hash="f5d3118a">2025.naacl-long.473</url>
      <bibkey>mukhopadhyay-etal-2025-mapwise</bibkey>
    </paper>
    <paper id="474">
      <title>Pay More Attention to Images: Numerous Images-Oriented Multimodal Summarization</title>
      <author><first>Min</first><last>Xiao</last></author>
      <author><first>Junnan</first><last>Zhu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Feifei</first><last>Zhai</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>9379-9392</pages>
      <abstract>Existing multimodal summarization approaches struggle with scenarios involving numerous images as input, leading to a heavy load for readers. Summarizing both the input text and numerous images helps readers quickly grasp the key points of multimodal input. This paper introduces a novel task, Numerous Images-Oriented Multimodal Summarization (NIMMS). To benchmark this task, we first construct the dataset based on a public multimodal summarization dataset. Considering that most existing metrics evaluate summaries from a unimodal perspective, we propose a new Multimodal Information evaluation (M-info) method, measuring the differences between the generated summary and the multimodal input. Finally, we compare various summarization methods on NIMMS and analyze associated challenges. Experimental results have shown that M-info correlates more closely with human judgments than five widely used metrics. Meanwhile, existing models struggle with summarizing numerous images. We hope that this research will shed light on the development of multimodal summarization. Furthermore, our code and dataset will be released to the public.</abstract>
      <url hash="4947906d">2025.naacl-long.474</url>
      <bibkey>xiao-etal-2025-pay</bibkey>
    </paper>
    <paper id="475">
      <title>S<tex-math>^2</tex-math>-<fixed-case>MAD</fixed-case>: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency</title>
      <author><first>Yuting</first><last>Zeng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weizhe</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Lei</first><last>Jiang</last></author>
      <author><first>Tongxuan</first><last>Liu</last><affiliation>JD.com</affiliation></author>
      <author><first>XiTai</first><last>Jin</last></author>
      <author><first>Chen Tianying</first><last>Tiana</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Xiaohua</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9393-9408</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5% in token costs while maintaining performance degradation below 2.0%.</abstract>
      <url hash="1bd5bbcb">2025.naacl-long.475</url>
      <bibkey>zeng-etal-2025-s2</bibkey>
    </paper>
    <paper id="476">
      <title><fixed-case>MASTER</fixed-case>: A Multi-Agent System with <fixed-case>LLM</fixed-case> Specialized <fixed-case>MCTS</fixed-case></title>
      <author><first>Bingzheng</first><last>Gan</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yufan</first><last>Zhao</last><affiliation>Huawei International Pte. Ltd.</affiliation></author>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Jing</first><last>Huang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Li</first><last>Yusu</last></author>
      <author><first>Shu Xian</first><last>Teo</last></author>
      <author><first>Changwang</first><last>Zhang</last><affiliation>CCF Theoretical Computer Science Technical Committee and OPPO Research Institute</affiliation></author>
      <author><first>Wei</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>9409-9426</pages>
      <abstract>Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, which leads to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where simulation results can yield objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such as question answering, the result of a simulation is the answer to the question, which cannot yield an objective reward without the ground truth. Secondly, obtaining statistically significant reward estimations typically requires a sample size exceeding 30 simulations, resulting in excessive token usage and time consumption. To address these challenges, we present Multi-Agent System with Tactical Execution and Reasoning using LLM Specialized MCTS (MASTER), a novel framework that coordinates agent recruitment and communication through LLM specialized MCTS. This system autonomously adjusts the number of agents based on task complexity and ensures focused communication among them. Comprehensive experiments across various tasks demonstrate the effectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA and 80% on WebShop, setting new state of-the-art performance on these datasets.</abstract>
      <url hash="436f6e1d">2025.naacl-long.476</url>
      <bibkey>gan-etal-2025-master</bibkey>
    </paper>
    <paper id="477">
      <title><fixed-case>S</fixed-case>creen<fixed-case>QA</fixed-case>: Large-Scale Question-Answer Pairs Over Mobile App Screenshots</title>
      <author><first>Yu-Chung</first><last>Hsiao</last><affiliation>Cisco</affiliation></author>
      <author><first>Fedir</first><last>Zubach</last><affiliation>Google</affiliation></author>
      <author><first>Gilles</first><last>Baechler</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Srinivas</first><last>Sunkara</last></author>
      <author><first>Victor</first><last>Carbune</last><affiliation>Google</affiliation></author>
      <author><first>Jason</first><last>Lin</last><affiliation>Stanford University</affiliation></author>
      <author><first>Maria</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <pages>9427-9452</pages>
      <abstract>We introduce ScreenQA, a novel benchmarking dataset designed to advance screen content understanding through question answering. The existing screen datasets are focused either on low-level structural and component understanding, or on a much higher-level composite task such as navigation and task completion for autonomous agents. ScreenQA attempts to bridge this gap. By annotating 86k question-answer pairs over the RICO dataset, we aim to benchmark the screen reading comprehension capacity, thereby laying the foundation for vision-based automation over screenshots. Our annotations encompass full answers, short answer phrases, and corresponding UI contents with bounding boxes, enabling four subtasks to address various application scenarios. We evaluate the dataset’s efficacy using both open-weight and proprietary models in zero-shot, fine-tuned, and transfer learning settings. We further demonstrate positive transfer to web applications, highlighting its potential beyond mobile applications.</abstract>
      <url hash="9bd386fd">2025.naacl-long.477</url>
      <bibkey>hsiao-etal-2025-screenqa</bibkey>
    </paper>
    <paper id="478">
      <title>Cross-Lingual and Cross-Cultural Variation in Image Descriptions</title>
      <author><first>Uri</first><last>Berger</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>9453-9465</pages>
      <abstract>Do speakers of different languages talk differently about what they see? Behavioural and cognitive studies report cultural effects on perception; however, these are mostly limited in scope and hard to replicate. In this work, we conduct the first large-scale empirical study of cross-lingual variation in image descriptions. Using a multimodal dataset with 31 languages and images from diverse locations, we develop a method to accurately identify entities mentioned in captions and present in the images, then measure how they vary across languages. Our analysis reveals that pairs of languages that are geographically or genetically closer tend to mention the same entities more frequently. We also identify entity categories whose saliency is universally high (such as animate beings), low (clothing accessories) or displaying high variance across languages (landscape). In a case study, we measure the differences in a specific language pair (e.g., Japanese mentions clothing far more frequently than English). Furthermore, our method corroborates previous small-scale studies, including 1) Rosch et al. (1976)’s theory of basic-level categories, demonstrating a preference for entities that are neither too generic nor too specific, and 2) Miyamoto et al. (2006)’s hypothesis that environments afford patterns of perception, such as entity counts. Overall, our work reveals the presence of both universal and culture-specific patterns in entity mentions.</abstract>
      <url hash="d238d446">2025.naacl-long.478</url>
      <bibkey>berger-ponti-2025-cross</bibkey>
    </paper>
    <paper id="479">
      <title>Soft Syntactic Reinforcement for Neural Event Extraction</title>
      <author><first>Anran</first><last>Hao</last></author>
      <author><first>Jian</first><last>Su</last><affiliation>A*STAR</affiliation></author>
      <author><first>Shuo</first><last>Sun</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Teo Yong</first><last>Sen</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>9466-9478</pages>
      <abstract>Recent event extraction (EE) methods rely on pre-trained language models (PLMs) but still suffer from errors due to a lack of syntactic knowledge. While syntactic information is crucial for EE, there is a need for effective methods to incorporate syntactic knowledge into PLMs. To address this gap, we present a novel method to incorporate syntactic information into PLM-based models for EE, which do not require external syntactic parsers to produce syntactic features of task data. Instead, our proposed soft syntactic reinforcement (SSR) mechanism learns to select syntax-related dimensions of PLM representation during pretraining on a standard dependency corpus. The adapted PLM weights and the syntax-aware representation then facilitate the model’s prediction over the task data. On both sentence-level and document-level EE benchmark datasets, our proposed method achieves state-of-the-art results, outperforming baseline models and existing syntactic reinforcement methods. To the best of our knowledge, this is the first work in this direction. Our code is available at <url>https://github.com/Anran971/sre-naacl25</url>.</abstract>
      <url hash="504e5db6">2025.naacl-long.479</url>
      <bibkey>hao-etal-2025-soft</bibkey>
    </paper>
    <paper id="480">
      <title>Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models</title>
      <author><first>Hyegang</first><last>Son</last><affiliation>Korea University</affiliation></author>
      <author><first>Yonglak</first><last>Son</last></author>
      <author><first>Changhoon</first><last>Kim</last><affiliation>Soongsil University</affiliation></author>
      <author><first>Young Geun</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <pages>9479-9496</pages>
      <abstract>Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes to both task performance and resource usage. Motivated by this insight, we propose Selective Adapter FrEezing (SAFE), which gradually freezes less important adapters early to reduce unnecessary resource usage while maintaining performance. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85%, 34.59%, and 11.82%, respectively, while achieving comparable or better task performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape, which enables the model to generalize better by avoiding sharp minima.</abstract>
      <url hash="39d92b25">2025.naacl-long.480</url>
      <bibkey>son-etal-2025-adapters</bibkey>
    </paper>
    <paper id="481">
      <title>Bridging the Gap between Expert and Language Models: Concept-guided Chess Commentary Generation and Evaluation</title>
      <author><first>Jaechang</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jinmin</first><last>Goh</last></author>
      <author><first>Inseok</first><last>Hwang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Jaewoong</first><last>Cho</last><affiliation>KRAFTON</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <pages>9497-9516</pages>
      <abstract>Deep learning-based expert models have reached superhuman performance in decision-making domains such as chess and Go. However, it is under-explored to explain or comment on given decisions although it is important for model explainability and human education. The outputs of expert models are accurate, but yet difficult to interpret for humans. On the other hand, large language models (LLMs) can produce fluent commentary but are prone to hallucinations due to their limited decision-making capabilities. To bridge this gap between expert models and LLMs, we focus on chess commentary as a representative task of explaining complex decision-making processes through language and address both the generation and evaluation of commentary. We introduce Concept-guided Chess Commentary generation (CCC) for producing commentary and GPT-based Chess Commentary Evaluation (GCC-Eval) for assessing it. CCC integrates the decision-making strengths of expert models with the linguistic fluency of LLMs through prioritized, concept-based explanations. GCC-Eval leverages expert knowledge to evaluate chess commentary based on informativeness and linguistic quality. Experimental results, validated by both human judges and GCC-Eval, demonstrate that CCC generates commentary which is accurate, informative, and fluent.</abstract>
      <url hash="cde19467">2025.naacl-long.481</url>
      <bibkey>kim-etal-2025-bridging</bibkey>
    </paper>
    <paper id="482">
      <title><fixed-case>TCP</fixed-case>ro<fixed-case>F</fixed-case>:Time-Complexity Prediction <fixed-case>SSL</fixed-case> Framework</title>
      <author><first>Joonghyuk</first><last>Hahn</last></author>
      <author><first>Hyeseon</first><last>Ahn</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jungin</first><last>Kim</last></author>
      <author><first>Soohan</first><last>Lim</last></author>
      <author><first>Yo-Sub</first><last>Han</last><affiliation>Yonsei University</affiliation></author>
      <pages>9517-9542</pages>
      <url hash="fc8e79c4">2025.naacl-long.482</url>
      <bibkey>hahn-etal-2025-tcprof</bibkey>
    </paper>
    <paper id="483">
      <title>Culture-<fixed-case>TRIP</fixed-case>: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement</title>
      <author><first>Suchae</first><last>Jeong</last></author>
      <author><first>Inseong</first><last>Choi</last></author>
      <author><first>Youngsik</first><last>Yun</last><affiliation>Dongguk University</affiliation></author>
      <author><first>Jihie</first><last>Kim</last><affiliation>Dongguk University</affiliation></author>
      <pages>9543-9573</pages>
      <url hash="68ea7a2f">2025.naacl-long.483</url>
      <bibkey>jeong-etal-2025-culture</bibkey>
    </paper>
    <paper id="484">
      <title>Behavior-<fixed-case>SD</fixed-case>: Behaviorally Aware Spoken Dialogue Generation with Large Language Models</title>
      <author><first>Sehun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kang-wook</first><last>Kim</last></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>9574-9593</pages>
      <abstract>Spoken dialogue involves behaviors like turn-taking, interruptions, filler words, and backchannels, which make interactions more natural and engaging but are often overlooked in language models. These models struggle to explicitly model these behavioral traits, resulting in a less natural and personalized communication style that aligns with user needs. To address this challenge, we make two key contributions. First, we introduce Behavior-SD, a large-scale dataset containing over 100K spoken dialogues (2,164 hours) annotated with various conversational behaviors, synthesized via LLMs to model diverse full-duplex interactions. Second, we propose BeDLM, the first dialogue model capable of generating natural conversations conditioned on specific behavioral and narrative contexts, supporting simultaneous contributions from both speakers. Through human evaluations and behavior-adherence metrics, we demonstrate that BeDLM outperforms baseline models in generating natural, coherent, and behaviorally rich dialogues. Our work opens new possibilities for developing behaviorally-aware dialogue systems that more closely mimic human conversational dynamics, enhancing user engagement and communication effectiveness.</abstract>
      <url hash="eca6d047">2025.naacl-long.484</url>
      <bibkey>lee-etal-2025-behavior</bibkey>
    </paper>
    <paper id="485">
      <title>Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models</title>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Yiran</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>9594-9614</pages>
      <abstract>Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key contribution lies in demonstrating that while translation into English can boost the performance of English-centric LLMs on NLP tasks, it is not universally optimal. For culture-related tasks that need deep language understanding, prompting in the native language proves more effective as it better captures the nuances of culture and language. Our experiments expose varied behaviors across LLMs and tasks in the multilingual context, underscoring the need for a more comprehensive approach to multilingual evaluation. Therefore, we call for greater efforts in developing and evaluating LLMs that go beyond English-centric paradigms.</abstract>
      <url hash="5a3a36b6">2025.naacl-long.485</url>
      <bibkey>liu-etal-2025-translation</bibkey>
    </paper>
    <paper id="486">
      <title><fixed-case>A</fixed-case>lgo<fixed-case>P</fixed-case>uzzle<fixed-case>VQA</fixed-case>: Diagnosing Multimodal Reasoning Challenges of Language Models with Algorithmic Multimodal Puzzles</title>
      <author><first>Deepanway</first><last>Ghosal</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Vernon</first><last>Toh</last></author>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>9615-9632</pages>
      <abstract>This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that multimodal language models such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.</abstract>
      <url hash="520f819d">2025.naacl-long.486</url>
      <bibkey>ghosal-etal-2025-algopuzzlevqa</bibkey>
    </paper>
    <paper id="487">
      <title>Towards Quantifying Commonsense Reasoning with Mechanistic Insights</title>
      <author><first>Abhinav</first><last>Joshi</last><affiliation>Indian Institute of Technology, Kanpur</affiliation></author>
      <author><first>Areeb</first><last>Ahmad</last></author>
      <author><first>Divyaksh</first><last>Shukla</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>9633-9660</pages>
      <url hash="b72afe1d">2025.naacl-long.487</url>
      <bibkey>joshi-etal-2025-towards</bibkey>
    </paper>
    <paper id="488">
      <title>Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection &amp; Grounding in <fixed-case>VLM</fixed-case>s</title>
      <author><first>Anirudh</first><last>Phukan</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Divyansh</first><last>Divyansh</last></author>
      <author><first>Harshit Kumar</first><last>Morj</last></author>
      <author><first>Vaishnavi</first><last>Vaishnavi</last></author>
      <author><first>Apoorv</first><last>Saxena</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Koustava</first><last>Goswami</last><affiliation>Adobe Systems</affiliation></author>
      <pages>9661-9675</pages>
      <abstract>The rapid development of Large Multimodal Models (LMMs) has significantly advanced multimodal understanding by harnessing the language abilities of Large Language Models (LLMs) and integrating modality-specific encoders. However, LMMs are plagued by hallucinations that limit their reliability and adoption. While traditional methods to detect and mitigate these hallucinations often involve costly training or rely heavily on external models, recent approaches utilizing internal model features present a promising alternative. In this paper, we critically assess the limitations of the state-of-the-art training-free technique, the logit lens, in handling generalized visual hallucinations. We introduce *ContextualLens*, a refined method that leverages contextual token embeddings from middle layers of LMMs. This approach significantly improves hallucination detection and grounding across diverse categories, including actions and OCR, while also excelling in tasks requiring contextual understanding, such as spatial relations and attribute comparison. Our novel grounding technique yields highly precise bounding boxes, facilitating a transition from Zero-Shot Object Segmentation to Grounded Visual Question Answering. Our contributions pave the way for more reliable and interpretable multimodal models.</abstract>
      <url hash="bf9c227e">2025.naacl-long.488</url>
      <bibkey>phukan-etal-2025-beyond</bibkey>
    </paper>
    <paper id="489">
      <title><fixed-case>M</fixed-case>2<fixed-case>L</fixed-case>ingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models</title>
      <author><first>Rishabh</first><last>Maheshwary</last><affiliation>ServiceNow</affiliation></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Hoang H</first><last>Nguyen</last></author>
      <author><first>Khyati</first><last>Mahajan</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Sathwik Tejaswi</first><last>Madhusudhan</last><affiliation>ServiceNow Inc</affiliation></author>
      <pages>9676-9713</pages>
      <abstract>Collecting instruction fine-tuning (IFT) data is a resource and time intensive task especially in multilingual setting where finding proficient native speakers is challenging. Moreover, traditional data collection is prone to privacy risks, toxicity and lacks scalability. While, fully synthetic datasets are a promising alternative, research on their use in multilingual domain is limited as existing approaches still rely on machine translation to improve multilingual performance. To bridge this gap we introduce M2Lingual, the first fully synthetic, multi-turn multilingual dataset having 175K conversations across 70 languages with a balanced mix of high, low and mid-resourced languages. M2Lingual is constructed using a cost-efficient and scalable method that uses our novel two-step Evol prompt taxonomy to transform a small set of human written instructions to complex and challenging conversations. Results across three model families, six baseline datasets and evaluation spanning 31 languages demonstrates the effectiveness of M2Lingual over other datasets.</abstract>
      <url hash="b12d544d">2025.naacl-long.489</url>
      <bibkey>maheshwary-etal-2025-m2lingual</bibkey>
    </paper>
    <paper id="490">
      <title><fixed-case>M</fixed-case>ulti<tex-math>^3</tex-math><fixed-case>H</fixed-case>ate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision–Language Models</title>
      <author><first>Minh Duc</first><last>Bui</last><affiliation>Johannes-Gutenberg Universität Mainz</affiliation></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>9714-9731</pages>
      <abstract>Hate speech moderation on global platforms poses unique challenges due to the multimodal and multilingual nature of content, along with the varying cultural perceptions. How well do current vision-language models (VLMs) navigate these nuances? To investigate this, we create the first multimodal and multilingual parallel hate speech dataset, annotated by a multiculturally diverse set of annotators, called Multi<tex-math>^3</tex-math>Hate. It contains 300 parallel meme samples across 5 languages: English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural background significantly affects multimodal hate speech annotation in our dataset. The average pairwise agreement among countries is just 74%, significantly lower than that of randomly selected annotator groups. Our qualitative analysis indicates that the lowest pairwise label agreement—only 67% between the USA and India—can be attributed to cultural factors. We then conduct experiments with 5 large VLMs in a zero-shot setting, finding that these models align more closely with annotations from the US than with those from other cultures, even when the memes and prompts are presented in the native language of the other culture.</abstract>
      <url hash="42a34f34">2025.naacl-long.490</url>
      <bibkey>bui-etal-2025-multi3hate</bibkey>
    </paper>
    <paper id="491">
      <title>Grounding Fallacies Misrepresenting Scientific Publications in Evidence</title>
      <author><first>Max</first><last>Glockner</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Yufang</first><last>Hou</last><affiliation>IT:U Interdisciplinary Transformation University Austria, Technische Universität Darmstadt and IBM Research Ireland</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>9732-9767</pages>
      <abstract>Health-related misinformation claims often falsely cite a credible biomedical publication as evidence. These publications only superficially seem to support the false claim, when logical fallacies are applied. In this work, we aim to detect and to highlight such fallacies, which requires assessing the exact content of the misrepresented publications. To achieve this, we introduce MissciPlus, an extension of the fallacy detection dataset Missci. MissciPlus extends Missci by grounding the applied fallacies in real-world passages from misrepresented studies. This creates a realistic test-bed for detecting and verbalizing fallacies under real-world input conditions, and enables new and realistic passage-retrieval tasks. MissciPlus is the first logical fallacy dataset which pairs the real-world misrepresented evidence with incorrect claims, identical to the input to evidence-based fact-checking models. With MissciPlus, we i) benchmark retrieval models in identifying passages that support claims only with fallacious reasoning, ii) evaluate how well LLMs verbalize fallacious reasoning based on misrepresented scientific passages, and iii) assess the effectiveness of fact-checking models in refuting claims that misrepresent biomedical research. Our findings show that current fact-checking models struggle to use misrepresented scientific passages to refute misinformation. Moreover, these passages can mislead LLMs into accepting false claims as true.</abstract>
      <url hash="b30b5e7d">2025.naacl-long.491</url>
      <bibkey>glockner-etal-2025-grounding</bibkey>
    </paper>
    <paper id="492">
      <title>Has this Fact been Edited? Detecting Knowledge Edits in Language Models</title>
      <author><first>Paul</first><last>Youssef</last></author>
      <author><first>Zhixue</first><last>Zhao</last><affiliation>University of Sheffield, University of Sheffield</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <pages>9768-9784</pages>
      <abstract>Knowledge editing methods (KEs) can update language models’ obsolete or inaccurate knowledge learned from pre-training. However, KEs can be used for malicious applications, e.g., inserting misinformation and toxic content. Knowing whether a generated output is based on edited knowledge or first-hand knowledge from pre-training can increase users’ trust in generative models and provide more transparency. Driven by this, we propose a novel task: detecting knowledge edits in language models. Given an edited model and a fact retrieved by a prompt from an edited model, the objective is to classify the knowledge as either unedited (based on the pre-training), or edited (based on subsequent editing). We instantiate the task with four KEs, two large language models (LLMs), and two datasets. Additionally, we propose using hidden state representations and probability distributions as features for the detection model. Our results reveal that using these features as inputs to a simple AdaBoost classifier establishes a strong baseline. This baseline classifier requires a small amount of training data and maintains its performance even in cross-domain settings. Our work lays the groundwork for addressing potential malicious model editing, which is a critical challenge associated with the strong generative capabilities of LLMs.</abstract>
      <url hash="303a4f0b">2025.naacl-long.492</url>
      <bibkey>youssef-etal-2025-fact</bibkey>
    </paper>
    <paper id="493">
      <title><fixed-case>A</fixed-case>da<fixed-case>M</fixed-case>erge<fixed-case>X</fixed-case>: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging</title>
      <author><first>Yiran</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Huiming</first><last>Wang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>9785-9800</pages>
      <url hash="c932d380">2025.naacl-long.493</url>
      <bibkey>zhao-etal-2025-adamergex</bibkey>
    </paper>
    <paper id="494">
      <title>Coverage-based Fairness in Multi-document Summarization</title>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>9801-9819</pages>
      <abstract>Fairness in multi-document summarization (MDS) measures whether a system can generate a summary fairly representing information from documents with different social attribute values. Fairness in MDS is crucial since a fair summary can offer readers a comprehensive view. Previous works focus on quantifying summary-level fairness using Proportional Representation, a fairness measure based on Statistical Parity. However, Proportional Representation does not consider redundancy in input documents and overlooks corpus-level unfairness. In this work, we propose a new summary-level fairness measure, Equal Coverage, which is based on coverage of documents with different social attribute values and considers the redundancy within documents. To detect the corpus-level unfairness, we propose a new corpus-level measure, Coverage Parity. Our human evaluations show that our measures align more with our definition of fairness. Using our measures, we evaluate the fairness of thirteen different LLMs. We find that Claude3-sonnet is the fairest among all evaluated LLMs. We also find that almost all LLMs overrepresent different social attribute values. The code is available at https://github.com/leehaoyuan/coverage_fairness</abstract>
      <url hash="440456f6">2025.naacl-long.494</url>
      <bibkey>li-etal-2025-coverage</bibkey>
    </paper>
    <paper id="495">
      <title>Grammar Control in Dialogue Response Generation for Language Learning Chatbots</title>
      <author><first>Dominik</first><last>Glandorf</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Peng</first><last>Cui</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Detmar</first><last>Meurers</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>9820-9839</pages>
      <abstract>Chatbots based on large language models offer cheap conversation practice opportunities for language learners. However, they are hard to control for linguistic forms that correspond to learners’ current needs, such as grammar. We control grammar in chatbot conversation practice by grounding a dialogue response generation model in a pedagogical repository of grammar skills. We also explore how this control helps learners to produce specific grammar. We comprehensively evaluate prompting, fine-tuning, and decoding strategies for grammar-controlled dialogue response generation. Strategically decoding Llama3 outperforms GPT-3.5 when tolerating minor response quality losses. Our simulation predicts grammar-controlled responses to support grammar acquisition adapted to learner proficiency. Existing language learning chatbots and research on second language acquisition benefit from these affordances. Code available on GitHub.</abstract>
      <url hash="94383d15">2025.naacl-long.495</url>
      <bibkey>glandorf-etal-2025-grammar</bibkey>
    </paper>
    <paper id="496">
      <title>Does Mapo Tofu Contain Coffee? Probing <fixed-case>LLM</fixed-case>s for Food-related Cultural Knowledge</title>
      <author><first>Li</first><last>Zhou</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Wanlong</first><last>Liu</last></author>
      <author><first>Nicolas</first><last>Garneau</last></author>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Wenyu</first><last>Chen</last></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>9840-9867</pages>
      <abstract>Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain—a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs’ ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.</abstract>
      <url hash="6087eb40">2025.naacl-long.496</url>
      <bibkey>zhou-etal-2025-mapo</bibkey>
    </paper>
    <paper id="497">
      <title>Palette of Language Models: A Solver for Controlled Text Generation</title>
      <author><first>Zhe</first><last>Yang</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Yi</first><last>Huang</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Yaqin</first><last>Chen</last></author>
      <author><first>XiaotingWu</first><last>XiaotingWu</last></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Chao</first><last>Deng</last><affiliation>China Mobile Research Institute</affiliation></author>
      <pages>9868-9881</pages>
      <abstract>Recent advancements in large language models have revolutionized text generation with their remarkable capabilities. These models can produce controlled texts that closely adhere to specific requirements when prompted appropriately. However, designing an optimal prompt to control multiple attributes simultaneously can be challenging. A common approach is to linearly combine single-attribute models, but this strategy often overlooks attribute overlaps and can lead to conflicts. Therefore, we propose a novel combination strategy inspired by the Law of Total Probability and Conditional Mutual Information Minimization on generative language models. This method has been adapted for single-attribute control scenario and is termed the Palette of Language Models due to its theoretical linkage between attribute strength and generation style, akin to blending colors on an artist’s palette. Moreover, positive correlation and attribute enhancement are advanced as theoretical properties to guide a rational combination strategy design. We conduct experiments on both single control and multiple control settings, and achieve surpassing results.</abstract>
      <url hash="7e7619a0">2025.naacl-long.497</url>
      <bibkey>yang-etal-2025-palette</bibkey>
    </paper>
    <paper id="498">
      <title><fixed-case>MAMM</fixed-case>-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration</title>
      <author><first>David</first><last>Wan</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Justin</first><last>Chen</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>9882-9901</pages>
      <abstract>Multi-agent collaboration among models has shown promise in reasoning tasks but is underexplored in long-form generation tasks like summarization and question-answering. We extend multi-agent multi-model reasoning to generation, specifically to improving faithfulness through refinement, i.e., revising model-generated outputs to remove factual inconsistencies. We investigate how iterative collaboration among multiple instances and types of large language models (LLMs) enhances subtasks in the refinement process, such as error detection, critiquing unfaithful sentences, and making corrections based on critiques. We design intrinsic evaluations for each subtask, with our findings indicating that both multi-agent (multiple instances) and multi-model (diverse LLM types) approaches benefit error detection and critiquing. Additionally, reframing critiquing and refinement as reranking rather than generation tasks improves multi-agent performance. We consolidate these insights into a final “recipe” called **M**ulti-**A**gent **M**ulti-**M**odel **Refine**ment (MAMM-Refine), where multi-agent and multi-model collaboration significantly boosts performance on three summarization datasets as well as on long-form question answering, demonstrating the effectiveness and generalizability of our recipe. Our code is publicly available.</abstract>
      <url hash="99575653">2025.naacl-long.498</url>
      <bibkey>wan-etal-2025-mamm</bibkey>
    </paper>
    <paper id="499">
      <title><fixed-case>MAD</fixed-case>ial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation</title>
      <author><first>Junqing</first><last>He</last><affiliation>International Digital Econemy Academy</affiliation></author>
      <author><first>Liang</first><last>Zhu</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Xi</first><last>Wang</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <author><first>Jiaxing</first><last>Zhang</last><affiliation>IDEA</affiliation></author>
      <pages>9902-9921</pages>
      <abstract>Long-term memory is important for chatbots and dialogue systems (DS) to create consistent and human-like conversations, evidenced by numerous developed memory-augmented DS (MADS). To evaluate the effectiveness of such MADS, existing commonly used evaluation metrics, like retrieval accuracy and perplexity (PPL), mainly focus on query-oriented factualness and language quality assessment. However, these metrics often lack practical value. Moreover, the evaluation dimensions are insufficient for human-like assessment in DS. Regarding memory-recalling paradigms, current evaluation schemes only consider passive memory retrieval while ignoring diverse memory recall with rich triggering factors, e.g., emotions and surroundings, which can be essential in emotional support scenarios. To bridge the gap, we construct a novel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various memory-recalling paradigms based on cognitive science and psychology theories. The benchmark assesses two tasks separately: memory retrieval and memory recognition with the incorporation of both passive and proactive memory recall data. We introduce new scoring criteria to the evaluation, including memory injection, emotion support (ES) proficiency, and intimacy, to comprehensively assess generated responses. Results from cutting-edge embedding models and large language models on this benchmark indicate the potential for further advancement. Extensive testing further reveals correlations between memory injection, ES proficiency, and intimacy.</abstract>
      <url hash="086394d1">2025.naacl-long.499</url>
      <bibkey>he-etal-2025-madial</bibkey>
    </paper>
    <paper id="500">
      <title>Assessing the State of the Art in Scene Segmentation</title>
      <author><first>Albin</first><last>Zehe</last><affiliation>University of Würzburg</affiliation></author>
      <author><first>Elisabeth</first><last>Fischer</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Andreas</first><last>Hotho</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>9922-9941</pages>
      <abstract>The detection of scenes in literary texts is a recently introduced segmentation task in computational literary studies. Its goal is to partition a fictional text into segments that are coherent across the dimensions time, space, action and character constellation. This task is very challenging for automatic methods, since it requires a high-level understanding of the text. In this paper, we provide a thorough analysis of the State of the Art and challenges in this task, identifying and solving a problem in the training procedure for previous approaches, analysing the generalisation capabilities of the models and comparing the BERT-based SotA to current Llama models, as well as providing an analysis of what causes errors in the models. Our change in training procedure provides a significant increase in performance. We find that Llama-based models are more robust to different types of texts, while their overall performance is slightly worse than that of BERT-based models.</abstract>
      <url hash="6ed586c8">2025.naacl-long.500</url>
      <bibkey>zehe-etal-2025-assessing</bibkey>
    </paper>
    <paper id="501">
      <title><fixed-case>DCE</fixed-case>-<fixed-case>LLM</fixed-case>: Dead Code Elimination with Large Language Models</title>
      <author><first>Minyu</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Guoqiang</first><last>Li</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Ling-I</first><last>Wu</last></author>
      <author><first>Ruibang</first><last>Liu</last></author>
      <pages>9942-9955</pages>
      <abstract>Dead code introduces several challenges in software development, such as increased binary size and maintenance difficulties. It can also obscure logical errors and be exploited for obfuscation in malware. For LLM-based code-related tasks, dead code introduces vulnerabilities that can mislead these models, raising security concerns. Although modern compilers and IDEs offer dead code elimination, sophisticated patterns can bypass these tools. A universal approach that includes classification, location, explanation, and correction is needed, yet current tools often require significant manual effort. We present DCE-LLM, a framework for automated dead code elimination using a small CodeBERT model with an attribution-based line selector to efficiently locate suspect code. LLMs then generate judgments and explanations, fine-tuned on a large-scale, annotated dead code dataset to provide detailed explanations and patches. DCE-LLM outperforms existing tools, with advanced unreachability detection, automated correction, and support for multiple programming languages. Experimental results show DCE-LLM achieves over 94% F1 scores for unused and unreachable code, significantly surpassing GPT-4o by 30%.</abstract>
      <url hash="6f0ef500">2025.naacl-long.501</url>
      <bibkey>chen-etal-2025-dce</bibkey>
    </paper>
    <paper id="502">
      <title>Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction</title>
      <author><first>Liping</first><last>Liu</last></author>
      <author><first>Chunhong</first><last>Zhang</last></author>
      <author><first>Likang</first><last>Wu</last><affiliation>Tianjin University and Tianjin University</affiliation></author>
      <author><first>Chuang</first><last>Zhao</last></author>
      <author><first>Zheng</first><last>Hu</last></author>
      <author><first>Ming</first><last>He</last><affiliation>Lenovo Group Limited</affiliation></author>
      <author><first>Jianping</first><last>Fan</last><affiliation>AI Lab at Lenovo Research, Hangzhou Dianzi University and Northwest University</affiliation></author>
      <pages>9956-9978</pages>
      <abstract>Self-reflection for Large LanguageModels (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs’ internal reflection ability or external feedback. However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance. Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn issues. To mitigate this, we introduce **I**nstruct-**o**f-**R**eflec**t**ion (**IoRT**), a novel and general reflection framework that leverages dynamic-meta instruction to enhance the iterative reflection capability of LLMs. Specifically, we propose the instructor driven by the meta-thoughts and self-consistency classifier, generates various instructions, including refresh, stop, and select, to guide the next reflection iteration. Our experiments demonstrate that IoRT achieves an average improvement of 10.1% over established baselines in mathematical and commonsense reasoning tasks, highlighting its efficacy and applicability. Our code is available at https://github.com/llp635/IoRT.</abstract>
      <url hash="ee0a0abe">2025.naacl-long.502</url>
      <bibkey>liu-etal-2025-instruct</bibkey>
    </paper>
    <paper id="503">
      <title>Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment</title>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jongyoon</first><last>Song</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Bongkyu</first><last>Hwang</last><affiliation>SAMSUNG SDS RESEARCH KOREA</affiliation></author>
      <author><first>Hoyoung</first><last>Kang</last></author>
      <author><first>Sooah</first><last>Cho</last><affiliation>SamsungSDS</affiliation></author>
      <author><first>Junhwa</first><last>Choi</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Seongho</first><last>Joe</last><affiliation>Samsung</affiliation></author>
      <author><first>Taehee</first><last>Lee</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Youngjune</first><last>Gwon</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>9979-10001</pages>
      <abstract>A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.</abstract>
      <url hash="e019a099">2025.naacl-long.503</url>
      <bibkey>yu-etal-2025-correcting</bibkey>
    </paper>
    <paper id="504">
      <title><fixed-case>M</fixed-case>i<fixed-case>CE</fixed-case>val: Unveiling Multimodal Chain of Thought’s Quality via Image Description and Reasoning Steps</title>
      <author><first>Xiongtao</first><last>Zhou</last><affiliation>Independent</affiliation></author>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Lanyu</first><last>Chen</last></author>
      <author><first>Jingyu</first><last>Li</last></author>
      <author><first>Haojing</first><last>Chen</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Victor</first><last>Gutierrez Basulto</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <pages>10002-10039</pages>
      <abstract>**Multimodal Chain of Thought (MCoT)** is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose **Multimodal Chain-of-Thought Evaluation (MiCEval)**, a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found at: [https://anonymous_github/MicEval](https://anonymous.4open.science/r/MiCEval-847F/README.md).</abstract>
      <url hash="53794c41">2025.naacl-long.504</url>
      <bibkey>zhou-etal-2025-miceval</bibkey>
    </paper>
    <paper id="505">
      <title><fixed-case>C</fixed-case>artesian<fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>: Boosting Knowledge Sharing among Experts via <fixed-case>C</fixed-case>artesian Product Routing in Mixture-of-Experts</title>
      <author><first>Zhenpeng</first><last>Su</last></author>
      <author><first>Xing</first><last>W</last></author>
      <author><first>Zijia</first><last>Lin</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Yizhe</first><last>Xiong</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <author><first>Minxuan</first><last>Lv</last></author>
      <author><first>Guangyuan</first><last>Ma</last></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <author><first>Guiguang</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <pages>10040-10055</pages>
      <abstract>Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy. To tackle that, previous works introduced shared experts and combined their outputs with those of the top <tex-math>K</tex-math> routed experts in an addition manner. In this paper, inspired by collective matrix factorization to learn shared knowledge among data, we propose CartesianMoE, which implements more effective knowledge sharing among experts in more like a multiplication manner. Extensive experimental results indicate that CartesianMoE outperforms previous MoE models for building LLMs, in terms of both perplexity and downstream task performance. And we also find that CartesianMoE achieves better expert routing robustness.</abstract>
      <url hash="3d1b174c">2025.naacl-long.505</url>
      <bibkey>su-etal-2025-cartesianmoe</bibkey>
    </paper>
    <paper id="506">
      <title>Measuring and Benchmarking Large Language Models’ Capabilities to Generate Persuasive Language</title>
      <author><first>Amalie Brogaard</first><last>Pauli</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Ira</first><last>Assent</last><affiliation>Aarhus University</affiliation></author>
      <pages>10056-10075</pages>
      <abstract>We are exposed to much information trying to influence us, such as teaser messages, debates, politically framed news, and propaganda — all of which use persuasive language. With the recent interest in Large Language Models (LLMs), we study the ability of LLMs to produce persuasive text. As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase. We construct the new dataset Persuasive-Pairs of pairs of a short text and its rewrite by an LLM to amplify or diminish persuasive language. We multi-annotate the pairs on a relative scale for persuasive language: a valuable resource in itself, and for training a regression model to score and benchmark persuasive language, including for new LLMs across domains. In our analysis, we find that different ‘personas’ in LLaMA3’s system prompt change persuasive language substantially, even when only instructed to paraphrase.</abstract>
      <url hash="10218bb8">2025.naacl-long.506</url>
      <bibkey>pauli-etal-2025-measuring</bibkey>
    </paper>
    <paper id="507">
      <title><fixed-case>MILU</fixed-case>: A Multi-task <fixed-case>I</fixed-case>ndic Language Understanding Benchmark</title>
      <author><first>Sshubam</first><last>Verma</last></author>
      <author><first>Mohammed Safi Ur Rahman</first><last>Khan</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <pages>10076-10132</pages>
      <abstract>Evaluating Large Language Models (LLMs) in low-resource and linguistically diverse languages remains a significant challenge in NLP, particularly for languages using non-Latin scripts like those spoken in India. Existing benchmarks predominantly focus on English, leaving substantial gaps in assessing LLM capabilities in these languages. We introduce MILU, a Multi-task Indic Language Understanding Benchmark, a comprehensive evaluation benchmark designed to address this gap. MILU spans 8 domains and 41 subjects across 11 Indic languages, reflecting general and culturally specific knowledge. With an India-centric design, incorporates material from regional and state-level examinations, covering topics such as local history, arts, festivals, and laws, alongside standard subjects like science and mathematics. We evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with GPT-4o achieving the highest average accuracy at 74 percent. Open multilingual models outperform language-specific fine-tuned models, which perform only slightly better than random baselines. Models also perform better in high resource languages as compared to low resource ones. Domain-wise analysis indicates that models perform poorly in culturally relevant areas like Arts and Humanities, Law and Governance compared to general fields like STEM. To the best of our knowledge, MILU is the first of its kind benchmark focused on Indic languages, serving as a crucial step towards comprehensive cultural evaluation. All code, benchmarks, and artifacts are publicly available to foster open research.</abstract>
      <url hash="3428663c">2025.naacl-long.507</url>
      <bibkey>verma-etal-2025-milu</bibkey>
    </paper>
    <paper id="508">
      <title><fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>val-<fixed-case>T</fixed-case>o<fixed-case>D</fixed-case>: Automated Evaluation of Task-oriented Dialog Systems</title>
      <author><first>Arihant</first><last>Jain</last><affiliation>Amazon</affiliation></author>
      <author><first>Purav</first><last>Aggarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Rishav</first><last>Sahay</last><affiliation>Amazon</affiliation></author>
      <author><first>Chaosheng</first><last>Dong</last><affiliation>Amazon</affiliation></author>
      <author><first>Anoop</first><last>Saladi</last><affiliation>Amazon</affiliation></author>
      <pages>10133-10148</pages>
      <abstract>Task-oriented Dialog systems (ToD) are essential in automating user interactions, but their complex design and dynamic nature make evaluation particularly challenging. Current evaluation methodologies heavily depend on human annotators, which can be inefficient, subjective, and expensive to scale. To advance the field, there is a pressing need for a reliable, scalable, and systematic evaluation framework that can provide comprehensive insights into ToD system performance. In this paper, we propose, AutoEval-TOD, an automated end-to-end evaluation framework using large language models (LLMs). Our framework first interacts with the ToD system and then assesses its performance across key dimensions by analyzing both the ToD’s responses and internal states. We validate our approach by applying it to multiple ToD systems, highlighting its adaptability and potential for widespread use in both research and industrial settings.</abstract>
      <url hash="31221d60">2025.naacl-long.508</url>
      <bibkey>jain-etal-2025-autoeval</bibkey>
    </paper>
    <paper id="509">
      <title>Self-calibration for Language Model Quantization and Pruning</title>
      <author><first>Miles</first><last>Williams</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>George</first><last>Chrysostomou</last><affiliation>AstraZeneca</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>10149-10167</pages>
      <abstract>Quantization and pruning are fundamental approaches for model compression, enabling efficient inference for language models. In a post-training setting, state-of-the-art quantization and pruning methods require calibration data, a small set of unlabeled examples. Conventionally, this is randomly sampled web text, aiming to reflect the model training data. However, this poses two key problems: (1) unrepresentative calibration examples can harm model performance, and (2) organizations increasingly avoid releasing model training data. In this paper, we propose self-calibration as a solution. Our approach requires no external data, instead leveraging the model itself to generate synthetic calibration data, with a view to better approximating the pre-training data distribution. We extensively compare the performance of self-calibration with several baselines, across a variety of models, compression methods, and tasks. Our approach proves consistently competitive in maximizing downstream task performance, frequently outperforming even using real data.</abstract>
      <url hash="d16ee23e">2025.naacl-long.509</url>
      <bibkey>williams-etal-2025-self</bibkey>
    </paper>
    <paper id="510">
      <title>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models</title>
      <author><first>Tongxuan</first><last>Liu</last><affiliation>JD.com</affiliation></author>
      <author><first>Wenjiang</first><last>Xu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Weizhe</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yuting</first><last>Zeng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiaxing</first><last>Wang</last><affiliation>JD.com</affiliation></author>
      <author><first>Xingyu</first><last>Wang</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Hailong</first><last>Yang</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <pages>10168-10185</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought’s performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency’s performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.</abstract>
      <url hash="1b8c6c81">2025.naacl-long.510</url>
      <bibkey>liu-etal-2025-logic</bibkey>
    </paper>
    <paper id="511">
      <title><fixed-case>IFIR</fixed-case>: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval</title>
      <author><first>Tingyu</first><last>Song</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Guo</first><last>Gan</last></author>
      <author><first>Mingsheng</first><last>Shang</last><affiliation>CIGIT</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <pages>10186-10204</pages>
      <abstract>We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.</abstract>
      <url hash="6baef6aa">2025.naacl-long.511</url>
      <bibkey>song-etal-2025-ifir</bibkey>
    </paper>
    <paper id="512">
      <title><fixed-case>QAVA</fixed-case>: Query-Agnostic Visual Attack to Large Vision-Language Models</title>
      <author><first>Yudong</first><last>Zhang</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Jiansheng</first><last>Chen</last><affiliation>University of Science and Technology Beijing</affiliation></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent AI Platform</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>10205-10218</pages>
      <abstract>In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single image to be associated with multiple questions, and LVLMs may still answer other questions correctly even for an adversarial image attacked by a specific question. To address this, we introduce the query-agnostic visual attack (QAVA), which aims to create robust adversarial examples that generate incorrect responses to unspecified and unknown questions. Compared to traditional adversarial attacks focused on specific images and questions, QAVA significantly enhances the effectiveness and efficiency of attacks on images when the question is unknown, achieving performance comparable to attacks on known target questions. Our research broadens the scope of visual adversarial attacks on LVLMs in practical settings, uncovering previously overlooked vulnerabilities, particularly in the context of visual adversarial threats. The code is available at https://github.com/btzyd/qava.</abstract>
      <url hash="1b0f65f4">2025.naacl-long.512</url>
      <bibkey>zhang-etal-2025-qava</bibkey>
    </paper>
    <paper id="513">
      <title>Evaluating and Improving Graph to Text Generation with Large Language Models</title>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Yijun</first><last>Yang</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Wanqiu</first><last>Long</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Victor</first><last>Gutierrez Basulto</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>10219-10244</pages>
      <abstract>Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triples. To further improve LLMs in planning with graph sequences and grounding in truth, we introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks: reordering and attribution. Through extensive automatic and human evaluations, we demonstrate significant improvements in the quality of generated text from both few-shot learning and fine-tuning perspectives using the PlanGTG dataset. Our study paves the way for new research directions in graph-to-text generation.</abstract>
      <url hash="7d198197">2025.naacl-long.513</url>
      <bibkey>he-etal-2025-evaluating-improving</bibkey>
    </paper>
    <paper id="514">
      <title>The Plagiarism Singularity Conjecture</title>
      <author><first>Sriram</first><last>Ranga</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Anupam</first><last>Chattopadhyay</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>10245-10255</pages>
      <url hash="01f60221">2025.naacl-long.514</url>
      <bibkey>ranga-etal-2025-plagiarism</bibkey>
    </paper>
    <paper id="515">
      <title>Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning</title>
      <author><first>Sungjin</first><last>Park</last></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Edward</first><last>Choi</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>10256-10277</pages>
      <abstract>Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov decision process. In this framework, states represent intermediate reasoning paths, while actions consist of generating the next reasoning step using one of the language models selected from a predefined pool. Guided by a process-based reward model, LE-MCTS performs a tree search over the reasoning steps generated by different language models, identifying the most accurate reasoning chain. Experimental results on five mathematical reasoning benchmarks demonstrate that our approach outperforms both single language model decoding algorithms and language model ensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the MATH and MQA datasets, respectively, highlighting its effectiveness in solving complex reasoning problems.</abstract>
      <url hash="642def49">2025.naacl-long.515</url>
      <bibkey>park-etal-2025-ensembling</bibkey>
    </paper>
    <paper id="516">
      <title>One Unified Model for Diverse Tasks: Emotion Cause Analysis via Self-Promote Cognitive Structure Modeling</title>
      <author><first>Zhaoxin</first><last>Yu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xinglin</first><last>Xiao</last></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>10278-10293</pages>
      <abstract>Emotion cause analysis is a critical topic in natural language processing. Key tasks include emotion cause extraction (ECE), emotion-cause pair extraction (ECPE), social emotion cause identification (SECI) as well as social emotion mining and its cause identification (SEMCI). While current emotion cause analysis methods often focus on task-specific model design, they tend to overlook the underlying common ground across these tasks rooted in cognitive emotion theories, in particular, the cognitive structure of emotions. Drawing inspiration from this theory, in this paper, we propose a unified model capable of tackling diverse emotion cause analysis tasks, which constructs the emotion cognitive structure through LLM-based in-context learning. To mitigate the hallucination inherent in LLMs, we introduce a self-promote mechanism built on iterative refinement. It dynamically assesses the reliability of substructures based on their cognitive consistency and leverages the more reliable substructures to promote the inconsistent ones. Experimental results on multiple emotion cause analysis tasks ECE, ECPE, SECI and SEMCI demonstrate the superiority of our unified model over existing SOTA methods and LLM-based baselines.</abstract>
      <url hash="3a0cc8bf">2025.naacl-long.516</url>
      <bibkey>yu-etal-2025-one</bibkey>
    </paper>
    <paper id="517">
      <title>Soft Language Prompts for Language Transfer</title>
      <author><first>Ivan</first><last>Vykopal</last><affiliation>Kempelen Institute of Intelligent Technologies and Brno University of Technology</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Marian</first><last>Simko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>10294-10313</pages>
      <abstract>Cross-lingual knowledge transfer, especially between high- and low-resource languages, remains challenging in natural language processing (NLP). This study offers insights for improving cross-lingual NLP applications through the combination of parameter-efficient fine-tuning methods. We systematically explore strategies for enhancing cross-lingual transfer through the incorporation of language-specific and task-specific adapters and soft prompts. We present a detailed investigation of various combinations of these methods, exploring their efficiency across 16 languages, focusing on 10 mid- and low-resource languages. We further present to our knowledge the first use of soft prompts for language transfer, a technique we call soft language prompts. Our findings demonstrate that in contrast to claims of previous work, a combination of language and task adapters does not always work best; instead, combining a soft language prompt with a task adapter outperforms most configurations in many cases.</abstract>
      <url hash="5465517f">2025.naacl-long.517</url>
      <bibkey>vykopal-etal-2025-soft</bibkey>
    </paper>
    <paper id="518">
      <title><fixed-case>PICL</fixed-case>e: Pseudo-annotations for In-Context Learning in Low-Resource Named Entity Detection</title>
      <author><first>Sepideh</first><last>Mamooler</last><affiliation>School of Computer and Communication Sciences, EPFL - EPF Lausanne</affiliation></author>
      <author><first>Syrielle</first><last>Montariol</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Alexander</first><last>Mathis</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <pages>10314-10331</pages>
      <abstract>In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to come by. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially-correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate large quantities of demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We extensively evaluate PICLe on five biomedical NED datasets and show that, with zero human-annotation, PICLe outperforms ICL in low-resource settings where few gold examples can be used as in-context demonstrations.</abstract>
      <url hash="1b3d75a5">2025.naacl-long.518</url>
      <bibkey>mamooler-etal-2025-picle</bibkey>
    </paper>
    <paper id="519">
      <title>Can Large Language Models Invent Algorithms to Improve Themselves?</title>
      <author><first>Yoichi</first><last>Ishibashi</last><affiliation>NEC</affiliation></author>
      <author><first>Taro</first><last>Yano</last><affiliation>NEC</affiliation></author>
      <author><first>Masafumi</first><last>Oyamada</last><affiliation>NEC</affiliation></author>
      <pages>10332-10363</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable performance improvements and are rapidly gaining adoption in industry. However, the methods for improving LLMs are still designed by humans, which restricts the invention of new model-improving algorithms to human expertise and imagination. To address this, we propose the <i>Self-Developing</i> framework, which enables LLMs to autonomously generate and learn model-improvement algorithms. In this framework, the seed model generates, applies, and learns model-improving algorithms, continuously improving both the seed model and the algorithms themselves. Among model-improving strategies, we focus on model merging algorithms. In mathematical reasoning tasks, Self-Developing discovers novel merging strategies and outperforms human-designed methods. On GSM8k, the discovered algorithms improve the seed model by 6% and surpass human-designed methods by 4.3%. Moreover, they exhibit strong transferability, achieving a 7.4% performance gain on out-of-domain models. These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.</abstract>
      <url hash="e667a776">2025.naacl-long.519</url>
      <bibkey>ishibashi-etal-2025-large</bibkey>
    </paper>
    <paper id="520">
      <title>Simulating Classroom Education with <fixed-case>LLM</fixed-case>-Empowered Agents</title>
      <author><first>Zheyuan</first><last>Zhang</last></author>
      <author><first>Daniel</first><last>Zhang-Li</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Linlu</first><last>Gong</last></author>
      <author><first>Jinchang</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhanxin</first><last>Hao</last></author>
      <author><first>Jianxiao</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Cao</last></author>
      <author><first>Huiqin</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>10364-10379</pages>
      <abstract>Large language models (LLMs) have been applied across various intelligent educational tasks to assist teaching. While preliminary studies have focused on task-specific, independent LLM-empowered agents, the potential of LLMs within a multi-agent collaborative framework for classroom simulation with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation teaching framework. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Using the Flanders Interactive Analysis System and Community of Inquiry theoretical frameworks from educational analysis, we demonstrate that LLMs can simulate a dynamic learning environment for users with active teacher-student and student-student interactions. We also observe group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching. Our implementation and service can be found at https://github.com/THU-MAIC/SimClass.</abstract>
      <url hash="716d0ce3">2025.naacl-long.520</url>
      <bibkey>zhang-etal-2025-simulating</bibkey>
    </paper>
    <paper id="521">
      <title>A Grounded Typology of Word Classes</title>
      <author><first>Coleman</first><last>Haley</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Sharon</first><last>Goldwater</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>10380-10399</pages>
      <abstract>In this work, we propose a grounded approach to meaning in language typology. Using images captioned across languages, we can treat the images as an empirical language agnostic representation of meaning, allowing the quantification of language function and semantics. Using principles from information theory, we define “groundedness”, an empirical measure of contextual semantic contentfulness which can be computed using multilingual (vision-and-)language models. As an initial application, we apply this measure to the typology of word classes. We find our measure captures the contentfulness asymmetry between functional (grammatical) and lexical (content) classes across languages, but contradicts the view that functional classes do not convey content. We release a dataset of groundedness scores for 30 languages. Our results suggest that the grounded typology approach can provide quantitative evidence about semantic function in language.</abstract>
      <url hash="d83b27af">2025.naacl-long.521</url>
      <bibkey>haley-etal-2025-grounded</bibkey>
    </paper>
    <paper id="522">
      <title><fixed-case>SSH</fixed-case>: Sparse Spectrum Adaptation via Discrete Hartley Transformation</title>
      <author><first>Yixian</first><last>Shen</last></author>
      <author><first>Qi</first><last>Bi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Jia-hong</first><last>Huang</last><affiliation>University of Amsterdam, King Abdullah University of Science and Technology and National Taiwan University</affiliation></author>
      <author><first>Hongyi</first><last>Zhu</last></author>
      <author><first>Andy D.</first><last>Pimentel</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Anuj</first><last>Pathania</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>10400-10415</pages>
      <abstract>Low-rank adaptation (LoRA) has been demonstrated effective in reducing the trainable parameter number when fine-tuning a large foundation model (LLM). However, it still encounters computational and memory challenges when scaling to larger models or addressing more complex task adaptation.In this work, we introduce **Sparse Spectrum Adaptation via Discrete Hartley Transformation (SSH)**, a novel approach that significantly reduces the number of trainable parameters while enhancing model performance. It selects the most informative spectral components across all layers, under the guidance of the initial weights after a discrete Hartley transformation (DHT). The lightweight inverse DHT then projects the spectrum back into the spatial domain for updates.Extensive experiments across both single-modality tasks—such as language understanding and generation—and multi-modality tasks—such as video-text understanding—demonstrate that SSH outperforms existing parameter-efficient fine-tuning (PEFT) methods while achieving substantial reductions in computational cost and memory requirements. For instance, during instruction tuning on the LLaMA3.1 8B model, SSH achieves higher accuracy with only 0.048M trainable parameters compared to LoRA’s 33.5M, while reducing computational intensity up to 55% compared to FourierFT.</abstract>
      <url hash="d754c7e4">2025.naacl-long.522</url>
      <bibkey>shen-etal-2025-ssh</bibkey>
    </paper>
    <paper id="523">
      <title><fixed-case>LLM</fixed-case>-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue</title>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Sohhyung</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewon</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jinseok</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungzoon</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <pages>10416-10430</pages>
      <abstract>Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction. The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances. Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction. Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task. Beyond its superior performance, PRAISE offers additional benefits. It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies. Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase.</abstract>
      <url hash="af493c03">2025.naacl-long.523</url>
      <bibkey>kim-etal-2025-llm</bibkey>
    </paper>
    <paper id="524">
      <title><fixed-case>LCIRC</fixed-case>: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sumin</first><last>An</last><affiliation>Korea University</affiliation></author>
      <author><first>Junyoung</first><last>Sung</last><affiliation>Korea University</affiliation></author>
      <author><first>Wonpyo</first><last>Park</last><affiliation>Google</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Paul Hongsuck</first><last>Seo</last><affiliation>Korea University</affiliation></author>
      <pages>10431-10442</pages>
      <abstract>While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form sequences beyond the model’s length limit through recurrent compression without retraining the entire model. We further introduce query dependent context modeling, which selectively compresses query-relevant information, ensuring that the model retains the most pertinent content. Our empirical results demonstrate that Query Dependent LCIRC (QD-LCIRC) significantly improves LLM’s ability to manage extended contexts, making it well-suited for tasks that require both comprehensive context understanding and query relevance.</abstract>
      <url hash="781649d1">2025.naacl-long.524</url>
      <bibkey>an-etal-2025-lcirc</bibkey>
    </paper>
    <paper id="525">
      <title>A Template Is All You Meme</title>
      <author><first>Luke</first><last>Bates</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Peter Ebert</first><last>Christensen</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>10443-10475</pages>
      <abstract>Templatic memes, characterized by a semantic structure adaptable to the creator’s intent, represent a significant yet underexplored area within meme processing literature. With the goal of establishing a new direction for computational meme analysis, here we create a knowledge base composed of more than 5,200 meme templates, information about them, and 54,000 examples of template instances (templatic memes). To investigate the semantic signal of meme templates, we show that we can match memes in datasets to base templates contained in our knowledge base with a distance-based lookup. To demonstrate the power of meme templates, we create TSplit, a method to reorganize datasets, where a template or templatic instance can only appear in either the training or test split. Our re-split datasets enhance general meme knowledge and improve sample efficiency, leading to more robust models. Our examination of meme templates results in state-of-the-art performance for every dataset we consider, paving the way for analysis grounded in templateness.</abstract>
      <url hash="f913ad7b">2025.naacl-long.525</url>
      <bibkey>bates-etal-2025-template</bibkey>
    </paper>
    <paper id="526">
      <title><fixed-case>LLM</fixed-case>s vs Established Text Augmentation Techniques for Classification: When do the Benefits Outweight the Costs?</title>
      <author><first>Jan</first><last>Cegin</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Jakub</first><last>Simko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Peter</first><last>Brusilovsky</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>10476-10496</pages>
      <abstract>The generative large language models (LLMs) are increasingly being used for data augmentation tasks, where text samples are LLM-paraphrased and then used for classifier fine-tuning. Previous studies have compared LLM-based augmentations with established augmentation techniques, but the results are contradictory: some report superiority of LLM-based augmentations, while other only marginal increases (and even decreases) in performance of downstream classifiers. A research that would confirm a clear cost-benefit advantage of LLMs over more established augmentation methods is largely missing. To study if (and when) is the LLM-based augmentation advantageous, we compared the effects of recent LLM augmentation methods with established ones on 6 datasets, 3 classifiers and 2 fine-tuning methods. We also varied the number of seeds and collected samples to better explore the downstream model accuracy space. Finally, we performed a cost-benefit analysis and show that LLM-based methods are worthy of deployment only when very small number of seeds is used. Moreover, in many cases, established methods lead to similar or better model accuracies.</abstract>
      <url hash="3225d5b5">2025.naacl-long.526</url>
      <bibkey>cegin-etal-2025-llms</bibkey>
    </paper>
    <paper id="527">
      <title>Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions</title>
      <author><first>Moran</first><last>Yanuka</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Assaf</first><last>Ben-Kish</last><affiliation>Massachusetts Institute of Technology and Tel Aviv University</affiliation></author>
      <author><first>Yonatan</first><last>Bitton</last><affiliation>Google</affiliation></author>
      <author><first>Idan</first><last>Szpektor</last><affiliation>Google</affiliation></author>
      <author><first>Raja</first><last>Giryes</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>10497-10518</pages>
      <abstract>Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing standard data curation techniques does not effectively resolve this issue. To tackle this challenge, we introduce Knowledge Adapted (KnowAda) fine-tuning, a data-centric approach that automatically adapts training data with the model’s existing knowledge and visual understanding. KnowAda minimizes hallucinations while preserving high descriptiveness. We validate this approach across several small-scale VLMs (up to 7B parameters) and dense caption datasets, demonstrating that KnowAda effectively balances hallucination reduction and descriptiveness. Our results show that KnowAda outperforms various baselines in both automatic metrics and human evaluations.</abstract>
      <url hash="c42cf2a5">2025.naacl-long.527</url>
      <bibkey>yanuka-etal-2025-bridging</bibkey>
    </paper>
    <paper id="528">
      <title>Self-Training Meets Consistency: Improving <fixed-case>LLM</fixed-case>s’ Reasoning with Consistency-Driven Rationale Evaluation</title>
      <author><first>Jaehyeok</first><last>Lee</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>JinYeong</first><last>Bak</last><affiliation>SungKyunKwan University</affiliation></author>
      <pages>10519-10539</pages>
      <abstract>Self-training approach for large language models (LLMs) improves reasoning abilities by training the models on their self-generated rationales. Previous approaches have labeled rationales that produce correct answers for a given question as appropriate for training. However, a single measure risks misjudging rationale quality, leading the models to learn flawed reasoning patterns. To address this issue, we propose CREST (Consistency-driven Rationale Evaluation for Self-Training), a self-training framework that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training. Specifically, we introduce two methods: (1) filtering out rationales that frequently result in incorrect answers on follow-up questions and (2) preference learning based on mixed preferences from rationale evaluation results of both original and follow-up questions. Experiments on three question-answering datasets using open LLMs show that CREST not only improves the logical robustness and correctness of rationales but also improves reasoning abilities compared to previous self-training approaches.</abstract>
      <url hash="3e8f9061">2025.naacl-long.528</url>
      <bibkey>lee-etal-2025-self</bibkey>
    </paper>
    <paper id="529">
      <title>Evaluating Defeasible Reasoning in <fixed-case>LLM</fixed-case>s with <fixed-case>DEFREASING</fixed-case></title>
      <author><first>Emily</first><last>Allaway</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>10540-10558</pages>
      <url hash="2e1f5e08">2025.naacl-long.529</url>
      <bibkey>allaway-mckeown-2025-evaluating</bibkey>
    </paper>
    <paper id="530">
      <title>Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation Framework</title>
      <author><first>Jingyi</first><last>Sun</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Pepa</first><last>Atanasova</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>10559-10577</pages>
      <abstract>Explaining the decision-making process of machine learning models is crucial for ensuring their reliability and transparency for end users. One popular explanation form highlights key input features, such as i) tokens (e.g., Shapley Values and Integrated Gradients), ii) interactions between tokens (e.g., Bivariate Shapley and Attention-based methods), or iii) interactions between spans of the input (e.g., Louvain Span Interactions). However, these explanation types have only been studied in isolation, making it difficult to judge their respective applicability. To bridge this gap, we develop a unified framework that facilitates an automated and direct comparison between highlight and interactive explanations comprised of four diagnostic properties. We conduct an extensive analysis across these three types of input feature explanations – each utilizing three different explanation techniques–across two datasets and two models, and reveal that each explanation has distinct strengths across the different diagnostic properties. Nevertheless, interactive span explanations outperform other types of input feature explanations across most diagnostic properties. Despite being relatively understudied, our analysis underscores the need for further research to improve methods generating these explanation types. Additionally, integrating them with other explanation types that perform better in certain characteristics could further enhance their overall effectiveness.</abstract>
      <url hash="1630db43">2025.naacl-long.530</url>
      <bibkey>sun-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="531">
      <title>From Evidence to Belief: A <fixed-case>B</fixed-case>ayesian Epistemology Approach to Language Models</title>
      <author><first>Minsu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sangryul</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>10578-10611</pages>
      <abstract>This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models’ responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different evidence types. Also, we demonstrated that language models can exhibit high confidence when given strong evidence, but this does not always guarantee high accuracy. Our analysis also reveals that language models are biased toward golden evidence and show varying performance depending on the degree of irrelevance, helping explain why they deviate from Bayesian assumptions.</abstract>
      <url hash="41929d2b">2025.naacl-long.531</url>
      <bibkey>kim-etal-2025-evidence</bibkey>
    </paper>
    <paper id="532">
      <title>Private Synthetic Text Generation with Diffusion Models</title>
      <author><first>Sebastian</first><last>Ochs</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Ruhr-Universität Bochum</affiliation></author>
      <pages>10612-10626</pages>
      <abstract>How capable are diffusion models of generating synthetics texts? Recent research shows their strengths, with performance reaching that of auto-regressive LLMs. But are they also good in generating synthetic data if the training was under differential privacy? Here the evidence is missing, yet the promises from private image generation look strong. In this paper we address this open question by extensive experiments. At the same time, we critically assess (and reimplement) previous works on synthetic private text generation with LLMs and reveal some unmet assumptions that might have led to violating the differential privacy guarantees. Our results partly contradict previous non-private findings and show that fully open-source LLMs outperform diffusion models in the privacy regime. Our complete source codes, datasets, and experimental setup is publicly available to foster future research.</abstract>
      <url hash="4efa29a1">2025.naacl-long.532</url>
      <bibkey>ochs-habernal-2025-private</bibkey>
    </paper>
    <paper id="533">
      <title>Mitigating Tail Narrowing in <fixed-case>LLM</fixed-case> Self-Improvement via Socratic-Guided Sampling</title>
      <author><first>Yiwen</first><last>Ding</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Wei</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lizhuoyuan</first><last>Lizhuoyuan</last></author>
      <author><first>Yitao</first><last>Zhai</last><affiliation>Meituan</affiliation></author>
      <author><first>Shi</first><last>Xiaowei</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>10627-10646</pages>
      <abstract>Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs’ reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost diminish. This phenomenon limits the performance gain of self-improving models. A straightforward solution is brute-force sampling to balance the distribution, which significantly raises computational costs. In this paper, we introduce Guided Self-Improvement (GSI), a strategy aimed at improving the efficiency of sampling challenging heavy-tailed data. It leverages Socratic-style guidance signals to help LLM reasoning with complex queries, reducing the exploration effort and minimizing computational overhead. Experiments on four models across diverse mathematical tasks show that GSI strikes a balance between performance and efficiency, while also being effective on held-out tasks.</abstract>
      <url hash="45966848">2025.naacl-long.533</url>
      <bibkey>ding-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="534">
      <title><fixed-case>F</fixed-case>act<fixed-case>E</fixed-case>val: Evaluating the Robustness of Fact Verification Systems in the Era of Large Language Models</title>
      <author><first>Mamta</first><last>Mamta</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Oana</first><last>Cocarascu</last><affiliation>King’s College London</affiliation></author>
      <pages>10647-10660</pages>
      <abstract>Whilst large language models (LLMs) have made significant advances in every natural language processing task, studies have shown that these models are vulnerable to small perturbations in the inputs, raising concerns about their robustness in the real-world. Given the rise of misinformation online and its significant impact on society, fact verification is one area in which assessing the robustness of models developed for this task is crucial. However, the robustness of LLMs in fact verification remains largely unexplored. In this paper, we introduce FactEval, a novel large-scale benchmark for extensive evaluation of LLMs in the fact verification domain covering 17 realistic word-level and character-level perturbations and 4 types of subpopulations. We investigate the robustness of several LLMs in zero-shot, few-shot, and chain-of-thought prompting. Our analysis using FEVER, one of the largest and most widely-used datasets for fact verification, reveals that LLMs are brittle to small input changes and also exhibit performance variations across different subpopulations.</abstract>
      <url hash="b5f74906">2025.naacl-long.534</url>
      <bibkey>mamta-cocarascu-2025-facteval</bibkey>
    </paper>
    <paper id="535">
      <title>Analyzing Memorization in Large Language Models through the Lens of Model Attribution</title>
      <author><first>Tarun Ram</first><last>Menta</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Susmit</first><last>Agrawal</last></author>
      <author><first>Chirag</first><last>Agarwal</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>10661-10689</pages>
      <abstract>Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on post-hoc analyses—such as extracting memorized content or developing memorization metrics—without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM’s architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the model’s generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPT-Neo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real-world applications.</abstract>
      <url hash="78ac5803">2025.naacl-long.535</url>
      <bibkey>menta-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="536">
      <title>Track-<fixed-case>SQL</fixed-case>: Enhancing Generative Language Models with Dual-Extractive Modules for Schema and Context Tracking in Multi-turn Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Bingfeng</first><last>Chen</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Shaobin</first><last>Shi</last></author>
      <author><first>Yongqi</first><last>Luo</last></author>
      <author><first>Boyan</first><last>Xu</last></author>
      <author><first>Ruichu</first><last>Cai</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Zhifeng</first><last>Hao</last><affiliation>Shantou University</affiliation></author>
      <pages>10690-10708</pages>
      <abstract>Generative language models have shown significant potential in single-turn Text-to-SQL. However, their performance does not extend equivalently to multi-turn Text-to-SQL. This is primarily due to generative language models’ inadequacy in handling the complexities of context information and dynamic schema linking in multi-turn interactions. In this paper, we propose a framework named Track-SQL, which enhances generative language models with dual-extractive modules designed to track schema and contextual changes in multi-turn Text-to-SQL. Specifically, Track-SQL incorporates a <i>Semantic-enhanced Schema Extractor</i> and a <i>Schema-aware Context Extractor</i>. Experimental results demonstrate that Track-SQL achieves state-of-the-art performance on the SparC and CoSQL datasets. Furthermore, detailed ablation studies reveal that Track-SQL significantly improves execution accuracy in multi-turn interactions by 7.1% and 9.55% on these datasets, respectively. Our implementation will be open-sourced at <url>https://github.com/DMIRLAB-Group/Track-SQL</url>.</abstract>
      <url hash="59690435">2025.naacl-long.536</url>
      <bibkey>chen-etal-2025-track</bibkey>
    </paper>
    <paper id="537">
      <title>Prototypical Extreme Multi-label Classification with a Dynamic Margin Loss</title>
      <author><first>Kunal</first><last>Dahiya</last><affiliation>KnowDis</affiliation></author>
      <author><first>Diego</first><last>Ortego</last><affiliation>NielsenIQ - Innovation</affiliation></author>
      <author><first>David</first><last>Jimenez-Cabello</last><affiliation>NielsenIQ</affiliation></author>
      <pages>10709-10727</pages>
      <abstract>Extreme Multi-label Classification (XMC) methods predict relevant labels for a given query in an extremely large label space. Recent works in XMC address this problem using deep encoders that project text descriptions to an embedding space suitable for recovering the closest labels. However, learning deep models can be computationally expensive in large output spaces, resulting in a trade-off between high performing brute-force approaches and efficient solutions. In this paper, we propose PRIME, a XMC method that employs a novel prototypical contrastive learning technique to reconcile efficiency and performance surpassing brute-force approaches. We frame XMC as a data-to-prototype prediction task where label prototypes aggregate information from related queries. More precisely, we use a shallow transformer encoder that we coin as Label Prototype Network, which enriches label representations by aggregating text-based embeddings, label centroids and learnable free vectors. We jointly train a deep encoder and the Label Prototype Network using an adaptive triplet loss objective that better adapts to the high granularity and ambiguity of extreme label spaces. PRIME achieves state-of-the-art results in several public benchmarks of different sizes and domains, while keeping the model efficient.</abstract>
      <url hash="f4413d77">2025.naacl-long.537</url>
      <bibkey>dahiya-etal-2025-prototypical</bibkey>
    </paper>
    <paper id="538">
      <title><fixed-case>MCQG</fixed-case>-<fixed-case>SR</fixed-case>efine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback</title>
      <author><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Aditya</first><last>Parashar</last></author>
      <author><first>Huixue</first><last>Zhou</last></author>
      <author><first>Won Seok</first><last>Jang</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first/><last>Feiyun Ouyang</last></author>
      <author><first>Zhichao</first><last>Yang</last><affiliation>Optum AI</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>10728-10777</pages>
      <abstract>Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.</abstract>
      <url hash="22ec1e19">2025.naacl-long.538</url>
      <bibkey>yao-etal-2025-mcqg</bibkey>
    </paper>
    <paper id="539">
      <title>Main Predicate and Their Arguments as Explanation Signals For Intent Classification</title>
      <author><first>Sameer</first><last>Pimparkhede</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>10778-10789</pages>
      <abstract>Intent classification is crucial for conversational agents (chatbots), and deep learning models perform well in this area. However, little research has been done on the explainability of intent classification due to the absence of suitable benchmark data. Human annotation of explanation signals in text samples is time-consuming and costly. However, from inspection of data on intent classification, we see that, more often than not, the main verb denotes the action, and the direct object indicates the domain of conversation, serving as explanation signals for intent. This observation enables us to hypothesize that the main predicate in the text utterances, along with the arguments of the main predicate, can serve as explanation signals. Leveraging this, we introduce a new technique to automatically augment text samples from intent classification datasets with word-level explanations. We mark main predicates (primarily verbs) and their arguments (dependency relations) as explanation signals in benchmark intent classification datasets ATIS and SNIPS, creating a unique 21k-instance dataset for explainability. Further, we experiment with deep learning and language models. We observe that models that work well for classification do not perform well in explainability metrics like plausibility and faithfulness. We also observe that guiding models to focus on explanation signals from our dataset during training improves the plausibility Token F1 score by 3-4%, improving the model’s reasoning.</abstract>
      <url hash="be700763">2025.naacl-long.539</url>
      <bibkey>pimparkhede-bhattacharyya-2025-main</bibkey>
    </paper>
    <paper id="540">
      <title>Handling Missing Entities in Zero-Shot Named Entity Recognition: Integrated Recall and Retrieval Augmentation</title>
      <author><first>Ruichu</first><last>Cai</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Junhao</first><last>Lu</last></author>
      <author><first>Zhongjie</first><last>Chen</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Boyan</first><last>Xu</last></author>
      <author><first>Zhifeng</first><last>Hao</last><affiliation>Shantou University</affiliation></author>
      <pages>10790-10802</pages>
      <abstract>Zero-shot Named Entity Recognition (ZS-NER) aims to recognize entities in unseen domains without specific annotated data. A key challenge is handling missing entities while ensuring accurate type recognition, hindered by: 1) the pre-training assumption that each entity has a single type, overlooking diversity, and 2) insufficient contextual knowledge for type reasoning. To address this, we propose IRRA (Integrated Recall and Retrieval Augmentation), a novel two-stage framework leveraging large language model techniques. In the <i>Recall Augmented Entity Extracting</i> stage, we built a perturbed dataset to induce the model to exhibit missing or erroneous extracted entities. Based on this, we trained an enhanced model to correct these errors. This approach can improve the ZS-NER’s recall rate. In the <i>Retrieval Augmented Type Correcting</i> stage, we employ Retrieval-Augmented Generation techniques to locate entity-related unannotated contexts, with the additional contextual information significantly improving the accuracy of type correcting. Extensive evaluations demonstrate the state-of-the-art performance of our IRRA, with significant improvements in zero-shot cross-domain settings validated through both auto-evaluated metrics and analysis. Our implementation will be open-sourced at<url>https://github.com/DMIRLAB-Group/IRRA</url>.</abstract>
      <url hash="a566176c">2025.naacl-long.540</url>
      <bibkey>cai-etal-2025-handling</bibkey>
    </paper>
    <paper id="541">
      <title><fixed-case>KMI</fixed-case>: A Dataset of <fixed-case>K</fixed-case>orean Motivational Interviewing Dialogues for Psychotherapy</title>
      <author><first>Hyunjong</first><last>Kim</last></author>
      <author><first>Suyeon</first><last>Lee</last><affiliation>Industrial Engineering</affiliation></author>
      <author><first>Yeongjae</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Eunseo</first><last>Ryu</last></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Suran</first><last>Seong</last><affiliation>Korea Counseling Graduate University</affiliation></author>
      <author><first>Sungzoon</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <pages>10803-10828</pages>
      <abstract>The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.</abstract>
      <url hash="0669690c">2025.naacl-long.541</url>
      <bibkey>kim-etal-2025-kmi</bibkey>
    </paper>
    <paper id="542">
      <title>Automatic Input Rewriting Improves Translation with Large Language Models</title>
      <author><first>Dayeon</first><last>Ki</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>10829-10856</pages>
      <abstract>Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.</abstract>
      <url hash="0845539f">2025.naacl-long.542</url>
      <bibkey>ki-carpuat-2025-automatic</bibkey>
    </paper>
    <paper id="543">
      <title><fixed-case>HIGGS</fixed-case>: Pushing the Limits of Large Language Model Quantization via the Linearity Theorem</title>
      <author><first>Vladimir</first><last>Malinovskii</last><affiliation>Yandex</affiliation></author>
      <author><first>Andrei</first><last>Panferov</last></author>
      <author><first>Ivan</first><last>Ilin</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Han</first><last>Guo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Peter</first><last>Richtárik</last><affiliation>King Abdullah University of Science and Technology (KAUST)</affiliation></author>
      <author><first>Dan</first><last>Alistarh</last></author>
      <pages>10857-10886</pages>
      <abstract>Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a “linearity theorem” establishing a direct relationship between the layer-wise reconstruction error and the model perplexity increase due to quantization. This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free approaches such as the extremely popular NF4 quantized format, and (2) an optimal solution to the problem of finding non-uniform per-layer quantization levels which match a given compression constraint, obtained by reduction to dynamic programming. On the practical side, we demonstrate improved accuracy-compression trade-offs on Llama-family models, advancing both data-free and non-uniform quantization for large language models.</abstract>
      <url hash="24fdc4c1">2025.naacl-long.543</url>
      <bibkey>malinovskii-etal-2025-higgs</bibkey>
    </paper>
    <paper id="544">
      <title>The <fixed-case>LLM</fixed-case> Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units</title>
      <author><first>Badr</first><last>AlKhamissi</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Greta</first><last>Tuckute</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Martin</first><last>Schrimpf</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>10887-10911</pages>
      <abstract>Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units – but not random units – leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.</abstract>
      <url hash="1c727ef7">2025.naacl-long.544</url>
      <bibkey>alkhamissi-etal-2025-llm</bibkey>
    </paper>
    <paper id="545">
      <title><fixed-case>M</fixed-case>ix<fixed-case>LLM</fixed-case>: Dynamic Routing in Mixed Large Language Models</title>
      <author><first>Xinyuan</first><last>Wang</last></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Zhengzhang</first><last>Chen</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Wenchao</first><last>Yu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yanjie</first><last>Fu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <pages>10912-10922</pages>
      <abstract>Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4’s quality at 24.18% of the cost under the time constraint).</abstract>
      <url hash="5913fd44">2025.naacl-long.545</url>
      <bibkey>wang-etal-2025-mixllm</bibkey>
    </paper>
    <paper id="546">
      <title>Continual Learning in Multilingual Sign Language Translation</title>
      <author><first>Shakib</first><last>Yazdani</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Josef Van</first><last>Genabith</last><affiliation>German Research Center for AI and Universität des Saarlandes</affiliation></author>
      <author><first>Cristina</first><last>España-Bonet</last><affiliation>Barcelona Supercomputing Center and German Research Center for AI</affiliation></author>
      <pages>10923-10938</pages>
      <abstract>The field of sign language translation (SLT) is still in its infancy, as evidenced by the low translation quality, even when using deep learn- ing approaches. Probably because of this, many common approaches in other machine learning fields have not been explored in sign language. Here, we focus on continual learning for mul- tilingual SLT. We experiment with three con- tinual learning methods and compare them to four more naive baseline and fine-tuning ap- proaches. We work with four sign languages (ASL, BSL, CSL and DGS) and three spo- ken languages (Chinese, English and German). Our results show that incremental fine-tuning is the best performing approach both in terms of translation quality and transfer capabilities, and that continual learning approaches are not yet fully competitive given the current SOTA in SLT.</abstract>
      <url hash="43dd3a89">2025.naacl-long.546</url>
      <bibkey>yazdani-etal-2025-continual</bibkey>
    </paper>
    <paper id="547">
      <title>Few-Shot Natural Language to First-Order Logic Translation via Code Generation</title>
      <author><first>Junnan</first><last>Liu</last></author>
      <pages>10939-10960</pages>
      <abstract>Translation of natural language to first-order logical formula (NL-FOL) has recently gained significant attention for its critical role in logic-based NLP applications. Some studies attempt to utilize pretrained language models in a sequence-to-sequence manner for the NL-FOL task. However, these methods encounter challenges such as (1) inconsistency between the training and inference phases and (2) the data-intensive and resource-intensive finetuning process. This paper introduces a novel NL-FOL translation method, dubbed Code4Logic, which is based on in-context learning and employs code snippets to bridge the gap between natural language and first-order logic. By converting the translation task into a progressive code generation task, Code4Logic demonstrates strong generalization within a training-free manner, and enhances the performance of large language models (LLMs) to generate complex first-order logical formulas. Experimental results on NL-FOL task and downstream task datasets indicate that Code4Logic surpasses prominent training-free baselines and is comparable to supervised models trained on the full training data.</abstract>
      <url hash="80c1c8c1">2025.naacl-long.547</url>
      <bibkey>liu-2025-shot</bibkey>
    </paper>
    <paper id="548">
      <title>How Good Are <fixed-case>LLM</fixed-case>s for Literary Translation, Really? Literary Translation Evaluation with Humans and <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ran</first><last>Zhang</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Wei</first><last>Zhao</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>10961-10988</pages>
      <abstract>Recent research has focused on literary machine translation (MT) as a new challenge in MT. However, the evaluation of literary MT remains an open problem. We contribute to this ongoing discussion by introducing LITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human translations and outputs from 9 MT systems, which totals over 2k translations and 13k evaluated sentences across four language pairs, costing 4.5k€. This corpus enables us to (i) examine the consistency and adequacy of human evaluation schemes with various degrees of complexity, (ii) compare evaluations by students and professionals, assess the effectiveness of (iii) LLM-based metrics and (iv) LLMs themselves. Our findings indicate that the adequacy of human evaluation is controlled by two factors: the complexity of the evaluation scheme (more complex is less adequate) and the expertise of evaluators (higher expertise yields more adequate evaluations). For instance, MQM (Multidimensional Quality Metrics), a complex scheme and the de facto standard for non-literary human MT evaluation, is largely inadequate for literary translation evaluation: with student evaluators, nearly 60% of human translations are misjudged as indistinguishable or inferior to machine translations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme, identifies human translations at a rate of 80-100%. Automatic metrics fare dramatically worse, with rates of at most 20%. Our overall evaluation indicates that published human translations consistently outperform LLM translations, where even the most recent LLMs tend to produce considerably more literal and less diverse translations compared to humans.</abstract>
      <url hash="4d33e268">2025.naacl-long.548</url>
      <bibkey>zhang-etal-2025-good</bibkey>
    </paper>
    <paper id="549">
      <title><fixed-case>PORT</fixed-case>: Preference Optimization on Reasoning Traces</title>
      <author><first>Salem</first><last>Lahlou</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Abdalgader</first><last>Abubaker</last><affiliation>Technology Innovation Institute and University of Khartoum</affiliation></author>
      <author><first>Hakim</first><last>Hacid</last><affiliation>TII</affiliation></author>
      <pages>10989-11005</pages>
      <abstract>Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical reasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved abilities transfer to non-mathematical tasks, including the ARC benchmark and symbolic reasoning challenges. For example, our method can lead to up to relative 8.47 and 18.73 increases in accuracy on the GSM8K and AQuA benchmarks respectively, without any extra annotations. This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces.</abstract>
      <url hash="eaa556d9">2025.naacl-long.549</url>
      <bibkey>lahlou-etal-2025-port</bibkey>
    </paper>
    <paper id="550">
      <title>Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?</title>
      <author><first>Xuan</first><last>He</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>11006-11046</pages>
      <abstract>How can “weak teacher models” (Bowman et al., 2022) such as average human annotators or existing AI systems, effectively supervise LLMs to improve performance on hard reasoning tasks, especially those that challenge and requires expertise or daily practice from the teacher models? In this paper, we seek for empirical answers to this question by investigating various data-driven strategies that offer supervision data at different quality levels upon tasks of varying complexity. Two intuitive strategies emerge for teacher models to provide supervision during alignment training: 1) using lower-quality supervision from complete tasks that match the difficulty of the target reasoning tasks, and 2) leveraging higher-quality supervision from easier subtasks that are less challenging. Interestingly, we find that even when the outcome error rate for hard task supervision is high (e.g., 90%), training on such data can outperform perfectly correct supervision on easier subtasks on multiple hard math benchmarks. We further identify a more critical factor influencing training performance: step-wise error rates, which indicate the severity of errors in solutions. Specifically, training on hard task supervision with the same outcome error rates but disparate step-wise error rates can lead to a 30% accuracy gap on MATH benchmark. Our results also reveal that supplementing hard task supervision with the corresponding subtask supervision can yield notable performance improvements than simply combining rephrased hard full task supervision, suggesting new avenues for data augmentation. Data and code will be released upon acceptance.</abstract>
      <url hash="1e6970b9">2025.naacl-long.550</url>
      <bibkey>he-etal-2025-guiding</bibkey>
    </paper>
    <paper id="551">
      <title>Fine-Grained Transfer Learning for Harmful Content Detection through Label-Specific Soft Prompt Tuning</title>
      <author><first>Faeze</first><last>Ghorbanpour</last></author>
      <author><first>Viktor</first><last>Hangya</last><affiliation>Fraunhofer IIS</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>11047-11061</pages>
      <abstract>The spread of harmful content online is a dynamic issue evolving over time. Existing detection models, reliant on static data, are becoming less effective and generalizable. Developing new models requires sufficient up-to-date data, which is challenging. A potential solution is to combine existing datasets with minimal new data. However, detection tasks vary—some focus on hate speech, offensive, or abusive content, which differ in the intent to harm, while others focus on identifying targets of harmful speech such as racism, sexism, etc.—raising the challenge of handling nuanced class differences. To address these issues, we introduce a novel transfer learning method that leverages class-specific knowledge to enhance harmful content detection. In our approach, we first present label-specific soft prompt tuning, which captures and represents class-level information. Secondly, we propose two approaches to transfer this fine-grained knowledge from source (existing tasks) to target (unseen and new tasks): initializing the target task prompts from source prompts and using an attention mechanism that learns and adjusts attention scores to utilize the most relevant information from source prompts. Experiments demonstrate significant improvements in harmful content detection across English and German datasets, highlighting the effectiveness of label-specific representations and knowledge transfer.</abstract>
      <url hash="5a217457">2025.naacl-long.551</url>
      <bibkey>ghorbanpour-etal-2025-fine</bibkey>
    </paper>
    <paper id="552">
      <title>A Systematic Examination of Preference Learning through the Lens of Instruction-Following</title>
      <author><first>Joongwon</first><last>Kim</last><affiliation>Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <author><first>Anirudh</first><last>Goyal</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Aston</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Bo</first><last>Xiong</last></author>
      <author><first>Rui</first><last>Hou</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Melanie</first><last>Kambadur</last><affiliation>Facebook</affiliation></author>
      <author><first>Dhruv</first><last>Mahajan</last><affiliation>Meta AI</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Liang</first><last>Tan</last><affiliation>Facebook</affiliation></author>
      <pages>11062-11082</pages>
      <abstract>In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use rejection sampling (RS) and Monte Carlo Tree Search (MCTS) to obtain preference pairs. Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts. Our experiments reveal that shared prefixes provide marginal but consistent improvements and greater stability across challenging training configurations. While high-contrast preference pairs generally outperform low-contrast pairs, combining both often yields the best performance. Additionally, training on prompts of moderate difficulty leads to better generalization across different tasks. Our findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment.</abstract>
      <url hash="85edf37e">2025.naacl-long.552</url>
      <bibkey>kim-etal-2025-systematic</bibkey>
    </paper>
    <paper id="553">
      <title>Lived Experience Not Found: <fixed-case>LLM</fixed-case>s Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use</title>
      <author><first>Mohit</first><last>Chandra</last></author>
      <author><first>Siddharth</first><last>Sriraman</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Gaurav</first><last>Verma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Harneet Singh</first><last>Khanuja</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jose Suarez</first><last>Campayo</last></author>
      <author><first>Zihang</first><last>Li</last></author>
      <author><first>Michael L.</first><last>Birnbaum</last></author>
      <author><first>Munmun</first><last>De Choudhury</last></author>
      <pages>11083-11113</pages>
      <abstract>Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, we introduce the **Psych-ADR** benchmark and the **A**dverse **D**rug Reaction **R**esponse **A**ssessment (**ADRA**) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies. Our analyses show that LLMs struggle with understanding the nuances of ADRs and differentiating between types of ADRs. While LLMs align with experts in terms of expressed emotions and tone of the text, their responses are more complex, harder to read, and only 70.86% aligned with expert strategies. Furthermore, they provide less actionable advice by a margin of 12.32% on average. Our work provides a comprehensive benchmark and evaluation framework for assessing LLMs in strategy-driven tasks within high-risk domains.</abstract>
      <url hash="50076ded">2025.naacl-long.553</url>
      <bibkey>chandra-etal-2025-lived</bibkey>
    </paper>
    <paper id="554">
      <title>Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision</title>
      <author><first>Zhouhang</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tushar</first><last>Khot</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bhavana</first><last>Dalvi Mishra</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Harshit</first><last>Surana</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>11114-11134</pages>
      <abstract>Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM’s reasoning ability and drops when the data is noisy or beyond LLM’s knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM’s instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short. Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.</abstract>
      <url hash="be8cdae4">2025.naacl-long.554</url>
      <bibkey>xie-etal-2025-latent</bibkey>
    </paper>
    <paper id="555">
      <title><fixed-case>LLM</fixed-case>-Supported Natural Language to Bash Translation</title>
      <author><first>Finnian</first><last>Westenfelder</last></author>
      <author><first>Erik</first><last>Hemberg</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Stephen</first><last>Moskal</last></author>
      <author><first>Una-May</first><last>O’Reilly</last></author>
      <author><first>Silviu</first><last>Chiricescu</last><affiliation>Charles Stark Draper</affiliation></author>
      <pages>11135-11147</pages>
      <abstract>The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning and constrained decoding can improve NL2SH accuracy by up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation. Our code is available at https://github.com/westenfelder/NL2SH</abstract>
      <url hash="e5c8e3ac">2025.naacl-long.555</url>
      <bibkey>westenfelder-etal-2025-llm</bibkey>
    </paper>
    <paper id="556">
      <title><fixed-case>REL</fixed-case>-<fixed-case>A</fixed-case>.<fixed-case>I</fixed-case>.: An Interaction-Centered Approach To Measuring Human-<fixed-case>LM</fixed-case> Reliance</title>
      <author><first>Kaitlyn</first><last>Zhou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jena D.</first><last>Hwang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>11148-11167</pages>
      <abstract>The ability to communicate uncertainty and knowledge limitations is crucial for the safety of large language models (LLMs). Current evaluations of these abilities typically examine the correspondence between model accuracy and its internal probabilities or linguistic outputs. However, evaluation of the uncertainty of LLM communication should also focus on the behaviors of their human interlocutors: how much do users rely on what the LLM says? We introduce an interaction-centered evaluation approach called Rel-A.I. (pronounced “rely”) that quantifies whether and how humans rely on LLMs’ responses, complementing existing calibration evaluations. Through nine user studies with 450 participants, we investigate three crucial aspects that influence user reliance. We show that emphatic expressions of politeness (e.g., “I’m happy to help!”) that precede LLM answers will cause participants to perceive these models as more competent, and in turn, rely 30% more on their generations. Additionally, the context of the interaction, such as the knowledge domain and nature of previous interactions with the LLM, substantially influences user reliance (e.g., users will rely 10% more on LLMs when responding to questions involving calculations). Our results show that calibration and language quality alone are insufficient in informing which LLMs are safely calibrated, and illustrate the need to consider features of the interactional context.</abstract>
      <url hash="6a1953ff">2025.naacl-long.556</url>
      <bibkey>zhou-etal-2025-rel</bibkey>
    </paper>
    <paper id="557">
      <title>Eliciting Critical Reasoning in Retrieval-Augmented Generation via Contrastive Explanations</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>11168-11183</pages>
      <abstract>Retrieval-augmented generation (RAG) have emerged as a critical mechanism in contemporary NLP to support Large Language Models (LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms bring its inherent challenges, as LLMs need to integrate potentially noisy contexts. Recent studies have shown that LLMs still struggle to critically analyse RAG-based in-context information, a limitation that may lead to incorrect inferences and hallucinations. In this paper, we investigate how to elicit critical arguments in RAG via contrastive explanations. In particular, we propose Contrastive-RAG (CRAG), a framework that (i) retrieves relevant documents given a query,(ii) selects and exemplifies relevant passages, and (iii) generates explanations that explicitly contrast the relevance of the passages to (iv) support the final answer. We show the impact of C-RAG building contrastive reasoning demonstrations from LLMs to instruct smaller models for retrieval-augmented tasks. Extensive experiments demonstrate that CRAG improves state-of-the-art RAG models while (a) requiring significantly fewer prompts and demonstrations and (b) being robust to perturbations in the retrieved documents.</abstract>
      <url hash="0f4dca48">2025.naacl-long.557</url>
      <bibkey>ranaldi-etal-2025-eliciting</bibkey>
    </paper>
    <paper id="558">
      <title>A Distributional Perspective on Word Learning in Neural Language Models</title>
      <author><first>Filippo</first><last>Ficarra</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Alex</first><last>Warstadt</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>11184-11207</pages>
      <abstract>Language models (LMs) are increasingly being studied as models of human language learners.Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models.Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models.However, there are no widely agreed-upon metrics for word learning in language models.We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word.We argue that distributional signatures studied in prior work fail to capture key distributional information. Thus, we propose an array of signatures that improve on earlier approaches by capturing knowledge of both where the target word can and cannot occur as well as gradient preferences about the word’s appropriateness.We obtain learning trajectories for a selection of small language models we train from scratch, study the relationship between different distributional signatures, compare how well they align with human word learning trajectories and interpretable lexical features, and address basic methodological questions about estimating these distributional signatures.Our metrics largely capture complementary information, suggesting that it is important not to rely on a single metric.However, across all metrics, language models’ learning trajectories fail to correlate with those of children.</abstract>
      <url hash="fbb4c57b">2025.naacl-long.558</url>
      <bibkey>ficarra-etal-2025-distributional</bibkey>
    </paper>
    <paper id="559">
      <title>Disentangling language change: sparse autoencoders quantify the semantic evolution of indigeneity in <fixed-case>F</fixed-case>rench</title>
      <author><first>Jacob A.</first><last>Matthews</last></author>
      <author><first>Laurent</first><last>Dubreuil</last><affiliation>Cornell University</affiliation></author>
      <author><first>Imane</first><last>Terhmina</last></author>
      <author><first>Yunci</first><last>Sun</last><affiliation>Cornell University</affiliation></author>
      <author><first>Matthew</first><last>Wilkens</last><affiliation>Cornell University</affiliation></author>
      <author><first>Marten Van</first><last>Schijndel</last><affiliation>Cornell University</affiliation></author>
      <pages>11208-11222</pages>
      <abstract>This study presents a novel approach to analyzing historical language change, focusing on the evolving semantics of the French term “indigène(s)” (“indigenous”) between 1825 and 1950. While existing approaches to measuring semantic change with contextual word embeddings (CWE) rely primarily on similarity measures or clustering, these methods may not be suitable for highly imbalanced datasets, and pose challenges for interpretation. For this reason, we propose an interpretable, feature-level approach to analyzing language change, which we use to trace the semantic evolution of “indigène(s)” over a 125-year period. Following recent work on sequence embeddings (O’Neill et al., 2024), we use <tex-math>k</tex-math>-sparse autoencoders (<tex-math>k</tex-math>-SAE) (Makhzani and Frey, 2013) to interpret over 210,000 CWEs generated using sentences sourced from the French National Library. We demonstrate that <tex-math>k</tex-math>-SAEs can learn interpretable features from CWEs, as well as how differences in feature activations across time periods reveal highly specific aspects of language change. In addition, we show that diachronic change in feature activation frequency reflects the evolution of French colonial legal structures during the 19th and 20th centuries.</abstract>
      <url hash="961a5a43">2025.naacl-long.559</url>
      <bibkey>matthews-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="560">
      <title>Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages</title>
      <author><first>Max</first><last>Zuo</last><affiliation>Brown University</affiliation></author>
      <author><first>Francisco Piedrahita</first><last>Velez</last><affiliation>Brown University</affiliation></author>
      <author><first>Xiaochen</first><last>Li</last></author>
      <author><first>Michael</first><last>Littman</last><affiliation>Brown University, Brown University, Brown University and Georgia Institute of Technology</affiliation></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Computer Science Department, Brown University and Snorkel AI</affiliation></author>
      <pages>11223-11240</pages>
      <abstract>Recent works have explored using language models for planning problems. One approach examines translating natural language descriptions of planning tasks into structured planning languages, such as the planning domain definition language (PDDL). Existing evaluation methods struggle to ensure semantic correctness and rely on simple or unrealistic datasets. To bridge this gap, we introduce Planetarium, a benchmark designed to evaluate language models’ ability to generate PDDL code from natural language descriptions of planning tasks. Planetarium features a novel PDDL equivalence algorithm that flexibly evaluates the correctness of generated PDDL against ground truth, along with a dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations with varying levels of difficulty. Finally, we evaluate several API-access and open-weight language models that reveal this task’s complexity. For example, 96.1% of the PDDL problem descriptions generated by GPT-4o are syntactically parseable, 94.4% are solvable, but only 24.8% are semantically correct, highlighting the need for a more rigorous benchmark for this problem.</abstract>
      <url hash="933564ce">2025.naacl-long.560</url>
      <bibkey>zuo-etal-2025-planetarium</bibkey>
    </paper>
    <paper id="561">
      <title>One fish, two fish, but not the whole sea: Alignment reduces language models’ conceptual diversity</title>
      <author><first>Sonia Krishna</first><last>Murthy</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Tomer</first><last>Ullman</last><affiliation>Harvard University</affiliation></author>
      <author><first>Jennifer</first><last>Hu</last><affiliation>Harvard University</affiliation></author>
      <pages>11241-11258</pages>
      <abstract>Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models’ internal diversity. Inspired by human studies, we use a new way of measuring the conceptual diversity of synthetically-generated LLM “populations” by relating the internal variability of simulated individuals to the population-level variability. We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data. While no model reaches human-like diversity, aligned models generally display less diversity than their instruction fine-tuned counterparts. Our findings highlight potential trade-offs between increasing models’ value alignment and decreasing the diversity of their conceptual representations.</abstract>
      <url hash="831283fd">2025.naacl-long.561</url>
      <bibkey>murthy-etal-2025-one</bibkey>
    </paper>
    <paper id="562">
      <title>Using Text-Based Causal Inference to Disentangle Factors Influencing Online Review Ratings</title>
      <author><first>Linsen</first><last>Li</last></author>
      <author><first>Aron</first><last>Culotta</last><affiliation>Tulane University</affiliation></author>
      <author><first>Nicholas</first><last>Mattei</last><affiliation>Tulane University</affiliation></author>
      <pages>11259-11277</pages>
      <abstract>Online reviews provide valuable insights into the perceived quality of facets of a product or service. While aspect-based sentiment analysis has focused on extracting these facets from reviews, there is less work understanding the impact of each aspect on overall perception. This is particularly challenging given correlations among aspects, making it difficult to isolate the effects of each. This paper introduces a methodology based on recent advances in text-based causal analysis, specifically CausalBERT, to disentangle the effect of each factor on overall review ratings. We enhance CausalBERT with three key improvements: temperature scaling for better calibrated treatment assignment estimates; hyperparameter optimization to reduce confound overadjustment; and interpretability methods to characterize discovered confounds. In this work, we treat the textual mentions in reviews as proxies for real-world attributes. We validate our approach on real and semi-synthetic data from over 600K reviews of U.S. K-12 schools. We find that the proposed enhancements result in more reliable estimates, and that perception of school administration and performance on benchmarks are significant drivers of overall school ratings.</abstract>
      <url hash="ad7d67f9">2025.naacl-long.562</url>
      <bibkey>li-etal-2025-using</bibkey>
    </paper>
    <paper id="563">
      <title>Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate</title>
      <author><first>Xiaomeng</first><last>Jin</last></author>
      <author><first>Zhiqi</first><last>Bu</last><affiliation>Amazon</affiliation></author>
      <author><first>Bhanukiran</first><last>Vinzamuri</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Volkan</first><last>Cevher</last><affiliation>EPFL - EPF Lausanne and Amazon Development Center Germany</affiliation></author>
      <author><first>Mingyi</first><last>Hong</last><affiliation>Amazon and University of Minnesota, Minneapolis</affiliation></author>
      <pages>11278-11294</pages>
      <abstract>Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.</abstract>
      <url hash="3314bc34">2025.naacl-long.563</url>
      <bibkey>jin-etal-2025-unlearning</bibkey>
    </paper>
    <paper id="564">
      <title><fixed-case>REFFLY</fixed-case>: Melody-Constrained Lyrics Editing Model</title>
      <author><first>Songyan</first><last>Zhao</last></author>
      <author><first>Bingxuan</first><last>Li</last></author>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>11295-11315</pages>
      <abstract>Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision—editing plain text draft to fit it into the melody—offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the melody intact), or style transfer (adapting lyrics to different genres). This paper introduces REFFLY (REvision Framework For LYrics), the first revision framework for editing and generating melody-aligned lyrics. We train the lyric revision module using our curated synthesized melody-aligned lyrics dataset, enabling it to transform plain text into lyrics that align with a given melody. To further enhance the revision ability, we propose training-free heuristics aimed at preserving both semantic meaning and musical consistency throughout the editing process. Experimental results demonstrate the effectiveness of REFFLY across various tasks (e.g. song translation), showing that our model outperforms strong baselines, including Lyra (CITATION) and GPT-4, by 25% in both musicality and text quality.</abstract>
      <url hash="8608b574">2025.naacl-long.564</url>
      <bibkey>zhao-etal-2025-reffly</bibkey>
    </paper>
    <paper id="565">
      <title>Exploring Safety-Utility Trade-Offs in Personalized Language Models</title>
      <author><first>Anvesh Rao</first><last>Vijjini</last></author>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>11316-11340</pages>
      <abstract>As large language models (LLMs) become increasingly integrated into daily applications, it is essential to ensure they function fairly across diverse user demographics. In this work, we show that LLMs suffer from personalization bias, where their performance is impacted when they are personalized to a user’s identity. We quantify personalization bias by evaluating the performance of LLMs along two axes - safety and utility. We measure safety by examining how benign LLM responses are to unsafe prompts. We measure utility by evaluating the LLM’s performance on various tasks, including general knowledge, mathematical abilities, programming, and reasoning skills. We find that various LLMs, ranging from open-source models like Llama-3.1 and Mistral to API-based ones like GPT-3.5 and GPT-4o, exhibit significant variance in performance in terms of safety and utility when personalized with different user identities. Finally, we discuss several strategies to mitigate personalization bias and investigate the origin of personalization bias.</abstract>
      <url hash="cf383a7f">2025.naacl-long.565</url>
      <bibkey>vijjini-etal-2025-exploring</bibkey>
    </paper>
    <paper id="566">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>C</fixed-case>hart<fixed-case>QA</fixed-case>: Benchmarking Vision-Language Models on Multi-Chart Problems</title>
      <author><first>Zifeng</first><last>Zhu</last></author>
      <author><first>Mengzhao</first><last>Jia</last></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Lang</first><last>Li</last></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>11341-11359</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have demonstrated impressive abilities across various tasks, including visual question answering and chart comprehension, yet existing benchmarks for chart-related tasks fall short in capturing the complexity of real-world multi-chart scenarios. Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications. To fill this gap, we introduce MultiChartQA, a benchmark that evaluates MLLMs’ capabilities in four key areas: direct question answering, parallel question answering, comparative reasoning, and sequential reasoning. Our evaluation of a wide range of MLLMs reveals significant performance gaps compared to humans. These results highlight the challenges in multi-chart comprehension and the potential of MultiChartQA to drive advancements in this field. Our code and data are available at https://github.com/Zivenzhu/Multi-chart-QA.</abstract>
      <url hash="c5699b3d">2025.naacl-long.566</url>
      <bibkey>zhu-etal-2025-multichartqa</bibkey>
    </paper>
    <paper id="567">
      <title>It Is Not Only the Negative that Deserves Attention! Understanding, Generation &amp; Evaluation of (Positive) Moderation</title>
      <author><first>Iman</first><last>Jundi</last></author>
      <author><first>Eva Maria</first><last>Vecchi</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Carlotta</first><last>Quensel</last></author>
      <author><first>Neele</first><last>Falk</last></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>GESIS – Leibniz Institute for the Social Sciences and Heinrich-Heine University Düsseldorf</affiliation></author>
      <pages>11360-11395</pages>
      <abstract>Moderation is essential for maintaining and improving the quality of online discussions. This involves: (1) countering negativity, e.g. hate speech and toxicity, and (2) promoting positive discourse, e.g. broadening the discussion to involve other users and perspectives. While significant efforts have focused on addressing negativity, driven by an urgency to address such issues, this left moderation promoting positive discourse (henceforth PositiveModeration) under-studied. With the recent advancements in LLMs, Positive Moderation can potentially be scaled to vast conversations, fostering more thoughtful discussions and bridging the increasing divide in online interactions.We advance the understanding of Positive Moderation by annotating a dataset on 13 moderation properties, e.g. neutrality, clarity and curiosity. We extract instructions from professional moderation guidelines and use them to prompt LLaMA to generate such moderation. This is followed by extensive evaluation showing that (1) annotators rate generated higher than professional moderation, but still slightly prefer professional moderation in pairwise comparison, and (2) LLMs can be used to estimate human evaluation as an efficient alternative.</abstract>
      <url hash="33fabe78">2025.naacl-long.567</url>
      <bibkey>jundi-etal-2025-negative</bibkey>
    </paper>
    <paper id="568">
      <title>Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice</title>
      <author><first>Sunny</first><last>Rai</last><affiliation>School of Engineering and Applied Science, University of Pennsylvania</affiliation></author>
      <author><first>Khushang</first><last>Zaveri</last></author>
      <author><first>Shreya</first><last>Havaldar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Soumna</first><last>Nema</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>11396-11415</pages>
      <abstract>Shame and pride are social emotions expressed across cultures to motivate and regulate people’s thoughts, feelings, and behaviors. In this paper, we introduce the first cross-cultural dataset of over 10k shame/pride-related expressions with underlying social expectations from ~5.4K Bollywood and Hollywood movies. We examine *how* and *why* shame and pride are expressed across cultures using a blend of psychology-informed language analysis combined with large language models. We find significant cross-cultural differences in shame and pride expression aligning with known cultural tendencies of the USA and India – e.g., in Hollywood, shame-expressions predominantly discuss *self* whereas shame is expressed toward *others* in Bollywood. Women are more sanctioned across cultures and for violating similar social expectations.</abstract>
      <url hash="6cf10788">2025.naacl-long.568</url>
      <bibkey>rai-etal-2025-social</bibkey>
    </paper>
    <paper id="569">
      <title>The Stochastic Parrot on <fixed-case>LLM</fixed-case>’s Shoulder: A Summative Assessment of Physical Concept Understanding</title>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Junjie</first><last>Wu</last><affiliation>HKUST</affiliation></author>
      <author><first>Tsz Ting</first><last>Chung</last></author>
      <author><first>Shunchi</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jiangnan</first><last>Li</last><affiliation>WeChat, Tencent Inc.</affiliation></author>
      <author><first>Dit-Yan</first><last>Yeung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>11416-11431</pages>
      <abstract>In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, P HYSI C O. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ∼40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.</abstract>
      <url hash="d3f1edb1">2025.naacl-long.569</url>
      <bibkey>yu-etal-2025-stochastic</bibkey>
    </paper>
    <paper id="570">
      <title>m<fixed-case>H</fixed-case>uman<fixed-case>E</fixed-case>val - A Multilingual Benchmark to Evaluate Large Language Models for Code Generation</title>
      <author><first>Md Nishat</first><last>Raihan</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>11432-11461</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly enhanced code generation from natural language prompts. The HumanEval Benchmark, developed by OpenAI, remains the most widely used code generation benchmark. However, this and other Code LLM benchmarks face critical limitations, particularly in task diversity, test coverage, and linguistic scope. Current evaluations primarily focus on English-to-Python conversion tasks with limited test cases, potentially overestimating model performance. While recent works have addressed test coverage and programming language (PL) diversity, code generation from low-resource language prompts remains largely unexplored. To address this gap, we introduce mHumanEval, an extended benchmark supporting prompts in over 200 natural languages. We employ established machine translation methods to compile the benchmark, coupled with a quality assurance process. Furthermore, we provide expert human translations for 15 diverse natural languages (NLs). We conclude by analyzing the multilingual code generation capabilities of state-of-the-art (SOTA) Code LLMs, offering insights into the current landscape of cross-lingual code generation.</abstract>
      <url hash="ed3acd2c">2025.naacl-long.570</url>
      <bibkey>raihan-etal-2025-mhumaneval</bibkey>
    </paper>
    <paper id="571">
      <title>What Do <fixed-case>VLM</fixed-case>s <fixed-case>NOTICE</fixed-case>? A Mechanistic Interpretability Pipeline for <fixed-case>G</fixed-case>aussian-Noise-free Text-Image Corruption and Evaluation</title>
      <author><first>Michal</first><last>Golovanevsky</last><affiliation>Brown University</affiliation></author>
      <author><first>William</first><last>Rudman</last></author>
      <author><first>Vedant</first><last>Palit</last></author>
      <author><first>Carsten</first><last>Eickhoff</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Ritambhara</first><last>Singh</last><affiliation>Brown University</affiliation></author>
      <pages>11462-11482</pages>
      <abstract>Vision-Language Models (VLMs) have gained prominence due to their success in solving complex cross-modal tasks. However, the internal mechanisms of VLMs, particularly the roles of cross-attention and self-attention in multimodal integration, are not fully understood. To address this gap, we introduce NOTICE, a Gaussian-Noise-free Text-Image Corruption and Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE introduces Semantic Image Pairs (SIP) corruption, the first visual counterpart to Symmetric Token Replacement (STR) for text. Through NOTICE, we uncover a set of “universal attention heads” in BLIP and LLaVA that consistently contribute across different tasks and modalities. In BLIP, cross-attention heads implement object detection, object suppression, and outlier suppression, whereas important self-attention heads in LLaVA only perform outlier suppression. Notably, our findings reveal that cross-attention heads perform image-grounding, while self-attention in LLaVA heads do not, highlighting key differences in how VLM architectures handle multimodal learning.</abstract>
      <url hash="7408430c">2025.naacl-long.571</url>
      <bibkey>golovanevsky-etal-2025-vlms</bibkey>
    </paper>
    <paper id="572">
      <title>Are explicit belief representations necessary? A comparison between Large Language Models and <fixed-case>B</fixed-case>ayesian probabilistic models</title>
      <author><first>Dingyi</first><last>Pan</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ben</first><last>Bergen</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>11483-11498</pages>
      <abstract>Large language models (LLMs) have exhibited certain indirect pragmatic capabilities, including interpreting indirect requests and non-literal meanings. Yet, it is unclear whether the success of LLMs on pragmatic tasks generalizes to phenomena that directly probe inferences about the beliefs of others. Indeed, LLMs’ performance on Theory of Mind (ToM) tasks is mixed. To date, the most successful computationally explicit approach to making inferences about others’ beliefs is the Rational Speech Act (RSA) framework, a Bayesian probabilistic model that encodes explicit representations of beliefs. In the present study, we ask whether LLMs outperform RSA in predicting human belief inferences, even though they do not explicitly encode belief representations. We focus specifically on projection inferences, a type of inference that directly probes belief attribution. We find that some LLMs are sensitive to factors that affect the inference process similarly to humans, yet there remains variance in human behavior not fully captured by LLMs. The RSA model, on the other hand, outperforms LLMs in capturing the variances in human data, suggesting that explicit belief representation might be necessary to construct human-like projection inferences.</abstract>
      <url hash="76fe801d">2025.naacl-long.572</url>
      <bibkey>pan-bergen-2025-explicit</bibkey>
    </paper>
    <paper id="573">
      <title>Self-Generated Critiques Boost Reward Modeling for Language Models</title>
      <author><first>Yue</first><last>Yu</last><affiliation>Meta</affiliation></author>
      <author><first>Zhengxing</first><last>Chen</last><affiliation>Facebook</affiliation></author>
      <author><first>Aston</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Liang</first><last>Tan</last><affiliation>Facebook</affiliation></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Facebook</affiliation></author>
      <author><first>Richard Yuanzhe</first><last>Pang</last><affiliation>Meta</affiliation></author>
      <author><first>Yundi</first><last>Qian</last><affiliation>Facebook</affiliation></author>
      <author><first>Xuewei</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Suchin</first><last>Gururangan</last><affiliation>Facebook and University of Washington, Seattle</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Melanie</first><last>Kambadur</last><affiliation>Facebook</affiliation></author>
      <author><first>Dhruv</first><last>Mahajan</last><affiliation>Meta AI</affiliation></author>
      <author><first>Rui</first><last>Hou</last><affiliation>Meta Inc.</affiliation></author>
      <pages>11499-11514</pages>
      <abstract>Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of the generated critiques.</abstract>
      <url hash="8bcc9036">2025.naacl-long.573</url>
      <bibkey>yu-etal-2025-self</bibkey>
    </paper>
    <paper id="574">
      <title>Characterizing the Role of Similarity in the Property Inferences of Language Models</title>
      <author><first>Juan Diego</first><last>Rodriguez</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Kanishka</first><last>Misra</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <pages>11515-11533</pages>
      <abstract>Property inheritance—a phenomenon where novel properties are projected from higher level categories (e.g., birds) to lower level ones (e.g., sparrows)—provides a unique window into how humans organize and deploy conceptual knowledge. It is debated whether this ability arises due to explicitly stored taxonomic knowledge vs. simple computations of similarity between mental representations. How are these mechanistic hypotheses manifested in contemporary language models? In this work, we investigate how LMs perform property inheritance with behavioral and causal representational analysis experiments. We find that taxonomy and categorical similarities are not mutually exclusive in LMs’ property inheritance behavior. That is, LMs are more likely to project novel properties from one category to the other when they are taxonomically related and at the same time, highly similar. Our findings provide insight into the conceptual structure of language models and may suggest new psycholinguistic experiments for human subjects.</abstract>
      <url hash="e4fc5bc2">2025.naacl-long.574</url>
      <bibkey>rodriguez-etal-2025-characterizing</bibkey>
    </paper>
    <paper id="575">
      <title><fixed-case>S</fixed-case>im<fixed-case>RAG</fixed-case>: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains</title>
      <author><first>Ran</first><last>Xu</last><affiliation>Emory University</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhenwei</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Yaochen</first><last>Xie</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Chen</first><last>Luo</last><affiliation>Amazon</affiliation></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Joyce C.</first><last>Ho</last><affiliation>Emory University</affiliation></author>
      <author><first>Carl</first><last>Yang</last><affiliation>Emory University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <pages>11534-11550</pages>
      <abstract>Retrieval-augmented generation (RAG) enhances the question answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips LLMs with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes LLMs on instruction-following, question-answering, and search-related data. Then, it prompts LLMs to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these synthetic examples, the LLMs can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets across three different domains verify the efficacy of SimRAG over baselines by 1.2%–8.6%.</abstract>
      <url hash="7c5c59f0">2025.naacl-long.575</url>
      <bibkey>xu-etal-2025-simrag</bibkey>
    </paper>
    <paper id="576">
      <title>Learning to Substitute Words with Model-based Score Ranking</title>
      <author><first>Hongye</first><last>Liu</last></author>
      <author><first>Ricardo</first><last>Henao</last><affiliation>Duke University and King Abdullah University of Science and Technology</affiliation></author>
      <pages>11551-11565</pages>
      <abstract>Smart word substitution aims to enhance sentence quality by improving word choices, however current benchmarks rely on human-labeled data , which suffers from subjectivity and lacks diversity due to limitations in the number of annotators. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based scoring (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others. Further, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution. Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions. Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA).</abstract>
      <url hash="688a1be9">2025.naacl-long.576</url>
      <bibkey>liu-henao-2025-learning</bibkey>
    </paper>
    <paper id="577">
      <title>Multilingual Reasoning via Self-training</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <pages>11566-11582</pages>
      <abstract>Although reasoning is innately language-agnostic, the multilingual capacities remains a significant challenge for large language models (LLMs). Their ability to generate structured, step-wise explanations is constantly restricted to dominant languages in pre-training data, making cross-lingual generalisation difficult and hindering broader global adoption. Recent works have introduced eclectic strategies to improve reasoning beyond English; however, these methods remain related to specific language that is not always optimal for reasoning.To improve LLMs’ multilingual reasoning abilities, we propose a modular approach that instructs the models to structure reasoning passages in a different problem space and then self-refine their capabilities to deliver step-wise reasoning passages that lead to the solution. Experiments show that our approach stably achieves significant improvements in the multilingual reasoning of various models and task, with improved reasoning consistency across languages.</abstract>
      <url hash="1c661533">2025.naacl-long.577</url>
      <bibkey>ranaldi-pucci-2025-multilingual</bibkey>
    </paper>
    <paper id="578">
      <title>x<fixed-case>LAM</fixed-case>: A Family of Large Action Models to Empower <fixed-case>AI</fixed-case> Agent Systems</title>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Tian</first><last>Lan</last><affiliation>SalesForce</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zuxin</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Thai Quoc</first><last>Hoang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Shirley</first><last>Kokane</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Weiran</first><last>Yao</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Juntao</first><last>Tan</last><affiliation>SalesForce.com and Rutgers University</affiliation></author>
      <author><first>Akshara</first><last>Prabhakar</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Haolin</first><last>Chen</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zhiwei</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Yihao</first><last>Feng</last><affiliation>Apple AI/ML</affiliation></author>
      <author><first>Tulika Manoj</first><last>Awalgaonkar</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Rithesh</first><last>R N</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zeyuan</first><last>Chen</last><affiliation>Salesforce Inc</affiliation></author>
      <author><first>Ran</first><last>Xu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Juan Carlos</first><last>Niebles</last><affiliation>Salesforce Research and Stanford University</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Huan</first><last>Wang</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>11583-11597</pages>
      <abstract>Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents’ generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, we aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks.</abstract>
      <url hash="b2ffd9c1">2025.naacl-long.578</url>
      <bibkey>zhang-etal-2025-xlam</bibkey>
    </paper>
    <paper id="579">
      <title><fixed-case>P</fixed-case>ro<fixed-case>MQA</fixed-case>: Question Answering Dataset for Multimodal Procedural Activity Understanding</title>
      <author><first>Kimihiro</first><last>Hasegawa</last></author>
      <author><first>Wiradee</first><last>Imrattanatrai</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Zhi-Qi</first><last>Cheng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Masaki</first><last>Asada</last></author>
      <author><first>Susan</first><last>Holm</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yuran</first><last>Wang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Ken</first><last>Fukuda</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Teruko</first><last>Mitamura</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>11598-11617</pages>
      <abstract>Multimodal systems have great potential to assist humans in procedural activities, where people follow instructions to achieve their goals. Despite diverse application scenarios, systems are typically evaluated on traditional classification tasks, e.g., action recognition or temporal action localization. In this paper, we present a novel evaluation dataset, ProMQA, to measure the advancement of systems in application-oriented scenarios. ProMQA consists of 401 multimodal procedural QA pairs on user recording of procedural activities, i.e., cooking, coupled with their corresponding instruction. For QA annotation, we take a cost-effective human-LLM collaborative approach, where the existing annotation is augmented with LLM-generated QA pairs that are later verified by humans. We then provide the benchmark results to set the baseline performance on ProMQA. Our experiment reveals a significant gap between human performance and that of current systems, including competitive proprietary multimodal models. We hope our dataset sheds light on new aspects of models’ multimodal understanding capabilities.</abstract>
      <url hash="34d40a43">2025.naacl-long.579</url>
      <bibkey>hasegawa-etal-2025-promqa</bibkey>
    </paper>
    <paper id="580">
      <title>Ethical Concern Identification in <fixed-case>NLP</fixed-case>: A Corpus of <fixed-case>ACL</fixed-case> <fixed-case>A</fixed-case>nthology Ethics Statements</title>
      <author><first>Antonia</first><last>Karamolegkou</last></author>
      <author><first>Sandrine Schiller</first><last>Hansen</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Ariadni</first><last>Christopoulou</last><affiliation>Verita International School</affiliation></author>
      <author><first>Filippos</first><last>Stamatiou</last><affiliation>Copenhagen University and University of Stellenbosch</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>11618-11635</pages>
      <abstract>What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (<tex-math>N=200</tex-math>), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies and guidelines pointing to gaps and actionable insights.</abstract>
      <url hash="d859dad6">2025.naacl-long.580</url>
      <bibkey>karamolegkou-etal-2025-ethical</bibkey>
    </paper>
    <paper id="581">
      <title><fixed-case>A</fixed-case>da<fixed-case>CAD</fixed-case>: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge</title>
      <author><first>Han</first><last>Wang</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>11636-11652</pages>
      <abstract>Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM’s output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that AdaCAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, AdaCAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.</abstract>
      <url hash="e2b8b5d2">2025.naacl-long.581</url>
      <bibkey>wang-etal-2025-adacad</bibkey>
    </paper>
    <paper id="582">
      <title>Are Multimodal <fixed-case>LLM</fixed-case>s Robust Against Adversarial Perturbations? <fixed-case>R</fixed-case>o<fixed-case>MM</fixed-case>ath: A Systematic Evaluation on Multimodal Math Reasoning</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Guo</first><last>Gan</last></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>NYU Shanghai</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>11653-11665</pages>
      <abstract>We introduce RoMMath, the first benchmark designed to evaluate the capabilities and robustness of multimodal large language models (MLLMs) in handling multimodal math reasoning, particularly when faced with adversarial perturbations. RoMMath consists of 4,800 expert-annotated examples, including an original set and seven adversarial sets, each targeting a specific type of perturbation at the text or vision levels. We evaluate a broad spectrum of 17 MLLMs on RoMMath and uncover a critical challenge regarding model robustness against adversarial perturbations. Through detailed error analysis by human experts, we gain a deeper understanding of the current limitations of MLLMs. Additionally, we explore various approaches to enhance the performance and robustness of MLLMs, providing insights that can guide future research efforts.</abstract>
      <url hash="ffc5e484">2025.naacl-long.582</url>
      <bibkey>zhao-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="583">
      <title><fixed-case>LBC</fixed-case>: Language-Based-Classifier for Out-Of-Variable Generalization</title>
      <author><first>Kangjun</first><last>Noh</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Baekryun</first><last>Seong</last></author>
      <author><first>Hoyoon</first><last>Byun</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Youngjun</first><last>Choi</last></author>
      <author><first>Sungjin</first><last>Song</last></author>
      <author><first>Kyungwoo</first><last>Song</last><affiliation>Yonsei University</affiliation></author>
      <pages>11666-11678</pages>
      <abstract>Large Language Models (LLMs) have great success in natural language processing tasks such as response generation. However, their use in tabular data has been limited due to their inferior performance compared to traditional machine learning models (TMLs) such as XGBoost. We find that the pre-trained knowledge of LLMs enables them to interpret new variables that appear in a test without additional training, a capability central to the concept of Out-of-Variable (OOV). From the findings, we propose a Language-Based-Classifier (LBC), a classifier that maximizes the benefits of LLMs to outperform TMLs on OOV tasks. LBC employs three key methodological strategies: 1) Categorical changes to adjust data to better fit the model’s understanding, 2) Advanced order and indicator to enhance data representation to the model, and 3) Using verbalizer to map logit scores to classes during inference to generate model predictions. These strategies, combined with the pre-trained knowledge of LBC, emphasize the model’s ability to effectively handle OOV tasks. We empirically and theoretically validate the superiority of LBC. LBC is the first study to apply an LLM-based model to OOV tasks. The source code is at https://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.</abstract>
      <url hash="38fcaaf8">2025.naacl-long.583</url>
      <bibkey>noh-etal-2025-lbc</bibkey>
    </paper>
    <paper id="584">
      <title>On the Impact of Fine-Tuning on Chain-of-Thought Reasoning</title>
      <author><first>Elita</first><last>Lobo</last></author>
      <author><first>Chirag</first><last>Agarwal</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Himabindu</first><last>Lakkaraju</last><affiliation>Harvard University</affiliation></author>
      <pages>11679-11698</pages>
      <abstract>Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs’ task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in *understanding the impact of fine-tuning on the reasoning capabilities of LLMs*. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.</abstract>
      <url hash="bc58c3cd">2025.naacl-long.584</url>
      <bibkey>lobo-etal-2025-impact</bibkey>
    </paper>
    <paper id="585">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>PO</fixed-case>: On Mutual Information Maximization for Large Language Model Alignment</title>
      <author><first>Teng</first><last>Xiao</last></author>
      <author><first>Zhen</first><last>Ge</last><affiliation>Amazon</affiliation></author>
      <author><first>Sujay</first><last>Sanghavi</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Tian</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Julian</first><last>Katz-Samuels</last><affiliation>Amazon</affiliation></author>
      <author><first>Marc</first><last>Versage</last><affiliation>Amazon</affiliation></author>
      <author><first>Qingjun</first><last>Cui</last><affiliation>Amazon</affiliation></author>
      <author><first>Trishul</first><last>Chilimbi</last><affiliation>Amazon</affiliation></author>
      <pages>11699-11711</pages>
      <abstract>We study the post-training of large language models (LLMs) with human preference data. Recently, direct preference optimization and its variants have shown considerable promise in aligning language models, eliminating the need for reward models and online sampling. Despite these benefits, these methods rely on explicit assumptions about the Bradley-Terry (BT) model, which makes them prone to overfitting and results in suboptimal performance, particularly on reasoning-heavy tasks. To address these challenges, we propose a principled preference fine-tuning algorithm called InfoPO, which effectively and efficiently aligns large language models using preference data. InfoPO eliminates the reliance on the BT model and prevents the likelihood of the chosen response from decreasing. Extensive experiments confirm that InfoPO consistently outperforms established baselines on widely used open benchmarks, particularly in reasoning tasks.</abstract>
      <url hash="210b9253">2025.naacl-long.585</url>
      <bibkey>xiao-etal-2025-infopo</bibkey>
    </paper>
    <paper id="586">
      <title>Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming</title>
      <author><first>Zhenghao</first><last>Zhou</last><affiliation>Yale University</affiliation></author>
      <author><first>Robert</first><last>Frank</last><affiliation>Yale University</affiliation></author>
      <author><first>R. Thomas</first><last>McCoy</last><affiliation>Yale University</affiliation></author>
      <pages>11712-11725</pages>
      <abstract>Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE)—a phenomenon in which an agent’s behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently). In that context, the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming with ICL and found that LLMs indeed display the IFE, with the effect being stronger in larger models. We conclude that at least in the case we studied, ICL is indeed a type of error-driven learning, supporting the hypothesis that an error signal is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of error-driven processing mechanisms in on-line processing.</abstract>
      <url hash="2ed4f05a">2025.naacl-long.586</url>
      <bibkey>zhou-etal-2025-context</bibkey>
    </paper>
    <paper id="587">
      <title>Guiding Medical Vision-Language Models with Diverse Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations</title>
      <author><first>Kangyu</first><last>Zhu</last></author>
      <author><first>Ziyuan</first><last>Qin</last></author>
      <author><first>Huahui</first><last>Yi</last></author>
      <author><first>Zekun</first><last>Jiang</last></author>
      <author><first>Qicheng</first><last>Lao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kang</first><last>Li</last></author>
      <pages>11726-11739</pages>
      <abstract>While mainstream vision-language models (VLMs) have advanced rapidly in understanding image-level information, they still lack the ability to focus on specific areas designated by humans. Rather, they typically rely on large volumes of high-quality image-text paired data to learn and generate posterior attention maps. To address this critical issue, we propose leveraging visual prompts—simple visual markers in various forms—to guide and enhance the formation of region-specific attention. Thus, we introduce **MedVP**, a pioneering framework that integrates medical entity extraction, visual prompt generation, and dataset adaptation for visual prompt-guided fine-tuning. We successfully outperform recent state-of-the-art large models across multiple medical VQA datasets. Extensive experiments and Human evaluation are conducted to analyze the impact of different visual prompt forms and how they contribute to performance improvement. The results demonstrate both the effectiveness and clinical significance of our approach.</abstract>
      <url hash="411eab3f">2025.naacl-long.587</url>
      <bibkey>zhu-etal-2025-guiding</bibkey>
    </paper>
    <paper id="588">
      <title>Analyzing and Improving Coherence of Large Language Models in Question Answering</title>
      <author><first>Ivano</first><last>Lauriola</last><affiliation>Amazon</affiliation></author>
      <author><first>Stefano</first><last>Campese</last></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon AGI</affiliation></author>
      <pages>11740-11755</pages>
      <abstract>Large language models (LLMs) have recently revolutionized natural language processing. These models, however, often suffer from instability or lack of coherence, that is the ability of the models to generate semantically equivalent outputs when receiving diverse yet semantically equivalent input variations. In this work, we analyze the behavior of multiple LLMs, including Mixtral-8x7B, Llama2-70b, Smaug-72b, and Phi-3, when dealing with multiple lexical variations of the same info-seeking questions. Our results suggest that various LLMs struggle to consistently answer diverse equivalent queries. To address this issue, we show how redundant information encoded as a prompt can increase the coherence of these models. In addition, we introduce a Retrieval-Augmented Generation (RAG) technique that supplements LLMs with the top-<tex-math>k</tex-math> most similar questions from a question retrieval engine. This knowledge-augmentation leads to 4-8 percentage point improvement in end-to-end performance in factual question answering tasks. These findings underscore the need to enhance LLM stability and coherence through semantic awareness.</abstract>
      <url hash="70ab985a">2025.naacl-long.588</url>
      <bibkey>lauriola-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="589">
      <title><fixed-case>AL</fixed-case>in<fixed-case>F</fixed-case>i<fixed-case>K</fixed-case>: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity <fixed-case>LLM</fixed-case> Data Valuation</title>
      <author><first>Yanzhou</first><last>Pan</last><affiliation>Google</affiliation></author>
      <author><first>Huawei</first><last>Lin</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Yide</first><last>Ran</last></author>
      <author><first>Jiamin</first><last>Chen</last></author>
      <author><first>Xiaodong</first><last>Yu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Weijie</first><last>Zhao</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Denghui</first><last>Zhang</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Zhaozhuo</first><last>Xu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <pages>11756-11771</pages>
      <abstract>Large Language Models (LLMs) heavily rely on high-quality training data, making data valuation crucial for optimizing model performance, especially when working within a limited budget. In this work, we aim to offer a third-party data valuation approach that benefits both data providers and model developers. We introduce a linearized future influence kernel (LinFiK), which assesses the value of individual data samples in improving LLM performance during training. We further propose ALinFiK, a learning strategy to approximate LinFiK, enabling scalable data valuation. Our comprehensive evaluations demonstrate that this approach surpasses existing baselines in effectiveness and efficiency, demonstrating significant scalability advantages as LLM parameters increase.</abstract>
      <url hash="03a6ec59">2025.naacl-long.589</url>
      <bibkey>pan-etal-2025-alinfik</bibkey>
    </paper>
    <paper id="590">
      <title><fixed-case>E</fixed-case>-Gen: Leveraging <fixed-case>E</fixed-case>-Graphs to Improve Continuous Representations of Symbolic Expressions</title>
      <author><first>Hongbo</first><last>Zheng</last></author>
      <author><first>Suyuan</first><last>Wang</last></author>
      <author><first>Neeraj</first><last>Gangwar</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Nickvash</first><last>Kani</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>11772-11788</pages>
      <abstract>Vector representations have been pivotal in advancing natural language processing (NLP), with prior research focusing on embedding techniques for mathematical expressions using mathematically equivalent formulations. While effective, these approaches are constrained by the size and diversity of training data. In this work, we address these limitations by introducing E-Gen, a novel e-graph-based dataset generation scheme that synthesizes large and diverse mathematical expression datasets, surpassing prior methods in size and operator variety. Leveraging this dataset, we train embedding models using two strategies: (1) generating mathematically equivalent expressions, and (2) contrastive learning to explicitly group equivalent expressions. We evaluate these embeddings on both in-distribution and out-of-distribution mathematical language processing tasks, comparing them against prior methods. Finally, we demonstrate that our embedding-based approach outperforms state-of-the-art large language models (LLMs) on several tasks, underscoring the necessity of optimizing embedding methods for the mathematical data modality. The source code and datasets are available at https://github.com/MLPgroup/E-Gen.</abstract>
      <url hash="ca78f137">2025.naacl-long.590</url>
      <bibkey>zheng-etal-2025-e</bibkey>
    </paper>
    <paper id="591">
      <title>Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech</title>
      <author><first>Eric</first><last>Battenberg</last><affiliation>Google</affiliation></author>
      <author><first>RJ</first><last>Skerry-Ryan</last></author>
      <author><first>Daisy</first><last>Stanton</last><affiliation>Research, Google</affiliation></author>
      <author><first>Soroosh</first><last>Mariooryad</last><affiliation>Google</affiliation></author>
      <author><first>Matt</first><last>Shannon</last><affiliation>Google</affiliation></author>
      <author><first>Julian</first><last>Salazar</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>David Teh-Hwa</first><last>Kao</last><affiliation>Google</affiliation></author>
      <pages>11789-11806</pages>
      <abstract>Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training. When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances. In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues. Our approach uses an alignment mechanism to provide cross-attention operations with relative location information. The associated alignment position is learned as a latent property of the model via backpropagation and requires no external alignment information during training. While the approach is tailored to the monotonic nature of TTS input-output alignment, it is still able to benefit from the flexible modeling power of interleaved multi-head self- and cross-attention operations. A system incorporating these improvements, which we call Very Attentive Tacotron, matches the naturalness and expressiveness of a baseline T5-based TTS system, while eliminating problems with repeated or dropped words and enabling generalization to any practical utterance length.</abstract>
      <url hash="b49750d2">2025.naacl-long.591</url>
      <bibkey>battenberg-etal-2025-robust</bibkey>
    </paper>
    <paper id="592">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>O</fixed-case>pt<fixed-case>M</fixed-case>e: Error-Aware Prompt Compression for <fixed-case>LLM</fixed-case>-based <fixed-case>MT</fixed-case> Evaluation Metrics</title>
      <author><first>Daniil</first><last>Larionov</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>11807-11820</pages>
      <abstract>Evaluating the quality of machine-generated natural language content is a challenging task in Natural Language Processing (NLP). Recently, large language models (LLMs) like GPT-4 have been employed for this purpose, but they are computationally expensive due to the extensive token usage required by complex evaluation prompts. In this paper, we propose a prompt optimization approach that uses a smaller, fine-tuned language model to compress input data for evaluation prompt, thus reducing token usage and computational cost when using larger LLMs for downstream evaluation. Our method involves a two-stage fine-tuning process: supervised fine-tuning followed by preference optimization to refine the model’s outputs based on human preferences. We focus on Machine Translation (MT) evaluation and utilize the GEMBA-MQM metric as a starting point. Our results show a <tex-math>2.37\times</tex-math> reduction in token usage without any loss in evaluation quality. This work makes state-of-the-art LLM-based metrics like GEMBA-MQM more cost-effective and efficient, enhancing their accessibility for broader use.</abstract>
      <url hash="81a1e508">2025.naacl-long.592</url>
      <bibkey>larionov-eger-2025-promptoptme</bibkey>
    </paper>
    <paper id="593">
      <title><fixed-case>A</fixed-case>uto<fixed-case>P</fixed-case>ar<fixed-case>LLM</fixed-case>: <fixed-case>GNN</fixed-case>-guided Context Generation for Zero-Shot Code Parallelization using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Quazi Ishtiaque</first><last>Mahmud</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Ali</first><last>TehraniJamsaz</last></author>
      <author><first>Hung D</first><last>Phan</last></author>
      <author><first>Le</first><last>Chen</last><affiliation>Argonne National Laboratory</affiliation></author>
      <author><first>Mihai</first><last>Capotă</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Theodore L.</first><last>Willke</last><affiliation>DataStax</affiliation></author>
      <author><first>Nesreen K.</first><last>Ahmed</last><affiliation>Intel AI Research</affiliation></author>
      <author><first>Ali</first><last>Jannesari</last><affiliation>Iowa State University</affiliation></author>
      <pages>11821-11841</pages>
      <abstract>In-Context Learning (ICL) has been shown to be a powerful technique to augment the capabilities of LLMs for a diverse range of tasks. This work proposes AutoParLLM, a novel way to generate context using guidance from graph neural networks (GNNs) to generate efficient parallel codes. We evaluate AutoParLLM on 12 applications from two well-known benchmark suites of parallel codes: NAS Parallel Benchmark and Rodinia Benchmark. Our results show that AutoParLLM improves the state-of-the-art LLMs (e.g., GPT-4) by 19.9% in NAS and 6.48% in Rodinia benchmark in terms of CodeBERTScore for the task of parallel code generation. Moreover, AutoParLLM improves the ability of the most powerful LLM to date, GPT-4, by achieving 17% (on NAS benchmark) and 16% (on Rodinia benchmark) better speedup. In addition, we propose OMPScore for evaluating the quality of the parallel code and show its effectiveness in evaluating parallel codes.</abstract>
      <url hash="8f6a5c26">2025.naacl-long.593</url>
      <bibkey>mahmud-etal-2025-autoparllm</bibkey>
    </paper>
    <paper id="594">
      <title>Causally Modeling the Linguistic and Social Factors that Predict Email Response</title>
      <author><first>Yinuo</first><last>Xu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Sushrita</first><last>Rakshit</last></author>
      <author><first>Aparna</first><last>Ananthasubramaniam</last></author>
      <author><first>Omkar</first><last>Yadav</last></author>
      <author><first>Mingqian</first><last>Zheng</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Michael</first><last>Jiang</last></author>
      <author><first>Lechen</first><last>Zhang</last></author>
      <author><first>Bowen</first><last>Yi</last></author>
      <author><first>Kenan</first><last>Alkiek</last></author>
      <author><first>Abraham</first><last>Israeli</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Bangzhao</first><last>Shu</last></author>
      <author><first>Hua</first><last>Shen</last></author>
      <author><first>Jiaxin</first><last>Pei</last><affiliation>Stanford University</affiliation></author>
      <author><first>Haotian</first><last>Zhang</last></author>
      <author><first>Miriam</first><last>Schirmer</last><affiliation>Northwestern University</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>11842-11866</pages>
      <abstract>Email is a vital conduit for human communication across businesses, organizations, and broader societal contexts. In this study, we aim to model the intents, expectations, and responsiveness in email exchanges. To this end, we release SIZZLER, a new dataset containing 1800 emails annotated with nuanced types of intents and expectations. We benchmark models ranging from feature-based logistic regression to zero-shot prompting of large language models. Leveraging the predictive model for intent, expectations, and 14 other features, we analyze 11.3M emails from GMANE to study how linguistic and social factors influence the conversational dynamics in email exchanges. Through our causal analysis, we find that the email response rates are influenced by social status, argumentation, and in certain limited contexts, the strength of social connection.</abstract>
      <url hash="7f0dd0d8">2025.naacl-long.594</url>
      <bibkey>xu-etal-2025-causally</bibkey>
    </paper>
    <paper id="595">
      <title><fixed-case>AI</fixed-case>-<fixed-case>L</fixed-case>ie<fixed-case>D</fixed-case>ar : Examine the Trade-off Between Utility and Truthfulness in <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Zhe</first><last>Su</last></author>
      <author><first>Xuhui</first><last>Zhou</last></author>
      <author><first>Sanketh</first><last>Rangreji</last></author>
      <author><first>Anubha</first><last>Kabra</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>11867-11894</pages>
      <abstract>Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), making it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents’ responses. Our experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.</abstract>
      <url hash="0e123f0f">2025.naacl-long.595</url>
      <bibkey>su-etal-2025-ai</bibkey>
    </paper>
    <paper id="596">
      <title>Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset</title>
      <author><first>Khaoula</first><last>Chehbouni</last></author>
      <author><first>Jonathan Colaço</first><last>Carr</last></author>
      <author><first>Yash</first><last>More</last><affiliation>Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <author><first>Golnoosh</first><last>Farnadi</last><affiliation>McGill University</affiliation></author>
      <pages>11895-11925</pages>
      <abstract>In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset’s content through both manual and automated evaluation; (2) experiments demonstrating the dataset’s impact on models’ safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.</abstract>
      <url hash="07f4c97b">2025.naacl-long.596</url>
      <bibkey>chehbouni-etal-2025-beyond</bibkey>
    </paper>
    <paper id="597">
      <title><fixed-case>F</fixed-case>ollow<fixed-case>IR</fixed-case>: Evaluating and Teaching Information Retrieval Models to Follow Instructions</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Benjamin</first><last>Chang</last></author>
      <author><first>Sean</first><last>MacAvaney</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>11926-11942</pages>
      <abstract>Modern Language Models (LMs) are capable of following long and complex instructions that enable a large and diverse set of user requests. While Information Retrieval (IR) models use these LMs as the backbone of their architectures, virtually none of them allow users to provide detailed instructions alongside queries, thus limiting their ability to satisfy complex information needs. In this work, we study the use of instructions in IR systems. First, we introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR repurposes detailed instructions – also known as narratives – developed for professional assessors to evaluate retrieval systems. In particular, we build our benchmark from three collections curated for shared tasks at the Text REtrieval Conference (TREC). These collections contains hundreds to thousands of labeled documents per query, making them suitable for our exploration. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form information. However, we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements after fine-tuning on our training set.</abstract>
      <url hash="61fa1d4c">2025.naacl-long.597</url>
      <bibkey>weller-etal-2025-followir</bibkey>
    </paper>
    <paper id="598">
      <title>Few-shot Personalization of <fixed-case>LLM</fixed-case>s with Mis-aligned Responses</title>
      <author><first>Jaehyung</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>11943-11974</pages>
      <abstract>As the diversity of users increases, the capability of providing personalized responses by large language models (LLMs) has become increasingly important. Existing approaches have only limited successes in LLM personalization, due to the absence of personalized learning or the reliance on shared personal data. This paper proposes a new approach for a few-shot personalization of LLMs with their mis-aligned responses (Fermi). Our key idea is to learn a set of personalized prompts for each user by progressively improving the prompts using LLMs, based on user profile (e.g., demographic information) and a few examples of previous opinions. During an iterative process of prompt improvement, we incorporate the contexts of mis-aligned responses by LLMs, which are especially crucial for the effective personalization of LLMs. In addition, we develop an effective inference method to further leverage the context of the test query and the personalized prompts. Our experimental results demonstrate that Fermi significantly improves performance across various benchmarks, compared to best-performing baselines.</abstract>
      <url hash="febdc807">2025.naacl-long.598</url>
      <bibkey>kim-yang-2025-shot</bibkey>
    </paper>
    <paper id="599">
      <title>Prompting with Phonemes: Enhancing <fixed-case>LLM</fixed-case>s’ Multilinguality for Non-<fixed-case>L</fixed-case>atin Script Languages</title>
      <author><first>Hoang H</first><last>Nguyen</last></author>
      <author><first>Khyati</first><last>Mahajan</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Julian</first><last>Salazar</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Masoud</first><last>Hashemi</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Rishabh</first><last>Maheshwary</last><affiliation>ServiceNow</affiliation></author>
      <pages>11975-11994</pages>
      <abstract>Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.</abstract>
      <url hash="38ecf458">2025.naacl-long.599</url>
      <bibkey>nguyen-etal-2025-prompting</bibkey>
    </paper>
    <paper id="600">
      <title><fixed-case>SHADES</fixed-case>: Towards a Multilingual Assessment of Stereotypes in Large Language Models</title>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Ioana</first><last>Baldini</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Miruna</first><last>Clinciu</last></author>
      <author><first>Jordan</first><last>Clive</last><affiliation>Chattermill</affiliation></author>
      <author><first>Pieter</first><last>Delobelle</last></author>
      <author><first>Manan</first><last>Dey</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Sil</first><last>Hamilton</last></author>
      <author><first>Timm</first><last>Dill</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Jad</first><last>Doughman</last></author>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Avijit</first><last>Ghosh</last><affiliation>Hugging Face and University of Connecticut</affiliation></author>
      <author><first>Jessica Zosa</first><last>Forde</last><affiliation>Brown University</affiliation></author>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Lucie-Aimée</first><last>Kaffee</last><affiliation>Hugging Face</affiliation></author>
      <author><first>Tanmay</first><last>Laud</last><affiliation>Hippocratic AI</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Roberto L</first><last>Lopez-Davila</last></author>
      <author><first>Maraim</first><last>Masoud</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Anaelia</first><last>Ovalle</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Giada</first><last>Pistilli</last><affiliation>Sorbonne University</affiliation></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <author><first>Beatrice</first><last>Savoldi</last></author>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Columbia University, Grammarly and International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Jeremy</first><last>Qin</last><affiliation>Université de Montréal</affiliation></author>
      <author><first>Esther</first><last>Ploeger</last></author>
      <author><first>Arjun</first><last>Subramonian</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kaustubh</first><last>Dhole</last><affiliation>Emory University</affiliation></author>
      <author><first>Kaiser</first><last>Sun</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Amirbek</first><last>Djanibekov</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Kayo</first><last>Yin</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Emilio Villa</first><last>Cueva</last></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Jerry</first><last>Huang</last><affiliation>The University of Tokyo and Université de Montréal &amp; Mila - Quebec AI Institute</affiliation></author>
      <author><first>Xudong</first><last>Shen</last></author>
      <author><first>Jay</first><last>Gala</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Hamdan</first><last>Al-Ali</last></author>
      <author><first/><last>Tair Djanibekov</last></author>
      <author><first>Nurdaulet</first><last>Mukhituly</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shangrui</first><last>Nie</last></author>
      <author><first>Shanya</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Karolina</first><last>Stanczak</last><affiliation>Mila - Quebec Artificial Intelligence Institute and McGill University, McGill University</affiliation></author>
      <author><first>Eliza</first><last>Szczechla</last><affiliation>Scott Tiger</affiliation></author>
      <author><first>Tiago</first><last>Timponi Torrent</last><affiliation>Federal University of Juiz de Fora</affiliation></author>
      <author><first>Deepak</first><last>Tunuguntla</last><affiliation>Saxion Universities</affiliation></author>
      <author><first>Marcelo</first><last>Viridiano</last></author>
      <author><first>Oskar</first><last>Van Der Wal</last></author>
      <author><first>Adina</first><last>Yakefu</last></author>
      <author><first>Aurélie</first><last>Névéol</last><affiliation>LISN-CNRS / Université Paris Saclay</affiliation></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Sydney</first><last>Zink</last><affiliation>KBR</affiliation></author>
      <author><first>Zeerak</first><last>Talat</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>11995-12041</pages>
      <abstract>Large Language Models (LLMs) reproduce and exacerbate the social biases present in their training data, and resources to quantify this issue are limited. While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English, lagging the rapid advancement of LLMs in multilingual settings. In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs. The dataset includes stereotypes from 20 regions around the world and 16 languages, spanning multiple identity categories subject to discrimination worldwide. We demonstrate its utility in a series of exploratory evaluations for both “base” and “instruction-tuned” language models. Our results suggest that stereotypes are consistently reflected across models and languages, with some languages and models indicating much stronger stereotype biases than others.</abstract>
      <url hash="da5edc9d">2025.naacl-long.600</url>
      <bibkey>mitchell-etal-2025-shades</bibkey>
    </paper>
    <paper id="601">
      <title>Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion</title>
      <author><first>Jacob K</first><last>Christopher</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Brian R.</first><last>Bartoldson</last><affiliation>Lawrence Livermore National Labs</affiliation></author>
      <author><first>Tal</first><last>Ben-Nun</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Michael</first><last>Cardei</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Ferdinando</first><last>Fioretto</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>12042-12059</pages>
      <abstract>Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speedups to the inference process. Our proposed approach, *Speculative Diffusion Decoding (SpecDiff)*, is validated on standard language generation benchmarks and empirically demonstrated to provide up to 7.2x speedups over standard generation processes and up to 1.75x speedups over existing speculative decoding approaches.</abstract>
      <url hash="0bce032a">2025.naacl-long.601</url>
      <bibkey>christopher-etal-2025-speculative</bibkey>
    </paper>
    <paper id="602">
      <title>Bayelemabaga: Creating Resources for <fixed-case>B</fixed-case>ambara <fixed-case>NLP</fixed-case></title>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Kevin</first><last>Assogba</last></author>
      <author><first>Christopher M</first><last>Homan</last></author>
      <author><first>M. Mustafa</first><last>Rafique</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>12060-12070</pages>
      <abstract>Data curation for under-resource languages enables the development of more accurate and culturally sensitive natural language processing models. However, the scarcity of well-structured multilingual datasets remains a challenge for advancing machine translation in these languages, especially for African languages. This paper focuses on creating high-quality parallel corpora that capture linguistic diversity to address this gap. We introduce Bayelemabaga, the most extensive curated multilingual dataset for machine translation in the Bambara language, the vehicular language of Mali. The dataset consists of 47K Bambara-French parallel sentences curated from 231 data sources, including short stories, formal documents, and religious literature, combining modern, historical, and indigenous languages. We present our data curation process and analyze its impact on neural machine translation by fine-tuning seven commonly used transformer-based language models, i.e., MBART, MT5, M2M-100, NLLB-200, Mistral-7B, Open-Llama-7B, and Meta-Llama3-8B on Bayelemabaga. Our evaluation on four Bambara-French language pair datasets (three existing datasets and the test set of Bayelemabaga) show up to <tex-math>+4.5</tex-math>, <tex-math>+11.4</tex-math>, and <tex-math>+0.27</tex-math> in gains, respectively, on BLEU, CHRF++, and AfriCOMET evaluation metrics. We also conducted machine and human evaluations of translations from studied models to compare the machine translation quality of encoder-decoder and decoder-only models. Our results indicate that encoder-decoder models remain the best, highlighting the importance of additional datasets to train decoder-only models.</abstract>
      <url hash="7d94f4ba">2025.naacl-long.602</url>
      <bibkey>tapo-etal-2025-bayelemabaga</bibkey>
    </paper>
    <paper id="603">
      <title>Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based Sentiment Analysis Evaluation</title>
      <author><first>Soyoung</first><last>Yang</last><affiliation>KAIST</affiliation></author>
      <author><first>Hojun</first><last>Cho</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jiyoung</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Sohee</first><last>Yoon</last><affiliation>Samsung</affiliation></author>
      <author><first>Edward</first><last>Choi</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Won Ik</first><last>Cho</last><affiliation>Samsung Advanced Institute of Technology</affiliation></author>
      <pages>12071-12096</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a challenging task of extracting sentiments along with their corresponding aspects and opinion terms from the text.The inherent subjectivity of span annotation makes variability in the surface forms of extracted terms, complicating the evaluation process.Traditional evaluation methods often constrain ground truths (GT) to a single term, potentially misrepresenting the accuracy of semantically valid predictions that differ in surface form.To address this limitation, we propose a novel and fully automated pipeline that expands existing evaluation sets by adding alternative valid terms for aspect and opinion. Our approach facilitates an equitable assessment of language models by accommodating multiple-answer candidates, resulting in enhanced human agreement compared to single-answer test sets (achieving up to a 10%p improvement in Kendall’s Tau score).Experimental results demonstrate that our expanded evaluation set helps uncover the capabilities of large language models (LLMs) in ABSA tasks, which is concealed by the single-answer GT sets.Consequently, our work contributes to the development of a flexible evaluation framework for ABSA by embracing diverse surface forms to span extraction tasks in a cost-effective and reproducible manner.Our code and dataset is open at https://github.com/dudrrm/zoom-in-n-out-absa.</abstract>
      <url hash="10334521">2025.naacl-long.603</url>
      <bibkey>yang-etal-2025-single</bibkey>
    </paper>
    <paper id="604">
      <title><fixed-case>DREAM</fixed-case>: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models</title>
      <author><first>Jianyu</first><last>Liu</last></author>
      <author><first>Hangyu</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ranjie</first><last>Duan</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yucheng</first><last>Wang</last></author>
      <author><first>Chenchen</first><last>Jing</last></author>
      <author><first>Xingwei</first><last>Qu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Xiao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Jihao</first><last>Gu</last></author>
      <author><first>Yangguang</first><last>Li</last><affiliation>VAST</affiliation></author>
      <author><first>Jianke</first><last>Zhu</last></author>
      <pages>12097-12118</pages>
      <abstract>Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce <b>DREAM</b> (<i>
          <b>D</b>isentangling <b>R</b>isks to <b>E</b>nhance Safety <b>A</b>lignment in <b>M</b>LLMs</i>), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17% improvement in the SIUO safe&amp;effective score compared to GPT-4V.</abstract>
      <url hash="d8c97066">2025.naacl-long.604</url>
      <bibkey>liu-etal-2025-dream</bibkey>
    </paper>
    <paper id="605">
      <title>In-Context Learning with Long-Context Models: An In-Depth Exploration</title>
      <author><first>Amanda</first><last>Bertsch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maor</first><last>Ivgi</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Emily</first><last>Xiao</last></author>
      <author><first>Uri</first><last>Alon</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Google and Tel Aviv University</affiliation></author>
      <author><first>Matthew R.</first><last>Gormley</last><affiliation>Solventum and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12119-12149</pages>
      <abstract>As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can exceed long-context ICL performance with additional data. We use the ICL setting to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples negatively impacts performance, and that the performance boosts do not arise from cumulative gain from encoding many examples together. We conclude that long-context ICL can be an effective tool, and may not require long-context attention for encoding the demonstration set at all.</abstract>
      <url hash="f766a30f">2025.naacl-long.605</url>
      <bibkey>bertsch-etal-2025-context</bibkey>
    </paper>
    <paper id="606">
      <title>Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora</title>
      <author><first>JoonHo</first><last>Lee</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>JuYoun</first><last>Son</last><affiliation>Samsung sds</affiliation></author>
      <author><first>Juree</first><last>Seok</last></author>
      <author><first>Wooseok</first><last>Jang</last></author>
      <author><first>Yeong-Dae</first><last>Kwon</last><affiliation>Samsung SDS</affiliation></author>
      <pages>12150-12169</pages>
      <abstract>Inconsistent annotations in training corpora, particularly within preference learning datasets, pose challenges in developing advanced language models. These inconsistencies often arise from variability among annotators and inherent multi-dimensional nature of the preferences. To address these issues, we introduce a self-curation method that preprocesses annotated datasets by leveraging proxy models trained directly on them. Our method enhances preference learning by automatically detecting and selecting consistent annotations. We validate the proposed approach through extensive instruction-following tasks, demonstrating performance improvements of up to 33% across various learning algorithms and proxy capabilities. This work offers a straightforward and reliable solution to address preference inconsistencies without relying on heuristics, serving as an initial step toward the development of more advanced preference learning methodologies. Code is available at https://github.com/Self-Curation/ .</abstract>
      <url hash="2fa3f97b">2025.naacl-long.606</url>
      <bibkey>lee-etal-2025-preference</bibkey>
    </paper>
    <paper id="607">
      <title><fixed-case>T</fixed-case>urtle<fixed-case>B</fixed-case>ench: A Visual Programming Benchmark in Turtle Geometry</title>
      <author><first>Sina</first><last>Rismanchian</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Yasaman</first><last>Razeghi</last></author>
      <author><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Shayan</first><last>Doroudi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12170-12188</pages>
      <abstract>Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce TurtleBench, a benchmark designed to evaluate LMMs’ capacity to interpret geometric patterns—given visual examples, textual instructions, or both—and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4V achieving only 19% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance (&lt;2%). TurtleBench highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area and stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research.</abstract>
      <url hash="886ff78c">2025.naacl-long.607</url>
      <bibkey>rismanchian-etal-2025-turtlebench</bibkey>
    </paper>
    <paper id="608">
      <title>Automatically Discovering How Misogyny is Framed on Social Media</title>
      <author><first>Rakshitha Rao</first><last>Ailneni</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Sanda M.</first><last>Harabagiu</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>12189-12208</pages>
      <abstract>Misogyny, which is widespread on social media, can be identified not only by recognizing its many forms but also by discovering how misogyny is framed. This paper considers the automatic discovery of misogyny problems and their frames through the Dis-MP&amp;F method, which enables the generation of a data-driven, rich Taxonomy of Misogyny (ToM), offering new insights in the complexity of expressions of misogyny. Furthermore, the Dis-MP&amp;F method, informed by the ToM, is capable of producing very promising results on a misogyny benchmark dataset.</abstract>
      <url hash="1b36de0b">2025.naacl-long.608</url>
      <bibkey>ailneni-harabagiu-2025-automatically</bibkey>
    </paper>
    <paper id="609">
      <title>Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation</title>
      <author><first>Mahnaz</first><last>Koupaee</last></author>
      <author><first>Jake W.</first><last>Vincent</last><affiliation>Amazon</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Han</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Raphael</first><last>Shu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jianfeng</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi</first><last>Nian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Amy Wing-mei</first><last>Wong</last><affiliation>Amazon</affiliation></author>
      <author><first>Kyu J.</first><last>Han</last><affiliation>Amazon Web Services (AWS)</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <pages>12209-12246</pages>
      <abstract>Faithfulness evaluators based on Large Language Models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries, usually leading to high false negative rate. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments here result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension ambiguity and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.</abstract>
      <url hash="1302c180">2025.naacl-long.609</url>
      <bibkey>koupaee-etal-2025-faithful</bibkey>
    </paper>
    <paper id="610">
      <title><fixed-case>R</fixed-case>e<fixed-case>IFE</fixed-case>: Re-evaluating Instruction-Following Evaluation</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Kejian</first><last>Shi</last></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>PeiFeng</first><last>Wang</last><affiliation>Salesforce AI</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>12247-12287</pages>
      <abstract>The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our evaluation reveals key findings: (1) Base LLM performance ranking remains largely consistent across evaluation protocols, with less capable LLMs showing greater improvement from protocol enhancements; (2) Robust evaluation of evaluation protocols requires many base LLMs with varying capability levels, as protocol effectiveness depends on the base LLM used; (3) Evaluation results on different datasets are not always consistent, so a rigorous evaluation requires multiple datasets with distinctive features. We release our meta-evaluation suite ReIFE, which provides the codebase and evaluation result collection for over 500 LLM-evaluators, laying groundwork for future research in instruction-following evaluation.</abstract>
      <url hash="6a3841e8">2025.naacl-long.610</url>
      <bibkey>liu-etal-2025-reife</bibkey>
    </paper>
    <paper id="611">
      <title>Language Models Predict Empathy Gaps Between Social In-groups and Out-groups</title>
      <author><first>Yu</first><last>Hou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Hal Daumé</first><last>Iii</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>12288-12304</pages>
      <abstract>Studies of human psychology have demonstrated that people are more motivated to extend empathy to in-group members than out-group members (Cikara et al., 2011). In this study, we investigate how this aspect of intergroup relations in humans is replicated by LLMs in an emotion intensity prediction task. In this task, the LLM is given a short description of an experience a person had that caused them to feel a particular emotion; the LLM is then prompted to predict the intensity of the emotion the person experienced on a numerical scale. By manipulating the group identities assigned to the LLM’s persona (the “perceiver”) and the person in the narrative (the “experiencer”), we measure how predicted emotion intensities differ between in-group and out-group settings. We observe that LLMs assign higher emotion intensity scores to in-group members than out-group members. This pattern holds across all three types of social groupings we tested: race/ethnicity, nationality, and religion. We perform an in-depth analysis on Llama-3.1-8B, the model which exhibited strongest intergroup bias among those tested.</abstract>
      <url hash="5c617adb">2025.naacl-long.611</url>
      <bibkey>hou-etal-2025-language</bibkey>
    </paper>
    <paper id="612">
      <title><fixed-case>HARP</fixed-case>: Hesitation-Aware Reframing in Transformer Inference Pass</title>
      <author><first>Romain</first><last>Storaï</last><affiliation>Seoul National University and Ecole Nationale Supérieure des Mines d’Alès</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>12305-12319</pages>
      <abstract>This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to “off-the-shelf” Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP provides insights into the potential of adaptive computation for enhancing the performance of Transformer-based language models.</abstract>
      <url hash="49ff6f8f">2025.naacl-long.612</url>
      <bibkey>storai-hwang-2025-harp</bibkey>
    </paper>
    <paper id="613">
      <title><fixed-case>JAWAHER</fixed-case>: A Multidialectal Dataset of <fixed-case>A</fixed-case>rabic Proverbs for <fixed-case>LLM</fixed-case> Benchmarking</title>
      <author><first>Samar Mohamed</first><last>Magdy</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sang Yun</first><last>Kwon</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>Safaa Taher</first><last>Abdelfadil</last></author>
      <author><first>Shady</first><last>Shehata</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>12320-12341</pages>
      <abstract>Recent advancements in instruction fine-tuning, alignment methods such as reinforcement learning from human feedback (RLHF), and optimization techniques like direct preference optimization (DPO), have significantly enhanced the adaptability of large language models (LLMs) to user preferences. However, despite these innovations, many LLMs continue to exhibit biases toward Western, Anglo-centric, or American cultures, with performance on English data consistently surpassing that of other languages. This reveals a persistent cultural gap in LLMs, which complicates their ability to accurately process culturally rich and diverse figurative language, such as proverbs. To address this, we introduce *Jawaher*, a benchmark designed to assess LLMs’ capacity to comprehend and interpret Arabic proverbs. *Jawaher* includes proverbs from various Arabic dialects, along with idiomatic translations and explanations. Through extensive evaluations of both open- and closed-source models, we find that while LLMs can generate idiomatically accurate translations, they struggle with producing culturally nuanced and contextually relevant explanations. These findings highlight the need for ongoing model refinement and dataset expansion to bridge the cultural gap in figurative language processing.</abstract>
      <url hash="13cda0ee">2025.naacl-long.613</url>
      <bibkey>magdy-etal-2025-jawaher</bibkey>
    </paper>
    <paper id="614">
      <title><fixed-case>E</fixed-case>moji<fixed-case>P</fixed-case>rompt: Generative Prompt Obfuscation for Privacy-Preserving Communication with Cloud-based <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sam</first><last>Lin</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Mingyu</first><last>Jin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Lizhou</first><last>Fan</last><affiliation>Brigham and Women’s Hospital, Harvard University</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>12342-12361</pages>
      <abstract>Cloud-based Large Language Models (LLMs) such as ChatGPT have become increasingly integral to daily operations. Nevertheless, they also introduce privacy concerns: firstly, numerous studies underscore the risks to user privacy posed by jailbreaking cloud-based LLMs; secondly, the LLM service providers have access to all user data, which deters individuals from confidently utilizing such services. To address such concerns, we propose a simple yet effective paradigm, **EmojiPrompt**, to protect user privacy. At its core, EmojiPrompt performs generative transformation, obfuscating private data within prompts with linguistic and non-linguistic elements before submitting them to cloud-based LLMs. We evaluate EmojiPrompt’s performance across 8 datasets from various domains. We also propose simulated inference attacks to assess EmojiPrompt’s ability to preserve user privacy. The results demonstrate that EmojiPrompt effectively obfuscates user private data, while largely maintaining, or even enhancing, performances compared to the unobfuscated version. Furthermore, EmojiPrompt’s atomic-level obfuscation allows it to function exclusively with cloud-based LLMs. For source code, please refer to: https://github.com/agiresearch/EmojiCrypt.</abstract>
      <url hash="1c8c4f5e">2025.naacl-long.614</url>
      <bibkey>lin-etal-2025-emojiprompt</bibkey>
    </paper>
    <paper id="615">
      <title><fixed-case>MICE</fixed-case> for <fixed-case>CAT</fixed-case>s: Model-Internal Confidence Estimation for Calibrating Agents with Tools</title>
      <author><first>Nishant</first><last>Subramani</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Justin</first><last>Svegliato</last><affiliation>University of California, Berkeley and Microsoft</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Sam</first><last>Thomson</last><affiliation>Microsoft</affiliation></author>
      <pages>12362-12375</pages>
      <abstract>Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logit lens and then computes similarity scores between each layer’s generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.</abstract>
      <url hash="64b081c3">2025.naacl-long.615</url>
      <bibkey>subramani-etal-2025-mice</bibkey>
    </paper>
    <paper id="616">
      <title><fixed-case>PAT</fixed-case>: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio Classification</title>
      <author><first>Ashish</first><last>Seth</last></author>
      <author><first>Ramaneswaran</first><last>Selvakumar</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>12376-12394</pages>
      <abstract>Audio-Language Models (ALMs) have demonstrated remarkable performance in zero-shot audio classification. In this paper, we introduce PAT (Parameter-free Audio-Text aligner), a simple and training-free method aimed at boosting zero-shot audio classification performance of CLAP-like ALMs. To achieve this, we propose to improve the cross-modal interaction between audio and language modalities by enhancing the representations for both modalities using mutual feedback. Precisely, to enhance textual representations, we propose a prompt ensemble algorithm that automatically selects and combines the most relevant prompts from a datastore with a large pool of handcrafted prompts and weighs them according to their relevance to the audio. On the other hand, to enhance audio representations, we reweigh the frame-level audio features based on the enhanced textual information. Our proposed method does not require any additional modules or parameters and can be used with any existing CLAP-like ALM to improve zero-shot audio classification performance. We experiment across 18 diverse benchmark datasets and 6 ALMs and show that the PAT outperforms vanilla zero-shot evaluation with significant margins of 0.42%-27.0%. Additionally, we demonstrate that PAT maintains robust performance even when input audio is degraded by varying levels of noise. We make our code publicly available.</abstract>
      <url hash="9a159629">2025.naacl-long.616</url>
      <bibkey>seth-etal-2025-pat</bibkey>
    </paper>
    <paper id="617">
      <title>Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks</title>
      <author><first>Justin</first><last>Zhao</last><affiliation>Columbia University</affiliation></author>
      <author><first>Flor Miriam</first><last>Plaza-del-Arco</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Amanda Cercas</first><last>Curry</last><affiliation>CENTAI Institute</affiliation></author>
      <pages>12395-12450</pages>
      <abstract>As Large Language Models (LLMs) continue to evolve, evaluating them remains a persistent challenge. Many recent evaluations use LLMs as judges to score outputs from other LLMs, often relying on a single large model like GPT-4o. However, using a single LLM judge is prone to intra-model bias, and many tasks – such as those related to emotional intelligence, creative writing, and persuasiveness – may be too subjective for a single model to judge fairly. We introduce the Language Model Council (LMC), where a group of LLMs collaborate to create tests, respond to them, and evaluate each other’s responses to produce a ranking in a democratic fashion. Unlike previous approaches that focus on reducing cost or bias by using a panel of smaller models, our work examines the benefits and nuances of a fully inclusive LLM evaluation system. In a detailed case study on emotional intelligence, we deploy a council of 20 recent LLMs to rank each other on open-ended responses to interpersonal conflicts. Our results show that the LMC produces rankings that are more separable and more robust, and through a user study, we show that they are more consistent with human evaluations than any individual LLM judge. Using all LLMs for judging can be costly, however, so we use Monte Carlo simulations and hand-curated sub-councils to study hypothetical council compositions and discuss the value of the incremental LLM judge.</abstract>
      <url hash="48591652">2025.naacl-long.617</url>
      <bibkey>zhao-etal-2025-language</bibkey>
    </paper>
    <paper id="618">
      <title><fixed-case>SCIUR</fixed-case>us: Shared Circuits for Interpretable Uncertainty Representations in Language Models</title>
      <author><first>Carter</first><last>Teplica</last></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tim G. J.</first><last>Rudner</last><affiliation>New York University</affiliation></author>
      <pages>12451-12469</pages>
      <abstract>We investigate the mechanistic sources of uncertainty in large language models (LLMs), an area with important implications for language model reliability and trustworthiness. To do so, we conduct a series of experiments designed to identify whether the factuality of generated responses and a model’s uncertainty originate in separate or shared circuits in the model architecture. We approach this question by adapting the well-established mechanistic interpretability techniques of causal tracing and zero-ablation to study the effect of different circuits on LLM generations. Our experiments on eight different models and five datasets, representing tasks predominantly requiring factual recall, provide strong evidence that a model’s uncertainty is produced in the same parts of the network that are responsible for the factuality of generated responses.</abstract>
      <url hash="cfe2e5ca">2025.naacl-long.618</url>
      <bibkey>teplica-etal-2025-sciurus</bibkey>
    </paper>
    <paper id="619">
      <title><fixed-case>P</fixed-case>ro<fixed-case>SE</fixed-case>: Diffusion Priors for Speech Enhancement</title>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Utkarsh</first><last>Tyagi</last></author>
      <author><first>Anton Jeran</first><last>Ratnarajah</last></author>
      <author><first>Chandra Kiran Reddy</first><last>Evuru</last></author>
      <author><first>Ramani</first><last>Duraiswami</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>12470-12483</pages>
      <abstract>Speech enhancement (SE) is the fundamental task of enhancing the clarity and quality of speech in the presence of non-stationary additive noise. While deterministic deep learning models have been commonly employed for SE, recent research indicates that generative models, such as denoising diffusion probabilistic models (DDPMs), have shown promise. However, different from speech generation, SE has a strong constraint to generate results in accordance with the underlying ground-truth signal. Additionally, for a wide variety of applications, SE systems need to be employed in real-time, and traditional diffusion models (DMs) requiring many iterations of a large model during inference are inefficient. To address these issues, we propose ProSE (diffusion-based Priors for SE), a novel methodology based on an alternative framework for applying diffusion models to SE. Specifically, we first apply DDPMs to generate priors in a latent space due to their powerful distribution mapping capabilities. The priors are then integrated into a transformer-based regression model for SE. The priors guide the regression model in the enhancement process. Since the diffusion process is applied to a compact latent space, the diffusion model takes fewer iterations than the traditional DM to obtain accurate estimations. Additionally, using a regression model for SE avoids the distortion issue caused by misaligned details generated by DMs. Comprehensive experiments show that ProSE achieves state-of-the-art performance on synthetic and real-world datasets using various metrics while consuming less computational costs.</abstract>
      <url hash="0a616353">2025.naacl-long.619</url>
      <bibkey>kumar-etal-2025-prose</bibkey>
    </paper>
    <paper id="620">
      <title>Mastering the Craft of Data Synthesis for <fixed-case>C</fixed-case>ode<fixed-case>LLM</fixed-case>s</title>
      <author><first>Meng</first><last>Chen</last></author>
      <author><first>Philip</first><last>Arthur</last><affiliation>Oracle</affiliation></author>
      <author><first>Qianyu</first><last>Feng</last><affiliation>Oracle</affiliation></author>
      <author><first>Cong Duy Vu</first><last>Hoang</last><affiliation>Oracle Corporation</affiliation></author>
      <author><first>Yu-Heng</first><last>Hong</last></author>
      <author><first>Mahdi Kazemi</first><last>Moghaddam</last><affiliation>Oracle</affiliation></author>
      <author><first>Omid</first><last>Nezami</last><affiliation>Oracle</affiliation></author>
      <author><first>Duc Thien</first><last>Nguyen</last></author>
      <author><first>Gioacchino</first><last>Tangari</last><affiliation>Oracle</affiliation></author>
      <author><first>Duy</first><last>Vu</last><affiliation>Oracle Australia</affiliation></author>
      <author><first>Thanh</first><last>Vu</last><affiliation>Oracle</affiliation></author>
      <author><first>Mark</first><last>Johnson</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Krishnaram</first><last>Kenthapadi</last><affiliation>Oracle Health AI</affiliation></author>
      <author><first>Don</first><last>Dharmasiri</last><affiliation>Oracle</affiliation></author>
      <author><first>Long</first><last>Duong</last><affiliation>Oracle</affiliation></author>
      <author><first>Yuan-Fang</first><last>Li</last><affiliation>Monash University and Oracle</affiliation></author>
      <pages>12484-12500</pages>
      <abstract>Large language models (LLMs) have shown impressive performance in <i>code</i> understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.</abstract>
      <url hash="8e30534e">2025.naacl-long.620</url>
      <bibkey>chen-etal-2025-mastering</bibkey>
    </paper>
    <paper id="621">
      <title><fixed-case>P</fixed-case>ara<fixed-case>ICL</fixed-case>: Towards Parallel In-Context Learning</title>
      <author><first>Xingxuan</first><last>Li</last></author>
      <author><first>Xuan-Phi</first><last>Nguyen</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>12501-12511</pages>
      <abstract>Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.</abstract>
      <url hash="775979e0">2025.naacl-long.621</url>
      <bibkey>li-etal-2025-paraicl</bibkey>
    </paper>
    <paper id="622">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>E</fixed-case>val: Towards Better Causal Reasoning in Language Models</title>
      <author><first>Longxuan</first><last>Yu</last></author>
      <author><first>Delin</first><last>Chen</last></author>
      <author><first>Siheng</first><last>Xiong</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Qingyang</first><last>Wu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Zhikai</first><last>Chen</last></author>
      <author><first>Xiaoze</first><last>Liu</last></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <pages>12512-12540</pages>
      <abstract>Causal reasoning (CR) is a crucial aspect of intelligence, essential for problem-solving, decision-making, and understanding the world. While language models (LMs) can generate rationales for their outputs, their ability to reliably perform causal reasoning remains uncertain, often falling short in tasks requiring a deep understanding of causality. In this paper, we introduce CausalEval, a comprehensive review of research aimed at enhancing LMs for causal reasoning, coupled with an empirical evaluation of current models and methods. We categorize existing methods based on the role of LMs: either as reasoning engines or as helpers providing knowledge or data to traditional CR methods, followed by a detailed discussion of methodologies in each category. We then assess the performance of current LMs and various enhancement methods on a range of causal reasoning tasks, providing key findings and in-depth analysis. Finally, we present insights from current studies and highlight promising directions for future research. We aim for this work to serve as a comprehensive resource, fostering further advancements in causal reasoning with LMs.</abstract>
      <url hash="ec4202ac">2025.naacl-long.622</url>
      <bibkey>yu-etal-2025-causaleval</bibkey>
    </paper>
    <paper id="623">
      <title>Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense</title>
      <author><first>Yang</first><last>Ouyang</last></author>
      <author><first>Hengrui</first><last>Gu</last><affiliation>Jilin University</affiliation></author>
      <author><first>Shuhang</first><last>Lin</last><affiliation>, Rutgers University</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Jie</first><last>Peng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Meijun</first><last>Gao</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Kaixiong</first><last>Zhou</last><affiliation>North Carolina State University</affiliation></author>
      <pages>12541-12554</pages>
      <abstract>As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then “unlearn” these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model’s responses to safe queries intact.We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak attacks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods. Our code is publicly available at: https://github.com/oyy2000/LayerAdvPatcher</abstract>
      <url hash="ec0f42cb">2025.naacl-long.623</url>
      <bibkey>ouyang-etal-2025-layer</bibkey>
    </paper>
    <paper id="624">
      <title><fixed-case>D</fixed-case>e<fixed-case>CAP</fixed-case>: Context-Adaptive Prompt Generation for Debiasing Zero-shot Question Answering in Large Language Models</title>
      <author><first>Suyoung</first><last>Bae</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>12555-12574</pages>
      <abstract>While Large Language Models (LLMs) excel in zero-shot Question Answering (QA), they tend to expose biases in their internal knowledge when faced with socially sensitive questions, leading to a degradation in performance. Existing zero-shot methods are efficient but failto consider context and prevent bias propagation in the answers. To address this, we propose *DeCAP*, a method for debiasing LLMs usingContext-Adaptive Prompt Generation. *DeCAP* leverages a *Question Ambiguity Detection* to take appropriate debiasing actions based on the context and a *Neutral Answer Guidance Generation* to suppress the LLMs make objective judgments about the context, minimizing thepropagation of bias from their internal knowledge. Our various experiments across eight LLMs show that *DeCAP* achieves state-of-the-art zero-shot debiased QA performance. This demonstrates *DeCAP*’s efficacy in enhancing the fairness and accuracy of LLMs in diverseQA settings.</abstract>
      <url hash="24bb03b2">2025.naacl-long.624</url>
      <bibkey>bae-etal-2025-decap</bibkey>
    </paper>
    <paper id="625">
      <title>Reward-Guided Tree Search for Inference Time Alignment of Large Language Models</title>
      <author><first>Chia-Yu</first><last>Hung</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Navonil</first><last>Majumder</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Ambuj</first><last>Mehrish</last></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>12575-12593</pages>
      <abstract>Inference-time computation methods enhance the performance of Large Language Models (LLMs) by leveraging additional computational resources to achieve superior results. Common techniques, such as Best-of-N sampling, Majority Voting, and variants of tree-search algorithm have proven to be effective in boosting the performance of LLMs. These approaches strategically trade increased computational resource for improved model responses. In this work, we proposed DARWIN, an inference-time alignment method that leverage the guidance of a reward model to achieve alignment through reward-guided tree search. Empirical evidences indicates that our method outperform other inference-time alignment methods such as Best-of-N and ARGS on two widely accepted alignment benchmarks AlpacaEval 2 and MT-Bench. Furthermore, we show that our inference-time approach achieves performance comparable to preference-tuned models on both benchmarks, highlighting the effectiveness of trading inference-time compute for enhanced performance during inference.</abstract>
      <url hash="aec6a593">2025.naacl-long.625</url>
      <bibkey>hung-etal-2025-reward</bibkey>
    </paper>
    <paper id="626">
      <title>Typographic Attacks in a Multi-Image Setting</title>
      <author><first>Xiaomeng</first><last>Wang</last></author>
      <author><first>Zhengyu</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Martha</first><last>Larson</last><affiliation>Radboud University</affiliation></author>
      <pages>12594-12604</pages>
      <abstract>Large Vision-Language Models (LVLMs) are susceptible to typographic attacks, which are misclassifications caused by an attack text that is added to an image. In this paper, we introduce a multi-image setting for studying typographic attacks, broadening the current emphasis of the literature on attacking individual images. Specifically, our focus is on attacking image sets without repeating the attack query. Such non-repeating attacks are stealthier, as they are more likely to evade a gatekeeper than attacks that repeat the same attack text. We introduce two attack strategies for the multi-image setting, leveraging the difficulty of the target image, the strength of the attack text, and text-image similarity. Our text-image similarity approach improves attack success rates by 21% over random, non-specific methods on the CLIP model using ImageNet while maintaining stealth in a multi-image scenario. An additional experiment demonstrates transferability, i.e., text-image similarity calculated using CLIP transfers when attacking InstructBLIP.</abstract>
      <url hash="42720181">2025.naacl-long.626</url>
      <bibkey>wang-etal-2025-typographic</bibkey>
    </paper>
    <paper id="627">
      <title>Tonguescape: Exploring Language Models Understanding of Vowel Articulation</title>
      <author><first>Haruki</first><last>Sakajo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>12605-12619</pages>
      <abstract>Vowels are primarily characterized by tongue position. Humans have discovered these features of vowel articulation through their own experience and explicit objective observation such as using MRI. With this knowledge and our experience, we can explain and understand the relationship between tongue positions and vowels, and this knowledge is helpful for language learners to learn pronunciation. Since language models (LMs) are trained on a large amount of data that includes linguistic and medical fields, our preliminary studies indicate that an LM is able to explain the pronunciation mechanisms of vowels. However, it is unclear whether multi-modal LMs, such as vision LMs, align textual information with visual information. One question arises: do LMs associate real tongue positions with vowel articulation? In this study, we created video and image datasets from the existing real-time MRI dataset and investigated whether LMs can understand vowel articulation based on tongue positions using vision-based information. Our findings suggest that LMs exhibit potential for understanding vowels and tongue positions when reference examples are provided while they have difficulties without them. Our code for dataset building is available on GitHub.</abstract>
      <url hash="7ac09571">2025.naacl-long.627</url>
      <bibkey>sakajo-etal-2025-tonguescape</bibkey>
    </paper>
    <paper id="628">
      <title><fixed-case>C</fixed-case>o<fixed-case>RAC</fixed-case>: Integrating Selective <fixed-case>API</fixed-case> Document Retrieval with Question Semantic Intent for Code Question Answering</title>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>CheolWon</first><last>Na</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>12620-12635</pages>
      <abstract>Automatic code question answering aims to generate precise answers to questions about code by analyzing code snippets. To provide an appropriate answer, it is necessary to accurately understand the relevant part of the code and correctly interpret the intent of the question. However, in real-world scenarios, the questioner often provides only a portion of the code along with the question, making it challenging to find an answer. The responder should be capable of providing a suitable answer using such limited information. We propose a knowledge-based framework, CoRAC, an automatic code question responder that enhances understanding through selective API document retrieval and question semantic intent clustering. We evaluate our method on three real-world benchmark datasets and demonstrate its effectiveness through various experiments. We also show that our method can generate high-quality answers compared to large language models, such as ChatGPT.</abstract>
      <url hash="84a2a44f">2025.naacl-long.628</url>
      <bibkey>choi-etal-2025-corac</bibkey>
    </paper>
    <paper id="629">
      <title>Pipeline Analysis for Developing Instruct <fixed-case>LLM</fixed-case>s in Low-Resource Languages: A Case Study on <fixed-case>B</fixed-case>asque</title>
      <author><first>Ander</first><last>Corral</last><affiliation>Orai NLP Technologies</affiliation></author>
      <author><first>Ixak Sarasua</first><last>Antero</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <pages>12636-12655</pages>
      <abstract>Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points. Moreover, instruction tuning and human preference alignment using automatically translated datasets proved highly effective, resulting in a 24-point improvement in instruction-following performance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct, establish a new state-of-the-art for Basque in the sub-10B parameter category.</abstract>
      <url hash="2a68ad23">2025.naacl-long.629</url>
      <bibkey>corral-etal-2025-pipeline</bibkey>
    </paper>
    <paper id="630">
      <title>How to Make <fixed-case>LLM</fixed-case>s Forget: On Reversing In-Context Knowledge Edits</title>
      <author><first>Paul</first><last>Youssef</last></author>
      <author><first>Zhixue</first><last>Zhao</last><affiliation>University of Sheffield, University of Sheffield</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <pages>12656-12669</pages>
      <abstract>In-context knowledge editing (IKE) enables efficient modification of large language model (LLM) outputs without parameter changes and at zero-cost. However, it can be misused to manipulate responses opaquely, e.g., insert misinformation or offensive content. Such malicious interventions could be incorporated into high-level wrapped APIs where the final input prompt is not shown to end-users. To address this issue, we investigate the detection and reversal of IKE-edits. First, we demonstrate that IKE-edits can be detected with high accuracy (F1 &gt; 80%) using only the top-10 output probabilities of the next token, even in a black-box setting, e.g. proprietary LLMs with limited output information. Further, we introduce the novel task of reversing IKE-edits using specially tuned reversal tokens. We explore using both continuous and discrete reversal tokens, achieving over 80% accuracy in recovering original, unedited outputs across multiple LLMs. Our continuous reversal tokens prove particularly effective, with minimal impact on unedited prompts. Through analysis of output distributions, attention patterns, and token rankings, we provide insights into IKE’s effects on LLMs and how reversal tokens mitigate them. This work represents a significant step towards enhancing LLM resilience against potential misuse of in-context editing, improving their transparency and trustworthiness.</abstract>
      <url hash="b48efdc6">2025.naacl-long.630</url>
      <bibkey>youssef-etal-2025-make</bibkey>
    </paper>
    <paper id="631">
      <title><fixed-case>P</fixed-case>er<fixed-case>C</fixed-case>ul: A Story-Driven Cultural Evaluation of <fixed-case>LLM</fixed-case>s in <fixed-case>P</fixed-case>ersian</title>
      <author><first>Erfan</first><last>Moosavi Monazzah</last></author>
      <author><first>Vahid</first><last>Rahimzadeh</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Azadeh</first><last>Shakery</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last><affiliation>Cardiff University and TeIAS</affiliation></author>
      <pages>12670-12687</pages>
      <abstract>Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios.Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the use of translation as a shortcut. We evaluate several state-of-the-art multilingual and Persian-specific LLMs, establishing a foundation for future research in cross-cultural NLP evaluation. Our experiments demonstrate a 11.3% gap between best closed source model and layperson baseline while the gap increases to 21.3% by using the best open-weight model. You can access the dataset from here:https://huggingface.co/datasets/teias-ai/percul</abstract>
      <url hash="ce46611f">2025.naacl-long.631</url>
      <bibkey>moosavi-monazzah-etal-2025-percul</bibkey>
    </paper>
    <paper id="632">
      <title>Towards Sustainable <fixed-case>NLP</fixed-case>: Insights from Benchmarking Inference Energy in Large Language Models</title>
      <author><first>Soham</first><last>Poddar</last></author>
      <author><first>Paramita</first><last>Koley</last><affiliation>Indian Institute of Technology Kharagpur, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Janardan</first><last>Misra</last></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>12688-12704</pages>
      <abstract>Large language models (LLMs) are increasingly recognized for their exceptional generative capabilities and versatility across various tasks. However, the high inference costs associated with these models have not received adequate attention, particularly when compared to the focus on training costs in existing research. In response to this gap, our study conducts a comprehensive benchmarking of LLM inference energy across a wide range of NLP tasks, where we analyze the impact of different models, tasks, prompts, and system-related factors on inference energy. Specifically, our experiments reveal several interesting insights, including strong correlation of inference energy with output token length and response time. Also, we find that quantization and optimal batch sizes, along with targeted prompt phrases, can significantly reduce energy usage. This study is the first to thoroughly benchmark LLM inference across such a diverse range of aspects, providing insights and offering several recommendations for improving energy efficiency in model deployment.</abstract>
      <url hash="1d94559d">2025.naacl-long.632</url>
      <bibkey>poddar-etal-2025-towards</bibkey>
    </paper>
    <paper id="633">
      <title><fixed-case>CSR</fixed-case>-Bench: Benchmarking <fixed-case>LLM</fixed-case> Agents in Deployment of Computer Science Research Repositories</title>
      <author><first>Yijia</first><last>Xiao</last></author>
      <author><first>Runhui</first><last>Wang</last></author>
      <author><first>Luyang</first><last>Kong</last><affiliation>Amazon</affiliation></author>
      <author><first>Davor</first><last>Golac</last></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>12705-12723</pages>
      <abstract>The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.</abstract>
      <url hash="dfe2ecde">2025.naacl-long.633</url>
      <bibkey>xiao-etal-2025-csr</bibkey>
    </paper>
    <paper id="634">
      <title><fixed-case>SALAD</fixed-case>: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and <fixed-case>LLM</fixed-case>-Driven Augmented Data</title>
      <author><first>Suyoung</first><last>Bae</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Hyojun</first><last>Kim</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>12724-12738</pages>
      <abstract>In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data.To address this problem, we propose **SALAD** (**S**tructure **A**ware and **L**LM-driven **A**ugmented **D**ata), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning.Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns. By applying contrastive learning, *SALAD* enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations.We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference. The results demonstrate that *SALAD* not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios.</abstract>
      <url hash="d87bbd73">2025.naacl-long.634</url>
      <bibkey>bae-etal-2025-salad</bibkey>
    </paper>
    <paper id="635">
      <title>Rationale-Guided Retrieval Augmented Generation for Medical Question Answering</title>
      <author><first>Jiwoong</first><last>Sohn</last></author>
      <author><first>Yein</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sihyeon</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Mujeen</first><last>Sung</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Hyunjae</first><last>Kim</last><affiliation>Yale University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>12739-12753</pages>
      <abstract>Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge.While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or unhelpful context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG<tex-math>^2</tex-math> (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG<tex-math>^2</tex-math> incorporates three key innovations: a small filtering model trained on perplexity-based labels of rationales, which selectively augments informative snippets of documents while filtering out distractors; LLM-generated rationales as queries to improve the utility of retrieved snippets; a structure designed to retrieve snippets evenly from a comprehensive set of four biomedical corpora, effectively mitigating retriever bias. Our experiments demonstrate that RAG<tex-math>^2</tex-math> improves the state-of-the-art LLMs of varying sizes, with improvements of up to 6.1%, and it outperforms the previous best medical RAG model by up to 5.6% across three medical question-answering benchmarks. Our code is available at https://github.com/dmis-lab/RAG2</abstract>
      <url hash="42cd4d1e">2025.naacl-long.635</url>
      <bibkey>sohn-etal-2025-rationale</bibkey>
    </paper>
    <paper id="636">
      <title>Prototype Conditioned Generative Replay for Continual Learning in <fixed-case>NLP</fixed-case></title>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Min</first><last>Zeng</last></author>
      <pages>12754-12770</pages>
      <abstract>Generative replay has proven effective in addressing the catastrophic forgetting issue of continual learning (CL) in natural language processing (NLP). However, relying on a single task-specific token or prompt often falls short in generating pseudo-samples that accurately reflect the true data distribution. This leads to issues of semantic inconsistency and scale inconsistency.To tackle these challenges, we propose a Prototype Conditioned Generative Replay (PCGR) method, which enhances generative reply by incorporating task-level statistics through a Prototype Conditioned Variational Autoencoder (PCVAE).Specifically, task-level embedding statistics are stored as prototypes for each old task. When a new task is introduced, PCVAE draws samples from task-specific prototype-based distributions to generate pseudo-samples.By incorporating the prototype, the generated pseudo-samples are both more representative and sufficiently diverse to reflect the real data distribution.Furthermore, as previously stored prototypes may become outdated due to evolving model parameters, we propose a Prototype Shift Estimation (PSE) to adjust for these changes.Experiments on NLP tasks across two different scenarios show that PCGR outperforms previous state-of-the-art (SOTA) methods.</abstract>
      <url hash="f53534c4">2025.naacl-long.636</url>
      <bibkey>chen-zeng-2025-prototype</bibkey>
    </paper>
    <paper id="637">
      <title><fixed-case>KODIS</fixed-case>: A Multicultural Dispute Resolution Dialogue Corpus</title>
      <author><first>James Anthony</first><last>Hale</last><affiliation>USC Institute for Creative Technologies, University of Southern California</affiliation></author>
      <author><first>Sushrita</first><last>Rakshit</last></author>
      <author><first>Kushal</first><last>Chawla</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Jeanne M</first><last>Brett</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Jonathan</first><last>Gratch</last><affiliation>University of Southern California</affiliation></author>
      <pages>12771-12785</pages>
      <url hash="45811041">2025.naacl-long.637</url>
      <bibkey>hale-etal-2025-kodis</bibkey>
    </paper>
  </volume>
  <volume id="short" ingest-date="2025-04-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)</booktitle>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>April</month>
      <year>2025</year>
      <url hash="d6c14097">2025.naacl-short</url>
      <venue>naacl</venue>
      <isbn>979-8-89176-190-2</isbn>
    </meta>
    <frontmatter>
      <url hash="42e31346">2025.naacl-short.0</url>
      <bibkey>naacl-2025-short</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Complete Chess Games Enable <fixed-case>LLM</fixed-case> Become A Chess Master</title>
      <author><first>Yinqi</first><last>Zhang</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Xintian</first><last>Han</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Haolong</first><last>Li</last></author>
      <author><first>Kedi</first><last>Chen</last></author>
      <author><first>Shaohui</first><last>Lin</last></author>
      <pages>1-7</pages>
      <abstract>Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM’s success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation. We show that by simply supervised fine-tuning, our model has achieved a professional-level Elo rating of 1788 in matches against the standard Elo-rated Stockfish when permitted to sample 10 times. We further show that data quality is important. Long-round data supervision enjoys a 350 Elo rating improvement over short-round data.</abstract>
      <url hash="c3f76bf8">2025.naacl-short.1</url>
      <bibkey>zhang-etal-2025-complete</bibkey>
    </paper>
    <paper id="2">
      <title>Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models</title>
      <author><first>Dipankar</first><last>Srirag</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Aditya</first><last>Joshi</last><affiliation>UNSW</affiliation></author>
      <author><first>Jacob</first><last>Eisenstein</last><affiliation>Google</affiliation></author>
      <pages>8-17</pages>
      <abstract>Dialect adapters that improve the performance of LLMs for NLU tasks on certain sociolects/dialects/national varieties (‘dialects’ for the sake of brevity) have been reported for encoder models. In this paper, we extend the idea of dialect adapters to decoder models in our architecture called LoRDD. Using MD-3, a publicly available dataset of word game-playing conversations between dialectal speakers, our task is Target Word Prediction (TWP) from a masked conversation. LoRDD combines task adapters and dialect adapters where the latter employ contrastive learning on pseudo-parallel conversations from MD-3. Our experiments on Indian English and Nigerian English conversations with two models (Mistral and Gemma) demonstrate that LoRDD outperforms four baselines on TWP. Additionally, it significantly reduces the performance gap with American English, narrowing it to 12% and 5.8% for word similarity, and 25% and 4.5% for accuracy, respectively. The focused contribution of LoRDD is in its promise for dialect adaptation of decoder models using TWP, a simplified version of the commonly used next-word prediction task.</abstract>
      <url hash="3106515b">2025.naacl-short.2</url>
      <bibkey>srirag-etal-2025-predicting</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>C</fixed-case>ha<fixed-case>I</fixed-case>-<fixed-case>T</fixed-case>e<fixed-case>A</fixed-case>: A Benchmark for Evaluating Autocompletion of Interactions with <fixed-case>LLM</fixed-case>-based Chatbots</title>
      <author><first>Shani</first><last>Goren</last></author>
      <author><first>Oren</first><last>Kalinsky</last><affiliation>Amazon</affiliation></author>
      <author><first>Tomer</first><last>Stav</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuri</first><last>Rapoport</last><affiliation>Amazon</affiliation></author>
      <author><first>Yaron</first><last>Fairstein</last><affiliation>Amazon</affiliation></author>
      <author><first>Ram</first><last>Yazdi</last></author>
      <author><first>Nachshon</first><last>Cohen</last><affiliation>Amazon</affiliation></author>
      <author><first>Alexander</first><last>Libov</last><affiliation>Amazon</affiliation></author>
      <author><first>Guy</first><last>Kushilevitz</last><affiliation>Amazon</affiliation></author>
      <pages>18-32</pages>
      <abstract>The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots.The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We present **ChaI-TeA**: **Cha**t **I**n**te**raction **A**utocomplete; An autocomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, curated datasets and suitable metrics. We use it to evaluate 11 models on this task, finding that while current off-the-shelf models perform fairly, there is still much room for improvement, mainly in ranking of the generated suggestions. We provide insights for practitioners working on this task and open new research directions for researchers in the field. We release our framework to serve as a foundation for future research.</abstract>
      <url hash="dac0ec0c">2025.naacl-short.3</url>
      <bibkey>goren-etal-2025-chai</bibkey>
    </paper>
    <paper id="4">
      <title>Cross-Lingual Transfer Learning for Speech Translation</title>
      <author><first>Rao</first><last>Ma</last></author>
      <author><first>Mengjie</first><last>Qian</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Yassir</first><last>Fathullah</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Siyuan</first><last>Tang</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Kate</first><last>Knill</last><affiliation>University of Cambridge</affiliation></author>
      <pages>33-43</pages>
      <abstract>There has been increasing interest in building multilingual foundation models for NLP and speech research. This paper examines how to expand the speech translation capability of these models with restricted data. Whisper, a speech foundation model with strong performance on speech recognition and English translation, is used as the example model. Using speech-to-speech retrieval to analyse the audio representations generated by the encoder, we show that utterances from different languages are mapped to a shared semantic space. This shared embedding space can then be leveraged for zero-shot cross-lingual transfer in speech translation. By fine-tuning the Whisper decoder with only English-to-Chinese speech translation data, improved performance for translation to Chinese can be obtained for multiple languages, in addition to English. Furthermore, for languages related to those seen in training it is possible to perform speech translation, despite the model never seeing the language in training, or being able to perform transcription.</abstract>
      <url hash="2aa12071">2025.naacl-short.4</url>
      <bibkey>ma-etal-2025-cross</bibkey>
    </paper>
    <paper id="5">
      <title>Reverse Question Answering: Can an <fixed-case>LLM</fixed-case> Write a Question so Hard (or Bad) that it Can’t Answer?</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Feng</first><last>Gu</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>University of Washington and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>George Washington University</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>44-64</pages>
      <abstract>Question answering (QA)—giving correct answers to questions—is a popular task, but we test **reverse question answering (RQA)**: for an input answer, give a question with that answer. Past work tests QA and RQA separately, but we test them jointly, comparing their difficulty, aiding benchmark design, and checking reasoning consistency. We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus RQA, LLMs are much less accurate in RQA for numerical answers, but slightly more accurate in RQA for textual answers; 2) LLMs often answer their own invalid questions from RQA accurately in QA, so RQA errors are not just from knowledge gaps; 3) RQA errors correlate with question difficulty and inversely correlate with answer frequencies in the Dolma corpus; and 4) LLMs struggle to give valid multi-hop questions. By finding question and answer types that lead to RQA errors, we suggest improvements for LLM reasoning.</abstract>
      <url hash="cac5de65">2025.naacl-short.5</url>
      <bibkey>balepur-etal-2025-reverse</bibkey>
    </paper>
    <paper id="6">
      <title>Personalized Help for Optimizing Low-Skilled Users’ Strategy</title>
      <author><first>Feng</first><last>Gu</last></author>
      <author><first>Wichayaporn</first><last>Wongkamjan</last></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Denis</first><last>Peskoff</last></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>65-74</pages>
      <abstract>AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it.</abstract>
      <url hash="996ae783">2025.naacl-short.6</url>
      <bibkey>gu-etal-2025-personalized</bibkey>
    </paper>
    <paper id="7">
      <title>Local Prompt Optimization</title>
      <author><first>Yash</first><last>Jain</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vishal</first><last>Chowdhary</last></author>
      <pages>75-81</pages>
      <abstract>In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.</abstract>
      <url hash="6f9a0b8c">2025.naacl-short.7</url>
      <bibkey>jain-chowdhary-2025-local</bibkey>
    </paper>
    <paper id="8">
      <title>Cross-lingual Transfer of Reward Models in Multilingual Alignment</title>
      <author><first>Jiwoo</first><last>Hong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Noah</first><last>Lee</last></author>
      <author><first>Rodrigo</first><last>Martínez-Castaño</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <author><first>César</first><last>Rodríguez</last></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>82-94</pages>
      <abstract>Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability.</abstract>
      <url hash="eafb2f16">2025.naacl-short.8</url>
      <bibkey>hong-etal-2025-cross</bibkey>
    </paper>
    <paper id="9">
      <title>Inference-Time Selective Debiasing to Enhance Fairness in Text Classification Models</title>
      <author><first>Gleb</first><last>Kuzmin</last><affiliation>Artificial Intelligence Research Institute and Institute for Systems Analysis of Russian Academy of Sciences</affiliation></author>
      <author><first>Neemesh</first><last>Yadav</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Ivan</first><last>Smirnov</last></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>95-107</pages>
      <abstract>We propose selective debiasing – an inference-time safety mechanism designed to enhance the overall model quality in terms of prediction performance and fairness, especially in scenarios where retraining the model is impractical. The method draws inspiration from selective classification, where at inference time, predictions with low quality, as indicated by their uncertainty scores, are discarded. In our approach, we identify the potentially biased model predictions and, instead of discarding them, we remove bias from these predictions using LEACE – a post-processing debiasing method. To select problematic predictions, we propose a bias quantification approach based on KL divergence, which achieves better results than standard uncertainty quantification methods. Experiments on text classification datasets with encoder-based classification models demonstrate that selective debiasing helps to reduce the performance gap between post-processing methods and debiasing techniques from the at-training and pre-processing categories.</abstract>
      <url hash="94c6358c">2025.naacl-short.9</url>
      <bibkey>kuzmin-etal-2025-inference</bibkey>
    </paper>
    <paper id="10">
      <title>Automatic Evaluation of Healthcare <fixed-case>LLM</fixed-case>s Beyond Question-Answering</title>
      <author><first>Anna</first><last>Arias-Duart</last></author>
      <author><first>Pablo Agustin</first><last>Martin-Torres</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Daniel</first><last>Hinjos</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Pablo</first><last>Bernabeu-Perez</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Lucia Urcelay</first><last>Ganzabal</last></author>
      <author><first>Marta Gonzalez</first><last>Mallo</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Ashwin Kumar</first><last>Gururajan</last></author>
      <author><first>Enrique</first><last>Lopez-Cuena</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Sergio</first><last>Alvarez-Napagao</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>Dario</first><last>Garcia-Gasulla</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <pages>108-130</pages>
      <abstract>Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model’s capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark–CareQA–, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations –Relaxed Perplexity– to mitigate the identified limitations.</abstract>
      <url hash="d15bd59e">2025.naacl-short.10</url>
      <bibkey>arias-duart-etal-2025-automatic</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>STRUX</fixed-case>: An <fixed-case>LLM</fixed-case> for Decision-Making with Structured Explanations</title>
      <author><first>Yiming</first><last>Lu</last></author>
      <author><first>Yebowen</first><last>Hu</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Hassan</first><last>Foroosh</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Wei</first><last>Jin</last><affiliation>Emory University</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>131-141</pages>
      <abstract>Countless decisions shape our lives, and it is crucial to understand the how and why behind them. In this paper, we introduce a new LLM decision-making framework called STRUX, which enhances LLM decision-making by providing structured explanations. These include favorable and adverse facts related to the decision, along with their respective strengths. STRUX begins by distilling lengthy information into a concise table of key facts. It then employs a series of self-reflection steps to determine which of these facts are pivotal, categorizing them as either favorable or adverse in relation to a specific decision. Lastly, we fine-tune an LLM to identify and prioritize these key facts to optimize decision-making. STRUX has been evaluated on the challenging task of forecasting stock investment decisions based on earnings call transcripts and demonstrated superior performance against strong baselines. It enhances decision transparency by allowing users to understand the impact of different factors, representing a meaningful step towards practical decision-making with LLMs.</abstract>
      <url hash="dd9618ae">2025.naacl-short.11</url>
      <bibkey>lu-etal-2025-strux</bibkey>
    </paper>
    <paper id="12">
      <title>Improving <fixed-case>V</fixed-case>ietnamese-<fixed-case>E</fixed-case>nglish Cross-Lingual Retrieval for Legal and General Domains</title>
      <author><first>Toan Ngoc</first><last>Nguyen</last></author>
      <author><first>Nam Le</first><last>Hai</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Nguyen Doan</first><last>Hieu</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Dai An</first><last>Nguyen</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <pages>142-153</pages>
      <abstract>Document retrieval plays a crucial role in numerous question-answering systems, yet research has concentrated on the general knowledge domain and resource-rich languages like English. In contrast, it remains largely underexplored in low-resource languages and cross-lingual scenarios within specialized domain knowledge such as legal. We present a novel dataset designed for cross-lingual retrieval between Vietnamese and English, which not only covers the general domain but also extends to the legal field. Additionally, we propose auxiliary loss function and symmetrical training strategy that significantly enhance the performance of state-of-the-art models on these retrieval tasks. Our contributions offer a significant resource and methodology aimed at improving cross-lingual retrieval in both legal and general QA settings, facilitating further advancements in document retrieval research across multiple languages and a broader spectrum of specialized domains. All the resources related to our work can be accessed at <url>huggingface.co/datasets/bkai-foundation-models/crosslingual</url>.</abstract>
      <url hash="d7741d67">2025.naacl-short.12</url>
      <bibkey>nguyen-etal-2025-improving</bibkey>
    </paper>
    <paper id="13">
      <title>Computational Discovery of Chiasmus in Ancient Religious Text</title>
      <author><first>Hope</first><last>McGovern</last></author>
      <author><first>Hale</first><last>Sirin</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tom</first><last>Lippincott</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>154-160</pages>
      <abstract>Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected patterns. Despite its computational efficiency, our method achieves robust results, with high inter-annotator agreement and system accuracy of 0.80 at the verse level and 0.60 at the half-verse level. We further provide a qualitative analysis of the distribution of detected chiasmi, along with selected examples that highlight the effectiveness of our approach.</abstract>
      <url hash="f5f5d1e1">2025.naacl-short.13</url>
      <bibkey>mcgovern-etal-2025-computational</bibkey>
    </paper>
    <paper id="14">
      <title>Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces</title>
      <author><first>Hope</first><last>McGovern</last></author>
      <author><first>Hale</first><last>Sirin</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tom</first><last>Lippincott</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>161-167</pages>
      <abstract>Rhetorical devices are difficult to translate, but they are crucial to the translation of literary documents. We investigate the use of multilingual embedding spaces to characterize the preservation of intertextuality, one common rhetorical device, across human and machine translation. To do so, we use Biblical texts, which are both full of intertextual references and are highly translated works. We provide a metric to characterize intertextuality at the corpus level and provide a quantitative analysis of the preservation of this rhetorical device across extant human translations and machine-generated counterparts. We go on to provide qualitative analysis of cases wherein human translations over- or underemphasize the intertextuality present in the text, whereas machine translations provide a neutral baseline. This provides support for established scholarship proposing that human translators have a propensity to amplify certain literary characteristics of the original manuscripts.</abstract>
      <url hash="9697a7fa">2025.naacl-short.14</url>
      <bibkey>mcgovern-etal-2025-characterizing</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>LLM</fixed-case>2: Let Large Language Models Harness System 2 Reasoning</title>
      <author><first>Cheng</first><last>Yang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chufan</first><last>Shi</last></author>
      <author><first>Siheng</first><last>Li</last></author>
      <author><first>Bo</first><last>Shui</last></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>168-177</pages>
      <abstract>Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based verifier (System 2). Within LLM2, the LLM is responsible for generating plausible candidates, while the verifier provides timely process-based feedback to distinguish desirable and undesirable outputs. The verifier is trained with a pairwise comparison loss on synthetic process-supervision data generated through our token quality exploration strategy. Empirical results on mathematical reasoning benchmarks substantiate the efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8 (+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with self-consistency, LLM2 achieves additional improvements, boosting major@20 accuracy from 56.2 to 70.2 (+14.0).</abstract>
      <url hash="9c91c80d">2025.naacl-short.15</url>
      <bibkey>yang-etal-2025-llm2</bibkey>
    </paper>
    <paper id="16">
      <title>Context-Efficient Retrieval with Factual Decomposition</title>
      <author><first>Yanhong</first><last>Li</last></author>
      <author><first>David</first><last>Yunis</last></author>
      <author><first>David</first><last>McAllester</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Jiawei</first><last>Zhou</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>178-194</pages>
      <abstract>There has recently been considerable interest in incorporating information retrieval into large language models (LLMs). Retrieval from a dynamically expanding external corpus of text allows a model to incorporate current events and can be viewed as a form of episodic memory. Here we demonstrate that pre-processing the external corpus into semi-structured “atomic facts” makes retrieval more efficient. More specifically, we demonstrate that our particular form of atomic facts improves performance on various question answering tasks when the amount of retrieved text is limited. Limiting the amount of retrieval reduces the size of the context and improves inference efficiency.</abstract>
      <url hash="215fb1fe">2025.naacl-short.16</url>
      <bibkey>li-etal-2025-context</bibkey>
    </paper>
    <paper id="17">
      <title>Sports and Women’s Sports: Gender Bias in Text Generation with Olympic Data</title>
      <author><first>Laura</first><last>Biester</last><affiliation>Middlebury College</affiliation></author>
      <pages>195-205</pages>
      <abstract>Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men’s and women’s events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men’s event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics.</abstract>
      <url hash="845b8ca9">2025.naacl-short.17</url>
      <bibkey>biester-2025-sports</bibkey>
    </paper>
    <paper id="18">
      <title>Alligators All Around: Mitigating Lexical Confusion in Low-resource Machine Translation</title>
      <author><first>Elizabeth</first><last>Nielsen</last><affiliation>Google</affiliation></author>
      <author><first>Isaac Rayburn</first><last>Caswell</last><affiliation>Google</affiliation></author>
      <author><first>Jiaming</first><last>Luo</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <pages>206-221</pages>
      <abstract>Current machine translation (MT) systems for low-resource languages have a particular failure mode: When translating words in a given domain, they tend to confuse words within that domain. So, for example, “lion” might be translated as “alligator”, and “orange” might be rendered as “purple.” We propose a recall-based metric for measuring this problem and show that the problem exists in 122 low-resource languages. We then show that this problem can be mitigated by using a large language model (LLM) to post-edit the MT output, specifically by including the entire GATITOS lexicon for the relevant language as a very long context prompt. We show gains in average ChrF score over the set of 122 languages, and we show that the recall score for relevant lexical items also improves. Finally, we demonstrate that a small dedicated MT system with a general-purpose LLM as a post-editor is outperforms a lexicon-based RAG-LLM translator, suggesting a new paradigm for LLM use.</abstract>
      <url hash="88479232">2025.naacl-short.18</url>
      <bibkey>nielsen-etal-2025-alligators</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>PROM</fixed-case>: Pivoted and Regulated Optimization for Multilingual Instruction Learning</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hojin</first><last>Lee</last><affiliation>Kakao brain</affiliation></author>
      <author><first>Yunju</first><last>Bak</last><affiliation>Kakao Corp</affiliation></author>
      <author><first>Changmin</first><last>Lee</last><affiliation>Kakao Corporation</affiliation></author>
      <pages>222-228</pages>
      <abstract>Large language models (LLMs) have become standard for natural language generation tasks, with instruction-tuning enhancing their capabilities. However, the lack of instruction-tuning datasets in languages other than English limits their application to diverse languages. To address this, researchers have adapted English-centric LLMs to other languages by appending English tuning data with its translated pair, from which we observe negative interference between the two. To resolve this, our contribution is identifying English as an internal pivot language, based on which we disentangle the roles of English and target language data in training. Specifically, we first design two roles as pivoted objectives, and also propose to regulate between the two, to better generalize for under-represented languages. Experiments across various languages demonstrate the effectiveness of our approach on multiple benchmarks. The code is publicly available for further exploration.</abstract>
      <url hash="2e6d44ef">2025.naacl-short.19</url>
      <bibkey>lee-etal-2025-prom</bibkey>
    </paper>
    <paper id="20">
      <title>Concept-Reversed <fixed-case>W</fixed-case>inograd Schema Challenge: Evaluating and Improving Robust Reasoning in Large Language Models via Abstraction</title>
      <author><first>Kaiqiao</first><last>Han</last></author>
      <author><first>Tianqing</first><last>Fang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Edinburgh University, University of Edinburgh and Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Mark</first><last>Steedman</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>229-243</pages>
      <abstract>While Large Language Models (LLMs) have showcased remarkable proficiency in reasoning, there is still a concern about hallucinations and unreliable reasoning issues due to semantic associations and superficial logical chains. To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset. By simply reversing the concepts to those that are more associated with the wrong answer, we find that the performance of LLMs drops significantly despite the rationale of reasoning remaining the same. Furthermore, we propose Abstraction-of-Thought (AoT), a novel prompt method for recovering adversarial cases to normal cases using conceptual abstraction to improve LLMs’ robustness and consistency in reasoning, as demonstrated by experiments on CR-WSC.</abstract>
      <url hash="61910dd0">2025.naacl-short.20</url>
      <bibkey>han-etal-2025-concept</bibkey>
    </paper>
    <paper id="21">
      <title>Defense against Prompt Injection Attacks via Mixture of Encodings</title>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>David</first><last>Sullivan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kyle</first><last>Jackson</last><affiliation>Microsoft</affiliation></author>
      <author><first>Pengtao</first><last>Xie</last><affiliation>University of California, San Diego and Carnegie Mellon University</affiliation></author>
      <author><first>Mei</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <pages>244-252</pages>
      <abstract>Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM’s output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics.</abstract>
      <url hash="21840720">2025.naacl-short.21</url>
      <bibkey>zhang-etal-2025-defense</bibkey>
    </paper>
    <paper id="22">
      <title>Watching the <fixed-case>AI</fixed-case> Watchdogs: A Fairness and Robustness Analysis of <fixed-case>AI</fixed-case> Safety Moderation Classifiers</title>
      <author><first>Akshit</first><last>Achara</last></author>
      <author><first>Anshuman</first><last>Chhabra</last><affiliation>University of South Florida</affiliation></author>
      <pages>253-264</pages>
      <abstract>AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential for disparate impact, it is crucial to ensure that these classifiers: (1) do not unfairly classify content belonging to users from minority groups as unsafe compared to those from majority groups and (2) that their behavior remains robust and consistent across similar inputs. In this work, we thus examine the fairness and robustness of four widely-used, closed-source ASM classifiers: OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL) API, and Clarifai API. We assess fairness using metrics such as demographic parity and conditional statistical parity, comparing their performance against ASM models and a fair-only baseline. Additionally, we analyze robustness by testing the classifiers’ sensitivity to small and natural input perturbations. Our findings reveal potential fairness and robustness gaps, highlighting the need to mitigate these issues in future versions of these models.</abstract>
      <url hash="adb11137">2025.naacl-short.22</url>
      <bibkey>achara-chhabra-2025-watching</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>C</fixed-case>o<fixed-case>RAG</fixed-case>: Collaborative Retrieval-Augmented Generation</title>
      <author><first>Aashiq</first><last>Muhamed</last></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Virginia</first><last>Smith</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>265-276</pages>
      <abstract>Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.</abstract>
      <url hash="c91269df">2025.naacl-short.23</url>
      <bibkey>muhamed-etal-2025-corag</bibkey>
    </paper>
    <paper id="24">
      <title>Is It <fixed-case>N</fixed-case>avajo? Accurate Language Detection for Endangered Athabaskan Languages</title>
      <author><first>Ivory</first><last>Yang</last></author>
      <author><first>Weicheng</first><last>Ma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>277-284</pages>
      <abstract>Endangered languages, such as Navajo—the most widely spoken Native American language—are significantly underrepresented in contemporary language technologies, exacerbating the challenges of their preservation and revitalization. This study evaluates Google’s Language Identification (LangID) tool, which does not currently support any Native American languages. To address this, we introduce a random forest classifier trained on Navajo and twenty erroneously suggested languages by LangID. Despite its simplicity, the classifier achieves near-perfect accuracy (97-100%). Additionally, the model demonstrates robustness across other Athabaskan languages—a family of Native American languages spoken primarily in Alaska, the Pacific Northwest, and parts of the Southwestern United States—suggesting its potential for broader application. Our findings underscore the pressing need for NLP systems that prioritize linguistic diversity and adaptability over centralized, one-size-fits-all solutions, especially in supporting underrepresented languages in a multicultural world. This work directly contributes to ongoing efforts to address cultural biases in language models and advocates for the development of culturally localized NLP tools that serve diverse linguistic communities.</abstract>
      <url hash="200bbbe9">2025.naacl-short.24</url>
      <bibkey>yang-etal-2025-navajo</bibkey>
    </paper>
    <paper id="25">
      <title>Don’t Touch My Diacritics</title>
      <author><first>Kyle</first><last>Gorman</last><affiliation>The Graduate Center, City University of New York and Google</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>285-291</pages>
      <abstract>The common practice of preprocessing text before feeding it into NLP models introduces many decision points which have unintended consequences on model performance. In this opinion piece, we focus on the handling of diacritics in texts originating in many languages and scripts. We demonstrate, through several case studies, the adverse effects of inconsistent encoding of diacritized characters and of removing diacritics altogether. We call on the community to adopt simple but necessary steps across all models and toolkits in order to improve handling of diacritized text and, by extension, increase equity in multilingual NLP.</abstract>
      <url hash="ebfe8679">2025.naacl-short.25</url>
      <bibkey>gorman-pinter-2025-dont</bibkey>
    </paper>
    <paper id="26">
      <title>Pretrained Image-Text Models are Secretly Video Captioners</title>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Yiren</first><last>Jian</last><affiliation>OpenAI</affiliation></author>
      <author><first>Zhongyu</first><last>Ouyang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>292-305</pages>
      <abstract>Developing video captioning models is computationally expensive. The dynamic nature of video also complicates the design of multimodal models that can effectively caption these sequences. However, we find that by using minimal computational resources and without complex modifications to address video dynamics, an image-based model can be repurposed to outperform several specialised video captioning systems. Our adapted model demonstrates top-tier performance on major benchmarks, ranking 2nd on MSR-VTT and MSVD, and 3rd on VATEX. We transform it into a competitive video captioner by post-training a typical image captioning model BLIP-2 with only 6,000 video-text pairs and simply concatenating frames—significantly fewer data than other methods, which use 2.5 to 144 million pairs. From a resource optimization perspective, this video captioning study focuses on three fundamental factors: optimizing model scale, maximizing data efficiency, and incorporating reinforcement learning. This extensive study demonstrates that a lightweight, image-based adaptation strategy can rival state-of-the-art video captioning systems, offering a practical solution for low-resource scenarios.</abstract>
      <url hash="8c011255">2025.naacl-short.26</url>
      <bibkey>zhang-etal-2025-pretrained</bibkey>
    </paper>
    <paper id="27">
      <title>Reverse Modeling in Large Language Models</title>
      <author><first>Sicheng</first><last>Yu</last></author>
      <author><first>Xu</first><last>Yuanchen</last></author>
      <author><first>Cunxiao</first><last>Du</last><affiliation>Sea AI LAB</affiliation></author>
      <author><first>Yanying</first><last>Zhou</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Qianru</first><last>Sun</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiawei</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <pages>306-320</pages>
      <abstract>Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference across multiple languages.Our case study shows that different-content texts result in different losses if input (to LLMs) in different directions—some get lower losses for forward while some for reverse. This leads us to a simple and nice solution for data selection based on the loss differences between forward and reverse directions. Using our selected data in continued pretraining can boost LLMs’ performance by a large margin across different language understanding benchmarks.</abstract>
      <url hash="a71f3724">2025.naacl-short.27</url>
      <bibkey>yu-etal-2025-reverse</bibkey>
    </paper>
    <paper id="28">
      <title>Preserving Multilingual Quality While Tuning Query Encoder on <fixed-case>E</fixed-case>nglish Only</title>
      <author><first>Oleg</first><last>Vasilyev</last><affiliation>Primer Technologies</affiliation></author>
      <author><first>Randy</first><last>Sawaya</last><affiliation>Primer Technologies</affiliation></author>
      <author><first>John</first><last>Bohannon</last></author>
      <pages>321-341</pages>
      <abstract>A query encoder of a dual passage retrieval system can be tuned for specific types of queries or domains, while the precomputed and stored documents representations are kept intact. Switching from one query encoder to another when needed is easily feasible, unlike overhauling the embeddings of a whole knowledge base. In this work we raise a question: Can the generic, original qualities of the encoder be preserved or at least left not too degraded when it is tuned on a narrow domain? We conducted experiments on a high quality multilingual embedding model: Tuning it on a single English-only dataset, we observe that the tuning not only preserves the multilingual qualities, but even improves them. The embedding qualities on distinctly different data are also improved or at least preserved. Drawing on our observations, we suggest a more general hypothesis: Tuning with intentionally low learning rate can preserve or improve a system’s properties acquired in training, but not specifically targeted by tuning. We call this <i>adiabatic tuning</i> and provide tentative explanations.</abstract>
      <url hash="0a40f467">2025.naacl-short.28</url>
      <bibkey>vasilyev-etal-2025-preserving</bibkey>
    </paper>
    <paper id="29">
      <title>Using Contextually Aligned Online Reviews to Measure <fixed-case>LLM</fixed-case>s’ Performance Disparities Across Language Varieties</title>
      <author><first>Zixin</first><last>Tang</last><affiliation>The Pennsylvania State University</affiliation></author>
      <author><first>Chieh-Yang</first><last>Huang</last><affiliation>MetaMetrics</affiliation></author>
      <author><first>Tsung-che</first><last>Li</last><affiliation>, Academia Sinica</affiliation></author>
      <author><first>Ho Yin Sam</first><last>Ng</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Ting-Hao Kenneth</first><last>Huang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>342-355</pages>
      <abstract>A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms,such as Booking.com, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin.</abstract>
      <url hash="c81df266">2025.naacl-short.29</url>
      <bibkey>tang-etal-2025-using</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Federated Low-Rank Adaptation of Language Models with Rank Heterogeneity</title>
      <author><first>Yuji</first><last>Byun</last></author>
      <author><first>Jaeho</first><last>Lee</last><affiliation>Google and Pohang University of Science and Technology</affiliation></author>
      <pages>356-362</pages>
      <abstract>Low-rank adaptation (LoRA) offers an efficient alternative to full-weight adaptation in federated fine-tuning of language models, significantly reducing computational costs. By adjusting ranks for each client, federated LoRA enables flexible resource allocation. However, we observe that heterogeneous ranks among clients lead to unstable performance. Our analysis attributes this instability to the conventional zero-padding aggregation strategy, which dilutes information from high-rank clients during model aggregation. To address this issue, we propose a replication-based padding strategy that better retains valuable information from clients with high-quality data. Empirically, this approach accelerates convergence and enhances the global model’s predictive performance.</abstract>
      <url hash="8ff37fe8">2025.naacl-short.30</url>
      <bibkey>byun-lee-2025-towards</bibkey>
    </paper>
    <paper id="31">
      <title>Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject</title>
      <author><first>Zenghao</first><last>Duan</last></author>
      <author><first>Wenbin</first><last>Duan</last><affiliation>People’s Public Security University of China</affiliation></author>
      <author><first>Zhiyi</first><last>Yin</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yinghan</first><last>Shen</last></author>
      <author><first>Shaoling</first><last>Jing</last></author>
      <author><first>Jie</first><last>Zhang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>363-373</pages>
      <url hash="fc9b036a">2025.naacl-short.31</url>
      <bibkey>duan-etal-2025-related</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>STEP</fixed-case>: Staged Parameter-Efficient Pre-training for Large Language Models</title>
      <author><first>Kazuki</first><last>Yano</last></author>
      <author><first>Takumi</first><last>Ito</last><affiliation>Langsmith Inc., Tohoku University and Machine Learning Solutions</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>374-384</pages>
      <abstract>Pre-training large language models (LLMs) faces significant memory challenges due to the large size of model weights. We introduce STaged parameter-Efficient Pre-training (STEP), which integrates parameter-efficient tuning techniques with model growth. We conduct experiments on pre-training LLMs of various sizes and demonstrate that STEP achieves up to a 53.9% reduction in maximum memory requirements compared to vanilla pre-training while maintaining equivalent performance. Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning.</abstract>
      <url hash="b2e85408">2025.naacl-short.32</url>
      <bibkey>yano-etal-2025-step</bibkey>
    </paper>
    <paper id="33">
      <title>Language Models Encode Numbers Using Digit Representations in Base 10</title>
      <author><first>Amit Arnold</first><last>Levy</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>385-395</pages>
      <abstract>Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10.This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs.</abstract>
      <url hash="fbae8d65">2025.naacl-short.33</url>
      <bibkey>levy-geva-2025-language</bibkey>
    </paper>
    <paper id="34">
      <title>A Systematic Study of Cross-Layer <fixed-case>KV</fixed-case> Sharing for Efficient <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>You</first><last>Wu</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Haoyi</first><last>Wu</last><affiliation>ShanghaiTech University</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>396-403</pages>
      <abstract>Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by <tex-math>2\times</tex-math>, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance.When further reducing the size of the KV cache, however, pairing queries of all layers with KVs of upper layers performs better, at the expense of additional training cost and prefilling latency. We hope that this work will help users make more informed choices of cross-layer KV sharing approaches and facilitate future research on efficient LLM inference.</abstract>
      <url hash="60929515">2025.naacl-short.34</url>
      <bibkey>wu-etal-2025-systematic</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>AMPS</fixed-case>: <fixed-case>ASR</fixed-case> with Multimodal Paraphrase Supervision</title>
      <author><first>Abhishek</first><last>Gupta</last></author>
      <author><first>Amruta</first><last>Parulekar</last></author>
      <author><first>Sameep</first><last>Chattopadhyay</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>404-413</pages>
      <abstract>Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS, that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics.</abstract>
      <url hash="3e3c3b00">2025.naacl-short.35</url>
      <bibkey>gupta-etal-2025-amps</bibkey>
    </paper>
    <paper id="36">
      <title>Taxi1500: A Dataset for Multilingual Text Classification in 1500 Languages</title>
      <author><first>Chunlan</first><last>Ma</last></author>
      <author><first>Ayyoob</first><last>Imani</last><affiliation>Microsoft</affiliation></author>
      <author><first>Haotian</first><last>Ye</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Renhao</first><last>Pei</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last><affiliation>Qatar Computing Research Institute and University of California, Berkeley</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>414-439</pages>
      <abstract>While broad-coverage multilingual natural language processing tools have been developed, a significant portion of the world’s over 7000 languages are still neglected. One reason is the lack of evaluation datasets that cover a diverse range of languages, particularly those that are low-resource or endangered. To address this gap, we present a large-scale text classification dataset encompassing 1504 languages many of which have otherwise limited or no annotated data. This dataset is constructed using parallel translations of the Bible. We develop relevant topics, annotate the English data through crowdsourcing and project these annotations onto other languages via aligned verses. We benchmark a range of existing multilingual models on this dataset. We make our dataset and code available to the public.</abstract>
      <url hash="9d8fb766">2025.naacl-short.36</url>
      <bibkey>ma-etal-2025-taxi1500</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>G</fixed-case>ame<fixed-case>T</fixed-case>ox: A Comprehensive Dataset and Analysis for Enhanced Toxicity Detection in Online Gaming Communities</title>
      <author><first>Usman</first><last>Naseem</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Shuvam</first><last>Shiwakoti</last></author>
      <author><first>Siddhant Bikram</first><last>Shah</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Surendrabikram</first><last>Thapa</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Tongji University</affiliation></author>
      <pages>440-447</pages>
      <abstract>The prevalence of toxic behavior in online gaming communities necessitates robust detection methods to ensure user safety. We introduce GameTox, a novel dataset comprising 53K game chat utterances annotated for toxicity detection through intent classification and slot filling. This dataset captures the complex relationship between user intent and specific linguistic features that contribute to toxic interactions. We extensively analyze the dataset to uncover key insights into the nature of toxic speech in gaming environments. Furthermore, we establish baseline performance metrics using state-of-the-art natural language processing and large language models, demonstrating the dataset’s contribution towards enhancing the detection of toxic behavior and revealing the limitations of contemporary models. Our results indicate that leveraging both intent detection and slot filling provides a significantly more granular and context-aware understanding of harmful messages. This dataset serves as a valuable resource to train advanced models that can effectively mitigate toxicity in online gaming and foster healthier digital spaces. Our dataset is publicly available at: https://github.com/shucoll/GameTox.</abstract>
      <url hash="1efe4814">2025.naacl-short.37</url>
      <bibkey>naseem-etal-2025-gametox</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>F</fixed-case>aith<fixed-case>B</fixed-case>ench: A Diverse Hallucination Benchmark for Summarization by <fixed-case>M</fixed-case>odern <fixed-case>LLM</fixed-case>s</title>
      <author><first>Forrest Sheng</first><last>Bao</last><affiliation>Vectara, Inc.</affiliation></author>
      <author><first>Miaoran</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Renyi</first><last>Qu</last><affiliation>Vectara</affiliation></author>
      <author><first>Ge</first><last>Luo</last><affiliation>Vectara Inc.</affiliation></author>
      <author><first>Erana</first><last>Wan</last></author>
      <author><first>Yujia</first><last>Tang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weisi</first><last>Fan</last></author>
      <author><first>Manveer Singh</first><last>Tamber</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Suleman</first><last>Kazi</last><affiliation>Vectara</affiliation></author>
      <author><first>Vivek</first><last>Sourabh</last></author>
      <author><first>Mike</first><last>Qi</last><affiliation>Textea Inc.</affiliation></author>
      <author><first>Ruixuan</first><last>Tu</last></author>
      <author><first>Chenyu</first><last>Xu</last></author>
      <author><first>Matthew</first><last>Gonzales</last><affiliation>Vectara</affiliation></author>
      <author><first>Ofer</first><last>Mendelevitch</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Amin</first><last>Ahmad</last><affiliation>Vectara</affiliation></author>
      <pages>448-461</pages>
      <abstract>Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10 modern LLMs from 8 different families, with ground truth annotations by human experts. “Challenging” here means summaries on which popular, state-of-the-art hallucination detection models, including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and GPT-3.5-Turbo produce the least hallucinations. However, most state-of-the-art hallucination detection models have near 50% accuracies on FaithBench, indicating lots of room for future improvement.</abstract>
      <url hash="a451f708">2025.naacl-short.38</url>
      <bibkey>bao-etal-2025-faithbench</bibkey>
    </paper>
    <paper id="39">
      <title>Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction</title>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Mao</first><last>Mao</last></author>
      <author><first>Shuo</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Haotian</first><last>Shangguan</last></author>
      <pages>462-470</pages>
      <abstract>The use of AI in legal analysis and prediction (LegalAI) has gained attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a lightweight yet robust solution. Comparative experiments show that it outperforms several general-purpose and domain-specific legal models, offering a dynamic reasoning process and a promising direction for future LegalAI research.</abstract>
      <url hash="773cc664">2025.naacl-short.39</url>
      <bibkey>chen-etal-2025-debate</bibkey>
    </paper>
    <paper id="40">
      <title>Great Memory, Shallow Reasoning: Limits of <tex-math>k</tex-math><fixed-case>NN</fixed-case>-<fixed-case>LM</fixed-case>s</title>
      <author><first>Shangyi</first><last>Geng</last></author>
      <author><first>Wenting</first><last>Zhao</last><affiliation>Cornell University</affiliation></author>
      <author><first>Alexander M</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <pages>471-482</pages>
      <abstract>K-nearest neighbor language models (kNN-LMs), which integrate retrieval with next-word prediction, have demonstrated strong performance in language modeling as well as some downstream NLP benchmarks. These results have led researchers to argue that models trained on poor quality or outdated data could perform well by employing a kNN extension that has access to a higher-quality datastore. In this work, we ask whether this improved ability to recall information really translates into downstream abilities. We extensively evaluate kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning. Results show that kNN-LMs excel at memory-intensive tasks, where utilizing the patterns in the input is sufficient for determining the output, but struggle with reasoning tasks that require integrating multiple pieces of information to derive new knowledge. We further demonstrate through oracle experiments and qualitative analysis that even with perfect retrieval, kNN-LMs still fail to determine the correct answers, placing an upper bound on their reasoning performance.</abstract>
      <url hash="ed57c588">2025.naacl-short.40</url>
      <bibkey>geng-etal-2025-great</bibkey>
    </paper>
    <paper id="41">
      <title>Repetition Neurons: How Do Language Models Produce Repetitions?</title>
      <author><first>Tatsuya</first><last>Hiraoka</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and RIKEN</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>483-495</pages>
      <abstract>This paper introduces repetition neurons, which can be regarded as “skill neurons” responsible for the repetition problem in text generation tasks. These neurons are progressively activated more strongly as repetition continues, indicating that they perceive repetition as a task to copy the previous context repeatedly, similar to in-context learning. We identify these repetition neurons by comparing activation values before and after the onset of repetition in texts generated by recent pre-trained language models. We analyze the repetition neurons in three English and one Japanese pre-trained language models and observe similar patterns across them.</abstract>
      <url hash="b585439a">2025.naacl-short.41</url>
      <bibkey>hiraoka-inui-2025-repetition</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>STAR</fixed-case>: Spectral Truncation and Rescale for Model Merging</title>
      <author><first>Yu-Ang</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Ching-Yun</first><last>Ko</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Tejaswini</first><last>Pedapati</last></author>
      <author><first>I-Hsin</first><last>Chung</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Mi-Yen</first><last>Yeh</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <pages>496-505</pages>
      <abstract>Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose **S**pectral **T**runcation **A**nd **R**escale (STAR) that aims at mitigating “merging conflicts” by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2% when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR.</abstract>
      <url hash="6e097130">2025.naacl-short.42</url>
      <bibkey>lee-etal-2025-star</bibkey>
    </paper>
    <paper id="43">
      <title>Task-driven Layerwise Additive Activation Intervention</title>
      <author><first>Hieu Trung</first><last>Nguyen</last><affiliation>Vinai Research</affiliation></author>
      <author><first>Bao</first><last>Nguyen</last></author>
      <author><first>Binh</first><last>Nguyen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Viet Anh</first><last>Nguyen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>506-513</pages>
      <abstract>Modern language models (LMs) have significantly advanced generative modeling in natural language processing (NLP). Despite their success, LMs often struggle with adaptation to new contexts in real-time applications. A promising approach to task adaptation is activation intervention, which steers the LMs’ generation process by identifying and manipulating the activations. However, existing interventions rely heavily on heuristic rules or require many prompt inputs to determine effective interventions. In this paper, we propose a layer-wise additive activation intervention framework that optimizes the intervention process, thereby enhancing sample efficiency. We evaluate our framework on various datasets, demonstrating improvements in the accuracy of pretrained LMs and competing intervention baselines.</abstract>
      <url hash="ff21b5dd">2025.naacl-short.43</url>
      <bibkey>nguyen-etal-2025-task</bibkey>
    </paper>
    <paper id="44">
      <title>Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches</title>
      <author><first>Adithya</first><last>Pratapa</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Teruko</first><last>Mitamura</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>514-528</pages>
      <abstract>Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To understand their utility on large-scale MDS, we evaluated them on three datasets, each containing approximately one hundred documents per summary. Our experiments cover a diverse set of long-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and compression methods (retrieval-augmented, hierarchical, incremental). Overall, we find that full-text and retrieval methods perform the best in most settings. With further analysis into the salient information retention patterns, we show that compression-based methods show strong promise at intermediate stages, even outperforming full-context. However, they suffer information loss due to their multi-stage pipeline and lack of global context. Our results highlight the need to develop hybrid approaches that combine compression and full-text approaches for optimal performance on large-scale multi-document summarization.</abstract>
      <url hash="ecff4b21">2025.naacl-short.44</url>
      <bibkey>pratapa-mitamura-2025-scaling</bibkey>
    </paper>
    <paper id="45">
      <title>Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</title>
      <author><first>Sangmin</first><last>Woo</last></author>
      <author><first>Kang</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Yun</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheng</first><last>Guan</last></author>
      <author><first>Haibo</first><last>Ding</last><affiliation>Amazon</affiliation></author>
      <author><first>Lin Lee</first><last>Cheong</last><affiliation>Amazon</affiliation></author>
      <pages>529-538</pages>
      <abstract>Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting—overlaying visual cues (e.g., bounding box, circle) on images—can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.</abstract>
      <url hash="204d05a1">2025.naacl-short.45</url>
      <bibkey>woo-etal-2025-black</bibkey>
    </paper>
    <paper id="46">
      <title>A Layered Debating Multi-Agent System for Similar Disease Diagnosis</title>
      <author><first>Yutian</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Huimin</first><last>Wang</last><affiliation>Jarvis Research Center, Tencent YouTu Lab</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <pages>539-549</pages>
      <abstract>Distinguishing between extremely similar diseases is a critical and challenging aspect of clinical decision-making. Traditional classification, contrastive learning, and Large Language Models (LLMs) based methods fail to detect the subtle clues necessary for differentiation. This task demands complex reasoning and a variety of tools to identify minor differences and make informed decisions. This paper probes a novel framework that leverages LLMs and a multi-agent system to achieve accurate disease diagnosis through a process of repeated debate and reassessment. The approach aims to identify subtle differences between similar disease candidates. We structure patient information and integrate extensive medical knowledge to guide the analysis towards discerning these differences for precise diagnosis. Comprehensive experiments were conducted on two public datasets and two newly introduced datasets, JarvisD2-Chinese and JarvisD2-English, to validate the effectiveness of our method. The results confirm the efficacy of our approach, demonstrating its potential to enhance diagnostic precision in healthcare.</abstract>
      <url hash="45d428b1">2025.naacl-short.46</url>
      <bibkey>zhao-etal-2025-layered</bibkey>
    </paper>
    <paper id="47">
      <title>The Geometry of Numerical Reasoning: Language Models Compare Numeric Properties in Linear Subspaces</title>
      <author><first>Ahmed Oumar</first><last>El-Shangiti</last></author>
      <author><first>Tatsuya</first><last>Hiraoka</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and RIKEN</affiliation></author>
      <author><first>Hilal</first><last>AlQuabeh</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>550-561</pages>
      <abstract>This paper investigates whether large language models (LLMs) utilize numerical attributes encoded in a low-dimensional subspace of theembedding space when answering questions involving numeric comparisons, e.g., Was Cristiano born before Messi? We first identified,using partial least squares regression, these subspaces, which effectively encode the numerical attributes associated with the entities in comparison prompts. Further, we demonstrate causality, by intervening in these subspaces to manipulate hidden states, thereby altering the LLM’s comparison outcomes. Experiments conducted on three different LLMs showed that our results hold across different numerical attributes, indicating that LLMs utilize the linearly encoded information for numerical reasoning.</abstract>
      <url hash="42b2171f">2025.naacl-short.47</url>
      <bibkey>el-shangiti-etal-2025-geometry</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>A</fixed-case>lign<fixed-case>F</fixed-case>reeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages</title>
      <author><first>Steve</first><last>Bakos</last></author>
      <author><first>David</first><last>Guzmán</last></author>
      <author><first>Riddhi</first><last>More</last></author>
      <author><first>Kelly Chutong</first><last>Li</last></author>
      <author><first>Félix</first><last>Gaschi</last><affiliation>Posos</affiliation></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <pages>562-586</pages>
      <abstract>Realignment techniques are often employed to enhance cross-lingual transfer in multilingual language models, still, they can sometimes degrade performance in languages that differ significantly from the fine-tuned source language. This paper introduces AlignFreeze, a method that freezes either the layers’ lower half or upper half during realignment. Through controlled experiments on 4 tasks, 3 models, and in 35 languages, we find that realignment affects all the layers but can be the most detrimental to the lower ones. Freezing the lower layers can prevent performance degradation. Particularly, AlignFreeze improves Part-of-Speech (PoS) tagging performances in languages where full realignment fails: with XLM-R, it provides improvements of more than one standard deviation in accuracy in seven more languages than full realignment.</abstract>
      <url hash="b3d54fe3">2025.naacl-short.48</url>
      <bibkey>bakos-etal-2025-alignfreeze</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>FLIQA</fixed-case>-<fixed-case>AD</fixed-case>: a Fusion Model with Large Language Model for Better Diagnose and <fixed-case>MMSE</fixed-case> Prediction of <fixed-case>A</fixed-case>lzheimer’s Disease</title>
      <author><first>Junhao</first><last>Chen</last></author>
      <author><first>Zhiyuan</first><last>Ding</last></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Xiangzhu</first><last>Zeng</last><affiliation>Peking University</affiliation></author>
      <author><first>Ling</first><last>Wang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <pages>587-594</pages>
      <abstract>Tracking a patient’s cognitive status early in the onset of the disease provides an opportunity to diagnose and intervene in Alzheimer’s disease (AD). However, relying solely on magnetic resonance imaging (MRI) images with traditional classification and regression models may not fully extract finer-grained information. This study proposes a multi-task Fusion Language Image Question Answering model (FLIQA-AD) to perform AD identification and Mini Mental State Examination (MMSE) prediction. Specifically, a 3D Adapter is introduced in Vision Transformer (ViT) model for image feature extraction. The patient electronic health records (EHR) information and questions related to the disease work as text prompts to be encoded. Then, an ADFormer model, which combines self-attention and cross-attention mechanisms, is used to capture the correlation between EHR information and structure features. After that, the extracted brain structural information and textual content are combined as input sequences for the large language model (LLM) to identify AD and predict the corresponding MMSE score. Experimental results demonstrate the strong discrimination and MMSE prediction performance of the model, as well as question-answer capabilities.</abstract>
      <url hash="f55312fe">2025.naacl-short.49</url>
      <bibkey>chen-etal-2025-fliqa</bibkey>
    </paper>
    <paper id="50">
      <title>Transform Retrieval for Textual Entailment in <fixed-case>RAG</fixed-case></title>
      <author><first>Quan</first><last>Guo</last><affiliation>Guangxi Minzu University</affiliation></author>
      <author><first>Xin</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <pages>595-599</pages>
      <abstract>In this paper, we introduce Transform Retrieval, a novel approach aimed at improving Textual Entailment Retrieval within the framework of Retrieval-Augmented Generation (RAG). While RAG has shown promise in enhancing Large Language Models by retrieving relevant documents to extract specific knowledge or mitigate hallucination, current retrieval methods often prioritize relevance without ensuring the retrieved documents semantically support answering the queries. Transform Retrieval addresses this gap by transforming query embeddings to better align with semantic entailment without re-encoding the document corpus. We achieve this by using a transform model and employing a contrastive learning strategy to optimize the alignment between transformed query embeddings and document embeddings for better entailment.We evaluated the framework using BERT as frozen pre-trained encoder and compared it with a fully fine-tuned skyline model. Experimental results show that Transform Retrieval with simple MLP consistently approaches the skyline across multiple datasets, demonstrating the method’s effectiveness. The high performance on HotpotQA highlights its strength in many-to-many retrieval scenarios.</abstract>
      <url hash="b8d65b98">2025.naacl-short.50</url>
      <bibkey>guo-liang-2025-transform</bibkey>
    </paper>
    <paper id="51">
      <title>How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations</title>
      <author><first>Hyunji</first><last>Lee</last></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Supriti</first><last>Sinhamahapatra</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>600-610</pages>
      <abstract>Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches’ effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.</abstract>
      <url hash="3f3f5bb4">2025.naacl-short.51</url>
      <bibkey>lee-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="52">
      <title>Explore the Reasoning Capability of <fixed-case>LLM</fixed-case>s in the Chess Testbed</title>
      <author><first>Shu</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Lei</first><last>Ji</last></author>
      <author><first>Renxi</first><last>Wang</last></author>
      <author><first>Wenxiao</first><last>Zhao</last></author>
      <author><first>Haokun</first><last>Liu</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Yifan</first><last>Hou</last><affiliation>Peking University</affiliation></author>
      <author><first>Ying Nian</first><last>Wu</last><affiliation>UCLA</affiliation></author>
      <pages>611-622</pages>
      <abstract>Reasoning is a central capability of human intelligence. In recent years, with the advent of large-scale datasets, pretrained large language models have emerged with new capabilities, including reasoning. However, these models still struggle with long-term, complex reasoning tasks, such as playing chess. Based on the observation that expert chess players employ a dual approach combining long-term strategic play with short-term tactical play along with language explanation, we propose improving the reasoning capability of large language models in chess by integrating annotated strategy and tactic. Specifically, we collect a dataset named MATE, which consists of 1 million chess positions with candidate moves annotated for strategy and tactics. We finetune the LLaMA-3-8B model and compare it against state-of-the-art commercial language models in the task of selecting better chess moves. Our experiments show that our models perform better than GPT, Claude, and Gemini models. We find that language explanations can enhance the reasoning capability of large language models.</abstract>
      <url hash="c1ea2591">2025.naacl-short.52</url>
      <bibkey>wang-etal-2025-explore</bibkey>
    </paper>
    <paper id="53">
      <title>Auto-Cypher: Improving <fixed-case>LLM</fixed-case>s on Cypher generation via <fixed-case>LLM</fixed-case>-supervised generation-verification framework</title>
      <author><first>Aman</first><last>Tiwari</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Shiva Krishna Reddy</first><last>Malay</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Masoud</first><last>Hashemi</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Sathwik Tejaswi</first><last>Madhusudhan</last><affiliation>ServiceNow Inc</affiliation></author>
      <pages>623-640</pages>
      <abstract>Graph databases like Neo4j are gaining popularity for handling complex, interconnected data, over traditional relational databases in modeling and querying relationships. While translating natural language into SQL queries is well-researched, generating Cypher queries for Neo4j remains relatively underexplored. In this work, we present an automated, LLM Supervised, pipeline to generate high quality synthetic data for Text2Cypher. Our Cypher data generation pipeline introduces LLM-As-Database-Filler, a novel strategy for ensuring Cypher query correctness, thus resulting in high quality generations. Using our pipeline, we generate high quality Text2Cypher data - SynthCypher containing 29.8k instances across various domains and queries with varying complexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and QWEN7B on SynthCypher results in performance gains of up to 40% on the Text2Cypher test split and 30% on the SPIDER benchmark, adapted for graph databases.</abstract>
      <url hash="0e76de1e">2025.naacl-short.53</url>
      <bibkey>tiwari-etal-2025-auto</bibkey>
    </paper>
    <paper id="54">
      <title>Leveraging Moment Injection for Enhanced Semi-supervised Natural Language Inference with Large Language Models</title>
      <author><first>Seo Yeon</first><last>Park</last><affiliation>Hanyang University</affiliation></author>
      <pages>641-648</pages>
      <abstract>Natural Language Inference (NLI) is crucial for evaluating models’ Natural Language Understanding (NLU) and reasoning abilities. The development of NLI, in part, has been driven by the creation of large datasets, which require significant human effort. This has spurred interest in semi-supervised learning (SSL) that leverages both labeled and unlabeled data. However, the absence of hypotheses and class labels in NLI tasks complicates SSL. Prior work has used class-specific fine-tuned large language models (LLMs) to generate hypotheses and assign pseudo-labels but discarded many LLM-constructed samples during training to ensure the quality. In contrast, we propose to leverage all LLM-constructed samples by handling potentially noisy samples by injecting the moments of labeled samples during training to properly adjust the level of noise. Our method outperforms strong baselines on multiple NLI datasets in low-resource settings.</abstract>
      <url hash="52189a38">2025.naacl-short.54</url>
      <bibkey>park-2025-leveraging</bibkey>
    </paper>
    <paper id="55">
      <title>A Fair Comparison without Translationese: <fixed-case>E</fixed-case>nglish vs. Target-language Instructions for Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Taisei</first><last>Enomoto</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Hwichan</first><last>Kim</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Zhousi</first><last>Chen</last></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Hitotsubashi University</affiliation></author>
      <pages>649-670</pages>
      <abstract>Most large language models are multilingual instruction executors. Prior studies suggested that English instructions are more effective than target-language instructions even for non-English tasks; however, these studies often use datasets and instructions translated from English, which introduce biases known as translationese, hindering an unbiased comparison. To address this issue, we conduct a fair comparison between English and target-language instructions by eliminating translationese effects. Contrary to previous studies, our experiments across several tasks reveal that the advantage of adopting English instructions is not overwhelming. Additionally, we report on the features of generated texts and the instruction-following abilities when using respective instructions.</abstract>
      <url hash="a0e8627d">2025.naacl-short.55</url>
      <bibkey>enomoto-etal-2025-fair</bibkey>
    </paper>
    <paper id="56">
      <title>Evaluating Multimodal Generative <fixed-case>AI</fixed-case> with <fixed-case>K</fixed-case>orean Educational Standards</title>
      <author><first>Sanghee</first><last>Park</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Geewook</first><last>Kim</last><affiliation>University of Seoul, NAVER Cloud and KAIST</affiliation></author>
      <pages>671-688</pages>
      <abstract>This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models—open-source, open-access, and closed APIs—by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-source.</abstract>
      <url hash="ac2e9acd">2025.naacl-short.56</url>
      <bibkey>park-kim-2025-evaluating</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>S</fixed-case>cratch<fixed-case>E</fixed-case>val: Are <fixed-case>GPT</fixed-case>-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges</title>
      <author><first>Rao</first><last>Fu</last></author>
      <author><first>Ziyang</first><last>Luo</last><affiliation>Salesforce Research and Hong Kong Baptist University</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Zhen</first><last>Ye</last></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>689-699</pages>
      <abstract>Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based visual programming language widely used in children’s programming education. By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both visual information and code structure, thereby comprehensively evaluating its programming intent understanding ability. Our evaluation approach goes beyond the traditional image-to-code mapping and focuses on unified logical thinking and problem-solving abilities, providing a more comprehensive and challenging framework for evaluating the visual programming ability of LMMs. ScratchEval not only fills the gap in existing evaluation methods, but also provides new insights for the future development of LMMs in the field of visual programming.</abstract>
      <url hash="03f34775">2025.naacl-short.57</url>
      <bibkey>fu-etal-2025-scratcheval</bibkey>
    </paper>
    <paper id="58">
      <title>Interpret and Control Dense Retrieval with Sparse Latent Features</title>
      <author><first>Hao</first><last>Kang</last></author>
      <author><first>Tevin</first><last>Wang</last></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>700-709</pages>
      <abstract>Dense embeddings deliver strong retrieval performance but often lack interpretability and controllability. This paper introduces a novel approach using sparse autoencoders (SAE) to interpret and control dense embeddings via the learned latent sparse features. Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks and thus meaningful to interpret. Experimental results demonstrate that both the learned latent sparse features and their reconstructed embeddings retain nearly the same retrieval accuracy as the original dense vectors, affirming their faithfulness. Our further examination of the sparse latent space reveals interesting features underlying the dense embeddings and we can control the retrieval behaviors via manipulating the latent sparse features, for example, prioritizing documents from specific perspectives in the retrieval results.</abstract>
      <url hash="436d004f">2025.naacl-short.58</url>
      <bibkey>kang-etal-2025-interpret</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>DART</fixed-case>: An <fixed-case>AIGT</fixed-case> Detector using <fixed-case>AMR</fixed-case> of Rephrased Text</title>
      <author><first>Hyeonchu</first><last>Park</last></author>
      <author><first>Byungjun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Bugeun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>710-721</pages>
      <abstract>As large language models (LLMs) generate more human-like texts, concerns about the side effects of AI-generated texts (AIGT) have grown. So, researchers have developed methods for detecting AIGT. However, two challenges remain. First, the performance of detecting black-box LLMs is low because existing models focus on probabilistic features. Second, most AIGT detectors have been tested on a single-candidate setting, which assumes that we know the origin of an AIGT and which may deviate from the real-world scenario. To resolve these challenges, we propose DART, which consists of four steps: rephrasing, semantic parsing, scoring, and multiclass classification. We conducted three experiments to test the performance of DART. The experimental result shows that DART can discriminate multiple black-box LLMs without probabilistic features and the origin of AIGT.</abstract>
      <url hash="9fc5d75a">2025.naacl-short.59</url>
      <bibkey>park-etal-2025-dart</bibkey>
    </paper>
    <paper id="60">
      <title>Scaling Graph-Based Dependency Parsing with Arc Vectorization and Attention-Based Refinement</title>
      <author><first>Nicolas</first><last>Floquet</last></author>
      <author><first>Joseph Le</first><last>Roux</last><affiliation>Université Paris 13</affiliation></author>
      <author><first>Nadi</first><last>Tomeh</last><affiliation>Université Sorbonne Paris Nord</affiliation></author>
      <author><first>Thierry</first><last>Charnois</last><affiliation>University of Sorbonne Paris Nord (Paris 13)</affiliation></author>
      <pages>722-734</pages>
      <abstract>We propose a novel architecture for graph-based dependency parsing that explicitly constructs vectors, from which both arcs and labels are scored. Our method addresses key limitations of the standard two-pipeline approach by unifying arc scoring and labeling into a single network, reducing scalability issues caused by the information bottleneck and lack of parameter sharing. Additionally, our architecture overcomes limited arc interactions with transformer layers to efficiently simulate higher-order dependencies. Experiments on PTB and UD show that our model outperforms state-of-the-art parsers in both accuracy and efficiency.</abstract>
      <url hash="4e85c70f">2025.naacl-short.60</url>
      <bibkey>floquet-etal-2025-scaling</bibkey>
    </paper>
    <paper id="61">
      <title>Language Models “Grok” to Copy</title>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent AI Platform</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>735-741</pages>
      <abstract>We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context—a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set. Our experiments yield three arguments: (1) The pre-training loss decreases rapidly, while the context copying ability of models initially lags and then abruptly saturates. (2) The speed of developing copying ability is independent of the number of tokens trained, similarly to how grokking speed is unaffected by dataset size as long as the data distribution is preserved. (3) Induction heads, the attention heads responsible for copying, form from shallow to deep layers during training, mirroring the development of circuits in deeper layers during grokking. We contend that the connection between grokking and context copying can provide valuable insights for more effective language model training, ultimately improving in-context performance. For example, we demonstrated that techniques that enhance grokking, such as regularization, either accelerate or enhance the development of context copying.</abstract>
      <url hash="479c95d2">2025.naacl-short.61</url>
      <bibkey>lv-etal-2025-language</bibkey>
    </paper>
    <paper id="62">
      <title>Evaluating <fixed-case>LLM</fixed-case>s for Quotation Attribution in Literary Texts: A Case Study of <fixed-case>LL</fixed-case>a<fixed-case>M</fixed-case>a3</title>
      <author><first>Gaspard</first><last>Michel</last></author>
      <author><first>Elena V.</first><last>Epure</last><affiliation>Deezer</affiliation></author>
      <author><first>Romain</first><last>Hennequin</last></author>
      <author><first>Christophe</first><last>Cerisara</last><affiliation>University of Lorraine</affiliation></author>
      <pages>742-755</pages>
      <abstract>Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination.We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in English literature. We release publicly our code and data.</abstract>
      <url hash="4fa477b7">2025.naacl-short.62</url>
      <bibkey>michel-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="63">
      <title>Beyond Literal Token Overlap: Token Alignability for Multilinguality</title>
      <author><first>Katharina</first><last>Hämmerl</last></author>
      <author><first>Tomasz</first><last>Limisiewicz</last><affiliation>Meta and University of Washington</affiliation></author>
      <author><first>Jindřich</first><last>Libovický</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>756-767</pages>
      <abstract>Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models. However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality. This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions. In this paper, we propose subword token alignability as a new way to understand the impact and quality of multilingual tokenisation. In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low. We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work. We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future. We publish our code and reproducibility details.</abstract>
      <url hash="b9c7f03d">2025.naacl-short.63</url>
      <bibkey>hammerl-etal-2025-beyond</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>I</fixed-case>dentify<fixed-case>M</fixed-case>e: A Challenging Long-Context Mention Resolution Benchmark for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kawshik</first><last>Manikantan</last></author>
      <author><first>Makarand</first><last>Tapaswi</last><affiliation>International Institute of Information Technology Hyderabad and Wadhwani Institute for Artificial Intelligence</affiliation></author>
      <author><first>Vineet</first><last>Gandhi</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Shubham</first><last>Toshniwal</last><affiliation>NVIDIA</affiliation></author>
      <pages>768-777</pages>
      <abstract>Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models’ referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained model performance analysis. We evaluate both closed- and open-source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.</abstract>
      <url hash="077ebaf2">2025.naacl-short.64</url>
      <bibkey>manikantan-etal-2025-identifyme</bibkey>
    </paper>
    <paper id="65">
      <title>k<fixed-case>NN</fixed-case> Retrieval for Simple and Effective Zero-Shot Multi-speaker Text-to-Speech</title>
      <author><first>Karl El</first><last>Hajal</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Ajinkya</first><last>Kulkarni</last></author>
      <author><first>Enno</first><last>Hermann</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Mathew</first><last>Magimai Doss</last></author>
      <pages>778-786</pages>
      <abstract>While recent zero-shot multi-speaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. Further, SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity. In this study, we introduce kNN-TTS, a simple and effective framework for zero-shot multi-speaker TTS using retrieval methods which leverage the linear relationships between SSL features. Objective and subjective evaluations show that our models, trained on transcribed speech from a single speaker only, achieve performance comparable to state-of-the-art models that are trained on significantly larger training datasets. The low training data requirements mean that kNN-TTS is well suited for the development of multi-speaker TTS systems for low-resource domains and languages. We also introduce an interpolation parameter which enables fine-grained voice morphing. Demo samples are available at https://idiap.github.io/knn-tts .</abstract>
      <url hash="944ccf7c">2025.naacl-short.65</url>
      <bibkey>hajal-etal-2025-knn</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>CORD</fixed-case>: Balancing <fixed-case>CO</fixed-case>nsistency and Rank Distillation for Robust Retrieval-Augmented Generation</title>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Daniel F</first><last>Campos</last><affiliation>Snowflake</affiliation></author>
      <author><first>Filip</first><last>Graliński</last><affiliation>Snowflake and Adam Mickiewicz University</affiliation></author>
      <author><first>Zhewei</first><last>Yao</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yuxiong</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>787-796</pages>
      <abstract>With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation: CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks.</abstract>
      <url hash="3f796202">2025.naacl-short.66</url>
      <bibkey>lee-etal-2025-cord</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>G</fixed-case>raph<fixed-case>LSS</fixed-case>: Integrating Lexical, Structural, and Semantic Features for Long Document Extractive Summarization</title>
      <author><first>Margarita</first><last>Bugueño</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Hazem Abou</first><last>Hamdan</last></author>
      <author><first>Gerard</first><last>De Melo</last><affiliation>Hasso Plattner Institute and University of Potsdam</affiliation></author>
      <pages>797-804</pages>
      <abstract>Heterogeneous graph neural networks have recently gained attention for long document summarization, modeling the extraction as a node classification task. Although effective, these models often require external tools or additional machine learning models to define graph components, producing highly complex and less intuitive structures. We present GraphLSS, a heterogeneous graph construction for long document extractive summarization, incorporating Lexical, Structural, and Semantic features. It defines two levels of information (words and sentences) and four types of edges (sentence semantic similarity, sentence occurrence order, word in sentence, and word semantic similarity) without any need for auxiliary learning models. Experiments on two benchmark datasets show that GraphLSS is competitive with top-performing graph-based methods, outperforming recent non-graph models. We release our code on GitHub.</abstract>
      <url hash="231dbcdc">2025.naacl-short.67</url>
      <bibkey>bugueno-etal-2025-graphlss</bibkey>
    </paper>
    <paper id="68">
      <title>Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Ivana</first><last>Hacajova</last></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>805-816</pages>
      <abstract>Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an iterative FV system on three medical fact-checking datasets and evaluate it with multiple settings, including different LLMs, external web search, and structured reasoning using logic predicates. We demonstrate improvements in the final performance over traditional approaches and the high potential of step-by-step FV systems for domain-specific claims.</abstract>
      <url hash="ad7c8872">2025.naacl-short.68</url>
      <bibkey>vladika-etal-2025-step</bibkey>
    </paper>
    <paper id="69">
      <title>Developing multilingual speech synthesis system for <fixed-case>O</fixed-case>jibwe, Mi’kmaq, and Maliseet</title>
      <author><first>Shenran</first><last>Wang</last></author>
      <author><first>Changbing</first><last>Yang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Michael l</first><last>Parkhill</last></author>
      <author><first>Chad</first><last>Quinn</last></author>
      <author><first>Christopher</first><last>Hammerly</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Jian</first><last>Zhu</last><affiliation>University of British Columbia</affiliation></author>
      <pages>817-826</pages>
      <abstract>We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi’kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research provides technical development to language revitalization for low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation.</abstract>
      <url hash="a6570d5b">2025.naacl-short.69</url>
      <bibkey>wang-etal-2025-developing</bibkey>
    </paper>
    <paper id="70">
      <title>Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts</title>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Maximillian</first><last>Chen</last></author>
      <author><first>Siyan</first><last>Li</last></author>
      <author><first>Arpit</first><last>Sharma</last><affiliation>Walmart Inc.</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>827-844</pages>
      <abstract>Training conversational question-answering (QA) systems demands a substantial amount of in-domain data, which is often scarce in practice. A common solution to this challenge is to generate synthetic data. Traditional methods typically follow a top-down approach, where a large language model (LLM) generates multi-turn dialogues from a broad prompt. While this method produces coherent conversations, it offers limited fine-grained control over the content and is susceptible to hallucinations. We introduce a bottom-up conversation synthesis approach, where QA pairs are generated first and then combined into a coherent dialogue. This method offers greater control and precision by dividing the process into two distinct steps, enabling refined instructions and validations to be handled separately. Additionally, this structure allows the use of non-local models in stages that do not involve proprietary knowledge, enhancing the overall quality of the generated data. Both human and automated evaluations demonstrate that our approach produces more realistic and higher-quality dialogues compared to top-down methods.</abstract>
      <url hash="88f8f9c5">2025.naacl-short.70</url>
      <bibkey>qian-etal-2025-bottom</bibkey>
    </paper>
    <paper id="71">
      <title>Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Huaman</first><last>Sun</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Jiaxin</first><last>Pei</last><affiliation>Stanford University</affiliation></author>
      <author><first>Minje</first><last>Choi</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>845-854</pages>
      <abstract>Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large LanguageModels (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differencesin subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their abilityto understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models’ predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task. Furthermore, sociodemographic prompting does not consistently improve and, in some cases, worsens LLMs’ ability to perceive language from specific sub-populations. These findings highlight potential demographic biases in LLMs when performing subjective judgment tasks and underscore the limitations of sociodemographic prompting as a strategy to achieve pluralistic alignment. Code and data are available at: https://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge.</abstract>
      <url hash="e0f78a05">2025.naacl-short.71</url>
      <bibkey>sun-etal-2025-sociodemographic</bibkey>
    </paper>
    <paper id="72">
      <title>Identifying Power Relations in Conversations using Multi-Agent Social Reasoning</title>
      <author><first>Zhaoqing</first><last>Wu</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Leora</first><last>Morgenstern</last><affiliation>SRI International</affiliation></author>
      <pages>855-865</pages>
      <abstract>Large language models (LLMs) struggle in social science domains, where critical thinking and human-level inference are crucial. In this work, we propose a multi-agent social reasoning framework that leverages the generative and reasoning capabilities of LLMs to generate and evaluate reasons from multiple perspectives grounded in social science theories, and construct a factor graph for inference. Experimental results on understanding power dynamics in conversations show that our method outperforms standard prompting baselines, demonstrating its potential for tackling hard Computational Social Science (CSS) tasks.</abstract>
      <url hash="19ebba95">2025.naacl-short.72</url>
      <bibkey>wu-etal-2025-identifying</bibkey>
    </paper>
    <paper id="73">
      <title>Examining <fixed-case>S</fixed-case>panish Counseling with <fixed-case>MIDAS</fixed-case>: a Motivational Interviewing Dataset in <fixed-case>S</fixed-case>panish</title>
      <author><first>Aylin Ece</first><last>Gunal</last></author>
      <author><first>Bowen</first><last>Yi</last></author>
      <author><first>John D.</first><last>Piette</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Veronica</first><last>Perez-Rosas</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>866-872</pages>
      <abstract>Cultural and language factors significantly influence counseling, but Natural Language Processing research has not yet examined whether the findings of conversational analysis for counseling conducted in English apply to other languages. This paper presents a first step towards this direction. We introduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling dataset created from public video sources that contains expert annotations for counseling reflections and questions. Using this dataset, we explore language-based differences in counselor behavior in English and Spanish and develop classifiers in monolingual and multilingual settings, demonstrating its applications in counselor behavioral coding tasks.</abstract>
      <url hash="c80196e8">2025.naacl-short.73</url>
      <bibkey>gunal-etal-2025-examining</bibkey>
    </paper>
    <paper id="74">
      <title>Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes</title>
      <author><first>Isabel O.</first><last>Gallegos</last><affiliation>Stanford University</affiliation></author>
      <author><first>Ryan</first><last>Aponte</last></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Joe</first><last>Barrow</last><affiliation>Pattern Data</affiliation></author>
      <author><first>Mehrab</first><last>Tanjim</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Hanieh</first><last>Deilamsalehy</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Nedim</first><last>Lipka</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Deonna</first><last>Owens</last></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <pages>873-888</pages>
      <abstract>Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.</abstract>
      <url hash="cf78c2cb">2025.naacl-short.74</url>
      <bibkey>gallegos-etal-2025-self</bibkey>
    </paper>
    <paper id="75">
      <title><fixed-case>E</fixed-case>qualize<fixed-case>IR</fixed-case>: Mitigating Linguistic Biases in Retrieval Models</title>
      <author><first>Jiali</first><last>Cheng</last></author>
      <author><first>Hadi</first><last>Amiri</last><affiliation>University of Massachusetts Lowell</affiliation></author>
      <pages>889-898</pages>
      <abstract>This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries.To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a <i>linguistically biased</i> weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in data. We propose four approaches for developing linguistically-biased models. Extensive experiments on several datasets show that our method reduces performance disparities across linguistically simple and complex queries, while improving overall retrieval performance.</abstract>
      <url hash="4685f407">2025.naacl-short.75</url>
      <bibkey>cheng-amiri-2025-equalizeir</bibkey>
    </paper>
    <paper id="76">
      <title>Do Audio-Language Models Understand Linguistic Variations?</title>
      <author><first>Ramaneswaran</first><last>Selvakumar</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Hemant Kumar</first><last>Giri</last></author>
      <author><first>Nishit</first><last>Anand</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Ashish</first><last>Seth</last></author>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>899-913</pages>
      <abstract>Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are treated as different views of the same audio scene and use this for training. Our proposed approach improves the text-to-audio retrieval performance of CLAP by 0.8%-13% across benchmarks and enhances robustness to linguistic variation. We make our code publicly available</abstract>
      <url hash="9ac30ed8">2025.naacl-short.76</url>
      <bibkey>selvakumar-etal-2025-audio</bibkey>
    </paper>
    <paper id="77">
      <title>Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained Decoding for Automatic Post-Editing</title>
      <author><first>Sourabh</first><last>Deoghare</last></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>914-925</pages>
      <abstract>Automatic Post-Editing (APE) systems often struggle with over-correction, where unnecessary modifications are made to a translation, diverging from the principle of minimal editing. In this paper, we propose a novel technique to mitigate over-correction by incorporating word-level Quality Estimation (QE) information during the decoding process. This method is architecture-agnostic, making it adaptable to any APE system, regardless of the underlying model or training approach. Our experiments on English-German, English-Hindi, and English-Marathi language pairs show the proposed approach yields significant improvements over their corresponding baseline APE systems, with TER gains of 0.65, 1.86, and 1.44 points, respectively. These results underscore the complementary relationship between QE and APE tasks and highlight the effectiveness of integrating QE information to reduce over-correction in APE systems.</abstract>
      <url hash="ea8c6bc9">2025.naacl-short.77</url>
      <bibkey>deoghare-etal-2025-giving</bibkey>
    </paper>
    <paper id="78">
      <title><fixed-case>R</fixed-case>ule<fixed-case>R</fixed-case>: Improving <fixed-case>LLM</fixed-case> Controllability by Rule-based Data Recycling</title>
      <author><first>Ming</first><last>Li</last></author>
      <author><first>Han</first><last>Chen</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Dang</first><last>Nguyen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Dianqi</first><last>Li</last><affiliation>Citadel Securities</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>926-943</pages>
      <abstract>Large language models (LLMs) still lack delicate controllability over their responses, which is critical to enhancing their performance and the user experience. However, curating supervised fine-tuning (SFT) datasets to improve LLM controllability usually relies on human experts or proprietary LLMs, which requires additional costs. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a data augmentation method incorporating multiple constraints into the original data samples according to predefined rules, which creates new training tasks to consolidate the controllability of LLMs. Instead of creating new data from scratch, RuleR “recycles” existing data by simply applying rule-based edits to their responses and appending the rule-instructions in their original instructions. Experimental results demonstrate RuleR’s effectiveness in improving LLM controllability while maintaining general instruction-following capabilities.</abstract>
      <url hash="473b830b">2025.naacl-short.78</url>
      <bibkey>li-etal-2025-ruler</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>M</fixed-case>ix<fixed-case>R</fixed-case>ev<fixed-case>D</fixed-case>etect: Towards Detecting <fixed-case>AI</fixed-case>-Generated Content in Hybrid Peer Reviews.</title>
      <author><first>Sandeep</first><last>Kumar</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Samarth</first><last>Garg</last></author>
      <author><first>Sagnik</first><last>Sengupta</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last><affiliation>Oak Ridge National Laboratory</affiliation></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>944-953</pages>
      <abstract>The growing use of large language models (LLMs) in academic peer review poses significant challenges, particularly in distinguishing AI-generated content from human-written feedback. This research addresses the problem of identifying AI-generated peer review comments, which are crucial to maintaining the integrity of scholarly evaluation. Prior research has primarily focused on generic AI-generated text detection or on estimating the fraction of peer reviews that may be AI-generated, often treating reviews as monolithic units. However, these methods fail to detect finer-grained AI-generated points within mixed-authorship reviews. To address this gap, we propose MixRevDetect, a novel method to identify AI-generated points in peer reviews. Our approach achieved an F1 score of 88.86%, significantly outperforming existing AI text detection methods.</abstract>
      <url hash="2574298d">2025.naacl-short.79</url>
      <bibkey>kumar-etal-2025-mixrevdetect</bibkey>
    </paper>
    <paper id="80">
      <title><fixed-case>D</fixed-case>isco<fixed-case>G</fixed-case>ra<fixed-case>MS</fixed-case>: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph</title>
      <author><first>Maitreya Prafulla</first><last>Chitale</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Uday</first><last>Bindal</last></author>
      <author><first>Rajakrishnan P</first><last>Rajkumar</last><affiliation>IISER Bhopal</affiliation></author>
      <author><first>Rahul</first><last>Mishra</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>954-965</pages>
      <abstract>Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the “lost in the middle” issue. To address these challenges, we introduce DiscoGraMS, a novel resource that represents movie scripts as a movie character-aware discourse graph (CaD Graph). This approach is well-suited for various downstream tasks, such as summarization, question-answering, and salience detection. The model aims to preserve all salient information, offering a more comprehensive and faithful representation of the screenplay’s content. We further explore a baseline method that combines the CaD Graph with the corresponding movie script through a late fusion of graph and text modalities, and we present very initial promising results. We have made our code and dataset publicly available.</abstract>
      <url hash="463e9be9">2025.naacl-short.80</url>
      <bibkey>chitale-etal-2025-discograms</bibkey>
    </paper>
    <paper id="81">
      <title>Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm</title>
      <author><first>Vasudha</first><last>Varadarajan</last></author>
      <author><first>Syeda</first><last>Mahwish</last></author>
      <author><first>Xiaoran</first><last>Liu</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Julia</first><last>Buffolino</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Christian</first><last>Luhmann</last><affiliation>SUNY at Stony Brook</affiliation></author>
      <author><first>Ryan L.</first><last>Boyd</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>966-979</pages>
      <abstract>While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic style of an individual talking about a recent decision they made. The participants then follow a classical decision-making experiment that captures their cognitive style, determined by how preferences change during a decision exercise. We find that language features, intended to capture cognitive style, can predict participants’ decision style with moderate-to-high accuracy (AUC 0.8), demonstrating that cognitive style can be partly captured and revealed by discourse patterns.</abstract>
      <url hash="d75966f1">2025.naacl-short.81</url>
      <bibkey>varadarajan-etal-2025-capturing</bibkey>
    </paper>
  </volume>
  <volume id="industry" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track)</booktitle>
      <editor><first>Weizhu</first><last>Chen</last></editor>
      <editor><first>Yi</first><last>Yang</last></editor>
      <editor><first>Mohammad</first><last>Kachuee</last></editor>
      <editor><first>Xue-Yong</first><last>Fu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>April</month>
      <year>2025</year>
      <url hash="16eb6006">2025.naacl-industry</url>
      <venue>naacl</venue>
      <isbn>979-8-89176-194-0</isbn>
    </meta>
    <frontmatter>
      <url hash="0e371bb6">2025.naacl-industry.0</url>
      <bibkey>naacl-2025-industry</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Understanding <fixed-case>LLM</fixed-case> Development Through Longitudinal Study: Insights from the Open <fixed-case>K</fixed-case>o-<fixed-case>LLM</fixed-case> Leaderboard</title>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonwoo</first><last>Kim</last></author>
      <pages>1-8</pages>
      <abstract>This paper conducts a longitudinal study over eleven months to address the limitations of prior research on the Open Ko-LLM Leaderboard, which have relied on empirical studies with restricted observation periods of only five months. By extending the analysis duration, we aim to provide a more comprehensive understanding of the progression in developing Korean large language models (LLMs). Our study is guided by three primary research questions: (1) What are the specific challenges in improving LLM performance across diverse tasks on the Open Ko-LLM Leaderboard over time? (2) How does model size impact task performance correlations across various benchmarks? (3) How have the patterns in leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By analyzing 1,769 models over this period, our research offers a comprehensive examination of the ongoing advancements in LLMs and the evolving nature of evaluation frameworks.</abstract>
      <url hash="92bd0297">2025.naacl-industry.1</url>
      <bibkey>park-kim-2025-understanding</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>RTSM</fixed-case>: Knowledge Distillation with Diverse Signals for Efficient Real-Time Semantic Matching in <fixed-case>E</fixed-case>-Commerce</title>
      <author><first>Sanjay</first><last>Agrawal</last><affiliation>Amazon</affiliation></author>
      <author><first>Vivek</first><last>Sembium</last><affiliation>Amazon</affiliation></author>
      <pages>9-19</pages>
      <abstract>Semantic matching plays a pivotal role in e-commerce by facilitating better product discovery and driving sales within online stores. Transformer models have proven exceptionally effective in mapping queries to an embedding space, positioning semantically related entities (queries or products) in close proximity. Despite their effectiveness, the high computational demands of large transformer models pose challenges for their deployment in real-time scenarios. This paper presents RTSM, an advanced knowledge distillation framework designed for Real-Time Semantic Matching. Our approach develops accurate, low-latency student models by leveraging both soft labels from a teacher model and ground truth generated from pairwise query-product and query-query signals. These signals are sourced from direct audits, synthetic examples created by LLMs, user interaction data, and taxonomy-based datasets, with custom loss functions enhancing learning efficiency. Experimental evaluations on internal and external e-commerce datasets demonstrate a 2-2.5% increase in ROC-AUC compared to directly trained student models, outperforming both the teacher model and state-of-the-art knowledge distillation benchmarks.</abstract>
      <url hash="b4dc116b">2025.naacl-industry.2</url>
      <bibkey>agrawal-sembium-2025-rtsm</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>W</fixed-case>ork<fixed-case>T</fixed-case>eam: Constructing Workflows from Natural Language with Multi-Agents</title>
      <author><first>Hanchao</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Rongjun</first><last>Li</last></author>
      <author><first>Weimin</first><last>Xiong</last></author>
      <author><first>Ziyu</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Peng</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>20-35</pages>
      <abstract>Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.</abstract>
      <url hash="ff476353">2025.naacl-industry.3</url>
      <bibkey>liu-etal-2025-workteam</bibkey>
    </paper>
    <paper id="4">
      <title>How <fixed-case>LLM</fixed-case>s React to Industrial Spatio-Temporal Data? Assessing Hallucination with a Novel Traffic Incident Benchmark Dataset</title>
      <author><first>Qiang</first><last>Li</last><affiliation>Accenture</affiliation></author>
      <author><first>Mingkun</first><last>Tan</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Xun</first><last>Zhao</last></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last></author>
      <author><first>Daoan</first><last>Zhang</last></author>
      <author><first>Shengzhao</first><last>Lei</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Anderson S.</first><last>Chu</last></author>
      <author><first>Lujun</first><last>Li</last></author>
      <author><first>Porawit</first><last>Kamnoedboon</last><affiliation>University of Zurich</affiliation></author>
      <pages>36-53</pages>
      <abstract>Large language models (LLMs) hold revolutionary potential to digitize and enhance the Health &amp; Public Services (H&amp;PS) industry. Despite their advanced linguistic abilities, concerns about accuracy, stability, and traceability still persist, especially in high-stakes areas such as transportation systems. Moreover, the predominance of English in LLM development raises questions about how they perform in non-English contexts. This study originated from a real world industrial GenAI application, introduces a novel cross-lingual benchmark dataset comprising nearly 99,869 real traffic incident records from Vienna (2013-2023) to assess the robustness of state-of-the-art LLMs (<tex-math>\geq</tex-math> 9) in the spatio vs temporal domain for traffic incident classification. We then explored three hypotheses — sentence indexing, date-to-text conversion, and German-to-English translation — and incorporated Retrieval Augmented Generation (RAG) to further examine the LLM hallucinations in both spatial and temporal domain. Our experiments reveal significant performance disparities in the spatio-temporal domain and demonstrate what types of hallucinations that RAG can mitigate and how it achieves this. We also provide open access to our H&amp;PS traffic incident dataset, with the project demo and code available at Website <url>https://sites.google.com/view/llmhallucination/home</url></abstract>
      <url hash="7656c1e7">2025.naacl-industry.4</url>
      <bibkey>li-etal-2025-llms</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>S</fixed-case>ql: Pure Fine-Tuning and Pure Knowledge Distillation</title>
      <author><first>Gao yu</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Shao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xichou</first><last>Zhu</last></author>
      <author><first>Lei</first><last>Yu</last></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>54-61</pages>
      <abstract>Text2Sql is a task that converts natural language questions into SQL queries. In previous research on LLM fine-tuning, researchers typically input both the entire database schema and the natural language question into the model. This approach has two issues: 1) the model’s context is limited when dealing with a large number of database tables; 2) the question is often related to only a few tables, leading to excessive irrelevant information that distracts the model. To address these issues, we employed pure fine-tuning strategy to reduce redundancy. The model fine-tuned with pure prompts, using prompts that are only 53% of the baseline length, outperforms the baseline (fine-tuned with all tables in the prompt) by 8.2% and 8.6% in Test-suite accuracy (TS) and exact-set-match accuracy (EM), respectively, on the Spider dev set. Under the most refined Spider dev set of prompts, the model achieves TS and EM scores of 73.5% and 75.4%, respectively, approaching state-of-the-art (SOTA) levels. To leverage the capabilities of the model with pure prompts, we applied pure knowledge distillation strategy to transfer its abilities. The distilled student model achieved a 1.9% improvement in TS, while the teacher model’s prompt length was only 23% of that of the student model.</abstract>
      <url hash="41e189e1">2025.naacl-industry.5</url>
      <bibkey>zhu-etal-2025-text2sql</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>M</fixed-case>o<fixed-case>EM</fixed-case>o<fixed-case>E</fixed-case>: Question Guided Dense and Scalable Sparse Mixture-of-Expert for Multi-source Multi-modal Answering</title>
      <author><first>Vinay Kumar</first><last>Verma</last><affiliation>Amazon</affiliation></author>
      <author><first>Shreyas Sunil</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <author><first>Happy</first><last>Mittal</last><affiliation>Amazon</affiliation></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <pages>62-69</pages>
      <abstract>Question Answering (QA) and Visual Question Answering (VQA) are well-studied problems in the language and vision domain. One challenging scenario involves multiple sources of information, each of a different modality, where the answer to the question may exist in one or more sources. This scenario contains richer information but is highly complex to handle. In this work, we formulate a novel question-answer generation (QAG) framework in an environment containing multi-source, multimodal information. The answer may belong to any or all sources; therefore, selecting the most prominent answer source or an optimal combination of all sources for a given question is challenging. To address this issue, we propose a question-guided attention mechanism that learns attention across multiple sources and decodes this information for robust and unbiased answer generation. To learn attention within each source, we introduce an explicit alignment between questions and various information sources, which facilitates identifying the most pertinent parts of the source information relative to the question. Scalability in handling diverse questions poses a challenge. We address this by extending our model to a sparse mixture-of-experts (sparse-MoE) framework, enabling it to handle thousands of question types. Experiments on T5 and Flan-T5 using three datasets demonstrate the model’s efficacy, supported by ablation studies.</abstract>
      <url hash="a3d83fb9">2025.naacl-industry.6</url>
      <bibkey>verma-etal-2025-moemoe</bibkey>
    </paper>
    <paper id="7">
      <title>Finding-Centric Structuring of <fixed-case>J</fixed-case>apanese Radiology Reports and Analysis of Performance Gaps for Multiple Facilities</title>
      <author><first>Yuki</first><last>Tagawa</last><affiliation>FUJIFILM</affiliation></author>
      <author><first>Yohei</first><last>Momoki</last></author>
      <author><first>Norihisa</first><last>Nakano</last><affiliation>富士フイルム株式会社</affiliation></author>
      <author><first>Ryota</first><last>Ozaki</last><affiliation>FUJIFILM</affiliation></author>
      <author><first>Motoki</first><last>Taniguchi</last><affiliation>Fujifilm Corporation</affiliation></author>
      <author><first>Masatoshi</first><last>Hori</last><affiliation>Osaka University Graduate School of Medicine</affiliation></author>
      <author><first>Noriyuki</first><last>Tomiyama</last><affiliation>Osaka University Graduate School of Medicine</affiliation></author>
      <pages>70-85</pages>
      <abstract>This study addresses two key challenges in structuring radiology reports: the lack of a practical structuring schema and datasets to evaluate model generalizability. To address these challenges, we propose a “Finding-Centric Structuring,” which organizes reports around individual findings, facilitating secondary use. We also construct JRadFCS, a large-scale dataset with annotated named entities (NEs) and relations, comprising 8,428 Japanese Computed Tomography (CT) reports from seven facilities, providing a comprehensive resource for evaluating model generalizability. Our experiments reveal performance gaps when applying models trained on single-facility reports to those from other facilities. We further analyze factors contributing to these gaps and demonstrate that augmenting the training set based on these performance-correlated factors can efficiently enhance model generalizability.</abstract>
      <url hash="ddf1da81">2025.naacl-industry.7</url>
      <bibkey>tagawa-etal-2025-finding</bibkey>
    </paper>
    <paper id="8">
      <title>Learning <fixed-case>LLM</fixed-case> Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings</title>
      <author><first>Xuanqing</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Luyang</first><last>Kong</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Niu</last><affiliation>AWS</affiliation></author>
      <author><first>Afshin</first><last>Khashei</last></author>
      <author><first>Belinda</first><last>Zeng</last><affiliation>Amazon</affiliation></author>
      <author><first>Steve</first><last>Johnson</last></author>
      <author><first>Jon</first><last>Jay</last><affiliation>Amazon</affiliation></author>
      <author><first>Davor</first><last>Golac</last></author>
      <author><first>Matt</first><last>Pope</last></author>
      <pages>86-98</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities in handling complex dialogue tasks without requiring use case-specific fine-tuning. However, analyzing live dialogues in real-time necessitates low-latency processing systems, making it impractical to deploy models with billions of parameters due to latency constraints. As a result, practitioners often prefer smaller models with millions of parameters, trained on high-quality, human-annotated datasets. Yet, curating such datasets is both time-consuming and costly. Consequently, there is a growing need to combine the scalability of LLM-generated labels with the precision of human annotations, enabling fine-tuned smaller models to achieve both higher speed and accuracy comparable to larger models. In this paper, we introduce a simple yet effective framework to address this challenge. Our approach is specifically designed for per-utterance classification problems, which encompass tasks such as intent detection, dialogue state tracking, and more. To mitigate the impact of labeling errors from LLMs – the primary source of inaccuracies in student models – we propose a noise-reduced preference learning loss. Experimental results demonstrate that our method significantly improves accuracy across utterance-level dialogue tasks, including sentiment detection (over 2%), dialogue act classification (over 1.5%), etc.</abstract>
      <url hash="fd6a6f51">2025.naacl-industry.8</url>
      <bibkey>liu-etal-2025-learning-llm</bibkey>
    </paper>
    <paper id="9">
      <title>Enhancing Function-Calling Capabilities in <fixed-case>LLM</fixed-case>s: Strategies for Prompt Formats, Data Integration, and Multilingual Translation</title>
      <author><first>Yi-Chang</first><last>Chen</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Po-Chun</first><last>Hsu</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Chan-Jan</first><last>Hsu</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Da-shan</first><last>Shiu</last></author>
      <pages>99-111</pages>
      <abstract>Large language models (LLMs) have significantly advanced autonomous agents, particularly in zero-shot tool usage, also known as function calling. This research delves into enhancing the function-calling capabilities of LLMs by exploring different approaches, including prompt formats for integrating function descriptions, blending function-calling and instruction-following data, introducing a novel Decision Token for conditional prompts, leveraging chain-of-thought reasoning, and overcoming multilingual challenges with a translation pipeline. Our key findings and contributions are as follows: (1) Instruction-following data improves both function-calling accuracy and relevance detection. (2) The use of the newly proposed Decision Token, combined with synthetic non-function-call data, enhances relevance detection. (3) A tailored translation pipeline effectively overcomes multilingual limitations, demonstrating significant improvements in Traditional Chinese. These insights highlight the potential for improved function-calling capabilities and multilingual applications in LLMs.</abstract>
      <url hash="83d7f089">2025.naacl-industry.9</url>
      <bibkey>chen-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="10">
      <title>Exploring Straightforward Methods for Automatic Conversational Red-Teaming</title>
      <author><first>George</first><last>Kour</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Naama</first><last>Zwerdling</last></author>
      <author><first>Marcel</first><last>Zalmanovici</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ora Nova</first><last>Fandina</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Eitan</first><last>Farchi</last><affiliation>International Business Machines</affiliation></author>
      <pages>112-128</pages>
      <abstract>Large language models (LLMs) are increasingly used in business dialogue systems but they also pose security and ethical risks. Multi-turn conversations, in which context influences the model’s behavior, can be exploited to generate undesired responses. In this paper, we investigate the use of off-the-shelf LLMs in conversational red-teaming settings, where an attacker LLM attempts to elicit undesired outputs from a target LLM. Our experiments address critical questions and offer valuable insights regarding the effectiveness of using LLMs as automated red-teamers, shedding light on key strategies and usage approaches that significantly impact their performance.Our findings demonstrate that off-the-shelf models can serve as effective red-teamers, capable of adapting their attack strategies based on prior attempts. Allowing these models to freely steer conversations and conceal their malicious intent further increases attack success. However, their effectiveness decreases as the alignment of the target model improves.</abstract>
      <url hash="8f0a70ed">2025.naacl-industry.10</url>
      <bibkey>kour-etal-2025-exploring</bibkey>
    </paper>
    <paper id="11">
      <title>A Diverse and Effective Retrieval-Based Debt Collection System with Expert Knowledge</title>
      <author><first>Jiaming</first><last>Luo</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Weiyi</first><last>Luo</last><affiliation>China Merchants Bank Credit Card Center</affiliation></author>
      <author><first>Guoqing</first><last>Sun</last><affiliation>China Merchants Bank Credit Card Center</affiliation></author>
      <author><first>Mengchen</first><last>Zhu</last></author>
      <author><first>Haifeng</first><last>Tang</last></author>
      <author><first>Kenny Q.</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Mengyue</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>129-137</pages>
      <abstract>Designing effective debt collection systems is crucial for improving operational efficiency and reducing costs in the financial industry. However, the challenges of maintaining script diversity, contextual relevance, and coherence make this task particularly difficult. This paper presents a debt collection system based on real debtor-collector data from a major commercial bank. We construct a script library from real-world debt collection conversations, and propose a two-stage retrieval based response system for contextual relevance. Experimental results show that our system improves script diversity, enhances response relevance, and achieves practical deployment efficiency through knowledge distillation. This work offers a scalable and automated solution, providing valuable insights for advancing debt collection practices in real-world applications.</abstract>
      <url hash="1fd2ec8c">2025.naacl-industry.11</url>
      <bibkey>luo-etal-2025-diverse</bibkey>
    </paper>
    <paper id="12">
      <title>Search Query Embeddings via User-behavior-driven Contrastive Learning</title>
      <author><first>Sosuke</first><last>Nishikawa</last></author>
      <author><first>Jun</first><last>Hirako</last></author>
      <author><first>Nobuhiro</first><last>Kaji</last><affiliation>LY Corporation</affiliation></author>
      <author><first>Koki</first><last>Watanabe</last><affiliation>LY Corporation</affiliation></author>
      <author><first>Hiroki</first><last>Asano</last><affiliation>LY Corporation</affiliation></author>
      <author><first>Souta</first><last>Yamashiro</last><affiliation>LY Corporation</affiliation></author>
      <author><first>Shumpei</first><last>Sano</last><affiliation>LY Corporation</affiliation></author>
      <pages>138-147</pages>
      <abstract>Universal query embeddings that accurately capture the semantic meaning of search queries are crucial for supporting a range of query understanding (QU) tasks within enterprises.However, current embedding approaches often struggle to effectively represent queries due to the shortness of search queries and their tendency for surface-level variations.We propose a user-behavior-driven contrastive learning approach which directly aligns embeddings according to user intent.This approach uses intent-aligned query pairs as positive examples, derived from two types of real-world user interactions: (1) clickthrough data, in which queries leading to clicks on the same URLs are assumed to share the same intent, and (2) session data, in which queries within the same user session are considered to share intent.By incorporating these query pairs into a robust contrastive learning framework, we can construct query embedding models that align with user intent while minimizing reliance on surface-level lexical similarities.Evaluations on real-world QU tasks demonstrated that these models substantially outperformed state-of-the-art text embedding models such as mE5 and SimCSE.Our models have been deployed in our search engine to support QU technologies.</abstract>
      <url hash="4420503a">2025.naacl-industry.12</url>
      <bibkey>nishikawa-etal-2025-search</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>QS</fixed-case>pell 250<fixed-case>K</fixed-case>: A Large-Scale, Practical Dataset for <fixed-case>C</fixed-case>hinese Search Query Spell Correction</title>
      <author><first>Dezhi</first><last>Ye</last><affiliation>Tencent PCG</affiliation></author>
      <author><first>Haomei</first><last>Jia</last><affiliation>Macau University of Science and Technology</affiliation></author>
      <author><first>Junwei</first><last>Hu</last></author>
      <author><first>Tian</first><last>Bowen</last></author>
      <author><first>Jie</first><last>Liu</last></author>
      <author><first>Haijin</first><last>Liang</last><affiliation>Tencent</affiliation></author>
      <author><first>Jin</first><last>Ma</last><affiliation>Tencent Search</affiliation></author>
      <author><first>Wenmin</first><last>Wang</last><affiliation>Macau University of Science and Technology</affiliation></author>
      <pages>148-155</pages>
      <abstract>Chinese Search Query Spell Correction is a task designed to autonomously identify and correct typographical errors within queries in the search engine. Despite the availability of comprehensive datasets like Microsoft Speller and Webis, their monolingual nature and limited scope pose significant challenges in evaluating modern pre-trained language models such as BERT and GPT. To address this, we introduce <b>QSpell 250K</b>, a large-scale benchmark specifically developed for Chinese Query Spelling Correction. QSpell 250K offers several advantages: 1) It contains over 250K samples, which is ten times more than previous datasets. 2) It covers a broad range of topics, from formal entities to everyday colloquialisms and idiomatic expressions. 3) It includes both Chinese and English, addressing the complexities of code-switching. Each query undergoes three rounds of high-fidelity annotation to ensure accuracy. Our extensive testing across three popular models demonstrates that QSpell 250K effectively evaluates the efficacy of representative spelling correctors. We believe that QSpell 250K will significantly advance spelling correction methodologies. The accompanying data and code will be made publicly available.</abstract>
      <url hash="cd229cac">2025.naacl-industry.13</url>
      <bibkey>ye-etal-2025-qspell</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>CONSTRUCTA</fixed-case>: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models</title>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Xue</first><last>Yang</last><affiliation>Intel</affiliation></author>
      <pages>156-172</pages>
      <abstract>Automating planning with LLMs presents transformative opportunities for traditional industries, yet remains underexplored. In commercial construction, the complexity of automated scheduling often requires manual intervention to ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to optimize construction schedules in complex projects like semiconductor fabrication. CONSTRUCTA addresses key challenges by: (1) integrating construction-specific knowledge through static RAG; (2) employing context-sampling techniques inspired by architectural expertise to provide relevant input; and (3) deploying Construction DPO to align schedules with expert preferences using RLHF. Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.</abstract>
      <url hash="75c77a3d">2025.naacl-industry.14</url>
      <bibkey>zhang-yang-2025-constructa</bibkey>
    </paper>
    <paper id="15">
      <title>Challenges and Remedies of Domain-Specific Classifiers as <fixed-case>LLM</fixed-case> Guardrails: Self-Harm as a Case Study</title>
      <author><first>Bing</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Guang-Jie</first><last>Ren</last><affiliation>International Business Machines</affiliation></author>
      <pages>173-182</pages>
      <abstract>Context:Despite the impressive capabilities of Large Language Models (LLMs), they pose significant risks in many domains and therefore require guardrails throughout the lifecycle.Problem:Many such guardrails are trained as classifiers with domain-specific human text datasets obtained from sources such as social media and they achieve reasonable performance against closed-domain benchmarks. When deployed in the real world, however, the guardrails have to deal with machine text in an open domain, and their performance deteriorates drastically, rendering them almost unusable due to a high level of false refusal.Solution:In this paper, using a self-harm detector as an example, we demonstrate the specific challenges facing guardrail deployment due to the data drift between training and production environments. More specifically, we formed two hypotheses about the potential causes, i.e. closed vs. open domain, human vs. LLM-generated text, and conducted five experiments to explore various potential remedies, including their respective advantages and disadvantages.Evaluation:While focusing on one example, our experience and knowledge of LLM guardrails give us great confidence that our work contributes to a more thorough understanding of guardrail deployment and can be generalized as a methodology to build more robust domain-specific guardrails in real-world applications.</abstract>
      <url hash="fe404f7d">2025.naacl-industry.15</url>
      <bibkey>zhang-ren-2025-challenges</bibkey>
    </paper>
    <paper id="16">
      <title>Mitigating Bias in Item Retrieval for Enhancing Exam Assembly in Vocational Education Services</title>
      <author><first>Alonso</first><last>Palomino</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Andreas</first><last>Fischer</last></author>
      <author><first>David</first><last>Buschhüter</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Roland</first><last>Roller</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Niels</first><last>Pinkwart</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Benjamin</first><last>Paassen</last></author>
      <pages>183-193</pages>
      <abstract>In education, high-quality exams must cover broad specifications across diverse difficulty levels during the assembly and calibration of test items to effectively measure examinees’ competence. However, balancing the trade-off of selecting relevant test items while fulfilling exam specifications without bias is challenging, particularly when manual item selection and exam assembly rely on a pre-validated item base. To address this limitation, we propose a new mixed-integer programming re-ranking approach to improve relevance, while mitigating bias on an industry-grade exam assembly platform. We evaluate our approach by comparing it against nine bias mitigation re-ranking methods in 225 experiments on a real-world benchmark data set from vocational education services. Experimental results demonstrate a 17% relevance improvement with a 9% bias reduction when integrating sequential optimization techniques with improved contextual relevance augmentation and scoring using a large language model. Our approach bridges information retrieval and exam assembly, enhancing the human-in-the-loop exam assembly process while promoting unbiased exam design</abstract>
      <url hash="5fd60291">2025.naacl-industry.16</url>
      <bibkey>palomino-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="17">
      <title>Breaking Boundaries: Investigating the Effects of Model Editing on Cross-linguistic Performance</title>
      <author><first>Somnath</first><last>Banerjee</last><affiliation>Cisco and IIT Kharagpur</affiliation></author>
      <author><first>Avik</first><last>Halder</last></author>
      <author><first>Rajarshi</first><last>Mandal</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Sayan</first><last>Layek</last></author>
      <author><first>Ian</first><last>Soboroff</last><affiliation>National Institute of Standards and Technology</affiliation></author>
      <author><first>Rima</first><last>Hazra</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>194-209</pages>
      <abstract>Pretrained language models (PLMs) have revolutionized NLP but amplify linguistic inequities in multilingual applications. While prior studies focused on transformer architectures such as BERT, we evaluate large language models (LLMs) including Mistral, TowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama. Through rigorous testing across eight languages spanning high-resource (English, German, French, Italian, Spanish) and low-resource (Hindi, Tamil, Kannada) settings, we reveal systemic failures in preserving multilingual reliability and adaptability. Using paradigms like each language for itself’ (ELFI) and each language for others’ (ELFO), we highlight the inability of current LLMs to bridge linguistic divides. Even model merging fail to mitigate these gaps, exposing fundamental limitations. These findings emphasize the critical need for reimagining AI architectures to deliver true linguistic inclusivity and equitable performance across diverse languages.</abstract>
      <url hash="21a2f260">2025.naacl-industry.17</url>
      <bibkey>banerjee-etal-2025-breaking</bibkey>
    </paper>
    <paper id="18">
      <title>Towards Reliable and Practical Phishing Detection</title>
      <author><first>Hyowon</first><last>Cho</last><affiliation>KAIST</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Config Intelligence</affiliation></author>
      <pages>210-225</pages>
      <abstract>As the prevalence of phishing attacks continues to rise, there is an increasing demand for more robust detection technologies. With recent advances in AI, we discuss how to construct a reliable and practical phishing detection system using language models. For this system, we introduce the first large-scale Korean dataset for phishing detection, encompassing six types of phishing attacks. We consider multiple factors for building a real-time detection system for edge devices, such as model size, Speech-To-Text quality, split length, training technique and multi-task learning. We evaluate the model’s ability twofold: in-domain, and unseen attack detection performance which is referred to as zero-day performance. Additionally, we demonstrate the importance of accurate comparison groups and evaluation datasets, showing that voice phishing detection performs reasonably well while smishing detection remains challenging. Both the dataset and the trained model will be available upon request.</abstract>
      <url hash="21025b79">2025.naacl-industry.18</url>
      <bibkey>cho-seo-2025-towards</bibkey>
    </paper>
    <paper id="19">
      <title>Zero-Shot <fixed-case>ATC</fixed-case> Coding with Large Language Models for Clinical Assessments</title>
      <author><first>Zijian</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>John-Michael</first><last>Gamble</last></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>226-232</pages>
      <abstract>Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to prescription records is a significant bottleneck in healthcare research and operations at Ontario Health and InterRAI Canada, requiring extensive expert time and effort. To automate this process while maintaining data privacy, we develop a practical approach using locally deployable large language models (LLMs). Inspired by recent advances in automatic International Classification of Diseases (ICD) coding, our method frames ATC coding as a hierarchical information extraction task, guiding LLMs through the ATC ontology level by level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus development on open-source Llama models suitable for privacy-sensitive deployment. Testing across Health Canada drug product data, the RABBITS benchmark, and real clinical notes from Ontario Health, our method achieves 78% exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate knowledge grounding through drug definitions, finding modest improvements in accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama 3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller models. Our results demonstrate the feasibility of automatic ATC coding in privacy-sensitive healthcare environments, providing a foundation for future deployments.</abstract>
      <url hash="bebb4c15">2025.naacl-industry.19</url>
      <bibkey>chen-etal-2025-zero</bibkey>
    </paper>
    <paper id="20">
      <title>Navigating the Path of Writing: Outline-guided Text Generation with Large Language Models</title>
      <author><first>Yukyung</first><last>Lee</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Soonwon</first><last>Ka</last><affiliation>NAVER</affiliation></author>
      <author><first>Bokyung</first><last>Son</last><affiliation>NAVER</affiliation></author>
      <author><first>Pilsung</first><last>Kang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewook</first><last>Kang</last><affiliation>Coupang</affiliation></author>
      <pages>233-250</pages>
      <abstract>Large Language Models (LLMs) have impacted the writing process, enhancing productivity by collaborating with humans in content creation platforms. However, generating high-quality, user-aligned text to satisfy real-world content creation needs remains challenging. We propose WritingPath, a framework that uses explicit outlines to guide LLMs in generating goal-oriented, high-quality text. Our approach draws inspiration from structured writing planning and reasoning paths, focusing on reflecting user intentions throughout the writing process. To validate our approach in real-world scenarios, we construct a diverse dataset from unstructured blog posts to benchmark writing performance and introduce a comprehensive evaluation framework assessing the quality of outlines and generated texts. Our evaluations with various LLMs demonstrate that the WritingPath approach significantly enhances text quality according to evaluations by both LLMs and professional writers.</abstract>
      <url hash="f68d0110">2025.naacl-industry.20</url>
      <bibkey>lee-etal-2025-navigating</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>T</fixed-case>ae<fixed-case>B</fixed-case>ench: Improving Quality of Toxic Adversarial Examples</title>
      <author><first>Jennifer</first><last>Zhu</last><affiliation>University of California, Berkeley and Amazon</affiliation></author>
      <author><first>Dmitriy</first><last>Bespalov</last><affiliation>Amazon</affiliation></author>
      <author><first>Liwen</first><last>You</last></author>
      <author><first>Ninad</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Amazon and University of Virginia</affiliation></author>
      <pages>251-265</pages>
      <abstract>Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of . Successful should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of <tex-math>940k</tex-math> raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size <tex-math>264k</tex-math>). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.</abstract>
      <url hash="3a7a6232">2025.naacl-industry.21</url>
      <bibkey>zhu-etal-2025-taebench</bibkey>
    </paper>
    <paper id="22">
      <title>Open <fixed-case>K</fixed-case>o-<fixed-case>LLM</fixed-case> Leaderboard2: Bridging Foundational and Practical Evaluation for <fixed-case>K</fixed-case>orean <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hyeonwoo</first><last>Kim</last></author>
      <author><first>Dahyun</first><last>Kim</last><affiliation>Twelve Labs</affiliation></author>
      <author><first>Jihoo</first><last>Kim</last></author>
      <author><first>Sukyung</first><last>Lee</last></author>
      <author><first>Yungi</first><last>Kim</last><affiliation>Upstage</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <pages>266-273</pages>
      <abstract>The Open Ko-LLM Leaderboard has been instrumental in benchmarking Korean Large Language Models (LLMs), yet it has certain limitations. Notably, the disconnect between quantitative improvements on the overly academic leaderboard benchmarks and the qualitative impact of the models should be addressed. Furthermore, the benchmark suite is largely composed of translated versions of their English counterparts, which may not fully capture the intricacies of the Korean language. To address these issues, we propose Open Ko-LLM Leaderboard2, an improved version of the earlier Open Ko-LLM Leaderboard. The original benchmarks are entirely replaced with new tasks that are more closely aligned with real-world capabilities. Additionally, four new native Korean benchmarks are introduced to better reflect the distinct characteristics of the Korean language. Through these refinements, Open Ko-LLM Leaderboard2 seeks to provide a more meaningful evaluation for advancing Korean LLMs.</abstract>
      <url hash="2789dddd">2025.naacl-industry.22</url>
      <bibkey>kim-etal-2025-open</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>C</fixed-case>urious<fixed-case>LLM</fixed-case>: Elevating Multi-Document Question Answering with <fixed-case>LLM</fixed-case>-Enhanced Knowledge Graph Reasoning</title>
      <author><first>Zukang</first><last>Yang</last></author>
      <author><first>Zixuan</first><last>Zhu</last></author>
      <author><first>Jennifer</first><last>Zhu</last><affiliation>University of California, Berkeley and Amazon</affiliation></author>
      <pages>274-286</pages>
      <abstract>Large Language Models (LLMs) have achieved significant success in open-domain question answering. However, they continue to face challenges such as hallucinations and knowledge cutoffs. These issues can be mitigated through in-context learning by providing LLMs with relevant context before generating answers. Recent literature proposes Knowledge Graph Prompting (KGP) which integrates knowledge graphs with an LLM-based traversal agent to substantially enhance document retrieval quality. However, KGP requires costly fine-tuning with large datasets and remains prone to hallucination. In this paper, we propose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning mechanism into an LLM agent. This mechanism enables the agent to generate relevant follow-up questions, thereby guiding the information retrieval process more efficiently.Central to our approach is the development of the new Follow-upQA dataset, which includes questions and supporting evidence as input, with follow-up questions serving as ground truths. These follow-up questions either inquire about what is still missing to fully answer the user’s query or use special tokens to signify that the retrieved evidence is sufficient. Our experiments show that CuriousLLM significantly boosts LLM performance in multi-document question answering (MD-QA), circumventing the substantial computational costs and latency from the original KGP framework.</abstract>
      <url hash="fc3e362d">2025.naacl-industry.23</url>
      <bibkey>yang-etal-2025-curiousllm</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>C</fixed-case>haracter<fixed-case>GPT</fixed-case>: A Persona Reconstruction Framework for Role-Playing Agents</title>
      <author><first>Jeiyoon</first><last>Park</last></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>287-303</pages>
      <abstract>The recent introduction of the Assistants API highlights its potential for large language models (LLMs) in role-playing agents (RPA). However, maintaining consistent character personas remains a significant challenge due to variability in information extraction, which frequently omits critical elements such as backstory or interpersonal relationships. To address this limitation, we introduce CharacterGPT, a framework designed to dynamically reconstruct character personas through Character Persona Training (CPT). This approach incrementally updates personas by extracting traits from chapter-wise novel summaries, reflecting the progression of the narrative. Our framework is evaluated through Big Five personality evaluations and creative tasks, in which characters generate original narratives, demonstrating the efficacy of CharacterGPT in preserving persona consistency. The code and results are available at https://github.com/Jeiyoon/charactergpt</abstract>
      <url hash="0b39de60">2025.naacl-industry.24</url>
      <bibkey>park-etal-2025-charactergpt</bibkey>
    </paper>
    <paper id="25">
      <title>Efficient Continual Pre-training of <fixed-case>LLM</fixed-case>s for Low-resource Languages</title>
      <author><first>Arijit</first><last>Nag</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <pages>304-317</pages>
      <abstract>Open-source large language models (Os-LLMs) propel the democratization of natural language research by giving the flexibility to augment or update model parameters for performance improvement. Nevertheless, like proprietary LLMs, Os-LLMs offer poorer performance on low-resource languages (LRLs) than high-resource languages (HRLs), owing to smaller amounts of training data and underrepresented vocabulary. On the other hand, continual pre-training (CPT) with large amounts of language-specific data is a costly proposition in terms of data acquisition and computational resources. Our goal is to drastically reduce CPT cost.To that end, we first develop a new algorithm to select a subset of texts from a larger corpus. We show the effectiveness of our technique using very little CPT data. In search of further improvement, we design a new algorithm to select tokens to include in the LLM vocabulary.We experiment with the recent Llama-3 model and nine Indian languages with diverse scripts and extent of resource availability.For evaluation, we use IndicGenBench, a generation task benchmark dataset for Indic languages. We experiment with various CPT corpora and augmented vocabulary size and offer insights across language families.</abstract>
      <url hash="a382d888">2025.naacl-industry.25</url>
      <bibkey>nag-etal-2025-efficient</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>DSRAG</fixed-case>: A Double-Stream Retrieval-Augmented Generation Framework for Countless Intent Detection</title>
      <author><first>Pei</first><last>Guo</last></author>
      <author><first>Enjie</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Ruichao</first><last>Zhong</last><affiliation>Tencent</affiliation></author>
      <author><first>Mochi</first><last>Gao</last><affiliation>Tencent</affiliation></author>
      <author><first>Yunzhi</first><last>Tan</last><affiliation>Tencent</affiliation></author>
      <author><first>Bo</first><last>Hu</last></author>
      <author><first>Zang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <pages>318-328</pages>
      <abstract>Current intent detection work experiments with minor intent categories. However, in real-world scenarios of data analysis dialogue systems, intents are composed of combinations of numerous metrics and dimensions, resulting in countless intents and posing challenges for the language model. The retrieval-augmented generation (RAG) method efficiently retrieves key intents. However, the single retrieval route sometimes fails to recall target intents and causes incorrect results. To alleviate the above challenges, we introduce the DSRAG framework combining query-to-query (Q2Q) and query-to-metadata (Q2M) double-stream RAG approaches. Specifically, we build a repository of query statements for Q2Q using the query templates with the key intents. When a user’s query comes, it rapidly matches repository statements. Once the relevant query is retrieved, the results can be quickly returned. In contrast, Q2M retrieves the relevant intents from the metadata and utilizes large language models to choose the answer. Experimental results show that DSRAG achieves significant improvements compared with merely using prompt engineering and a single retrieval route.</abstract>
      <url hash="cf65232a">2025.naacl-industry.26</url>
      <bibkey>guo-etal-2025-dsrag</bibkey>
    </paper>
    <paper id="27">
      <title>Octopus: On-device language model for function calling of software <fixed-case>API</fixed-case>s</title>
      <author><first>Wei</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Zhiyuan</first><last>Li</last><affiliation>Nexa AI</affiliation></author>
      <author><first>Mingyuan</first><last>Ma</last></author>
      <pages>329-339</pages>
      <abstract>Large Language Models (LLMs) are pivotal for advanced text processing and generation. This study presents a framework to train a series of on-device LLMs optimized for invoking software APIs. Using a curated dataset of 30,000 API function calls from software documentation, we fine-tune LLMs with 2B, 3B, and 7B parameters to enhance their proficiency in API interactions. Our approach improves the understanding of API structures and syntax, leading to significantly better accuracy in API function calls. We also propose a conditional masking technique to enforce correct output formats, reducing errors while maintaining inference speed, specifically tailored for API tasks. The fine-tuned model, Octopus, outperforms GPT-4 in API calling tasks, showcasing advancements in automated software development and API integration. The model checkpoints are publicly available.</abstract>
      <url hash="cd0e8147">2025.naacl-industry.27</url>
      <bibkey>chen-etal-2025-octopus</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>M</fixed-case>o<fixed-case>FE</fixed-case>: Mixture of Frozen Experts Architecture</title>
      <author><first>Jean</first><last>Seo</last><affiliation>AITRICS</affiliation></author>
      <author><first>Jaeyoon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyopil</first><last>Shin</last><affiliation>Seoul National University</affiliation></author>
      <pages>340-348</pages>
      <abstract>We propose the Mixture of Frozen Experts (MoFE) architecture, which integrates Parameter-efficient Fine-tuning (PEFT) and the Mixture of Experts (MoE) architecture to enhance both training efficiency and model scalability. By freezing the Feed Forward Network (FFN) layers within the MoE framework, MoFE significantly reduces the number of trainable parameters, improving training efficiency while still allowing for effective knowledge transfer from the expert models. This facilitates the creation of models proficient in multiple domains. We conduct experiments to evaluate the trade-offs between performance and efficiency, compare MoFE with other PEFT methodologies, assess the impact of domain expertise in the constituent models, and determine the optimal training strategy. The results show that, although there may be some trade-offs in performance, the efficiency gains are substantial, making MoFE a reasonable solution for real-world, resource-constrained environments.</abstract>
      <url hash="6aefdd25">2025.naacl-industry.28</url>
      <bibkey>seo-etal-2025-mofe</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>F</fixed-case>in<fixed-case>LLM</fixed-case>-<fixed-case>B</fixed-case>: When Large Language Models Meet Financial Breakout Trading</title>
      <author><first>Kang</first><last>Zhang</last></author>
      <author><first>Osamu</first><last>Yoshie</last></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Weiran</first><last>Huang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>349-357</pages>
      <abstract>Trading range breakout is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Traditional quantitative methods require large amounts of data and cannot directly present the reasoning process to users, making them less than perfect in this field. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we created the first financial breakout dataset and introduce FinLLM-B, the premier large language model for financial breakout detection, which enhances the effectiveness of breakout trading strategies. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, FinanceGPT-B improves the average accuracy of answers and rational by 49.97%, with the multi-stage structure contributing 9.72% to the improvement. Additionally, it outperforms ChatGPT-4 by 42.38%.</abstract>
      <url hash="99bbe1d2">2025.naacl-industry.29</url>
      <bibkey>zhang-etal-2025-finllm</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>Q</fixed-case>uery<fixed-case>S</fixed-case>hield: A Platform to Mitigate Enterprise Data Leakage in Queries to External <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nitin</first><last>Ramrakhiyani</last><affiliation>International Institute of Information Technology Hyderabad and Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Delton</first><last>Myalil</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Manoj</first><last>Apte</last></author>
      <author><first>Rajan M</first><last>A</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Divyesh</first><last>Saglani</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Imtiyazuddin</first><last>Shaik</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>358-369</pages>
      <abstract>Unrestricted access to external Large Language Models (LLM) based services like ChatGPT and Gemini can lead to potential data leakages, especially for large enterprises providing products and services to clients that require legal confidentiality guarantees. However, a blanket restriction on such services is not ideal as these LLMs boost employee productivity. Our goal is to build a solution that enables enterprise employees to query such external LLMs, without leaking confidential internal and client information. In this paper, we propose QueryShield - a platform that enterprises can use to interact with external LLMs without leaking data through queries. It detects if a query leaks data, and rephrases it to minimize data leakage while limiting the impact to its semantics. We construct a dataset of 1500 queries and manually annotate them for their sensitivity labels and their low sensitivity rephrased versions. We fine-tune a set of lightweight model candidates using this dataset and evaluate them using multiple metrics including one we propose specific to this problem.</abstract>
      <url hash="f5da3832">2025.naacl-industry.30</url>
      <bibkey>ramrakhiyani-etal-2025-queryshield</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>S</fixed-case>wiss<fixed-case>ADT</fixed-case>: An Audio Description Translation System for <fixed-case>S</fixed-case>wiss Languages</title>
      <author><first>Lukas</first><last>Fischer</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Alexa</first><last>Lintner</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Annette</first><last>Rios</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <pages>370-379</pages>
      <abstract>Audio description (AD) is a crucial accessibility service provided to blind persons and persons with visual impairment, designed to convey visual information in acoustic form. Despite recent advancements in multilingual machine translation research, the lack of well-crafted and time-synchronized AD data impedes the development of audio description translation (ADT) systems that address the needs of multilingual countries such as Switzerland. Furthermore, most ADT systems rely on text alone, and it is unclear whether incorporating visual information from video clips improves the quality of ADT outputs.In this work, we introduce SwissADT, an **emerging** ADT system for three main Swiss languages and English, designed for future use by our industry partners. By collecting well-crafted AD data augmented with video clips in German, French, Italian, and English, and leveraging the power of Large Language Models (LLMs), we aim to enhance information accessibility for diverse language populations in Switzerland by automatically translating AD scripts to the desired Swiss language. Our extensive experimental ADT results, composed of both automatic and human evaluations of ADT quality, demonstrate the promising capability of SwissADT for the ADT task. We believe that combining human expertise with the generation power of LLMs can further enhance the performance of ADT systems, ultimately benefiting a larger multilingual target population.</abstract>
      <url hash="a5dc5427">2025.naacl-industry.31</url>
      <bibkey>fischer-etal-2025-swissadt</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>C</fixed-case>hinese Morph Resolution in <fixed-case>E</fixed-case>-commerce Live Streaming Scenarios</title>
      <author><first>Jiahao</first><last>Zhu</last></author>
      <author><first>Jipeng</first><last>Qiang</last><affiliation>Yangzhou University</affiliation></author>
      <author><first>Ran</first><last>Bai</last><affiliation>China Academic of Electronics and Information Technology</affiliation></author>
      <author><first>Chenyu</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Ouyang</last></author>
      <pages>380-389</pages>
      <abstract>E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.</abstract>
      <url hash="b4823c6a">2025.naacl-industry.32</url>
      <bibkey>zhu-etal-2025-chinese</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>M</fixed-case>ono<fixed-case>TOD</fixed-case>ia: Translating Monologue Requests to Task-Oriented Dialogues</title>
      <author><first>Sebastian</first><last>Steindl</last><affiliation>Ostbayerische Technische Hochschule Amberg-Weiden</affiliation></author>
      <author><first>Ulrich</first><last>Schäfer</last><affiliation>Ostbayerische Technische Hochschule Amberg-Weiden</affiliation></author>
      <author><first>Bernd</first><last>Ludwig</last><affiliation>Universität Regensburg</affiliation></author>
      <pages>390-403</pages>
      <abstract>Data scarcity is one of the main problems when it comes to real-world applications of transformer-based models.This is especially evident for task-oriented dialogue (TOD) systems, which require specialized datasets, that are usually not readily available. This can hinder companies from adding TOD systems to their services.This study therefore investigates a novel approach to sourcing annotated dialogues from existing German monologue material.Focusing on a real-world example, we investigate whether these monologues can be transformed into dialogue formats suitable for training TOD systems.We show the approach with the concrete example of a company specializing in travel bookings via e-mail. We fine-tune state-of-the-art Large Language Models for the task of rewriting e-mails as dialogues and annotating them.To ensure the quality and validity of the generated data, we employ crowd workers to evaluate the dialogues across multiple criteria and to provide gold-standard annotations for the test dataset.We further evaluate the usefulness of the dialogues for training TOD systems.Our evaluation shows that the dialogues and annotations are of high quality and can serve as a valuable starting point for training TOD systems.Finally, we make the annotated dataset publicly available to foster future research.</abstract>
      <url hash="f3b2d333">2025.naacl-industry.33</url>
      <bibkey>steindl-etal-2025-monotodia</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>M</fixed-case>ed<fixed-case>E</fixed-case>thic<fixed-case>E</fixed-case>val: Evaluating Large Language Models Based on <fixed-case>C</fixed-case>hinese Medical Ethics</title>
      <author><first>Haoan</first><last>Jin</last></author>
      <author><first>Jiacheng</first><last>Shi</last></author>
      <author><first>Hanhui</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Kenny Q.</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Mengyue</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>404-421</pages>
      <abstract>Large language models (LLMs) demonstrate significant potential in advancing medical applications, yet their capabilities in addressing medical ethics challenges remain underexplored. This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics. Our framework encompasses two key components: knowledge, assessing the models’ grasp of medical ethics principles, and application, focusing on their ability to apply these principles across diverse scenarios. To support this benchmark, we consulted with medical ethics researchers and developed three datasets addressing distinct ethical challenges: blatant violations of medical ethics, priority dilemmas with clear inclinations, and equilibrium dilemmas without obvious resolutions. MedEthicEval serves as a critical tool for understanding LLMs’ ethical reasoning in healthcare, paving the way for their responsible and effective use in medical contexts.</abstract>
      <url hash="60965921">2025.naacl-industry.34</url>
      <bibkey>jin-etal-2025-medethiceval</bibkey>
    </paper>
    <paper id="35">
      <title>Predicting <fixed-case>ICU</fixed-case> Length of Stay for Patients using Latent Categorization of Health Conditions</title>
      <author><first>Tirthankar</first><last>Dasgupta</last></author>
      <author><first>Manjira</first><last>Sinha</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Sudeshna</first><last>Jana</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>422-430</pages>
      <abstract>Predicting the duration of a patient’s stay in an Intensive Care Unit (ICU) is a critical challenge for healthcare administrators, as it impacts resource allocation, staffing, and patient care strategies. Traditional approaches often rely on structured clinical data, but recent developments in language models offer significant potential to utilize unstructured text data such as nursing notes, discharge summaries, and clinical reports for ICU length-of-stay (LoS) predictions. In this study, we introduce a method for analyzing nursing notes to predict the remaining ICU stay duration of patients. Our approach leverages a joint model of latent note categorization, which identifies key health-related patterns and disease severity factors from unstructured text data. This latent categorization enables the model to derive high-level insights that influence patient care planning. We evaluate our model on the widely used MIMIC-III dataset, and our preliminary findings show that it significantly outperforms existing baselines, suggesting promising industrial applications for resource optimization and operational efficiency in healthcare settings.</abstract>
      <url hash="65b2c5b8">2025.naacl-industry.35</url>
      <bibkey>dasgupta-etal-2025-predicting</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>R</fixed-case>evie<fixed-case>W</fixed-case>eaver: Weaving Together Review Insights by Leveraging <fixed-case>LLM</fixed-case>s and Semantic Similarity</title>
      <author><first>Jiban</first><last>Adhikary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mohammad</first><last>Alqudah</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Arun Palghat</first><last>Udayashankar</last><affiliation>Best Buy</affiliation></author>
      <pages>431-448</pages>
      <abstract>With the rise of online retail, customer reviews have become a critical factor in shaping purchasing decisions. The sheer volume of customer reviews being generated continuously presents a challenge for consumers who must sift through an overwhelming amount of feedback. To address this issue, we introduce RevieWeaver, a novel framework that extracts key product features and provides concise review summaries. Our innovative approach not only scales efficiently to 30 million reviews but also ensures reproducibility and controllability. Moreover, it delivers unbiased and reliable assessments of products that accurately reflect the input reviews.</abstract>
      <url hash="0152280b">2025.naacl-industry.36</url>
      <bibkey>adhikary-etal-2025-revieweaver</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>M</fixed-case>ed<fixed-case>C</fixed-case>od<fixed-case>ER</fixed-case>: A Generative <fixed-case>AI</fixed-case> Assistant for Medical Coding</title>
      <author><first>Krishanu Das</first><last>Baksi</last><affiliation>Databricks, Databricks</affiliation></author>
      <author><first>Elijah</first><last>Soba</last></author>
      <author><first>John J</first><last>Higgins</last></author>
      <author><first>Ravi</first><last>Saini</last></author>
      <author><first>Jaden</first><last>Wood</last><affiliation>Deloitte Consulting</affiliation></author>
      <author><first>Jane</first><last>Cook</last></author>
      <author><first>Jack I</first><last>Scott</last></author>
      <author><first>Nirmala</first><last>Pudota</last><affiliation>Deloitte</affiliation></author>
      <author><first>Tim</first><last>Weninger</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Edward</first><last>Bowen</last></author>
      <author><first>Sanmitra</first><last>Bhattacharya</last><affiliation>Deloitte Consulting</affiliation></author>
      <pages>449-459</pages>
      <abstract>Medical coding standardizes clinical data but is both time-consuming and error-prone. Traditional Natural Language Processing (NLP) methods struggle with automating coding due to the large label space, lengthy text inputs, and the absence of supporting evidence annotations that justify code selection. Recent advancements in Generative Artificial Intelligence (AI) offer promising solutions to these challenges. In this work, we introduce MedCodER, an emerging Generative AI framework for automatic medical coding that leverages extraction, retrieval, and re-ranking techniques as core components. MedCodER achieves a micro-F1 score of 0.62 on International Classification of Diseases (ICD) code prediction, significantly outperforming state-of-the-art methods. Additionally, we present a new dataset containing medical records annotated with disease diagnoses, ICD codes, and supporting evidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests confirm that MedCodER’s performance depends on the integration of each of its aforementioned components, as performance declines when these components are evaluated in isolation.</abstract>
      <url hash="2f36f7cf">2025.naacl-industry.37</url>
      <bibkey>baksi-etal-2025-medcoder</bibkey>
    </paper>
    <paper id="38">
      <title>Visual Zero-Shot <fixed-case>E</fixed-case>-Commerce Product Attribute Value Extraction</title>
      <author><first>Jiaying</first><last>Gong</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Hongda</first><last>Shen</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Pierre-Yves</first><last>Vandenbussche</last></author>
      <author><first>Janet</first><last>Jenq</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Hoda</first><last>Eldardiry</last><affiliation>, Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>460-469</pages>
      <abstract>Existing zero-shot product attribute value (aspect) extraction approaches in e-Commerce industry rely on uni-modal or multi-modal models, where the sellers are asked to provide detailed textual inputs (product descriptions) for the products. However, manually providing (typing) the product descriptions is time-consuming and frustrating for the sellers. Thus, we propose a cross-modal zero-shot attribute value generation framework (ViOC-AG) based on CLIP, which only requires product images as the inputs. ViOC-AG follows a text-only training process, where a task-customized text decoder is trained with the frozen CLIP text encoder to alleviate the modality gap and task disconnection. During the zero-shot inference, product aspects are generated by the frozen CLIP image encoder connected with the trained task-customized text decoder. OCR tokens and outputs from a frozen prompt-based LLM correct the decoded outputs for out-of-domain attribute values. Experiments show that ViOC-AG significantly outperforms other fine-tuned vision-language models for zero-shot attribute value extraction.</abstract>
      <url hash="fcba884f">2025.naacl-industry.38</url>
      <bibkey>gong-etal-2025-visual</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>SCORE</fixed-case>: Systematic <fixed-case>CO</fixed-case>nsistency and Robustness Evaluation for Large Language Models</title>
      <author><first>Grigor</first><last>Nalbandyan</last></author>
      <author><first>Rima</first><last>Shahbazyan</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Evelina</first><last>Bakhturina</last><affiliation>NVIDIA</affiliation></author>
      <pages>470-484</pages>
      <abstract>Typical evaluations of Large Language Models (LLMs) report a single metric per dataset, often representing the model’s best-case performance under carefully selected settings. Unfortunately, this approach overlooks model robustness and reliability in real-world applications. For instance, simple paraphrasing of prompts on the MMLU-Pro dataset causes accuracy fluctuations of up to 10%, while reordering answer choices in the AGIEval dataset results in accuracy differences of up to 6.1%. While some studies discuss issues with LLM robustness, there is no unified or centralized framework for evaluating the robustness of language models. To address this gap and consolidate existing research on model robustness, we present SCORE (<b>S</b>ystematic <b>CO</b>nsistency and <b>R</b>obustness <b>E</b>valuation), a comprehensive framework for non-adversarial evaluation of LLMs. The SCORE framework evaluates models by repeatedly testing them on the same benchmarks in various setups to give a realistic estimate of their accuracy and consistency. We will make the code publicly available to facilitate further development and research.</abstract>
      <url hash="d5a7dc92">2025.naacl-industry.39</url>
      <bibkey>nalbandyan-etal-2025-score</bibkey>
    </paper>
    <paper id="40">
      <title>Evaluating Large Language Models with Enterprise Benchmarks</title>
      <author><first>Bing</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Mikio</first><last>Takeuchi</last><affiliation>IBM Japan</affiliation></author>
      <author><first>Ryo</first><last>Kawahara</last><affiliation>IBM Research - Tokyo, International Business Machines</affiliation></author>
      <author><first>Shubhi</first><last>Asthana</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Maruf</first><last>Hossain</last></author>
      <author><first>Guang-Jie</first><last>Ren</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kate</first><last>Soule</last><affiliation>MIT-IBM Watson AI Lab and IBM Research</affiliation></author>
      <author><first>Yifan</first><last>Mai</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yada</first><last>Zhu</last><affiliation>IBM Research</affiliation></author>
      <pages>485-505</pages>
      <abstract>The advancement of large language models (LLMs) has led to a greater challenge of having a rigorous and systematic evaluation of complex tasks performed, especially in enterprise applications. Therefore, LLMs need to be benchmarked with enterprise datasets for a variety of NLP tasks. This work explores benchmarking strategies focused on LLM evaluation, with a specific emphasis on both English and Japanese. The proposed evaluation framework encompasses 25 publicly available domain-specific English benchmarks from diverse enterprise domains like financial services, legal, climate, cyber security, and 2 public Japanese finance benchmarks. The diverse performance of 8 models across different enterprise tasks highlights the importance of selecting the right model based on the specific requirements of each task. Code and prompts are available on GitHub.</abstract>
      <url hash="8eeefb5c">2025.naacl-industry.40</url>
      <bibkey>zhang-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="41">
      <title>Can Post-Training Quantization Benefit from an Additional <fixed-case>QL</fixed-case>o<fixed-case>RA</fixed-case> Integration?</title>
      <author><first>Xiliang</first><last>Zhu</last><affiliation>Dialpad Inc.</affiliation></author>
      <author><first>Elena</first><last>Khasanova</last><affiliation>Dialpad, Inc.</affiliation></author>
      <author><first>Cheng</first><last>Chen</last><affiliation>Dialpad</affiliation></author>
      <pages>506-514</pages>
      <abstract>Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.</abstract>
      <url hash="6e424944">2025.naacl-industry.41</url>
      <bibkey>zhu-etal-2025-post</bibkey>
    </paper>
    <paper id="42">
      <title>From Generating Answers to Building Explanations: Integrating Multi-Round <fixed-case>RAG</fixed-case> and Causal Modeling for Scientific <fixed-case>QA</fixed-case></title>
      <author><first>Victor</first><last>Barres</last><affiliation>Elemental Cognition</affiliation></author>
      <author><first>Clifton James</first><last>McFate</last><affiliation>Elemental Cognition</affiliation></author>
      <author><first>Aditya</first><last>Kalyanpur</last></author>
      <author><first>Kailash Karthik</first><last>Saravanakumar</last><affiliation>Elemental Cognition</affiliation></author>
      <author><first>Lori</first><last>Moon</last><affiliation>MoonWorks, Inc.</affiliation></author>
      <author><first>Natnael</first><last>Seifu</last></author>
      <author><first>Abraham</first><last>Bautista-Castillo</last></author>
      <pages>515-522</pages>
      <abstract>Application of LLMs for complex causal question answering can be stymied by their opacity and propensity for hallucination. Although recent approaches such as Retrieval Augmented Generation and Chain of Thought prompting have improved reliability, we argue current approaches are insufficient and further fail to satisfy key criteria humans use to select and evaluate causal explanations. Inspired by findings from the social sciences, we present an implemented causal QA approach that combines iterative RAG with guidance from a formal model of causation. Our causal model is backed by the Cogent reasoning engine, allowing users to interactively perform counterfactual analysis and refine their answer. Our approach has been integrated into a deployed Collaborative Research Assistant (Cora) and we present a pilot evaluation in the life sciences domain.</abstract>
      <url hash="32fd01ea">2025.naacl-industry.42</url>
      <bibkey>barres-etal-2025-generating</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>T</fixed-case>urbo<fixed-case>F</fixed-case>uzz<fixed-case>LLM</fixed-case>: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice</title>
      <author><first>Aman</first><last>Goel</last><affiliation>Amazon</affiliation></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhe</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Dmitriy</first><last>Bespalov</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Amazon and University of Virginia</affiliation></author>
      <pages>523-534</pages>
      <abstract>Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves <tex-math>\geq</tex-math> 95% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o &amp; GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks.</abstract>
      <url hash="ce436d8e">2025.naacl-industry.43</url>
      <bibkey>goel-etal-2025-turbofuzzllm</bibkey>
    </paper>
    <paper id="44">
      <title>Does Self-Attention Need Separate Weights in Transformers?</title>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Nusrat Jahan</first><last>Prottasha</last></author>
      <author><first>Chun-Nam</first><last>Yu</last><affiliation>Nokia Bell Labs and Department of Computer Science</affiliation></author>
      <author><first>Ozlem</first><last>Garibay</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <pages>535-543</pages>
      <abstract>Self-attention has revolutionized natural language processing by capturing long-range dependencies and improving context understanding. However, it comes with high computational costs and struggles with sequential data’s inherent directionality. This paper investigates and presents a simplified approach called “shared weight self-attention,” where a single weight matrix is used for Keys, Queries, and Values instead of separate matrices for each. This approach cuts training parameters by more than half and significantly reduces training time. Our method not only improves efficiency but also achieves strong performance on tasks from the GLUE benchmark, even outperforming the standard BERT baseline in handling noisy and out-of-domain data. Experimental results show a 66.53% reduction in parameter size within the attention block and competitive accuracy improvements of 3.55% and 0.89% over symmetric and pairwise attention-based BERT models, respectively.</abstract>
      <url hash="8300b34b">2025.naacl-industry.44</url>
      <bibkey>kowsher-etal-2025-self</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>S</fixed-case>uper<fixed-case>RAG</fixed-case>: Beyond <fixed-case>RAG</fixed-case> with Layout-Aware Graph Modeling</title>
      <author><first>Chening</first><last>Yang</last></author>
      <author><first>Duy-Khanh</first><last>Vu</last><affiliation>Cinnamon AI</affiliation></author>
      <author><first>Minh-Tien</first><last>Nguyen</last></author>
      <author><first>Xuan-Quang</first><last>Nguyen</last><affiliation>Cinnamon AI</affiliation></author>
      <author><first>Linh</first><last>Nguyen</last><affiliation>Cinnamon AI</affiliation></author>
      <author><first>Hung</first><last>Le</last><affiliation>Deakin University</affiliation></author>
      <pages>544-557</pages>
      <abstract>This paper introduces layout-aware graph modeling for multimodal RAG. Different from traditional RAG methods that only deal with flat text chunks, the proposed method takes into account the relationship of multimodalities by using a graph structure. To do that, a graph modeling structure is defined based on document layout parsing. The structure of an input document is retained with the connection of text chunks, tables, and figures. This representation allows the method to handle complex questions that require information from multimodalities. To confirm the efficiency of the graph modeling, a flexible RAG pipeline is developed using robust components. Experimental results on four benchmark test sets confirm the contribution of the layout-aware modeling for performance improvement of the RAG pipeline.</abstract>
      <url hash="0e92b9cd">2025.naacl-industry.45</url>
      <bibkey>yang-etal-2025-superrag</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>S</fixed-case>we<fixed-case>E</fixed-case>val: Do <fixed-case>LLM</fixed-case>s Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use</title>
      <author><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <author><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author><first>Arion</first><last>Das</last></author>
      <author><first>Bhargava</first><last>Kumar</last><affiliation>TD Securities</affiliation></author>
      <author><first>Srikant</first><last>Panda</last><affiliation>Oracle</affiliation></author>
      <author><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Taki Hasan</first><last>Rafi</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Tejaswini</first><last>Kumar</last><affiliation>Columbia University</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>558-582</pages>
      <abstract>Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity.</abstract>
      <url hash="bbb45122">2025.naacl-industry.46</url>
      <bibkey>patel-etal-2025-sweeval</bibkey>
    </paper>
    <paper id="47">
      <title>Natural Language Processing for Human Resources: A Survey</title>
      <author><first>Naoki</first><last>Otani</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last><affiliation>Megagon Labs, Inc</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>583-597</pages>
      <abstract>Advances in Natural Language Processing (NLP) have the potential to transform HR processes, from recruitment to employee management. While recent breakthroughs in NLP have generated significant interest in its industrial applications, a comprehensive overview of how NLP can be applied across HR activities is still lacking. This paper discovers opportunities for researchers and practitioners to harness NLP’s transformative potential in this domain. We analyze key fundamental tasks such as information extraction and text classification, and their roles in downstream applications like recommendation and language generation, while also discussing ethical concerns. Additionally, we identify gaps in current research and encourage future work to explore holistic approaches for achieving broader objectives in this field.</abstract>
      <url hash="323d80f6">2025.naacl-industry.47</url>
      <bibkey>otani-etal-2025-natural</bibkey>
    </paper>
    <paper id="48">
      <title>Implementing Retrieval Augmented Generation Technique on Unstructured and Structured Data Sources in a Call Center of a Large Financial Institution</title>
      <author><first>Syed Shariyar</first><last>Murtaza</last><affiliation>Ryerson University</affiliation></author>
      <author><first>Yifan</first><last>Nie</last><affiliation>Manulife</affiliation></author>
      <author><first>Elias</first><last>Avan</last><affiliation>Manulife</affiliation></author>
      <author><first>Utkarsh</first><last>Soni</last><affiliation>Manulife</affiliation></author>
      <author><first>Wanyu</first><last>Liao</last></author>
      <author><first>Adam</first><last>Carnegie</last><affiliation>Manulife</affiliation></author>
      <author><first>Cyril John</first><last>Mathias</last><affiliation>Manulife</affiliation></author>
      <author><first>Junlin</first><last>Jiang</last></author>
      <author><first>Eugene</first><last>Wen</last><affiliation>Manulife Financial</affiliation></author>
      <pages>598-606</pages>
      <abstract>The retrieval-augmented generation (RAG) technique enables generative AI models to extract accurate facts from external unstructureddata sources. For structured data, RAG is further augmented by function calls to query databases. This paper presents an industrialcase study that implements RAG in a large financial institution’s call center. The study showcases experiences and architecture for ascalable RAG deployment. It also introduces enhancements to RAG for retrieving facts from structured data sources using data embeddings, achieving low latency and high reliability. Our optimized production application demonstratesan average response time of only 7.33 seconds. Additionally, the paper compares various open-source and closed-source models for answer generation in an industrial context.</abstract>
      <url hash="0af3713a">2025.naacl-industry.48</url>
      <bibkey>murtaza-etal-2025-implementing</bibkey>
    </paper>
    <paper id="49">
      <title>Granite Guardian: Comprehensive <fixed-case>LLM</fixed-case> Safeguarding</title>
      <author><first>Inkit</first><last>Padhi</last></author>
      <author><first>Manish</first><last>Nagireddy</last><affiliation>IBM Research</affiliation></author>
      <author><first>Giandomenico</first><last>Cornacchia</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Subhajit</first><last>Chaudhury</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Tejaswini</first><last>Pedapati</last></author>
      <author><first>Pierre</first><last>Dognin</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Erik</first><last>Miehling</last><affiliation>IBM Research</affiliation></author>
      <author><first>Martín</first><last>Santillán Cooper</last></author>
      <author><first>Kieran</first><last>Fraser</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Giulio</first><last>Zizzo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Muhammad Zaid</first><last>Hameed</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Mark</first><last>Purcell</last><affiliation>IBM TJ Watson Research Center</affiliation></author>
      <author><first>Michael</first><last>Desmond</last></author>
      <author><first>Qian</first><last>Pan</last><affiliation>IBM, International Business Machines</affiliation></author>
      <author><first>Inge</first><last>Vejsbjerg</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elizabeth M.</first><last>Daly</last><affiliation>IBM Research</affiliation></author>
      <author><first>Michael</first><last>Hind</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Werner</first><last>Geyer</last></author>
      <author><first>Ambrish</first><last>Rawat</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kush R.</first><last>Varshney</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Prasanna</first><last>Sattigeri</last><affiliation>IBM Research</affiliation></author>
      <pages>607-615</pages>
      <abstract>The deployment of language models in real-world applications exposes users to various risks, including hallucinations and harmful or unethical content. These challenges highlight the urgent need for robust safeguards to ensure safe and responsible AI. To address this, we introduce Granite Guardian, a suite of advanced models designed to detect and mitigate risks associated with prompts and responses, enabling seamless integration with any large language model (LLM). Unlike existing open-source solutions, our Granite Guardian models provide comprehensive coverage across a wide range of risk dimensions, including social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, and hallucination-related issues such as context relevance, groundedness, and answer accuracy in retrieval-augmented generation (RAG) scenarios. Trained on a unique dataset combining diverse human annotations and synthetic data, Granite Guardian excels in identifying risks often overlooked by traditional detection systems, particularly jailbreak attempts and RAG-specific challenges. https://github.com/ibm-granite/granite-guardian</abstract>
      <url hash="4356581c">2025.naacl-industry.49</url>
      <bibkey>padhi-etal-2025-granite</bibkey>
    </paper>
    <paper id="50">
      <title>Breaking Down Power Barriers in On-Device Streaming <fixed-case>ASR</fixed-case>: Insights and Solutions</title>
      <author><first>Yang</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Yuan</first><last>Shangguan</last><affiliation>Current: Google</affiliation></author>
      <author><first>Yuhao</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Liangzhen</first><last>Lai</last><affiliation>Facebook</affiliation></author>
      <author><first>Ernie</first><last>Chang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Changsheng</first><last>Zhao</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Yangyang</first><last>Shi</last><affiliation>Meta</affiliation></author>
      <author><first>Vikas</first><last>Chandra</last><affiliation>Meta</affiliation></author>
      <pages>616-626</pages>
      <abstract>Power consumption plays a crucial role in on-device streaming speech recognition, significantly influencing the user experience. This study explores how the configuration of weight parameters in speech recognition models affects their overall energy efficiency. We found that the influence of these parameters on power consumption varies depending on factors such as invocation frequency and memory allocation. Leveraging these insights, we propose design principles that enhance on-device speech recognition models by reducing power consumption with minimal impact on accuracy. Our approach, which adjusts model components based on their specific energy sensitivities, achieves up to 47% lower energy usage while preserving comparable model accuracy and improving real-time performance compared to leading methods.</abstract>
      <url hash="2bb6a657">2025.naacl-industry.50</url>
      <bibkey>li-etal-2025-breaking</bibkey>
    </paper>
    <paper id="51">
      <title>Break-Ideate-Generate (<fixed-case>B</fixed-case>r<fixed-case>I</fixed-case>d<fixed-case>G</fixed-case>e): Moving beyond Translations for Localization using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Swapnil</first><last>Gupta</last></author>
      <author><first>Lucas Pereira</first><last>Carlini</last><affiliation>Amazon</affiliation></author>
      <author><first>Prateek</first><last>Sircar</last></author>
      <author><first>Deepak</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <pages>627-637</pages>
      <abstract>Language localization is the adaptation of written content to different linguistic and cultural contexts. Ability to localize written content is crucial for global businesses to provide consistent and reliable customer experience across diverse markets. Traditional methods have approached localization as an application of machine translation (MT), but localization requires more than linguistic conversion – content needs to align with the target audience’s cultural norms, linguistic nuances, and technical requirements. This difference is prominent for long-form text, where multiple facts are present in a creative choice of language. We propose a novel prompt approach for Large Languages Models (LLMs), called Break-Ideate-Generate (BrIdGe), for language localization. BrIdGe ‘breaks’ the source content into granular facts, ‘ideates’ an action plan for content creation in the target language by organizing the granular facts, and finally executes the plan to ‘generate’ localized content. This approach emulates the cognitive processes humans employ in writing that begin with identifying important points, followed by brainstorming on how to structure and organize the output. We evaluated the BrIdGe methodology from multiple perspectives, including impact of BrIdGe prompt on different LLMs and performance comparisons with traditional MT models and direct translation through LLMs on public benchmark and proprietary e-commerce datasets. Through human and LLM-based automated evaluations across content in multiple languages, we demonstrate effectiveness of BrIdGe in generating fluent localized content while preserving factual consistency between source and target languages.</abstract>
      <url hash="881d09cf">2025.naacl-industry.51</url>
      <bibkey>gupta-etal-2025-break</bibkey>
    </paper>
    <paper id="52">
      <title>Concept Distillation from Strong to Weak Models via Hypotheses-to-Theories Prompting</title>
      <author><first>Emmanuel Aboah</first><last>Boateng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Cassiano O</first><last>Becker</last></author>
      <author><first>Nabiha</first><last>Asghar</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kabir</first><last>Walia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ashwin</first><last>Srinivasan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ehi</first><last>Nosakhare</last><affiliation>Microsoft</affiliation></author>
      <author><first>Soundararajan</first><last>Srinivasan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Victor</first><last>Dibia</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>638-654</pages>
      <abstract>Hand-crafting high quality prompts to optimize the performance of language models is a complicated and labor-intensive process. Furthermore, when migrating to newer, smaller, or weaker models (possibly due to latency or cost gains), prompts need to be updated to re-optimize the task performance. We propose Concept Distillation (CD), an automatic prompt optimization technique for enhancing weaker models on complex tasks. CD involves: (1) collecting mistakes made by weak models with a base prompt (initialization), (2) using a strong model to generate reasons for these mistakes and create rules/concepts for weak models (induction), and (3) filtering these rules based on validation set performance and integrating them into the base prompt (deduction/verification). We evaluated CD on NL2Code and mathematical reasoning tasks, observing significant performance boosts for small and weaker language models. Notably, Mistral-7B’s accuracy on Multi-Arith increased by 20%, and Phi-3-mini-3.8B’s accuracy on HumanEval rose by 34%. Compared to other automated methods, CD offers an effective, cost-efficient strategy for improving weak models’ performance on complex tasks and enables seamless workload migration across different language models without compromising performance.</abstract>
      <url hash="e6c24f16">2025.naacl-industry.52</url>
      <bibkey>boateng-etal-2025-concept</bibkey>
    </paper>
    <paper id="53">
      <title>Towards Reliable Agents: Benchmarking Customized <fixed-case>LLM</fixed-case>-Based Retrieval-Augmented Generation Frameworks with Deployment Validation</title>
      <author><first>Kevin Shukang</first><last>Wang</last></author>
      <author><first>Karel Joshua</first><last>Harjono</last></author>
      <author><first>Ramon</first><last>Lawrence</last><affiliation>University of British Columbia</affiliation></author>
      <pages>655-661</pages>
      <abstract>The emergence of Large Language Models has created new opportunities for building agent applications across various domains. To address the lack of targeted open benchmarks for agent frameworks, we designed a benchmark that features domain-specific, small knowledge bases, and includes a diverse set of questions categorized by type, such as simple, multi-hop, aggregation, and reasoning questions. We evaluated OpenAI’s Assistants API versus a RAG assistant built with Langchain and deployed a RAG system based on benchmark insights as a course assistant over a two-year span in a computer science course. Our findings reveal how domain-specific retrieval impacts response accuracy and highlight key challenges in real-world deployment. Notably, in smaller agentic systems with constrained knowledge bases, the primary challenge shifts from retrieval accuracy to data availability in the knowledge bases. We present insights from both benchmark evaluation and real-world usage data to guide the development of more reliable and effective agentic applications.</abstract>
      <url hash="b7592228">2025.naacl-industry.53</url>
      <bibkey>wang-etal-2025-towards</bibkey>
    </paper>
    <paper id="54">
      <title>Query Variant Detection Using Retriever as Environment</title>
      <author><first>Minji</first><last>Seo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seoho</first><last>Song</last></author>
      <author><first>Hee-Cheol</first><last>Seo</last></author>
      <author><first>Young-In</first><last>Song</last></author>
      <pages>662-671</pages>
      <abstract>This paper addresses the challenge of detecting query variants—pairs of queries with identical intents. One application in commercial search engines is reformulating user queries with its variant online. While measuring pairwise query similarity has been an established standard, it often falls short of capturing semantic equivalence when word forms or order differ. We propose leveraging the retrieval as an environment feedback (EF), based on the premise that desirable retrieval outcomes from equivalent queries should be interchangeable. Experimental results on both proprietary and public datasets demonstrate the efficacy of the proposed method, both with and without LLM calls.</abstract>
      <url hash="5468e58e">2025.naacl-industry.54</url>
      <bibkey>seo-etal-2025-query</bibkey>
    </paper>
    <paper id="55">
      <title>Evaluating Bias in <fixed-case>LLM</fixed-case>s for Job-Resume Matching: Gender, Race, and Education</title>
      <author><first>Hayate</first><last>Iso</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last><affiliation>Megagon Labs, Inc</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>672-683</pages>
      <abstract>Large Language Models (LLMs) offer the potential to automate hiring by matching job descriptions with candidate resumes, streamlining recruitment processes, and reducing operational costs. However, biases inherent in these models may lead to unfair hiring practices, reinforcing societal prejudices and undermining workplace diversity. This study examines the performance and fairness of LLMs in job-resume matching tasks within the English language and U.S. context. It evaluates how factors such as gender, race, and educational background influence model decisions, providing critical insights into the fairness and reliability of LLMs in HR applications.Our findings indicate that while recent models have reduced biases related to explicit attributes like gender and race, implicit biases concerning educational background remain significant. These results highlight the need for ongoing evaluation and the development of advanced bias mitigation strategies to ensure equitable hiring practices when using LLMs in industry settings.</abstract>
      <url hash="dda6362a">2025.naacl-industry.55</url>
      <bibkey>iso-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="56">
      <title>Goal-Driven Data Story, Narrations and Explanations</title>
      <author><first>Aniya</first><last>Aggarwal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Ankush</first><last>Gupta</last><affiliation>IBM India Research Lab</affiliation></author>
      <author><first>Shivangi</first><last>Bithel</last></author>
      <author><first>Arvind</first><last>Agarwal</last><affiliation>International Business Machines and University of Maryland, College Park</affiliation></author>
      <pages>684-694</pages>
      <abstract>In this paper, we propose a system designed to process and interpret vague, open-ended, and multi-line complex natural language queries, transforming them into coherent, actionable data stories. Our system’s modular architecture comprises five components—Question Generation, Answer Generation, NLG/Chart Generation, Chart2Text, and Story Representation—each utilizing LLMs to transform data into human-readable narratives and visualizations. Unlike existing tools, our system uniquely addresses the ambiguity of vague, multi-line queries, setting a new benchmark in data storytelling by tackling complexities no existing system comprehensively handles. Our system is cost-effective, which uses open-source models without extra training and emphasizes transparency by showcasing end-to-end processing and intermediate outputs. This enhances explainability, builds user trust, and clarifies the data story generation process.</abstract>
      <url hash="3666e6e7">2025.naacl-industry.56</url>
      <bibkey>aggarwal-etal-2025-goal</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>VIT</fixed-case>-Pro: Visual Instruction Tuning for Product Images</title>
      <author><first>Vishnu</first><last>Prabhakaran</last><affiliation>Amazon</affiliation></author>
      <author><first>Purav</first><last>Aggarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Vishruit</first><last>Kulshreshtha</last><affiliation>Amazon</affiliation></author>
      <author><first>Arunita</first><last>Das</last><affiliation>Amazon</affiliation></author>
      <author><first>Sahini Venkata Sitaram</first><last>Sruti</last></author>
      <author><first>Anoop</first><last>Saladi</last><affiliation>Amazon</affiliation></author>
      <pages>695-707</pages>
      <abstract>General vision-language models (VLMs) trained on web data struggle to understand and converse about real-world e-commerce product images. We propose a cost-efficient approach for collecting training data to train a generative VLM for e-commerce product images. The key idea is to leverage large-scale, loosely-coupled image-text pairs from e-commerce stores, use a pretrained LLM to generate multimodal instruction-following data, and fine-tune a general vision-language model using LoRA. Our instruction-finetuned model, VIT-Pro, can understand and respond to queries about product images, covering diverse concepts and tasks. VIT-Pro outperforms several general-purpose VLMs on multiple vision tasks in the e-commerce domain.</abstract>
      <url hash="44a09332">2025.naacl-industry.57</url>
      <bibkey>prabhakaran-etal-2025-vit</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>A</fixed-case>uto<fixed-case>KB</fixed-case>: Automated Creation of Structured Knowledge Bases for Domain-Specific Support</title>
      <author><first>Rishav</first><last>Sahay</last><affiliation>Amazon</affiliation></author>
      <author><first>Arihant</first><last>Jain</last><affiliation>Amazon</affiliation></author>
      <author><first>Purav</first><last>Aggarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Anoop</first><last>Saladi</last><affiliation>Amazon</affiliation></author>
      <pages>708-723</pages>
      <abstract>Effective customer support requires domain-specific solutions tailored to users’ issues. However, LLMs like ChatGPT, while excelling in open-domain tasks, often face challenges such as hallucinations, lack of domain compliance, and imprecise solutions when applied to specialized contexts. RAG-based systems, designed to combine domain context from unstructured knowledge bases (KBs) with LLMs, often struggle with noisy retrievals, further limiting their effectiveness in addressing user issues. Consequently, a sanitized KB is essential to ensure solution accuracy, precision, and domain compliance. To address this, we propose AutoKB, an automated pipeline for building a domain-specific KB with a hierarchical tree structure that maps user issues to precise and domain-compliant solutions. This structure facilitates granular issue resolution by improving real-time retrieval of user-specific solutions. Experiments in troubleshooting and medical domains demonstrate that our approach significantly enhances solution correctness, preciseness, and domain compliance, outperforming LLMs and unstructured KB baselines. Moreover, AutoKB is 75 times more cost-effective than manual methods.</abstract>
      <url hash="40ed9280">2025.naacl-industry.58</url>
      <bibkey>sahay-etal-2025-autokb</bibkey>
    </paper>
    <paper id="59">
      <title>Medical Spoken Named Entity Recognition</title>
      <author><first>Khai</first><last>Le-Duc</last></author>
      <author><first>David</first><last>Thulke</last><affiliation>RWTH Aachen University and AppTek</affiliation></author>
      <author><first>Hung-Phong</first><last>Tran</last></author>
      <author><first>Long</first><last>Vo-Dang</last></author>
      <author><first>Khai-Nguyen</first><last>Nguyen</last></author>
      <author><first>Truong-Son</first><last>Hy</last><affiliation>University of Alabama at Birmingham</affiliation></author>
      <author><first>Ralf</first><last>Schlüter</last><affiliation>AppTek GmbH and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <pages>724-783</pages>
      <abstract>Spoken Named Entity Recognition (NER) aims to extract named entities from speech and categorise them into types like person, location, organization, etc. In this work, we present *VietMed-NER* - the first spoken NER dataset in the medical domain. To our knowledge, our Vietnamese real-world dataset is the largest spoken NER dataset in the world regarding the number of entity types, featuring 18 distinct types. Furthermore, we present baseline results using various state-of-the-art pre-trained models: encoder-only and sequence-to-sequence; and conduct quantitative and qualitative error analysis. We found that pre-trained multilingual models generally outperform monolingual models on reference text and ASR output and encoders outperform sequence-to-sequence models in NER tasks. By translating the transcripts, the dataset can also be utilised for text NER in the medical domain in other languages than Vietnamese. All code, data and models are publicly available.</abstract>
      <url hash="bf7a80d5">2025.naacl-industry.59</url>
      <bibkey>le-duc-etal-2025-medical</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>PLEX</fixed-case>: Adaptive Parameter-Efficient Fine-Tuning for Code <fixed-case>LLM</fixed-case>s using Lottery-Tickets</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hojae</first><last>Han</last></author>
      <author><first>Jongyoon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Naun</first><last>Kang</last><affiliation>Samsung</affiliation></author>
      <author><first>KyungJun</first><last>An</last><affiliation>Samsung</affiliation></author>
      <author><first>Sungho</first><last>Jang</last><affiliation>Samsung SDS</affiliation></author>
      <pages>784-793</pages>
      <abstract>Fine-tuning large language models (LLMs) for code generation is challenging due to computational costs and the underrepresentation of some programming languages (PLs) in pre-training. We propose PLEX, a lottery-ticket based parameter-efficient fine-tuning (PEFT) method that adapts LLMs to either well-supported and underrepresented PLs.During lottery ticket selection, PLEX employs a dual strategy: for well-represented PLs, it leverages the LLM’s full parametric knowledge by selecting from full layers, while for underrepresented PLs, it narrows the selection scope to dense layers, prioritizing the most influential parameters.Additionally, PLEX-E, a low-rank extension of PLEX, further reduces computational costs by limiting the scope of fine-tuning. On MultiPL-E benchmarks, PLEX achieves state-of-the-art performance among PEFT methods, while PLEX-E maintains competitive results with reduced computational overhead. Both variants demonstrate effective adaptation across diverse programming languages, particularly for those underrepresented in pre-training.</abstract>
      <url hash="8b101ab5">2025.naacl-industry.60</url>
      <bibkey>lee-etal-2025-plex</bibkey>
    </paper>
    <paper id="61">
      <title>Evaluating the Performance of <fixed-case>RAG</fixed-case> Methods for Conversational <fixed-case>AI</fixed-case> in the Airport Domain</title>
      <author><first>Yuyang</first><last>Li</last></author>
      <author><first>Pjm</first><last>Kerbusch</last><affiliation>Schiphol</affiliation></author>
      <author><first>Rhr</first><last>Pruim</last><affiliation>Radboud University Medical Center</affiliation></author>
      <author><first>Tobias</first><last>Käfer</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <pages>794-808</pages>
      <abstract>Airports from the top 20 in terms of annual passengers are highly dynamic environment with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions.</abstract>
      <url hash="c412ee21">2025.naacl-industry.61</url>
      <bibkey>li-etal-2025-evaluating-performance</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>LLM</fixed-case> Safety for Children</title>
      <author><first>Prasanjit</first><last>Rath</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hari</first><last>Shrawgi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Parag</first><last>Agrawal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <pages>809-821</pages>
      <abstract>This paper analyzes the safety of Large Language Models (LLMs) in interactions with children below age of 18 years. Despite the transformative applications of LLMs in various aspects of children’s lives, such as education and therapy, there remains a significant gap in understanding and mitigating potential content harms specific to this demographic. The study acknowledges the diverse nature of children, often overlooked by standard safety evaluations, and proposes a comprehensive approach to evaluating LLM safety specifically for children. We list down potential risks that children may encounter when using LLM-powered applications. Additionally, we develop Child User Models that reflect the varied personalities and interests of children, informed by literature in child care and psychology. These user models aim to bridge the existing gap in child safety literature across various fields. We utilize Child User Models to evaluate the safety of six state-of-the-art LLMs. Our observations reveal significant safety gaps in LLMs, particularly in categories harmful to children but not adults.</abstract>
      <url hash="9881a003">2025.naacl-industry.62</url>
      <bibkey>rath-etal-2025-llm</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>R</fixed-case>x<fixed-case>L</fixed-case>ens: Multi-Agent <fixed-case>LLM</fixed-case>-powered Scan and Order for Pharmacy</title>
      <author><first>Akshay</first><last>Jagatap</last><affiliation>Amazon</affiliation></author>
      <author><first>Srujana</first><last>Merugu</last></author>
      <author><first>Prakash Mandayam</first><last>Comar</last><affiliation>Amazon</affiliation></author>
      <pages>822-832</pages>
      <abstract>Automated construction of shopping cart frommedical prescriptions is a vital prerequisite forscaling up online pharmaceutical servicesin emerging markets due to the high prevalence of paper prescriptionsthat are challenging for customers to interpret.We present RxLens, a multi-step end-end Large Language Model (LLM)-based deployed solutionfor automated pharmacy cart construction comprisingmultiple steps: redaction of Personal Identifiable Information (PII),Optical Character Recognition (OCR), medication extraction, matching against the catalog, and bounding box detection for lineage. Our multi-step design leverages the synergy between retrieval and LLM-based generationto mitigate the vocabulary gaps in LLMs and fuzzy matching errors during retrieval.Empirical evaluation demonstrates that RxLens can yield up to 19% - 40% and 11% - 26% increase in Recall@3 relative to SOTA methods such as Medical Comprehend and vanilla retrieval augmentation of LLMs on handwritten and printed prescriptions respectively.We also explore LLM-based auto-evaluation as an alternative to costly manual annotations and observe a 76% - 100% match relative to human judgements on various tasks.</abstract>
      <url hash="ed7deb04">2025.naacl-industry.63</url>
      <bibkey>jagatap-etal-2025-rxlens</bibkey>
    </paper>
    <paper id="64">
      <title>Distill-<fixed-case>C</fixed-case>: Enhanced <fixed-case>NL</fixed-case>2<fixed-case>SQL</fixed-case> via Distilled Customization with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Cong Duy Vu</first><last>Hoang</last><affiliation>Oracle Corporation</affiliation></author>
      <author><first>Gioacchino</first><last>Tangari</last><affiliation>Oracle</affiliation></author>
      <author><first>Clemence</first><last>Lanfranchi</last><affiliation>Oracle</affiliation></author>
      <author><first>Dalu</first><last>Guo</last></author>
      <author><first>Paul</first><last>Cayet</last><affiliation>Oracle</affiliation></author>
      <author><first>Steve</first><last>Siu</last><affiliation>Oracle</affiliation></author>
      <author><first>Don</first><last>Dharmasiri</last><affiliation>Oracle</affiliation></author>
      <author><first>Yuan-Fang</first><last>Li</last><affiliation>Monash University and Oracle</affiliation></author>
      <author><first>Long</first><last>Duong</last><affiliation>Oracle</affiliation></author>
      <author><first>Damien</first><last>Hilloulin</last><affiliation>Oracle labs</affiliation></author>
      <author><first>Rhicheek</first><last>Patra</last><affiliation>Oracle</affiliation></author>
      <author><first>Sungpack</first><last>Hong</last><affiliation>Oracle</affiliation></author>
      <author><first>Hassan</first><last>Chafi</last><affiliation>Oracle</affiliation></author>
      <pages>833-848</pages>
      <abstract>The growing adoption of large language models (LLMs) in business applications has amplified interest in Natural Language to SQL (NL2SQL) solutions, in which there is competing demand for high performance and efficiency. Domain- and customer-specific requirements further complicate the problem. To address this conundrum, we introduce Distill-C, a distilled customization framework tailored for NL2SQL tasks. Distill-C utilizes large teacher LLMs to produce high-quality synthetic data through a robust and scalable pipeline. Finetuning smaller and open-source LLMs on this synthesized data enables them to rival or outperform teacher models an order of magnitude larger. Evaluated on multiple challenging benchmarks, Distill-C achieves an average improvement of 36% in execution accuracy compared to the base models from three distinct LLM families. Additionally, on three internal customer benchmarks, Distill-C demonstrates a 22.6% performance improvement over the base models. Our results demonstrate that Distill-C is an effective, high-performing and generalizable approach for deploying lightweight yet powerful NL2SQL models, delivering exceptional accuracies while maintaining low computational cost.</abstract>
      <url hash="85d6e6e1">2025.naacl-industry.64</url>
      <bibkey>hoang-etal-2025-distill</bibkey>
    </paper>
    <paper id="65">
      <title>e<fixed-case>C</fixed-case>-<fixed-case>T</fixed-case>ab2<fixed-case>T</fixed-case>ext: Aspect-Based Text Generation from e-Commerce Product Tables</title>
      <author><first>Luis Antonio Gutierrez</first><last>Guanilo</last><affiliation>UTEC - Universidad de Ingeniería y Tecnología and UTEC - Universidad de Ingeniería y Tecnología</affiliation></author>
      <author><first>Mir Tafseer</first><last>Nayeem</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Cristian Jose Lopez Del</first><last>Alamo</last><affiliation>UTEC - Universidad de Ingeniería y Tecnología</affiliation></author>
      <author><first>Davood</first><last>Rafiei</last><affiliation>University of Alberta</affiliation></author>
      <pages>849-867</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.</abstract>
      <url hash="2e86690f">2025.naacl-industry.65</url>
      <bibkey>guanilo-etal-2025-ec</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>RAD</fixed-case>-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues</title>
      <author><first>Tzu-Lin</first><last>Kuo</last></author>
      <author><first>FengTing</first><last>Liao</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Mu-Wei</first><last>Hsieh</last></author>
      <author><first>Fu-Chieh</first><last>Chang</last><affiliation>National Taiwan University and Mediatek Research</affiliation></author>
      <author><first>Po-Chun</first><last>Hsu</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Da-shan</first><last>Shiu</last></author>
      <pages>868-902</pages>
      <url hash="fde0fba0">2025.naacl-industry.66</url>
      <bibkey>kuo-etal-2025-rad</bibkey>
    </paper>
    <paper id="67">
      <title>Conflict and Overlap Classification in Construction Standards Using a Large Language Model</title>
      <author><first>Seong-Jin</first><last>Park</last><affiliation>The Catholic University of Korea</affiliation></author>
      <author><first>Youn-Gyu</first><last>Jin</last></author>
      <author><first>Hyun-Young</first><last>Moon</last></author>
      <author><first>Choi</first><last>Bong-Hyuck</last><affiliation>KOREA INSTITUTE of CIVIL ENGINEERING and BUILDING TECHNOLOGY</affiliation></author>
      <author><first>Lee Seung</first><last>Hwan</last><affiliation>한국건설기술연구원</affiliation></author>
      <author><first>Ohjoon</first><last>Kwon</last><affiliation>NAVER</affiliation></author>
      <author><first>Kang-Min</first><last>Kim</last><affiliation>The Catholic University of Korea</affiliation></author>
      <pages>903-917</pages>
      <abstract>Construction standards across different countries provide technical guidelines to ensure the quality and safety of buildings and facilities, with periodic revisions to accommodate advances in construction technology. However, these standards often contain overlapping or conflicting content owing to their broad scope and interdependence, complicating the revision process and creating public inconvenience. Although current expert-driven manual approaches aim to mitigate these issues, they are time-consuming, costly, and error-prone. To address these challenges, we propose conflict and overlap classification in construction standards using a large language model (COSLLM), a framework that leverages a construction domain-adapted large language model for the semantic comparison of sentences in construction standards. COSLLM utilizes a two-step reasoning process that adaptively employs chain-of-thought reasoning for the in-depth analysis of sentences suspected of overlaps or conflicts, ensuring computational and temporal efficiency while maintaining high classification accuracy. The framework achieved an accuracy of 97.9% and a macro F1-score of 0.907 in classifying real-world sentence pairs derived from Korean construction standards as overlapping, conflicting, or neutral. Furthermore, we develop and deploy a real-time web-based system powered by COSLLM to facilitate the efficient establishment and revision of construction standards.</abstract>
      <url hash="0c4a4ed1">2025.naacl-industry.67</url>
      <bibkey>park-etal-2025-conflict</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>P</fixed-case>rotein2<fixed-case>T</fixed-case>ext: Resampling Mechanism to Translate Protein Sequences into Human-Interpretable Text</title>
      <author><first>Ala</first><last>Jararweh</last></author>
      <author><first>Oladimeji</first><last>Macaulay</last><affiliation>University of New Mexico</affiliation></author>
      <author><first>David</first><last>Arredondo</last></author>
      <author><first>Yue</first><last>Hu</last><affiliation>University of New Mexico</affiliation></author>
      <author><first>Luis E</first><last>Tafoya</last></author>
      <author><first>Kushal</first><last>Virupakshappa</last></author>
      <author><first>Avinash</first><last>Sahu</last><affiliation>University of New Mexico</affiliation></author>
      <pages>918-937</pages>
      <abstract>Proteins play critical roles in biological systems, yet 99.7% of over 227 million known protein sequences remain uncharacterized due to the limitations of experimental methods. To assist experimentalists in narrowing down hypotheses and accelerating protein characterization, we present Protein2Text, a multimodal large language model that interprets protein sequences and generates informative text to address open-ended questions about protein functions and attributes. By integrating a resampling mechanism within an adapted LLaVA framework, our model effectively maps protein sequences into a language-compatible space, enhancing its capability to handle diverse and complex queries. Trained on a newly curated dataset derived from PubMed articles and rigorously evaluated using four comprehensive benchmarks—including in-domain and cross-domain evaluations—Protein2Text outperforms several existing models in open-ended question-answering tasks. Our work also highlights the limitations of current evaluation metrics applied to template-based approaches, which may lead to misleading results, emphasizing the need for unbiased assessment methods. Our model weights, evaluation datasets, and evaluation scripts are publicly available at https://github.com/alaaj27/Protein2Text.git.</abstract>
      <url hash="7e9ce47d">2025.naacl-industry.68</url>
      <bibkey>jararweh-etal-2025-protein2text</bibkey>
    </paper>
    <paper id="69">
      <title>Cracking the Code: Multi-domain <fixed-case>LLM</fixed-case> Evaluation on Real-World Professional Exams in <fixed-case>I</fixed-case>ndonesia</title>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>938-948</pages>
      <abstract>While knowledge evaluation in large language models has predominantly focused on academic subjects like math and physics, these assessments often fail to capture the practical demands of real-world professions. In this paper, we introduce IndoCareer, a dataset comprising 8,834 multiple-choice questions designed to evaluate performance in vocational and professional certification exams across various fields. With a focus on Indonesia, IndoCareer provides rich local contexts, spanning six key sectors: (1) healthcare, (2) insurance and finance, (3) creative and design, (4) tourism and hospitality, (5) education and training, and (6) law. Our comprehensive evaluation of 27 large language models shows that these models struggle particularly in fields with strong local contexts, such as insurance and finance. Additionally, while using the entire dataset, shuffling answer options generally maintains consistent evaluation results across models, but it introduces instability specifically in the insurance and finance sectors.</abstract>
      <url hash="0c1d83e7">2025.naacl-industry.69</url>
      <bibkey>koto-2025-cracking</bibkey>
    </paper>
    <paper id="70">
      <title><fixed-case>C</fixed-case>ode<fixed-case>G</fixed-case>en<fixed-case>W</fixed-case>rangler: Data Wrangling task automation using Code-Generating Models</title>
      <author><first>Ashlesha</first><last>Akella</last></author>
      <author><first>Abhijit</first><last>Manatkar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Krishnasuri</first><last>Narayanam</last></author>
      <author><first>Sameep</first><last>Mehta</last><affiliation>International Business Machines</affiliation></author>
      <pages>949-960</pages>
      <abstract>Assuring the data quality of tabular datasets is essential for the efficiency of the diverse tabular downstream tasks (like summarization and fact-checking). Data-wrangling tasks effectively address the challenges associated with structured data processing to improve the quality of tabular data. Traditional statistical methods handle numeric data efficiently but often fail to understand the semantic context of the textual data in tables. Deep learning approaches are resource-intensive, requiring task and dataset-specific training. Addressing these shortcomings, we present an automated system that leverages LLMs to generate executable code for data-wrangling tasks like missing value imputation, error detection, and error correction. Our system aims to identify inherent patterns in the data while leveraging external knowledge, effectively addressing both memory-independent and memory-dependent tasks.</abstract>
      <url hash="3738922c">2025.naacl-industry.70</url>
      <bibkey>akella-etal-2025-codegenwrangler</bibkey>
    </paper>
    <paper id="71">
      <title>Dialogue Language Model with Large-Scale Persona Data Engineering</title>
      <author><first>Mengze</first><last>Hong</last></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <author><first>Chaotao</first><last>Chen</last></author>
      <author><first>Rongzhong</first><last>Lian</last></author>
      <author><first>Di</first><last>Jiang</last></author>
      <pages>961-970</pages>
      <abstract>Maintaining persona consistency is paramount in the application of open-domain dialogue systems, as exemplified by models like ChatGPT. Despite significant advancements, the limited scale and diversity of current persona dialogue datasets remain challenges to achieving robust persona-consistent dialogue models. In this study, drawing inspiration from the success of large-scale pre-training, we introduce PPDS, an open-domain persona dialogue system that employs extensive generative pre-training on a persona dialogue dataset to enhance persona consistency. Specifically, we present a persona extraction model designed to autonomously and precisely generate vast persona dialogue datasets. Additionally, we unveil a pioneering persona augmentation technique to address the invalid persona bias inherent in the constructed dataset. Both quantitative and human evaluations consistently highlight the superior response quality and persona consistency of our proposed model, underscoring its effectiveness.</abstract>
      <url hash="0cc9b987">2025.naacl-industry.71</url>
      <bibkey>hong-etal-2025-dialogue</bibkey>
    </paper>
    <paper id="72">
      <title>Developing a Reliable, Fast, General-Purpose Hallucination Detection and Mitigation Service</title>
      <author><first>Song</first><last>Wang</last><affiliation>Microsoft Azure AI</affiliation></author>
      <author><first>Xun</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jie</first><last>Mei</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yujia</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <author><first>Si-Qing</first><last>Chen</last></author>
      <author><first>Wayne</first><last>Xiong</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>971-978</pages>
      <abstract>Hallucination, a phenomenon where large language models (LLMs) produce output that is factually incorrect or unrelated to the input, is a major challenge for LLM applications that require accuracy and dependability. In this paper, we introduce a reliable and high-speed production system aimed at detecting and rectifying the hallucination issue within LLMs. Our system encompasses named entity recognition (NER), natural language inference (NLI), span-based detection (SBD), and an intricate decision tree-based process to reliably detect a wide range of hallucinations in LLM responses. Furthermore, we have crafted a rewriting mechanism that maintains an optimal mix of precision, response time, and cost-effectiveness. We detail the core elements of our framework and underscore the paramount challenges tied to response time, availability, and performance metrics, which are crucial for real-world deployment of these technologies. Our extensive evaluation, utilizing offline data and live production traffic, confirms the efficacy of our proposed framework and service.</abstract>
      <url hash="9b082764">2025.naacl-industry.72</url>
      <bibkey>wang-etal-2025-developing-reliable</bibkey>
    </paper>
    <paper id="73">
      <title>Improved Near-Duplicate Detection for Aggregated and Paywalled News-Feeds</title>
      <author><first>Siddharth</first><last>Tumre</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Sangameshwar</first><last>Patil</last><affiliation>Indian Institute of Technology, Madras and Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Alok</first><last>Kumar</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>979-987</pages>
      <abstract>News aggregators play a key role in the rapidly evolving digital landscape by providing comprehensive and timely news stories aggregated from diverse sources into one feed. As these articles are sourced from different outlets, they often end up covering the same underlying event but differ in phrasing, formatting or supplemented with additional details. It is crucial for the news aggregators to identify these near-duplicates, improving the content quality and user engagement by steering away from redundant information. The problem of near-duplicate news detection has become harder with increasing use of paywalls by the news websites resulting in restricted access to the content. It is now common to get only the headline and a short snippet from the article. Previous works have concentrated on full length versions of documents such as webpages. There is very little work that focuses on this variation of the near-duplicate detection problem in which only headline and a small text blurb is available for each news article. We propose Near-Duplicate Detection Using Metadata Augmented Communities (NDD-MAC) approach that combines embeddings from pretrained language model (PLM) and latent metadata of a news article followed by community detection to identify clusters of near-duplicates. We show the efficacy of proposed approach using 2 different real-world datasets. By integrating metadata with community detection, NDD-MAC is able to detect nuanced similarities and differences in news snippets and offers an industrial scale solution for the near-duplicate detection in scenarios with restricted content availability.</abstract>
      <url hash="7f88a190">2025.naacl-industry.73</url>
      <bibkey>tumre-etal-2025-improved</bibkey>
    </paper>
    <paper id="74">
      <title>Pisets: A Robust Speech Recognition System for Lectures and Interviews</title>
      <author><first>Ivan</first><last>Bondarenko</last><affiliation>Novosibirsk State University</affiliation></author>
      <author><first>Daniil</first><last>Grebenkin</last></author>
      <author><first>Oleg</first><last>Sedukhin</last><affiliation>Siberian Neuronets LLC</affiliation></author>
      <author><first>Mikhail</first><last>Klementev</last></author>
      <author><first>Derunets</first><last>Roman</last><affiliation>Novosibirsk State University</affiliation></author>
      <author><first>Lyudmila</first><last>Budneva</last><affiliation>Novosibirsk State University</affiliation></author>
      <pages>988-997</pages>
      <abstract>This work presents a speech-to-text system “Pisets” for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system’s effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of “Pisets” system is publicly available at GitHub: https://github.com/bond005/pisets.</abstract>
      <url hash="06dd1389">2025.naacl-industry.74</url>
      <bibkey>bondarenko-etal-2025-pisets</bibkey>
    </paper>
    <paper id="75">
      <title><fixed-case>CPRM</fixed-case>: A <fixed-case>LLM</fixed-case>-based Continual Pre-training Framework for Relevance Modeling in Commercial Search</title>
      <author><first>Kaixin</first><last>Wu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yixin</first><last>Ji</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zeyuan</first><last>Chen</last><affiliation>Ant Group</affiliation></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Hong</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Baijun</first><last>Ji</last><affiliation>Soochow University</affiliation></author>
      <author><first>Xu</first><last>Jia</last></author>
      <author><first>Zhongyi</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <author><first>Yuan</first><last>Zhou</last></author>
      <author><first>Linjian</first><last>Mo</last><affiliation>Ant Group</affiliation></author>
      <pages>998-1008</pages>
      <abstract>Relevance modeling between queries and items stands as a pivotal component in commercial search engines, directly affecting the user experience. Given the remarkable achievements of large language models (LLMs) in various natural language processing (NLP) tasks, LLM-based relevance modeling is gradually being adopted within industrial search systems. Nevertheless, foundational LLMs lack domain-specific knowledge and do not fully exploit the potential of in-context learning. Furthermore, structured item text remains underutilized, and there is a shortage in the supply of corresponding queries and background knowledge. We thereby propose CPRM (Continual Pre-training for Relevance Modeling), a framework designed for the continual pre-training of LLMs to address these issues. Our CPRM framework includes three modules: 1) employing both queries and multi-field item to jointly pre-train for enhancing domain knowledge, 2) applying in-context pre-training, a novel approach where LLMs are pre-trained on a sequence of related queries or items, and 3) conducting reading comprehension on items to produce associated domain knowledge and background information (e.g., generating summaries and corresponding queries) to further strengthen LLMs. Results on offline experiments and online A/B testing demonstrate that our model achieves convincing performance compared to strong baselines.</abstract>
      <url hash="cba452a5">2025.naacl-industry.75</url>
      <bibkey>wu-etal-2025-cprm</bibkey>
    </paper>
    <paper id="76">
      <title>Schema and Natural Language Aware In-Context Learning for Improved <fixed-case>G</fixed-case>raph<fixed-case>QL</fixed-case> Query Generation</title>
      <author><first>Nitin</first><last>Gupta</last></author>
      <author><first>Manish</first><last>Kesarwani</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sambit</first><last>Ghosh</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sameep</first><last>Mehta</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Carlos</first><last>Eberhardt</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dan</first><last>Debrunner</last><affiliation>International Business Machines</affiliation></author>
      <pages>1009-1015</pages>
      <abstract>GraphQL offers a flexible alternative to REST APIs, allowing precise data retrieval across multiple sources in a single query. However, generating complex GraphQL queries remains a significant challenge. Large Language Models (LLMs), while powerful, often produce suboptimal queries due to limited exposure to GraphQL schemas and their structural intricacies.Custom prompt engineering with in-context examples is a common approach to guide LLMs, but existing methods, like randomly selecting examples, often yield unsatisfactory results. While semantic similarity-based selection is effective in other domains, it falls short for GraphQL, where understanding schema-specific nuances is crucial for accurate query formulation.To address this, we propose a Schema and NL-Aware In-context Learning (SNAIL) framework that integrates both structural and semantic information from GraphQL schemas with natural language inputs, enabling schema-aware in-context learning. Unlike existing methods, our approach captures the complexities of GraphQL schemas to improve query generation accuracy.We validate this framework on a publicly available complex GraphQL test dataset, demonstrating notable performance improvements, with specific query classes showing up to a 20% performance improvement for certain LLMs. As GraphQL adoption grows, with Gartner predicting over 60% of enterprises will use it in production by 2027, this work addresses a critical need, paving the way for more efficient and reliable GraphQL query generation in enterprise applications.</abstract>
      <url hash="6538cc3e">2025.naacl-industry.76</url>
      <bibkey>gupta-etal-2025-schema</bibkey>
    </paper>
    <paper id="77">
      <title>Chatbot Arena Estimate: towards a generalized performance benchmark for <fixed-case>LLM</fixed-case> capabilities</title>
      <author><first>Lucas</first><last>Spangher</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Tianle</first><last>Li</last></author>
      <author><first>William F.</first><last>Arnold</last></author>
      <author><first>Nick</first><last>Masiewicki</last><affiliation>Google</affiliation></author>
      <author><first>Xerxes</first><last>Dotiwalla</last><affiliation>Google</affiliation></author>
      <author><first>Rama Kumar</first><last>Pasumarthi</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Grabowski</last><affiliation>University of California, Berkeley and Google</affiliation></author>
      <author><first>Eugene</first><last>Ie</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Gruhl</last><affiliation>Google</affiliation></author>
      <pages>1016-1025</pages>
      <abstract>In industrial LLM development, evaluating large language models (LLMs) is critical for tasks like benchmarking internal models and detecting regressions during fine-tuning, but existing benchmark aggregation methods, such as Elo-based systems, can be resource-intensive, public facing, and time-consuming. Here, we describe <b>Chatbot Arena Estimate (CAE)</b>, a practical framework for aggregating performance across diverse benchmarks. The framework, developed and widely adopted within our organization, addresses the need for quick, accurate, and cost-efficient evaluations of LLMs. CAE generates two primary metrics: a “Goodness” score (answer accuracy) and a “Fastness” score (cost or queries per second, QPS). These metrics allow for model ranking both overall and within specific subdomains, enabling informed decisions during model iteration and deployment. We demonstrate CAE’s effectiveness by comparing it with existing benchmarks, including the full Chatbot Arena and the MMLU leaderboard. Notably, our approach achieves higher Pearson correlation with Chatbot Arena Elo scores than MMLU’s correlation with Chatbot Arena Elo scores, validating its reliability for real-world LLM evaluation.</abstract>
      <url hash="442b19a0">2025.naacl-industry.77</url>
      <bibkey>spangher-etal-2025-chatbot</bibkey>
    </paper>
    <paper id="78">
      <title>Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models</title>
      <author><first>Arvind Krishna</first><last>Sridhar</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Yinyi</first><last>Guo</last></author>
      <author><first>Erik</first><last>Visser</last></author>
      <pages>1026-1035</pages>
      <abstract>The Audio Question Answering (AQA) task includes audio event classification, audio captioning, and open-ended reasoning. Recently, AQA has garnered attention due to the advent of Large Audio Language Models (LALMs). Current literature focuses on constructing LALMs by integrating audio encoders with text-only Large Language Models (LLMs) through a projection module. While LALMs excel in general audio understanding, they are limited in temporal reasoning, which may hinder their commercial applications and on-device deployment. This paper addresses these challenges and limitations in audio temporal reasoning. First, we introduce a data augmentation technique for generating reliable audio temporal questions and answers using an LLM. Second, we perform a further fine-tuning of an existing baseline using curriculum learning strategy to specialize in temporal reasoning without compromising performance on fine-tuned tasks. We demonstrate the performance of our model using state-of-the-art LALMs on public audio benchmark datasets. Third, we implement our AQA model on-device locally and investigate its CPU inference for edge applications.</abstract>
      <url hash="a1fd8df3">2025.naacl-industry.78</url>
      <bibkey>sridhar-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>H</fixed-case>y<fixed-case>PA</fixed-case>-<fixed-case>RAG</fixed-case>: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for <fixed-case>AI</fixed-case> Legal and Policy Applications</title>
      <author><first>Rishi</first><last>Kalra</last></author>
      <author><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Ayesha</first><last>Gulley</last></author>
      <author><first>Airlie</first><last>Hilliard</last></author>
      <author><first>Xin</first><last>Guan</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Adriano</first><last>Koshiyama</last></author>
      <author><first>Philip Colin</first><last>Treleaven</last><affiliation>University College London, University of London</affiliation></author>
      <pages>1036-1054</pages>
      <abstract>Large Language Models (LLMs) face limitations in AI legal and policy applications due to outdated knowledge, hallucinations, and poor reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems address these issues by incorporating external knowledge, but suffer from retrieval errors, ineffective context integration, and high operational costs. This paper presents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the AI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG integrates a query complexity classifier for adaptive parameter tuning, a hybrid retrieval approach combining dense, sparse, and knowledge graph methods, and a comprehensive evaluation framework with tailored question types and metrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval accuracy, response fidelity, and contextual precision, offering a robust and adaptable solution for high-stakes legal and policy applications.</abstract>
      <url hash="8662d53b">2025.naacl-industry.79</url>
      <bibkey>kalra-etal-2025-hypa</bibkey>
    </paper>
    <paper id="80">
      <title>An Efficient Context-Dependent Memory Framework for <fixed-case>LLM</fixed-case>-Centric Agents</title>
      <author><first>Pengyu</first><last>Gao</last></author>
      <author><first>Jinming</first><last>Zhao</last><affiliation>Qiyuan Lab</affiliation></author>
      <author><first>Xinyue</first><last>Chen</last></author>
      <author><first>Long</first><last>Yilin</last></author>
      <pages>1055-1069</pages>
      <abstract>In human cognitive memory psychology, the context-dependent effect helps retrieve key memory cues essential for recalling relevant knowledge in problem-solving. Inspired by this, we introduce the context-dependent memory framework (CDMem), an efficient architecture miming human memory processes through multistage encoding, context-aware storage, and retrieval strategies for LLM-centric agents. We propose multistage memory encoding strategies for acquiring high-quality multilevel knowledge: expert encoding compresses raw trajectories from a domain-expert perspective, short-term encoding consolidates experiences from current tasks, and long-term encoding reflects insights from past tasks. For memory storage and retrieval, we design a graph-structured, context-dependent indexing mechanism that allows agents to efficiently and accurately recall the most relevant multilevel knowledge tailored to the current task and environmental context. Furthermore, the proposed CDMem framework is an online learning architecture, enabling agents to efficiently learn and update memory while adapting to novel environments and tasks in real-world applications. We conducted extensive experiments on two interactive decision-making benchmarks in the navigation and manipulation domain, ALFWorld and ScienceWorld. Using GPT-4o-mini, our method surpasses state-of-the-art online LLM-centric approaches, achieving success rates of 85.8% and 56.0%, respectively. We hope this work will serve as a valuable reference for the academic and industrial communities in advancing agent-based applications.</abstract>
      <url hash="bd5f4c8c">2025.naacl-industry.80</url>
      <bibkey>gao-etal-2025-efficient-context</bibkey>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 4: Student Research Workshop)</booktitle>
      <editor><first>Abteen</first><last>Ebrahimi</last></editor>
      <editor><first>Samar</first><last>Haider</last></editor>
      <editor><first>Emmy</first><last>Liu</last></editor>
      <editor><first>Sammar</first><last>Haider</last></editor>
      <editor><first>Maria</first><last>Leonor Pacheco</last></editor>
      <editor><first>Shira</first><last>Wein</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, USA</address>
      <month>April</month>
      <year>2025</year>
      <url hash="8ebf6864">2025.naacl-srw</url>
      <venue>naacl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-192-6</isbn>
    </meta>
    <frontmatter>
      <url hash="47e20006">2025.naacl-srw.0</url>
      <bibkey>naacl-2025-srw</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation</title>
      <author><first>Yirong</first><last>Sun</last></author>
      <author><first>Dawei</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Chen</last></author>
      <author><first>Erjia</first><last>Xiao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xinghao</first><last>Chen</last></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <pages>1-17</pages>
      <abstract>Large language models (LLMs) have excelled in various NLP tasks, including machine translation (MT), yet most studies focus on sentence-level translation. This work investigates the inherent capability of instruction-tuned LLMs for document-level translation (docMT). Unlike prior approaches that require specialized techniques, we evaluate LLMs by directly prompting them to translate entire documents in a single pass. Our results show that this method improves translation quality compared to translating sentences separately, even without document-level fine-tuning. However, this advantage is not reflected in BLEU scores, which often favor sentence-based translations. We propose using the LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess document coherence, accuracy, and fluency in a more nuanced way than n-gram-based metrics. Overall, our work demonstrates that instruction-tuned LLMs can effectively leverage document context for translation. However, we caution against using BLEU scores for evaluating docMT, as they often provide misleading outcomes, failing to capture the quality of document-level translation.</abstract>
      <url hash="499e2777">2025.naacl-srw.1</url>
      <bibkey>sun-etal-2025-fine</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>INSIGHTBUDDY</fixed-case>-<fixed-case>AI</fixed-case>: Medication Extraction and Entity Linking using Pre-Trained Language Models and Ensemble Learning</title>
      <author><first>Pablo</first><last>Romero</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>18-27</pages>
      <abstract>This paper presents our system, InsightBuddy-AI, designed for extracting medication mentions and their associated attributes, and for linking these entities to established clinical terminology resources, including SNOMED-CT, the British National Formulary (BNF), ICD, and the Dictionary of Medicines and Devices (dm+d).To perform medication extraction, we investigated various ensemble learning approaches, including stacked and voting ensembles (using first, average, and max voting methods) built upon eight pre-trained language models (PLMs). These models include general-domain PLMs—BERT, RoBERTa, and RoBERTa-Large—as well as domain-specific models such as BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT.The system targets the extraction of drug-related attributes such as adverse drug effects (ADEs), dosage, duration, form, frequency, reason, route, and strength.Experiments conducted on the n2c2-2018 shared task dataset demonstrate that ensemble learning methods outperformed individually fine-tuned models, with notable improvements of 2.43% in Precision and 1.35% in F1-score.We have also developed cross-platform desktop applications for both entity recognition and entity linking, available for Windows and macOS.The InsightBuddy-AI application is freely accessible for research use at <url>https://github.com/HECTA-UoM/InsightBuddy-AI</url>.</abstract>
      <url hash="fcb97dc0">2025.naacl-srw.2</url>
      <bibkey>romero-etal-2025-insightbuddy</bibkey>
    </paper>
    <paper id="3">
      <title>Linguistic Features in <fixed-case>G</fixed-case>erman <fixed-case>BERT</fixed-case>: The Role of Morphology, Syntax, and Semantics in Multi-Class Text Classification</title>
      <author><first>Henrike</first><last>Beyer</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>28-39</pages>
      <abstract>Most studies on the linguistic information encoded by BERT primarily focus on English. Our study examines a monolingual German BERT model using a semantic classification task on newspaper articles, analysing the linguistic features influencing classification decisions through SHAP values. We use the TüBa-D/Z corpus, a resource with gold-standard annotations for a set of linguistic features, including POS, inflectional morphology, phrasal, clausal, and dependency structures. Semantic features of nouns are evaluated via the GermaNet ontology using shared hypernyms. Our results indicate that the features identified in English also affect classification in German but suggests important language- and task-specific features as well.</abstract>
      <url hash="8c5f8685">2025.naacl-srw.3</url>
      <bibkey>beyer-frassinelli-2025-linguistic</bibkey>
    </paper>
    <paper id="4">
      <title>Thesis Proposal: Uncertainty in Knowledge Graph Embeddings</title>
      <author><first>Yuqicheng</first><last>Zhu</last><affiliation>Universität Stuttgart</affiliation></author>
      <pages>40-47</pages>
      <abstract>Knowledge Graph Embedding (KGE) methods are widely used to map entities and relations from knowledge graphs (KGs) into continuous vector spaces, enabling non-classical reasoning over knowledge structures. Despite their effectiveness, the uncertainty of KGE methods has not been extensively studied in the literature. This gap poses significant challenges, particularly when deploying KGE models in high-stakes domains like medicine, where reliability and risk assessment are critical. This dissertation seeks to investigate various types of uncertainty in KGE methods and explore strategies to quantify, mitigate, and reason under uncertainty effectively. The outcomes of this research will contribute to enhancing the reliability of KGE methods, providing greater confidence in their use beyond benchmark datasets, and supporting their application in real-world, high-stakes domains.</abstract>
      <url hash="b83314cf">2025.naacl-srw.4</url>
      <bibkey>zhu-2025-thesis</bibkey>
    </paper>
    <paper id="5">
      <title>Detecting Sexism in Tweets: A Sentiment Analysis and Graph Neural Network Approach</title>
      <author><first>Diana P.</first><last>Madera-Espíndola</last></author>
      <author><first>Zoe</first><last>Caballero-Domínguez</last><affiliation>Instituto Tecnológico y de Estudios Superiores de Monterrey</affiliation></author>
      <author><first>Valeria J.</first><last>Ramírez-Macías</last><affiliation>Instituto Tecnológico y de Estudios Superiores de Monterrey</affiliation></author>
      <author><first>Sabur</first><last>Butt</last></author>
      <author><first>Hector</first><last>Ceballos</last><affiliation>Tecnologico de Monterrey</affiliation></author>
      <pages>48-54</pages>
      <abstract>In the digital age, social media platforms like Twitter serve as an extensive repository of public discourse, including instances of sexism. It is important to identify such behavior since radicalized ideologies can lead to real-world violent acts. This project aims to develop a deep learning-based tool that leverages a combination of BERT (both English and multilingual versions) and GraphSAGE, a Graph Neural Network (GNN) model, alongside sentiment analysis and natural language processing (NLP) techniques. The tool is designed to analyze tweets for sexism detection and classify them into five categories.</abstract>
      <url hash="0afb1af2">2025.naacl-srw.5</url>
      <bibkey>madera-espindola-etal-2025-detecting</bibkey>
    </paper>
    <paper id="6">
      <title>Towards Codec-<fixed-case>LM</fixed-case> Co-design for Neural Codec Language Models</title>
      <author><first>Shih-Lun</first><last>Wu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Aakash</first><last>Lahoti</last></author>
      <author><first>Arjun D</first><last>Desai</last></author>
      <author><first>Karan</first><last>Goel</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chris</first><last>Donahue</last><affiliation>Carnegie Mellon University and Google</affiliation></author>
      <author><first>Albert</first><last>Gu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>55-65</pages>
      <abstract>Neural codec language models (or codec LMs) are emerging as a powerful framework for audio generation tasks like text-to-speech (TTS). These models leverage advancements in language modeling and residual vector quantization (RVQ)-based audio codecs, which compress audios into discrete codes for LMs to process. Despite the close interdependence of codecs and LMs in these systems, research on codecs and LMs has largely remained siloed. In this work, we propose three techniques for better codec-LM co-design: (i) a frame-wise codec encoder that improves both LM log-likelihood and end-to-end TTS metrics, (ii) LM codebook level dropout, a method to efficiently navigate a portion of the codec-LM design space by training a single LM, and (iii) increased codec frame duration, which we show can accelerate inference while maintaining end-to-end performance. Our experiments demonstrate that combining all three co-design techniques results in doubled inference speed, and improvements in intelligibility, audio quality, and speaker control in TTS relative to a siloed baseline.</abstract>
      <url hash="405890f8">2025.naacl-srw.6</url>
      <bibkey>wu-etal-2025-towards</bibkey>
    </paper>
    <paper id="7">
      <title>Low-resource Machine Translation for Code-switched <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian Language Pair</title>
      <author><first>Maksim</first><last>Borisov</last></author>
      <author><first>Zhanibek</first><last>Kozhirbayev</last><affiliation>National Laboratory Astana, Nazarbayev University</affiliation></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <pages>66-76</pages>
      <abstract>Machine translation for low-resource language pairs is a challenging task. This task could become extremely difficult once a speaker uses code switching. We present the first code-switching Kazakh-Russian parallel corpus.Additionally, we propose a method to build a machine translation model for code-switched Kazakh-Russian language pair with no labeled data. Our method is basing on generation of synthetic data. This method results in a model beating an existing commercial system by human evaluation.</abstract>
      <url hash="1f171cd8">2025.naacl-srw.7</url>
      <bibkey>borisov-etal-2025-low</bibkey>
    </paper>
    <paper id="8">
      <title>Generative Product Recommendations for Implicit Superlative Queries</title>
      <author><first>Kaustubh</first><last>Dhole</last><affiliation>Emory University</affiliation></author>
      <author><first>Nikhita</first><last>Vedula</last><affiliation>Amazon</affiliation></author>
      <author><first>Saar</first><last>Kuzi</last><affiliation>Amazon</affiliation></author>
      <author><first>Giuseppe</first><last>Castellucci</last><affiliation>Amazon</affiliation></author>
      <author><first>Eugene</first><last>Agichtein</last><affiliation>Emory University</affiliation></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>77-91</pages>
      <abstract>In recommender systems, users often seek the best products through indirect, vague, or under-specified queries such as “best shoes for trail running.” These queries, referred to as implicit superlative queries, pose a challenge for standard retrieval and ranking systems due to their lack of explicit attribute mentions and the need for identifying and reasoning over complex attributes. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking and reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema, called SUPERB, for annotating the best product candidates for superlative queries, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our newly created dataset, providing insights and discussing how to integrate these findings into real-world e-commerce production systems.</abstract>
      <url hash="9b333aa9">2025.naacl-srw.8</url>
      <bibkey>dhole-etal-2025-generative</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>on<fixed-case>Q</fixed-case>uer: A Framework for Concept-Based Quiz Generation</title>
      <author><first>Yicheng</first><last>Fu</last></author>
      <author><first>Zikui</first><last>Wang</last></author>
      <author><first>Liuxin</first><last>Yang</last></author>
      <author><first>Meiqing</first><last>Huo</last></author>
      <author><first>Zhongdongming</first><last>Dai</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>92-104</pages>
      <abstract>Quizzes play a crucial role in education by reinforcing students’ understanding of key concepts and encouraging self-directed exploration. However, compiling high-quality quizzes can be challenging and require deep expertise and insight into specific subject matter. Although LLMs have greatly enhanced the efficiency of quiz generation, concerns remain regarding the quality of these AI-generated quizzes and their educational impact on students. To address these issues, we introduce ConQuer, a concept-based quiz generation framework that leverages external knowledge sources. We employ comprehensive evaluation dimensions to assess the quality of the generated quizzes, using LLMs as judges. Our experiment results demonstrate a 4.8% improvement in evaluation scores and a 77.52% win rate in pairwise comparisons against baseline quiz sets. Ablation studies further underscore the effectiveness of each component in our framework.</abstract>
      <url hash="f7167de5">2025.naacl-srw.9</url>
      <bibkey>fu-etal-2025-conquer</bibkey>
    </paper>
    <paper id="10">
      <title>What is it? Towards a Generalizable Native <fixed-case>A</fixed-case>merican Language Identification System</title>
      <author><first>Ivory</first><last>Yang</last></author>
      <author><first>Weicheng</first><last>Ma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Carlos Guerrero</first><last>Alvarez</last><affiliation>Dartmouth College and Dartmouth College</affiliation></author>
      <author><first>William</first><last>Dinauer</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>105-111</pages>
      <abstract>This paper presents a research thesis proposal to develop a generalizable Native American language identification system. Despite their cultural and historical significance, Native American languages remain entirely unsupported by major commercial language identification systems. This omission not only underscores the systemic neglect of endangered languages in technological development, but also highlights the urgent need for dedicated, community-driven solutions. We propose a two-pronged approach: (1) systematically curating linguistic resources across all Native American languages for robust training, and (2) tailored data augmentation to generate synthetic yet linguistically coherent training samples. As proof of concept, we extend an existing rudimentary Athabaskan language classifier by integrating Plains Apache, an extinct Southern Athabaskan language, as an additional language class. We also adapt a data generation framework for low-resource languages to create synthetic Plains Apache data, highlighting the potential of data augmentation. This proposal advocates for a community-driven, technological approach to supporting Native American languages.</abstract>
      <url hash="bef42d16">2025.naacl-srw.10</url>
      <bibkey>yang-etal-2025-towards</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>M</fixed-case>ed-<fixed-case>C</fixed-case>o<fixed-case>DE</fixed-case>: Medical Critique based Disagreement Evaluation Framework</title>
      <author><first>Mohit</first><last>Gupta</last></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Rajiv Ratn</first><last>Shah</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>112-119</pages>
      <abstract>The emergence of large language models (LLMs) has significantly influenced numerous fields, including healthcare, by enhancing the capabilities of automated systems to process and generate human-like text. However, despite their advancements, the reliability and accuracy of LLMs in medical contexts remain critical concerns. Current evaluation methods often lack robustness and fail to provide a comprehensive assessment of LLM performance, leading to potential risks in clinical settings. In this work, we propose Med-CoDE, a specifically designed evaluation framework for medical LLMs to address these challenges. The framework leverages a critique-based approach to quantitatively measure the degree of disagreement between model-generated responses and established medical ground truths. This framework captures both accuracy and reliability in medical settings. The proposed evaluation framework aims to fill the existing gap in LLM assessment by offering a systematic method to evaluate the quality and trustworthiness of medical LLMs. Through extensive experiments and case studies, we illustrate the practicality of our framework in providing a comprehensive and reliable evaluation of medical LLMs.</abstract>
      <url hash="7f70d79d">2025.naacl-srw.11</url>
      <bibkey>gupta-etal-2025-med</bibkey>
    </paper>
    <paper id="12">
      <title>Sentimatic: Sentiment-guided Automatic Generation of Preference Datasets for Customer Support Dialogue System</title>
      <author><first>Suhyun</first><last>Lee</last></author>
      <author><first>ChangHeon</first><last>Han</last></author>
      <pages>120-128</pages>
      <abstract>Supervised Fine-tuning (SFT) and preference optimization (PO) are key methods for enhancing language models and aligning them with human preferences. However, scaling preference datasets for PO training is challenging, leading AI customer support systems to rely on SFT. To address this, we propose the Sentiment-guided Automatic Generation of Preference Datasets (Sentimatic) methodology to automatically generate customer preference datasets without human intervention using a publicly available dataset constructed for SFT. Our approach classifies responses by sentiment, fine-tunes models on them, and applies advanced sampling and evaluation techniques to ensure diversity and quality. Ultimately, we generated 1,174 customer preference datasets based on 357 test datasets, and through experiments, we confirmed that the AI customer support system trained on these datasets is capable of carefully considering customer emotions and generating professional and appropriate responses.</abstract>
      <url hash="ff3c421a">2025.naacl-srw.12</url>
      <bibkey>lee-han-2025-sentimatic</bibkey>
    </paper>
    <paper id="13">
      <title>Privacy-Preserving Federated Learning for Hate Speech Detection</title>
      <author><first>Ivo De Souza Bueno</first><last>Júnior</last></author>
      <author><first>Haotian</first><last>Ye</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Axel</first><last>Wisiorek</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>129-141</pages>
      <abstract>This paper presents a federated learning system with differential privacy for hate speech detection, tailored to low-resource languages. By fine-tuning pre-trained language models, ALBERT emerged as the most effective option for balancing performance and privacy. Experiments demonstrated that federated learning with differential privacy performs adequately in low-resource settings, though datasets with fewer than 20 sentences per client struggled due to excessive noise. Balanced datasets and augmenting hateful data with non-hateful examples proved critical for improving model utility. These findings offer a scalable and privacy-conscious framework for integrating hate speech detection into social media platforms and browsers, safeguarding user privacy while addressing online harm.</abstract>
      <url hash="c74cf96b">2025.naacl-srw.13</url>
      <bibkey>junior-etal-2025-privacy</bibkey>
    </paper>
    <paper id="14">
      <title>From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models</title>
      <author><first>Nikita</first><last>Neveditsin</last><affiliation>St. Mary’s University</affiliation></author>
      <author><first>Pawan</first><last>Lingras</last><affiliation>St. Mary’s University</affiliation></author>
      <author><first>Vijay Kumar</first><last>Mago</last><affiliation>York University</affiliation></author>
      <pages>142-161</pages>
      <abstract>This study examines the performance of Large Language Models (LLMs) in Aspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect extraction in a novel domain. Using a synthetic sports feedback dataset, we evaluate open-weight LLMs’ ability to extract aspect-polarity pairs and propose a metric to facilitate the evaluation of aspect extraction with generative models. Our findings highlight both the potential and limitations of LLMs in the ABSA task.</abstract>
      <url hash="df07d69a">2025.naacl-srw.14</url>
      <bibkey>neveditsin-etal-2025-annotation</bibkey>
    </paper>
    <paper id="15">
      <title>Developing <fixed-case>J</fixed-case>apanese <fixed-case>CLIP</fixed-case> Models Leveraging an Open-weight <fixed-case>LLM</fixed-case> for Large-scale Dataset Translation</title>
      <author><first>Issa</first><last>Sugiura</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Shuhei</first><last>Kurita</last><affiliation>National Institute of Informatics and New York University</affiliation></author>
      <author><first>Yusuke</first><last>Oda</last><affiliation>National Institute of Informatics and Nara Institute of Science and Technology</affiliation></author>
      <author><first>Daisuke</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <pages>162-170</pages>
      <abstract>CLIP is a foundational model that bridges images and text, widely adopted as a key component in numerous vision-language models.However, the lack of large-scale open Japanese image-text pairs poses a significant barrier to the development of Japanese vision-language models.In this study, we constructed a Japanese image-text pair dataset with 1.5 billion examples using machine translation with open-weight LLMs and pre-trained Japanese CLIP models on the dataset.The performance of the pre-trained models was evaluated across seven benchmark datasets, achieving competitive average scores compared to models of similar size without the need for extensive data curation. However, the results also revealed relatively low performance on tasks specific to Japanese culture, highlighting the limitations of translation-based approaches in capturing cultural nuances. Our dataset, models, and code are publicly available.</abstract>
      <url hash="edb0376f">2025.naacl-srw.15</url>
      <bibkey>sugiura-etal-2025-developing</bibkey>
    </paper>
    <paper id="16">
      <title>Self-Vocabularizing Training for Neural Machine Translation</title>
      <author><first>Pin-Jie</first><last>Lin</last></author>
      <author><first>Ernie</first><last>Chang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yangyang</first><last>Shi</last><affiliation>Meta</affiliation></author>
      <author><first>Vikas</first><last>Chandra</last><affiliation>Meta</affiliation></author>
      <pages>171-177</pages>
      <abstract>Past vocabulary learning techniques identify relevant vocabulary before training, relying on statistical and entropy-based assumptions that largely neglect the role of model training.Empirically, we observe that trained translation models are induced to use a byte-pair encoding (BPE) vocabulary subset distinct from the original BPE vocabulary, leading to performance improvements when retrained with the induced vocabulary.In this paper, we analyze this discrepancy in neural machine translation by examining vocabulary and entropy shifts during self-training—where each iteration generates a labeled dataset by pairing source sentences with the model’s predictions to define a new vocabulary.Building on these insights, we propose *self-vocabularizing training*, an iterative method that self-selects a smaller, more optimal vocabulary, yielding up to a 1.49 BLEU improvement.Moreover, we find that deeper model architectures lead to both an increase in unique token usage and a 6–8% reduction in vocabulary size.</abstract>
      <url hash="5f9e795f">2025.naacl-srw.16</url>
      <bibkey>lin-etal-2025-self</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>CCT</fixed-case>-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search</title>
      <author><first>Nikita</first><last>Sorokin</last><affiliation>MTS AI</affiliation></author>
      <author><first>Tikhonov</first><last>Anton</last><affiliation>ITMO University and MTS AI</affiliation></author>
      <author><first>Dmitry</first><last>Abulkhanov</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ivan</first><last>Sedykh</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Valentin</first><last>Malykh</last></author>
      <pages>178-185</pages>
      <abstract>We consider the well-known and important tasks of clone detection and information retrieval for source code. The most standard setup is to search clones inside the same language code snippets. But it is also useful to find code snippets with identical behaviour in different programming languages. Nevertheless multi- and cross-lingual clone detection has been little studied in literature. We present a novel training procedure, cross-consistency training (CCT) leveraging cross-lingual similarity, that we apply to train language models on source code in various programming languages. We show that this training is effective both for encoder- and decoder-based models.The trained encoder-based CCT-LM model%and fine-tuned with CCT,achieves a new state of the art on POJ-104 (monolingual C++ clone detection benchmark) with 96.73% MAP and AdvTest (monolingual Python code search benchmark) with 47.18% MRR. The decoder-based CCT-LM model shows comparable performance in these tasks. In addition, we formulate the multi- and cross-lingual clone detection problem and present XCD, a new benchmark dataset produced from CodeForces submissions.</abstract>
      <url hash="8704d994">2025.naacl-srw.17</url>
      <bibkey>sorokin-etal-2025-cct</bibkey>
    </paper>
    <paper id="18">
      <title>Text Compression for Efficient Language Generation</title>
      <author><first>David</first><last>Gu</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Peter</first><last>Belcak</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <pages>186-192</pages>
      <abstract>We challenge the prevailing assumption that LLMs must rely fully on sub-word tokens for high-quality text generation. To this end, we propose the “Generative Pretrained Thoughtformer” (GPTHF), a hierarchical transformer language model capable of text generation by compressing text into sentence embeddings and employing a sentence attention mechanism. GPTHF retains GPT’s architecture, modifying only token interactions via dynamic sparse attention masks. Our experiments show that GPTHF achieves an up to an order of magnitude improvement in FLOPs efficiency and a threefold increase in runtime speed compared to equally-sized GPT models in the low-size regime. This is achieved through a unique generation method that caches and reuses sentence embeddings, allowing significant portions of the input to bypass large parts of the network.</abstract>
      <url hash="2b2ee652">2025.naacl-srw.18</url>
      <bibkey>gu-etal-2025-text</bibkey>
    </paper>
    <paper id="19">
      <title>Multilingual Native Language Identification with Large Language Models</title>
      <author><first>Dhiman</first><last>Goswami</last><affiliation>George Mason University</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>193-199</pages>
      <abstract>Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of individuals based on their second language (L2) production. The introduction of Large Language Models (LLMs) with billions of parameters has renewed interest in text-based NLI, with new studies exploring LLM-based approaches to NLI on English L2. The capabilities of state-of-the-art LLMs on non-English NLI corpora, however, have not yet been fully evaluated. To fill this important gap, we present the first evaluation of LLMs for multilingual NLI. We evaluated the performance of several LLMs compared to traditional statistical machine learning models and language-specific BERT-based models on NLI corpora in English, Italian, Norwegian, and Portuguese. Our results show that fine-tuned GPT-4 models achieve state-of-the-art NLI performance.</abstract>
      <url hash="a33431ef">2025.naacl-srw.19</url>
      <bibkey>goswami-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="20">
      <title>Generating Synthetic Free-text Medical Records with Low Re-identification Risk using Masked Language Modeling</title>
      <author><first>Samuel</first><last>Belkadi</last></author>
      <author><first>Libo</first><last>Ren</last></author>
      <author><first>Nicolo</first><last>Micheletti</last></author>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>200-206</pages>
      <abstract>The abundance of medical records holds great promise for enhancing healthcare and advancing biomedical research. However, due to <i>privacy</i> constraints, access to such data is typically limited to internal use.Recent studies have attempted to overcome this challenge by generating synthetic data through Causal Language Modelling. Yet, this approach often fails to ensure patient anonymity and offers limited control over output diversity—unless additional computational cost is introduced.In response, we propose a method for generating synthetic free-text medical records based on <i>Masked Language Modelling</i>. Our approach retains key medical details while introducing variability in the generated texts and reducing the risk of patient re-identification. With a relatively lightweight architecture of approximately 120 million parameters, the system ensures low inference costs.Experimental results show that our method produces high-quality synthetic data, achieving a HIPAA-compliant PHI recall of 96% and a re-identification risk of only 3.5%. Furthermore, downstream evaluations reveal that models trained on the synthetic data perform comparably to those trained on real-world data. Our trained models are publicly available on Github as SynDeidMLM (at <url>https://github.com/SamySam0/SynDeidMLM</url>) (meaning <b>syn</b>thetic and <b>de-id</b>entified data generation using <b>MLM</b>).</abstract>
      <url hash="752dee61">2025.naacl-srw.20</url>
      <bibkey>belkadi-etal-2025-generating</bibkey>
    </paper>
    <paper id="21">
      <title>How many words does it take to understand a low-resource language?</title>
      <author><first>Emily</first><last>Chang</last></author>
      <author><first>Nada</first><last>Basit</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>207-224</pages>
      <abstract>When developing language technology, researchers have routinely turned to transfer learning to resolve the data scarcity conundrum presented in low-resource languages. As far as we know, this study is the first to evaluate the amount of documentation needed for transfer learning, specifically the smallest vocabulary size needed to create a sentence embedding space. In adopting widely spoken languages as a proxy for low-resource languages, our experiments show that the relationship between a sentence embedding’s vocabulary size and performance is logarithmic with performance leveling at a vocabulary size of 25,000. It should be noted that this relationship cannot be replicated across all languages and this level of documentation does not exist for many low-resource languages. We do observe, however, that performance accelerates at a vocabulary size of <tex-math>\le</tex-math> 1000, a quantity that is present in most low-resource language documentation. These results can aid researchers in understanding whether a low-resource language has enough documentation necessary to support the creation of a sentence embedding and language model.</abstract>
      <url hash="3c06270e">2025.naacl-srw.21</url>
      <bibkey>chang-basit-2025-many</bibkey>
    </paper>
    <paper id="22">
      <title>Linear Relational Decoding of Morphology in Language Models</title>
      <author><first>Eric</first><last>Xia</last></author>
      <author><first>Jugal</first><last>Kalita</last><affiliation>University of Colorado at Colorado Springs</affiliation></author>
      <pages>225-235</pages>
      <abstract>A two-part affine approximation has been found to be a good approximation for transformer computations over certain subject-object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation W s , where s is a middle-layer representation of a subject token and W is derived from model derivatives, can accurately reproduce final object states for many relations. This linear technique achieves 90% faithfulness on morphological relations, with similar findings across languages and models. Our results suggest that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space and are sparsely encoded by cross-layer linear transformations.</abstract>
      <url hash="b23a4d01">2025.naacl-srw.22</url>
      <bibkey>xia-kalita-2025-linear</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>SPY</fixed-case>: Enhancing Privacy with Synthetic <fixed-case>PII</fixed-case> Detection Dataset</title>
      <author><first>Maksim</first><last>Savkin</last><affiliation>Moscow Institute of Physics and Technology</affiliation></author>
      <author><first>Timur</first><last>Ionov</last></author>
      <author><first>Vasily</first><last>Konovalov</last><affiliation>AIRI</affiliation></author>
      <pages>236-246</pages>
      <abstract>We introduce **SPY Dataset**: a novel synthetic dataset for the task of **Personal Identifiable Information (PII) detection**, underscoring the significance of protecting PII in modern data processing. Our research innovates by leveraging Large Language Models (LLMs) to generate a dataset that emulates real-world PII scenarios. Through evaluation, we validate the dataset’s quality, providing a benchmark for PII detection. Comparative analyses reveal that while PII and Named Entity Recognition (NER) share similarities, **dedicated NER models exhibit limitations** when applied to PII-specific contexts. This work contributes to the field by making the generation methodology and the generated dataset publicly, thereby enabling further research and development in this field.</abstract>
      <url hash="a2d87896">2025.naacl-srw.23</url>
      <bibkey>savkin-etal-2025-spy</bibkey>
    </paper>
    <paper id="24">
      <title>Tighter Clusters, Safer Code? Improving Vulnerability Detection with Enhanced Contrastive Loss</title>
      <author><first>Pranav</first><last>Kapparad</last></author>
      <author><first>Biju R</first><last>Mohan</last></author>
      <pages>247-252</pages>
      <abstract>Distinguishing vulnerable code from non-vulnerable code is challenging due to high inter-class similarity. Supervised contrastive learning (SCL) improves embedding separation but struggles with intra-class clustering, especially when variations within the same class are subtle. We propose Cluster-Enhanced Supervised Contrastive Loss (CESCL), an extension of SCL with a distance-based regularization term that tightens intra-class clustering while maintaining inter-class separation. Evaluating on CodeBERT and GraphCodeBERT with Binary Cross Entropy (BCE), BCE + SCL, and BCE + CESCL, our method improves F1 score by 1.76% on CodeBERT and 4.1% on GraphCodeBERT, demonstrating its effectiveness in code vulnerability detection and broader applicability to high-similarity classification tasks.</abstract>
      <url hash="c820b62c">2025.naacl-srw.24</url>
      <bibkey>kapparad-mohan-2025-tighter</bibkey>
    </paper>
    <paper id="25">
      <title>Text Extraction and Script Completion in Images of <fixed-case>A</fixed-case>rabic Script-Based Calligraphy: A Thesis Proposal</title>
      <author><first>Dilara Zeynep</first><last>Gürer</last></author>
      <author><first>Ümit</first><last>Atlamaz</last><affiliation>Bogazici University</affiliation></author>
      <author><first>Şaziye Betül</first><last>Özateş</last><affiliation>Boğaziçi University</affiliation></author>
      <pages>253-259</pages>
      <abstract>Arabic calligraphy carries rich historical information and meaning. However, the complexity of its artistic elements and the absence of a consistent baseline make text extraction from such works highly challenging. In this paper, we provide an in-depth analysis of the unique obstacles in processing and interpreting these images, including the variability in calligraphic styles, the influence of artistic distortions, and the challenges posed by missing or damaged text elements. We explore potential solutions by leveraging state-of-the-art architectures and deep learning models, including visual language models, to improve text extraction and script completion.</abstract>
      <url hash="3a922881">2025.naacl-srw.25</url>
      <bibkey>gurer-etal-2025-text</bibkey>
    </paper>
    <paper id="26">
      <title>Subasa - Adapting Language Models for Low-resourced Offensive Language Detection in <fixed-case>S</fixed-case>inhala</title>
      <author><first>Shanilka</first><last>Haturusinghe</last></author>
      <author><first>Tharindu Cyril</first><last>Weerasooriya</last><affiliation>Accenture</affiliation></author>
      <author><first>Christopher M</first><last>Homan</last></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <author><first>Sidath Ravindra</first><last>Liyanage</last></author>
      <pages>260-270</pages>
      <abstract>Accurate detection of offensive language is essential for a number of applications related to social media safety. There is a sharp contrast in performance in this task between low and high-resource languages. In this paper, we adapt fine-tuning strategies that have not been previously explored for Sinhala in the downstream task of offensive language detection. Using this approach, we introduce four models: “Subasa-XLM-R”, which incorporates an intermediate Pre-Finetuning step using Masked Rationale Prediction. Two variants of “Subasa-Llama” and “Subasa-Mistral”, are fine-tuned versions of Llama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We evaluate our models on the SOLD benchmark dataset for Sinhala offensive language detection. All our models outperform existing baselines. Subasa-XLM-R achieves the highest Macro F1 score (0.84) surpassing state-of-the-art large language models like GPT-4o when evaluated on the same SOLD benchmark dataset under zero-shot settings. The models and code are publicly available.</abstract>
      <url hash="db5cbf9d">2025.naacl-srw.26</url>
      <bibkey>haturusinghe-etal-2025-subasa</bibkey>
    </paper>
    <paper id="27">
      <title>Integrating Symbolic Execution into the Fine-Tuning of Code-Generating <fixed-case>LLM</fixed-case>s</title>
      <author><first>Marina</first><last>Sakharova</last></author>
      <author><first>Abhinav</first><last>Anand</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Mira</first><last>Mezini</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <pages>271-278</pages>
      <abstract>Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark.</abstract>
      <url hash="bcbe4b3d">2025.naacl-srw.27</url>
      <bibkey>sakharova-etal-2025-integrating</bibkey>
    </paper>
    <paper id="28">
      <title>Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</title>
      <author><first>Elisei</first><last>Rykov</last><affiliation>Skolkovo Institute of Science and Technology and Tinkoff</affiliation></author>
      <author><first>Kseniia</first><last>Petrushina</last></author>
      <author><first>Kseniia</first><last>Titova</last></author>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Vasily</first><last>Konovalov</last><affiliation>AIRI</affiliation></author>
      <pages>279-293</pages>
      <abstract>Measuring how real images look is a complex task in artificial intelligence research. For example, an image of Albert Einstein holding a smartphone violates common-sense because modern smartphone were invented after Einstein’s death. We introduce a novel method, which we called Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLM to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.</abstract>
      <url hash="70ed2e5d">2025.naacl-srw.28</url>
      <bibkey>rykov-etal-2025-looking</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>C</fixed-case>olor<fixed-case>F</fixed-case>oil: Investigating Color Blindness in Large Vision and Language Models</title>
      <author><first>Ahnaf Mozib</first><last>Samin</last></author>
      <author><first>M Firoz</first><last>Ahmed</last></author>
      <author><first>Md. Mushtaq Shahriyar</first><last>Rafee</last><affiliation>Metropolitan University</affiliation></author>
      <pages>294-300</pages>
      <abstract>With the utilization of Transformer architecture, large Vision and Language (V&amp;L) models have shown promising performance in even zero-shot settings. Several studies, however, indicate a lack of robustness of the models when dealing with complex linguistics and visual attributes. In this work, we introduce a novel V&amp;L benchmark - ColorFoil, by creating color-related foils to assess the models’ perception ability to detect colors like red, white, green, etc. We evaluate seven state-of-the-art V&amp;L models including CLIP, ViLT, GroupViT, and BridgeTower, etc. in a zero-shot setting and present intriguing findings from the V&amp;L models. The experimental evaluation indicates that ViLT and BridgeTower demonstrate much better color perception capabilities compared to CLIP and its variants and GroupViT. Moreover, CLIP-based models and GroupViT struggle to distinguish colors that are visually distinct to humans with normal color perception ability.</abstract>
      <url hash="4b0517ce">2025.naacl-srw.29</url>
      <bibkey>samin-etal-2025-colorfoil</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Practical and Knowledgeable <fixed-case>LLM</fixed-case>s for a Multilingual World: A Thesis Proposal</title>
      <author><first>Bryan</first><last>Li</last></author>
      <pages>301-310</pages>
      <abstract>The frontier of large language model (LLM) development has largely been substantiated by knowledge-intensive tasks specified in English. In this proposed thesis, I argue for the key role that multilinguality occupies in the development of <i>practical</i> and <i>knowledgeable</i> LLMs.First, I consider practical methods to improve LLM’s performance on standard natural language processing (NLP) tasks by leveraging their existing multilingual knowledge.Then, I investigate the underlying multilingual knowledge of LLMs with two benchmarks: on complex reasoning, and on territorial disputes. These benchmarks reveal LLMs’ inconsistent performance across languages. I then design efficient techniques, both at inference-time and training-time, to address these discrepancies. Finally, I extend the territorial disputes benchmark to retrieval-augmented generation (RAG) setting, comparing the effects of different retrieval settings on cross-lingual robustness. My proposal shows that informed use of multilinguality enhances LLMs’ capabilities, and our understanding thereof.</abstract>
      <url hash="98e8691b">2025.naacl-srw.30</url>
      <bibkey>li-2025-towards</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>MDC</fixed-case>3: A Novel Multimodal Dataset for Commercial Content Classification in <fixed-case>B</fixed-case>engali</title>
      <author><first>Anik Mahmud</first><last>Shanto</last></author>
      <author><first>Mst. Sanjida Jamal</first><last>Priya</last></author>
      <author><first>Fahim Shakil</first><last>Tamim</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>311-320</pages>
      <abstract>Identifying commercial posts in resource-constrained languages among diverse and unstructured content remains a significant challenge for automatic text classification tasks. To address this, this work introduces a novel dataset named MDC<tex-math>^3</tex-math> (Multimodal Dataset for Commercial Content Classification), comprising 5,007 annotated Bengali social media posts classified as commercial and noncommercial. A comprehensive annotation guideline accompanying the dataset is included to aid future dataset creation in resource-constrained languages. Furthermore, we performed extensive experiments on MDC<tex-math>^3</tex-math> considering both unimodal and multimodal domains. Specifically, the late fusion of textual (mBERT) and visual (ViT) models (i.e., ViT+mBERT) achieves the highest F1 score of 90.91, significantly surpassing other baselines.</abstract>
      <url hash="8e00ff8f">2025.naacl-srw.31</url>
      <bibkey>shanto-etal-2025-mdc3</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>D</fixed-case>ate<fixed-case>L</fixed-case>ogic<fixed-case>QA</fixed-case>: Benchmarking Temporal Biases in Large Language Models</title>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>Ming Ze</first><last>Tang</last></author>
      <author><first>Cristina</first><last>Mahanta</last></author>
      <author><first>Madiha</first><last>Kazi</last></author>
      <pages>321-332</pages>
      <abstract>We introduce DateLogicQA, a human-curated benchmark of 190 questions specifically designed to understand temporal bias in Large Language Models (LLMs). Covering seven date formats across past, present, and future contexts, DateLogicQA examines four reasoning types: commonsense, factual, conceptual, and numerical. Through human-led evaluations of 12 state-of-the-art LLMs, we identify Representation-Level Bias, arising from suboptimal embeddings that distort date semantics, and Logical-Level Bias, manifesting when correct date tokens yield flawed temporal reasoning. Our findings underscore persistent challenges in handling various date formats and temporal contexts, revealing the need for more robust pretraining data, targeted post-training methods, and precise tokenization strategies. By illuminating these biases, we provide actionable insights to guide the development of LLMs for accurate temporal reasoning across diverse real-world applications.</abstract>
      <url hash="2c9b11e4">2025.naacl-srw.32</url>
      <bibkey>bhatia-etal-2025-datelogicqa</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>AMR</fixed-case>-<fixed-case>RE</fixed-case>: <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentations for Retrieval-Based In-Context Learning in Relation Extraction</title>
      <author><first>Peitao</first><last>Han</last></author>
      <author><first>Lis</first><last>Pereira</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Wan Jou</first><last>She</last><affiliation>Kyoto Institute of Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>333-342</pages>
      <abstract>Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which may result in overlooking entity relationships. We propose an AMR-enhanced retrieval-based ICL method for RE to address this issue. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. We conducted experiments in the supervised setting on four standard English RE datasets. The results show that our method achieves state-of-the-art performance on three datasets and competitive results on the fourth. Furthermore, our method outperforms baselines by a large margin across all datasets in the more demanding unsupervised setting.</abstract>
      <url hash="80a6d9c1">2025.naacl-srw.33</url>
      <bibkey>han-etal-2025-amr</bibkey>
    </paper>
    <paper id="34">
      <title>Linguistic Analysis of Veteran Job Interviews to Assess Effectiveness in Translating Military Expertise to the Civilian Workforce</title>
      <author><first>Caroline J.</first><last>Wendt</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Ehsanul Haque</first><last>Nirjhar</last></author>
      <author><first>Theodora</first><last>Chaspari</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>343-355</pages>
      <abstract>The ways in which natural language processing (NLP) can inform how veterans can improve effectiveness in translating military experience to workforce utility is underexplored. We design NLP experiments to evaluate the degree of explanation in veteran job interview responses as a proxy for perceived hireability. We examine linguistic and psycholinguistic features, context, and participant variability to investigate the mechanics of effective communication in employee selection. Results yield good performance when distinguishing between varying degrees of explanation in responses using LIWC features, indicating robustness of linguistic feature integration. Classifying Over- and Under-explained responses reflects challenges of class imbalance and the limitations of tested NLP methods for detecting subtleties in overly verbose or concise communication. Our findings have immediate applications for assistive technologies in job interview settings, and broader implications for enhancing automated communication assessment tools and refining strategies for training and interventions in communication-heavy fields.</abstract>
      <url hash="6dbd695b">2025.naacl-srw.34</url>
      <bibkey>wendt-etal-2025-linguistic</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>M</fixed-case>eta<fixed-case>M</fixed-case>eme: A Dataset for Meme Template and Meta-Category Classification</title>
      <author><first>Benjamin</first><last>Lambright</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Jordan</first><last>Youner</last></author>
      <author><first>Constantine</first><last>Lignos</last><affiliation>Brandeis University</affiliation></author>
      <pages>356-367</pages>
      <abstract>This paper introduces a new dataset for classifying memes by their template and communicative intent.It includes a broad selection of meme templates and examples scraped from imgflip and a smaller hand-annotated set of memes scraped from Reddit.The Reddit memes have been annotated for meta-category using a novel annotation scheme that classifies memes by the structure of the perspective they are being used to communicate.YOLOv11 and ChatGPT 4o are used to provide baseline modeling results.We find that YOLO struggles with template classification on real-world data but outperforms ChatGPT in classifying meta-categories.</abstract>
      <url hash="2106ae4a">2025.naacl-srw.35</url>
      <bibkey>lambright-etal-2025-metameme</bibkey>
    </paper>
    <paper id="36">
      <title>Representing and Clustering Errors in Offensive Language Detection</title>
      <author><first>Jood</first><last>Otey</last><affiliation>Oakland University</affiliation></author>
      <author><first>Laura</first><last>Biester</last><affiliation>Middlebury College</affiliation></author>
      <author><first>Steven R</first><last>Wilson</last><affiliation>University of Michigan - Flint</affiliation></author>
      <pages>368-380</pages>
      <abstract>Content moderation is essential in preventing the spread of harmful content on the Internet. However, there are instances where moderation fails and it is important to understand when and why that happens. Workflows that aim to uncover a system’s weakness typically use clustering of the data points’ embeddings to group errors together. In this paper, we evaluate the K-Means clustering of four text representations for the task of offensive language detection in English and Levantine Arabic. We find Sentence-BERT (SBERT) embeddings give the most human-interpretable clustering for English errors and the grouping is mainly based on the targeted group in the text. Meanwhile, SBERT embeddings of Large Language Model (LLM)-generated linguistic features give the most interpretable clustering for Arabic errors.</abstract>
      <url hash="7db91afa">2025.naacl-srw.36</url>
      <bibkey>otey-etal-2025-representing</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>ELIOT</fixed-case>: Zero-Shot Video-Text Retrieval through Relevance-Boosted Captioning and Structural Information Extraction</title>
      <author><first>Xuye</first><last>Liu</last></author>
      <author><first>Yimu</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jian</first><last>Zhao</last><affiliation>University of Waterloo</affiliation></author>
      <pages>381-391</pages>
      <abstract>Recent advances in video-text retrieval (VTR) have largely relied on supervised learning and fine-tuning. In this paper, we introduce , a novel zero-shot VTR framework that leverages off-the-shelf video captioners, large language models (LLMs), and text retrieval methods—entirely <b>without</b> additional training or annotated data. Due to the limited power of captioning methods, the captions often miss important content in the video, resulting in unsatisfactory retrieval performance. To translate more information into video captions, we first generates initial captions for videos, then enhances them using a relevance-boosted captioning strategy powered by LLMs, enriching video descriptions with salient details. To further emphasize key content, we propose structural information extraction, organizing visual elements such as objects, events, and attributes into structured templates, further boosting the retrieval performance. Benefiting from the enriched captions and structuralized information, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of over existing fine-tuned and pretraining methods without any data. They also show that the enriched captions capture key details from the video with minimal noise. Code and data will be released to facilitate future research.</abstract>
      <url hash="ee9a070d">2025.naacl-srw.37</url>
      <bibkey>liu-etal-2025-eliot</bibkey>
    </paper>
    <paper id="38">
      <title>Can Large Language Models Advance Crosswalks? The Case of <fixed-case>D</fixed-case>anish Occupation Codes</title>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Cynthia A.</first><last>Huang</last><affiliation>Monash University</affiliation></author>
      <author><first>Anna-Carolina</first><last>Haensch</last><affiliation>University of Maryland, College Park and Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>392-399</pages>
      <abstract>Crosswalks, which map one classification system to another, are critical tools for harmonizing data across time, countries, or frameworks. However, constructing crosswalks is labor-intensive and often requires domain expertise. This paper investigates the potential of Large Language Models (LLMs) to assist in creating crosswalks, focusing on two Danish occupational classification systems from different time periods as a case study. We propose a two-stage, prompt-based framework for this task, where LLMs perform similarity assessments between classification codes and identify final mappings through a guided decision process. Using four instruction-tuned LLMs and comparing them against an embedding-based baseline, we evaluate the performance of different models in crosswalks. Our results highlight the strengths of LLMs in crosswalk creation compared to the embedding-based baseline, showing the effectiveness of the interactive prompt-based framework for conducting crosswalks by LLMs. Furthermore, we analyze the impact of model combinations across two interactive rounds, highlighting the importance of model selection and consistency. This work contributes to the growing field of NLP applications for domain-specific knowledge mapping and demonstrates the potential of LLMs in advancing crosswalk methodologies.</abstract>
      <url hash="6f4b70a4">2025.naacl-srw.38</url>
      <bibkey>ma-etal-2025-large</bibkey>
    </paper>
    <paper id="39">
      <title>Paraphrase-based Contrastive Learning for Sentence Pair Modeling</title>
      <author><first>Seiji</first><last>Sugiyama</last><affiliation>Ehime University</affiliation></author>
      <author><first>Risa</first><last>Kondo</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <pages>400-407</pages>
      <abstract>To improve the performance of sentence pair modeling tasks, we propose an additional pre-training method, also known as transfer fine-tuning, for pre-trained masked language models.Pre-training for masked language modeling is not necessarily designed to bring semantically similar sentences closer together in the embedding space.Our proposed method aims to improve the performance of sentence pair modeling by applying contrastive learning to pre-trained masked language models, in which sentence embeddings of paraphrase pairs are made similar to each other.While natural language inference corpora, which are standard in previous studies on contrastive learning, are not available on a large-scale for non-English languages, our method can construct a training corpus for contrastive learning from a raw corpus and a paraphrase dictionary at a low cost.Experimental results on four sentence pair modeling tasks revealed the effectiveness of our method in both English and Japanese.</abstract>
      <url hash="e6ab45ad">2025.naacl-srw.39</url>
      <bibkey>sugiyama-etal-2025-paraphrase</bibkey>
    </paper>
    <paper id="40">
      <title>Do Video Language Models really understand the video contexts?</title>
      <author><first>Jeongwan</first><last>Shin</last><affiliation>Kyungpook National University</affiliation></author>
      <author><first>Jinhyeong</first><last>Lim</last><affiliation>HD Korea Shipbuilding &amp; Offshore Engineering</affiliation></author>
      <author><first>Hyeyoung</first><last>Park</last><affiliation>Kyungpook National University</affiliation></author>
      <pages>408-417</pages>
      <abstract>This paper examines how well visual language models (VLMs) understand video question answering (VideoQA) tasks and generate responses accordingly. Recently, VLMs based on Large Language Models (LLMs) have shown remarkable performance, but the processes of understanding and reasoning in VLMs remain under-explored. To tackle this challenge, we propose Video Understanding and Response Consistency Assessment, VURCA, a framework that incorporates a fine-grained question generation and answering process to measure how well the responses generated by VLMs align with what the model understands. In addition, we introduce an extended benchmark dataset, FgNExT-QA, which builds upon NExT-QA by incorporating more fine-grained VideoQA tasks. FgNExT-QA is designed to evaluate fine-grained understanding in video question answering. Through experiments, we found that despite the strong overall QA performance of VLMs, their understanding of both the video content and the question remains limited. In particular, they exhibit poor video comprehension in fine-grained VideoQA tasks.</abstract>
      <url hash="9d402c7c">2025.naacl-srw.40</url>
      <bibkey>shin-etal-2025-video</bibkey>
    </paper>
    <paper id="41">
      <title>Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?</title>
      <author><first>Sourabrata</first><last>Mukherjee</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>University of Galway, Ireland, Insight SFI Research Centre for Data Analytics, DSI, University of Galway, Ireland and Panlingua Languague Processing LLP, India</affiliation></author>
      <author><first>John Philip</first><last>McCrae</last><affiliation>National University of Ireland Galway</affiliation></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>418-434</pages>
      <abstract>Text style transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TSToutputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Us-ing human evaluation is ideal but costly, as is common in other natural language processing (NLP) tasks; however, automatic metrics forTST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set ofexisting and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks—sentiment transfer and detoxification—in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with hu-man judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigatethe potential of large language models (LLMs) as tools for TST evaluation. Our findings highlight newly applied advanced NLP metrics andLLM-based evaluations provide better insights than existing TST metrics. Our oracle ensemble approaches show even more potential.</abstract>
      <url hash="8294e2c2">2025.naacl-srw.41</url>
      <bibkey>mukherjee-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="42">
      <title>(<fixed-case>CPER</fixed-case>) From Guessing to Asking: An Approach to Resolving Persona Knowledge Gap in <fixed-case>LLM</fixed-case>s during Multi-Turn Conversations</title>
      <author><first>Sarvesh</first><last>Baskar</last></author>
      <author><first>Manas</first><last>Gaur</last><affiliation>University of Maryland Baltimore County</affiliation></author>
      <author><first>Srinivasan</first><last>Parthasarathy</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Tanmay Tulsidas</first><last>Verlekar</last><affiliation>BITS Pilani, Goa campus</affiliation></author>
      <pages>435-447</pages>
      <abstract>In multi-turn dialogues, large language models face a critical challenge of ensuring coherence while adapting to user-specific information.. This study introduces the persona knowledge gap, the discrepancy between a model’s internal understanding and the knowledge required for coherent, personalized conversations. While prior research has recognized these gaps, computational methods for their identification and resolution remain underexplored. We propose Conversation Preference Elicitation and Recommendation (CPER), a novel framework that dynamically detects and resolves persona knowledge gaps using intrinsic uncertainty quantification and feedback-driven refinement. CPER consists of three key modules: a Contextual Understanding Module for preference extraction, a Dynamic Feedback Module for measuring uncertainty and refining persona alignment, and a Persona-Driven Response Generation module for adapting responses based on accumulated user context. We evaluate CPER on two real-world datasets: CCPE-M for preferential movie recommendations and ESConv for mental health support. Using A/B testing, human evaluators preferred CPER’s responses 42% more often than baseline models in CCPE-M and 27% more often in ESConv. A qualitative human evaluation confirms that CPER’s responses are preferred for maintaining contextual relevance and coherence, particularly in longer (12+ turn) conversations.</abstract>
      <url hash="ce191a19">2025.naacl-srw.42</url>
      <bibkey>baskar-etal-2025-cper</bibkey>
    </paper>
    <paper id="43">
      <title>Streamlining <fixed-case>LLM</fixed-case>s: Adaptive Knowledge Distillation for Tailored Language Models</title>
      <author><first>Prajvi</first><last>Saxena</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Sabine</first><last>Janzen</last></author>
      <author><first>Wolfgang</first><last>Maass</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>448-455</pages>
      <abstract>Large language models (LLMs) like GPT-4 and LLaMA-3 offer transformative potential across industries, e.g., enhancing customer service, revolutionizing medical diagnostics, or identifying crises in news articles. However, deploying LLMs faces challenges such as limited training data, high computational costs, and issues with transparency and explainability. Our research focuses on distilling compact, parameter-efficient tailored language models (TLMs) from LLMs for domain-specific tasks with comparable performance. Current approaches like knowledge distillation, fine-tuning, and model parallelism address computational efficiency but lack hybrid strategies to balance efficiency, adaptability, and accuracy. We present ANON - an adaptive knowledge distillation framework integrating knowledge distillation with adapters to generate computationally efficient TLMs without relying on labeled datasets. ANON uses cross-entropy loss to transfer knowledge from the teacher’s outputs and internal representations while employing adaptive prompt engineering and a progressive distillation strategy for phased knowledge transfer. We evaluated ANON’s performance in the crisis domain, where accuracy is critical and labeled data is scarce. Experiments showed that ANON outperforms recent approaches of knowledge distillation, both in terms of the resulting TLM performance and in reducing the computational costs for training and maintaining accuracy compared to LLMs for domain-specific applications.</abstract>
      <url hash="c397bb83">2025.naacl-srw.43</url>
      <bibkey>saxena-etal-2025-streamlining</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>LLM</fixed-case> <fixed-case>DEBATE</fixed-case> <fixed-case>OPPONENT</fixed-case> : Counter-argument Generation focusing on Implicit and Critical Premises</title>
      <author><first>Taisei</first><last>Ozaki</last></author>
      <author><first>Chihiro</first><last>Nakagawa</last></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Shoichi</first><last>Naito</last><affiliation>Tohoku University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Kenshi</first><last>Yamaguchi</last><affiliation>Tohoku University, Tokyo Institute of Technology</affiliation></author>
      <pages>456-465</pages>
      <abstract>Debate education fosters critical thinking skills but often incurs high human costs. Recent advancements in Large Language Models (LLMs) show promise in automating counter-argument generation. However, it remains unclear how best to guide LLMs to target both implicit and critical premises. In this study, we systematically compare multi-step and one-step generation methods for counter-arguments across 100 debate topics. Our findings reveal that one-step approaches consistently outperform multi-step pipelines, owing to their better grasp of the “motion spirit,” minimized propagation of hallucinations, and avoidance of challenging intermediate tasks. Among premise-targeting methods, a one-step strategy that accounts for both implicit and explicit premises—Generated and Targeted Premise Attack (GTG)—emerges as the strongest performer in expert and automated evaluations. These results highlight the value of direct, integrated prompts for leveraging LLMs in complex argumentation tasks and offer insights for developing more effective automated debate agents.</abstract>
      <url hash="dcfa4aa5">2025.naacl-srw.44</url>
      <bibkey>ozaki-etal-2025-llm</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>A</fixed-case>uto<fixed-case>ML</fixed-case> Meets Hugging Face: Domain-Aware Pretrained Model Selection for Text Classification</title>
      <author><first>Parisa</first><last>Safikhani</last><affiliation>Otto-von-Guericke-Universität Magdeburg - Engineering, Otto von Guericke University Magdeburg and The German Centre for Higher Education Research and Science Studies (DZHW)</affiliation></author>
      <author><first>David</first><last>Broneske</last><affiliation>Deutsches Zentrum für Hochschul- und Wissenschaftsforschung</affiliation></author>
      <pages>466-473</pages>
      <abstract>The effectiveness of embedding methods is crucial for optimizing text classification performance in Automated Machine Learning (AutoML). However, selecting the most suitable pre-trained model for a given task remains challenging. This study introduces the Corpus-Driven Domain Mapping (CDDM) pipeline, which utilizes a domain-annotated corpus of pre-fine-tuned models from the Hugging Face Model Hub to improve model selection. Integrating these models into AutoML systems significantly boosts classification performance across multiple datasets compared to baseline methods. Despite some domain recognition inaccuracies, results demonstrate CDDM’s potential to enhance model selection, streamline AutoML workflows, and reduce computational costs.</abstract>
      <url hash="6096bdb9">2025.naacl-srw.45</url>
      <bibkey>safikhani-broneske-2025-automl</bibkey>
    </paper>
    <paper id="46">
      <title>Paraphrasing Attack Resilience of Various Machine-Generated Text Detection Methods</title>
      <author><first>Andrii</first><last>Shportko</last></author>
      <author><first>Inessa</first><last>Verbitsky</last></author>
      <pages>474-484</pages>
      <abstract>The recent large-scale emergence of LLMs has left an open space for dealing with their consequences, such as plagiarism or the spread of false information on the Internet. Coupling this with the rise of AI detector bypassing tools, reliable machine-generated text detection is in increasingly high demand. We investigate the paraphrasing attack resilience of various machine-generated text detection methods, evaluating three approaches: fine-tuned RoBERTa, Binoculars, and text feature analysis, along with their ensembles using Random Forest classifiers. We discovered that Binoculars-inclusive ensembles yield the strongest results, but they also suffer the most significant losses during attacks. In this paper, we present the dichotomy of performance versus resilience in the world of AI text detection, which complicates the current perception of reliability among state-of-the-art techniques.</abstract>
      <url hash="f831f0a1">2025.naacl-srw.46</url>
      <bibkey>shportko-verbitsky-2025-paraphrasing</bibkey>
    </paper>
    <paper id="47">
      <title>Detecting, Generating, and Evaluating in the Writing Style of Different Authors</title>
      <author><first>Mosab</first><last>Rezaei</last><affiliation>Northern Illinois University</affiliation></author>
      <pages>485-491</pages>
      <abstract>In recent years, stylometry has been investigated in many different fields. Hence, in this work, we are going to tackle this problem, detecting, generating, and evaluating textual documents according to the writing style by leveraging state-of-the-art models. In the first step, the sentences will be extracted from several different books, each belonging to a different author, to create a dataset. Then the selected models will be trained to detect the author of sentences in the dataset. After that, generator models are utilized to generate sentences based on the authors’ writing styles with unpaired samples in the dataset. Finally, to evaluate the performance of the generators, the previously trained models will be used to assess the generated sentences and to compare the distribution of various syntactic features between the original and generated sentences. We hope the result shows that models can be achieved to detect and generate textual documents for the given authors according to their writing style.</abstract>
      <url hash="e2323adf">2025.naacl-srw.47</url>
      <bibkey>rezaei-2025-detecting</bibkey>
    </paper>
    <paper id="48">
      <title>Collaborative Data Exploration through Visualization: A Thesis Proposal Analyzing Impact of Conversational Assistants</title>
      <author><first>Abari</first><last>Bhattacharya</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Barbara</first><last>Di Eugenio</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>492-500</pages>
      <abstract>Data visualization is integral to any Exploratory Data Analysis (EDA) task. However, generating visualization requires expertise, presenting a steep learning curve and a significant cognitive load. Natural language interfaces for EDA aim to lower this barrier by allowing users to generate visualizations through natural language queries. However, complexity remains when EDA is performed collaboratively, requiring an environment to support multi-user interaction. In this thesis proposal, we discuss challenges in user-system interaction in a collaborative multi-user setup, such as errors in visualization generation due to misinterpretation of user requests. We hypothesize that a Conversational Assistant (CA) capable of understanding user-initiated clarification requests and generating accurate responses can improve user experience and support collaborative EDA tasks. To this end, we propose to develop such a CA (Figure tab:system_issues) and evaluate it through a user study, thus examining its impact on user experience in a collaborative environment for EDA.</abstract>
      <url hash="b6bc49d4">2025.naacl-srw.48</url>
      <bibkey>bhattacharya-di-eugenio-2025-collaborative</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>MENDER</fixed-case>: Multi-hop Commonsense and Domain-specific <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Reasoning for Knowledge-grounded Empathetic Counseling of Crime Victims</title>
      <author><first>Abid</first><last>Hossain</last></author>
      <author><first>Priyanshu</first><last>Priya</last></author>
      <author><first>Armita Mani</first><last>Tripathi</last></author>
      <author><first>Pradeepika</first><last>Verma</last><affiliation>Indian Institute of Technology, Patna, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>501-516</pages>
      <abstract>Commonsense inference and domain-specific expertise are crucial for understanding and responding to emotional, cognitive, and topic-specific cues in counseling conversations with crime victims. However, these key evidences are often dispersed across multiple utterances, making it difficult to capture through single-hop reasoning. To address this, we propose MENDER, a novel Multi-hop commonsensE and domaiN-specific Chain-of-Thought (CoT) reasoning framework for knowleDge-grounded empathEtic Response generation in counseling dialogues. MENDER leverages large language models (LLMs) to integrate commonsense and domain knowledge via multi-hop reasoning over the dialogue context. It employs two specialized reasoning chains, viz. Commonsense Knowledge-driven CoT and Domain Knowledge-driven CoT rationales, which extract and aggregate dispersed emotional, cognitive, and topical evidences to generate knowledge-grounded empathetic counseling responses. Experimental evaluations on counseling dialogue dataset, POEM validate MENDER’s efficacy in generating coherent, empathetic, knowledge-grounded responses.</abstract>
      <url hash="b478bdd5">2025.naacl-srw.49</url>
      <bibkey>hossain-etal-2025-mender</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>S</fixed-case>kip<fixed-case>CLM</fixed-case>: Enhancing Crosslingual Alignment of Decoder Transformer Models via Contrastive Learning and Skip Connection</title>
      <author><first>Nikita</first><last>Sushko</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <pages>517-528</pages>
      <abstract>This paper proposes SkipCLM, a novel method for improving multilingual machine translation in Decoder Transformers. We augment contrastive learning for cross-lingual alignment with a trainable skip connection to preserve information crucial for accurate target language generation. Experiments with XGLM-564M on the Flores-101 benchmark demonstrate improved performance, particularly for en-de and en-zh direction translations, compared to direct sequence-to-sequence training and existing contrastive learning methods. Code is available at: https://github.com/s-nlp/skipclm.</abstract>
      <url hash="6fb8f34b">2025.naacl-srw.50</url>
      <bibkey>sushko-etal-2025-skipclm</bibkey>
    </paper>
    <paper id="51">
      <title>Towards <fixed-case>LLM</fixed-case>s Robustness to Changes in Prompt Format Styles</title>
      <author><first>Lilian</first><last>Ngweta</last></author>
      <author><first>Kiran</first><last>Kate</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Jason</first><last>Tsay</last><affiliation>IBM Research</affiliation></author>
      <author><first>Yara</first><last>Rizk</last><affiliation>International Business Machines</affiliation></author>
      <pages>529-537</pages>
      <abstract>Large language models (LLMs) have gained popularity in recent years for their utility in various applications. However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations. In the literature, this problem is commonly referred to as prompt brittleness. Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks. Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge. We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt few-shot examples. MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable. Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</abstract>
      <url hash="2018ac59">2025.naacl-srw.51</url>
      <bibkey>ngweta-etal-2025-towards</bibkey>
    </paper>
    <paper id="52">
      <title>Reliability of Distribution Predictions by <fixed-case>LLM</fixed-case>s: Insights from Counterintuitive Pseudo-Distributions</title>
      <author><first>Toma</first><last>Suzuki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Ayuki</first><last>Katayama</last></author>
      <author><first>Seiji</first><last>Gobara</last></author>
      <author><first>Ryo</first><last>Tsujimoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hibiki</first><last>Nakatani</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kazuki</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>538-550</pages>
      <abstract>The proportion of responses to a question and its options, known as the response distribution, enables detailed analysis of human society. Recent studies highlight the use of Large Language Models (LLMs) for predicting response distributions as a cost-effective survey method. However, the reliability of these predictions remains unclear. LLMs often generate answers by blindly following instructions rather than applying rational reasoning based on pretraining-acquired knowledge. This study investigates whether LLMs can rationally estimate distributions when presented with explanations of “artificially generated distributions” that are against commonsense. Specifically, we assess whether LLMs recognize counterintuitive explanations and adjust their predictions or simply follow these inconsistent explanations. Results indicate that smaller or less human-optimized LLMs tend to follow explanations uncritically, while larger or more optimized models are better at resisting counterintuitive explanations by leveraging their pretraining-acquired knowledge. These findings shed light on factors influencing distribution prediction performance in LLMs and are crucial for developing reliable distribution predictions using language models.</abstract>
      <url hash="bc14627d">2025.naacl-srw.52</url>
      <bibkey>suzuki-etal-2025-reliability</bibkey>
    </paper>
    <paper id="53">
      <title>Rosetta-<fixed-case>PL</fixed-case>: Propositional Logic as a Benchmark for Large Language Model Reasoning</title>
      <author><first>Shaun Lee</first><last>Baek</last></author>
      <author><first>Shaun</first><last>Esua-Mensah</last></author>
      <author><first>Cyrus</first><last>Tsui</last></author>
      <author><first>Sejan</first><last>Vigneswaralingam</last><affiliation>Trinity School</affiliation></author>
      <author><first>Abdullah</first><last>Alali</last></author>
      <author><first>Michael</first><last>Lu</last></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Facebook</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse AI Research</affiliation></author>
      <pages>551-562</pages>
      <abstract>Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs’ logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.</abstract>
      <url hash="5a085a0d">2025.naacl-srw.53</url>
      <bibkey>baek-etal-2025-rosetta</bibkey>
    </paper>
  </volume>
  <volume id="tutorial" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 5: Tutorial Abstracts)</booktitle>
      <editor><first>Maria</first><last>Lomeli</last></editor>
      <editor><first>Swabha</first><last>Swayamdipta</last></editor>
      <editor><first>Rui</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <venue>naacl</venue>
      <isbn>979-8-89176-193-3</isbn>
    </meta>
    <frontmatter>
      <url hash="e73ea6f8">2025.naacl-tutorial.0</url>
      <bibkey>naacl-2025-tutorial</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Creative Planning with Language Models: Practice, Evaluation and Applications</title>
      <author><first>Alexander</first><last>Spangher</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tenghao</first><last>Huang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1-9</pages>
      <abstract>The use of large language models (LLMs) in human-centered creative tasks — such as journalism, scientific writing, and storytelling — has showcased their potential for content generation but highlighted a critical gap: planning. Planning, used here to describe the “actions” humans perform before (and during) the writing process, is a fundamental process in many creative domains. This tutorial explores how planning has been learned and deployed in creative workflows, unifying three scenarios: Full Data Regimens (when observational data for actions and the resulting text exist), Partial (when text exists but actions can be inferred) and Low (when neither exist). The tutorial discusses forward and backward learning approaches for planning in LLMs, evaluation metrics tailored to latent plans, and practical applications in computational journalism, web agents, and other creative domains. By bridging theoretical concepts and practical demonstrations, this tutorial aims to inspire new research directions in leveraging LLMs for creative and goal-oriented planning tasks.</abstract>
      <url hash="7c4dac55">2025.naacl-tutorial.1</url>
      <bibkey>spangher-etal-2025-creative</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>DAMAG</fixed-case>e<fixed-case>R</fixed-case>: Deploying Automatic and Manual Approaches to <fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case> Red-teaming</title>
      <author><first>Manish</first><last>Nagireddy</last><affiliation>IBM Research</affiliation></author>
      <author><first>Michael</first><last>Feffer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ioana</first><last>Baldini</last><affiliation>Bloomberg</affiliation></author>
      <pages>10-14</pages>
      <abstract>In this tutorial, we will review and apply current automatic and manual red-teaming techniques for GenAI models(including LLMs and multimodal models). In doing so, we aim to emphasize the importance of using a mixture of techniques and establishing a balance between automatic and manual approaches. Lastly, we aim to engage tutorial participants in live red-teaming activities to collaboratively learn impactful red-teaming strategies and share insights.</abstract>
      <url hash="c8f9df02">2025.naacl-tutorial.2</url>
      <bibkey>nagireddy-etal-2025-damager</bibkey>
    </paper>
    <paper id="3">
      <title>Foundation Models Meet Embodied Agents</title>
      <author><first>Manling</first><last>Li</last><affiliation>Northwestern</affiliation></author>
      <author><first>Yunzhu</first><last>Li</last><affiliation>Columbia</affiliation></author>
      <author><first>Jiayuan</first><last>Mao</last><affiliation>MIT</affiliation></author>
      <author><first>Wenlong</first><last>Huang</last><affiliation>Stanford</affiliation></author>
      <pages>15-24</pages>
      <abstract>This tutorial will present a systematic overview of recent advances in foundation models for embodied agents, covering three types of foundation models based on input and output: Large Language Models (LLMs), Vision-Language Models (VLMs), Vision-Language-Action Models (VLAs)</abstract>
      <url hash="28009d99">2025.naacl-tutorial.3</url>
      <bibkey>li-etal-2025-foundation</bibkey>
    </paper>
    <paper id="4">
      <title>Knowledge Distillation for Language Models</title>
      <author><first>Yuqiao</first><last>Wen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Lili</first><last>Mou</last><affiliation>University of Alberta</affiliation></author>
      <pages>25-29</pages>
      <abstract>Knowledge distillation (KD) aims to transfer the knowledge of a teacher (usually a large model) to a student (usually a small one). In this tutorial, our goal is to provide participants with a comprehensive understanding of the techniques and applications of KD for language models. After introducing the basic concepts including intermediate-layer matching and prediction matching, we will present advanced techniques such as reinforcement learning-based KD and multi-teacher distillation. For applications, we will focus on KD for large language models (LLMs), covering topics ranging from LLM sequence compression to LLM self-distillation. The target audience is expected to know the basics of machine learning and NLP, but do not have to be familiar with the details of math derivation and neural models</abstract>
      <url hash="8be7c66a">2025.naacl-tutorial.4</url>
      <bibkey>wen-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="5">
      <title>Adaptation of Large Language Models</title>
      <author><first>Zixuan</first><last>Ke</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Yifei</first><last>Ming</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Salesforce AI Research</affiliation></author>
      <pages>30-37</pages>
      <abstract>This tutorial on adaptation of Large Language Models (LLMs) is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs</abstract>
      <url hash="8658c3b0">2025.naacl-tutorial.5</url>
      <bibkey>ke-etal-2025-adaptation</bibkey>
    </paper>
    <paper id="6">
      <title>Learning Language through Grounding</title>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Ziqiao</first><last>Ma</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jiayuan</first><last>Mao</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Joyce</first><last>Chai</last><affiliation>University of Michigan</affiliation></author>
      <pages>38-43</pages>
      <abstract>Grounding has been a long-standing concept in natural language processing (NLP) and computational linguistics (CL). This tutorial provides a historical overview and introduces recent advances in learning language through grounding, with a particular emphasis on the latter. We will begin by tracing the history of grounding and presenting a unified perspective on the term. In Parts II to IV, we will delve into recent progress in learning lexical semantics, syntax, and complex meanings through various forms of grounding. We will conclude by discussing future directions and open challenges, particularly those related to the growing trend of large language models and scaling.</abstract>
      <url hash="6abbf88e">2025.naacl-tutorial.6</url>
      <bibkey>shi-etal-2025-learning</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>LLM</fixed-case>s and Copyright Risks: Benchmarks and Mitigation Approaches</title>
      <author><first>Denghui</first><last>Zhang</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Zhaozhuo</first><last>Xu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Weijie</first><last>Zhao</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <pages>44-50</pages>
      <abstract>Large Language Models (LLMs) have revolutionized natural language processing, but their widespread use has raised significant copyright concerns. This tutorial addresses the complex intersection of LLMs and copyright law, providing researchers and practitioners with essential knowledge and tools to navigate this challenging landscape. The tutorial begins with an overview of relevant copyright principles and their application to AI, followed by an examination of specific copyright issues in LLM development and deployment. A key focus will be on technical approaches to copyright risk assessment and mitigation in LLMs. We will introduce benchmarks for evaluating copyright-related risks, including memorization detection and probing techniques. The tutorial will then cover practical mitigation strategies, such as machine unlearning, efficient fine-tuning methods, and alignment approaches to reduce copyright infringement risks. Ethical considerations and future directions in copyright-aware AI development will also be discussed.</abstract>
      <url hash="ba535436">2025.naacl-tutorial.7</url>
      <bibkey>zhang-etal-2025-llms</bibkey>
    </paper>
    <paper id="8">
      <title>Social Intelligence in the Age of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hao</first><last>Zhu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>51-55</pages>
      <abstract>With the emergence of Large Language Models (LLMs), we now have unprecedented opportunities to incorporate human-like communication and context-aware interactions into artificial systems. But what is the current state of LLMs’ capability of social interaction? Can they truly understand social scenarios, perform social reasoning, or interact with humans as socially competent agents? We propose this tutorial as an introduction to and an overview of different aspects of artificial social intelligence and their relationship with LLMs. In this tutorial, we will explore these questions by introducing scientific methods for evaluating social intelligence in LLMs, highlighting the key challenges, and identifying promising research directions. Participants will not only gain a comprehensive overview of the field’s progress, but also acquire technical skills on analysing and developing LLM-based social intelligence.</abstract>
      <url hash="cf13eb9c">2025.naacl-tutorial.8</url>
      <bibkey>zhu-etal-2025-social</bibkey>
    </paper>
  </volume>
  <event id="naacl-2025">
    <colocated>
      <volume-id>2025.findings-naacl</volume-id>
      <volume-id>2025.aisd-main</volume-id>
      <volume-id>2025.alp-1</volume-id>
      <volume-id>2025.americasnlp-1</volume-id>
      <volume-id>2025.c3nlp-1</volume-id>
      <volume-id>2025.calcs-1</volume-id>
      <volume-id>2025.cl4health-1</volume-id>
      <volume-id>2025.clpsych-1</volume-id>
      <volume-id>2025.cmcl-1</volume-id>
      <volume-id>2025.dravidianlangtech-1</volume-id>
      <volume-id>2025.insights-1</volume-id>
      <volume-id>2025.knowledgenlp-1</volume-id>
      <volume-id>2025.lm4uc-1</volume-id>
      <volume-id>2025.latechclfl-1</volume-id>
      <volume-id>2025.loresmt-1</volume-id>
      <volume-id>2025.mwe-1</volume-id>
      <volume-id>2025.nlp4dh-1</volume-id>
      <volume-id>2025.privatenlp-main</volume-id>
      <volume-id>2025.queerinai-main</volume-id>
      <volume-id>2025.repl4nlp-1</volume-id>
      <volume-id>2025.sigmorphon-main</volume-id>
      <volume-id>2025.trustnlp-main</volume-id>
      <volume-id>2025.wnut-1</volume-id>
      <volume-id>2025.wnu-1</volume-id>
      <volume-id>2025.in2writing-1</volume-id>
    </colocated>
  </event>
</collection>
