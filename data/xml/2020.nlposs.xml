<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.nlposs">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
      <editor><first>Eunjeong L.</first><last>Park</last></editor>
      <editor><first>Masato</first><last>Hagiwara</last></editor>
      <editor><first>Dmitrijs</first><last>Milajevs</last></editor>
      <editor><first>Nelson F.</first><last>Liu</last></editor>
      <editor><first>Geeticka</first><last>Chauhan</last></editor>
      <editor><first>Liling</first><last>Tan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
      <venue>nlposs</venue>
    </meta>
    <frontmatter>
      <url hash="ba8bcc58">2020.nlposs-1.0</url>
      <bibkey>nlposs-2020-nlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Framework to Assist Chat Operators of Mental Healthcare Services</title>
      <author><first>Thiago</first><last>Madeira</last></author>
      <author><first>Heder</first><last>Bernardino</last></author>
      <author><first>Jairo</first><last>Francisco De Souza</last></author>
      <author><first>Henrique</first><last>Gomide</last></author>
      <author><first>Nathália</first><last>Munck Machado</last></author>
      <author><first>Bruno</first><last>Marcos Pinheiro da Silva</last></author>
      <author><first>Alexandre</first><last>Vieira Pereira Pacelli</last></author>
      <pages>1–7</pages>
      <abstract>Conversational agents can be used to make diagnoses, classify mental states, promote health education, and provide emotional support. The benefits of adopting conversational agents include widespread access, increased treatment engagement, and improved patient relationships with the intervention. We propose here a framework to assist chat operators of mental healthcare services, instead of a fully automated conversational agent. This design eases to avoid the adverse effects of applying chatbots in mental healthcare. The proposed framework is capable of improving the quality and reducing the time of interactions via chat between a user and a chat operator. We also present a case study in the context of health promotion on reducing tobacco use. The proposed framework uses artificial intelligence, specifically natural language processing (NLP) techniques, to classify messages from chat users. A list of suggestions is offered to the chat operator, with topics to be discussed in the session. These suggestions were created based on service protocols and the classification of previous chat sessions. The operator can also edit the suggested messages. Data collected can be used in the future to improve the quality of the suggestions offered.</abstract>
      <url hash="ef5fc687">2020.nlposs-1.1</url>
      <doi>10.18653/v1/2020.nlposs-1.1</doi>
      <video href="https://slideslive.com/38939738"/>
      <bibkey>madeira-etal-2020-framework</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>ARBML</fixed-case>: Democritizing <fixed-case>A</fixed-case>rabic Natural Language Processing Tools</title>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Maged</first><last>Al-Shaibani</last></author>
      <pages>8–13</pages>
      <abstract>Automating natural language understanding is a lifelong quest addressed for decades. With the help of advances in machine learning and particularly, deep learning, we are able to produce state of the art models that can imitate human interactions with languages. Unfortunately, these advances are controlled by the availability of language resources. Arabic advances in this field , although it has a great potential, are still limited. This is apparent in both research and development. In this paper, we showcase some NLP models we trained for Arabic. We also present our methodology and pipeline to build such models from data collection, data preprocessing, tokenization and model deployment. These tools help in the advancement of the field and provide a systematic approach for extending NLP tools to many languages.</abstract>
      <url hash="34330d7e">2020.nlposs-1.2</url>
      <doi>10.18653/v1/2020.nlposs-1.2</doi>
      <video href="https://slideslive.com/38939739"/>
      <bibkey>alyafeai-al-shaibani-2020-arbml</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>CLEVR</fixed-case> Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes</title>
      <author><first>Raeid</first><last>Saqur</last></author>
      <author><first>Ameet</first><last>Deshpande</last></author>
      <pages>14–19</pages>
      <abstract>The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning (ML) and Natural Language Processing (NLP). We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components – parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network (GNN) libraries. Additionally, we discuss downstream usage and applications of the library, and how it can accelerate research for the NLP community.</abstract>
      <url hash="7f977eb7">2020.nlposs-1.3</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7fef6daa">2020.nlposs-1.3.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlposs-1.3</doi>
      <video href="https://slideslive.com/38939740"/>
      <bibkey>saqur-deshpande-2020-clevr</bibkey>
      <pwccode url="https://github.com/raeidsaqur/clevr-parser" additional="false">raeidsaqur/clevr-parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
    </paper>
    <paper id="4">
      <title>End-to-end <fixed-case>NLP</fixed-case> Pipelines in Rust</title>
      <author><first>Guillaume</first><last>Becquin</last></author>
      <pages>20–25</pages>
      <abstract>The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage state-of-the-art models have been instrumental in the democratization of this technology. The maturity of the open-source NLP ecosystem however varies between languages. This work proposes a new open-source library aimed at bringing state-of-the-art NLP to Rust. Rust is a systems programming language for which the foundations required to build machine learning applications are available but still lacks ready-to-use, end-to-end NLP libraries. The proposed library, rust-bert, implements modern language models and ready-to-use pipelines (for example translation or summarization). This allows further development by the Rust community from both NLP experts and non-specialists. It is hoped that this library will accelerate the development of the NLP ecosystem in Rust. The library is under active development and available at https://github.com/guillaume-be/rust-bert.</abstract>
      <url hash="ab1aeb36">2020.nlposs-1.4</url>
      <doi>10.18653/v1/2020.nlposs-1.4</doi>
      <video href="https://slideslive.com/38939741"/>
      <bibkey>becquin-2020-end</bibkey>
      <pwccode url="https://github.com/guillaume-be/rust-bert" additional="false">guillaume-be/rust-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="5">
      <title>Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings</title>
      <author><first>Vaibhav</first><last>Kumar</last></author>
      <author><first>Tenzin</first><last>Bhotia</last></author>
      <author><first>Vaibhav</first><last>Kumar</last></author>
      <pages>26–31</pages>
      <abstract>Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.</abstract>
      <url hash="807067c7">2020.nlposs-1.5</url>
      <doi>10.18653/v1/2020.nlposs-1.5</doi>
      <video href="https://slideslive.com/38939742"/>
      <bibkey>kumar-etal-2020-fair</bibkey>
      <pwccode url="https://github.com/FEE-Fair-Embedding-Engine/FEE" additional="false">FEE-Fair-Embedding-Engine/FEE</pwccode>
    </paper>
    <paper id="6">
      <title>Flexible retrieval with <fixed-case>NMSLIB</fixed-case> and <fixed-case>F</fixed-case>lex<fixed-case>N</fixed-case>eu<fixed-case>ART</fixed-case></title>
      <author><first>Leonid</first><last>Boytsov</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <pages>32–43</pages>
      <abstract>Our objective is to introduce to the NLP community NMSLIB, describe a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations (with weights learned from training data), which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations (e.g., Lucene), purely dense representations (e.g., FAISS and Annoy), or only perform mixing at the re-ranking stage.</abstract>
      <url hash="2656705d">2020.nlposs-1.6</url>
      <doi>10.18653/v1/2020.nlposs-1.6</doi>
      <video href="https://slideslive.com/38939743"/>
      <bibkey>boytsov-nyberg-2020-flexible</bibkey>
      <pwccode url="https://github.com/oaqa/FlexNeuART" additional="true">oaqa/FlexNeuART</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="7">
      <title>fugashi, a Tool for Tokenizing <fixed-case>J</fixed-case>apanese in Python</title>
      <author><first>Paul</first><last>McCann</last></author>
      <pages>44–51</pages>
      <abstract>Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.</abstract>
      <url hash="7d9e9651">2020.nlposs-1.7</url>
      <doi>10.18653/v1/2020.nlposs-1.7</doi>
      <video href="https://slideslive.com/38939744"/>
      <bibkey>mccann-2020-fugashi</bibkey>
      <pwccode url="https://github.com/polm/fugashi" additional="false">polm/fugashi</pwccode>
    </paper>
    <paper id="8">
      <title>Going Beyond <fixed-case>T</fixed-case>-<fixed-case>SNE</fixed-case>: Exposing whatlies in Text Embeddings</title>
      <author><first>Vincent</first><last>Warmerdam</last></author>
      <author><first>Thomas</first><last>Kober</last></author>
      <author><first>Rachael</first><last>Tatman</last></author>
      <pages>52–60</pages>
      <abstract>We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.</abstract>
      <url hash="68daa59c">2020.nlposs-1.8</url>
      <doi>10.18653/v1/2020.nlposs-1.8</doi>
      <video href="https://slideslive.com/38939745"/>
      <bibkey>warmerdam-etal-2020-going</bibkey>
    </paper>
    <paper id="9">
      <title>Howl: A Deployed, Open-Source Wake Word Detection System</title>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>Jaejun</first><last>Lee</last></author>
      <author><first>Afsaneh</first><last>Razi</last></author>
      <author><first>Julia</first><last>Cambre</last></author>
      <author><first>Ian</first><last>Bicking</last></author>
      <author><first>Jofish</first><last>Kaye</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>61–65</pages>
      <abstract>We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.</abstract>
      <url hash="f6952bec">2020.nlposs-1.9</url>
      <doi>10.18653/v1/2020.nlposs-1.9</doi>
      <video href="https://slideslive.com/38939746"/>
      <bibkey>tang-etal-2020-howl</bibkey>
      <pwccode url="https://github.com/castorini/howl" additional="true">castorini/howl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/musan">MUSAN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/speech-commands">Speech Commands</pwcdataset>
    </paper>
    <paper id="10">
      <title>i<fixed-case>NLTK</fixed-case>: Natural Language Toolkit for Indic Languages</title>
      <author><first>Gaurav</first><last>Arora</last></author>
      <pages>66–71</pages>
      <abstract>We present iNLTK, an open-source NLP library consisting of pre-trained language models and out-of-the-box support for Data Augmentation, Textual Similarity, Sentence Embeddings, Word Embeddings, Tokenization and Text Generation in 13 Indic Languages. By using pre-trained models from iNLTK for text classification on publicly available datasets, we significantly outperform previously reported results. On these datasets, we also show that by using pre-trained models and data augmentation from iNLTK, we can achieve more than 95% of the previous best performance by using less than 10% of the training data. iNLTK is already being widely used by the community and has 40,000+ downloads, 600+ stars and 100+ forks on GitHub. The library is available at https://github.com/goru001/inltk.</abstract>
      <url hash="7a9c6c43">2020.nlposs-1.10</url>
      <doi>10.18653/v1/2020.nlposs-1.10</doi>
      <video href="https://slideslive.com/38939747"/>
      <bibkey>arora-2020-inltk</bibkey>
      <pwccode url="https://github.com/goru001/inltk" additional="false">goru001/inltk</pwccode>
    </paper>
    <paper id="11">
      <title><fixed-case>KLPT</fixed-case> – <fixed-case>K</fixed-case>urdish Language Processing Toolkit</title>
      <author><first>Sina</first><last>Ahmadi</last></author>
      <pages>72–84</pages>
      <abstract>Despite the recent advances in applying language-independent approaches to various natural language processing tasks thanks to artificial intelligence, some language-specific tools are still essential to process a language in a viable manner. Kurdish language is a less-resourced language with a remarkable diversity in dialects and scripts and lacks basic language processing tools. To address this issue, we introduce a language processing toolkit to handle such a diversity in an efficient way. Our toolkit is composed of fundamental components such as text preprocessing, stemming, tokenization, lemmatization and transliteration and is able to get further extended by future developers. The project is publicly available.</abstract>
      <url hash="b9a0c603">2020.nlposs-1.11</url>
      <doi>10.18653/v1/2020.nlposs-1.11</doi>
      <video href="https://slideslive.com/38939750"/>
      <bibkey>ahmadi-2020-klpt</bibkey>
      <pwccode url="https://github.com/sinaahmadi/klpt" additional="false">sinaahmadi/klpt</pwccode>
    </paper>
    <paper id="12">
      <title>Open <fixed-case>K</fixed-case>orean Corpora: A Practical Report</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Sangwhan</first><last>Moon</last></author>
      <author><first>Youngsook</first><last>Song</last></author>
      <pages>85–93</pages>
      <abstract>Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Korean corpora, first describing institution-level resource development, then further iterate through a list of current open datasets for different types of tasks. We then propose a direction on how open-source dataset construction and releases should be done for less-resourced languages to promote research.</abstract>
      <url hash="6aeea4aa">2020.nlposs-1.12</url>
      <doi>10.18653/v1/2020.nlposs-1.12</doi>
      <video href="https://slideslive.com/38939751"/>
      <bibkey>cho-etal-2020-open</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/clovacall">ClovaCall</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="13">
      <title>Open-Source Morphology for Endangered Mordvinic Languages</title>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <pages>94–100</pages>
      <abstract>This document describes shared development of finite-state description of two closely related but endangered minority languages, Erzya and Moksha. It touches upon morpholexical unity and diversity of the two languages and how this provides a motivation for shared open-source FST development. We describe how we have designed the transducers so that they can benefit from existing open-source infrastructures and are as reusable as possible.</abstract>
      <url hash="d4175345">2020.nlposs-1.13</url>
      <doi>10.18653/v1/2020.nlposs-1.13</doi>
      <video href="https://slideslive.com/38939752"/>
      <bibkey>rueter-etal-2020-open</bibkey>
      <pwccode url="https://github.com/giellalt/lang-mdf" additional="true">giellalt/lang-mdf</pwccode>
    </paper>
    <paper id="14">
      <title>Pimlico: A toolkit for corpus-processing pipelines and reproducible experiments</title>
      <author><first>Mark</first><last>Granroth-Wilding</last></author>
      <pages>101–109</pages>
      <abstract>We present Pimlico, an open source toolkit for building pipelines for processing large corpora. It is especially focused on processing linguistic corpora and provides wrappers around existing, widely used NLP tools. A particular goal is to ease distribution of reproducible and extensible experiments by making it easy to document and re-run all steps involved, including data loading, pre-processing, model training and evaluation. Once a pipeline is released, it is easy to adapt, for example, to run on a new dataset, or to re-run an experiment with different parameters. The toolkit takes care of many common challenges in writing and distributing corpus-processing code, such as managing data between the steps of a pipeline, installing required software and combining existing toolkits with new, task-specific code.</abstract>
      <url hash="9784fdf3">2020.nlposs-1.14</url>
      <doi>10.18653/v1/2020.nlposs-1.14</doi>
      <video href="https://slideslive.com/38939753"/>
      <bibkey>granroth-wilding-2020-pimlico</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>P</fixed-case>y<fixed-case>SBD</fixed-case>: Pragmatic Sentence Boundary Disambiguation</title>
      <author><first>Nipun</first><last>Sadvilkar</last></author>
      <author><first>Mark</first><last>Neumann</last></author>
      <pages>110–114</pages>
      <abstract>We present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to Python with additional improvements and functionality. PySBD passes 97.92% of the Golden Rule Set examplars for English, an improvement of 25% over the next best open source Python tool.</abstract>
      <url hash="a8f7aca7">2020.nlposs-1.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="629e1b58">2020.nlposs-1.15.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2020.nlposs-1.15</doi>
      <video href="https://slideslive.com/38939754"/>
      <bibkey>sadvilkar-neumann-2020-pysbd</bibkey>
      <pwccode url="https://github.com/nipunsadvilkar/pySBD" additional="false">nipunsadvilkar/pySBD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/genia">GENIA</pwcdataset>
    </paper>
    <paper id="16">
      <title>iobes: Library for Span Level Processing</title>
      <author><first>Brian</first><last>Lester</last></author>
      <pages>115–119</pages>
      <abstract>Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each token is given a label and these labels are prefixed with special tokens such as B- or I-. After a model assigns labels to each token, these prefixes are used to group the tokens into spans. Properly parsing these annotations is critical for producing fair and comparable metrics; however, despite its importance, there is not an easy-to-use, standardized, programmatically integratable library to help work with span labeling. To remedy this, we introduce our open-source library, iobes. iobes is used for parsing, converting, and processing spans represented as token level decisions.</abstract>
      <url hash="52b5feeb">2020.nlposs-1.16</url>
      <doi>10.18653/v1/2020.nlposs-1.16</doi>
      <video href="https://slideslive.com/38939748"/>
      <bibkey>lester-2020-iobes</bibkey>
      <pwccode url="https://github.com/blester125/iobes" additional="false">blester125/iobes</pwccode>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>acre<fixed-case>ROUGE</fixed-case>: An Open-Source Library for Using and Developing Summarization Evaluation Metrics</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>120–125</pages>
      <abstract>We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.</abstract>
      <url hash="2c4f247d">2020.nlposs-1.17</url>
      <attachment type="OptionalSupplementaryMaterial" hash="16198289">2020.nlposs-1.17.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2020.nlposs-1.17</doi>
      <video href="https://slideslive.com/38939755"/>
      <bibkey>deutsch-roth-2020-sacrerouge</bibkey>
      <pwccode url="https://github.com/danieldeutsch/sacrerouge" additional="false">danieldeutsch/sacrerouge</pwccode>
    </paper>
    <paper id="18">
      <title><fixed-case>T</fixed-case>ext<fixed-case>A</fixed-case>ttack: Lessons learned in designing Python frameworks for <fixed-case>NLP</fixed-case></title>
      <author><first>John</first><last>Morris</last></author>
      <author><first>Jin Yong</first><last>Yoo</last></author>
      <author><first>Yanjun</first><last>Qi</last></author>
      <pages>126–131</pages>
      <abstract>TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.</abstract>
      <url hash="cbfe79ce">2020.nlposs-1.18</url>
      <doi>10.18653/v1/2020.nlposs-1.18</doi>
      <video href="https://slideslive.com/38939756"/>
      <bibkey>morris-etal-2020-textattack-lessons</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>TOMODAPI</fixed-case>: A Topic Modeling <fixed-case>API</fixed-case> to Train, Use and Compare Topic Models</title>
      <author><first>Pasquale</first><last>Lisena</last></author>
      <author><first>Ismail</first><last>Harrando</last></author>
      <author><first>Oussama</first><last>Kandakji</last></author>
      <author><first>Raphael</first><last>Troncy</last></author>
      <pages>132–140</pages>
      <abstract>From LDA to neural models, different topic modeling approaches have been proposed in the literature. However, their suitability and performance is not easy to compare, particularly when the algorithms are being used in the wild on heterogeneous datasets. In this paper, we introduce ToModAPI (TOpic MOdeling API), a wrapper library to easily train, evaluate and infer using different topic modeling algorithms through a unified interface. The library is extensible and can be used in Python environments or through a Web API.</abstract>
      <url hash="c15ed09f">2020.nlposs-1.19</url>
      <doi>10.18653/v1/2020.nlposs-1.19</doi>
      <video href="https://slideslive.com/38939757"/>
      <bibkey>lisena-etal-2020-tomodapi</bibkey>
      <pwccode url="https://github.com/d2klab/tomodapi" additional="false">d2klab/tomodapi</pwccode>
    </paper>
    <paper id="20">
      <title>User-centered &amp; Robust <fixed-case>NLP</fixed-case> <fixed-case>OSS</fixed-case>: Lessons Learned from Developing &amp; Maintaining <fixed-case>RSMT</fixed-case>ool</title>
      <author><first>Nitin</first><last>Madnani</last></author>
      <author><first>Anastassia</first><last>Loukina</last></author>
      <pages>141–146</pages>
      <abstract>For the last 5 years, we have developed and maintained RSMTool – an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its cross-disciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper.</abstract>
      <url hash="d6f266ab">2020.nlposs-1.20</url>
      <doi>10.18653/v1/2020.nlposs-1.20</doi>
      <video href="https://slideslive.com/38939758"/>
      <bibkey>madnani-loukina-2020-user</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>WAFFLE</fixed-case>: A Graph for <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Applied to <fixed-case>F</fixed-case>ree<fixed-case>F</fixed-case>orm Linguistic Exploration</title>
      <author><first>Berk</first><last>Ekmekci</last></author>
      <author><first>Blake</first><last>Howald</last></author>
      <pages>147–157</pages>
      <abstract>The WordNet database of English (Fellbaum, 1998) is a key source of semantic information for research and development of natural language processing applications. As the sophistication of these applications increases with the use of large datasets, deep learning, and graph-based methods, so should the use of WordNet. To this end, we introduce WAFFLE: WordNet Applied to FreeForm Linguistic Exploration which makes WordNet available in an open source graph data structure. The WAFFLE graph relies on platform agnostic formats for robust interrogation and flexibility. Where existing implementations of WordNet offer dictionary-like lookup, single degree neighborhood operations, and path based similarity-scoring, the WAFFLE graph makes all nodes (semantic relation sets) and relationships queryable at scale, enabling local and global analysis of all relationships without the need for custom code. We demonstrate WAFFLE’s ease of use, visualization capabilities, and scalable efficiency with common queries, operations, and interactions. WAFFLE is available at github.com/TRSS-NLP/WAFFLE.</abstract>
      <url hash="b573660b">2020.nlposs-1.21</url>
      <doi>10.18653/v1/2020.nlposs-1.21</doi>
      <video href="https://slideslive.com/38939759"/>
      <bibkey>ekmekci-howald-2020-waffle</bibkey>
      <pwccode url="https://github.com/trss-nlp/waffle" additional="false">trss-nlp/waffle</pwccode>
    </paper>
  </volume>
</collection>
