<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.calcs">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the The 4th Workshop on Computational Approaches to Code Switching</booktitle>
      <editor><first>Thamar</first><last>Solorio</last></editor>
      <editor><first>Monojit</first><last>Choudhury</last></editor>
      <editor><first>Kalika</first><last>Bali</last></editor>
      <editor><first>Sunayana</first><last>Sitaram</last></editor>
      <editor><first>Amitava</first><last>Das</last></editor>
      <editor><first>Mona</first><last>Diab</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-66-5</isbn>
      <venue>calcs</venue>
    </meta>
    <frontmatter>
      <url hash="e74f52e0">2020.calcs-1.0</url>
      <bibkey>calcs-2020-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>An Annotated Corpus of Emerging Anglicisms in <fixed-case>S</fixed-case>panish Newspaper Headlines</title>
      <author><first>Elena</first><last>Alvarez-Mellado</last></author>
      <pages>1–8</pages>
      <abstract>The extraction of anglicisms (lexical borrowings from English) is relevant both for lexicographic purposes and for NLP downstream tasks. We introduce a corpus of European Spanish newspaper headlines annotated with anglicisms and a baseline model for anglicism extraction. In this paper we present: (1) a corpus of 21,570 newspaper headlines written in European Spanish annotated with emergent anglicisms and (2) a conditional random field baseline model with handcrafted features for anglicism extraction. We present the newspaper headlines corpus, describe the annotation tagset and guidelines and introduce a CRF model that can serve as baseline for the task of detecting anglicisms. The presented work is a first step towards the creation of an anglicism extractor for Spanish newswire.</abstract>
      <url hash="2990531b">2020.calcs-1.1</url>
      <language>eng</language>
      <bibkey>alvarez-mellado-2020-annotated</bibkey>
    </paper>
    <paper id="2">
      <title>A New Dataset for Natural Language Inference from Code-mixed Conversations</title>
      <author><first>Simran</first><last>Khanuja</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>9–16</pages>
      <abstract>Natural Language Inference (NLI) is the task of inferring the logical relationship, typically entailment or contradiction, between a premise and hypothesis. Code-mixing is the use of more than one language in the same conversation or utterance, and is prevalent in multilingual communities all over the world. In this paper, we present the first dataset for code-mixed NLI, in which both the premises and hypotheses are in code-mixed Hindi-English. We use data from Hindi movies (Bollywood) as premises, and crowd-source hypotheses from Hindi-English bilinguals. We conduct a pilot annotation study and describe the final annotation protocol based on observations from the pilot. Currently, the data collected consists of 400 premises in the form of code-mixed conversation snippets and 2240 code-mixed hypotheses. We conduct an extensive analysis to infer the linguistic phenomena commonly observed in the dataset obtained. We evaluate the dataset using a standard mBERT-based pipeline for NLI and report results.</abstract>
      <url hash="4a460eca">2020.calcs-1.2</url>
      <language>eng</language>
      <bibkey>khanuja-etal-2020-new</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="3">
      <title>When is Multi-task Learning Beneficial for Low-Resource Noisy Code-switched User-generated <fixed-case>A</fixed-case>lgerian Texts?</title>
      <author><first>Wafia</first><last>Adouane</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <pages>17–25</pages>
      <abstract>We investigate when is it beneficial to simultaneously learn representations for several tasks, in low-resource settings. For this, we work with noisy user-generated texts in Algerian, a low-resource non-standardised Arabic variety. That is, to mitigate the problem of the data scarcity, we experiment with jointly learning progressively 4 tasks, namely code-switch detection, named entity recognition, spell normalisation and correction, and identifying users’ sentiments. The selection of these tasks is motivated by the lack of labelled data for automatic morpho-syntactic or semantic sequence-tagging tasks for Algerian, in contrast to the case of much multi-task learning for NLP. Our empirical results show that multi-task learning is beneficial for some tasks in particular settings, and that the effect of each task on another, the order of the tasks, and the size of the training data of the task with more data do matter. Moreover, the data augmentation that we performed with no external resources has been shown to be beneficial for certain tasks.</abstract>
      <url hash="2905d6e6">2020.calcs-1.3</url>
      <language>eng</language>
      <bibkey>adouane-bernardy-2020-multi</bibkey>
    </paper>
    <paper id="4">
      <title>Evaluating Word Embeddings for <fixed-case>I</fixed-case>ndonesian–<fixed-case>E</fixed-case>nglish Code-Mixed Text Based on Synthetic Data</title>
      <author><first>Arra’Di Nur</first><last>Rizal</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <pages>26–35</pages>
      <abstract>Code-mixed texts are abundant, especially in social media, and poses a problem for NLP tools, which are typically trained on monolingual corpora. In this paper, we explore and evaluate different types of word embeddings for Indonesian–English code-mixed text. We propose the use of code-mixed embeddings, i.e. embeddings trained on code-mixed text. Because large corpora of code-mixed text are required to train embeddings, we describe a method for synthesizing a code-mixed corpus, grounded in literature and a survey. Using sentiment analysis as a case study, we show that code-mixed embeddings trained on synthesized data are at least as good as cross-lingual embeddings and better than monolingual embeddings.</abstract>
      <url hash="3a54e343">2020.calcs-1.4</url>
      <language>eng</language>
      <bibkey>rizal-stymne-2020-evaluating</bibkey>
    </paper>
    <paper id="5">
      <title>Understanding Script-Mixing: A Case Study of <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Bilingual <fixed-case>T</fixed-case>witter Users</title>
      <author><first>Abhishek</first><last>Srivastava</last></author>
      <author><first>Kalika</first><last>Bali</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>36–44</pages>
      <abstract>In a multi-lingual and multi-script society such as India, many users resort to code-mixing while typing on social media. While code-mixing has received a lot of attention in the past few years, it has mostly been studied within a single-script scenario. In this work, we present a case study of Hindi-English bilingual Twitter users while considering the nuances that come with the intermixing of different scripts. We present a concise analysis of how scripts and languages interact in communities and cultures where code-mixing is rampant and offer certain insights into the findings. Our analysis shows that both intra-sentential and inter-sentential script-mixing are present on Twitter and show different behavior in different contexts. Examples suggest that script can be employed as a tool for emphasizing certain phrases within a sentence or disambiguating the meaning of a word. Script choice can also be an indicator of whether a word is borrowed or not. We present our analysis along with examples that bring out the nuances of the different cases.</abstract>
      <url hash="a543c07f">2020.calcs-1.5</url>
      <language>eng</language>
      <bibkey>srivastava-etal-2020-understanding</bibkey>
    </paper>
    <paper id="6">
      <title>Sentiment Analysis for <fixed-case>H</fixed-case>inglish Code-mixed Tweets by means of Cross-lingual Word Embeddings</title>
      <author><first>Pranaydeep</first><last>Singh</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>45–51</pages>
      <abstract>This paper investigates the use of unsupervised cross-lingual embeddings for solving the problem of code-mixed social media text understanding. We specifically investigate the use of these embeddings for a sentiment analysis task for Hinglish Tweets, viz. English combined with (transliterated) Hindi. In a first step, baseline models, initialized with monolingual embeddings obtained from large collections of tweets in English and code-mixed Hinglish, were trained. In a second step, two systems using cross-lingual embeddings were researched, being (1) a supervised classifier and (2) a transfer learning approach trained on English sentiment data and evaluated on code-mixed data. We demonstrate that incorporating cross-lingual embeddings improves the results (F1-score of 0.635 versus a monolingual baseline of 0.616), without any parallel data required to train the cross-lingual embeddings. In addition, the results show that the cross-lingual embeddings not only improve the results in a fully supervised setting, but they can also be used as a base for distant supervision, by training a sentiment model in one of the source languages and evaluating on the other language projected in the same space. The transfer learning experiments result in an F1-score of 0.556, which is almost on par with the supervised settings and speak to the robustness of the cross-lingual embeddings approach.</abstract>
      <url hash="fb3df722">2020.calcs-1.6</url>
      <language>eng</language>
      <bibkey>singh-lefever-2020-sentiment</bibkey>
    </paper>
    <paper id="7">
      <title>Semi-supervised acoustic and language model training for <fixed-case>E</fixed-case>nglish-isi<fixed-case>Z</fixed-case>ulu code-switched speech recognition</title>
      <author><first>Astik</first><last>Biswas</last></author>
      <author><first>Febe</first><last>De Wet</last></author>
      <author><first>Ewald</first><last>Van der westhuizen</last></author>
      <author><first>Thomas</first><last>Niesler</last></author>
      <pages>52–56</pages>
      <abstract>We present an analysis of semi-supervised acoustic and language model training for English-isiZulu code-switched (CS) ASR using soap opera speech. Approximately 11 hours of untranscribed multilingual speech was transcribed automatically using four bilingual CS transcription systems operating in English-isiZulu, English-isiXhosa, English-Setswana and English-Sesotho. These transcriptions were incorporated into the acoustic and language model training sets. Results showed that the TDNN-F acoustic models benefit from the additional semi-supervised data and that even better performance could be achieved by including additional CNN layers. Using these CNN-TDNN-F acoustic models, a first iteration of semi-supervised training achieved an absolute mixed-language WER reduction of 3.44%, and a further 2.18% after a second iteration. Although the languages in the untranscribed data were unknown, the best results were obtained when all automatically transcribed data was used for training and not just the utterances classified as English-isiZulu. Despite perplexity improvements, the semi-supervised language model was not able to improve the ASR performance.</abstract>
      <url hash="ae648970">2020.calcs-1.7</url>
      <language>eng</language>
      <bibkey>biswas-etal-2020-semi</bibkey>
    </paper>
    <paper id="8">
      <title>Code-mixed parse trees and how to find them</title>
      <author><first>Anirudh</first><last>Srinivasan</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>57–64</pages>
      <abstract>In this paper, we explore the methods of obtaining parse trees of code-mixed sentences and analyse the obtained trees. Existing work has shown that linguistic theories can be used to generate code-mixed sentences from a set of parallel sentences. We build upon this work, using one of these theories, the Equivalence-Constraint theory to obtain the parse trees of synthetically generated code-mixed sentences and evaluate them with a neural constituency parser. We highlight the lack of a dataset non-synthetic code-mixed constituency parse trees and how it makes our evaluation difficult. To complete our evaluation, we convert a code-mixed dependency parse tree set into “pseudo constituency trees” and find that a parser trained on synthetically generated trees is able to decently parse these as well.</abstract>
      <url hash="be73adf0">2020.calcs-1.8</url>
      <language>eng</language>
      <bibkey>srinivasan-etal-2020-code</bibkey>
    </paper>
    <paper id="9">
      <title>Towards an Efficient Code-Mixed Grapheme-to-Phoneme Conversion in an Agglutinative Language: A Case Study on To-<fixed-case>K</fixed-case>orean Transliteration</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Seok Min</first><last>Kim</last></author>
      <author><first>Nam Soo</first><last>Kim</last></author>
      <pages>65–70</pages>
      <abstract>Code-mixed grapheme-to-phoneme (G2P) conversion is a crucial issue for modern speech recognition and synthesis task, but has been seldom investigated in sentence-level in literature. In this study, we construct a system that performs precise and efficient multi-stage code-mixed G2P conversion, for a less studied agglutinative language, Korean. The proposed system undertakes a sentence-level transliteration that is effective in the accurate processing of Korean text. We formulate the underlying philosophy that supports our approach and demonstrate how it fits with the contemporary document.</abstract>
      <url hash="1e89f2de">2020.calcs-1.9</url>
      <language>eng</language>
      <bibkey>cho-etal-2020-towards</bibkey>
    </paper>
  </volume>
</collection>
