<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.lchange">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change</booktitle>
      <editor><first>Nina</first><last>Tahmasebi</last></editor>
      <editor><first>Syrielle</first><last>Montariol</last></editor>
      <editor><first>Andrey</first><last>Kutuzov</last></editor>
      <editor><first>Simon</first><last>Hengchen</last></editor>
      <editor><first>Haim</first><last>Dubossarsky</last></editor>
      <editor><first>Lars</first><last>Borin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="fa41faed">2022.lchange-1</url>
      <venue>lchange</venue>
    </meta>
    <frontmatter>
      <url hash="a8d4ed9d">2022.lchange-1.0</url>
      <bibkey>lchange-2022-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Multilingual Benchmark to Capture Olfactory Situations over Time</title>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Teresa</first><last>Paccosi</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <author><first>Marieke</first><last>Van Erp</last></author>
      <author><first>Inger</first><last>Leemans</last></author>
      <author><first>Pasquale</first><last>Lisena</last></author>
      <author><first>Raphael</first><last>Troncy</last></author>
      <author><first>William</first><last>Tullett</last></author>
      <author><first>Ali</first><last>Hürriyetoğlu</last></author>
      <author><first>Ger</first><last>Dijkstra</last></author>
      <author><first>Femke</first><last>Gordijn</last></author>
      <author><first>Elias</first><last>Jürgens</last></author>
      <author><first>Josephine</first><last>Koopman</last></author>
      <author><first>Aron</first><last>Ouwerkerk</last></author>
      <author><first>Sanne</first><last>Steen</last></author>
      <author><first>Inna</first><last>Novalija</last></author>
      <author><first>Janez</first><last>Brank</last></author>
      <author><first>Dunja</first><last>Mladenic</last></author>
      <author><first>Anja</first><last>Zidar</last></author>
      <pages>1-10</pages>
      <abstract>We present a benchmark in six European languages containing manually annotated information about olfactory situations and events following a FrameNet-like approach. The documents selection covers ten domains of interest to cultural historians in the olfactory domain and includes texts published between 1620 to 1920, allowing a diachronic analysis of smell descriptions. With this work, we aim to foster the development of olfactory information extraction approaches as well as the analysis of changes in smell descriptions over time.</abstract>
      <url hash="fd7d9e3f">2022.lchange-1.1</url>
      <bibkey>menini-etal-2022-multilingual</bibkey>
      <doi>10.18653/v1/2022.lchange-1.1</doi>
      <video href="2022.lchange-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Language Acquisition, Neutral Change, and Diachronic Trends in Noun Classifiers</title>
      <author><first>Aniket</first><last>Kali</last></author>
      <author><first>Jordan</first><last>Kodner</last></author>
      <pages>11-22</pages>
      <abstract>Languages around the world employ classifier systems as a method of semantic organization and categorization. These systems are rife with variability, violability, and ambiguity, and are prone to constant change over time. We explicitly model change in classifier systems as the population-level outcome of child language acquisition over time in order to shed light on the factors that drive change to classifier systems. Our research consists of two parts: a contrastive corpus study of Cantonese and Mandarin child-directed speech to determine the role that ambiguity and homophony avoidance may play in classifier learning and change followed by a series of population-level learning simulations of an abstract classifier system. We find that acquisition without reference to ambiguity avoidance is sufficient to drive broad trends in classifier change and suggest an additional role for adults and discourse factors in classifier death.</abstract>
      <url hash="489370e3">2022.lchange-1.2</url>
      <bibkey>kali-kodner-2022-language</bibkey>
      <doi>10.18653/v1/2022.lchange-1.2</doi>
      <video href="2022.lchange-1.2.mp4"/>
      <pwccode url="https://github.com/an-k45/classifier-change" additional="false">an-k45/classifier-change</pwccode>
    </paper>
    <paper id="3">
      <title>Deconstructing destruction: A Cognitive Linguistics perspective on a computational analysis of diachronic change</title>
      <author><first>Karlien</first><last>Franco</last></author>
      <author><first>Mariana</first><last>Montes</last></author>
      <author><first>Kris</first><last>Heylen</last></author>
      <pages>23-32</pages>
      <abstract>In this paper, we aim to introduce a Cognitive Linguistics perspective into a computational analysis of near-synonyms. We focus on a single set of Dutch near-synonyms, vernielen and vernietigen, roughly translated as ‘to destroy’, replicating the analysis from Geeraerts (1997) with distributional models. Our analysis, which tracks the meaning of both words in a corpus of 16th-20th century prose data, shows that both lexical items have undergone semantic change, led by differences in their prototypical semantic core.</abstract>
      <url hash="497a6d0d">2022.lchange-1.3</url>
      <bibkey>franco-etal-2022-deconstructing</bibkey>
      <doi>10.18653/v1/2022.lchange-1.3</doi>
    </paper>
    <paper id="4">
      <title>What is Done is Done: an Incremental Approach to Semantic Shift Detection</title>
      <author><first>Francesco</first><last>Periti</last></author>
      <author><first>Alfio</first><last>Ferrara</last></author>
      <author><first>Stefano</first><last>Montanelli</last></author>
      <author><first>Martin</first><last>Ruskov</last></author>
      <pages>33-43</pages>
      <abstract>Contextual word embedding techniques for semantic shift detection are receiving more and more attention. In this paper, we present What is Done is Done (WiDiD), an incremental approach to semantic shift detection based on incremental clustering techniques and contextual embedding methods to capture the changes over the meanings of a target word along a diachronic corpus. In WiDiD, the word contexts observed in the past are consolidated as a set of clusters that constitute the “memory” of the word meanings observed so far. Such a memory is exploited as a basis for subsequent word observations, so that the meanings observed in the present are stratified over the past ones.</abstract>
      <url hash="389884d6">2022.lchange-1.4</url>
      <bibkey>periti-etal-2022-done</bibkey>
      <doi>10.18653/v1/2022.lchange-1.4</doi>
      <video href="2022.lchange-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>From qualifiers to quantifiers: semantic shift at the paradigm level</title>
      <author><first>Quentin</first><last>Feltgen</last></author>
      <pages>44-53</pages>
      <abstract>Language change has often been conceived as a competition between linguistic variants. However, language units may be complex organizations in themselves, e.g. in the case of schematic constructions, featuring a free slot. Such a slot is filled by words forming a set or ‘paradigm’ and engaging in inter-related dynamics within this constructional environment. To tackle this complexity, a simple computational method is offered to automatically characterize their interactions, and visualize them through networks of cooperation and competition. Applying this method to the French paradigm of quantifiers, I show that this method efficiently captures phenomena regarding the evolving organization of constructional paradigms, in particular the constitution of competing clusters of fillers that promote different semantic strategies overall.</abstract>
      <url hash="4a3df93e">2022.lchange-1.5</url>
      <bibkey>feltgen-2022-qualifiers</bibkey>
      <doi>10.18653/v1/2022.lchange-1.5</doi>
      <video href="2022.lchange-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change</title>
      <author><first>Mario</first><last>Giulianelli</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <pages>54-67</pages>
      <abstract>Morphological and syntactic changes in word usage — as captured, e.g., by grammatical profiles — have been shown to be good predictors of a word’s meaning change. In this work, we explore whether large pre-trained contextualised language models, a common tool for lexical semantic change detection, are sensitive to such morphosyntactic changes. To this end, we first compare the performance of grammatical profiles against that of a multilingual neural language model (XLM-R) on 10 datasets, covering 7 languages, and then combine the two approaches in ensembles to assess their complementarity. Our results show that ensembling grammatical profiles with XLM-R improves semantic change detection performance for most datasets and languages. This indicates that language models do not fully cover the fine-grained morphological and syntactic signals that are explicitly represented in grammatical profiles. An interesting exception are the test sets where the time spans under analysis are much longer than the time gap between them (for example, century-long spans with a one-year gap between them). Morphosyntactic change is slow so grammatical profiles do not detect in such cases. In contrast, language models, thanks to their access to lexical information, are able to detect fast topical changes.</abstract>
      <url hash="af30a035">2022.lchange-1.6</url>
      <bibkey>giulianelli-etal-2022-fire</bibkey>
      <doi>10.18653/v1/2022.lchange-1.6</doi>
      <video href="2022.lchange-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Explainable Publication Year Prediction of Eighteenth Century Texts with the <fixed-case>BERT</fixed-case> Model</title>
      <author><first>Iiro</first><last>Rastas</last></author>
      <author><first>Yann</first><last>Ciarán Ryan</last></author>
      <author><first>Iiro</first><last>Tiihonen</last></author>
      <author><first>Mohammadreza</first><last>Qaraei</last></author>
      <author><first>Liina</first><last>Repo</last></author>
      <author><first>Rohit</first><last>Babbar</last></author>
      <author><first>Eetu</first><last>Mäkelä</last></author>
      <author><first>Mikko</first><last>Tolonen</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>68-77</pages>
      <abstract>In this paper, we describe a BERT model trained on the Eighteenth Century Collections Online (ECCO) dataset of digitized documents. The ECCO dataset poses unique modelling challenges due to the presence of Optical Character Recognition (OCR) artifacts. We establish the performance of the BERT model on a publication year prediction task against linear baseline models and human judgement, finding the BERT model to be superior to both and able to date the works, on average, with less than 7 years absolute error. We also explore how language change over time affects the model by analyzing the features the model uses for publication year predictions as given by the Integrated Gradients model explanation method.</abstract>
      <url hash="5ebf777d">2022.lchange-1.7</url>
      <bibkey>rastas-etal-2022-explainable</bibkey>
      <doi>10.18653/v1/2022.lchange-1.7</doi>
      <video href="2022.lchange-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Using Cross-Lingual Part of Speech Tagging for Partially Reconstructing the Classic Language Family Tree Model</title>
      <author><first>Anat</first><last>Samohi</last></author>
      <author><first>Daniel</first><last>Weisberg Mitelman</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>78-88</pages>
      <abstract>The tree model is well known for expressing the historic evolution of languages. This model has been considered as a method of describing genetic relationships between languages. Nevertheless, some researchers question the model’s ability to predict the proximity between two languages, since it represents genetic relatedness rather than linguistic resemblance. Defining other language proximity models has been an active research area for many years. In this paper we explore a part-of-speech model for defining proximity between languages using a multilingual language model that was fine-tuned on the task of cross-lingual part-of-speech tagging. We train the model on one language and evaluate it on another; the measured performance is then used to define the proximity between the two languages. By further developing the model, we show that it can reconstruct some parts of the tree model.</abstract>
      <url hash="a07155f3">2022.lchange-1.8</url>
      <bibkey>samohi-etal-2022-using</bibkey>
      <doi>10.18653/v1/2022.lchange-1.8</doi>
      <video href="2022.lchange-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>A New Framework for Fast Automated Phonological Reconstruction Using Trimmed Alignments and Sound Correspondence Patterns</title>
      <author><first>Johann-Mattis</first><last>List</last></author>
      <author><first>Robert</first><last>Forkel</last></author>
      <author><first>Nathan</first><last>Hill</last></author>
      <pages>89-96</pages>
      <abstract>Computational approaches in historical linguistics have been increasingly applied during the past decade and many new methods that implement parts of the traditional comparative method have been proposed. Despite these increased efforts, there are not many easy-to-use and fast approaches for the task of phonological reconstruction. Here we present a new framework that combines state-of-the-art techniques for automated sequence comparison with novel techniques for phonetic alignment analysis and sound correspondence pattern detection to allow for the supervised reconstruction of word forms in ancestral languages. We test the method on a new dataset covering six groups from three different language families. The results show that our method yields promising results while at the same time being not only fast but also easy to apply and expand.</abstract>
      <url hash="d5bee8a3">2022.lchange-1.9</url>
      <bibkey>list-etal-2022-new</bibkey>
      <doi>10.18653/v1/2022.lchange-1.9</doi>
      <video href="2022.lchange-1.9.mp4"/>
      <pwccode url="https://github.com/lingpy/supervised-reconstruction-paper" additional="false">lingpy/supervised-reconstruction-paper</pwccode>
    </paper>
    <paper id="10">
      <title>Caveats of Measuring Semantic Change of Cognates and Borrowings using Multilingual Word Embeddings</title>
      <author><first>Clémentine</first><last>Fourrier</last></author>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <pages>97-112</pages>
      <abstract>Cognates and borrowings carry different aspects of etymological evolution. In this work, we study semantic change of such items using multilingual word embeddings, both static and contextualised. We underline caveats identified while building and evaluating these embeddings. We release both said embeddings and a newly-built historical words lexicon, containing typed relations between words of varied Romance languages.</abstract>
      <url hash="9ca0968d">2022.lchange-1.10</url>
      <bibkey>fourrier-montariol-2022-caveats</bibkey>
      <doi>10.18653/v1/2022.lchange-1.10</doi>
      <video href="2022.lchange-1.10.mp4"/>
      <pwccode url="https://github.com/clefourrier/historical-semantic-change" additional="false">clefourrier/historical-semantic-change</pwccode>
    </paper>
    <paper id="11">
      <title>Lexicon of Changes: Towards the Evaluation of Diachronic Semantic Shift in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Chu-ren</first><last>Huang</last></author>
      <pages>113-118</pages>
      <abstract>Recent research has brought a wind of using computational approaches to the classic topic of semantic change, aiming to tackle one of the most challenging issues in the evolution of human language. While several methods for detecting semantic change have been proposed, such studies are limited to a few languages, where evaluation datasets are available. This paper presents the first dataset for evaluating Chinese semantic change in contexts preceding and following the Reform and Opening-up, covering a 50-year period in Modern Chinese. Following the DURel framework, we collected 6,000 human judgments for the dataset. We also reported the performance of alignment-based word embedding models on this evaluation dataset, achieving high and significant correlation scores.</abstract>
      <url hash="e78e4c6c">2022.lchange-1.11</url>
      <bibkey>chen-etal-2022-lexicon</bibkey>
      <doi>10.18653/v1/2022.lchange-1.11</doi>
      <video href="2022.lchange-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Low <fixed-case>S</fixed-case>axon dialect distances at the orthographic and syntactic level</title>
      <author><first>Janine</first><last>Siewert</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Martijn</first><last>Wieling</last></author>
      <pages>119-124</pages>
      <abstract>We compare five Low Saxon dialects from the 19th and 21st century from Germany and the Netherlands with each other as well as with modern Standard Dutch and Standard German. Our comparison is based on character n-grams on the one hand and PoS n-grams on the other and we show that these two lead to different distances. Particularly in the PoS-based distances, one can observe all of the 21st century Low Saxon dialects shifting towards the modern majority languages.</abstract>
      <url hash="cbcabff5">2022.lchange-1.12</url>
      <bibkey>siewert-etal-2022-low</bibkey>
      <doi>10.18653/v1/2022.lchange-1.12</doi>
      <video href="2022.lchange-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>“Vaderland”, “Volk” and “Natie”: Semantic Change Related to Nationalism in <fixed-case>D</fixed-case>utch Literature Between 1700 and 1880 Captured with Dynamic <fixed-case>B</fixed-case>ernoulli Word Embeddings</title>
      <author><first>Marije</first><last>Timmermans</last></author>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <pages>125-130</pages>
      <abstract>Languages can respond to external events in various ways - the creation of new words or named entities, additional senses might develop for already existing words or the valence of words can change. In this work, we explore the semantic shift of the Dutch words “natie” (“nation”), “volk” (“people”) and “vaderland” (“fatherland”) over a period that is known for the rise of nationalism in Europe: 1700-1880. The semantic change is measured by means of Dynamic Bernoulli Word Embeddings which allow for comparison between word embeddings over different time slices. The word embeddings were generated based on Dutch fiction literature divided over different decades. From the analysis of the absolute drifts, it appears that the word “natie” underwent a relatively small drift. However, the drifts of “vaderland’” and “volk”’ show multiple peaks, culminating around the turn of the nineteenth century. To verify whether this semantic change can indeed be attributed to nationalistic movements, a detailed analysis of the nearest neighbours of the target words is provided. From the analysis, it appears that “natie”, “volk” and “vaderlan”’ became more nationalistically-loaded over time.</abstract>
      <url hash="62c3d2e5">2022.lchange-1.13</url>
      <bibkey>timmermans-etal-2022-vaderland</bibkey>
      <doi>10.18653/v1/2022.lchange-1.13</doi>
      <video href="2022.lchange-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Using neural topic models to track context shifts of words: a case study of <fixed-case>COVID</fixed-case>-related terms before and after the lockdown in <fixed-case>A</fixed-case>pril 2020</title>
      <author><first>Olga</first><last>Kellert</last></author>
      <author><first>Md</first><last>Mahmud Uz Zaman</last></author>
      <pages>131-139</pages>
      <abstract>This paper explores lexical meaning changes in a new dataset, which includes tweets from before and after the COVID-related lockdown in April 2020. We use this dataset to evaluate traditional and more recent unsupervised approaches to lexical semantic change that make use of contextualized word representations based on the BERT neural language model to obtain representations of word usages. We argue that previous models that encode local representations of words cannot capture global context shifts such as the context shift of face masks since the pandemic outbreak. We experiment with neural topic models to track context shifts of words. We show that this approach can reveal textual associations of words that go beyond their lexical meaning representation. We discuss future work and how to proceed capturing the pragmatic aspect of meaning change as opposed to lexical semantic change.</abstract>
      <url hash="3208f67c">2022.lchange-1.14</url>
      <bibkey>kellert-mahmud-uz-zaman-2022-using</bibkey>
      <doi>10.18653/v1/2022.lchange-1.14</doi>
      <video href="2022.lchange-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Roadblocks in Gender Bias Measurement for Diachronic Corpora</title>
      <author><first>Saied</first><last>Alshahrani</last></author>
      <author><first>Esma</first><last>Wali</last></author>
      <author><first>Abdullah</first><last>R Alshamsan</last></author>
      <author><first>Yan</first><last>Chen</last></author>
      <author><first>Jeanna</first><last>Matthews</last></author>
      <pages>140-148</pages>
      <abstract>The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text. One important question that has been raised about word embeddings is the degree of gender bias learned from corpora. Bolukbasi et al. (2016) proposed an important technique for quantifying gender bias in word embeddings that, at its heart, is lexically based and relies on sets of highly gendered word pairs (e.g., mother/father and madam/sir) and a list of professions words (e.g., doctor and nurse). In this paper, we document problems that arise with this method to quantify gender bias in diachronic corpora. Focusing on Arabic and Chinese corpora, in particular, we document clear changes in profession words used over time and, somewhat surprisingly, even changes in the simpler gendered defining set word pairs. We further document complications in languages such as Arabic, where many words are highly polysemous/homonymous, especially female professions words.</abstract>
      <url hash="b7125197">2022.lchange-1.15</url>
      <bibkey>alshahrani-etal-2022-roadblocks</bibkey>
      <doi>10.18653/v1/2022.lchange-1.15</doi>
      <video href="2022.lchange-1.15.mp4"/>
      <pwccode url="https://github.com/clarkson-accountability-transparency/gbiasroadblocks" additional="false">clarkson-accountability-transparency/gbiasroadblocks</pwccode>
    </paper>
    <paper id="16">
      <title><fixed-case>LSCD</fixed-case>iscovery: A shared task on semantic change discovery and detection in <fixed-case>S</fixed-case>panish</title>
      <author><first>Frank D.</first><last>Zamora-Reina</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <pages>149-164</pages>
      <abstract>We present the first shared task on semantic change discovery and detection in Spanish. We create the first dataset of Spanish words manually annotated by semantic change using the DURel framewok (Schlechtweg et al., 2018). The task is divided in two phases: 1) graded change discovery, and 2) binary change detection. In addition to introducing a new language for this task, the main novelty with respect to the previous tasks consists in predicting and evaluating changes for all vocabulary words in the corpus. Six teams participated in phase 1 and seven teams in phase 2 of the shared task, and the best system obtained a Spearman rank correlation of 0.735 for phase 1 and an F1 score of 0.735 for phase 2. We describe the systems developed by the competing teams, highlighting the techniques that were particularly useful.</abstract>
      <url hash="218c1308">2022.lchange-1.16</url>
      <bibkey>d-zamora-reina-etal-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.16</doi>
      <video href="2022.lchange-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title><fixed-case>BOS</fixed-case> at <fixed-case>LSCD</fixed-case>iscovery: Lexical Substitution for Interpretable Lexical Semantic Change Detection</title>
      <author><first>Artem</first><last>Kudisov</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>165-172</pages>
      <abstract>We propose a solution for the LSCDiscovery shared task on Lexical Semantic Change Detection in Spanish. Our approach is based on generating lexical substitutes that describe old and new senses of a given word. This approach achieves the second best result in sense loss and sense gain detection subtasks. By observing those substitutes that are specific for only one time period, one can understand which senses were obtained or lost. This allows providing more detailed information about semantic change to the user and makes our method interpretable.</abstract>
      <url hash="b76ade55">2022.lchange-1.17</url>
      <bibkey>kudisov-arefyev-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.17</doi>
      <video href="2022.lchange-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>eep<fixed-case>M</fixed-case>istake at <fixed-case>LSCD</fixed-case>iscovery: Can a Multilingual Word-in-Context Model Replace Human Annotators?</title>
      <author><first>Daniil</first><last>Homskiy</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>173-179</pages>
      <abstract>In this paper we describe our solution of the LSCDiscovery shared task on Lexical Semantic Change Discovery (LSCD) in Spanish. Our solution employs a Word-in-Context (WiC) model, which is trained to determine if a particular word has the same meaning in two given contexts. We basically try to replicate the annotation of the dataset for the shared task, but replacing human annotators with a neural network. In the graded change discovery subtask, our solution has achieved the 2nd best result according to all metrics. In the main binary change detection subtask, our F1-score is 0.655 compared to 0.716 of the best submission, corresponding to the 5th place. However, in the optional sense gain detection subtask we have outperformed all other participants. During the post-evaluation experiments we compared different ways to prepare WiC data in Spanish for fine-tuning. We have found that it helps leaving only examples annotated as 1 (unrelated senses) and 4 (identical senses) rather than using 2x more examples including intermediate annotations.</abstract>
      <url hash="a81f99f9">2022.lchange-1.18</url>
      <bibkey>homskiy-arefyev-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.18</doi>
      <video href="2022.lchange-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title><fixed-case>UA</fixed-case>lberta at <fixed-case>LSCD</fixed-case>iscovery: Lexical Semantic Change Detection via Word Sense Disambiguation</title>
      <author><first>Daniela</first><last>Teodorescu</last></author>
      <author><first>Spencer</first><last>von der Ohe</last></author>
      <author><first>Grzegorz</first><last>Kondrak</last></author>
      <pages>180-186</pages>
      <abstract>We describe our two systems for the shared task on Lexical Semantic Change Discovery in Spanish. For binary change detection, we frame the task as a word sense disambiguation (WSD) problem. We derive sense frequency distributions for target words in both old and modern corpora. We assume that the word semantics have changed if a sense is observed in only one of the two corpora, or the relative change for any sense exceeds a tuned threshold. For graded change discovery, we follow the design of CIRCE (Pömsl and Lyapin, 2020) by combining both static and contextual embeddings. For contextual embeddings, we use XLM-RoBERTa instead of BERT, and train the model to predict a masked token instead of the time period. Our language-independent methods achieve results that are close to the best-performing systems in the shared task.</abstract>
      <url hash="58ef2833">2022.lchange-1.19</url>
      <bibkey>teodorescu-etal-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>o<fixed-case>H</fixed-case>i<fixed-case>L</fixed-case>i at <fixed-case>LSCD</fixed-case>iscovery: the Role of Linguistic Features in Predicting Semantic Change</title>
      <author><first>Ana</first><last>Sabina Uban</last></author>
      <author><first>Alina</first><last>Maria Cristea</last></author>
      <author><first>Anca</first><last>Daniela Dinu</last></author>
      <author><first>Liviu</first><last>P Dinu</last></author>
      <author><first>Simona</first><last>Georgescu</last></author>
      <author><first>Laurentiu</first><last>Zoicas</last></author>
      <pages>187-192</pages>
      <abstract>This paper presents the contributions of the CoToHiLi team for the LSCDiscovery shared task on semantic change in the Spanish language. We participated in both tasks (graded discovery and binary change, including sense gain and sense loss) and proposed models based on word embedding distances combined with hand-crafted linguistic features, including polysemy, number of neological synonyms, and relation to cognates in English. We find that models that include linguistically informed features combined using weights assigned manually by experts lead to promising results.</abstract>
      <url hash="13885633">2022.lchange-1.20</url>
      <bibkey>sabina-uban-etal-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>HSE</fixed-case> at <fixed-case>LSCD</fixed-case>iscovery in <fixed-case>S</fixed-case>panish: Clustering and Profiling for Lexical Semantic Change Discovery</title>
      <author><first>Kseniia</first><last>Kashleva</last></author>
      <author><first>Alexander</first><last>Shein</last></author>
      <author><first>Elizaveta</first><last>Tukhtina</last></author>
      <author><first>Svetlana</first><last>Vydrina</last></author>
      <pages>193-197</pages>
      <abstract>This paper describes the methods used for lexical semantic change discovery in Spanish. We tried the method based on BERT embeddings with clustering, the method based on grammatical profiles and the grammatical profiles method enhanced with permutation tests. BERT embeddings with clustering turned out to show the best results for both graded and binary semantic change detection outperforming the baseline. Our best submission for graded discovery was the 3rd best result, while for binary detection it was the 2nd place (precision) and the 7th place (both F1-score and recall). Our highest precision for binary detection was 0.75 and it was achieved due to improving grammatical profiling with permutation tests.</abstract>
      <url hash="bfc7112d">2022.lchange-1.21</url>
      <bibkey>kashleva-etal-2022-black</bibkey>
      <revision id="1" href="2022.lchange-1.21v1" hash="072a9322"/>
      <revision id="2" href="2022.lchange-1.21v2" hash="bfc7112d" date="2022-05-21">Various fixes throughout the paper.</revision>
      <doi>10.18653/v1/2022.lchange-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>G</fixed-case>loss<fixed-case>R</fixed-case>eader at <fixed-case>LSCD</fixed-case>iscovery: Train to Select a Proper Gloss in <fixed-case>E</fixed-case>nglish – Discover Lexical Semantic Change in <fixed-case>S</fixed-case>panish</title>
      <author><first>Maxim</first><last>Rachinskiy</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>198-203</pages>
      <abstract>The contextualized embeddings obtained from neural networks pre-trained as Language Models (LM) or Masked Language Models (MLM) are not well suitable for solving the Lexical Semantic Change Detection (LSCD) task because they are more sensitive to changes in word forms rather than word meaning, a property previously known as the word form bias or orthographic bias. Unlike many other NLP tasks, it is also not obvious how to fine-tune such models for LSCD. In order to conclude if there are any differences between senses of a particular word in two corpora, a human annotator or a system shall analyze many examples containing this word from both corpora. This makes annotation of LSCD datasets very labour-consuming. The existing LSCD datasets contain up to 100 words that are labeled according to their semantic change, which is hardly enough for fine-tuning. To solve these problems we fine-tune the XLM-R MLM as part of a gloss-based WSD system on a large WSD dataset in English. Then we employ zero-shot cross-lingual transferability of XLM-R to build the contextualized embeddings for examples in Spanish. In order to obtain the graded change score for each word, we calculate the average distance between our improved contextualized embeddings of its old and new occurrences. For the binary change detection subtask, we apply thresholding to the same scores. Our solution has shown the best results among all other participants in all subtasks except for the optional sense gain detection subtask.</abstract>
      <url hash="ff0c8fb2">2022.lchange-1.22</url>
      <bibkey>rachinskiy-arefyev-2022-black</bibkey>
      <doi>10.18653/v1/2022.lchange-1.22</doi>
    </paper>
  </volume>
</collection>
