<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.csrr">
  <volume id="1" ingest-date="2022-05-15">
    <meta>
      <booktitle>Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)</booktitle>
      <editor><first>Antoine</first><last>Bosselut</last></editor>
      <editor><first>Xiang</first><last>Li</last></editor>
      <editor><first>Bill Yuchen</first><last>Lin</last></editor>
      <editor><first>Vered</first><last>Shwartz</last></editor>
      <editor><first>Bodhisattwa Prasad</first><last>Majumder</last></editor>
      <editor><first>Yash Kumar</first><last>Lal</last></editor>
      <editor><first>Rachel</first><last>Rudinger</last></editor>
      <editor><first>Xiang</first><last>Ren</last></editor>
      <editor><first>Niket</first><last>Tandon</last></editor>
      <editor><first>Vilém</first><last>Zouhar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="e20a4fcb">2022.csrr-1</url>
      <venue>csrr</venue>
    </meta>
    <frontmatter>
      <url hash="98ae1339">2022.csrr-1.0</url>
      <bibkey>csrr-2022-commonsense</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Identifying relevant common sense information in knowledge graphs</title>
      <author><first>Guy</first><last>Aglionby</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>1-7</pages>
      <abstract>Knowledge graphs are often used to store common sense information that is useful for various tasks. However, the extraction of contextually-relevant knowledge is an unsolved problem, and current approaches are relatively simple. Here we introduce a triple selection method based on a ranking model and find that it improves question answering accuracy over existing methods. We additionally investigate methods to ensure that extracted triples form a connected graph. Graph connectivity is important for model interpretability, as paths are frequently used as explanations for the reasoning that connects question and answer.</abstract>
      <url hash="ecea388b">2022.csrr-1.1</url>
      <bibkey>aglionby-teufel-2022-identifying</bibkey>
      <doi>10.18653/v1/2022.csrr-1.1</doi>
      <video href="2022.csrr-1.1.mp4"/>
      <pwccode url="https://github.com/guyaglionby/kg-common-sense-extraction" additional="false">guyaglionby/kg-common-sense-extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
    </paper>
    <paper id="2">
      <title>Cloze Evaluation for Deeper Understanding of Commonsense Stories in <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>8-16</pages>
      <abstract>Story comprehension that involves complex causal and temporal relations is a critical task in NLP, but previous studies have focused predominantly on English, leaving open the question of how the findings generalize to other languages, such as Indonesian. In this paper, we follow the Story Cloze Test framework of Mostafazadeh et al. (2016) in evaluating story understanding in Indonesian, by constructing a four-sentence story with one correct ending and one incorrect ending. To investigate commonsense knowledge acquisition in language models, we experimented with: (1) a classification task to predict the correct ending; and (2) a generation task to complete the story with a single sentence. We investigate these tasks in two settings: (i) monolingual training and ii) zero-shot cross-lingual transfer between Indonesian and English.</abstract>
      <url hash="dfe0a8c0">2022.csrr-1.2</url>
      <bibkey>koto-etal-2022-cloze</bibkey>
      <doi>10.18653/v1/2022.csrr-1.2</doi>
      <video href="2022.csrr-1.2.mp4"/>
      <pwccode url="https://github.com/fajri91/indocloze" additional="false">fajri91/indocloze</pwccode>
    </paper>
    <paper id="3">
      <title>Psycholinguistic Diagnosis of Language Models’ Commonsense Reasoning</title>
      <author><first>Yan</first><last>Cong</last></author>
      <pages>17-22</pages>
      <abstract>Neural language models have attracted a lot of attention in the past few years. More and more researchers are getting intrigued by how language models encode commonsense, specifically what kind of commonsense they understand, and why they do. This paper analyzed neural language models’ understanding of commonsense pragmatics (i.e., implied meanings) through human behavioral and neurophysiological data. These psycholinguistic tests are designed to draw conclusions based on predictive responses in context, making them very well suited to test word-prediction models such as BERT in natural settings. They can provide the appropriate prompts and tasks to answer questions about linguistic mechanisms underlying predictive responses. This paper adopted psycholinguistic datasets to probe language models’ commonsense reasoning. Findings suggest that GPT-3’s performance was mostly at chance in the psycholinguistic tasks. We also showed that DistillBERT had some understanding of the (implied) intent that’s shared among most people. Such intent is implicitly reflected in the usage of conversational implicatures and presuppositions. Whether or not fine-tuning improved its performance to human-level depends on the type of commonsense reasoning.</abstract>
      <url hash="a7945d4d">2022.csrr-1.3</url>
      <bibkey>cong-2022-psycholinguistic</bibkey>
      <doi>10.18653/v1/2022.csrr-1.3</doi>
      <video href="2022.csrr-1.3.mp4"/>
      <pwccode url="https://github.com/yancong222/pragamtics-commonsense-lms" additional="false">yancong222/pragamtics-commonsense-lms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="4">
      <title>Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks</title>
      <author><first>Yue</first><last>Wan</last></author>
      <author><first>Yueen</first><last>Ma</last></author>
      <author><first>Haoxuan</first><last>You</last></author>
      <author><first>Zhecan</first><last>Wang</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <pages>23-35</pages>
      <abstract>Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.</abstract>
      <url hash="342c8f2d">2022.csrr-1.4</url>
      <bibkey>wan-etal-2022-bridging</bibkey>
      <doi>10.18653/v1/2022.csrr-1.4</doi>
      <video href="2022.csrr-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="5">
      <title>Materialized Knowledge Bases from Commonsense Transformers</title>
      <author><first>Tuan-Phong</first><last>Nguyen</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <pages>36-42</pages>
      <abstract>Starting from the COMET methodology by Bosselut et al. (2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention. Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap, and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore, we identify common problem cases, and outline use cases enabled by materialized resources. We posit that the availability of these resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.</abstract>
      <url hash="43ca1e59">2022.csrr-1.5</url>
      <bibkey>nguyen-razniewski-2022-materialized</bibkey>
      <doi>10.18653/v1/2022.csrr-1.5</doi>
      <video href="2022.csrr-1.5.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="6">
      <title>Knowledge-Augmented Language Models for Cause-Effect Relation Classification</title>
      <author><first>Pedram</first><last>Hosseini</last></author>
      <author><first>David A.</first><last>Broniatowski</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>43-48</pages>
      <abstract>Previous studies have shown the efficacy of knowledge augmentation methods in pretrained language models. However, these methods behave differently across domains and downstream tasks. In this work, we investigate the augmentation of pretrained language models with knowledge graph data in the cause-effect relation classification and commonsense causal reasoning tasks. After automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, we continually pretrain BERT and evaluate the resulting model on cause-effect pair classification and answering commonsense causal reasoning questions. Our results show that a continually pretrained language model augmented with commonsense reasoning knowledge outperforms our baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, and a Temporal and Causal Reasoning (TCR) dataset, without additional improvement in model architecture or using quality-enhanced data for fine-tuning.</abstract>
      <url hash="2ea72cde">2022.csrr-1.6</url>
      <bibkey>hosseini-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.csrr-1.6</doi>
      <video href="2022.csrr-1.6.mp4"/>
      <pwccode url="https://github.com/phosseini/causal-reasoning" additional="false">phosseini/causal-reasoning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bcopa-ce">BCOPA-CE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glucose">GLUCOSE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>CURIE</fixed-case>: An Iterative Querying Approach for Reasoning About Situations</title>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Eduard H</first><last>Hovy</last></author>
      <pages>49-63</pages>
      <abstract>Predicting the effects of unexpected situations is an important reasoning task, e.g., would cloudy skies help or hinder plant growth? Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context. We propose CURIE, a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st graph) using natural language queries over a finetuned language model. Across multiple domains, CURIE generates st graphs that humans find relevant and meaningful in eliciting the consequences of a new situation (75% of the graphs were judged correct by humans). We present a case study of a situation reasoning end task (WIQA-QA), where simply augmenting their input with st graphs improves accuracy by 3 points. We show that these improvements mainly come from a hard subset of the data, that requires background knowledge and multi-hop reasoning.</abstract>
      <url hash="e6029afd">2022.csrr-1.7</url>
      <bibkey>rajagopal-etal-2022-curie</bibkey>
      <doi>10.18653/v1/2022.csrr-1.7</doi>
      <video href="2022.csrr-1.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/quartz">QuaRTz</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quarel">QuaRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiqa">WIQA</pwcdataset>
    </paper>
  </volume>
</collection>
