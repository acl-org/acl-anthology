<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.hackashop">
  <volume id="1" ingest-date="2021-04-19" type="proceedings">
    <meta>
      <booktitle>Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation</booktitle>
      <editor><first>Hannu</first><last>Toivonen</last></editor>
      <editor><first>Michele</first><last>Boggia</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <venue>hackashop</venue>
    </meta>
    <frontmatter>
      <url hash="30c5e14d">2021.hackashop-1.0</url>
      <bibkey>hackashop-2021-eacl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus.</title>
      <author><first>Costanza</first><last>Conforti</last></author>
      <author><first>Jakob</first><last>Berndt</last></author>
      <author><first>Marco</first><last>Basaldella</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Chryssi</first><last>Giannitsarou</last></author>
      <author><first>Flavio</first><last>Toxvaerd</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>1–7</pages>
      <abstract>Cross-target generalization constitutes an important issue for news Stance Detection (SD). In this short paper, we investigate adversarial cross-genre SD, where knowledge from annotated user-generated data is leveraged to improve news SD on targets unseen during training. We implement a BERT-based adversarial network and show experimental performance improvements over a set of strong baselines. Given the abundance of user-generated data, which are considerably less expensive to retrieve and annotate than news articles, this constitutes a promising research direction.</abstract>
      <url hash="1f6761b2">2021.hackashop-1.1</url>
      <bibkey>conforti-etal-2021-adversarial</bibkey>
    </paper>
    <paper id="2">
      <title>Related Named Entities Classification in the Economic-Financial Context</title>
      <author><first>Daniel</first><last>De Los Reyes</last></author>
      <author><first>Allan</first><last>Barcelos</last></author>
      <author><first>Renata</first><last>Vieira</last></author>
      <author><first>Isabel</first><last>Manssour</last></author>
      <pages>8–15</pages>
      <abstract>The present work uses the Bidirectional Encoder Representations from Transformers (BERT) to process a sentence and its entities and indicate whether two named entities present in a sentence are related or not, constituting a binary classification problem. It was developed for the Portuguese language, considering the financial domain and exploring deep linguistic representations to identify a relation between entities without using other lexical-semantic resources. The results of the experiments show an accuracy of 86% of the predictions.</abstract>
      <url hash="87a49d2b">2021.hackashop-1.2</url>
      <bibkey>de-los-reyes-etal-2021-related</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>BERT</fixed-case> meets Shapley: Extending <fixed-case>SHAP</fixed-case> Explanations to Transformer-based Classifiers</title>
      <author><first>Enja</first><last>Kokalj</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Nada</first><last>Lavrač</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <pages>16–21</pages>
      <abstract>Transformer-based neural networks offer very good classification performance across a wide range of domains, but do not provide explanations of their predictions. While several explanation methods, including SHAP, address the problem of interpreting deep learning models, they are not adapted to operate on state-of-the-art transformer-based neural networks such as BERT. Another shortcoming of these methods is that their visualization of explanations in the form of lists of most relevant words does not take into account the sequential and structurally dependent nature of text. This paper proposes the TransSHAP method that adapts SHAP to transformer models including BERT-based text classifiers. It advances SHAP visualizations by showing explanations in a sequential manner, assessed by human evaluators as competitive to state-of-the-art solutions.</abstract>
      <url hash="81a8ff39">2021.hackashop-1.3</url>
      <bibkey>kokalj-etal-2021-bert</bibkey>
    </paper>
    <paper id="4">
      <title>Extending Neural Keyword Extraction with <fixed-case>TF</fixed-case>-<fixed-case>IDF</fixed-case> tagset matching</title>
      <author><first>Boshko</first><last>Koloski</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <pages>22–29</pages>
      <abstract>Keyword extraction is the task of identifying words (or multi-word expressions) that best describe a given document and serve in news portals to link articles of similar topics. In this work, we develop and evaluate our methods on four novel data sets covering less-represented, morphologically-rich languages in European news media industry (Croatian, Estonian, Latvian, and Russian). First, we perform evaluation of two supervised neural transformer-based methods, Transformer-based Neural Tagger for Keyword Identification (TNT-KID) and Bidirectional Encoder Representations from Transformers (BERT) with an additional Bidirectional Long Short-Term Memory Conditional Random Fields (BiLSTM CRF) classification head, and compare them to a baseline Term Frequency - Inverse Document Frequency (TF-IDF) based unsupervised approach. Next, we show that by combining the keywords retrieved by both neural transformer-based methods and extending the final set of keywords with an unsupervised TF-IDF based technique, we can drastically improve the recall of the system, making it appropriate for usage as a recommendation system in the media house environment.</abstract>
      <url hash="b29eccff">2021.hackashop-1.4</url>
      <bibkey>koloski-etal-2021-extending</bibkey>
      <pwccode url="https://github.com/bkolosk1/extending-neural-keyword-extraction-with-tf-idf-tagset-matching" additional="false">bkolosk1/extending-neural-keyword-extraction-with-tf-idf-tagset-matching</pwccode>
    </paper>
    <paper id="5">
      <title>Zero-shot Cross-lingual Content Filtering: Offensive Language and Hate Speech Detection</title>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>30–34</pages>
      <abstract>We present a system for zero-shot cross-lingual offensive language and hate speech classification. The system was trained on English datasets and tested on a task of detecting hate speech and offensive social media content in a number of languages without any additional training. Experiments show an impressive ability of both models to generalize from English to other languages. There is however an expected gap in performance between the tested cross-lingual models and the monolingual models. The best performing model (offensive content classifier) is available online as a REST API.</abstract>
      <url hash="37116b94">2021.hackashop-1.5</url>
      <bibkey>pelicon-etal-2021-zero</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hateval">HatEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="6">
      <title>Exploring Linguistically-Lightweight Keyword Extraction Techniques for Indexing News Articles in a Multilingual Set-up</title>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Nicolas</first><last>Stefanovitch</last></author>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Aldo</first><last>Podavini</last></author>
      <pages>35–44</pages>
      <abstract>This paper presents a study of state-of-the-art unsupervised and linguistically unsophisticated keyword extraction algorithms, based on statistic-, graph-, and embedding-based approaches, including, i.a., Total Keyword Frequency, TF-IDF, RAKE, KPMiner, YAKE, KeyBERT, and variants of TextRank-based keyword extraction algorithms. The study was motivated by the need to select the most appropriate technique to extract keywords for indexing news articles in a real-world large-scale news analysis engine. The algorithms were evaluated on a corpus of circa 330 news articles in 7 languages. The overall best F1 scores for all languages on average were obtained using a combination of the recently introduced YAKE algorithm and KPMiner (20.1%, 46.6% and 47.2% for exact, partial and fuzzy matching resp.).</abstract>
      <url hash="b8e03661">2021.hackashop-1.6</url>
      <bibkey>piskorski-etal-2021-exploring</bibkey>
    </paper>
    <paper id="7">
      <title>No <fixed-case>NLP</fixed-case> Task Should be an Island: Multi-disciplinarity for Diversity in News Recommender Systems</title>
      <author><first>Myrthe</first><last>Reuver</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <pages>45–55</pages>
      <abstract>Natural Language Processing (NLP) is defined by specific, separate tasks, with each their own literature, benchmark datasets, and definitions. In this position paper, we argue that for a complex problem such as the threat to democracy by non-diverse news recommender systems, it is important to take into account a higher-order, normative goal and its implications. Experts in ethics, political science and media studies have suggested that news recommendation systems could be used to support a deliberative democracy. We reflect on the role of NLP in recommendation systems with this specific goal in mind and show that this theory of democracy helps to identify which NLP tasks and techniques can support this goal, and what work still needs to be done. This leads to recommendations for NLP researchers working on this specific problem as well as researchers working on other complex multidisciplinary problems.</abstract>
      <url hash="edaf08a6">2021.hackashop-1.7</url>
      <bibkey>reuver-etal-2021-nlp</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>T</fixed-case>e<fixed-case>M</fixed-case>o<fixed-case>T</fixed-case>opic: Temporal Mosaic Visualisation of Topic Distribution, Keywords, and Context</title>
      <author><first>Shane</first><last>Sheehan</last></author>
      <author><first>Saturnino</first><last>Luz</last></author>
      <author><first>Masood</first><last>Masoodian</last></author>
      <pages>56–61</pages>
      <abstract>In this paper we present TeMoTopic, a visualization component for temporal exploration of topics in text corpora. TeMoTopic uses the temporal mosaic metaphor to present topics as a timeline of stacked bars along with related keywords for each topic. The visualization serves as an overview of the temporal distribution of topics, along with the keyword contents of the topics, which collectively support detail-on-demand interactions with the source text of the corpora. Through these interactions and the use of keyword highlighting, the content related to each topic and its change over time can be explored.</abstract>
      <url hash="228f12ee">2021.hackashop-1.8</url>
      <bibkey>sheehan-etal-2021-temotopic</bibkey>
    </paper>
    <paper id="9">
      <title>Using contextual and cross-lingual word embeddings to improve variety in template-based <fixed-case>NLG</fixed-case> for automated journalism</title>
      <author><first>Miia</first><last>Rämö</last></author>
      <author><first>Leo</first><last>Leppänen</last></author>
      <pages>62–70</pages>
      <abstract>In this work, we describe our efforts in improving the variety of language generated from a rule-based NLG system for automated journalism. We present two approaches: one based on inserting completely new words into sentences generated from templates, and another based on replacing words with synonyms. Our initial results from a human evaluation conducted in English indicate that these approaches successfully improve the variety of the language without significantly modifying sentence meaning. We also present variations of the methods applicable to low-resource languages, simulated here using Finnish, where cross-lingual aligned embeddings are harnessed to make use of linguistic resources in a high-resource language. A human evaluation indicates that while proposed methods show potential in the low-resource case, additional work is needed to improve their performance.</abstract>
      <url hash="e30c633c">2021.hackashop-1.9</url>
      <bibkey>ramo-leppanen-2021-using</bibkey>
    </paper>
    <paper id="10">
      <title>Aligning <fixed-case>E</fixed-case>stonian and <fixed-case>R</fixed-case>ussian news industry keywords with the help of subtitle translations and an environmental thesaurus</title>
      <author><first>Andraž</first><last>Repar</last></author>
      <author><first>Andrej</first><last>Shumakov</last></author>
      <pages>71–75</pages>
      <abstract>This paper presents the implementation of a bilingual term alignment approach developed by Repar et al. (2019) to a dataset of unaligned Estonian and Russian keywords which were manually assigned by journalists to describe the article topic. We started by separating the dataset into Estonian and Russian tags based on whether they are written in the Latin or Cyrillic script. Then we selected the available language-specific resources necessary for the alignment system to work. Despite the domains of the language-specific resources (subtitles and environment) not matching the domain of the dataset (news articles), we were able to achieve respectable results with manual evaluation indicating that almost 3/4 of the aligned keyword pairs are at least partial matches.</abstract>
      <url hash="c67461bb">2021.hackashop-1.10</url>
      <bibkey>repar-shumakov-2021-aligning</bibkey>
    </paper>
    <paper id="11">
      <title>Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces</title>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Shane</first><last>Sheehan</last></author>
      <author><first>Nika</first><last>Eržen</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <author><first>Saturnino</first><last>Luz</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>76–83</pages>
      <abstract>Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.</abstract>
      <url hash="8d9d6b36">2021.hackashop-1.11</url>
      <bibkey>skrlj-etal-2021-exploring</bibkey>
      <pwccode url="https://github.com/SkBlaz/attviz" additional="false">SkBlaz/attviz</pwccode>
    </paper>
    <paper id="12">
      <title>Comment Section Personalization: Algorithmic, Interface, and Interaction Design</title>
      <author><first>Yixue</first><last>Wang</last></author>
      <pages>84–88</pages>
      <abstract>Comment sections allow users to share their personal experiences, discuss and form different opinions, and build communities out of organic conversations. However, many comment sections present chronological ranking to all users. In this paper, I discuss personalization approaches in comment sections based on different objectives for newsrooms and researchers to consider. I propose algorithmic and interface designs when personalizing the presentation of comments based on different objectives including relevance, diversity, and education/background information. I further explain how transparency, user control, and comment type diversity could help users most benefit from the personalized interacting experience.</abstract>
      <url hash="fb016a8e">2021.hackashop-1.12</url>
      <bibkey>wang-2021-comment</bibkey>
    </paper>
    <paper id="13">
      <title>Unsupervised Approach to Multilingual User Comments Summarization</title>
      <author><first>Aleš</first><last>Žagar</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <pages>89–98</pages>
      <abstract>User commenting is a valuable feature of many news outlets, enabling them a contact with readers and enabling readers to express their opinion, provide different viewpoints, and even complementary information. Yet, large volumes of user comments are hard to filter, let alone read and extract relevant information. The research on the summarization of user comments is still in its infancy, and human-created summarization datasets are scarce, especially for less-resourced languages. To address this issue, we propose an unsupervised approach to user comments summarization, which uses a modern multilingual representation of sentences together with standard extractive summarization techniques. Our comparison of different sentence representation approaches coupled with different summarization approaches shows that the most successful combinations are the same in news and comment summarization. The empirical results and presented visualisation show usefulness of the proposed methodology for several languages.</abstract>
      <url hash="fb1253d7">2021.hackashop-1.13</url>
      <bibkey>zagar-robnik-sikonja-2021-unsupervised</bibkey>
      <pwccode url="https://github.com/azagsam/xl-user-comments" additional="false">azagsam/xl-user-comments</pwccode>
    </paper>
    <paper id="14">
      <title><fixed-case>EMBEDDIA</fixed-case> Tools, Datasets and Challenges: Resources and Hackathon Contributions</title>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Marko</first><last>Robnik-Šikonja</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <author><first>Michele</first><last>Boggia</last></author>
      <author><first>Ravi</first><last>Shekhar</last></author>
      <author><first>Marko</first><last>Pranjić</last></author>
      <author><first>Salla</first><last>Salmela</last></author>
      <author><first>Ivar</first><last>Krustok</last></author>
      <author><first>Tarmo</first><last>Paju</last></author>
      <author><first>Carl-Gustav</first><last>Linden</last></author>
      <author><first>Leo</first><last>Leppänen</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Matej</first><last>Ulčar</last></author>
      <author><first>Linda</first><last>Freienthal</last></author>
      <author><first>Silver</first><last>Traat</last></author>
      <author><first>Luis Adrián</first><last>Cabrera-Diego</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Nada</first><last>Lavrač</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Martin</first><last>Žnidaršič</last></author>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Boshko</first><last>Koloski</last></author>
      <author><first>Vid</first><last>Podpečan</last></author>
      <author><first>Janez</first><last>Kranjc</last></author>
      <author><first>Shane</first><last>Sheehan</last></author>
      <author><first>Emanuela</first><last>Boros</last></author>
      <author><first>Jose G.</first><last>Moreno</last></author>
      <author><first>Antoine</first><last>Doucet</last></author>
      <author><first>Hannu</first><last>Toivonen</last></author>
      <pages>99–109</pages>
      <abstract>This paper presents tools and data sources collected and released by the EMBEDDIA project, supported by the European Union’s Horizon 2020 research and innovation program. The collected resources were offered to participants of a hackathon organized as part of the EACL Hackashop on News Media Content Analysis and Automated Report Generation in February 2021. The hackathon had six participating teams who addressed different challenges, either from the list of proposed challenges or their own news-industry-related tasks. This paper goes beyond the scope of the hackathon, as it brings together in a coherent and compact form most of the resources developed, collected and released by the EMBEDDIA project. Moreover, it constitutes a handy source for news media industry and researchers in the fields of Natural Language Processing and Social Science.</abstract>
      <url hash="956a7f13">2021.hackashop-1.14</url>
      <bibkey>pollak-etal-2021-embeddia</bibkey>
    </paper>
    <paper id="15">
      <title>A <fixed-case>COVID</fixed-case>-19 news coverage mood map of <fixed-case>E</fixed-case>urope</title>
      <author><first>Frankie</first><last>Robertson</last></author>
      <author><first>Jarkko</first><last>Lagus</last></author>
      <author><first>Kaisla</first><last>Kajava</last></author>
      <pages>110–115</pages>
      <abstract>We present a COVID-19 news dashboard which visualizes sentiment in pandemic news coverage in different languages across Europe. The dashboard shows analyses for positive/neutral/negative sentiment and moral sentiment for news articles across countries and languages. First we extract news articles from news-crawl. Then we use a pre-trained multilingual BERT model for sentiment analysis of news article headlines and a dictionary and word vectors -based method for moral sentiment analysis of news articles. The resulting dashboard gives a unified overview of news events on COVID-19 news overall sentiment, and the region and language of publication from the period starting from the beginning of January 2020 to the end of January 2021.</abstract>
      <url hash="a33c2c51">2021.hackashop-1.15</url>
      <bibkey>robertson-etal-2021-covid</bibkey>
    </paper>
    <paper id="16">
      <title>Interesting cross-border news discovery using cross-lingual article linking and document similarity</title>
      <author><first>Boshko</first><last>Koloski</last></author>
      <author><first>Elaine</first><last>Zosa</last></author>
      <author><first>Timen</first><last>Stepišnik-Perdih</last></author>
      <author><first>Blaž</first><last>Škrlj</last></author>
      <author><first>Tarmo</first><last>Paju</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>116–120</pages>
      <abstract>Team Name: team-8 Embeddia Tool: Cross-Lingual Document Retrieval Zosa et al. Dataset: Estonian and Latvian news datasets abstract: Contemporary news media face increasing amounts of available data that can be of use when prioritizing, selecting and discovering new news. In this work we propose a methodology for retrieving interesting articles in a cross-border news discovery setting. More specifically, we explore how a set of seed documents in Estonian can be projected in Latvian document space and serve as a basis for discovery of novel interesting pieces of Latvian news that would interest Estonian readers. The proposed methodology was evaluated by Estonian journalist who confirmed that in the best setting, from top 10 retrieved Latvian documents, half of them represent news that are potentially interesting to be taken by the Estonian media house and presented to Estonian readers.</abstract>
      <url hash="275a0935">2021.hackashop-1.16</url>
      <bibkey>koloski-etal-2021-interesting</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>EMBEDDIA</fixed-case> hackathon report: Automatic sentiment and viewpoint analysis of <fixed-case>S</fixed-case>lovenian news corpus on the topic of <fixed-case>LGBTIQ</fixed-case>+</title>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Nina</first><last>Perger</last></author>
      <author><first>Andraž</first><last>Pelicon</last></author>
      <author><first>Matej</first><last>Ulčar</last></author>
      <author><first>Andreja</first><last>Vezovnik</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>121–126</pages>
      <abstract>We conduct automatic sentiment and viewpoint analysis of the newly created Slovenian news corpus containing articles related to the topic of LGBTIQ+ by employing the state-of-the-art news sentiment classifier and a system for semantic change detection. The focus is on the differences in reporting between quality news media with long tradition and news media with financial and political connections to SDS, a Slovene right-wing political party. The results suggest that political affiliation of the media can affect the sentiment distribution of articles and the framing of specific LGBTIQ+ specific topics, such as same-sex marriage.</abstract>
      <url hash="7e9fdd46">2021.hackashop-1.17</url>
      <bibkey>martinc-etal-2021-embeddia</bibkey>
    </paper>
    <paper id="18">
      <title>To Block or not to Block: Experiments with Machine Learning for News Comment Moderation</title>
      <author><first>Damir</first><last>Korencic</last></author>
      <author><first>Ipek</first><last>Baris</last></author>
      <author><first>Eugenia</first><last>Fernandez</last></author>
      <author><first>Katarina</first><last>Leuschel</last></author>
      <author><first>Eva</first><last>Sánchez Salido</last></author>
      <pages>127–133</pages>
      <abstract>Today, news media organizations regularly engage with readers by enabling them to comment on news articles. This creates the need for comment moderation and removal of disallowed comments – a time-consuming task often performed by human moderators. In this paper we approach the problem of automatic news comment moderation as classification of comments into blocked and not blocked categories. We construct a novel dataset of annotated English comments, experiment with cross-lingual transfer of comment labels and evaluate several machine learning models on datasets of Croatian and Estonian news comments. Team name: SuperAdmin; Challenge: Detection of blocked comments; Tools/models: CroSloEn BERT, FinEst BERT, 24Sata comment dataset, Ekspress comment dataset.</abstract>
      <url hash="47254c32">2021.hackashop-1.18</url>
      <bibkey>korencic-etal-2021-block</bibkey>
    </paper>
    <paper id="19">
      <title>Implementing Evaluation Metrics Based on Theories of Democracy in News Comment Recommendation (Hackathon Report)</title>
      <author><first>Myrthe</first><last>Reuver</last></author>
      <author><first>Nicolas</first><last>Mattis</last></author>
      <pages>134–139</pages>
      <abstract>Diversity in news recommendation is important for democratic debate. Current recommendation strategies, as well as evaluation metrics for recommender systems, do not explicitly focus on this aspect of news recommendation. In the 2021 Embeddia Hackathon, we implemented one novel, normative theory-based evaluation metric, “activation”, and use it to compare two recommendation strategies of New York Times comments, one based on user likes and another on editor picks. We found that both comment recommendation strategies lead to recommendations consistently less activating than the available comments in the pool of data, but the editor’s picks more so. This might indicate that New York Times editors’ support a deliberative democratic model, in which less activation is deemed ideal for democratic debate.</abstract>
      <url hash="5f297b86">2021.hackashop-1.19</url>
      <bibkey>reuver-mattis-2021-implementing</bibkey>
    </paper>
  </volume>
</collection>
