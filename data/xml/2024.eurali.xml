<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.eurali">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Resources and Technologies for Indigenous, Endangered and Lesser-resourced Languages in Eurasia (EURALI) @ LREC-COLING 2024</booktitle>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Sina</first><last>Ahmadi</last></editor>
      <editor><first>Silvie</first><last>Cinková</last></editor>
      <editor><first>Theodorus</first><last>Fransen</last></editor>
      <editor><first>Chao-Hong</first><last>Liu</last></editor>
      <editor><first>John P.</first><last>McCrae</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="15d51877">2024.eurali-1</url>
      <venue>eurali</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="486e9c6b">2024.eurali-1.0</url>
      <bibkey>eurali-2024-resources</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Low-Resource Machine Translation through Retrieval-Augmented <fixed-case>LLM</fixed-case> Prompting: A Study on the <fixed-case>M</fixed-case>ambai Language</title>
      <author><first>Raphaël</first><last>Merx</last></author>
      <author><first>Aso</first><last>Mahmudi</last></author>
      <author><first>Katrina</first><last>Langford</last></author>
      <author><first>Leo Alberto</first><last>de Araujo</last></author>
      <author><first>Ekaterina</first><last>Vylomova</last></author>
      <pages>1–11</pages>
      <abstract>This study explores the use of large language models (LLMs) for translating English into Mambai, a low-resource Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of few-shot LLM prompting for machine translation (MT) in this low-resource context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy, using open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with BLEU scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing MT for low-resource languages. Our research provides insights into few-shot LLM prompting for low-resource MT, and makes available an initial corpus for the Mambai language.</abstract>
      <url hash="d5d13d3f">2024.eurali-1.1</url>
      <bibkey>merx-etal-2024-low</bibkey>
    </paper>
    <paper id="2">
      <title>Improved Neural Word Segmentation for Standard <fixed-case>T</fixed-case>ibetan</title>
      <author><first>Collin J.</first><last>Brown</last></author>
      <pages>12–17</pages>
      <abstract>As Tibetan is traditionally not written with word delimiters, various means of word segmentation are necessary to prepare data for downstream tasks. Neural word segmentation has proven a successful means of parsing Tibetan text, but current performance lags behind that of neural word segmenters in other languages, such as Chinese or Japanese, and even behind languages with relatively similar orthographic structures, such as Vietnamese or Thai. We apply methods that have proven useful for these latter two languages , in addition to Classical Tibetan, toward the development of a neural word segmenter with the goal of raising the peak performance of Tibetan neural word segmentation to a level comparable to that reached for orthographically similar languages.</abstract>
      <url hash="a0a9210b">2024.eurali-1.2</url>
      <bibkey>brown-2024-improved</bibkey>
    </paper>
    <paper id="3">
      <title>Open Text Collections as a Resource for Doing <fixed-case>NLP</fixed-case> with Eurasian Languages</title>
      <author><first>Sebastian</first><last>Nordhoff</last></author>
      <author><first>Christian</first><last>Döhler</last></author>
      <author><first>Mandana</first><last>Seyfeddinipur</last></author>
      <pages>18–23</pages>
      <abstract>The Open Text Collections project establishes a high-quality publication channel for interlinear glossed text from endangered languages. Text collection will by made available in an open interoperable format and as a more traditional book publication. The project addresses a variety of audiences, eg. community members, typological linguists, anthropologists, NLP practitioners.</abstract>
      <url hash="ddaa29f5">2024.eurali-1.3</url>
      <bibkey>nordhoff-etal-2024-open</bibkey>
    </paper>
    <paper id="4">
      <title>The Extraction and Fine-grained Classification of Written <fixed-case>C</fixed-case>antonese Materials through Linguistic Feature Detection</title>
      <author><first>Chaak-ming</first><last>Lau</last></author>
      <author><first>Mingfei</first><last>Lau</last></author>
      <author><first>Ann Wai Huen</first><last>To</last></author>
      <pages>24–29</pages>
      <abstract>This paper presents a linguistically-informed, non-machine-learning tool for classifying Written Cantonese, Standard Written Chinese, and the intermediate varieties used by Cantonese-speaking users from Hong Kong, which are often grouped into a single “Traditional Chinese” label. Our approach addresses the lack of textual materials for Cantonese NLP, a consequence of a lower sociolinguistic status of Written Cantonese and the interchangeable use of these varieties by users without sufficient language labeling. The tool utilizes key strings and quotation markers, which can be reduced to string operations, to effectively extract Written Cantonese sentences and documents from materials mixed with Standard Written Chinese. This allows for the flexible and efficient extraction of high-quality Cantonese data from large datasets, catering to specific classification needs. This implementation ensures that the tool can process large amounts of data at a low cost by bypassing model-inferencing, which is particularly significant for marginalized languages. The tool also aims to provide a baseline measure for future classification systems, and the approach may be applicable to other low-resource regional or diglossic languages.</abstract>
      <url hash="64fcb370">2024.eurali-1.4</url>
      <bibkey>lau-etal-2024-extraction</bibkey>
    </paper>
    <paper id="5">
      <title>Neural Mining of <fixed-case>P</fixed-case>ersian Short Argumentative Texts</title>
      <author><first>Mohammad</first><last>Yeghaneh Abkenar</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>30–35</pages>
      <abstract>Argumentation mining (AM) is concerned with extracting arguments from texts and classifying the elements (e.g.,claim and premise) and relations between them, as well as creating an argumentative structure. A significant hurdle to research in this area for the Persian language is the lack of annotated Persian language corpora. This paper introduces the first argument-annotated corpus in Persian and thereby the possibility of expanding argumentation mining to this low-resource language. The starting point is the English argumentative microtext corpus (AMT) (Peldszus and Stede, 2015), and we built the Persian variant by machine translation (MT) and careful post-editing of the output. We call this corpus Persian argumentative microtext (PAMT). Moreover, we present the first results for Argumentative Discourse Unit (ADU) classification for Persian, which is considered to be one of the main fundamental subtasks of argumentation mining. We adopted span categorization using the deep learning model of spaCy Version 3.0 (a CNN model on top of Bloom embedding with attention) on the corpus for determing argumentative units and their type (claim vs. premise).</abstract>
      <url hash="e54512d9">2024.eurali-1.5</url>
      <bibkey>yeghaneh-abkenar-stede-2024-neural</bibkey>
    </paper>
    <paper id="6">
      <title>Endangered Language Preservation: A Model for Automatic Speech Recognition Based on Khroskyabs Data</title>
      <author><first>Ruiyao</first><last>Li</last></author>
      <author><first>Yunfan</first><last>Lai</last></author>
      <pages>36–40</pages>
      <abstract>This is a report on an Automatic Speech Recognition (ASR) experiment conducted using the Khroskyabs data. With the impact of information technology development and globalization challenges on linguistic diversity, this study focuses on the preservation crisis of the endangered Gyalrongic language, particularly the Khroskyabs language. We used Automatic Speech Recognition technology and the Wav2Vec2 model to transcribe the Khroskyabs language. Despite challenges such as data scarcity and the language’s complex morphology, preliminary results show promising character accuracy from the model. Additionally, the linguist also has given relatively high evaluations to the transcription results of our model. Therefore, the experimental and evaluation results demonstrate the high practicality of our model. At the same time, the results also reveal issues with high word error rates, so we plan to augment our existing dataset with additional Khroskyabs data in our further studies. This study provides insights and methodologies for using Automatic Speech Recognition to transcribe and protect Khroskyabs, and we hope that this can contribute to the preservation efforts of other endangered languages.</abstract>
      <url hash="2889d54d">2024.eurali-1.6</url>
      <bibkey>li-lai-2024-endangered</bibkey>
    </paper>
    <paper id="7">
      <title>This Word Mean What: Constructing a <fixed-case>S</fixed-case>inglish Dictionary with <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>Siew Yeng</first><last>Chow</last></author>
      <author><first>Chang-Uk</first><last>Shin</last></author>
      <author><first>Francis</first><last>Bond</last></author>
      <pages>41–50</pages>
      <abstract>Despite the magnitude of recent progress in natural language processing and multilingual language modeling research, the vast majority of NLP research is focused on English and other major languages. This is because recent NLP research is mainly data-driven, and there is more data for resource-rich languages. In particular, Large Language Models (LLM) make use of large unlabeled datasets, a resource that many languages do not have. In this project, we built a new, open-sourced dictionary of Singlish, a contact variety that contains features from English and other local languages and is syntactically, phonologically and lexically distinct from Standard English (Tan, 2010). First, a list of Singlish words was extracted from various online sources. Then using an open Chat-GPT LLM API, the description, including the defintion, part of speech, pronunciation and examples was produced. These were then refined through post processing carried out by a native speaker. The dictionary currently has 1,783 entries and is published under the CC-BY-SA license. The project was carried out with the intention of facilitating future Singlish research and other applications as the accumulation and management of language resources will be of great help in promoting research on the language in the future.</abstract>
      <url hash="8dc68020">2024.eurali-1.7</url>
      <bibkey>chow-etal-2024-word</bibkey>
    </paper>
    <paper id="8">
      <title>An Evaluation of Language Models for Hyperpartisan Ideology Detection in <fixed-case>P</fixed-case>ersian <fixed-case>T</fixed-case>witter</title>
      <author><first>Sahar</first><last>Omidi Shayegan</last></author>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <author><first>Hao</first><last>Yu</last></author>
      <author><first>Sacha</first><last>Levy</last></author>
      <author><first>Zachary</first><last>Yang</last></author>
      <author><first>Jean-François</first><last>Godbout</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last></author>
      <pages>51–62</pages>
      <abstract>Large Language Models (LLMs) have shown significant promise in various tasks, including identifying the political beliefs of English-speaking social media users from their posts. However, assessing LLMs for this task in non-English languages remains unexplored. In this work, we ask to what extent LLMs can predict the political ideologies of users in Persian social media. To answer this question, we first acknowledge that political parties are not well-defined among Persian users, and therefore, we simplify the task to a much simpler task of hyperpartisan ideology detection. We create a new benchmark and show the potential and limitations of both open-source and commercial LLMs in classifying the hyper-partisan ideologies of users. We compare these models with smaller fine-tuned models, both on the Persian language (ParsBERT) and translated data (RoBERTa), showing that they considerably outperform generative LLMs in this task. We further demonstrate that the performance of the generative LLMs degrades when classifying users based on their tweets instead of their bios and even when tweets are added as additional information, whereas the smaller fine-tuned models are robust and achieve similar performance for all classes. This study is a first step toward political ideology detection in Persian Twitter, with implications for future research to understand the dynamics of ideologies in Persian social media.</abstract>
      <url hash="bb58bfce">2024.eurali-1.8</url>
      <bibkey>omidi-shayegan-etal-2024-evaluation</bibkey>
    </paper>
  </volume>
</collection>
