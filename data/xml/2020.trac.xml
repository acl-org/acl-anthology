<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.trac">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying</booktitle>
      <editor><first>Ritesh</first><last>Kumar</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <editor><first>Bornini</first><last>Lahiri</last></editor>
      <editor><first>Marcos</first><last>Zampieri</last></editor>
      <editor><first>Shervin</first><last>Malmasi</last></editor>
      <editor><first>Vanessa</first><last>Murdock</last></editor>
      <editor><first>Daniel</first><last>Kadar</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-56-6</isbn>
    </meta>
    <frontmatter>
      <url hash="2c81684d">2020.trac-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Evaluating Aggression Identification in Social Media</title>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–5</pages>
      <abstract>In this paper, we present the report and findings of the Shared Task on Aggression and Gendered Aggression Identification organised as part of the Second Workshop on Trolling, Aggression and Cyberbullying (TRAC - 2) at LREC 2020. The task consisted of two sub-tasks - aggression identification (sub-task A) and gendered identification (sub-task B) - in three languages - Bangla, Hindi and English. For this task, the participants were provided with a dataset of approximately 5,000 instances from YouTube comments in each language. For testing, approximately 1,000 instances were provided in each language for each sub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages.</abstract>
      <url hash="2d83c7dd">2020.trac-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title><fixed-case>TOCP</fixed-case>: A Dataset for <fixed-case>C</fixed-case>hinese Profanity Processing</title>
      <author><first>Hsu</first><last>Yang</last></author>
      <author><first>Chuan-Jie</first><last>Lin</last></author>
      <pages>6–12</pages>
      <abstract>This paper introduced TOCP, a larger dataset of Chinese profanity. This dataset contains natural sentences collected from social media sites, the profane expressions appearing in the sentences, and their rephrasing suggestions which preserve their meanings in a less offensive way. We proposed several baseline systems using neural network models to test this benchmark. We trained embedding models on a profanity-related dataset and proposed several profanity-related features. Our baseline systems achieved an F1-score of 86.37% in profanity detection and an accuracy of 77.32% in profanity rephrasing.</abstract>
      <url hash="86ca8a3a">2020.trac-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>A Multi-Dimensional View of Aggression when voicing Opinion</title>
      <author><first>Arjit</first><last>Srivastava</last></author>
      <author><first>Avijit</first><last>Vajpayee</last></author>
      <author><first>Syed Sarfaraz</first><last>Akhtar</last></author>
      <author><first>Naman</first><last>Jain</last></author>
      <author><first>Vinay</first><last>Singh</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>13–20</pages>
      <abstract>The advent of social media has immensely proliferated the amount of opinions and arguments voiced on the internet. These virtual debates often present cases of aggression. While research has been focused largely on analyzing aggression and stance in isolation from each other, this work is the first attempt to gain an extensive and fine-grained understanding of patterns of aggression and figurative language use when voicing opinion. We present a Hindi-English code-mixed dataset of opinion on the politico-social issue of ‘2016 India banknote demonetisation‘ and annotate it across multiple dimensions such as aggression, hate speech, emotion arousal and figurative language usage (such as sarcasm/irony, metaphors/similes, puns/word-play).</abstract>
      <url hash="a4555ee1">2020.trac-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Towards Non-Toxic Landscapes: Automatic Toxic Comment Detection Using <fixed-case>DNN</fixed-case></title>
      <author><first>Ashwin Geet</first><last>D’Sa</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>21–25</pages>
      <abstract>The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the umbrella of “toxic speech”. The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN based classifiers. Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using BERT fine-tuning outperforms feature-based BERT, Mikolov’s and fastText representations with different DNN classifiers.</abstract>
      <url hash="40838878">2020.trac-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Aggression Identification in Social Media: a Transfer Learning Based Approach</title>
      <author><first>Faneva</first><last>Ramiandrisoa</last></author>
      <author><first>Josiane</first><last>Mothe</last></author>
      <pages>26–31</pages>
      <abstract>The way people communicate have changed in many ways with the outbreak of social media. One of the aspects of social media is the ability for their information producers to hide, fully or partially, their identity during a discussion; leading to cyber-aggression and interpersonal aggression. Automatically monitoring user-generated content in order to help moderating it is thus a very hot topic. In this paper, we propose to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) (Devlin et al., 2019) to identify aggressive content. Our model is also used to predict the level of aggressiveness. The evaluation part of this paper is based on the dataset provided by the TRAC shared task (Kumar et al., 2018a). When compared to the other participants of this shared task, our model achieved the third best performance according to the weighted F1 measure on both Facebook and Twitter collections.</abstract>
      <url hash="d5ce4c07">2020.trac-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Multimodal Meme Dataset (<fixed-case>M</fixed-case>ulti<fixed-case>OFF</fixed-case>) for Identifying Offensive Content in Image and Text</title>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>32–41</pages>
      <abstract>A meme is a form of media that spreads an idea or emotion across the internet. As posting meme has become a new form of communication of the web, due to the multimodal nature of memes, postings of hateful memes or related events like trolling, cyberbullying are increasing day by day. Hate speech, offensive content and aggression content detection have been extensively explored in a single modality such as text or image. However, combining two modalities to detect offensive content is still a developing area. Memes make it even more challenging since they express humour and sarcasm in an implicit way, because of which the meme may not be offensive if we only consider the text or the image. Therefore, it is necessary to combine both modalities to identify whether a given meme is offensive or not. Since there was no publicly available dataset for multimodal offensive meme content detection, we leveraged the memes related to the 2016 U.S. presidential election and created the MultiOFF multimodal meme dataset for offensive content detection dataset. We subsequently developed a classifier for this task using the MultiOFF dataset. We use an early fusion technique to combine the image and text modality and compare it with a text- and an image-only baseline to investigate its effectiveness. Our results show improvements in terms of Precision, Recall, and F-Score. The code and dataset for this paper is published in <i>
          <url>https://github.com/bharathichezhiyan/Multimodal-Meme-Classification-Identifying-Offensive-Content-in-Image-and-Text</url>
        </i>
      </abstract>
      <url hash="666b05ac">2020.trac-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>A Comparative Study of Different State-of-the-Art Hate Speech Detection Methods in <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Mixed Data</title>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Koustava</first><last>Goswami</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <pages>42–48</pages>
      <abstract>Hate speech detection in social media communication has become one of the primary concerns to avoid conflicts and curb undesired activities. In an environment where multilingual speakers switch among multiple languages, hate speech detection becomes a challenging task using methods that are designed for monolingual corpora. In our work, we attempt to analyze, detect and provide a comparative study of hate speech in a code-mixed social media text. We also provide a Hindi-English code-mixed data set consisting of Facebook and Twitter posts and comments. Our experiments show that deep learning models trained on this code-mixed corpus perform better.</abstract>
      <url hash="320bed95">2020.trac-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title><fixed-case>IRIT</fixed-case> at <fixed-case>TRAC</fixed-case> 2020</title>
      <author><first>Faneva</first><last>Ramiandrisoa</last></author>
      <author><first>Josiane</first><last>Mothe</last></author>
      <pages>49–54</pages>
      <abstract>This paper describes the participation of the IRIT team in the TRAC (Trolling, Aggression and Cyberbullying) 2020 shared task (Bhattacharya et al., 2020) on Aggression Identification and more precisely to the shared task in English language. The shared task was further divided into two sub-tasks: (a) aggression identification and (b) misogynistic aggression identification. We proposed to use the transformer based language model BERT (Bidirectional Encoder Representation from Transformer) for the two sub-tasks. Our team was qualified as twelfth out of sixteen participants on sub-task (a) and eleventh out of fifteen participants on sub-task (b).</abstract>
      <url hash="c8922873">2020.trac-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title>Bagging <fixed-case>BERT</fixed-case> Models for Robust Aggression Identification</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>55–61</pages>
      <abstract>Modern transformer-based models with hundreds of millions of parameters, such as BERT, achieve impressive results at text classification tasks. This also holds for aggression identification and offensive language detection, where deep learning approaches consistently outperform less complex models, such as decision trees. While the complex models fit training data well (low bias), they also come with an unwanted high variance. Especially when fine-tuning them on small datasets, the classification performance varies significantly for slightly different training data. To overcome the high variance and provide more robust predictions, we propose an ensemble of multiple fine-tuned BERT models based on bootstrap aggregating (bagging). In this paper, we describe such an ensemble system and present our submission to the shared tasks on aggression identification 2020 (team name: Julian). Our submission is the best-performing system for five out of six subtasks. For example, we achieve a weighted F1-score of 80.3% for task A on the test dataset of English social media posts. In our experiments, we compare different model configurations and vary the number of models used in the ensemble. We find that the F1-score drastically increases when ensembling up to 15 models, but the returns diminish for more models.</abstract>
      <url hash="1d277012">2020.trac-1.9</url>
      <language>eng</language>
    </paper>
    <paper id="10">
      <title>Scmhl5 at <fixed-case>TRAC</fixed-case>-2 Shared Task on Aggression Identification: Bert Based Ensemble Learning Approach</title>
      <author><first>Han</first><last>Liu</last></author>
      <author><first>Pete</first><last>Burnap</last></author>
      <author><first>Wafa</first><last>Alorainy</last></author>
      <author><first>Matthew</first><last>Williams</last></author>
      <pages>62–68</pages>
      <abstract>This paper presents a system developed during our participation (team name: scmhl5) in the TRAC-2 Shared Task on aggression identification. In particular, we participated in English Sub-task A on three-class classification (‘Overtly Aggressive’, ‘Covertly Aggressive’ and ‘Non-aggressive’) and English Sub-task B on binary classification for Misogynistic Aggression (‘gendered’ or ‘non-gendered’). For both sub-tasks, our method involves using the pre-trained Bert model for extracting the text of each instance into a 768-dimensional vector of embeddings, and then training an ensemble of classifiers on the embedding features. Our method obtained accuracy of 0.703 and weighted F-measure of 0.664 for Sub-task A, whereas for Sub-task B the accuracy was 0.869 and weighted F-measure was 0.851. In terms of the rankings, the weighted F-measure obtained using our method for Sub-task A is ranked in the 10th out of 16 teams, whereas for Sub-task B the weighted F-measure is ranked in the 8th out of 15 teams.</abstract>
      <url hash="0b03f293">2020.trac-1.10</url>
      <language>eng</language>
    </paper>
    <paper id="11">
      <title>The Role of Computational Stylometry in Identifying (Misogynistic) Aggression in <fixed-case>E</fixed-case>nglish Social Media Texts</title>
      <author><first>Antonio</first><last>Pascucci</last></author>
      <author><first>Raffaele</first><last>Manna</last></author>
      <author><first>Vincenzo</first><last>Masucci</last></author>
      <author><first>Johanna</first><last>Monti</last></author>
      <pages>69–75</pages>
      <abstract>In this paper, we describe UniOr_ExpSys team participation in TRAC-2 (Trolling, Aggression and Cyberbullying) shared task, a workshop organized as part of LREC 2020. TRAC-2 shared task is organized in two sub-tasks: Aggression Identification (a 3-way classification between “Overtly Aggressive”, “Covertly Aggressive” and “Non-aggressive” text data) and Misogynistic Aggression Identification (a binary classifier for classifying the texts as “gendered” or “non-gendered”). Our approach is based on linguistic rules, stylistic features extraction through stylometric analysis and Sequential Minimal Optimization algorithm in building the two classifiers.</abstract>
      <url hash="89cffcb8">2020.trac-1.11</url>
      <language>eng</language>
    </paper>
    <paper id="12">
      <title>Aggression Identification in <fixed-case>E</fixed-case>nglish, <fixed-case>H</fixed-case>indi and <fixed-case>B</fixed-case>angla Text using <fixed-case>BERT</fixed-case>, <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a and <fixed-case>SVM</fixed-case></title>
      <author><first>Arup</first><last>Baruah</last></author>
      <author><first>Kaushik</first><last>Das</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>76–82</pages>
      <abstract>This paper presents the results of the classifiers we developed for the shared tasks in aggression identification and misogynistic aggression identification. These two shared tasks were held as part of the second workshop on Trolling, Aggression and Cyberbullying (TRAC). Both the subtasks were held for English, Hindi and Bangla language. In our study, we used English BERT (En-BERT), RoBERTa, DistilRoBERTa, and SVM based classifiers for English language. For Hindi and Bangla language, multilingual BERT (M-BERT), XLM-RoBERTa and SVM classifiers were used. Our best performing models are EN-BERT for English Subtask A (Weighted F1 score of 0.73, Rank 5/16), SVM for English Subtask B (Weighted F1 score of 0.87, Rank 2/15), SVM for Hindi Subtask A (Weighted F1 score of 0.79, Rank 2/10), XLMRoBERTa for Hindi Subtask B (Weighted F1 score of 0.87, Rank 2/10), SVM for Bangla Subtask A (Weighted F1 score of 0.81, Rank 2/10), and SVM for Bangla Subtask B (Weighted F1 score of 0.93, Rank 4/8). It is seen that the superior performance of the SVM classifier was achieved mainly because of its better prediction of the majority class. BERT based classifiers were found to predict the minority classes better.</abstract>
      <url hash="75fb6ceb">2020.trac-1.12</url>
      <language>eng</language>
    </paper>
    <paper id="13">
      <title><fixed-case>L</fixed-case>a<fixed-case>STUS</fixed-case>/<fixed-case>TALN</fixed-case> at <fixed-case>TRAC</fixed-case> - 2020 Trolling, Aggression and Cyberbullying</title>
      <author><first>Lütfiye Seda</first><last>Mut Altın</last></author>
      <author><first>Alex</first><last>Bravo</last></author>
      <author><first>Horacio</first><last>Saggion</last></author>
      <pages>83–86</pages>
      <abstract>This paper presents the participation of the LaSTUS/TALN team at TRAC-2020 Trolling, Aggression and Cyberbullying shared task. The aim of the task is to determine whether a given text is aggressive and contains misogynistic content. Our approach is based on a bidirectional Long Short Term Memory network (bi-LSTM). Our system performed well at sub-task A, aggression detection; however underachieved at sub-task B, misogyny detection.</abstract>
      <url hash="8fb4f657">2020.trac-1.13</url>
      <language>eng</language>
    </paper>
    <paper id="14">
      <title><fixed-case>S</fixed-case>pyder: Aggression Detection on Multilingual Tweets</title>
      <author><first>Anisha</first><last>Datta</last></author>
      <author><first>Shukrity</first><last>Si</last></author>
      <author><first>Urbi</first><last>Chakraborty</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <pages>87–92</pages>
      <abstract>In the last few years, hate speech and aggressive comments have covered almost all the social media platforms like facebook, twitter etc. As a result hatred is increasing. This paper describes our (<b>Team name:</b>
        <b>Spyder</b>) participation in the Shared Task on Aggression Detection organised by TRAC-2, Second Workshop on Trolling, Aggression and Cyberbullying. The Organizers provided datasets in three languages – English, Hindi and Bengali. The task was to classify each instance of the test sets into three categories – “Overtly Aggressive” (OAG), “Covertly Aggressive” (CAG) and “Non-Aggressive” (NAG). In this paper, we propose three different models using Tf-Idf, sentiment polarity and machine learning based classifiers. We obtained f1 score of 43.10%, 59.45% and 44.84% respectively for English, Hindi and Bengali.</abstract>
      <url hash="f0f94f08">2020.trac-1.14</url>
      <language>eng</language>
    </paper>
    <paper id="15">
      <title><fixed-case>BERT</fixed-case> of all trades, master of some</title>
      <author><first>Denis</first><last>Gordeev</last></author>
      <author><first>Olga</first><last>Lykova</last></author>
      <pages>93–98</pages>
      <abstract>This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages (Bengali, English and Hindi) where the participants’ task was to classify aggression in short texts from social media and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model placed first in English and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.</abstract>
      <url hash="b26cb56f">2020.trac-1.15</url>
      <language>eng</language>
    </paper>
    <paper id="16">
      <title><fixed-case>SAJA</fixed-case> at <fixed-case>TRAC</fixed-case> 2020 Shared Task: Transfer Learning for Aggressive Identification with <fixed-case>XGB</fixed-case>oost</title>
      <author><first>Saja</first><last>Tawalbeh</last></author>
      <author><first>Mahmoud</first><last>Hammad</last></author>
      <author><first>Mohammad</first><last>AL-Smadi</last></author>
      <pages>99–105</pages>
      <abstract>we have developed a system based on transfer learning technique depending on universal sentence encoder (USE) embedding that will be trained in our developed model using xgboost classifier to identify the aggressive text data from English content. A reference dataset has been provided from TRAC 2020 to evaluate the developed approach. The developed approach achieved in sub-task EN-A 60.75% F1 (weighted) which ranked fourteenth out of sixteen teams and achieved 85.66% F1 (weighted) in sub-task EN-B which ranked six out of fifteen teams.</abstract>
      <url hash="c63382e7">2020.trac-1.16</url>
      <language>eng</language>
    </paper>
    <paper id="17">
      <title><fixed-case>F</fixed-case>lor<fixed-case>U</fixed-case>ni<fixed-case>T</fixed-case>o@<fixed-case>TRAC</fixed-case>-2: Retrofitting Word Embeddings on an Abusive Lexicon for Aggressive Language Detection</title>
      <author><first>Anna</first><last>Koufakou</last></author>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>106–112</pages>
      <abstract>This paper describes our participation to the TRAC-2 Shared Tasks on Aggression Identification. Our team, FlorUniTo, investigated the applicability of using an abusive lexicon to enhance word embeddings towards improving detection of aggressive language. The embeddings used in our paper are word-aligned pre-trained vectors for English, Hindi, and Bengali, to reflect the languages in the shared task data sets. The embeddings are retrofitted to a multilingual abusive lexicon, HurtLex. We experimented with an LSTM model using the original as well as the transformed embeddings and different language and setting variations. Overall, our systems placed toward the middle of the official rankings based on weighted F1 score. However, the results on the development and test sets show promising improvements across languages, especially on the misogynistic aggression sub-task.</abstract>
      <url hash="f34a5fa1">2020.trac-1.17</url>
      <language>eng</language>
    </paper>
    <paper id="18">
      <title><fixed-case>AI</fixed-case>_<fixed-case>ML</fixed-case>_<fixed-case>NIT</fixed-case>_<fixed-case>P</fixed-case>atna @ <fixed-case>TRAC</fixed-case> - 2: Deep Learning Approach for Multi-lingual Aggression Identification</title>
      <author><first>Kirti</first><last>Kumari</last></author>
      <author><first>Jyoti Prakash</first><last>Singh</last></author>
      <pages>113–119</pages>
      <abstract>This paper describes the details of developed models and results of team AI_ML_NIT_Patna for the shared task of TRAC - 2. The main objective of the said task is to identify the level of aggression and whether the comment is gendered based or not. The aggression level of each comment can be marked as either Overtly aggressive or Covertly aggressive or Non-aggressive. We have proposed two deep learning systems: Convolutional Neural Network and Long Short Term Memory with two different input text representations, FastText and One-hot embeddings. We have found that the LSTM model with FastText embedding is performing better than other models for Hindi and Bangla datasets but for the English dataset, the CNN model with FastText embedding has performed better. We have also found that the performances of One-hot embedding and pre-trained FastText embedding are comparable. Our system got 11th and 10th positions for English Sub-task A and Sub-task B, respectively, 8th and 7th positions, respectively for Hindi Sub-task A and Sub-task B and 7th and 6th positions for Bangla Sub-task A and Sub-task B, respectively among the total submitted systems.</abstract>
      <url hash="18fbc3ff">2020.trac-1.18</url>
      <language>eng</language>
    </paper>
    <paper id="19">
      <title>Multilingual Joint Fine-tuning of Transformer models for identifying Trolling, Aggression and Cyberbullying at <fixed-case>TRAC</fixed-case> 2020</title>
      <author><first>Sudhanshu</first><last>Mishra</last></author>
      <author><first>Shivangi</first><last>Prasad</last></author>
      <author><first>Shubhanshu</first><last>Mishra</last></author>
      <pages>120–125</pages>
      <abstract>We present our team ‘3Idiots’ (referred as ‘sdhanshu’ in the official rankings) approach for the Trolling, Aggression and Cyberbullying (TRAC) 2020 shared tasks. Our approach relies on fine-tuning various Transformer models on the different datasets. We also investigated the utility of task label marginalization, joint label classification, and joint training on multilingual datasets as possible improvements to our models. Our team came second in English sub-task A, a close fourth in the English sub-task B and third in the remaining 4 sub-tasks. We find the multilingual joint training approach to be the best trade-off between computational efficiency of model deployment and model’s evaluation performance. We open source our approach at https://github.com/socialmediaie/TRAC2020.</abstract>
      <url hash="2eb1175e">2020.trac-1.19</url>
      <language>eng</language>
    </paper>
    <paper id="20">
      <title>Aggression and Misogyny Detection using <fixed-case>BERT</fixed-case>: A Multi-Task Approach</title>
      <author><first>Niloofar</first><last>Safi Samghabadi</last></author>
      <author><first>Parth</first><last>Patwa</last></author>
      <author><first>Srinivas</first><last>PYKL</last></author>
      <author><first>Prerana</first><last>Mukherjee</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>126–131</pages>
      <abstract>In recent times, the focus of the NLP community has increased towards offensive language, aggression, and hate-speech detection.This paper presents our system for TRAC-2 shared task on “Aggression Identification” (sub-task A) and “Misogynistic Aggression Identification” (sub-task B). The data for this shared task is provided in three different languages - English, Hindi, and Bengali. Each data instance is annotated into one of the three aggression classes - Not Aggressive, Covertly Aggressive, Overtly Aggressive, as well as one of the two misogyny classes - Gendered and Non-Gendered. We propose an end-to-end neural model using attention on top of BERT that incorporates a multi-task learning paradigm to address both the sub-tasks simultaneously. Our team, “na14”, scored 0.8579 weighted F1-measure on the English sub-task B and secured 3rd rank out of 15 teams for the task. The code and the model weights are publicly available at https://github.com/NiloofarSafi/TRAC-2. Keywords: Aggression, Misogyny, Abusive Language, Hate-Speech Detection, BERT, NLP, Neural Networks, Social Media</abstract>
      <url hash="3063822a">2020.trac-1.20</url>
      <language>eng</language>
    </paper>
    <paper id="21">
      <title>Automatic Detection of Offensive Language in Social Media: Defining Linguistic Criteria to build a <fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>panish Dataset</title>
      <author><first>María José</first><last>Díaz-Torres</last></author>
      <author><first>Paulina Alejandra</first><last>Morán-Méndez</last></author>
      <author><first>Luis</first><last>Villasenor-Pineda</last></author>
      <author><first>Manuel</first><last>Montes-y-Gómez</last></author>
      <author><first>Juan</first><last>Aguilera</last></author>
      <author><first>Luis</first><last>Meneses-Lerín</last></author>
      <pages>132–136</pages>
      <abstract>Phenomena such as bullying, homophobia, sexism and racism have transcended to social networks, motivating the development of tools for their automatic detection. The challenge becomes greater for languages rich in popular sayings, colloquial expressions and idioms which may contain vulgar, profane or rude words, but not always have the intention of offending, as is the case of Mexican Spanish. Under these circumstances, the identification of the offense goes beyond the lexical and syntactic elements of the message. This first work aims to define the main linguistic features of aggressive, offensive and vulgar language in social networks in order to establish linguistic-based criteria to facilitate the identification of abusive language. For this purpose, a Mexican Spanish Twitter corpus was compiled and analyzed. The dataset included words that, despite being rude, need to be considered in context to determine they are part of an offense. Based on the analysis of this corpus, linguistic criteria were defined to determine whether a message is offensive. To simplify the application of these criteria, an easy-to-follow diagram was designed. The paper presents an example of the use of the diagram, as well as the basic statistics of the corpus.</abstract>
      <url hash="88a89a83">2020.trac-1.21</url>
      <language>eng</language>
    </paper>
    <paper id="22">
      <title>Offensive Language Detection Explained</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Robin</first><last>Ruff</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>137–143</pages>
      <abstract>Many online discussion platforms use a content moderation process, where human moderators check user comments for offensive language and other rule violations. It is the moderator’s decision which comments to remove from the platform because of violations and which ones to keep. Research so far focused on automating this decision process in the form of supervised machine learning for a classification task. However, even with machine-learned models achieving better classification accuracy than human experts, there is still a reason why human moderators are preferred. In contrast to black-box models, such as neural networks, humans can give explanations for their decision to remove a comment. For example, they can point out which phrase in the comment is offensive or what subtype of offensiveness applies. In this paper, we analyze and compare four explanation methods for different offensive language classifiers: an interpretable machine learning model (naive Bayes), a model-agnostic explanation method (LIME), a model-based explanation method (LRP), and a self-explanatory model (LSTM with an attention mechanism). We evaluate these approaches with regard to their explanatory power and their ability to point out which words are most relevant for a classifier’s decision. We find that the more complex models achieve better classification accuracy while also providing better explanations than the simpler models.</abstract>
      <url hash="f2fdefb2">2020.trac-1.22</url>
      <language>eng</language>
    </paper>
    <paper id="23">
      <title>Detecting Early Signs of Cyberbullying in Social Media</title>
      <author><first>Niloofar</first><last>Safi Samghabadi</last></author>
      <author><first>Adrián Pastor</first><last>López Monroy</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>144–149</pages>
      <abstract>Nowadays, the amount of users’ activities on online social media is growing dramatically. These online environments provide excellent opportunities for communication and knowledge sharing. However, some people misuse them to harass and bully others online, a phenomenon called cyberbullying. Due to its harmful effects on people, especially youth, it is imperative to detect cyberbullying as early as possible before it causes irreparable damages to victims. Most of the relevant available resources are not explicitly designed to detect cyberbullying, but related content, such as hate speech and abusive language. In this paper, we propose a new approach to create a corpus suited for cyberbullying detection. We also investigate the possibility of designing a framework to monitor the streams of users’ online messages and detects the signs of cyberbullying as early as possible.</abstract>
      <url hash="6c6da8e9">2020.trac-1.23</url>
      <language>eng</language>
    </paper>
    <paper id="24">
      <title>Lexicon-Enhancement of Embedding-based Approaches Towards the Detection of Abusive Language</title>
      <author><first>Anna</first><last>Koufakou</last></author>
      <author><first>Jason</first><last>Scott</last></author>
      <pages>150–157</pages>
      <abstract>Detecting abusive language is a significant research topic, which has received a lot of attention recently. Our work focuses on detecting personal attacks in online conversations. As previous research on this task has largely used deep learning based on embeddings, we explore the use of lexicons to enhance embedding-based methods in an effort to see how these methods apply in the particular task of detecting personal attacks. The methods implemented and experimented with in this paper are quite different from each other, not only in the type of lexicons they use (sentiment or semantic), but also in the way they use the knowledge from the lexicons, in order to construct or to change embeddings that are ultimately fed into the learning model. The sentiment lexicon approaches focus on integrating sentiment information (in the form of sentiment embeddings) into the learning model. The semantic lexicon approaches focus on transforming the original word embeddings so that they better represent relationships extracted from a semantic lexicon. Based on our experimental results, semantic lexicon methods are superior to the rest of the methods in this paper, with at least 4% macro-averaged F1 improvement over the baseline.</abstract>
      <url hash="793a0f46">2020.trac-1.24</url>
      <language>eng</language>
    </paper>
    <paper id="25">
      <title>Developing a Multilingual Annotated Corpus of Misogyny and Aggression</title>
      <author><first>Shiladitya</first><last>Bhattacharya</last></author>
      <author><first>Siddharth</first><last>Singh</last></author>
      <author><first>Ritesh</first><last>Kumar</last></author>
      <author><first>Akanksha</first><last>Bansal</last></author>
      <author><first>Akash</first><last>Bhagat</last></author>
      <author><first>Yogesh</first><last>Dawer</last></author>
      <author><first>Bornini</first><last>Lahiri</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <pages>158–168</pages>
      <abstract>In this paper, we discuss the development of a multilingual annotated corpus of misogyny and aggression in Indian English, Hindi, and Indian Bangla as part of a project on studying and automatically identifying misogyny and communalism on social media (the ComMA Project). The dataset is collected from comments on YouTube videos and currently contains a total of over 20,000 comments. The comments are annotated at two levels - aggression (overtly aggressive, covertly aggressive, and non-aggressive) and misogyny (gendered and non-gendered). We describe the process of data collection, the tagset used for annotation, and issues and challenges faced during the process of annotation. Finally, we discuss the results of the baseline experiments conducted to develop a classifier for misogyny in the three languages.</abstract>
      <url hash="c4a52c38">2020.trac-1.25</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
