<?xml version='1.0' encoding='UTF-8'?>
<volume id="Q19">
  <paper id="1000">
    <title>Transactions of the Association for Computational Linguistics, Volume 7</title>
    <year>2019</year>
  </paper>

  <paper id="1001">
    <title>Grammar Error Correction in Morphologically Rich Languages: The Case of Russian</title>
    <author><first>Alla</first><last>Rozovskaya</last></author>
    <author><first>Dan</first><last>Roth</last></author>
    <year>2019</year>
    <month>March</month>
    <doi>10.1162/tacl_a_00251</doi>
    <abstract>Until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically rich languages, with a focus on Russian. We present a corrected and error-tagged corpus of Russian learner writing and develop models that make use of existing state-of-the-art methods that have been well studied for English. Although impressive results have recently been achieved for grammar error correction of non-native English writing, these results are limited to domains where plentiful training data are available. Because annotation is extremely costly, these approaches are not suitable for the majority of domains and languages. We thus focus on methods that use “minimal supervision”; that is, those that do not rely on large amounts of annotated training data, and show how existing minimal-supervision approaches extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology.</abstract>
    <pages>1–17</pages>
    <url>https://www.aclweb.org/anthology/Q19-1001</url>
  </paper>

  <paper id="1002">
    <title>Semantic Neural Machine Translation Using AMR</title>
    <author><first>Linfeng</first><last>Song</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <author><first>Yue</first><last>Zhang</last></author>
    <author><first>Zhiguo</first><last>Wang</last></author>
    <author><first>Jinsong</first><last>Su</last></author>
    <year>2019</year>
    <month>March</month>
    <doi>10.1162/tacl_a_00252</doi>
    <abstract>It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.</abstract>
    <pages>19–31</pages>
    <url>https://www.aclweb.org/anthology/Q19-1002</url>
  </paper>

  <paper id="1003">
    <title>Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for MRLs and a Case Study from Modern Hebrew</title>
    <author><first>Amir</first><last>More</last></author>
    <author><first>Amit</first><last>Seker</last></author>
    <author><first>Victoria</first><last>Basmova</last></author>
    <author><first>Reut</first><last>Tsarfaty</last></author>
    <year>2019</year>
    <month>March</month>
    <doi>10.1162/tacl_a_00253</doi>
    <abstract>In standard NLP pipelines, morphological analysis and disambiguation (MA&amp;D) precedes syntactic and semantic downstream tasks. However, for languages with complex and ambiguous word-internal structure, known as morphologically rich languages (MRLs), it has been hypothesized that syntactic context may be crucial for accurate MA&amp;D, and vice versa. In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA&amp;D results obtained in the joint settings outperform MA&amp;D results obtained by the respective standalone components, and that end-to-end parsing results obtained by our joint system present a new state of the art for Hebrew dependency parsing.</abstract>
    <pages>33–48</pages>
    <url>https://www.aclweb.org/anthology/Q19-1003</url>
  </paper>

  <paper id="1004">
    <title>Analysis Methods in Neural Language Processing: A Survey</title>
    <author><first>Yonatan</first><last>Belinkov</last></author>
    <author><first>James</first><last>Glass</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00254</doi>
    <abstract>The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.</abstract>
    <pages>49–72</pages>
    <url>https://www.aclweb.org/anthology/Q19-1004</url>
  </paper>

  <paper id="1005">
    <title>Unlexicalized Transition-based Discontinuous Constituency Parsing</title>
    <author><first>Maximin</first><last>Coavoux</last></author>
    <author><first>Benoît</first><last>Crabbé</last></author>
    <author><first>Shay B.</first><last>Cohen</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00255</doi>
    <abstract>Lexicalized parsing models are based on the assumptions that (i) constituents are organized around a lexical head and (ii) bilexical statistics are crucial to solve ambiguities. In this paper, we introduce an unlexicalized transition-based parser for discontinuous constituency structures, based on a structure-label transition system and a bi-LSTM scoring system. We compare it with lexicalized parsing models in order to address the question of lexicalization in the context of discontinuous constituency parsing. Our experiments show that unlexicalized models systematically achieve higher results than lexicalized models, and provide additional empirical evidence that lexicalization is not necessary to achieve strong parsing results. Our best unlexicalized model sets a new state of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents.</abstract>
    <pages>73–89</pages>
    <url>https://www.aclweb.org/anthology/Q19-1005</url>
  </paper>

  <paper id="1006">
    <title>Synchronous Bidirectional Neural Machine Translation</title>
    <author><first>Long</first><last>Zhou</last></author>
    <author><first>Jiajun</first><last>Zhang</last></author>
    <author><first>Chengqing</first><last>Zong</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00256</doi>
    <abstract>Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional–neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese-English, WMT14 English-German, and WMT18 Russian-English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art per- formance on Chinese-English and English- German translation tasks.1</abstract>
    <pages>91–105</pages>
    <url>https://www.aclweb.org/anthology/Q19-1006</url>
  </paper>

  <paper id="1007">
    <title>Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach</title>
    <author><first>Pratik</first><last>Jawanpuria</last></author>
    <author><first>Arjun</first><last>Balgovind</last></author>
    <author><first>Anoop</first><last>Kunchukuttan</last></author>
    <author><first>Bamdev</first><last>Mishra</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00257</doi>
    <abstract>We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.</abstract>
    <pages>107–120</pages>
    <url>https://www.aclweb.org/anthology/Q19-1007</url>
  </paper>

  <paper id="1008">
    <title>Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications</title>
    <author><first>Rumen</first><last>Dangovski</last></author>
    <author><first>Li</first><last>Jing</last></author>
    <author><first>Preslav</first><last>Nakov</last></author>
    <author><first>Mićo</first><last>Tatalović</last></author>
    <author><first>Marin</first><last>Soljačić</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00258</doi>
    <abstract>Stacking long short-term memory (LSTM) cells or gated recurrent units (GRUs) as part of a recurrent neural network (RNN) has become a standard approach to solving a number of tasks ranging from language modeling to text summarization. Although LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past. Here, we derive a phase-coded representation of the memory state, Rotational Unit of Memory (RUM), that unifies the concepts of unitary learning and associative memory. We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs/GRUs. We further demonstrate that by replacing LSTM/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art.</abstract>
    <pages>121–138</pages>
    <url>https://www.aclweb.org/anthology/Q19-1008</url>
  </paper>

  <paper id="1009">
    <title>GILE: A Generalized Input-Label Embedding for Text Classification</title>
    <author><first>Nikolaos</first><last>Pappas</last></author>
    <author><first>James</first><last>Henderson</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00259</doi>
    <abstract>Neural text classification models typically treat output labels as categorical variables that lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels. The model consists of a joint nonlinear input-label embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios.</abstract>
    <pages>139–155</pages>
    <url>https://www.aclweb.org/anthology/Q19-1009</url>
  </paper>

  <paper id="1010">
    <title>Autosegmental Input Strictly Local Functions</title>
    <author><first>Jane</first><last>Chandlee</last></author>
    <author><first>Adam</first><last>Jardine</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00260</doi>
    <abstract>Autosegmental representations (ARs; Goldsmith, 1976) are claimed to enable local analyses of otherwise non-local phenomena Odden (1994). Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of locality extended from Chandlee (2014). The result is a more nuanced understanding of the way in which ARs interact with phonological locality.</abstract>
    <pages>157–168</pages>
    <url>https://www.aclweb.org/anthology/Q19-1010</url>
  </paper>

  <paper id="1011">
    <title>SECTOR: A Neural Model for Coherent Topic Segmentation and Classification</title>
    <author><first>Sebastian</first><last>Arnold</last></author>
    <author><first>Rudolf</first><last>Schneider</last></author>
    <author><first>Philippe</first><last>Cudré-Mauroux</last></author>
    <author><first>Felix A.</first><last>Gers</last></author>
    <author><first>Alexander</first><last>Löser</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00261</doi>
    <abstract>When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.</abstract>
    <pages>169–184</pages>
    <url>https://www.aclweb.org/anthology/Q19-1011</url>
  </paper>

  <paper id="1012">
    <title>Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs</title>
    <author><first>Amrita</first><last>Saha</last></author>
    <author><first>Ghulam Ahmed</first><last>Ansari</last></author>
    <author><first>Abhishek</first><last>Laddha</last></author>
    <author><first>Karthik</first><last>Sankaranarayanan</last></author>
    <author><first>Soumen</first><last>Chakrabarti</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00262</doi>
    <abstract>Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the ‘‘gold’’ program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3× higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5–10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.1</abstract>
    <pages>185–200</pages>
    <url>https://www.aclweb.org/anthology/Q19-1012</url>
  </paper>

  <paper id="1013">
    <title>Categorical Metadata Representation for Customized Text Classification</title>
    <author><first>Jihyeok</first><last>Kim</last></author>
    <author><first>Reinald Kim</first><last>Amplayo</last></author>
    <author><first>Kyungjae</first><last>Lee</last></author>
    <author><first>Sua</first><last>Sung</last></author>
    <author><first>Minji</first><last>Seo</last></author>
    <author><first>Seung-won</first><last>Hwang</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00263</doi>
    <abstract>The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. This information has been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose using basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various data sets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.</abstract>
    <pages>201–215</pages>
    <url>https://www.aclweb.org/anthology/Q19-1013</url>
  </paper>

  <paper id="1014">
    <title>DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension</title>
    <author><first>Kai</first><last>Sun</last></author>
    <author><first>Dian</first><last>Yu</last></author>
    <author><first>Jianshu</first><last>Chen</last></author>
    <author><first>Dong</first><last>Yu</last></author>
    <author><first>Yejin</first><last>Choi</last></author>
    <author><first>Claire</first><last>Cardie</last></author>
    <year>2019</year>
    <month>April</month>
    <doi>10.1162/tacl_a_00264</doi>
    <abstract>We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/.</abstract>
    <pages>217–231</pages>
    <url>https://www.aclweb.org/anthology/Q19-1014</url>
  </paper>
</volume>
