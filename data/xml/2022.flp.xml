<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.flp">
  <volume id="1" ingest-date="2022-12-13" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Figurative Language Processing (FLP)</booktitle>
      <editor><first>Debanjan</first><last>Ghosh</last></editor>
      <editor><first>Beata</first><last>Beigman Klebanov</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Anna</first><last>Feldman</last></editor>
      <editor><first>Soujanya</first><last>Poria</last></editor>
      <editor><first>Tuhin</first><last>Chakrabarty</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="531392d8">2022.flp-1</url>
      <venue>figlang</venue>
    </meta>
    <frontmatter>
      <url hash="316aef8f">2022.flp-1.0</url>
      <bibkey>flp-2022-figurative</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>TEDB</fixed-case> System Description to a Shared Task on Euphemism Detection 2022</title>
      <author><first>Peratham</first><last>Wiriyathammabhum</last><affiliation>Self</affiliation></author>
      <pages>1-7</pages>
      <abstract>In this report, we describe our Transformers for euphemism detection baseline (TEDB) submissions to a shared task on euphemism detection 2022. We cast the task of predicting euphemism as text classification. We considered Transformer-based models which are the current state-of-the-art methods for text classification. We explored different training schemes, pretrained models, and model architectures. Our best result of 0.816 F1-score (0.818 precision and 0.814 recall) consists of a euphemism-detection-finetuned TweetEval/TimeLMs-pretrained RoBERTa model as a feature extractor frontend with a KimCNN classifier backend trained end-to-end using a cosine annealing scheduler. We observed pretrained models on sentiment analysis and offensiveness detection to correlate with more F1-score while pretraining on other tasks, such as sarcasm detection, produces less F1-scores. Also, putting more word vector channels does not improve the performance in our experiments.</abstract>
      <url hash="06bae619">2022.flp-1.1</url>
      <bibkey>wiriyathammabhum-2022-tedb</bibkey>
      <video href="2022.flp-1.1.mp4"/>
      <doi>10.18653/v1/2022.flp-1.1</doi>
    </paper>
    <paper id="2">
      <title>A Prompt Based Approach for Euphemism Detection</title>
      <author><first>Abulimiti</first><last>Maimaitituoheti</last><affiliation>Xinjiang Normal University, China</affiliation></author>
      <author><first>Yang</first><last>Yong</last><affiliation>Xinjiang Normal University, China</affiliation></author>
      <author><first>Fan</first><last>Xiaochao</last><affiliation>Xinjiang Normal University, China</affiliation></author>
      <pages>8-12</pages>
      <abstract>Euphemism is an indirect way to express sensitive topics. People can comfortably communicate with each other about sensitive topics or taboos by using euphemisms. The Euphemism Detection Shared Task in the Third Workshop on Figurative Language Processing co-located with EMNLP 2022 provided a euphemism detection dataset that was divided into the train set and the test set. We made euphemism detection experiments by prompt tuning pre-trained language models on the dataset. We used RoBERTa as the pre-trained language model and created suitable templates and verbalizers for the euphemism detection task. Our approach achieved the third-best score in the euphemism detection shared task. This paper describes our model participating in the task.</abstract>
      <url hash="c0ae8740">2022.flp-1.2</url>
      <bibkey>maimaitituoheti-etal-2022-prompt</bibkey>
      <doi>10.18653/v1/2022.flp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Transfer Learning Parallel Metaphor using Bilingual Embeddings</title>
      <author><first>Maria</first><last>Berger</last><affiliation>Ruhr University Bochum</affiliation></author>
      <pages>13-23</pages>
      <abstract>Automated metaphor detection in languages other than English is highly restricted as training corpora are comparably rare. One way to overcome this problem is transfer learning. This paper gives an overview on transfer learning techniques applied to NLP. We first introduce types of transfer learning, then we present work focusing on: i) transfer learning with cross-lingual embeddings; ii) transfer learning in machine translation; and iii) transfer learning using pre-trained transformer models. The paper is complemented by first experiments that make use of bilingual embeddings generated from different sources of parallel data: We i) present the preparation of a parallel Gold corpus; ii) examine the embeddings spaces to search for metaphoric words cross-lingually; iii) run first experiments in transfer learning German metaphor from English labeled data only. Results show that finding data sources for bilingual embeddings training and the vocabulary covered by these embeddings is critical for learning metaphor cross-lingually.</abstract>
      <url hash="5edf8c7d">2022.flp-1.3</url>
      <bibkey>berger-2022-transfer</bibkey>
      <video href="2022.flp-1.3.mp4"/>
      <doi>10.18653/v1/2022.flp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in Videos</title>
      <author><first>Khalid</first><last>Alnajjar</last><affiliation>University of Helsinki, Finland</affiliation></author>
      <author><first>Mika</first><last>Hämäläinen</last><affiliation>University of Helsinki, Finland</affiliation></author>
      <author><first>Shuo</first><last>Zhang</last><affiliation>Bose Corporation, USA</affiliation></author>
      <pages>24-33</pages>
      <abstract>We present the first openly available multimodal metaphor annotated corpus. The corpus consists of videos including audio and subtitles that have been annotated by experts. Furthermore, we present a method for detecting metaphors in the new dataset based on the textual content of the videos. The method achieves a high F1-score (62%) for metaphorical labels. We also experiment with other modalities and multimodal methods; however, these methods did not out-perform the text-based model. In our error analysis, we do identify that there are cases where video could help in disambiguating metaphors, however, the visual cues are too subtle for our model to capture. The data is available on Zenodo.</abstract>
      <url hash="47a59f81">2022.flp-1.4</url>
      <bibkey>alnajjar-etal-2022-ring</bibkey>
      <video href="2022.flp-1.4.mp4"/>
      <doi>10.18653/v1/2022.flp-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>P</fixed-case>icard understanding Darmok: A Dataset and Model for Metaphor-Rich Translation in a Constructed Language</title>
      <author><first>Peter A.</first><last>Jansen</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland</affiliation></author>
      <pages>34-38</pages>
      <abstract>Tamarian, a fictional language introduced in the Star Trek episode Darmok, communicates meaning through utterances of metaphorical references, such as “Darmok and Jalad at Tanagra” instead of “We should work together.” This work assembles a Tamarian-English dictionary of utterances from the original episode and several follow-on novels, and uses this to construct a parallel corpus of 456 English-Tamarian utterances. A machine translation system based on a large language model (T5) is trained using this parallel corpus, and is shown to produce an accuracy of 76% when translating from English to Tamarian on known utterances.</abstract>
      <url hash="8c7b7aa4">2022.flp-1.5</url>
      <bibkey>jansen-boyd-graber-2022-picard</bibkey>
      <video href="2022.flp-1.5.mp4"/>
      <doi>10.18653/v1/2022.flp-1.5</doi>
    </paper>
    <paper id="6">
      <title>The Secret of Metaphor on Expressing Stronger Emotion</title>
      <author><first>Yucheng</first><last>Li</last><affiliation>Department of Computer Science, University of Surrey, UK</affiliation></author>
      <author><first>Frank</first><last>Guerin</last><affiliation>Department of Computer Science, University of Surrey, UK</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>Department of Computer Science, University of Sheffield, UK</affiliation></author>
      <pages>39-43</pages>
      <abstract>Metaphors are proven to have stronger emotional impact than literal expressions. Although this conclusion is shown to be promising in benefiting various NLP applications, the reasons behind this phenomenon are not well studied. This paper conducts the first study in exploring how metaphors convey stronger emotion than their literal counterparts. We find that metaphors are generally more specific than literal expressions. The more specific property of metaphor can be one of the reasons for metaphors’ superiority in emotion expression. When we compare metaphors with literal expressions with the same specificity level, the gap of emotion expressing ability between both reduces significantly. In addition, we observe specificity is crucial in literal language as well, as literal language can express stronger emotion by making it more specific.</abstract>
      <url hash="2868eccd">2022.flp-1.6</url>
      <bibkey>li-etal-2022-secret</bibkey>
      <video href="2022.flp-1.6.mp4"/>
      <doi>10.18653/v1/2022.flp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Drum Up <fixed-case>SUPPORT</fixed-case>: Systematic Analysis of Image-Schematic Conceptual Metaphors</title>
      <author><first>Lennart</first><last>Wachowiak</last><affiliation>King’s College London</affiliation></author>
      <author><first>Dagmar</first><last>Gromann</last><affiliation>University of Vienna</affiliation></author>
      <author><first>Chao</first><last>Xu</last><affiliation>Shanxi University</affiliation></author>
      <pages>44-53</pages>
      <abstract>Conceptual metaphors represent a cognitive mechanism to transfer knowledge structures from one onto another domain. Image-schematic conceptual metaphors (ISCMs) specialize on transferring sensorimotor experiences to abstract domains. Natural language is believed to provide evidence of such metaphors. However, approaches to verify this hypothesis largely rely on top-down methods, gathering examples by way of introspection, or on manual corpus analyses. In order to contribute towards a method that is systematic and can be replicated, we propose to bring together existing processing steps in a pipeline to detect ISCMs, exemplified for the image schema SUPPORT in the COVID-19 domain. This pipeline consist of neural metaphor detection, dependency parsing to uncover construction patterns, clustering, and BERT-based frame annotation of dependent constructions to analyse ISCMs.</abstract>
      <url hash="7c7f859f">2022.flp-1.7</url>
      <bibkey>wachowiak-etal-2022-drum</bibkey>
      <video href="2022.flp-1.7.mp4"/>
      <doi>10.18653/v1/2022.flp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Effective Cross-Task Transfer Learning for Explainable Natural Language Inference with T5</title>
      <author><first>Irina</first><last>Bigoulaeva</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <author><first>Rachneet</first><last>Singh Sachdeva</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <author><first>Aline</first><last>Villavicencio</last><affiliation>Department of Computer Science, University of Sheffield, UK</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>54-60</pages>
      <abstract>We compare sequential fine-tuning with a model for multi-task learning in the context where we are interested in boosting performance on two of the tasks, one of which depends on the other. We test these models on the FigLang2022 shared task which requires participants to predict language inference labels on figurative language along with corresponding textual explanations of the inference predictions. Our results show that while sequential multi-task learning can be tuned to be good at the first of two target tasks, it performs less well on the second and additionally struggles with overfitting. Our findings show that simple sequential fine-tuning of text-to-text models is an extraordinarily powerful method of achieving cross-task knowledge transfer while simultaneously predicting multiple interdependent targets. So much so, that our best model achieved the (tied) highest score on the task.</abstract>
      <url hash="1ca0e7e6">2022.flp-1.8</url>
      <bibkey>bigoulaeva-etal-2022-effective</bibkey>
      <video href="2022.flp-1.8.mp4"/>
      <doi>10.18653/v1/2022.flp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Detecting Euphemisms with Literal Descriptions and Visual Imagery</title>
      <author><first>Ilker</first><last>Kesen</last><affiliation>Koç University, KUIS AI Center</affiliation></author>
      <author><first>Aykut</first><last>Erdem</last><affiliation>Koç University, KUIS AI Center</affiliation></author>
      <author><first>Erkut</first><last>Erdem</last><affiliation>Hacettepe University, Computer Engineering Department</affiliation></author>
      <author><first>Iacer</first><last>Calixto</last><affiliation>Amsterdam UMC, University of Amsterdam, Department of Medical Informatics</affiliation></author>
      <pages>61-67</pages>
      <abstract>This paper describes our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.</abstract>
      <url hash="332cefbc">2022.flp-1.9</url>
      <bibkey>kesen-etal-2022-detecting</bibkey>
      <video href="2022.flp-1.9.mp4"/>
      <doi>10.18653/v1/2022.flp-1.9</doi>
    </paper>
    <paper id="10">
      <title>Distribution-Based Measures of Surprise for Creative Language: Experiments with Humor and Metaphor</title>
      <author><first>Razvan C.</first><last>Bunescu</last><affiliation>Department of Computer Science, University of North Carolina at Charlotte</affiliation></author>
      <author><first>Oseremen O.</first><last>Uduehi</last><affiliation>School of EECS, Ohio University</affiliation></author>
      <pages>68-78</pages>
      <abstract>Novelty or surprise is a fundamental attribute of creative output. As such, we postulate that a writer’s creative use of language leads to word choices and, more importantly, corresponding semantic structures that are unexpected for the reader. In this paper we investigate measures of surprise that rely solely on word distributions computed by language models and show empirically that creative language such as humor and metaphor is strongly correlated with surprise. Surprisingly at first, information content is observed to be at least as good a predictor of creative language as any of the surprise measures investigated. However, the best prediction performance is obtained when information and surprise measures are combined, showing that surprise measures capture an aspect of creative language that goes beyond information content.</abstract>
      <url hash="73335e45">2022.flp-1.10</url>
      <bibkey>bunescu-uduehi-2022-distribution</bibkey>
      <video href="2022.flp-1.10.mp4"/>
      <doi>10.18653/v1/2022.flp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Euphemism Detection by Transformers and Relational Graph Attention Network</title>
      <author><first>Yuting</first><last>Wang</last><affiliation>Institute of computing technology, Chinese academy of sciences</affiliation></author>
      <author><first>Yiyi</first><last>Liu</last><affiliation>Institute of computing technology, Chinese academy of sciences</affiliation></author>
      <author><first>Ruqing</first><last>Zhang</last><affiliation>CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yixing</first><last>Fan</last><affiliation>Institute of Computing Technology, CAS</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technology, CAS</affiliation></author>
      <pages>79-83</pages>
      <abstract>Euphemism is a type of figurative language broadly adopted in social media and daily conversations. People use euphemism for politeness or to conceal what they are discussing. Euphemism detection is a challenging task because of its obscure and figurative nature. Even humans may not agree on if a word expresses euphemism. In this paper, we propose to employ bidirectional encoder representations transformers (BERT), and relational graph attention network in order to model the semantic and syntactic relations between the target words and the input sentence. The best performing method of ours reaches a Macro-F1 score of 84.0 on the euphemism detection dataset of the third workshop on figurative language processing shared task 2022.</abstract>
      <url hash="4c5a1ac7">2022.flp-1.11</url>
      <bibkey>wang-etal-2022-euphemism</bibkey>
      <doi>10.18653/v1/2022.flp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Just-<fixed-case>DREAM</fixed-case>-about-it: Figurative Language Understanding with <fixed-case>DREAM</fixed-case>-<fixed-case>FLUTE</fixed-case></title>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Yao</first><last>Fu</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Ian</first><last>Magnusson</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Bhavana</first><last>Dalvi Mishra</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for AI</affiliation></author>
      <pages>84-93</pages>
      <abstract>Figurative language (e.g., “he flew like the wind”) is challenging to understand, as it is hard to tell what implicit information is being conveyed from the surface form alone. We hypothesize that to perform this task well, the reader needs to mentally elaborate the scene being described to identify a sensible meaning of the language. We present DREAM-FLUTE, a figurative language understanding system that does this, first forming a “mental model” of situations described in a premise and hypothesis before making an entailment/contradiction decision and generating an explanation. DREAM-FLUTE uses an existing scene elaboration model, DREAM, for constructing its “mental model.” In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint) first place (Acc@60=63.3%), and can perform even better with ensemble techniques, demonstrating the effectiveness of this approach. More generally, this work suggests that adding a reflective component to pretrained language models can improve their performance beyond standard fine-tuning (3.3% improvement in Acc@60).</abstract>
      <url hash="913a5d80">2022.flp-1.12</url>
      <bibkey>gu-etal-2022-just</bibkey>
      <video href="2022.flp-1.12.mp4"/>
      <doi>10.18653/v1/2022.flp-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>B</fixed-case>ayes at <fixed-case>F</fixed-case>ig<fixed-case>L</fixed-case>ang 2022 Euphemism Detection shared task: Cost-Sensitive <fixed-case>B</fixed-case>ayesian Fine-tuning and <fixed-case>V</fixed-case>enn-Abers Predictors for Robust Training under Class Skewed Distributions</title>
      <author><first>Paul</first><last>Trust</last><affiliation>University College Cork, Cork, Ireland</affiliation></author>
      <author><first>Kadusabe</first><last>Provia</last><affiliation>Worldquant University, Louisiana, USA</affiliation></author>
      <author><first>Kizito</first><last>Omala</last><affiliation>Makerere University, Kampala, Uganda</affiliation></author>
      <pages>94-99</pages>
      <abstract>Transformers have achieved a state of the art performance across most natural language processing tasks. However the performance of these models degrade when being trained on skewed class distributions (class imbalance) because training tends to be biased towards head classes with most of the data points . Classical methods that have been proposed to handle this problem (re-sampling and re-weighting) often suffer from unstable performance, poor applicability and poor calibration. In this paper, we propose to use Bayesian methods and Venn-Abers predictors for well calibrated and robust training against class imbalance. Our proposed approach improves f1-score of the baseline RoBERTa (A Robustly Optimized Bidirectional Embedding from Transformers Pretraining Approach) model by about 6 points (79.0% against 72.6%) when training with class imbalanced data.</abstract>
      <url hash="50a00d81">2022.flp-1.13</url>
      <bibkey>trust-etal-2022-bayes</bibkey>
      <doi>10.18653/v1/2022.flp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Food for Thought: How can we exploit contextual embeddings in the translation of idiomatic expressions?</title>
      <author><first>Lukas</first><last>Santing</last><affiliation>Department of Advanced Computing Sciences, Maastricht University</affiliation></author>
      <author><first>Ryan</first><last>Sijstermans</last><affiliation>Department of Advanced Computing Sciences, Maastricht University</affiliation></author>
      <author><first>Giacomo</first><last>Anerdi</last><affiliation>Department of Advanced Computing Sciences, Maastricht University</affiliation></author>
      <author><first>Pedro</first><last>Jeuris</last><affiliation>Department of Advanced Computing Sciences, Maastricht University</affiliation></author>
      <author><first>Marijn</first><last>ten Thij</last><affiliation>Department of Advanced Computing Sciences, Maastricht University</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>Department of Computer Science, The University of Manchester</affiliation></author>
      <pages>100-110</pages>
      <abstract>Idiomatic expressions (or idioms) are phrases where the meaning of the phrase cannot be determined from the meaning of the individual words in the expression. Translating idioms between languages is therefore a challenging task. Transformer models based on contextual embeddings have advanced the state-of-the-art across many domains in the field of natural language processing. While research using transformers has advanced both idiom detection as well as idiom disambiguation, idiom translation has not seen a similar advancement. In this work, we investigate two approaches to fine-tuning a pretrained Text-to-Text Transfer Transformer (T5) model to perform idiom translation from English to German. The first approach directly translates English idiom-containing sentences to German, while the second is underpinned by idiom paraphrasing, firstly paraphrasing English idiomatic expressions to their simplified English versions before translating them to German. Results of our evaluation show that each of the approaches is able to generate adequate translations.</abstract>
      <url hash="ceba0ec5">2022.flp-1.14</url>
      <bibkey>santing-etal-2022-food</bibkey>
      <video href="2022.flp-1.14.mp4"/>
      <doi>10.18653/v1/2022.flp-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>EUREKA</fixed-case>: <fixed-case>EU</fixed-case>phemism Recognition Enhanced through Knn-based methods and Augmentation</title>
      <author><first>Sedrick Scott</first><last>Keh</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Rohit</first><last>Bharadwaj</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Emmy</first><last>Liu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Simone</first><last>Tedeschi</last><affiliation>Babelscape, Italy</affiliation></author>
      <author><first>Varun</first><last>Gangal</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>111-117</pages>
      <abstract>We introduce EUREKA, an ensemble-based approach for performing automatic euphemism detection. We (1) identify and correct potentially mislabelled rows in the dataset, (2) curate an expanded corpus called EuphAug, (3) leverage model representations of Potentially Euphemistic Terms (PETs), and (4) explore using representations of semantically close sentences to aid in classification. Using our augmented dataset and kNN-based methods, EUREKA was able to achieve state-of-the-art results on the public leaderboard of the Euphemism Detection Shared Task, ranking first with a macro F1 score of 0.881.</abstract>
      <url hash="49e1a656">2022.flp-1.15</url>
      <bibkey>keh-etal-2022-eureka</bibkey>
      <doi>10.18653/v1/2022.flp-1.15</doi>
    </paper>
    <paper id="16">
      <title>An insulin pump? Identifying figurative links in the construction of the drug lexicon</title>
      <author><first>Antonio</first><last>Reyes</last><affiliation>Uaq-uabc</affiliation></author>
      <author><first>Rafael</first><last>Saldivar</last><affiliation>Uaq-uabc</affiliation></author>
      <pages>118-124</pages>
      <abstract>One of the remarkable characteristics of the drug lexicon is its elusive nature. In order to communicate information related to drugs or drug trafficking, the community uses several terms that are mostly unknown to regular people, or even to the authorities. For instance, the terms jolly green, joystick, or jive are used to refer to marijuana. The selection of such terms is not necessarily a random or senseless process, but a communicative strategy in which figurative language plays a relevant role. In this study, we describe an ongoing research to identify drug-related terms by applying machine learning techniques. To this end, a data set regarding drug trafficking in Spanish was built. This data set was used to train a word embedding model to identify terms used by the community to creatively refer to drugs and related matters. The initial findings show an interesting repository of terms created to consciously veil drug-related contents by using figurative language devices, such as metaphor or metonymy. These findings can provide preliminary evidence to be applied by law agencies in order to address actions against crime, drug transactions on the internet, illicit activities, or human trafficking.</abstract>
      <url hash="e157ad6f">2022.flp-1.16</url>
      <bibkey>reyes-saldivar-2022-insulin</bibkey>
      <video href="2022.flp-1.16.mp4"/>
      <doi>10.18653/v1/2022.flp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Can Yes-No Question-Answering Models be Useful for Few-Shot Metaphor Detection?</title>
      <author><first>Lena</first><last>Dankin</last><affiliation>School of Computer Science, Tel Aviv University</affiliation></author>
      <author><first>Kfir</first><last>Bar</last><affiliation>School of Computer Science, Reichman University</affiliation></author>
      <author><first>Nachum</first><last>Dershowitz</last><affiliation>School of Computer Science, Tel Aviv University</affiliation></author>
      <pages>125-130</pages>
      <abstract>Metaphor detection has been a challenging task in the NLP domain both before and after the emergence of transformer-based language models. The difficulty lies in subtle semantic nuances that are required to detect metaphor and in the scarcity of labeled data. We explore few-shot setups for metaphor detection, and also introduce new question answering data that can enhance classifiers that are trained on a small amount of data. We formulate the classification task as a question-answering one, and train a question-answering model. We perform extensive experiments for few shot on several architectures and report the results of several strong baselines. Thus, the answer to the question posed in the title is a definite “Yes!”</abstract>
      <url hash="c4062867">2022.flp-1.17</url>
      <bibkey>dankin-etal-2022-yes</bibkey>
      <video href="2022.flp-1.17.mp4"/>
      <doi>10.18653/v1/2022.flp-1.17</doi>
    </paper>
    <paper id="18">
      <title>An Exploration of Linguistically-Driven and Transfer Learning Methods for Euphemism Detection</title>
      <author><first>Devika</first><last>Tiwari</last><affiliation>Department of Computer Science, University of Illinois at Chicago</affiliation></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>Department of Computer Science, University of Illinois at Chicago</affiliation></author>
      <pages>131-136</pages>
      <abstract>Euphemisms are often used to drive rhetoric, but their automated recognition and interpretation are under-explored. We investigate four methods for detecting euphemisms in sentences containing potentially euphemistic terms. The first three linguistically-motivated methods rest on an understanding of (1) euphemism’s role to attenuate the harsh connotations of a taboo topic and (2) euphemism’s metaphorical underpinnings. In contrast, the fourth method follows recent innovations in other tasks and employs transfer learning from a general-domain pre-trained language model. While the latter method ultimately (and perhaps surprisingly) performed best (F1 = 0.74), we comprehensively evaluate all four methods to derive additional useful insights from the negative results.</abstract>
      <url hash="dc26eee1">2022.flp-1.18</url>
      <bibkey>tiwari-parde-2022-exploration</bibkey>
      <doi>10.18653/v1/2022.flp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Back to the Roots: Predicting the Source Domain of Metaphors using Contrastive Learning</title>
      <author><first>Meghdut</first><last>Sengupta</last><affiliation>Leibniz Universität Hannover, Hannover, Germany</affiliation></author>
      <author><first>Milad</first><last>Alshomary</last><affiliation>Leibniz Universität Hannover, Hannover, Germany</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover, Hannover, Germany</affiliation></author>
      <pages>137-142</pages>
      <abstract>Metaphors frame a given target domain using concepts from another, usually more concrete, source domain. Previous research in NLP has focused on the identification of metaphors and the interpretation of their meaning. In contrast, this paper studies to what extent the source domain can be predicted computationally from a metaphorical text. Given a dataset with metaphorical texts from a finite set of source domains, we propose a contrastive learning approach that ranks source domains by their likelihood of being referred to in a metaphorical text. In experiments, it achieves reasonable performance even for rare source domains, clearly outperforming a classification baseline.</abstract>
      <url hash="e9148824">2022.flp-1.19</url>
      <bibkey>sengupta-etal-2022-back</bibkey>
      <video href="2022.flp-1.19.mp4"/>
      <doi>10.18653/v1/2022.flp-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>SBU</fixed-case> Figures It Out: Models Explain Figurative Language</title>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Mohaddeseh</first><last>Bastan</last><affiliation>Stony Brook University</affiliation></author>
      <pages>143-149</pages>
      <abstract>Figurative language is ubiquitous in human communication. However, current NLP models are unable to demonstrate a significant understanding of instances of this phenomena. The EMNLP 2022 shared task on figurative language understanding posed the problem of predicting and explaining the relation between a premise and a hypothesis containing an instance of the use of figurative language. We experiment with different variations of using T5-large for this task and build a model that significantly outperforms the task baseline. Treating it as a new task for T5 and simply finetuning on the data achieves the best score on the defined evaluation. Furthermore, we find that hypothesis-only models are able to achieve most of the performance.</abstract>
      <url hash="d4f25d0c">2022.flp-1.20</url>
      <bibkey>lal-bastan-2022-sbu</bibkey>
      <doi>10.18653/v1/2022.flp-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>NLP</fixed-case>@<fixed-case>UIT</fixed-case> at <fixed-case>F</fixed-case>ig<fixed-case>L</fixed-case>ang-<fixed-case>EMNLP</fixed-case> 2022: A Divide-and-Conquer System For Shared Task On Understanding Figurative Language</title>
      <author><first>Khoa Thi-Kim</first><last>Phan</last><affiliation>University of Information Technology, Vietnam National University Ho chi minh City</affiliation></author>
      <author><first>Duc-Vu</first><last>Nguyen</last><affiliation>University of Information Technology, Vietnam National University Ho chi minh City</affiliation></author>
      <author><first>Ngan Luu-Thuy</first><last>Nguyen</last><affiliation>University of Information Technology, Vietnam National University Ho chi minh City</affiliation></author>
      <pages>150-153</pages>
      <abstract>This paper describes our submissions to the EMNLP 2022 shared task on Understanding Figurative Language as part of the Figurative Language Workshop (FigLang 2022). Our systems based on pre-trained language model T5 are divide-and-conquer models which can address both two requirements of the task: 1) classification, and 2) generation. In this paper, we introduce different approaches in which each approach we employ a processing strategy on input model. We also emphasize the influence of the types of figurative language on our systems.</abstract>
      <url hash="84de416e">2022.flp-1.21</url>
      <bibkey>phan-etal-2022-nlp</bibkey>
      <doi>10.18653/v1/2022.flp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Adversarial Perturbations Augmented Language Models for Euphemism Identification</title>
      <author><first>Guneet</first><last>Kohli</last><affiliation>Thapar University, Patiala, India</affiliation></author>
      <author><first>Prabsimran</first><last>Kaur</last><affiliation>Thapar University, Patiala, India</affiliation></author>
      <author><first>Jatin</first><last>Bedi</last><affiliation>Thapar University, Patiala, India</affiliation></author>
      <pages>154-159</pages>
      <abstract>Euphemisms are mild words or expressions used instead of harsh or direct words while talking to someone to avoid discussing something unpleasant, embarrassing, or offensive. However, they are often ambiguous, thus making it a challenging task. The Third Workshop on Figurative Language Processing, colocated with EMNLP 2022 organized a shared task on Euphemism Detection to better understand euphemisms. We have used the adversarial augmentation technique to construct new data. This augmented data was then trained using two language models: BERT and longformer. To further enhance the overall performance, various combinations of the results obtained using longformer and BERT were passed through a voting ensembler. We achieved an F1 score of 71.5 using the combination of two adversarial longformers, two adversarial BERT, and one non-adversarial BERT.</abstract>
      <url hash="2182b2bb">2022.flp-1.22</url>
      <bibkey>kohli-etal-2022-adversarial</bibkey>
      <doi>10.18653/v1/2022.flp-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>F</fixed-case>igurative<fixed-case>QA</fixed-case>: A Test Benchmark for Figurativeness Comprehension for Question Answering</title>
      <author><first>Geetanjali</first><last>Rakshit</last><affiliation>Computer Science and Engineering Department, UC Santa Cruz</affiliation></author>
      <author><first>Jeffrey</first><last>Flanigan</last><affiliation>Computer Science and Engineering Department, UC Santa Cruz</affiliation></author>
      <pages>160-166</pages>
      <abstract>Figurative language is widespread in human language (Lakoff and Johnson, 2008) posing potential challenges in NLP applications. In this paper, we investigate the effect of figurative language on the task of question answering (QA). We construct FigQA, a test set of 400 yes-no questions with figurative and non-figurative contexts, extracted from product reviews and restaurant reviews. We demonstrate that a state-of-the-art RoBERTa QA model has considerably lower performance in question answering when the contexts are figurative rather than literal, indicating a gap in current models. We propose a general method for improving the performance of QA models by converting the figurative contexts into non-figurative by prompting GPT-3, and demonstrate its effectiveness. Our results indicate a need for building QA models infused with figurative language understanding capabilities.</abstract>
      <url hash="a14f91f4">2022.flp-1.23</url>
      <bibkey>rakshit-flanigan-2022-figurativeqa</bibkey>
      <video href="2022.flp-1.23.mp4"/>
      <doi>10.18653/v1/2022.flp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Exploring Euphemism Detection in Few-Shot and Zero-Shot Settings</title>
      <author><first>Sedrick Scott</first><last>Keh</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>167-172</pages>
      <abstract>This work builds upon the Euphemism Detection Shared Task proposed in the EMNLP 2022 FigLang Workshop, and extends it to few-shot and zero-shot settings. We demonstrate a few-shot and zero-shot formulation using the dataset from the shared task, and we conduct experiments in these settings using RoBERTa and GPT-3. Our results show that language models are able to classify euphemistic terms relatively well even on new terms unseen during training, indicating that it is able to capture higher-level concepts related to euphemisms.</abstract>
      <url hash="91d41bba">2022.flp-1.24</url>
      <bibkey>keh-2022-exploring</bibkey>
      <doi>10.18653/v1/2022.flp-1.24</doi>
    </paper>
    <paper id="25">
      <title>On the Cusp of Comprehensibility: Can Language Models Distinguish Between Metaphors and Nonsense?</title>
      <author><first>Bernadeta</first><last>Griciūtė</last><affiliation>University of Malta</affiliation></author>
      <author><first>Marc</first><last>Tanti</last><affiliation>University of Malta</affiliation></author>
      <author><first>Lucia</first><last>Donatelli</last><affiliation>Saarland University</affiliation></author>
      <pages>173-177</pages>
      <abstract>Utterly creative texts can sometimes be difficult to understand, balancing on the edge of comprehensibility. However, good language skills and common sense allow advanced language users both to interpret creative texts and to reject some linguistic input as nonsense. The goal of this paper is to evaluate whether the current language models are also able to make the distinction between a creative language use and nonsense. To test this, we have computed mean rank and pseudo-log-likelihood score (PLL) of metaphorical and nonsensical sentences, and fine-tuned several pretrained models (BERT, RoBERTa) for binary classification between the two categories. There was a significant difference in the mean ranks and PPL scores of the categories, and the classifier reached around 85.5% accuracy. The results raise further questions on what could have let to such satisfactory performance.</abstract>
      <url hash="4fc91841">2022.flp-1.25</url>
      <bibkey>griciute-etal-2022-cusp</bibkey>
      <video href="2022.flp-1.25.mp4"/>
      <doi>10.18653/v1/2022.flp-1.25</doi>
    </paper>
    <paper id="26">
      <title>A Report on the <fixed-case>F</fixed-case>ig<fixed-case>L</fixed-case>ang 2022 Shared Task on Understanding Figurative Language</title>
      <author><first>Arkadiy</first><last>Saakyan</last><affiliation>Department of Computer Science, Columbia University</affiliation></author>
      <author><first>Tuhin</first><last>Chakrabarty</last><affiliation>Department of Computer Science, Columbia University</affiliation></author>
      <author><first>Debanjan</first><last>Ghosh</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Department of Computer Science, Columbia University</affiliation></author>
      <pages>178-183</pages>
      <abstract>We present the results of the Shared Task on Understanding Figurative Language that we conducted as a part of the 3rd Workshop on Figurative Language Processing (FigLang 2022) at EMNLP 2022. The shared task is based on the FLUTE dataset (Chakrabarty et al., 2022), which consists of NLI pairs containing figurative language along with free text explanations for each NLI instance. The task challenged participants to build models that are able to not only predict the right label for a figurative NLI instance, but also generate a convincing free-text explanation. The participants were able to significantly improve upon provided baselines in both automatic and human evaluation settings. We further summarize the submitted systems and discuss the evaluation results.</abstract>
      <url hash="605822b0">2022.flp-1.26</url>
      <bibkey>saakyan-etal-2022-report</bibkey>
      <doi>10.18653/v1/2022.flp-1.26</doi>
    </paper>
    <paper id="27">
      <title>A Report on the Euphemisms Detection Shared Task</title>
      <author><first>Patrick</first><last>Lee</last><affiliation>Montclair State University, New Jersey, USA</affiliation></author>
      <author><first>Anna</first><last>Feldman</last><affiliation>Montclair State University, New Jersey, USA</affiliation></author>
      <author><first>Jing</first><last>Peng</last><affiliation>Montclair State University, New Jersey, USA</affiliation></author>
      <pages>184-190</pages>
      <abstract>This paper presents The Shared Task on Euphemism Detection for the Third Workshop on Figurative Language Processing (FigLang 2022) held in conjunction with EMNLP 2022. Participants were invited to investigate the euphemism detection task: given input text, identify whether it contains a euphemism. The input data is a corpus of sentences containing potentially euphemistic terms (PETs) collected from the GloWbE corpus, and are human-annotated as containing either a euphemistic or literal usage of a PET. In this paper, we present the results and analyze the common themes, methods and findings of the participating teams.</abstract>
      <url hash="f8687629">2022.flp-1.27</url>
      <bibkey>lee-etal-2022-report</bibkey>
      <doi>10.18653/v1/2022.flp-1.27</doi>
    </paper>
  </volume>
</collection>
