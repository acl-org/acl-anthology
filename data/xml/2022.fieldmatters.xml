<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.fieldmatters">
  <volume id="1" ingest-date="2022-10-17" type="proceedings">
    <meta>
      <booktitle>Proceedings of the first workshop on NLP applications to field linguistics</booktitle>
      <editor><first>Oleg</first><last>Serikov</last></editor>
      <editor><first>Ekaterina</first><last>Voloshina</last></editor>
      <editor><first>Anna</first><last>Postnikova</last></editor>
      <editor><first>Elena</first><last>Klyachko</last></editor>
      <editor><first>Ekaterina</first><last>Neminova</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <editor><first>Tatiana</first><last>Shavrina</last></editor>
      <editor><first>Eric Le</first><last>Ferrand</last></editor>
      <editor><first>Valentin</first><last>Malykh</last></editor>
      <editor><first>Francis</first><last>Tyers</last></editor>
      <editor><first>Timofey</first><last>Arkhangelskiy</last></editor>
      <editor><first>Vladislav</first><last>Mikhailov</last></editor>
      <editor><first>Alena</first><last>Fenogenova</last></editor>
      <publisher>International Conference on Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="f58f2d52">2022.fieldmatters-1</url>
      <venue>fieldmatters</venue>
    </meta>
    <frontmatter>
      <url hash="887211bf">2022.fieldmatters-1.0</url>
      <bibkey>fieldmatters-2022-nlp</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Finite State Aproach to Interactive Transcription</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>1–10</pages>
      <abstract>We describe a novel approach to transcribing morphologically complex, local, oral languages. The approach connects with local motivations for participating in language work which center on language learning, accessing the content of audio collections, and applying this knowledge in language revitalization and maintenance. We develop a constraint-based approach to interactive word completion, expressed using Optimality Theoretic constraints, implemented in a finite state transducer, and applied to an Indigenous language. We show that this approach suggests correct full word predictions on 57.9% of the test utterances, and correct partial word predictions on 67.5% of the test utterances. In total, 87% of the test utterances receive full or partial word suggestions which serve to guide the interactive transcription process.</abstract>
      <url hash="8519e14c">2022.fieldmatters-1.1</url>
      <bibkey>lane-bird-2022-finite</bibkey>
    </paper>
    <paper id="2">
      <title>Corpus-Guided Contrast Sets for Morphosyntactic Feature Detection in Low-Resource <fixed-case>E</fixed-case>nglish Varieties</title>
      <author><first>Tessa</first><last>Masis</last></author>
      <author><first>Anissa</first><last>Neal</last></author>
      <author><first>Lisa</first><last>Green</last></author>
      <author><first>Brendan</first><last>O’Connor</last></author>
      <pages>11–25</pages>
      <abstract>The study of language variation examines how language varies between and within different groups of speakers, shedding light on how we use language to construct identities and how social contexts affect language use. A common method is to identify instances of a certain linguistic feature - say, the zero copula construction - in a corpus, and analyze the feature’s distribution across speakers, topics, and other variables, to either gain a qualitative understanding of the feature’s function or systematically measure variation. In this paper, we explore the challenging task of automatic morphosyntactic feature detection in low-resource English varieties. We present a human-in-the-loop approach to generate and filter effective contrast sets via corpus-guided edits. We show that our approach improves feature detection for both Indian English and African American English, demonstrate how it can assist linguistic research, and release our fine-tuned models for use by other researchers.</abstract>
      <url hash="fa693f7e">2022.fieldmatters-1.2</url>
      <bibkey>masis-etal-2022-corpus</bibkey>
      <pwccode url="https://github.com/slanglab/cgedit" additional="false">slanglab/cgedit</pwccode>
    </paper>
    <paper id="3">
      <title>Machine Translation Between High-resource Languages in a Language Documentation Setting</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Abteen</first><last>Ebrahimi</last></author>
      <author><first>Kristine</first><last>Stenzel</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <pages>26–33</pages>
      <abstract>Language documentation encompasses translation, typically into the dominant high-resource language in the region where the target language is spoken. To make data accessible to a broader audience, additional translation into other high-resource languages might be needed. Working within a project documenting Kotiria, we explore the extent to which state-of-the-art machine translation (MT) systems can support this second translation – in our case from Portuguese to English. This translation task is challenging for multiple reasons: (1) the data is out-of-domain with respect to the MT system’s training data, (2) much of the data is conversational, (3) existing translations include non-standard and uncommon expressions, often reflecting properties of the documented language, and (4) the data includes borrowings from other regional languages. Despite these challenges, existing MT systems perform at a usable level, though there is still room for improvement. We then conduct a qualitative analysis and suggest ways to improve MT between high-resource languages in a language documentation setting.</abstract>
      <url hash="e58d57ef">2022.fieldmatters-1.3</url>
      <bibkey>kann-etal-2022-machine</bibkey>
    </paper>
    <paper id="4">
      <title>Automatic Detection of Borrowings in Low-Resource Languages of the <fixed-case>C</fixed-case>aucasus: <fixed-case>A</fixed-case>ndic branch</title>
      <author><first>Konstantin</first><last>Zaitsev</last></author>
      <author><first>Anzhelika</first><last>Minchenko</last></author>
      <pages>34–41</pages>
      <abstract>Linguistic borrowings occur in all languages. Andic languages of the Caucasus have borrowings from different donor-languages like Russian, Arabic, Persian. To automatically detect these borrowings, we propose a logistic regression model. The model was trained on the dataset which contains words in IPA from dictionaries of Andic languages. To improve model’s quality, we compared TfIdf and Count vectorizers and chose the second one. Besides, we added new features to the model. They were extracted using analysis of vectorizer features and using a language model. The model was evaluated by classification quality metrics (precision, recall and F1-score). The best average F1-score of all languages for words in IPA was about 0.78. Experiments showed that our model reaches good results not only with words in IPA but also with words in Cyrillic.</abstract>
      <url hash="4a60f233">2022.fieldmatters-1.4</url>
      <bibkey>zaitsev-minchenko-2022-automatic</bibkey>
    </paper>
    <paper id="5">
      <title>The interaction between cognitive ease and informativeness shapes the lexicons of natural languages</title>
      <author><first>Thomas</first><last>Brochhagen</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <pages>42–44</pages>
      <abstract>It is common for languages to express multiple meanings with the same word, a phenomenon known as colexification. For instance, the meanings FINGER and TOE colexify in the word “dedo” in Spanish, while they do not colexify in English. Colexification has been suggested to follow universal constraints. In particular, previous work has shown that related meanings are more prone to colexify. This tendency has been explained in terms of the cognitive pressure for ease, since expressing related meanings with the same word makes lexicons easier to learn and use. The present study examines the interplay between this pressure and a competing universal constraint, the functional pressure for languages to maximize informativeness. We hypothesize that meanings are more likely to colexify if they are related (fostering ease), but not so related as to become confusable and cause misunderstandings (fostering informativeness). We find support for this principle in data from over 1200 languages and 1400 meanings. Our results thus suggest that universal principles shape the lexicons of natural languages. More broadly, they contribute to the growing body of evidence suggesting that languages evolve to strike a balance between competing functional and cognitive pressures.</abstract>
      <url hash="95beeafd">2022.fieldmatters-1.5</url>
      <bibkey>brochhagen-boleda-2022-interaction-cognitive</bibkey>
    </paper>
    <paper id="6">
      <title>The first neural machine translation system for the <fixed-case>E</fixed-case>rzya language</title>
      <author><first>David</first><last>Dale</last></author>
      <pages>45–53</pages>
      <abstract>We present the first neural machine translation system for translation between the endangered Erzya language and Russian and the dataset collected by us to train and evaluate it. The BLEU scores are 17 and 19 for translation to Erzya and Russian respectively, and more than half of the translations are rated as acceptable by native speakers. We also adapt our model to translate between Erzya and 10 other languages, but without additional parallel data, the quality on these directions remains low. We release the translation models along with the collected text corpus, a new language identification model, and a multilingual sentence encoder adapted for the Erzya language. These resources will be available at <url>https://github.com/slone-nlp/myv-nmt</url>.</abstract>
      <url hash="6b5a4e68">2022.fieldmatters-1.6</url>
      <bibkey>dale-2022-first</bibkey>
      <pwccode url="https://github.com/slone-nlp/myv-nmt" additional="false">slone-nlp/myv-nmt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccmatrix">CCMatrix</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>A</fixed-case>bui <fixed-case>W</fixed-case>ordnet: Using a Toolbox Dictionary to develop a wordnet for a low-resource language</title>
      <author><first>Frantisek</first><last>Kratochvil</last></author>
      <author><first>Luís</first><last>Morgado da Costa</last></author>
      <pages>54–63</pages>
      <abstract>This paper describes a procedure to link a Toolbox dictionary of a low-resource language to correct synsets, generating a new wordnet. We introduce a bootstrapping technique utilising the information in the gloss fields (English, national, and regional) to generate sense candidates using a naive algorithm based on multilingual sense intersection. We show that this technique is quite effective when glosses are available in more than one language. Our technique complements the previous work by Rosman et al. (2014) which linked the SIL Semantic Domains to wordnet senses. Through this work we have created a small, fully hand-checked wordnet for Abui, containing over 1,400 concepts and 3,600 senses.</abstract>
      <url hash="99391264">2022.fieldmatters-1.7</url>
      <bibkey>kratochvil-morgado-da-costa-2022-abui</bibkey>
    </paper>
    <paper id="8">
      <title>How to encode arbitrarily complex morphology in word embeddings, no corpus needed</title>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Coleman</first><last>Haley</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>64–76</pages>
      <abstract>In this paper, we present a straightforward technique for constructing interpretable word embeddings from morphologically analyzed examples (such as interlinear glosses) for all of the world’s languages. Currently, fewer than 300-400 languages out of approximately 7000 have have more than a trivial amount of digitized texts; of those, between 100-200 languages (most in the Indo-European language family) have enough text data for BERT embeddings of reasonable quality to be trained. The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics. We demonstrate the applicability of our representation through examples drawn from a typologically diverse set of languages whose morphology includes prefixes, suffixes, infixes, circumfixes, templatic morphemes, derivational morphemes, inflectional morphemes, and reduplication.</abstract>
      <url hash="ed78aec0">2022.fieldmatters-1.8</url>
      <bibkey>schwartz-etal-2022-encode</bibkey>
    </paper>
    <paper id="9">
      <title>Predictive Text for Agglutinative and Polysynthetic Languages</title>
      <author><first>Sergey</first><last>Kosyak</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>77–85</pages>
      <abstract>This paper presents a set of experiments in the area of morphological modelling and prediction. We test whether morphological segmentation can compete against statistical segmentation in the tasks of language modelling and predictive text entry for two under-resourced and indigenous languages, K’iche’ and Chukchi. We use different segmentation methods — both statistical and morphological — to make datasets that are used to train models of different types: single-way segmented, which are trained using data from one segmenter; two-way segmented, which are trained using concatenated data from two segmenters; and finetuned, which are trained on two datasets from different segmenters. We compute word and character level perplexities and find that single-way segmented models trained on morphologically segmented data show the highest performance. Finally, we evaluate the language models on the task of predictive text entry using gold standard data and measure the average number of clicks per character and keystroke savings rate. We find that the models trained on morphologically segmented data show better scores, although with substantial room for improvement. At last, we propose the usage of morphological segmentation in order to improve the end-user experience while using predictive text and we plan on testing this assumption by doing end-user evaluation.</abstract>
      <url hash="38f51546">2022.fieldmatters-1.9</url>
      <bibkey>kosyak-tyers-2022-predictive</bibkey>
    </paper>
  </volume>
</collection>
