<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.americasnlp">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fifth Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)</booktitle>
      <editor><first>Manuel</first><last>Mager</last></editor>
      <editor><first>Abteen</first><last>Ebrahimi</last></editor>
      <editor><first>Robert</first><last>Pugh</last></editor>
      <editor><first>Shruti</first><last>Rijhwani</last></editor>
      <editor><first>Katharina</first><last>Von Der Wense</last></editor>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Rolando</first><last>Coto-Solano</last></editor>
      <editor><first>Arturo</first><last>Oncevay</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="c1e25380">2025.americasnlp-1</url>
      <venue>americasnlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-236-7</isbn>
    </meta>
    <frontmatter>
      <url hash="1ad0ed9d">2025.americasnlp-1.0</url>
      <bibkey>americasnlp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Text-to-speech system for low-resource languages: A case study in <fixed-case>S</fixed-case>hipibo-Konibo (a <fixed-case>P</fixed-case>anoan language from <fixed-case>P</fixed-case>eru)</title>
      <author><first>Daniel</first><last>Menendez</last><affiliation>Pontificia Universidad Catolica del Peru</affiliation></author>
      <author><first>Hector</first><last>Gomez</last><affiliation>Pontificia Universidad Católica del Perú</affiliation></author>
      <pages>1-7</pages>
      <abstract>This paper presents the design and development of a Text-to-Speech (TTS) model for Shipibo-Konibo, a low-resource indigenous language spoken mainly in the Peruvian Amazon. Despite the challenge posed by the scarcity of data, the model was trained with over 4 hours of recordings and 3,025 meticulously collected written sentences. The tests results demon strated an intelligibility rate (IR) exceeding 88% and a mean opinion score (MOS) of 4.01, confirming the quality of the audio generated by the model, which comprises the Tacotron 2 spectrogram predictor and the HiFi-GAN vocoder. Furthermore, the potential of this model to be trained in other indigenous languages spoken in Peru is highlighted, opening a promising avenue for the documentation and revitalization of these languages.</abstract>
      <url hash="4badc5b2">2025.americasnlp-1.1</url>
      <bibkey>menendez-gomez-2025-text</bibkey>
    </paper>
    <paper id="2">
      <title>Does a code-switching dialogue system help users learn conversational fluency in <fixed-case>C</fixed-case>hoctaw?</title>
      <author><first>Jacqueline</first><last>Brixey</last><affiliation>USC Institute for Creative Technologies</affiliation></author>
      <author><first>David</first><last>Traum</last><affiliation>University of Southern California Institute for Creative Technologies</affiliation></author>
      <pages>8-17</pages>
      <abstract>We investigate the learning outcomes and user response to a chatbot for practicing conversational Choctaw, an endangered American Indigenous language. Conversational fluency is a goal for many language learners, however, for learners of endangered languages in North America, access to fluent speakers may be limited. Chatbots are potentially ideal dialogue partners as this kind of dialogue system fulfills a non-authoritative role by focusing on carrying on a conversation as an equal conversational partner. The goal of the chatbot investigated in this work is to serve as a conversational partner in the absence of a fluent Choctaw-speaking human interlocutor. We investigate the impact of code-switching in the interaction, comparing a bilingual chatbot against a monolingual Choctaw version. We evaluate the systems for user engagement and enjoyment, as well as gains in conversational fluency from interacting with the system.</abstract>
      <url hash="211fd825">2025.americasnlp-1.2</url>
      <bibkey>brixey-traum-2025-code</bibkey>
    </paper>
    <paper id="3">
      <title>A hybrid Approach to low-resource machine translation for <fixed-case>O</fixed-case>jibwe verbs</title>
      <author><first>Minh</first><last>Nguyen</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Christopher</first><last>Hammerly</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Miikka</first><last>Slifverberg</last><affiliation>Independent Researcher</affiliation></author>
      <pages>18-26</pages>
      <abstract>Machine translation is a tool that can help teachers, learners, and users of low-resourced languages. However, there are significant challenges in developing these tools, such as the lack of large-scale parallel corpora and complex morphology. We propose a novel hybrid system that combines LLM and rule-based methods in two distinct stages to translate inflected Ojibwe verbs into English. We use an LLM to automatically annotate dictionary data to build translation templates. Then, our rulebased module performs translation using inflection and slot-filling processes built on top of an FST-based analyzer. We test the system with a set of automated tests. Thanks to the ahead-of-time nature of the template-building process and the light-weight rule-based translation module, the end-to-end translation process has an average translation speed of 70 milliseconds per word. The system achieved an average ChrF score of 0.82 and a semantic similarity score of 0.93 among the successfully translated verbs in a test set. The approach has the potential to be extended to other low-resource Indigenous languages with dictionary data.</abstract>
      <url hash="30cdb5b8">2025.americasnlp-1.3</url>
      <bibkey>nguyen-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="4">
      <title>Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered <fixed-case>C</fixed-case>omanche Language</title>
      <author><first>Jesus</first><last>Alvarez C</last><affiliation>College of the Canyons</affiliation></author>
      <author><first>Daua</first><last>Karajeanes</last><affiliation>Eindhoven University Of Technolgy</affiliation></author>
      <author><first>Ashley</first><last>Prado</last><affiliation>Student</affiliation></author>
      <author><first>John</first><last>Ruttan</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Ivory</first><last>Yang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Sean</first><last>O’brien</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse</affiliation></author>
      <pages>27-37</pages>
      <abstract>The digital exclusion of endangered languages remains a critical challenge in NLP, limiting both linguistic research and revitalization efforts. This study introduces the first computational investigation of Comanche, an Uto-Aztecan language on the verge of extinction, demonstrating how minimal-cost, community-informed NLP interventions can support language preservation. We present a manually curated dataset of 412 phrases, a synthetic data generation pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language identification. Our experiments reveal that while LLMs struggle with Comanche in zero-shot settings, few-shot prompting significantly improves performance, achieving near-perfect accuracy with just five examples. Our findings highlight the potential of targeted NLP methodologies in low-resource contexts and emphasize that visibility is the first step toward inclusion. By establishing a foundation for Comanche in NLP, we advocate for computational approaches that prioritize accessibility, cultural sensitivity, and community engagement.</abstract>
      <url hash="c8236111">2025.americasnlp-1.4</url>
      <bibkey>alvarez-c-etal-2025-advancing</bibkey>
    </paper>
    <paper id="5">
      <title>Py-Elotl: A Python <fixed-case>NLP</fixed-case> package for the languages of <fixed-case>M</fixed-case>exico</title>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Victor</first><last>Mijangos</last><affiliation>National Autonomous University of Mexico (UNAM)</affiliation></author>
      <author><first>Diego</first><last>Barriga Martínez</last><affiliation>UNAM</affiliation></author>
      <author><first>Paul</first><last>Aguilar</last><affiliation>SocialTIC</affiliation></author>
      <author><first>Mikel</first><last>Segura</last><affiliation>National Autonomous University of Mexico (UNAM)</affiliation></author>
      <author><first>Paola</first><last>Innes</last><affiliation>National Autonomous University of Mexico (UNAM)</affiliation></author>
      <author><first>Javier</first><last>Santillan</last><affiliation>Honeynet Project</affiliation></author>
      <author><first>Cynthia</first><last>Montaño</last><affiliation>University of Cailfornia, Berkeley</affiliation></author>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University</affiliation></author>
      <pages>38-47</pages>
      <abstract>This work presents Py-elotl, a suite of tools and resources in Python for processing text in several indigenous languages spoken in Mexico. These resources include parallel corpora, linguistic taggers/analyzers, and orthographic normalization tools. This work aims to develop essential resources to support language pre-processing and linguistic research, and the future creation of more complete downstream applications that could be useful for the speakers and enhance the visibility of these languages. The current version supports language groups such as Nahuatl, Otomi, Mixtec, and Huave. This project is open-source and freely available for use and collaboration</abstract>
      <url hash="63fb1786">2025.americasnlp-1.5</url>
      <bibkey>gutierrez-vasques-etal-2025-py</bibkey>
    </paper>
    <paper id="6">
      <title>Analyzing and generating <fixed-case>E</fixed-case>nglish phrases with finite-state methods to match and translate inflected <fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree word-forms</title>
      <author><first>Antti</first><last>Arppe</last><affiliation>University of Alberta</affiliation></author>
      <pages>48-62</pages>
      <abstract>This paper presents two finite-state transducer tools, which can be used to analyze or generate simple English verb and noun phrases, that can be mapped with inflected Plains Cree (nêhiyawêwin) verb and noun forms. These tools support fetching an inflected Cree word-form directly with an appropriate plain English phrase, and conversely providing a rough translation of an inflected Cree word-form. Such functionalities can be used to improve the user friendliness of on-line dictionaries. The tools are extendable to other similarly morphologically complex languages.</abstract>
      <url hash="9b12c796">2025.americasnlp-1.6</url>
      <bibkey>arppe-2025-analyzing</bibkey>
    </paper>
    <paper id="7">
      <title>Unsupervised, Semi-Supervised and <fixed-case>LLM</fixed-case>-Based Morphological Segmentation for <fixed-case>B</fixed-case>ribri</title>
      <author><first>Carter</first><last>Anderson</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Mien</first><last>Nguyen</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <pages>63-76</pages>
      <abstract>Morphological Segmentation is a major task in Indigenous language documentation. In this paper we (a) introduce a novel statistical algorithm called Morphemo to split words into their constituent morphemes. We also (b) study how large language models perform on this task. We use these tools to analyze Bribri, an under-resourced Indigenous language from Costa Rica. Morphemo has better performance than the LLM when splitting multimorphemic words, mainly because the LLMs are more conservative, which also gives them an advantage when splitting monomorphemic words. In future work we will use these tools to tag Bribri language corpora, which currently lack morphological segmentation.</abstract>
      <url hash="69738e8a">2025.americasnlp-1.7</url>
      <bibkey>anderson-etal-2025-unsupervised</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>FUSE</fixed-case> : A Ridge and Random Forest-Based Metric for Evaluating <fixed-case>MT</fixed-case> in Indigenous Languages</title>
      <author><first>Rahul</first><last>Raja</last><affiliation>Linkedin</affiliation></author>
      <author><first>Arpita</first><last>Vats</last><affiliation>Santa Clara University</affiliation></author>
      <pages>77-83</pages>
      <abstract>This paper presents the winning submission of the RaaVa team to the AmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine Translation (MT) into Indigenous Languages of America, where our system ranked first overall based on average Pearson correlation with the human annotations. We introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge regression and Gradient Boosting to model translation quality. In addition to FUSE, we explore five alternative approaches leveraging different combinations of linguistic similarity features and learning paradigms. FUSE Score highlights the effectiveness of combining lexical, phonetic, semantic, and fuzzy token similarity with learning-based modeling to improve MT evaluation for morphologically rich and low-resource languages. MT into Indigenous languages poses unique challenges due to polysynthesis, complex morphology, and non-standardized orthography. Conventional automatic metrics such as BLEU, TER, and ChrF often fail to capture deeper aspects like semantic adequacy and fluency. Our proposed framework, formerly referred to as FUSE, incorporates multilingual sentence embeddings and phonological encodings to better align with human evaluation. We train supervised models on human-annotated development sets and evaluate held-out test data. Results show that FUSE consistently achieves higher Pearson and Spearman correlations with human judgments, offering a robust and linguistically informed solution for MT evaluation in low-resource settings.</abstract>
      <url hash="2f8c6d14">2025.americasnlp-1.8</url>
      <bibkey>raja-vats-2025-fuse</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>UCSP</fixed-case> Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2025 Shared Task</title>
      <author><first>Jorge</first><last>Asillo Congora</last><affiliation>Universidad Católica San Pablo</affiliation></author>
      <author><first>Julio</first><last>Santisteban</last><affiliation>San Pablo Catholic University (UCSP)</affiliation></author>
      <author><first>Ricardo</first><last>Lazo Vasquez</last><affiliation>Universidad Católica San Pablo</affiliation></author>
      <pages>84-91</pages>
      <abstract>Quechua is a low-resource language spoken by more than 7 million people in South America. While Quechua is primarily an oral language, several orthographic standards do exist. There is no universally adopted writing standard for Quechua, and variations exist across dialects and regions; its current writing is based on how it is uttered and how the sound is written. Quechua is a family of languages with similarities among the seven variants. The lack of a parallel dataset has reduced the opportunities for developing machine translation. We investigated whether increasing the current Quechua Parallel dataset with synthetic sentences and using a pre-trained large language model improves the performance of a Quechua machine translation. A Large language model has been used to generate synthetic sentences to extend the current parallel dataset. We use the mt5 model to fine-tune it to develop a machine translation for Quechua to Spanish and vice versa. Our survey identified the gaps in the state of the art of Quechua machine translation, and our BLEU/Chrf++ results show an improvement over the state of the art.</abstract>
      <url hash="e1975e1d">2025.americasnlp-1.9</url>
      <bibkey>asillo-congora-etal-2025-ucsp</bibkey>
    </paper>
    <paper id="10">
      <title>Machine Translation Using Grammar Materials for <fixed-case>LLM</fixed-case> Post-Correction</title>
      <author><first>Jonathan</first><last>Hus</last><affiliation>GMU</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <author><first>Nathaniel</first><last>Krasner</last><affiliation>George Mason University</affiliation></author>
      <pages>92-99</pages>
      <abstract>This paper describes George Mason University’s submission to the AmericasNLP 2025 Shared Task on Machine Translation into Indigenous Languages. We prompt a large language model (LLM) with grammar reference materials to correct the translations produced by a finetuned Encoder-Decoder machine translation system. This system leads to improvements when translating from the indigenous languages into Spanish indicating that LLMs are capable of using grammar materials to decipher an unseen language.</abstract>
      <url hash="03d131a2">2025.americasnlp-1.10</url>
      <bibkey>hus-etal-2025-machine</bibkey>
    </paper>
    <paper id="11">
      <title>Machine Translation Metrics for Indigenous Languages Using Fine-tuned Semantic Embeddings</title>
      <author><first>Nathaniel</first><last>Krasner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Belu</first><last>Ticona</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <author><first>Chi-Kiu</first><last>Lo</last><affiliation>National Research Council of Canada</affiliation></author>
      <pages>100-104</pages>
      <abstract>This paper describes the Tekio submission to the AmericasNLP 2025 shared task on machine translation metrics for Indigenous languages. We developed two primary metric approaches leveraging multilingual semantic embeddings. First, we fine-tuned the Language-agnostic BERT Sentence Encoder (LaBSE) specifically for Guarani, Bribri, and Nahuatl, significantly enhancing semantic representation quality. Next, we integrated our fine-tuned LaBSE into the semantic similarity metric YiSi-1, exploring the effectiveness of averaging multiple layers. Additionally, we trained regression-based COMET metrics (COMET-DA) using the fine-tuned LaBSE embeddings as a semantic backbone, comparing Mean Absolute Error (MAE) and Mean Squared Error (MSE) loss functions. Our YiSi-1 metric using layer-averaged embeddings chosen by having the best performance on the development set for each individual language achieved the highest average correlation across languages among our submitted systems, and our COMET models demonstrated competitive performance for Guarani.</abstract>
      <url hash="e459e97a">2025.americasnlp-1.11</url>
      <bibkey>krasner-etal-2025-machine</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>JHU</fixed-case>’s Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2025 Shared Task on the Creation of Educational Materials for Indigenous Languages</title>
      <author><first>Tom</first><last>Lupicki</last><affiliation>Center for Language and Speech Processing, Johns Hopkins University</affiliation></author>
      <author><first>Lavanya</first><last>Shankar</last><affiliation>Center for Language and Speech Processing, Johns Hopkins University</affiliation></author>
      <author><first>Kaavya</first><last>Chaparala</last><affiliation>Center for Language and Speech Processing, Johns Hopkins University</affiliation></author>
      <author><first>David</first><last>Yarowsky</last><affiliation>Center for Language and Speech Processing, Johns Hopkins University</affiliation></author>
      <pages>105-111</pages>
      <abstract>This paper presents JHU’s submission to the AmericasNLP shared task on the creation of educational materials for Indigenous languages. The task involves transforming a base sentence given one or more tags that correspond to grammatical features, such as negation or tense. The task also spans four languages: Bribri, Maya, Guaraní, and Nahuatl. We experiment with augmenting prompts to large language models with different information, chain of thought prompting, ensembling large language models by majority voting, and training a pointer-generator network. Our System 1, an ensemble of large language models, achieves the best performance on Maya and Guaraní, building upon the previous successes in leveraging large language models for this task and highlighting the effectiveness of ensembling large language models.</abstract>
      <url hash="af344a41">2025.americasnlp-1.12</url>
      <bibkey>lupicki-etal-2025-jhus</bibkey>
    </paper>
    <paper id="13">
      <title>Leveraging Dictionaries and Grammar Rules for the Creation of Educational Materials for Indigenous Languages</title>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Haruki</first><last>Sakajo</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Arturo</first><last>Martínez Peguero</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Frederikus</first><last>Hudi</last><affiliation>Nara Institute of Science and Technology, WAP Tokushima Lab. of AI&amp;NLP</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>112-118</pages>
      <abstract>This paper describes the NAIST submission to the AmericasNLP 2025 shared task on the creation of educational materials for Indigenous languages. We implement three systems to tackle the unique challenges of each language. The first system, used for Maya and Guarani, employs a straightforward GPT-4o few-shot prompting technique, enhanced by synthetically generated examples to ensure coverage of all grammatical variations encountered. The second system, used for Bribri, integrates dictionary-based alignment and linguistic rules to systematically manage linguisticand lexical transformations. Finally, we developed a specialized rule-based system for Nahuatl that systematically reduces sentences to their base form, simplifying the generation of correct morphology variants.</abstract>
      <url hash="ba356e33">2025.americasnlp-1.13</url>
      <bibkey>vasselli-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="14">
      <title>Harnessing <fixed-case>NLP</fixed-case> for Indigenous Language Education: Fine-Tuning Large Language Models for Sentence Transformation</title>
      <author><first>Mahshar</first><last>Yahan</last><affiliation>Lecturer</affiliation></author>
      <author><first>Dr. Mohammad</first><last>Islam</last><affiliation>Assistant Professor</affiliation></author>
      <pages>119-125</pages>
      <abstract>Indigenous languages face significant challenges due to their endangered status and limited resources which makes their integration into NLP systems difficult. This study investigates the use of Large Language Models (LLMs) for sentence transformation tasks in Indigenous languages, focusing on Bribri, Guarani, and Maya. Here, the dataset from the AmericasNLP 2025 Shared Task 2 is used to explore sentence transformations in Indigenous languages. The goal is to create educational tools by modifying sentences based on linguistic instructions, such as changes in tense, aspect, voice, person, and other grammatical features. The methodology involves preprocessing data, simplifying transformation tags, and designing zero-shot and few-shot prompts to guide LLMs in sentence rewriting. Fine-tuning techniques like LoRA and Bits-and-Bytes quantization were employed to optimize model performance while reducing computational costs. Among the tested models, Llama 3.2(3B-Instruct) demonstrated superior performance across all languages with high BLEU and ChrF++ scores, particularly excelling in few-shot settings. The Llama 3.2 model achieved BLEU scores of 19.51 for Bribri, 13.67 for Guarani, and 55.86 for Maya in test settings. Additionally, ChrF++ scores reached 50.29 for Bribri, 58.55 for Guarani, and 80.12 for Maya, showcasing its effectiveness in handling sentence transformation. These results highlight the potential of LLMs that can improve NLP tools for indigenous languages and help preserve linguistic diversity.</abstract>
      <url hash="308b6062">2025.americasnlp-1.14</url>
      <bibkey>yahan-islam-2025-harnessing</bibkey>
    </paper>
    <paper id="15">
      <title>Leveraging Large Language Models for <fixed-case>S</fixed-case>panish-Indigenous Language Machine Translation at <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2025</title>
      <author><first>Mahshar</first><last>Yahan</last><affiliation>Lecturer</affiliation></author>
      <author><first>Dr. Mohammad</first><last>Islam</last><affiliation>Assistant Professor</affiliation></author>
      <pages>126-133</pages>
      <abstract>This paper presents our approach to machine translation between Spanish and 13 Indigenous languages of the Americas as part of the AmericasNLP 2025 shared task. Addressing the challenges of low-resource translation, we fine-tuned advanced multilingual models, including NLLB-200 (Distilled-600M), Llama 3.1 (8B-Instruct) and XGLM 1.7B, using techniques such as dynamic batching, token adjustments, and embedding initialization. Data preprocessing steps like punctuation removal and tokenization refinements were employed to achieve data generalization. While our models demonstrated strong performance for Awajun and Quechua translations, they struggled with morphologically complex languages like Nahuatl and Otomí. Our approach achieved competitive ChrF++ scores for Awajun (35.16) and Quechua (31.01) in the Spanish-to-Indigenous translation track (Es→Xx). Similarly, in the Indigenous-to-Spanish track (Xx→Es), we obtained ChrF++ scores of 33.70 for Awajun and 31.71 for Quechua. These results underscore the potential of tailored methodologies in preserving linguistic diversity while advancing machine translation for endangered languages.</abstract>
      <url hash="24919d13">2025.americasnlp-1.15</url>
      <bibkey>yahan-islam-2025-leveraging</bibkey>
    </paper>
    <paper id="16">
      <title>Findings of the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2025 Shared Tasks on Machine Translation, Creation of Educational Material, and Translation Metrics for Indigenous Languages of the <fixed-case>A</fixed-case>mericas</title>
      <author><first>Ona</first><last>De Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Ali</first><last>Marashian</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Raul</first><last>Vazquez</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Abteen</first><last>Ebrahimi</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Pavel</first><last>Denisov</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Enora</first><last>Rice</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Edward</first><last>Gow-Smith</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Juan</first><last>Prieto</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Melissa</first><last>Robles</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Rubén</first><last>Manrique</last><affiliation>Universidad de los Andes</affiliation></author>
      <author><first>Oscar</first><last>Moreno</last><affiliation>Pontifical Catholic University of Peru</affiliation></author>
      <author><first>Angel</first><last>Lino</last><affiliation>Pontificia Universidad Católica del Perú</affiliation></author>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Aldo</first><last>Alvarez</last><affiliation>Universidad Nacional de Itapua</affiliation></author>
      <author><first>Marvin</first><last>Agüero-Torales</last><affiliation>Fujitsu</affiliation></author>
      <author><first>John E.</first><last>Ortega</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la Republica</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>JPMorgan</affiliation></author>
      <author><first>Shruti</first><last>Rijhwani</last><affiliation>Google</affiliation></author>
      <author><first>Katharina</first><last>Von Der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Manuel</first><last>Mager</last><affiliation>Amazon AWS</affiliation></author>
      <pages>134-152</pages>
      <abstract>This paper presents the findings of the AmericasNLP 2025 Shared Tasks: (1) machine translation for truly low-resource languages, (2) morphological adaptation for generating educational examples, and (3) developing metrics for machine translation in Indigenous languages. The shared tasks cover 14 diverse Indigenous languages of the Americas. A total of 11 teams participated, submitting 26 systems across all tasks, languages, and models. We describe the shared tasks, introduce the datasets and evaluation metrics used, summarize the baselines and submitted systems, and report our findings.</abstract>
      <url hash="03e9c348">2025.americasnlp-1.16</url>
      <bibkey>de-gibert-etal-2025-findings</bibkey>
    </paper>
  </volume>
</collection>
