<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.econlp">
  <volume id="1" ingest-date="2021-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Economics and Natural Language Processing</booktitle>
      <editor><first>Udo</first><last>Hahn</last></editor>
      <editor><first>Veronique</first><last>Hoste</last></editor>
      <editor><first>Amanda</first><last>Stent</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <venue>econlp</venue>
    </meta>
    <frontmatter>
      <url hash="4e91a557">2021.econlp-1.0</url>
      <bibkey>econlp-2021-economics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Fine-Grained Annotated Corpus for Target-Based Opinion Analysis of Economic and Financial Narratives</title>
      <author><first>Jiahui</first><last>Hu</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>1–12</pages>
      <abstract>In this paper about aspect-based sentiment analysis (ABSA), we present the first version of a fine-grained annotated corpus for target-based opinion analysis (TBOA) to analyze economic activities or financial markets. We have annotated, at an intra-sentential level, a corpus of sentences extracted from documents representative of financial analysts’ most-read materials by considering how financial actors communicate about the evolution of event trends and analyze related publications (news, official communications, etc.). Since we focus on identifying the expressions of opinions related to the economy and financial markets, we annotated the sentences that contain at least one subjective expression about a domain-specific term. Candidate sentences for annotations were randomly chosen from texts of specialized press and professional information channels over a period ranging from 1986 to 2021. Our annotation scheme relies on various linguistic markers like domain-specific vocabulary, syntactic structures, and rhetorical relations to explicitly describe the author’s subjective stance. We investigated and evaluated the recourse to automatic pre-annotation with existing natural language processing technologies to alleviate the annotation workload. Our aim is to propose a corpus usable on the one hand as training material for the automatic detection of the opinions expressed on an extensive range of domain-specific aspects and on the other hand as a gold standard for evaluation TBOA. In this paper, we present our pre-annotation models and evaluations of their performance, introduce our annotation scheme and report on the main characteristics of our corpus.</abstract>
      <url hash="06570a64">2021.econlp-1.1</url>
      <bibkey>hu-paroubek-2021-fine</bibkey>
      <doi>10.18653/v1/2021.econlp-1.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL 2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval-2014 Task-4</pwcdataset>
    </paper>
    <paper id="2">
      <title><fixed-case>EDGAR</fixed-case>-<fixed-case>CORPUS</fixed-case>: Billions of Tokens Make The World Go Round</title>
      <author><first>Lefteris</first><last>Loukas</last></author>
      <author><first>Manos</first><last>Fergadiotis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <pages>13–18</pages>
      <abstract>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP corpus available to date. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use JSON format. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC embeddings for the financial domain. We employ these embeddings in a battery of financial NLP tasks and showcase their superiority over generic GloVe embeddings and other existing financial word embeddings. We also open-source EDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future annual reports.</abstract>
      <url hash="7cc0391b">2021.econlp-1.2</url>
      <bibkey>loukas-etal-2021-edgar</bibkey>
      <doi>10.18653/v1/2021.econlp-1.2</doi>
      <video href="2021.econlp-1.2.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/edgar-corpus">EDGAR-CORPUS</pwcdataset>
    </paper>
    <paper id="3">
      <title>The Global Banking Standards <fixed-case>QA</fixed-case> Dataset (<fixed-case>GBS</fixed-case>-<fixed-case>QA</fixed-case>)</title>
      <author><first>Kyunghwan</first><last>Sohn</last></author>
      <author><first>Sunjae</first><last>Kwon</last></author>
      <author><first>Jaesik</first><last>Choi</last></author>
      <pages>19–25</pages>
      <abstract>A domain specific question answering (QA) dataset dramatically improves the machine comprehension performance. This paper presents a new Global Banking Standards QA dataset (GBS-QA) in the banking regulation domain. The GBS-QA has three values. First, it contains actual questions from market players and answers from global rule setter, the Basel Committee on Banking Supervision (BCBS) in the middle of creating and revising banking regulations. Second, financial regulation experts analyze and verify pairs of questions and answers in the annotation process. Lastly, the GBS-QA is a totally different dataset with existing datasets in finance and is applicable to stimulate transfer learning research in the banking regulation domain.</abstract>
      <url hash="cc80d688">2021.econlp-1.3</url>
      <bibkey>sohn-etal-2021-global</bibkey>
      <doi>10.18653/v1/2021.econlp-1.3</doi>
      <video href="2021.econlp-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Corporate Bankruptcy Prediction with Domain-Adapted <fixed-case>BERT</fixed-case></title>
      <author><first>Alex Gunwoo</first><last>Kim</last></author>
      <author><first>Sangwon</first><last>Yoon</last></author>
      <pages>26–36</pages>
      <abstract>This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&amp;A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.</abstract>
      <url hash="570b9d58">2021.econlp-1.4</url>
      <bibkey>kim-yoon-2021-corporate</bibkey>
      <doi>10.18653/v1/2021.econlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Is Domain Adaptation Worth Your Investment? Comparing <fixed-case>BERT</fixed-case> and <fixed-case>F</fixed-case>in<fixed-case>BERT</fixed-case> on Financial Tasks</title>
      <author><first>Bo</first><last>Peng</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Yu-Yin</first><last>Hsu</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>37–44</pages>
      <abstract>With the recent rise in popularity of Transformer models in Natural Language Processing, research efforts have been dedicated to the development of domain-adapted versions of BERT-like architectures. In this study, we focus on FinBERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pretraining from scratch, conversely, seems to be less effective.</abstract>
      <url hash="d789343a">2021.econlp-1.5</url>
      <bibkey>peng-etal-2021-domain</bibkey>
      <doi>10.18653/v1/2021.econlp-1.5</doi>
      <video href="2021.econlp-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>From Stock Prediction to Financial Relevance: Repurposing Attention Weights to Assess News Relevance Without Manual Annotations</title>
      <author><first>Luciano</first><last>Del Corro</last></author>
      <author><first>Johannes</first><last>Hoffart</last></author>
      <pages>45–49</pages>
      <abstract>We present a method to automatically identify financially relevant news using stock price movements and news headlines as input. The method repurposes the attention weights of a neural network initially trained to predict stock prices to assign a relevance score to each headline, eliminating the need for manually labeled training data. Our experiments on the four most relevant US stock indices and 1.5M news headlines show that the method ranks relevant news highly, positively correlated with the accuracy of the initial stock price prediction task.</abstract>
      <url hash="21b91a9c">2021.econlp-1.6</url>
      <bibkey>del-corro-hoffart-2021-stock</bibkey>
      <doi>10.18653/v1/2021.econlp-1.6</doi>
      <video href="2021.econlp-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning</title>
      <author><first>Priyam</first><last>Basu</last></author>
      <author><first>Tiasa Singha</first><last>Roy</last></author>
      <author><first>Rakshit</first><last>Naidu</last></author>
      <author><first>Zumrut</first><last>Muftuoglu</last></author>
      <pages>50–55</pages>
      <abstract>Privacy is of primary importance when it comes to the Financial Domain as the data is highly confidential and no third party can be having access to it. Natural Language Processing (NLP) techniques can be applied for text classification and entity detection purposes in financial domains like customer feedback sentiment analysis, invoice entity detection, categorisation of financial documents by type etc. Due to the sensitive nature of such data, privacy measures need to be taken for handling and training large models with such data. In this work, we propose a contextualized transformer (BERT and RoBERTa) based text classification model integrated with privacy features like Differential Privacy (DP) and Federated Learning (FL). We present how to privately train NLP models and desirable privacy utility trade-offs and evaluate it on the Financial Phrase Bank dataset.</abstract>
      <url hash="558ba602">2021.econlp-1.7</url>
      <bibkey>basu-etal-2021-privacy</bibkey>
      <doi>10.18653/v1/2021.econlp-1.7</doi>
      <video href="2021.econlp-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Using Word Embedding to Reveal Monetary Policy Explanation Changes</title>
      <author><first>Akira</first><last>Matsui</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Emilio</first><last>Ferrara</last></author>
      <pages>56–61</pages>
      <abstract>Documents have been an essential tool of communication for governments to announce their policy operations. Most policy announcements have taken the form of text to inform their new policies or changes to the public. To understand such policymakers’ communication, many researchers exploit published policy documents. However, the methods well-used in other research domains such as sentiment analysis or topic modeling are not suitable for studying policy communications. Their training corpora and methods are not for policy documents where technical terminologies are used, and sentiment expressions are refrained. We leverage word embedding techniques to extract semantic changes in the monetary policy documents. Our empirical study shows that the policymaker uses different semantics according to the type of documents when they change their policy.</abstract>
      <url hash="4f784a70">2021.econlp-1.8</url>
      <bibkey>matsui-etal-2021-using</bibkey>
      <doi>10.18653/v1/2021.econlp-1.8</doi>
      <video href="2021.econlp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>To What Extent Can <fixed-case>E</fixed-case>nglish-as-a-Second Language Learners Read Economic News Texts?</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>62–68</pages>
      <abstract>In decision making in the economic field, an especially important requirement is to rapidly understand news to absorb ever-changing economic situations. Given that most economic news is written in English, the ability to read such information without waiting for a translation is particularly valuable in economics in contrast to other fields. In consideration of this issue, this research investigated the extent to which non-native English speakers are able to read economic news to make decisions accordingly – an issue that has been rarely addressed in previous studies. Using an existing standard dataset as training data, we created a classifier that automatically evaluates the readability of text with high accuracy for English learners. Our assessment of the readability of an economic news corpus revealed that most news texts can be read by intermediate English learners. We also found that in some cases, readability varies considerably depending on the knowledge of certain words specific to the economic field.</abstract>
      <url hash="fd7d735f">2021.econlp-1.9</url>
      <bibkey>ehara-2021-extent</bibkey>
      <doi>10.18653/v1/2021.econlp-1.9</doi>
      <video href="2021.econlp-1.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="10">
      <title>Effective Use of Graph Convolution Network and Contextual Sub-Tree for Commodity News Event Extraction</title>
      <author><first>Meisin</first><last>Lee</last></author>
      <author><first>Lay-Ki</first><last>Soon</last></author>
      <author><first>Eu-Gene</first><last>Siew</last></author>
      <pages>69–81</pages>
      <abstract>Event extraction in commodity news is a less researched area as compared to generic event extraction. However, accurate event extraction from commodity news is useful in abroad range of applications such as under-standing event chains and learning event-event relations, which can then be used for commodity price prediction. The events found in commodity news exhibit characteristics different from generic events, hence posing a unique challenge in event extraction using existing methods. This paper proposes an effective use of Graph Convolutional Networks(GCN) with a pruned dependency parse tree, termed contextual sub-tree, for better event ex-traction in commodity news. The event ex-traction model is trained using feature embed-dings from ComBERT, a BERT-based masked language model that was produced through domain-adaptive pre-training on a commodity news corpus. Experimental results show the efficiency of the proposed solution, which out-performs existing methods with F1 scores as high as 0.90. Furthermore, our pre-trained language model outperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles classification. For the goal of re-producibility, the code and trained models are made publicly available.</abstract>
      <url hash="a4d3e250">2021.econlp-1.10</url>
      <bibkey>lee-etal-2021-effective</bibkey>
      <doi>10.18653/v1/2021.econlp-1.10</doi>
      <video href="2021.econlp-1.10.mp4"/>
      <pwccode url="https://github.com/meisin/commodity-news-event-extraction" additional="false">meisin/commodity-news-event-extraction</pwccode>
    </paper>
    <paper id="11">
      <title>Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse</title>
      <author><first>Anna Paula</first><last>Pawlicka Maule</last></author>
      <author><first>Kristen</first><last>Johnson</last></author>
      <pages>82–92</pages>
      <abstract>With 56 million people actively trading and investing in cryptocurrency online and globally in 2020, there is an increasing need for automatic social media analysis tools to help understand trading discourse and behavior. In this work, we present a dual natural language modeling pipeline which leverages language and social network behaviors for the prediction of cryptocurrency day trading actions and their associated framing patterns. This pipeline first predicts if tweets can be used to guide day trading behavior, specifically if a cryptocurrency investor should buy, sell, or hold their cryptocurrencies in order to make a profit. Next, tweets are input to an unsupervised deep clustering approach to automatically detect trading framing patterns. Our contributions include the modeling pipeline for this novel task, a new Cryptocurrency Tweets Dataset compiled from influential accounts, and a Historical Price Dataset. Our experiments show that our approach achieves an 88.78% accuracy for day trading behavior prediction and reveals framing fluctuations prior to and during the COVID-19 pandemic that could be used to guide investment actions.</abstract>
      <url hash="834d6ddc">2021.econlp-1.11</url>
      <attachment type="Software" hash="b88ae3e2">2021.econlp-1.11.Software.zip</attachment>
      <bibkey>pawlicka-maule-johnson-2021-cryptocurrency</bibkey>
      <doi>10.18653/v1/2021.econlp-1.11</doi>
      <video href="2021.econlp-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Extracting Economic Signals from Central Bank Speeches</title>
      <author><first>Maximilian</first><last>Ahrens</last></author>
      <author><first>Michael</first><last>McMahon</last></author>
      <pages>93–114</pages>
      <abstract>Estimating the effects of monetary policy is one of the fundamental research questions in monetary economics. Many economies are facing ultra-low interest rate environments ever since the global financial crisis of 2007-9. The Covid pandemic recently reinforced this situation. In the US and Europe, interest rates are close to (or even below) zero, which limits the scope of traditional monetary policy measures for central banks. Dedicated central bank communication has hence become an increasingly important tool to steer and control market expectations these days. However, incorporating central bank language directly as features into economic models is still a very nascent research area. In particular, the content and effect of central bank speeches has been mostly neglected from monetary policy modelling so far. With our paper, we aim to provide to the research community a novel, monetary policy shock series based on central bank speeches. We use a supervised topic modeling approach that can deal with text as well as numeric covariates to estimate a monetary policy signal dispersion index along three key economic dimensions: GDP, CPI and unemployment. This “dispersion shock” series is not only more frequent than series that classically focus on policy announcement dates, it also opens up the possibility of answering new questions that have up until now been difficult to analyse. For example, do markets form different expectations when facing a “cacophony of policy voices”? Our initial findings for the US point towards the fact that more dispersed or incongruent monetary policy stance communication in the build up to Federal Open Market Committee (FOMC) meetings might be associated with stronger subsequent market surprises at FOMC policy announcement time.</abstract>
      <url hash="7947ddd6">2021.econlp-1.12</url>
      <bibkey>ahrens-mcmahon-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.econlp-1.12</doi>
      <video href="2021.econlp-1.12.mp4"/>
    </paper>
  </volume>
</collection>
