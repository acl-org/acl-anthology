<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.findings">
  <volume id="naacl" ingest-date="2025-04-25" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: NAACL 2025</booktitle>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>April</month>
      <year>2025</year>
      <url hash="7a54f87b">2025.findings-naacl</url>
      <venue>findings</venue>
      <isbn>979-8-89176-195-7</isbn>
      <doi>10.18653/v1/2025.findings-naacl</doi>
    </meta>
    <frontmatter>
      <url hash="a93538d7">2025.findings-naacl.0</url>
      <bibkey>findings-2025-naacl</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.0</doi>
    </frontmatter>
    <paper id="1">
      <title>From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary Extreme Classification by Positive-Unlabeled Sequence Learning</title>
      <author><first>Ranran Haoran</first><last>Zhang</last></author>
      <author><first>Bensu</first><last>Uçar</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Soumik</first><last>Dey</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Hansi</first><last>Wu</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Binbin</first><last>Li</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1-16</pages>
      <url hash="be65fe89">2025.findings-naacl.1</url>
      <bibkey>zhang-etal-2025-lazy</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>D</fixed-case>iff<fixed-case>ZOO</fixed-case>: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization</title>
      <author><first>Pucheng</first><last>Dang</last></author>
      <author><first>Xing</first><last>Hu</last></author>
      <author><first>Dong</first><last>Li</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Institute of Computing Technology, CAS</affiliation></author>
      <author><first>Qi</first><last>Guo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>17-31</pages>
      <abstract>Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I model’s capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of purely black-box attacks without prior knowledge of the T2l model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T2I diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2l models.</abstract>
      <url hash="6d25059a">2025.findings-naacl.2</url>
      <bibkey>dang-etal-2025-diffzoo</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>M</fixed-case>ed<fixed-case>O</fixed-case>dyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200<fixed-case>K</fixed-case> Tokens</title>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Hongli</first><last>Sun</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>32-56</pages>
      <abstract>Numerous advanced Large Language Models (LLMs) now support context lengths up to 128K, and some extend to 200K. Some benchmarks in the generic domain have also followed up on evaluating long-context capabilities. In the medical domain, tasks are distinctive due to the unique contexts and need for domain expertise, necessitating further evaluation. However, despite the frequent presence of long texts in medical scenarios, evaluation benchmarks of long-context capabilities for LLMs in this field are still rare. In this paper, we propose MedOdyssey, the first medical long-context benchmark with seven length levels ranging from 4K to 200K tokens. MedOdyssey consists of two primary components: the medical-context “needles in a haystack” task and a series of tasks specific to medical applications, together comprising 10 datasets. The first component includes challenges such as counter-intuitive reasoning and novel (unknown) facts injection to mitigate knowledge leakage and data contamination of LLMs. The second component confronts the challenge of requiring professional medical expertise. Especially, we design the ‘“Maximum Identical Context” principle to improve fairness by guaranteeing that different LLMs observe as many identical contexts as possible. Our experiment evaluates advanced proprietary and open-source LLMs tailored for processing long contexts and presents detailed performance analyses. This highlights that LLMs still face challenges and need for further research in this area. Our code and data are released in the repository: <url>https://github.com/JOHNNY-fans/MedOdyssey</url>.</abstract>
      <url hash="7929ab46">2025.findings-naacl.3</url>
      <bibkey>fan-etal-2025-medodyssey</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.3</doi>
    </paper>
    <paper id="4">
      <title>Can <fixed-case>LLM</fixed-case>s Learn Macroeconomic Narratives from Social Media?</title>
      <author><first>Almog</first><last>Gueta</last><affiliation>Google</affiliation></author>
      <author><first>Amir</first><last>Feder</last><affiliation>Columbia University and Google</affiliation></author>
      <author><first>Zorik</first><last>Gekhman</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Ariel</first><last>Goldstein</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Roi</first><last>Reichart</last><affiliation>Technion, Israel Institute of Technology</affiliation></author>
      <pages>57-78</pages>
      <abstract>This study empirically tests the <tex-math>Narrative Economics</tex-math> hypothesis, which posits that narratives (ideas that are spread virally and affect public beliefs) can influence economic fluctuations. We introduce two curated datasets containing posts from X (formerly Twitter) which capture economy-related narratives (Data will be shared upon paper acceptance). Employing Natural Language Processing (NLP) methods, we extract and summarize narratives from the tweets. We test their predictive power for <tex-math>macroeconomic</tex-math> forecasting by incorporating the tweets’ or the extracted narratives’ representations in downstream financial prediction tasks. Our work highlights the challenges in improving macroeconomic models with narrative data, paving the way for the research community to realistically address this important challenge. From a scientific perspective, our investigation offers valuable insights and NLP tools for narrative extraction and summarization using Large Language Models (LLMs), contributing to future research on the role of narratives in economics.</abstract>
      <url hash="7e2299a8">2025.findings-naacl.4</url>
      <bibkey>gueta-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.4</doi>
    </paper>
    <paper id="5">
      <title>Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency</title>
      <author><first>Leonidas</first><last>Gee</last></author>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ignacio</first><last>Iacobacci</last><affiliation>Elm Europe</affiliation></author>
      <pages>79-94</pages>
      <abstract>Code Language Models have been trained togenerate accurate solutions, typically with noregard for runtime. On the other hand, previousworks that explored execution optimisationhave observed corresponding drops infunctional correctness. To that end, we introduceCode-Optimise, a framework that incorporatesboth correctness (passed, failed) andruntime (quick, slow) as learning signals viaself-generated preference data. Our frameworkis both lightweight and robust as it dynamicallyselects solutions to reduce overfitting whileavoiding a reliance on larger models for learningsignals. Code-Optimise achieves significantimprovements in pass@k while decreasingthe competitive baseline runtimes by anadditional 6% for in-domain data and up to3% for out-of-domain data. As a by-product,the average length of the generated solutionsis reduced by up to 48% on MBPP and 23%on HumanEval, resulting in faster and cheaperinference. The generated data and codebaseis open-sourced at https://github.com/huawei-noah/HEBO/tree/Code_Optimise.</abstract>
      <url hash="dfed7a1a">2025.findings-naacl.5</url>
      <bibkey>gee-etal-2025-code</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.5</doi>
    </paper>
    <paper id="6">
      <title>People will agree what <fixed-case>I</fixed-case> think: Investigating <fixed-case>LLM</fixed-case>’s False Consensus Effect</title>
      <author><first>Junhyuk</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Yeseon</first><last>Hong</last></author>
      <author><first>Bugeun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>95-126</pages>
      <abstract>Large Language Models (LLMs) have been recently adopted in interactive systems requiring communication. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Psychologists especially focus on the False Consensus Effect (FCE), a cognitive bias where individuals overestimate the extent to which others share their beliefs or behaviors, because FCE can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when FCE becomes more or less prevalent compared to normal usage.</abstract>
      <url hash="7f8c866f">2025.findings-naacl.6</url>
      <bibkey>choi-etal-2025-people</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>L</fixed-case>aw<fixed-case>I</fixed-case>nstruct: A Resource for Studying Language Model Adaptation to the Legal Domain</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>Harvey</affiliation></author>
      <author><first>Lucia</first><last>Zheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Arya D.</first><last>McCarthy</last><affiliation>Scaled Cognition</affiliation></author>
      <author><first>Christopher</first><last>Hahn</last><affiliation>X, the moonshot factory</affiliation></author>
      <author><first>Brian M</first><last>Rosen</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Henderson</last><affiliation>Princeton University</affiliation></author>
      <author><first>Daniel E.</first><last>Ho</last><affiliation>Stanford University</affiliation></author>
      <author><first>Garrett</first><last>Honke</last></author>
      <author><first>Percy</first><last>Liang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher D</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <pages>127-152</pages>
      <abstract>Instruction tuning is an important step in making language models useful for direct user interaction. However, the legal domain is underrepresented in typical instruction datasets (e.g., only 10 out of 1600+ tasks in Super-NaturalInstructions). To study whether instruction tuning on legal datasets is necessary for strong legal reasoning, we aggregate 58 annotated legal datasets and write instructions for each, creating LawInstruct. LawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M examples across diverse tasks such as legal QA, summarization of court cases, and legal argument mining. We evaluate our models on LegalBench, measuring legal reasoning across five categories in 162 challenging and realistic legal tasks, and MMLU, to measure potential drops in general reasoning capabilities. We find that legal-specific instruction tuning on Flan-T5 – yielding FLawN-T5 – improves performance on LegalBench across all model sizes, with an aggregate increase of 15 points or 50% over Flan-T5 for the base size. No model size shows performance drops in MMLU. We publish LawInstruct as a resource for further study of instruction tuning in the legal domain.</abstract>
      <url hash="62c4ba93">2025.findings-naacl.7</url>
      <bibkey>niklaus-etal-2025-lawinstruct</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.7</doi>
    </paper>
    <paper id="8">
      <title>Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Hongyuan</first><last>Lu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xinhua</first><last>Zeng</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Zhang</last><affiliation>facemind</affiliation></author>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Yumeng</first><last>Zhang</last></author>
      <author><first>Shan</first><last>Huang</last></author>
      <author><first>Yiran</first><last>Wei</last></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>153-166</pages>
      <abstract>In the rapidly evolving field of natural language processing, dialogue systems primarily employ a single-step dialogue paradigm. Although this paradigm is commonly adopted, it lacks the depth and fluidity of human interactions and does not appear natural. We introduce a novel **Step**-by-Step Dialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of human conversations. By employing a dual learning strategy and a further-split post-editing method, we generated and utilized a high-quality step-by-step dialogue dataset to fine-tune existing large language models, enabling them to perform step-by-step dialogues. We thoroughly present Stephanie. Tailored automatic and human evaluations are conducted to assess its effectiveness compared to the traditional single-step dialogue paradigm. We will release code, Stephanie datasets, and Stephanie LLMs to facilitate the future of chatbot eras.</abstract>
      <url hash="3cb66799">2025.findings-naacl.8</url>
      <bibkey>yang-etal-2025-stephanie</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>hift: Sense-based Language Variation Analysis using Flexible Alignment</title>
      <author><first>Clare</first><last>Arrington</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Mauricio</first><last>Gruppi</last><affiliation>Villanova University</affiliation></author>
      <author><first>Sibel</first><last>Adali</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>167-181</pages>
      <abstract>We introduce ConShift, a family of alignment-based algorithms that enable semantic variation analysis at the sense-level. Using independent senses of words induced from the context of tokens in two corpora, sense-enriched word embeddings are aligned using self-supervision and a flexible matching mechanism. This approach makes it possible to test for multiple sense-level language variations such as sense gain/presence, loss/absence and broadening/narrowing, while providing explanation of the changes through visualization of related concepts. We illustrate the utility of the method with sense- and word-level semantic shift detection results for multiple evaluation datasets in diachronic settings and dialect variation in the synchronic setting.</abstract>
      <url hash="c07ce6df">2025.findings-naacl.9</url>
      <bibkey>arrington-etal-2025-conshift</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.9</doi>
    </paper>
    <paper id="10">
      <title>Breaking the Stigma! Unobtrusively Probe Symptoms in Depression Disorder Diagnosis Dialogue</title>
      <author><first>Jieming</first><last>Cao</last></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yanan</first><last>Zhang</last></author>
      <author><first>Ruibo</first><last>Deng</last></author>
      <author><first>Jincheng</first><last>Zhang</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>182-200</pages>
      <url hash="26aafd84">2025.findings-naacl.10</url>
      <bibkey>cao-etal-2025-breaking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>T</fixed-case>o<fixed-case>V</fixed-case>o: Toxicity Taxonomy via Voting</title>
      <author><first>Tinh Son</first><last>Luong</last></author>
      <author><first>Thanh-Thien</first><last>Le</last></author>
      <author><first>Thang Viet</first><last>Doan</last></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Nguyen Thi Ngoc</first><last>Diep</last></author>
      <pages>201-212</pages>
      <abstract>Existing toxic detection models face significant limitations, such as lack of transparency, customization, and reproducibility. These challenges stem from the closed-source nature of their training data and the paucity of explanations for their evaluation mechanism. To address these issues, we propose a dataset creation mechanism that integrates voting and chain-of-thought processes, producing a high-quality open-source dataset for toxic content detection. Our methodology ensures diverse classification metrics for each sample and includes both classification scores and explanatory reasoning for the classifications.We utilize the dataset created through our proposed mechanism to train our model, which is then compared against existing widely-used detectors. Our approach not only enhances transparency and customizability but also facilitates better fine-tuning for specific use cases. This work contributes a robust framework for developing toxic content detection models, emphasizing openness and adaptability, thus paving the way for more effective and user-specific content moderation solutions.</abstract>
      <url hash="6c642c2f">2025.findings-naacl.11</url>
      <bibkey>luong-etal-2025-tovo</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>HALLUCANA</fixed-case>: Fixing <fixed-case>LLM</fixed-case> Hallucination with A Canary Lookahead</title>
      <author><first>Tianyi</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Erenay</first><last>Dayanik</last><affiliation>Amazon</affiliation></author>
      <author><first>Shubhi</first><last>Tyagi</last><affiliation>Amazon</affiliation></author>
      <author><first>Andrea</first><last>Pierleoni</last><affiliation>Alexa AI</affiliation></author>
      <pages>213-230</pages>
      <abstract>In this paper, we present HALLUCANA, a canary lookahead to detect and correct factual hallucinations of Large Language Models (LLMs) in long-form generation. HALLUCANA detects and intervenes as soon as traces of hallucination emerge, during and even before generation. To support timely detection, we exploit the internal factuality representation in the LLM hidden space, where we investigate various proxies to the LLMs’ factuality self-assessment, and discuss its relation to the models’ context familiarity from their pre-training. On biography generation, our method improves generation quality by up to 2.5x, while consuming over 6 times less compute.</abstract>
      <url hash="ec7c8f02">2025.findings-naacl.12</url>
      <bibkey>li-etal-2025-hallucana</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.12</doi>
    </paper>
    <paper id="13">
      <title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Aoyang</first><last>Zhou</last></author>
      <author><first>Kun</first><last>He</last><affiliation>Huazhong University of Sceince and Technology</affiliation></author>
      <pages>231-245</pages>
      <abstract>Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</abstract>
      <url hash="61689032">2025.findings-naacl.13</url>
      <bibkey>liu-etal-2025-enhancing-adversarial</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>D</fixed-case>is2<fixed-case>D</fixed-case>is: Explaining Ambiguity in Fact-Checking</title>
      <author><first>Ieva</first><last>Staliunaite</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>246-267</pages>
      <abstract>Ambiguity is a linguistic tool for encoding information efficiently, yet it also causes misunderstandings and disagreements. It is particularly relevant to the domain of misinformation, as fact-checking ambiguous claims is difficult even for experts. In this paper we argue that instead of predicting a veracity label for which there is genuine disagreement, it would be more beneficial to explain the ambiguity. Thus, this work introduces claim disambiguation, a constrained generation task, for explaining ambiguous claims in fact-checking. This involves editing them to spell out an interpretation that can then be unequivocally supported by the given evidence. We collect a dataset of 1501 such claim revisions and conduct experiments with sequence-to-sequence models. The performance is compared to a simple copy baseline and a Large Language Model baseline. The best results are achieved by employing Minimum Bayes Decoding, with a BertScore F1 of 92.22. According to human evaluation, the model successfully disambiguates the claims 72% of the time.</abstract>
      <url hash="83832e83">2025.findings-naacl.14</url>
      <bibkey>staliunaite-vlachos-2025-dis2dis</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.14</doi>
    </paper>
    <paper id="15">
      <title>Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement</title>
      <author><first>Xiyao</first><last>Wang</last></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Zhaoyang</first><last>Wang</last></author>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Yiyang</first><last>Zhou</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Tom</first><last>Goldstein</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Taha</first><last>Kass-Hout</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <pages>268-282</pages>
      <abstract>Large vision-language models (LVLMs) have achieved impressive results in visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there remains significant room for improvement in aligning visual and language modalities. Existing methods often depend on external models or data, leading to uncontrollable and unstable alignment results. In this paper, we propose SIMA, a self-improvement framework that enhances visual and language modality alignment without external dependencies. SIMA leverages existing vision instruction tuning datasets to self-generate responses, incorporating an in-context self-critic mechanism that constructs preference pairs for tuning. Crucially, our approach allows LVLMs to act as critics by designing effective critic prompts, eliminating the need for additional fine-tuning with external instruction data. We introduce three novel visual metrics within the self-critic process to guide judgement, significantly improving the accuracy of self-critic. Through extensive experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA significantly improves LVLM’s performance and outperforms previous approaches, achieving superior modality alignment.</abstract>
      <url hash="51745fec">2025.findings-naacl.15</url>
      <bibkey>wang-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>R</fixed-case>e<fixed-case>PD</fixed-case>: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process</title>
      <author><first>Peiran</first><last>Wang</last></author>
      <author><first>Xiaogeng</first><last>Liu</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>283-294</pages>
      <abstract>In this study, we introduce RePD, an innovative attack Retrieval-based Prompt Decomposition framework designed to mitigate the risk of jailbreak attacks on large language models (LLMs). Despite rigorous pre-training and fine-tuning focused on ethical alignment, LLMs are still susceptible to jailbreak exploits. RePD operates on a one-shot learning model, wherein it accesses a database of pre-collected jailbreak prompt templates to identify and decompose harmful inquiries embedded within user prompts. This process involves integrating the decomposition of the jailbreak prompt into the user’s original query into a one-shot learning example to effectively teach the LLM to discern and separate malicious components. Consequently, the LLM is equipped to first neutralize any potentially harmful elements before addressing the user’s prompt in a manner that aligns with its ethical guidelines. RePD is versatile and compatible with a variety of open-source LLMs acting as agents. Through comprehensive experimentation with both harmful and benign prompts, we have demonstrated the efficacy of our proposed RePD in enhancing the resilience of LLMs against jailbreak attacks, without compromising their performance in responding to typical user requests.</abstract>
      <url hash="1572b1ed">2025.findings-naacl.16</url>
      <bibkey>wang-etal-2025-repd</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>C</fixed-case>hat<fixed-case>CRS</fixed-case>: Incorporating External Knowledge and Goal Guidance for <fixed-case>LLM</fixed-case>-based Conversational Recommender Systems</title>
      <author><first>Chuang</first><last>Li</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hengchang</first><last>Hu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>295-312</pages>
      <abstract>This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.</abstract>
      <url hash="2f68f51e">2025.findings-naacl.17</url>
      <bibkey>li-etal-2025-chatcrs</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.17</doi>
    </paper>
    <paper id="18">
      <title>Data-Efficiently Learn Large Language Model for Universal 3<fixed-case>D</fixed-case> Scene Perception</title>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Haifeng</first><last>Huang</last></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ziang</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>313-333</pages>
      <abstract>3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. With limited data, Chat-3D achieves a 82.2% relative score compared with GPT-4 on the constructed instruction dataset, and comparable performance to state-of-the-art LLM-based methods.</abstract>
      <url hash="5e64585f">2025.findings-naacl.18</url>
      <bibkey>wang-etal-2025-data</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>U</fixed-case>nified<fixed-case>MLLM</fixed-case>: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model</title>
      <author><first>Zhaowei</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>YiQing</first><last>Cai</last></author>
      <author><first>Qi</first><last>Xu</last></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Hang</first><last>Song</last></author>
      <author><first>Botian</first><last>Jiang</last></author>
      <author><first>Zhida</first><last>Huang</last></author>
      <author><first>Tao</first><last>Wang</last><affiliation>Bytedance group</affiliation></author>
      <pages>334-344</pages>
      <abstract>Significant advancements has recently been achieved in the field of multi-modal large language models (MLLMs), demonstrating their remarkable capabilities in understanding and reasoning across diverse tasks. However, these models are often trained for specific tasks and rely on task-specific input-output formats, limiting their applicability to a broader range of tasks. This raises a fundamental question: Can we develop a unified approach to represent and handle different multi-modal tasks to maximize the generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a comprehensive model designed to represent various tasks using a unified representation. Our model exhibits strong capabilities in comprehending the implicit intent of user instructions and preforming reasoning. In addition to generating textual responses, our model also outputs task tokens and grounding tokens, serving as indicators of task types and task granularity. These outputs are subsequently routed through the task router and directed to specific expert models for task completion. To train our model, we construct a task-specific dataset and an 100k multi-task dataset encompassing complex scenarios. Employing a three-stage training strategy, we equip our model with robust reasoning and task processing capabilities while preserving its generalization capacity and knowledge reservoir. Extensive experiments showcase the impressive performance of our unified representation approach across various tasks, surpassing existing methodologies. Furthermore, our approach exhibits exceptional scalability and generality.</abstract>
      <url hash="0fb9f356">2025.findings-naacl.19</url>
      <bibkey>li-etal-2025-unifiedmllm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>PEMV</fixed-case>: Improving Spatial Distribution for Emotion Recognition in Conversations Using Proximal Emotion Mean Vectors</title>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <pages>345-357</pages>
      <abstract>Emotion Recognition in Conversation (ERC) aims to identify the emotions expressed in each utterance within a dialogue. Existing research primarily focuses on the analysis of contextual structure in dialogue and the interactions between different emotions. Nonetheless, ERC datasets often contain difficult-to-classify samples and suffer from imbalanced label distributions, which pose challenges to the spatial distribution of dialogue features. To tackle this issue, we propose a method that generates Proximal Emotion Mean Vectors (PEMV) based on emotion feature queues to optimize the spatial representation of text features. We design a Center Loss based on PEMVs to pull hard-to-classify samples closer to their respective category centers and employ Angle Loss to maximize the angular separation between different PEMVs. Furthermore, we utilize PEMV as a classifier to better adapt to the spatial structure of dialogue features. Extensive experiments on three widely used benchmark datasets demonstrate that our method achieves state-of-the-art performance and validates its effectiveness in optimizing feature space representations.</abstract>
      <url hash="3ff6a62e">2025.findings-naacl.20</url>
      <bibkey>lin-etal-2025-pemv</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>D</fixed-case>iscover<fixed-case>GPT</fixed-case>: Multi-task Fine-tuning Large Language Model for Related Table Discovery</title>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiao</first><last>Qin</last><affiliation>Amazon</affiliation></author>
      <author><first>Chuan</first><last>Lei</last></author>
      <author><first>Asterios</first><last>Katsifodimos</last><affiliation>Delft University of Technology</affiliation></author>
      <author><first>Zhengyuan</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Balasubramaniam</first><last>Srinivasan</last><affiliation>Amazon</affiliation></author>
      <author><first>Huzefa</first><last>Rangwala</last><affiliation>Amazon and Computer Science, George Mason University</affiliation></author>
      <pages>358-373</pages>
      <abstract>Natural language understanding over tabular data has played a significant role in data discovery tasks such as joinable and unionable table search. State-of-the-art approaches adopt large language models (LLMs) pre-trained over massive text corpora to learn and evaluate the table semantic relatedness. Existing methods typically follow a pretrain-and-finetune paradigm, namely fine-tuning an LLM using tabular data with table relatedness labels. To enhance model’s understanding of tabular data, recent studies include auxiliary tasks such as entity resolution and column type classification in the fine-tuning phase. In spite of achieving performance gain from these supervisions, there is a lack of study on how these supervisions complement or even contrast each other, leading to a subpar performance on the final data discovery tasks. In this paper, we propose a simple yet effective multi-task fine-tuning framework named DiscoverGPT that holistically discovers and leverages the intricate relationships among the supervisions to optimize the performance on the data discovery task. Moreover, DiscoverGPT is plug-and-play that allows a broad range of open-domain auxiliary tasks to be incorporated, by utilizing the generative power of LLMs. We demonstrate the usability and effectiveness of DiscoverGPT with baseline comparisons and ablation studies. DiscoverGPT outperforms the best performing baseline by up to 7% in F1 score.</abstract>
      <url hash="17142b89">2025.findings-naacl.21</url>
      <bibkey>hu-etal-2025-discovergpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.21</doi>
    </paper>
    <paper id="22">
      <title>Can <fixed-case>GPT</fixed-case>-4 Sway Experts’ Investment Decisions?</title>
      <author><first>Takehiro</first><last>Takayanagi</last></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Kiyoshi</first><last>Izumi</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>374-383</pages>
      <abstract>In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers’ decisions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers’ decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience decisions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the decisions of readers. We release our dataset to assist future research.</abstract>
      <url hash="66f617d6">2025.findings-naacl.22</url>
      <bibkey>takayanagi-etal-2025-gpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>P</fixed-case>oly<fixed-case>J</fixed-case>oin: Semantic Multi-key Joinable Table Search in Data Lakes</title>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chuan</first><last>Lei</last></author>
      <author><first>Xiao</first><last>Qin</last><affiliation>Amazon</affiliation></author>
      <author><first>Asterios</first><last>Katsifodimos</last><affiliation>Delft University of Technology</affiliation></author>
      <author><first>Christos</first><last>Faloutsos</last><affiliation>Amazon and Carnegie Mellon University</affiliation></author>
      <author><first>Huzefa</first><last>Rangwala</last><affiliation>Amazon and Computer Science, George Mason University</affiliation></author>
      <pages>384-395</pages>
      <abstract>Given a query table, how can we effectively discover multi-key joinable tables on the web? This can be seen as a retrieval task, where users can lookup on the web for tables related to an existing one. Searching and discovering such joinable tables is critical to data analysts and data scientists for reporting, establishing correlations and training machine learning models. Existing joinable table search methods have mostly focused on single key (unary) joins, where a single column is the join key. However, these methods are ineffective when dealing with join keys composed of multiple columns (n-ary joins), which are prevalent on web table corpora. In this paper, we introduce PolyJoin, which finds multi-key semantically-joinable tables on the web, given a query table. PolyJoin employs a multi-key encoder and a novel self-supervised training method to generate the representations of multiple join keys, preserving the alignment across multiple columns. In particular, PolyJoin is equipped with a hierarchical contrastive learning technique to further enhance the model’s semantic understanding of multi-key joinable tables. PolyJoin outperforms the state-of-the-art methods by 2.89% and 3.67% with respect to MAP@30 and R@30 on two real-world web table benchmarks, respectively.</abstract>
      <url hash="fda0431d">2025.findings-naacl.23</url>
      <bibkey>hu-etal-2025-polyjoin</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.23</doi>
    </paper>
    <paper id="24">
      <title>Marrying <fixed-case>LLM</fixed-case>s with Dynamic Forecasting: A Graph Mixture-of-expert Perspective</title>
      <author><first>Dapeng</first><last>Jiang</last></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>396-410</pages>
      <abstract>Dynamical system modeling is a crucial area of research in machine learning with extensive applications in physics and social science. Recent data-driven approaches often employ graph neural networks (GNNs) to learn relationships in dynamical systems using message passing mechanisms. Despite their advancements, these methods often suffer from performance degradation when it comes to potential environmental change with distribution shifts in real-world applications. In this work, we propose a new perspective which leverages large language models (LLMs) to enhance the generalization capabilities of dynamical system modeling. In particular, we develop a novel framework named LLM Judge with Graph Mixture-of-expert LEGO which incorporates multiple graph experts to learn diverse dynamics within the systems. More importantly, LEGO utilizes LLMs with hierarchical prompts at object, edge, and system levels as a context-aware routing function to determine which experts carry the most relevant information to different environments. The whole framework is optimized by updating the weights and expert parameters in an alternative fashion. Extensive experiments across various datasets demonstrate the effectiveness of our proposed LEGO in comparison to extensive baselines.</abstract>
      <url hash="fa7d5aaf">2025.findings-naacl.24</url>
      <bibkey>jiang-luo-2025-marrying</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>G</fixed-case>en: Multi-modal Interactive Dialogue System with Multi-turn Text-Image Generation</title>
      <author><first>Minbin</first><last>Huang</last></author>
      <author><first>Yanxin</first><last>Long</last><affiliation>Tencent Data Platform</affiliation></author>
      <author><first>Xinchi</first><last>Deng</last></author>
      <author><first>Ruihang</first><last>Chu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jiangfeng</first><last>Xiong</last><affiliation>Tencent Data Platform</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qinglin</first><last>Lu</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <pages>411-426</pages>
      <abstract>Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user’s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model’s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen in producing correct output modalities and coherent multi-modal outputs compared with other State-of-the-Art models. We hope that DialogBen can contribute to the community for building more powerful MIDS.</abstract>
      <url hash="7d49678c">2025.findings-naacl.25</url>
      <bibkey>huang-etal-2025-dialoggen</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>REL</fixed-case>ex<fixed-case>ED</fixed-case>: Retrieval-Enhanced Legal Summarization with Exemplar Diversity</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Chen</first><last>Jia</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Patrick</first><last>Goroncy</last><affiliation>Department of Informatics, Technische Universität München</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>427-434</pages>
      <abstract>This paper addresses the task of legal summarization, which involves distilling complex legal documents into concise, coherent summaries. Current approaches often struggle with content theme deviation and inconsistent writing styles due to their reliance solely on source documents. We propose RELexED, a retrieval-augmented framework that utilizes exemplar summaries along with the source document to guide the model. RELexED employs a two-stage exemplar selection strategy, leveraging a determinantal point process to balance the trade-off between similarity of exemplars to the query and diversity among exemplars, with scores computed via influence functions. Experimental results on two legal summarization datasets demonstrate that RELexED significantly outperforms models that do not utilize exemplars and those that rely solely on similarity-based exemplar selection.</abstract>
      <url hash="cc8840d8">2025.findings-naacl.26</url>
      <bibkey>t-y-s-s-etal-2025-relexed</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>CL</fixed-case>a<fixed-case>MP</fixed-case> 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models</title>
      <author><first>Shangda</first><last>Wu</last></author>
      <author><first>Yashan</first><last>Wang</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Guo</first><last>Zhancheng</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Monan</first><last>Zhou</last></author>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Xuefeng</first><last>Mu</last></author>
      <author><first>Yuejie</first><last>Gao</last></author>
      <author><first>Yuanliang</first><last>Dong</last></author>
      <author><first>Jiafeng</first><last>Liu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Xiaobing</first><last>Li</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Feng</first><last>Yu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>435-451</pages>
      <abstract>Challenges in managing linguistic diversity and integrating various musical modalities are faced by current music information retrieval systems. These limitations reduce their effectiveness in a global, multimodal music environment. To address these issues, we introduce CLaMP 2, a system compatible with 101 languages that supports both ABC notation (a text-based musical notation format) and MIDI (Musical Instrument Digital Interface) for music information retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text triplets, includes a multilingual text encoder and a multimodal music encoder aligned via contrastive learning. By leveraging large language models, we obtain refined and consistent multilingual descriptions at scale, significantly reducing textual noise and balancing language distribution. Our experiments show that CLaMP 2 achieves state-of-the-art results in both multilingual semantic search and music classification across modalities, thus establishing a new standard for inclusive and global music information retrieval.</abstract>
      <url hash="3ce836db">2025.findings-naacl.27</url>
      <bibkey>wu-etal-2025-clamp</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>L</fixed-case>og<fixed-case>R</fixed-case>ules: Enhancing Log Analysis Capability of Large Language Models through Rules</title>
      <author><first>Xin</first><last>Huang</last></author>
      <author><first>Ting</first><last>Zhang</last></author>
      <author><first>Wen</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>452-470</pages>
      <abstract>Currently, large language models (LLMs) have achieved impressive performance in natural language processing tasks. However, LLMs still exhibit many hallucinations when analyzing system logs, which is due to the implicit knowledge and rules in logs that LLMs cannot capture. Based on this, we propose LogRules, a lightweight log analysis framework that generates and utilizes rules through LLMs. LogRules consists of three stages: an induction stage, an alignment stage, and a reasoning stage. Firstly, in the induction stage, an strong LLM (e.g., GPT-4o-mini) is tasked with generating a series of rules related to logs, which are then validated on the training set. When the rules are confirmed to produce correct reasoning results, they are added to a rule repository. Secondly, considering that the LLMs with small size (<tex-math>\approx</tex-math>8B parameters) still face challenges in utilizing rules, we design an alignment method based on rule-case contrastive preference optimization (CPO) to effectively enhance the rule reasoning capabilities of these LLMs. Finally, in the reasoning stage, the LLM constructs prompt using the rule repository and performs log analysis on the test set. Experiments show that LogRules outperforms LLM-based methods in log parsing and anomaly detection tasks, and achieves better performance compared to case-based methods.</abstract>
      <url hash="ecba03cd">2025.findings-naacl.28</url>
      <bibkey>huang-etal-2025-logrules</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.28</doi>
    </paper>
    <paper id="29">
      <title>Audio Description Generation in the Era of <fixed-case>LLM</fixed-case>s and <fixed-case>VLM</fixed-case>s: A Review of Transferable Generative <fixed-case>AI</fixed-case> Technologies</title>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lukas</first><last>Fischer</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Alexa</first><last>Lintner</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <pages>471-490</pages>
      <abstract>Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.</abstract>
      <url hash="51fba01b">2025.findings-naacl.29</url>
      <bibkey>gao-etal-2025-audio</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.29</doi>
    </paper>
    <paper id="30">
      <title>Adaptive Retrieval-Augmented Generation for Conversational Systems</title>
      <author><first>Xi</first><last>Wang</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Procheta</first><last>Sen</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>491-503</pages>
      <abstract>With the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. While many existing studies agree on the necessity of Retrieval Augmented Generation (RAG), further investigation into the necessity and value of applying RAG to every turn of the conversation is needed. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models, joined with well-rounded analyses of various conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying if system responses require RAG to generate high-quality responses with high confidence. This study also identifies and shows the correlation between the generation’s confidence level and the relevance of the augmented knowledge. We have also released the implementation code and resources in https://github.com/wangxieric/RAGate.</abstract>
      <url hash="a8962ab6">2025.findings-naacl.30</url>
      <bibkey>wang-etal-2025-adaptive</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.30</doi>
    </paper>
    <paper id="31">
      <title>Multimodal Generation with Consistency Transferring</title>
      <author><first>Junxiang</first><last>Qiu</last></author>
      <author><first>Jinda</first><last>Lu</last></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>504-513</pages>
      <abstract>Multimodal content generation has become an area of considerable interest. However, existing methods are hindered by limitations related to model constraints and training strategies: (1) Most current approaches rely on training models from scratch, resulting in inefficient training processes when extending these models; (2) There is a lack of constraints on adjacent steps within the models, leading to slow sampling and poor generation stability across various sampling methods. To address the issues, we introduce Multimodal Generation with Consistency Transferring (MGCT). The method introduces two key improvements: (1) A Model Consistency Transferring (MCT) strategy to acquire low-cost prior knowledge, increasing training efficiency and avoiding error accumulation; (2) A Layer Consistency Transferring (LCT) between adjacent steps, enhancing denoising capabilities at each step and improving model stability across various generation methods. These strategies ensure the consistency of jointly generated multimodal content and improving training efficiency. Experiments show that the algorithm enhances the model’s ability to capture actions and depict backgrounds more effectively. In both the AIST++ and Landscape datasets, it improves video generation speed by approximately 40% and quality by about 39.3%, while also achieving a slight 3% improvement in audio quality over the baseline.</abstract>
      <url hash="cfc5f78e">2025.findings-naacl.31</url>
      <bibkey>qiu-etal-2025-multimodal</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.31</doi>
    </paper>
    <paper id="32">
      <title>On the Impact of Noise in Differentially Private Text Rewriting</title>
      <author><first>Stephen</first><last>Meisenbacher</last></author>
      <author><first>Maulik</first><last>Chevli</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>514-532</pages>
      <abstract>The field of text privatization often leverages the notion of *Differential Privacy* (DP) to provide formal guarantees in the rewriting or obfuscation of sensitive textual data. A common and nearly ubiquitous form of DP application necessitates the addition of calibrated noise to vector representations of text, either at the data- or model-level, which is governed by the privacy parameter <tex-math>\varepsilon</tex-math>. However, noise addition almost undoubtedly leads to considerable utility loss, thereby highlighting one major drawback of DP in NLP. In this work, we introduce a new sentence infilling privatization technique, and we use this method to explore the effect of noise in DP text rewriting. We empirically demonstrate that non-DP privatization techniques excel in utility preservation and can find an acceptable empirical privacy-utility trade-off, yet cannot outperform DP methods in empirical privacy protections. Our results highlight the significant impact of noise in current DP rewriting mechanisms, leading to a discussion of the merits and challenges of DP in NLP as well as the opportunities that non-DP methods present.</abstract>
      <url hash="355e2dd7">2025.findings-naacl.32</url>
      <bibkey>meisenbacher-etal-2025-impact</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.32</doi>
    </paper>
    <paper id="33">
      <title>Teaching Large Language Models Number-Focused Headline Generation With Key Element Rationales</title>
      <author><first>Zhen</first><last>Qian</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiaofei</first><last>Xu</last></author>
      <author><first>Feng</first><last>Xia</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>533-550</pages>
      <abstract>Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy.</abstract>
      <url hash="c8dda3f5">2025.findings-naacl.33</url>
      <bibkey>qian-etal-2025-teaching</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.33</doi>
    </paper>
    <paper id="34">
      <title>Zero-Shot Strategies for Length-Controllable Summarization</title>
      <author><first>Fabian</first><last>Retkowski</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>551-572</pages>
      <abstract>Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs’ length control capabilities across multiple measures and propose practical methods to improve controllability. Our experiments with LLaMA 3 reveal stark differences in length adherence across measures and highlight inherent biases of the model. To address these challenges, we introduce a set of methods: length approximation, target adjustment, sample filtering, and automated revisions. By combining these methods, we demonstrate substantial improvements in length compliance while maintaining or enhancing summary quality, providing highly effective zero-shot strategies for precise length control without the need for model fine-tuning or architectural changes. With our work, we not only advance our understanding of LLM behavior in controlled text generation but also pave the way for more reliable and adaptable summarization systems in real-world applications.</abstract>
      <url hash="ef02ede1">2025.findings-naacl.34</url>
      <bibkey>retkowski-waibel-2025-zero</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>SIMPLOT</fixed-case>: Enhancing Chart Question Answering by Distilling Essentials</title>
      <author><first>Wonjoong</first><last>Kim</last></author>
      <author><first>Sangwu</first><last>Park</last></author>
      <author><first>Yeonjun</first><last>In</last></author>
      <author><first>Seokwon</first><last>Han</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Chanyoung</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>573-593</pages>
      <abstract>Recently, interpreting complex charts with logical reasoning has emerged as challenges due to the development of vision-language models. A prior state-of-the-art (SOTA) model has presented an end-to-end method that leverages the vision-language model to convert charts into table format utilizing Large Language Model (LLM) for reasoning. However, unlike natural images, charts contain a mix of essential and irrelevant information required for chart reasoning, and we discover that this characteristic can lower the performance of chart-to-table extraction. In this paper, we introduce SIMPLOT, a method designed to extract only the elements necessary for chart reasoning. The proposed method involves two steps: 1) training to mimic a simple plot that contains only the essential information from a complex chart for table extraction, followed by 2) performing reasoning based on the table. Our model enables accurate chart reasoning without the need for additional annotations or datasets, and its effectiveness is demonstrated through various experiments.</abstract>
      <url hash="570c419a">2025.findings-naacl.35</url>
      <bibkey>kim-etal-2025-simplot</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>A</fixed-case>ny2<fixed-case>P</fixed-case>ix: Image Editing with Multi-Modal Prompts</title>
      <author><first>Shufan</first><last>Li</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Harkanwar</first><last>Singh</last></author>
      <author><first>Aditya</first><last>Grover</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>594-619</pages>
      <abstract>Image Editing has made incredible progress in recent years. Earliest work only supported caption-guided editing. Recently, free-form text instructions and reference images are incorporated to allow more flexibility. However, existing methods still struggle with complicated editing instructions involving multiple objects or reference images. We present InstructAny2Pix, a novel image editing model that leverages a multi-modal LLM to execute complicated edit instructions. Compared with previous, works, InstructAny2Pix extends the flexibility of edit instructions in three ways: First, it can perform complex instructions involving multiple object edits; Second, it supports interleaving text instructions with multiple reference images; Third, it supports audio and music inputs as part of edit prompts, unlocking many creative applications, such as album cover generation and music-inspired merchandise design. To evaluate the effectiveness of InstructAny2Pix, we propose two new benchmark datasets MM-Inst and Dream-booth++ consisting of human written, multi-modal prompts. InstructAny2Pix outperforms baselines in these two proposed multi-modal benchmarks, as well as conventional image editing benchmarks such as InstructPix2Pix.</abstract>
      <url hash="fe2eb128">2025.findings-naacl.36</url>
      <bibkey>li-etal-2025-instructany2pix</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.36</doi>
    </paper>
    <paper id="37">
      <title>Lost in Overlap: Exploring Logit-based Watermark Collision in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yiyang</first><last>Luo</last></author>
      <author><first>Ke</first><last>Lin</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chao</first><last>Gu</last></author>
      <author><first>Jiahui</first><last>Hou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Luo</first><last>Ping</last></author>
      <pages>620-637</pages>
      <abstract>The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation.In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.</abstract>
      <url hash="e6fa5ae1">2025.findings-naacl.37</url>
      <bibkey>luo-etal-2025-lost</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.37</doi>
    </paper>
    <paper id="38">
      <title>Prompt-Guided Selective Masking Loss for Context-Aware Emotive Text-to-Speech</title>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Youngjae</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jihyun</first><last>Lee</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>638-650</pages>
      <abstract>Emotional dialogue speech synthesis (EDSS) aims to generate expressive speech by leveraging the dialogue context between interlocutors. This is typically done by concatenating global representations of previous utterances as conditions for text-to-speech (TTS) systems. However, such approaches overlook the importance of integrating localized acoustic cues that convey emotion. To address this, we introduce a novel approach that utilizes a large language model (LLM) to generate holistic emotion tags based on prior dialogue context, while also pinpointing key words in the target utterance that align with the predicted emotional state. Furthermore, we enhance the emotional richness of synthesized speech by incorporating concentrated acoustic features of these key words through a novel selective audio masking loss function. This methodology not only improves emotional expressiveness, but also facilitates automatic emotion speech generation during inference by eliminating the need for manual emotion tag selection. Comprehensive subjective and objective evaluations and analyses demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="9891e401">2025.findings-naacl.38</url>
      <bibkey>jeon-etal-2025-prompt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.38</doi>
      <revision id="1" href="2025.findings-naacl.38v1" hash="5f9d08ca"/>
      <revision id="2" href="2025.findings-naacl.38v2" hash="9891e401" date="2025-06-13">Updated acknowledgements section.</revision>
    </paper>
    <paper id="39">
      <title>Identifying and Mitigating Social Bias Knowledge in Language Models</title>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Yichen</first><last>Li</last></author>
      <author><first>Jianfei</first><last>Yang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Joey Tianyi</first><last>Zhou</last><affiliation>A*STAR Centre for Frontier AI Research</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>651-672</pages>
      <abstract>Generating fair and accurate predictions plays a pivotal role in deploying pre-trained language models (PLMs) in the real world. However, existing debiasing methods may inevitably generate incorrect or nonsensical predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. This paper introduces a novel debiasing framework that first identifies the encoding locations of biases within language models and then applies the Fairness-Stamp (FAST). FAST focuses on fine-grained, individual bias mitigation and integrates a lightweight network into PLMs, specifically targeting identified biases while preserving essential knowledge and maintaining factual integrity. We also present BiaScope, a new benchmark comprising datasets and metrics designed to evaluate the retention of commonsense knowledge and the generalization across paraphrased social biases. Our extensive experiments across multiple datasets demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in PLMs. Code will be publicly available.</abstract>
      <url hash="7ec9a178">2025.findings-naacl.39</url>
      <bibkey>chen-etal-2025-identifying</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>D</fixed-case>ia<fixed-case>S</fixed-case>ynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications</title>
      <author><first>Sathya Krishnan</first><last>Suresh</last></author>
      <author><first>Wu</first><last>Mengjun</last></author>
      <author><first>Tushar</first><last>Pranav</last><affiliation>Singapore Institute of Technology</affiliation></author>
      <author><first>EngSiong</first><last>Chng</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>673-690</pages>
      <url hash="b12c1143">2025.findings-naacl.40</url>
      <bibkey>suresh-etal-2025-diasynth</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.40</doi>
    </paper>
    <paper id="41">
      <title>Do Not Design, Learn: A Trainable Scoring Function for Uncertainty Estimation in Generative <fixed-case>LLM</fixed-case>s</title>
      <author><first>Duygu Nur</first><last>Yaldiz</last></author>
      <author><first>Yavuz Faruk</first><last>Bakman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Baturalp</first><last>Buyukates</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Chenyang</first><last>Tao</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Dimitrios</first><last>Dimitriadis</last><affiliation>Amazon</affiliation></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <pages>691-713</pages>
      <abstract>Uncertainty estimation (UE) of generative large language models (LLMs) is crucial for evaluating the reliability of generated sequences. A significant subset of UE methods utilize token probabilities to assess uncertainty, aggregating multiple token probabilities into a single UE score using a scoring function. Existing scoring functions for probability-based UE, such as length-normalized scoring and semantic contribution-based weighting, are designed to solve certain aspects of the problem but exhibit limitations, including the inability to handle biased probabilities and complex semantic dependencies between tokens. To address these issues, in this work, we propose Learnable Response Scoring (LARS) function, a novel scoring function that leverages supervised data to capture complex dependencies between tokens and probabilities, thereby producing more reliable and calibrated response scores in computing the uncertainty of LLM generations. Our comprehensive experiments across question-answering and arithmetical reasoning tasks with various datasets demonstrate that LARS significantly outperforms existing scoring functions, achieving improvements of up to 16% AUROC score.</abstract>
      <url hash="ed0b7223">2025.findings-naacl.41</url>
      <bibkey>yaldiz-etal-2025-design</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.41</doi>
    </paper>
    <paper id="42">
      <title>Joint Learning Event-Specific Probe and Argument Library with Differential Optimization for Document-Level Multi-Event Extraction</title>
      <author><first>Jianpeng</first><last>Hu</last></author>
      <author><first>Chao</first><last>Xue</last></author>
      <author><first>Chunqing</first><last>Yu</last><affiliation>Tongji University</affiliation></author>
      <author><first>JiaCheng</first><last>Xu</last></author>
      <author><first>Chengxiang</first><last>Tan</last></author>
      <pages>714-726</pages>
      <abstract>Document-level multi-event extraction aims to identify a list of event types and corresponding arguments from the document. However, most of the current methods neglect the fine-grained difference among events in multi-event documents, which leads to event confusion and missing. This is also one of the reasons why the recall and F1-score of multi-event recognition are lower compared to single-event recognition. In this paper, we propose an event-specific probe-based method to sniff multiple events by querying each corresponding argument library, which uses a novel probe-label alignment method for differential optimization. In addition, the role contrastive loss and probe consistent loss are designed to fine-tune the fine-grained role differences and probe differences in each event. The experimental results on two general datasets show that our method outperforms the state-of-the-art method in the F1-score, especially in the recall of multi-events.</abstract>
      <url hash="ecd1a8b2">2025.findings-naacl.42</url>
      <bibkey>hu-etal-2025-joint</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.42</doi>
    </paper>
    <paper id="43">
      <title>Synonym-unaware Fast Adversarial Training against Textual Adversarial Attacks</title>
      <author><first>Yichen</first><last>Yang</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Kun</first><last>He</last><affiliation>Huazhong University of Sceince and Technology</affiliation></author>
      <pages>727-739</pages>
      <abstract>Numerous adversarial defense methods have been proposed to strengthen the robustness of Natural Language Processing (NLP) models against adversarial attacks. However, many of these methods rely on predetermined linguistic knowledge and assume that attackers’ synonym candidates are known, which is often unrealistic. In this work, we investigate adversarial training in the embedding space and introduce a Fast Adversarial Training (FAT) method to improve the model robustness without requiring synonym awareness. FAT leverages single-step perturbation generation and effective perturbation initialization based on two key insights: (1) adversarial perturbations generated by single-step and multi-step gradient ascent are similar, and (2) perturbations generated on the same training sample across successive epochs exhibit resemblance. By employing single-step gradient ascent and leveraging historical perturbation information, FAT not only expedites the training process but also efficiently initializes perturbations. Extensive experiments demonstrate that FAT significantly enhances the robustness of popular NLP models under scenarios where synonyms are unknown, outperforming other defense baselines under various character-level and word-level attacks.</abstract>
      <url hash="39944f86">2025.findings-naacl.43</url>
      <bibkey>yang-etal-2025-synonym</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.43</doi>
    </paper>
    <paper id="44">
      <title>Tethering Broken Themes: Aligning Neural Topic Models with Labels and Authors</title>
      <author><first>Mayank</first><last>Nagda</last></author>
      <author><first>Phil</first><last>Ostheimer</last><affiliation>RPTU Kaiserslautern-Landau</affiliation></author>
      <author><first>Sophie</first><last>Fellenz</last><affiliation>Universität Kaiserslautern</affiliation></author>
      <pages>740-760</pages>
      <abstract>Topic models are a popular approach for extracting semantic information from large document collections. However, recent studies suggest that the topics generated by these models often do not align well with human intentions. Although metadata such as labels and authorship information are available, it has not yet been effectively incorporated into neural topic models. To address this gap, we introduce FANToM, a novel method to align neural topic models with both labels and authorship information. FANToM allows for the inclusion of this metadata when available, producing interpretable topics and author distributions for each topic. Our approach demonstrates greater expressiveness than conventional topic models by learning the alignment between labels, topics, and authors. Experimental results show that FANToM improves existing models in terms of both topic quality and alignment. Additionally, it identifies author interests and similarities.</abstract>
      <url hash="4fc0d1e1">2025.findings-naacl.44</url>
      <bibkey>nagda-etal-2025-tethering</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.44</doi>
    </paper>
    <paper id="45">
      <title>Towards Zero-Shot Multimodal Machine Translation</title>
      <author><first>Matthieu</first><last>Futeral</last></author>
      <author><first>Cordelia</first><last>Schmid</last><affiliation>Google, INRIA and Inria</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>761-778</pages>
      <abstract>Current multimodal machine translation (MMT) systems rely on fully supervised data (i.e sentences with their translations and accompanying images), which is costly to collect and prevents the extension of MMT to language pairs with no such data. We propose a method to bypass the need for fully supervised data to train MMT systems, using multimodal English data only. Our method ( ZeroMMT) consists in adapting a strong text-only machine translation (MT) model by training it jointly on two objectives: visually conditioned masked language modelling and the Kullback-Leibler divergence between the original MT and new MMT outputs. We evaluate on standard MMT benchmarks and on CoMMuTE, a contrastive test set designed to evaluate how well models use images to disambiguate translations. ZeroMMT obtains disambiguation results close to state-of-the-art MMT models trained on fully supervised examples. To prove that ZeroMMT generalizes to languages with no fully supervised training data, we extend CoMMuTE to three new languages: Arabic, Russian and Chinese. We also show that we can control the trade-off between disambiguation capabilities and translation fidelity at inference time using classifier-free guidance and without any additional data. Our code, data and trained models are publicly accessible.</abstract>
      <url hash="e9cb5b52">2025.findings-naacl.45</url>
      <bibkey>futeral-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.45</doi>
    </paper>
    <paper id="46">
      <title>Large-Scale Corpus Construction and Retrieval-Augmented Generation for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Poetry: New Method and Data Insights</title>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Lan</first><last>Lan</last></author>
      <author><first>Jiahuan</first><last>Cao</last></author>
      <author><first>Hiuyi</first><last>Cheng</last></author>
      <author><first>Kai</first><last>Ding</last><affiliation>INTSIG Information</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>779-817</pages>
      <abstract>Ancient Chinese Poetry (ACP), a critical aspect of Chinese cultural heritage, presents unique challenges for Large Language Models (LLMs). One of the most pressing challenges is the significant hallucination issues faced by LLMs due to data scarcity and limited ability of general LLMs when dealing with ACP. To address these challenges, this paper constructs the ACP-Corpus, which encompasses 1.1 million ancient poems and 990K related texts, designed to enhance the training and performance of LLMs. Alongside this, we develop the ACP-QA dataset, comprising over 12 million question-answer pairs across 24 task categories, and the ACP-Eval dataset for rigorous evaluation purposes, containing 7,050 entries. Building on this resources, we propose the ACP-RAG framework, a specialized Retrieval-Augmented Generation (RAG) approach that significantly improves the performance of LLMs in the domain of ancient poetry from 49.2% to 89.0%. The ACP-RAG contains five modules of semantic coarse-grained retrieval, semantic fine-grained retrieval, keyword retrieval, keyword matching, and context filtering. Experiments show that ACP-RAG achieves a promising response accuracy of 89.0%, surpassing existing LLMs by a remarkable margin. We believe this work not only advances the capabilities of LLMs in processing ancient Chinese poetry but also contributes to the preservation and innovative development within this rich literary tradition. The datasets and code are available at https://github.com/SCUT-DLVCLab/ACP-RAG.</abstract>
      <url hash="84ec3ce1">2025.findings-naacl.46</url>
      <bibkey>liu-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>O</fixed-case>pen<fixed-case>B</fixed-case>io<fixed-case>NER</fixed-case>: Lightweight Open-Domain Biomedical Named Entity Recognition Through Entity Type Description</title>
      <author><first>Alessio</first><last>Cocchieri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Giacomo</first><last>Frisoni</last></author>
      <author><first>Marcos</first><last>Martínez Galindo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <author><first>Giuseppe</first><last>Tagliavini</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Francesco</first><last>Candoli</last><affiliation>University of Bologna</affiliation></author>
      <pages>818-837</pages>
      <abstract>Biomedical Named Entity Recognition (BioNER) faces significant challenges in real-world applications due to limited annotated data and the constant emergence of new entity types, making zero-shot learning capabilities crucial. While Large Language Models (LLMs) possess extensive domain knowledge necessary for specialized fields like biomedicine, their computational costs often make them impractical. To address these challenges, we introduce OpenBioNER, a lightweight BERT-based cross-encoder architecture that can identify any biomedical entity using only its description, eliminating the need for retraining on new, unseen entity types. Through comprehensive evaluation on established biomedical benchmarks, we demonstrate that OpenBioNER surpasses state-of-the-art baselines, including specialized 7B NER LLMs and GPT-4o, achieving up to 10% higher F1 scores while using 110M parameters only. Moreover, OpenBioNER outperforms existing small-scale models that match textual spans with entity types rather than descriptions, both in terms of accuracy and computational efficiency.</abstract>
      <url hash="24112436">2025.findings-naacl.47</url>
      <bibkey>cocchieri-etal-2025-openbioner</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.47</doi>
    </paper>
    <paper id="48">
      <title>Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum</title>
      <author><first>Ryan Soh-Eun</first><last>Shim</last><affiliation>Ludwig-Maximilians-Universität München, University of Stuttgart, Universität Stuttgart and Institute for Natural Language Processing, University of Stuttgart</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>838-849</pages>
      <abstract>There is increasing interest in looking at dialects in NLP. However, most work to date still treats dialects as discrete categories. For instance, evaluative work in variation-oriented NLP for English often works with Indian English or African-American Venacular English as homogeneous categories, yet even within one variety there is substantial variation. We examine within-dialect variation and show that performance critically varies within categories. We measure speech-to-text performance on Italian dialects, and empirically observe a geographical performance disparity. This disparity correlates substantially (-0.5) with linguistic similarity to the highest performing dialect variety. We cross-examine our results against dialectometry methods, and interpret the performance disparity to be due to a bias towards dialects that are more similar to the standard variety in the speech-to-text model examined. We additionally leverage geostatistical methods to predict zero-shot performance at unseen sites, and find the incorporation of geographical information to substantially improve prediction performance, indicating there to be geographical structure in the performance distribution.</abstract>
      <url hash="2adec0a7">2025.findings-naacl.48</url>
      <bibkey>shim-plank-2025-dialetto</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.48</doi>
    </paper>
    <paper id="49">
      <title>Linguistically Grounded Analysis of Language Models using Shapley Head Values</title>
      <author><first>Marcell</first><last>Fekete</last></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>850-865</pages>
      <abstract>Understanding how linguistic knowledge is encoded in language models is crucial for improving their generalisation capabilities. In this paper, we investigate the processing of morphosyntactic phenomena, by leveraging a recently proposed method for probing language models via Shapley Head Values (SHVs). Using the English language BLiMP dataset, we test our approach on two widely used models, BERT and RoBERTa, and compare how linguistic constructions such as anaphor agreement and filler-gap dependencies are handled. Through quantitative pruning and qualitative clustering analysis, we demonstrate that attention heads responsible for processing related linguistic phenomena cluster together. Our results show that SHV-based attributions reveal distinct patterns across both models, providing insights into how language models organize and process linguistic information. These findings support the hypothesis that language models learn subnetworks corresponding to linguistic theory, with potential implications for cross-linguistic model analysis and interpretability in Natural Language Processing (NLP).</abstract>
      <url hash="9d8a1eb9">2025.findings-naacl.49</url>
      <bibkey>fekete-bjerva-2025-linguistically</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.49</doi>
    </paper>
    <paper id="50">
      <title>How Do Large Language Models Perform in Dynamical System Modeling</title>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Binqi</first><last>Chen</last></author>
      <author><first>Haixin</first><last>Wang</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>866-880</pages>
      <abstract>This paper studies the problem of dynamical system modeling, which involves the evolution of multiple interacting objects. Recent data-driven methods often utilize graph neural networks (GNNs) to learn these interactions by optimizing the neural network in an end-to-end fashion. While large language models (LLMs) have shown exceptional zero-shot performance across various applications, their potential for modeling dynamical systems has not been extensively explored. In this work, we design prompting techniques for dynamical system modeling and systematically evaluate the capabilities of LLMs on two tasks, including dynamic forecasting and relational reasoning. An extensive benchmark LLM4DS across nine datasets is built for performance comparison. Our extensive experiments yield several key findings: (1) LLMs demonstrate competitive performance without training compared to state-of-the-art methods in dynamical system modeling. (2) LLMs effectively infer complex interactions among objects to capture system evolution. (3) Prompt engineering plays a crucial role in enabling LLMs to accurately understand and predict the evolution of systems.</abstract>
      <url hash="818cfa0c">2025.findings-naacl.50</url>
      <bibkey>luo-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>LMM</fixed-case>s-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
      <author><first>Kaichen</first><last>Zhang</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Peiyuan</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Fanyi</first><last>Pu</last></author>
      <author><first>Joshua Adrian</first><last>Cahyono</last></author>
      <author><first>Kairui</first><last>Hu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Shuai</first><last>Liu</last></author>
      <author><first>Yuanhan</first><last>Zhang</last></author>
      <author><first>Jingkang</first><last>Yang</last></author>
      <author><first>Chunyuan</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>881-916</pages>
      <abstract>The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models’ generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs.</abstract>
      <url hash="e64a10ea">2025.findings-naacl.51</url>
      <bibkey>zhang-etal-2025-lmms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.51</doi>
    </paper>
    <paper id="52">
      <title>Pairwise Prompt-Based Tuning with Parameter Efficient Fast Adaptation for Generalized Zero-Shot Intent Detection</title>
      <author><first>Xiaotong</first><last>Zhang</last></author>
      <author><first>Qianru</first><last>Zhou</last></author>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>917-929</pages>
      <abstract>Generalized zero-shot intent detection (GZID) aims to recognize the labels of utterances from both seen and unseen intents by utilizing the knowledge learned from seen intents. Enhancing the generalization ability from seen intents to unseen intents is a key challenge in the GZID setting. Existing methods attempt to tackle this challenge by distinguishing unseen intents from seen intents or focusing on enhancing the model discriminability. However, the challenge is not solved substantially as they ignore to promote the representation learning ability of the model itself and neglect to strengthen the model adaptability to new tasks, resulting in overfitting on the seen intents. In this paper, we propose a pairwise prompt-based tuning model with parameter efficient fast adaptation which involves two training steps. In the first step, we leverage hybrid contrastive learning in discriminant space and masked language modeling to make predictions at both sentence and token levels, which can enhance the model discriminability and representation learning ability respectively. In the second step, we design a pipeline for generating and filtering unseen data by only providing unseen intent labels, and utilize parameter-efficient fine-tuning to quickly adapt to unseen intents. Experiments on four intent detection datasets demonstrate that our two-step training method has better comprehension and generalization capabilities.</abstract>
      <url hash="171b8196">2025.findings-naacl.52</url>
      <bibkey>zhang-etal-2025-pairwise</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.52</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>F</fixed-case>aithful<fixed-case>P</fixed-case>ersona: Balancing Faithfulness and Personalization in Code Explanations through Self-Critique</title>
      <author><first>Zhuang</first><last>Luo</last></author>
      <author><first>Yichuan</first><last>Li</last></author>
      <author><first>Zexing</first><last>Xu</last></author>
      <author><first>Kyumin</first><last>Lee</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>S. Rasoul</first><last>Etesami</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>930-944</pages>
      <abstract>Code explanations are crucial in real-world life, from educating students to aligning technical projects with business goals. However, existing approaches face challenges balancing faithfulness to the original code and personalization for diverse user needs. This paper addresses these challenges by introducing a novel benchmark and method for generating faithful personalized code explanations. Our benchmark, FaithfulPersonaCodeX, incorporates code samples and user profiles, employing various evaluation metrics to evaluate both faithfulness and personalization. We propose DISCO, a new method that uses a self-critique mechanism and two-stage optimization to balance faithfulness and personalization in code explanations, addressing the limitations of current large language model approaches. Our proposed model, DISCO, achieves a notable 3.7% improvement in Pass@5 compared to the strong baseline method, Self-Consistency, while maintaining high personalization with a 61.08% win rate in the LLM-as-a-Judge evaluation, effectively balancing faithfulness and user-specific needs in code explanations.</abstract>
      <url hash="c8dbc2e5">2025.findings-naacl.53</url>
      <bibkey>luo-etal-2025-faithfulpersona</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.53</doi>
    </paper>
    <paper id="54">
      <title>Efficient Multi-Agent Collaboration with Tool Use for Online Planning in Complex Table Question Answering</title>
      <author><first>Wei</first><last>Zhou</last><affiliation>Robert Bosch GmbH, Bosch</affiliation></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <pages>945-968</pages>
      <abstract>Complex table question answering (TQA) aims to answer questions that require complex reasoning, such as multi-step or multi-category reasoning, over data represented in tabular form. Previous approaches demonstrate notable performance by leveraging either closed-source large language models (LLMs) or fine-tuned open-weight LLMs. However, fine-tuning LLMs requires high-quality training data, which is costly to obtain. The use of closed-source LLMs poses accessibility challenges and leads to reproducibility issues. In this paper, we propose Multi Agent Collaboration with Tool use (MACT), a framework that requires neither fine-tuning nor closed-source models. In MACT, a planning agent and a coding agent that also make use of tools collaborate for TQA. MACT outperforms previous SoTA systems on three out of four benchmarks and performs comparably to the larger and more expensive closed-source model GPT-4 on two benchmarks, even when using only open-weight models without any fine-tuning. Our extensive analyses prove the effectiveness of MACT’s multi-agent collaboration in TQA. We release our code publicly.</abstract>
      <url hash="a0bc8540">2025.findings-naacl.54</url>
      <bibkey>zhou-etal-2025-efficient</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.54</doi>
    </paper>
    <paper id="55">
      <title>Ground Every Sentence: Improving Retrieval-Augmented <fixed-case>LLM</fixed-case>s with Interleaved Reference-Claim Generation</title>
      <author><first>Sirui</first><last>Xia</last></author>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <author><first>Weikang</first><last>Zhou</last></author>
      <author><first>Jiaji</first><last>Deng</last></author>
      <author><first>Fei</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>969-988</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. To enhance credibility and verifiability in RAG systems, Attributed Text Generation (ATG) is proposed, which provides citations to retrieval knowledge in LLM-generated responses. Prior methods mainly adopt coarse-grained attributions, with passage-level or paragraph-level references or citations, which fall short in verifiability. This paper proposes ReClaim(Refer &amp; Claim), a fine-grained ATG method that alternates the generation of references and answers step by step. Different from previous coarse-grained attribution, ReClaim provides sentence-level citations in long-form question-answering tasks. With extensive experiments, we verify the effectiveness of ReClaim in extensive settings, achieving a citation accuracy rate of 90%.</abstract>
      <url hash="c12c279c">2025.findings-naacl.55</url>
      <bibkey>xia-etal-2025-ground</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.55</doi>
    </paper>
    <paper id="56">
      <title>Understanding the Role of Mental Models in User Interaction with an Adaptive Dialog Agent</title>
      <author><first>Lindsey Morgan</first><last>Vanderlyn</last></author>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Thang</first><last>Vu</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <pages>989-1015</pages>
      <abstract>Mental models play an important role in whether user interactions with intelligent systems, such as dialog agents, are successful. Adaptive dialog systems present the opportunity to align a dialog agent’s behavior with heterogeneous user expectations. However, there has been little research into what mental models users form when interacting with a task-oriented dialog system, how these models affect users’ interactions, or what role system adaptation can play in this process. This can make it challenging to avoid damage to human-AI partnership. In this work, we collect a new publicly available dataset for exploring user mental models of information seeking dialog systems. We demonstrate that users have a variety of conflicting mental models about such systems, the validity of which directly impacts the success and perception of their interactions. Furthermore, we show that adapting a dialog agent’s behavior to better align with users’ mental models, even when done implicitly, can improve dialog efficiency, success, and user perception of the interaction. This shows that implicit adaptation can be beneficial for task-oriented dialog systems, so long as developers understand the mental models of their users.</abstract>
      <url hash="d0d8364e">2025.findings-naacl.56</url>
      <bibkey>vanderlyn-etal-2025-understanding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>C</fixed-case>o<fixed-case>PERL</fixed-case>ex: Content Planning with Event-based Representations for Legal Case Summarization</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Youssef</first><last>Farag</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1016-1032</pages>
      <abstract>Legal professionals often struggle with lengthy judgments and require efficient summarization for quick comprehension. To address this challenge, we investigate the need for structured planning in legal case summarization, particularly through event-centric representations that reflect the narrative nature of legal case documents. We propose our framework, CoPERLex, which operates in three stages: first, it performs content selection to identify crucial information from the judgment; second, the selected content is utilized to generate intermediate plans through event-centric representations modeled as Subject-Verb-Object tuples; and finally, it generates coherent summaries based on both the content and the structured plan. Our experiments on four legal summarization datasets demonstrate the effectiveness of integrating content selection and planning components, highlighting the advantages of event-centric plans over traditional entity-centric approaches in the context of legal judgements.</abstract>
      <url hash="22890085">2025.findings-naacl.57</url>
      <bibkey>t-y-s-s-etal-2025-coperlex</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>D</fixed-case>is<fixed-case>C</fixed-case>omp: A Two-Stage Prompt Optimization Framework Combining Task-Agnostic and Task-Aware Compression</title>
      <author><first>Liu</first><last>Quancai</last></author>
      <author><first>Haihui</first><last>Fan</last><affiliation>Institute of Information Engineering,CAS</affiliation></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Lixiangfang</first><last>Lixiangfang</last></author>
      <author><first>Lichuanrong</first><last>Lichuanrong</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>1033-1044</pages>
      <abstract>Large language models (LLMs) exhibit exceptional performance across a wide range of natural language processing tasks, often relying on lengthy prompts to harness their full capabilities. However, extended prompts can lead to substantial computational overhead and increased hardware demands, limiting the scalability and efficiency of such models. In this paper, we propose DisComp, a two-stage prompt compression framework based on knowledge distillation that combines task-agnostic and task-aware strategies, designed to efficiently compress prompt length without compromising performance.In the first stage, task-agnostic compression is achieved through knowledge distillation, transferring the summarization capabilities of a LLM to a smaller, more efficient model. The distillation process combines cross-entropy loss and keyword matching loss to ensure the smaller model generates concise and informative summaries. In the second stage, sentence-level pruning is applied, where sentences are ranked by relevance to the query, and irrelevant sentences are pruned to retain only task-critical information. We evaluate our method on three benchmark datasets, LongBench , ZeroSCROLLS and NaturalQuestions. The results show that DisComp significantly outperforms previous task-agnostic and task-specific compression approaches, and it is up to 6.56× faster at inference compared to the best token-level compression method.</abstract>
      <url hash="cd154829">2025.findings-naacl.58</url>
      <bibkey>quancai-etal-2025-discomp</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.58</doi>
    </paper>
    <paper id="59">
      <title>A Large-Scale Benchmark for <fixed-case>V</fixed-case>ietnamese Sentence Paraphrases</title>
      <author><first>Sang Quang</first><last>Nguyen</last></author>
      <author><first>Kiet Van</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <pages>1045-1060</pages>
      <abstract>This paper presents ViSP, a high-quality Vietnamese dataset for sentence paraphrasing, consisting of 1.2M original–paraphrase pairs collected from various domains. The dataset was constructed using a hybrid approach that combines automatic paraphrase generation with manual evaluation to ensure high quality. We conducted experiments using methods such as back-translation, EDA, and baseline models like BART and T5, as well as large language models (LLMs), including GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To the best of our knowledge, this is the first large-scale study on Vietnamese paraphrasing. We hope that our dataset and findings will serve as a valuable foundation for future research and applications in Vietnamese paraphrase tasks. The dataset is available for research purposes at <url>https://github.com/ngwgsang/ViSP</url>.</abstract>
      <url hash="7751d47e">2025.findings-naacl.59</url>
      <bibkey>nguyen-nguyen-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>RAMQA</fixed-case>: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering</title>
      <author><first>Yang</first><last>Bai</last><affiliation>Facebook</affiliation></author>
      <author><first>Christan</first><last>Grant</last><affiliation>University of Florida</affiliation></author>
      <author><first>Daisy Zhe</first><last>Wang</last><affiliation>University of Florida</affiliation></author>
      <pages>1061-1076</pages>
      <abstract>Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Data and code will be made public once the paper is accepted.</abstract>
      <url hash="04f85d0e">2025.findings-naacl.60</url>
      <bibkey>bai-etal-2025-ramqa</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.60</doi>
    </paper>
    <paper id="61">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>CAT</fixed-case>: Multimodal Communication Annotations for Teams</title>
      <author><first>Adarsh</first><last>Pyarelal</last><affiliation>University of Arizona</affiliation></author>
      <author><first>John M</first><last>Culnan</last><affiliation>US Department of Veterans Affairs</affiliation></author>
      <author><first>Ayesha</first><last>Qamar</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Meghavarshini</first><last>Krishnaswamy</last></author>
      <author><first>Yuwei</first><last>Wang</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Cheonkam</first><last>Jeong</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Md Messal Monem</first><last>Miah</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shahriar</first><last>Hormozi</last></author>
      <author><first>Jonathan</first><last>Tong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>1077-1111</pages>
      <abstract>Successful teamwork requires team members to understand each other and communicate effectively, managing multiple linguistic and paralinguistic tasks at once. Because of the potential for interrelatedness of these tasks, it is important to have the ability to make multiple types of predictions on the same dataset. Here, we introduce Multimodal Communication Annotations for Teams (MultiCAT), a speech- and text-based dataset consisting of audio recordings, automated and hand-corrected transcriptions. MultiCAT builds upon data from teams working collaboratively to save victims in a simulated search and rescue mission, and consists of annotations and benchmark results for the following tasks: (1) dialog act classification, (2) adjacency pair detection, (3) sentiment and emotion recognition, (4) closed-loop communication detection, and (5) vocal (phonetic) entrainment detection. We also present exploratory analyses on the relationship between our annotations and team outcomes. We posit that additional work on these tasks and their intersection will further improve understanding of team communication and its relation to team performance. Code &amp; data: https://doi.org/10.5281/zenodo.14834835</abstract>
      <url hash="611d2ab3">2025.findings-naacl.61</url>
      <bibkey>pyarelal-etal-2025-multicat</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.61</doi>
    </paper>
    <paper id="62">
      <title>Prototype Tuning: A Meta-Learning Approach for Few-Shot Document-Level Relation Extraction with Large Language Models</title>
      <author><first>Dinghao</first><last>Pan</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Yuanyuan</first><last>Sun</last></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Jiru</first><last>Li</last></author>
      <author><first>Zhihao</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Ling</first><last>Luo</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Jian</first><last>Wang</last></author>
      <pages>1112-1128</pages>
      <abstract>Few-Shot Document-Level Relation Extraction (FSDLRE) aims to develop models capable of generalizing to new categories with minimal support examples. Although Large Language Models (LLMs) demonstrate exceptional In-Context Learning (ICL) capabilities on many few-shot tasks, their performance on FSDLRE tasks remains suboptimal due to the significant gap between the task format and the intrinsic capabilities of language models, coupled with the complexity of ICL prompts for document-level text. To address these challenges, we introduce a novel meta-training approach for LLMs termed Prototype Tuning. We construct simulated episodes using data with relation types that do not overlap with the test corpus, fundamentally enhancing the ICL capabilities of LLMs in FSDLRE through meta-learning. To further enhance the effects of meta-learning, we innovatively integrate the concept of prototype into the fine-tuning process of LLMs. This involves aggregating entity pairs from support documents into prototypes within the prompts and altering the way of determining relation categories to identifying the closest prototype. Experimental results demonstrate that our LLMs trained with this approach outperform all baselines. Our proposed approach markedly improves the ICL capabilities of LLMs in FSDLRE and mitigates the impact of relation semantic discrepancies between the training corpus and the test corpus on model performance.</abstract>
      <url hash="ced75298">2025.findings-naacl.62</url>
      <bibkey>pan-etal-2025-prototype</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>L</fixed-case>egal<fixed-case>S</fixed-case>eg: Unlocking the Structure of <fixed-case>I</fixed-case>ndian Legal Judgments Through Rhetorical Role Classification</title>
      <author><first>Shubham Kumar</first><last>Nigam</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Tanmay</first><last>Dubey</last></author>
      <author><first>Govind</first><last>Sharma</last></author>
      <author><first>Noel</first><last>Shallum</last><affiliation>Symbiosis Law School Pune</affiliation></author>
      <author><first>Kripabandhu</first><last>Ghosh</last><affiliation>Indian Institute of Science Education and Research Kolkata</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>1129-1144</pages>
      <abstract>In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce **LegalSeg**, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory **RhetoricLLaMA**, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.</abstract>
      <url hash="0000ac1f">2025.findings-naacl.63</url>
      <bibkey>nigam-etal-2025-legalseg</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.63</doi>
    </paper>
    <paper id="64">
      <title>Claim-Guided Textual Backdoor Attack for Practical Applications</title>
      <author><first>Minkyoo</first><last>Song</last></author>
      <author><first>Hanna</first><last>Kim</last></author>
      <author><first>Jaehan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Youngjin</first><last>Jin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Texas A&amp;M University - College Station</affiliation></author>
      <pages>1145-1159</pages>
      <abstract>Recent advances in natural language processing and the increased use of large language models have exposed new security vulnerabilities, such as backdoor attacks. Previous backdoor attacks require input manipulation after model distribution to activate the backdoor, posing limitations in real-world applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor Attack (CGBA), which eliminates the need for such manipulations by utilizing inherent textual claims as triggers. CGBA leverages claim extraction, clustering, and targeted training to trick models to misbehave on targeted claims without affecting their performance on clean data. CGBA demonstrates its effectiveness and stealthiness across various datasets and models, significantly enhancing the feasibility of practical backdoor attacks. Our code and data will be available at https://github.com/minkyoo9/CGBA.</abstract>
      <url hash="455380fe">2025.findings-naacl.64</url>
      <bibkey>song-etal-2025-claim</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.64</doi>
    </paper>
    <paper id="65">
      <title><fixed-case>T</fixed-case>ool<fixed-case>S</fixed-case>andbox: A Stateful, Conversational, Interactive Evaluation Benchmark for <fixed-case>LLM</fixed-case> Tool Use Capabilities</title>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Thomas</first><last>Holleis</last><affiliation>Apple</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Bernhard</first><last>Aumayer</last><affiliation>Apple</affiliation></author>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Haoping</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Shuang</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Shen</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Mengyu</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Guoli</first><last>Yin</last><affiliation>Apple</affiliation></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <pages>1160-1183</pages>
      <abstract>Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over arbitrary trajectory. We show that open source and proprietary models has a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights to tool-use LLM capabilities. Datasets and evaluation scripts of ToolSandbox are released at &lt;placeholder&gt;.</abstract>
      <url hash="89e64cf1">2025.findings-naacl.65</url>
      <bibkey>lu-etal-2025-toolsandbox</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.65</doi>
    </paper>
    <paper id="66">
      <title><tex-math>SusGen-GPT</tex-math>: A Data-Centric <fixed-case>LLM</fixed-case> for Financial <fixed-case>NLP</fixed-case> and Sustainability Report Generation</title>
      <author><first>Qilong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaoneng</first><last>Xiang</last><affiliation>Huawei Singapore Research Center</affiliation></author>
      <author><first>Huang</first><last>Hejia</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Yeo</first><last>Wei Jie</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Ranjan</first><last>Satapathy</last></author>
      <author><first>Ricardo Shirota</first><last>Filho</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Bharadwaj</first><last>Veeravalli</last></author>
      <pages>1184-1203</pages>
      <abstract>The rapid growth of the financial sector and the increasing focus on Environmental, Social, and Governance (ESG) considerations have created a pressing need for advanced natural language processing (NLP) tools. Despite recent advancements, there is still a notable absence of open-source Large Language Models (LLMs) that are proficient across both general finance and ESG domains, such as generating ESG reports. To address this gap, we introduce <tex-math>SusGen</tex-math>-<tex-math>30k</tex-math>, a high-quality, category-balanced dataset comprising seven financial NLP tasks. In addition, we propose <tex-math>TCFD</tex-math>-<tex-math>Bench</tex-math>, a benchmark designed to improve the evaluation of sustainability report generation. Our data-centric approach led to the development of a suite of models, <tex-math>SusGen</tex-math>-<tex-math>GPT</tex-math>, trained on the curated dataset. These models were evaluated across six adapted tasks and two off-the-shelf tasks, showing state-of-the-art performance, surpassing all other models except GPT-4. Remarkably, <tex-math>SusGen</tex-math>-<tex-math>GPT</tex-math> achieved an average score only 0.02 below GPT-4, despite using models with only 7-8B parameters compared to much larger GPT-4. This demonstrates the efficiency of our approach in delivering high performance with significantly fewer resources, addressing existing challenges and fostering further advancements in the financial and ESG research community.</abstract>
      <url hash="7d056bfd">2025.findings-naacl.66</url>
      <bibkey>wu-etal-2025-susgen</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.66</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>G</fixed-case>r<fixed-case>E</fixed-case>m<fixed-case>LI</fixed-case>n: A Repository of Green Baseline Embeddings for 87 Low-Resource Languages Injected with Multilingual Graph Knowledge</title>
      <author><first>Daniil</first><last>Gurgurov</last></author>
      <author><first>Rishu</first><last>Kumar</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>1204-1221</pages>
      <url hash="ba6a69da">2025.findings-naacl.67</url>
      <bibkey>gurgurov-etal-2025-gremlin</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.67</doi>
    </paper>
    <paper id="68">
      <title>In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation</title>
      <author><first>Armel Randy</first><last>Zebaze</last><affiliation>INRIA</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>1222-1252</pages>
      <abstract>The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. In this paper, we focus on machine translation (MT), a task that has been shown to benefit from in-context translation examples. However no systematic studies have been published on how best to select examples, and mixed results have been reported on the usefulness of similarity-based selection over random selection, although these results have mainly been shown for high-resource languages only. We provide a study covering multiple LLMs and in-context example retrieval strategies. Contrarily to previously published results, we find that retrieval based on sentence embedding similarity can improve MT, especially for low-resource language directions, and we also discuss the balance between selection pool diversity and quality. Code and outputs will be made freely available.</abstract>
      <url hash="c0600ee4">2025.findings-naacl.68</url>
      <bibkey>zebaze-etal-2025-context</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.68</doi>
    </paper>
    <paper id="69">
      <title>Self-Training Large Language Models for Tool-Use Without Demonstrations</title>
      <author><first>Ne</first><last>Luo</last></author>
      <author><first>Aryo Pradipta</first><last>Gema</last><affiliation>Anthropic and University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Emile</first><last>Van Krieken</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Pietro</first><last>Lesci</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>1253-1271</pages>
      <abstract>Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.</abstract>
      <url hash="6ad5be04">2025.findings-naacl.69</url>
      <bibkey>luo-etal-2025-self</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.69</doi>
    </paper>
    <paper id="70">
      <title>Can Large Language Models Generate High-quality Patent Claims?</title>
      <author><first>Lekang</first><last>Jiang</last></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Pascal A.</first><last>Scherz</last></author>
      <author><first>Stefan</first><last>Goetz</last><affiliation>University of Cambridge and Duke University</affiliation></author>
      <pages>1272-1287</pages>
      <abstract>Large language models (LLMs) have shown exceptional performance across various text generation tasks, but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions’ features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.</abstract>
      <url hash="a6af09bb">2025.findings-naacl.70</url>
      <bibkey>jiang-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.70</doi>
    </paper>
    <paper id="71">
      <title>Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm</title>
      <author><first>Jaehan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minkyoo</first><last>Song</last></author>
      <author><first>Seung Ho</first><last>Na</last></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Texas A&amp;M University - College Station</affiliation></author>
      <pages>1288-1307</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6%<tex-math>\downarrow</tex-math>). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks. Source code will be obtained at https://github.com/jaehanwork/Obliviate.</abstract>
      <url hash="c5be84fc">2025.findings-naacl.71</url>
      <bibkey>kim-etal-2025-obliviate</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>CORAL</fixed-case>: Benchmarking Multi-turn Conversational Retrieval-Augmented Generation</title>
      <author><first>Yiruo</first><last>Cheng</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Hongjin</first><last>Qian</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yongkang</first><last>Wu</last></author>
      <author><first>Tetsuya</first><last>Sakai</last><affiliation>NAVER and Waseda University</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1308-1330</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.</abstract>
      <url hash="585f2dfe">2025.findings-naacl.72</url>
      <bibkey>cheng-etal-2025-coral</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.72</doi>
    </paper>
    <paper id="73">
      <title>Beyond <fixed-case>E</fixed-case>nglish: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Itai</first><last>Mondshine</last></author>
      <author><first>Tzuf</first><last>Paz-Argaman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>1331-1354</pages>
      <abstract>Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings</abstract>
      <url hash="1b12d9bb">2025.findings-naacl.73</url>
      <bibkey>mondshine-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>Q</fixed-case>ua<fixed-case>LLM</fixed-case>: An <fixed-case>LLM</fixed-case>-based Framework to Extract Quantitative Insights from Online Forums</title>
      <author><first>Varun</first><last>Nagaraj Rao</last><affiliation>Princeton University</affiliation></author>
      <author><first>Eesha</first><last>Agarwal</last></author>
      <author><first>Samantha</first><last>Dalal</last></author>
      <author><first>Dana</first><last>Calacci</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Andrés</first><last>Monroy-Hernández</last><affiliation>Department of Computer Science, Princeton University</affiliation></author>
      <pages>1355-1369</pages>
      <abstract>Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit’s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.</abstract>
      <url hash="afdeea63">2025.findings-naacl.74</url>
      <bibkey>nagaraj-rao-etal-2025-quallm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.74</doi>
    </paper>
    <paper id="75">
      <title>The Promises and Pitfalls of <fixed-case>LLM</fixed-case> Annotations in Dataset Labeling: a Case Study on Media Bias Detection</title>
      <author><first>Tomáš</first><last>Horych</last></author>
      <author><first>Christoph</first><last>Mandl</last><affiliation>Media Bias Group</affiliation></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Andre</first><last>Greiner-Petter</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Timo</first><last>Spinde</last></author>
      <pages>1370-1386</pages>
      <abstract>High annotation costs from hiring or crowdsourcing complicate the creation of large, high-quality datasets needed for training reliable text classifiers. Recent research suggests using Large Language Models (LLMs) to automate the annotation process, reducing these costs while maintaining data quality. LLMs have shown promising results in annotating downstream tasks like hate speech detection and political framing. Building on the success in these areas, this study investigates whether LLMs are viable for annotating a complex task of media bias detection and whether a downstream media bias classifier can be trained on such data. We create Annolexical, the first large-scale dataset for media bias classification with over 48k synthetically annotated examples. Our classifier fine-tuned on it surpasses all of the annotator LLMs by 5-9% in Mathew’s Correlation Coefficient (MCC) and performs close to or outperforms the model trained on human-labeled data when evaluated on two media bias benchmark datasets (BABE and BASIL). This study demonstrates how our approach significantly reduces the cost of dataset creation in the media bias domain and, by extension - the development of the classifiers, while our subsequent behavioral stress-testing reveals some of its current limitations and trade-offs.</abstract>
      <url hash="9416a6ed">2025.findings-naacl.75</url>
      <bibkey>horych-etal-2025-promises</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.75</doi>
    </paper>
    <paper id="76">
      <title>Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning</title>
      <author><first>Lin</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Lijie</first><last>Hu</last><affiliation>KAUST</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>1387-1404</pages>
      <abstract>Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.</abstract>
      <url hash="bf371113">2025.findings-naacl.76</url>
      <bibkey>zhang-etal-2025-mechanistic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.76</doi>
    </paper>
    <paper id="77">
      <title>Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models</title>
      <author><first>Yuyi</first><last>Huang</last></author>
      <author><first>Runzhe</first><last>Zhan</last><affiliation>University of Macau</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia S.</first><last>Chao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Ailin</first><last>Tao</last><affiliation>Guangzhou Medical University</affiliation></author>
      <pages>1405-1425</pages>
      <abstract>Large language models (LLMs) have significantly influenced various industries but suffer from a critical flaw, the potential sensitivity of generating harmful content, which poses severe societal risks. We developed and tested novel attack strategies on popular LLMs to expose their vulnerabilities in generating inappropriate content. These strategies, inspired by psychological phenomena such as the “Priming Effect”, “Safe Attention Shift”, and “Cognitive Dissonance”, effectively attack the models’ guarding mechanisms. Our experiments achieved an attack success rate (ASR) of 100% on various open-source models, including Meta’s Llama-3.2, Google’s Gemma-2, Mistral’s Mistral-NeMo, Falcon’s Falcon-mamba, Apple’s DCLM, Microsoft’s Phi3, and Qwen’s Qwen2.5, among others. Similarly, for closed-source models such as OpenAI’s GPT-4o, Google’s Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95% on the AdvBench dataset, which represents the current state-of-the-art. This study underscores the urgent need to reassess the use of generative models in critical applications to mitigate potential adverse societal impacts.</abstract>
      <url hash="9c51c543">2025.findings-naacl.77</url>
      <bibkey>huang-etal-2025-intrinsic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>A</fixed-case>d<fixed-case>P</fixed-case>araphrase: Paraphrase Dataset for Analyzing Linguistic Features toward Generating Attractive Ad Texts</title>
      <author><first>Soichiro</first><last>Murakami</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>1426-1439</pages>
      <abstract>Effective linguistic choices that attract potential customers play crucial roles in advertising success. This study aims to explore the linguistic features of ad texts that influence human preferences. Although the creation of attractive ad texts is an active area of research, progress in understanding the specific linguistic features that affect attractiveness is hindered by several obstacles. First, human preferences are complex and influenced by multiple factors, including their content, such as brand names, and their linguistic styles, making analysis challenging. Second, publicly available ad text datasets that include human preferences are lacking, such as ad performance metrics and human feedback, which reflect people’s interests. To address these problems, we present AdParaphrase, a paraphrase dataset that contains human preferences for pairs of ad texts that are semantically equivalent but differ in terms of wording and style. This dataset allows for preference analysis that focuses on the differences in linguistic features. Our analysis revealed that ad texts preferred by human judges have higher fluency, longer length, more nouns, and use of bracket symbols. Furthermore, we demonstrate that an ad text-generation model that considers these findings significantly improves the attractiveness of a given text. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase.</abstract>
      <url hash="50d0f480">2025.findings-naacl.78</url>
      <bibkey>murakami-etal-2025-adparaphrase</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.78</doi>
    </paper>
    <paper id="79">
      <title>Token Weighting for Long-Range Language Modeling</title>
      <author><first>Falko</first><last>Helm</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Nico</first><last>Daheim</last><affiliation>ETHZ - ETH Zurich and Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>1440-1459</pages>
      <abstract>Many applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training could contribute to this, because each token is assigned equal weight. Yet, intuitively, the amount of context needed to predict the next token accurately varies greatly across different data. To reflect this, we propose various novel token-weighting schemes that assign different weights to each training token in the loss, thereby generalizing existing works. For this, we categorize token-weighting methods using a two-step framework which compares the confidences of a long-context and short-context model to score tokens. We evaluate all methods on multiple long-context understanding tasks and show that non-uniform loss weights are helpful to improve the long-context abilities of LLMs.Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained.All in all, this work contributes to a better understanding of the trade-offs long-context language modeling faces and provides guidelines for model steering via loss-weighting based on empirical evidence. The code can be found on [Github](https://github.com/UKPLab/naacl2025-token-weighting).</abstract>
      <url hash="2e8645cb">2025.findings-naacl.79</url>
      <bibkey>helm-etal-2025-token</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.79</doi>
    </paper>
    <paper id="80">
      <title>Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation</title>
      <author><first>Takyoung</first><last>Kim</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Kyungjae</first><last>Lee</last><affiliation>University of Seoul</affiliation></author>
      <author><first>Young Rok</first><last>Jang</last></author>
      <author><first>Ji Yong</first><last>Cho</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <author><first>Minseok</first><last>Cho</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>LG Corporation and University of Illinois, Chicago</affiliation></author>
      <pages>1460-1480</pages>
      <abstract>Interactions with large language models (LLMs) often yield long and detailed responses, leveraging both parametric knowledge and retrieval-augmented generation (RAG). While these responses can provide rich insights, they often include redundant or less engaging content not aligned with user interests. This issue becomes apparent when users specify particular subtopics to include or exclude – termed **coverage-conditioned (<tex-math>C^2</tex-math>)** queries – as LLMs often struggle to provide tailored responses. To address this challenge, we investigate the role of query outlines, sequences of subqueries designed to guide LLMs in generating responses that meet specific user requirements. To systematically create and evaluate these outlines, we introduce **QTree**, a dataset of 10K hierarchical sets of information-seeking subqueries that define structured boundaries for outline creation and evaluation in <tex-math>C^2</tex-math> scenarios. Additionally, we develop **QPlanner**, a 7B language model trained to generate customized outlines within boundaries of QTree. We evaluate the effectiveness of the generated outlines through automatic and human judgements, focusing on their impact within retrieval-augmented generation (RAG) systems. Experimental results demonstrate that QPlanner, especially when trained with alignment techniques like DPO, generates higher-quality outlines that better fulfill diverse user needs.</abstract>
      <url hash="0eadc692">2025.findings-naacl.80</url>
      <bibkey>kim-etal-2025-learning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.80</doi>
    </paper>
    <paper id="81">
      <title><fixed-case>L</fixed-case>ay<fixed-case>A</fixed-case>lign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy</title>
      <author><first>Zhiwen</first><last>Ruan</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>1481-1495</pages>
      <abstract>Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder’s output, overlooking valuable information from other layers. We propose Layer-Wise Adaptive Fusion and Alignment Strategy (LayAlign), a framework that integrates representations from all encoder layers, coupled with the adaptive fusion-enhanced attention mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.</abstract>
      <url hash="6a5a650b">2025.findings-naacl.81</url>
      <bibkey>ruan-etal-2025-layalign</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.81</doi>
    </paper>
    <paper id="82">
      <title>On the Impacts of Contexts on Repository-Level Code Generation</title>
      <author><first>Nam Le</first><last>Hai</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Dung Manh</first><last>Nguyen</last></author>
      <author><first>Nghi D. Q.</first><last>Bui</last></author>
      <pages>1496-1524</pages>
      <abstract>CodeLLMs are widely used for code generation, yet their ability to handle repository-level dependencies remains underexplored. We introduce RepoExec, a benchmark for evaluating repository-level code generation, focusing on executability, functional correctness, and dependency utilization. Our study evaluates 18 models, revealing that retaining full dependency context yields the best performance, while smaller context sizes can be misleading. Pretrained LLMs excel in correctness but often reimplement dependencies, while instruction-tuned models better utilize dependencies but sometimes introduce unnecessary complexity. We propose an instruction-tuning dataset that improves dependency handling and introduce a new metric, Dependency Invocation Rate (DIR), to measure context utilization. Experiments show that instruction-tuned models improve DIR by over 10%, and multi-round debugging further enhances both correctness and dependency use. RepoExec provides a comprehensive framework to advance CodeLLMs for real-world applications. The dataset and source code are available at <url>https://github.com/FSoft-AI4Code/RepoExec</url>.</abstract>
      <url hash="ba83aa59">2025.findings-naacl.82</url>
      <bibkey>hai-etal-2025-impacts</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.82</doi>
    </paper>
    <paper id="83">
      <title>From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis</title>
      <author><first>Moritz</first><last>Plenz</last><affiliation>Institute for Computational Linguistics, Heidelberg University, Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Janosch</first><last>Gehring</last></author>
      <author><first>Philipp</first><last>Cimiano</last><affiliation>Bielefeld University and Bielefeld University</affiliation></author>
      <author><first>Anette</first><last>Frank</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>1525-1553</pages>
      <abstract>Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer’s stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.</abstract>
      <url hash="6434720c">2025.findings-naacl.83</url>
      <bibkey>plenz-etal-2025-argumentation</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.83</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>LVLM</fixed-case>-Compress-Bench: Benchmarking the Broader Impact of Large Vision-Language Model Compression</title>
      <author><first>Souvik</first><last>Kundu</last><affiliation>Intel</affiliation></author>
      <author><first>Anahita</first><last>Bhiwandiwalla</last></author>
      <author><first>Sungduk</first><last>Yu</last></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <author><first>Tiep</first><last>Le</last><affiliation>Intel</affiliation></author>
      <author><first>Sharath</first><last>Nittur Sridhar</last><affiliation>Intel Labs</affiliation></author>
      <author><first>David</first><last>Cobbley</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Hao</first><last>Kang</last></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel</affiliation></author>
      <pages>1554-1570</pages>
      <abstract>Despite recent efforts in understanding the compression impact on Large Language Models (LLMs) in terms of their downstream task performance and trustworthiness on relatively simpler uni-modal benchmarks (e.g. question answering, common sense reasoning), their detailed study on multi-modal Large Vision Language Models (LVLMs) is yet to be unveiled. Towards mitigating this gap, we present LVLM-Compress-Bench, a framework to first thorough study on the broad impact of compression on the generative performance of LVLMs on multi-modal input driven tasks. In specific, we consider two major classes of compression for autoregressive models, namely KV cache and weight compression, for the dynamically growing intermediate cache and static weights, respectively. We use four LVLM variants of the popular LLaVA framework to present our analysis to integrate various state-of-the-art KV and weight compression methods including uniform, outlier-reduced, and group quantization. With this framework we demonstrate on ten different multi-modal datasets with varied capabilities including recognition, knowledge, language generation, spatial awareness, visual reasoning, hallucination and visual illusion identification, toxicity, stereotypes and bias. In specific, our framework demonstrates the compression impact on both general and ethically critical metrics leveraging a combination of real world and synthetic datasets to encompass diverse societal intersectional attributes. Extensive experimental evaluations yield diverse and intriguing observations on the behavior of LVLMs at different quantization budget of KV and weights, in both maintaining and losing performance as compared to the baseline model with FP16 data format. We believe LVLM-Compress-Bench would help the community to have a deeper insight on the parting impact of compression and the societal impact the compressed models may pose. Code will be released soon.</abstract>
      <url hash="e4768d1b">2025.findings-naacl.84</url>
      <bibkey>kundu-etal-2025-lvlm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.84</doi>
    </paper>
    <paper id="85">
      <title>Does Generative <fixed-case>AI</fixed-case> speak <fixed-case>N</fixed-case>igerian-<fixed-case>P</fixed-case>idgin?: Issues about Representativeness and Bias for Multilingualism in <fixed-case>LLM</fixed-case>s</title>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Ghent University</affiliation></author>
      <author><first>Iyanuoluwa</first><last>Shode</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <pages>1571-1583</pages>
      <abstract>Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.</abstract>
      <url hash="b99ac33c">2025.findings-naacl.85</url>
      <bibkey>adelani-etal-2025-generative</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.85</doi>
    </paper>
    <paper id="86">
      <title>A Guide To Effectively Leveraging <fixed-case>LLM</fixed-case>s for Low-Resource Text Summarization: Data Augmentation and Semi-supervised Approaches</title>
      <author><first>Gaurav</first><last>Sahu</last></author>
      <author><first>Olga</first><last>Vechtomova</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Issam H.</first><last>Laradji</last><affiliation>ServiceNow</affiliation></author>
      <pages>1584-1603</pages>
      <abstract>Existing approaches for low-resource text summarization primarily employ large language models (LLMs) like GPT-3 or GPT-4 at inference time to generate summaries directly; however, such approaches often suffer from inconsistent LLM outputs and are difficult to adapt to domain-specific data in low-resource scenarios. In this work, we propose two novel methods to effectively utilize LLMs for low-resource text summarization: 1) MixSumm, an LLM-based data augmentation regime that synthesizes high-quality documents (short and long) for few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling strategy for sample-efficient semi-supervised text summarization. Specifically, MixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new documents by mixing topical information derived from a small seed set, and PPSL leverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels in a semi-supervised learning setup. We evaluate our methods on the TweetSumm, WikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation metric, and ROUGE scores to measure the quality of generated summaries. Our experiments on extractive and abstractive summarization show that MixSumm and PPSL achieve competitive ROUGE scores as a fully supervised method with 5% of the labeled data. We release our codebase here: https://github.com/ServiceNow/text-summarization-with-llms/</abstract>
      <url hash="b5be357a">2025.findings-naacl.86</url>
      <bibkey>sahu-etal-2025-guide</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.86</doi>
    </paper>
    <paper id="87">
      <title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title>
      <author><first>Aashiq</first><last>Muhamed</last></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Virginia</first><last>Smith</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1604-1635</pages>
      <abstract>Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and <tex-math>L_0</tex-math> sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5% increase in worst-group classification accuracy over the pretrained general-purpose SAE when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.</abstract>
      <url hash="f2e717fe">2025.findings-naacl.87</url>
      <bibkey>muhamed-etal-2025-decoding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.87</doi>
    </paper>
    <paper id="88">
      <title><fixed-case>MA</fixed-case>i<fixed-case>DE</fixed-case>-up: Multilingual Deception Detection of <fixed-case>AI</fixed-case>-generated Hotel Reviews</title>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Xiaomeng</first><last>Xu</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>1636-1653</pages>
      <abstract>Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews.</abstract>
      <url hash="e6812d65">2025.findings-naacl.88</url>
      <bibkey>ignat-etal-2025-maide</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>L</fixed-case>e<fixed-case>C</fixed-case>o<fixed-case>PCR</fixed-case>: Legal Concept-guided Prior Case Retrieval for <fixed-case>E</fixed-case>uropean Court of Human Rights cases</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Isaac Misael Olguín</first><last>Nolasco</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1654-1661</pages>
      <abstract>Prior case retrieval (PCR) is crucial for legal practitioners to find relevant precedent cases given the facts of a query case. Existing approaches often overlook the underlying semantic intent in determining relevance with respect to the query case. In this work, we propose LeCoPCR, a novel approach that explicitly generate intents in the form of legal concepts from a given query case facts and then augments the query with these concepts to enhance models understanding of semantic intent that dictates relavance. To overcome the unavailability of annotated legal concepts, We employ a weak supervision approach to extract key legal concepts from the reasoning section using Determinantal Point Process (DPP) to balance quality and diversity. Experimental results on the ECtHR-PCR dataset demonstrate the effectiveness of leveraging legal concepts and DPP-based key concept extraction.</abstract>
      <url hash="a00f2570">2025.findings-naacl.89</url>
      <bibkey>t-y-s-s-etal-2025-lecopcr</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.89</doi>
    </paper>
    <paper id="90">
      <title>How much do contextualized representations encode long-range context?</title>
      <author><first>Simeng</first><last>Sun</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Cheng-Ping</first><last>Hsieh</last><affiliation>NVIDIA</affiliation></author>
      <pages>1662-1679</pages>
      <abstract>We analyze contextual representations in neural autoregressive language models, emphasizing long-range contexts that span several thousand tokens. Our methodology employs a perturbation setup and the metric Anisotropy-Calibrated Cosine Similarity, to capture the degree of contextualization of long-range patterns from the perspective of representation geometry. We begin the analysis with a case study on standard decoder-only Transformers, demonstrating that similar perplexity can exhibit markedly different downstream task performance, which can be explained by the difference in contextualization of long-range content. Next, we extend the analysis to other models, covering recent novel architectural designs and various training configurations. The representation-level results illustrate a reduced capacity for high-complexity (i.e., less compressible) sequences across architectures, and that fully recurrent models rely heavily on local context, whereas hybrid models more effectively encode the entire sequence structure. Finally, preliminary analysis of model size and training configurations on the encoding of long-range context suggest potential directions for improving existing language models.</abstract>
      <url hash="53b5cd89">2025.findings-naacl.90</url>
      <bibkey>sun-hsieh-2025-much</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.90</doi>
    </paper>
    <paper id="91">
      <title>Data Poisoning for In-context Learning</title>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Makoto</first><last>Yamada</last><affiliation>Okinawa Institute of Science and Technology (OIST)</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>1680-1700</pages>
      <abstract>In-context learning (ICL) has emerged as a capability of large language models (LLMs), enabling them to adapt to new tasks using provided examples. While ICL has demonstrated its strong effectiveness, there is limited understanding of its vulnerability against potential threats. This paper examines ICL’s vulnerability to data poisoning attacks. We introduce ICLPoison, an attacking method specially designed to exploit ICL’s unique learning mechanisms by identifying discrete text perturbations that influence LLM hidden states. We propose three representative attack strategies, evaluated across various models and tasks. Our experiments, including those on GPT-4, show that ICL performance can be significantly compromised by these attacks, highlighting the urgent need for improved defense mechanisms to protect LLMs’ integrity and reliability.</abstract>
      <url hash="80817191">2025.findings-naacl.91</url>
      <bibkey>he-etal-2025-data</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.91</doi>
    </paper>
    <paper id="92">
      <title>Synthetic Audio Helps for Cognitive State Tasks</title>
      <author><first>Adil</first><last>Soubki</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>John</first><last>Murzaku</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Peter</first><last>Zeng</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>1701-1708</pages>
      <abstract>The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio.</abstract>
      <url hash="03416ad1">2025.findings-naacl.92</url>
      <bibkey>soubki-etal-2025-synthetic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.92</doi>
    </paper>
    <paper id="93">
      <title><fixed-case>B</fixed-case>io<fixed-case>EL</fixed-case>: A Comprehensive Python Package for Biomedical Entity Linking</title>
      <author><first>Prasanth</first><last>Bathala</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Christophe</first><last>Ye</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Batuhan</first><last>Nursal</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Shubham</first><last>Lohiya</last></author>
      <author><first>David</first><last>Kartchner</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Cassie S.</first><last>Mitchell</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>1709-1721</pages>
      <url hash="13948d10">2025.findings-naacl.93</url>
      <bibkey>bathala-etal-2025-bioel</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.93</doi>
    </paper>
    <paper id="94">
      <title><fixed-case>P</fixed-case>air<fixed-case>S</fixed-case>cale: Analyzing Attitude Change with Pairwise Comparisons</title>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Patrick Y.</first><last>Wu</last><affiliation>American University</affiliation></author>
      <author><first>Kristina</first><last>Miler</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Alexander Miserlis</first><last>Hoyle</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Philip</first><last>Resnik</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>1722-1738</pages>
      <abstract>We introduce a text-based framework for measuring attitudes in communities toward issues of interest, going beyond the pro/con/neutral of conventional stance detection to characterize attitudes on a continuous scale using both implicit and explicit evidence in language. The framework exploits LLMs both to extract attitude-related evidence and to perform pairwise comparisons that yield unidimensional attitude scores via the classic Bradley-Terry model. We validate the LLM-based steps using human judgments, and illustrate the utility of the approach for social science by examining the evolution of attitudes on two high-profile issues in U.S. politics in two political communities on Reddit over the period spanning from the 2016 presidential campaign to the 2022 mid-term elections. WARNING: Potentially sensitive political content.</abstract>
      <url hash="cd6347d1">2025.findings-naacl.94</url>
      <bibkey>sarkar-etal-2025-pairscale</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.94</doi>
    </paper>
    <paper id="95">
      <title>Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation</title>
      <author><first>Chenyu</first><last>Wang</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Weichao</first><last>Zhou</last></author>
      <author><first>Shantanu</first><last>Ghosh</last></author>
      <author><first>Kayhan</first><last>Batmanghelich</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Wenchao</first><last>Li</last><affiliation>Boston University</affiliation></author>
      <pages>1739-1754</pages>
      <abstract>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by 10%, achieved by rejecting 20% of reports on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an 82.9% success rate. Our implementation is open-source and available at https://github.com/BU-DEPEND-Lab/SCUQ-RRG.</abstract>
      <url hash="2840ec26">2025.findings-naacl.95</url>
      <bibkey>wang-etal-2025-semantic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.95</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>R</fixed-case>eward<fixed-case>B</fixed-case>ench: Evaluating Reward Models for Language Modeling</title>
      <author><first>Nathan</first><last>Lambert</last></author>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Jacob</first><last>Morrison</last></author>
      <author><first>LJ</first><last>Miranda</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Khyathi</first><last>Chandu</last></author>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Tom</first><last>Zick</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>1755-1797</pages>
      <abstract>Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate RMs trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</abstract>
      <url hash="fd90914b">2025.findings-naacl.96</url>
      <bibkey>lambert-etal-2025-rewardbench</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.96</doi>
    </paper>
    <paper id="97">
      <title>Evaluating Vision-Language Models for Emotion Recognition</title>
      <author><first>Sree</first><last>Bhattacharyya</last></author>
      <author><first>James Z.</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1798-1820</pages>
      <abstract>Large Vision-Language Models (VLMs) have achieved unprecedented success in several objective multimodal reasoning tasks. However, to further enhance their capabilities of empathetic and effective communication with humans, improving how VLMs process and understand emotions is crucial. Despite significant research attention on improving affective understanding, there is a lack of detailed evaluations of VLMs for emotion-related tasks, which can potentially help inform downstream fine-tuning efforts. In this work, we present the first comprehensive evaluation of VLMs for recognizing evoked emotions from images. We create a benchmark for the task of evoked emotion recognition and study the performance of VLMs for this task, from perspectives of correctness and robustness. Through several experiments, we demonstrate important factors that emotion recognition performance depends on, and also characterize the various errors made by VLMs in the process. Finally, we pinpoint potential causes for errors through a human evaluation study. We use our experimental results to inform recommendations for the future of emotion research in the context of VLMs.</abstract>
      <url hash="b64188aa">2025.findings-naacl.97</url>
      <bibkey>bhattacharyya-wang-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.97</doi>
    </paper>
    <paper id="98">
      <title>Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?</title>
      <author><first>Crystina</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jing</first><last>Lu</last><affiliation>Google</affiliation></author>
      <author><first>Vinh Q.</first><last>Tran</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google DeepMind and Google</affiliation></author>
      <author><first>Donald</first><last>Metzler</last><affiliation>Google</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>1821-1837</pages>
      <abstract>Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form “semantic tokens” by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.</abstract>
      <url hash="07382737">2025.findings-naacl.98</url>
      <bibkey>zhang-etal-2025-tomato</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.98</doi>
    </paper>
    <paper id="99">
      <title>Open Domain Question Answering with Conflicting Contexts</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Qiang</first><last>Ning</last><affiliation>Jump Trading</affiliation></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Qi</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Phu Mon</first><last>Htut</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>AWS AI</affiliation></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1838-1854</pages>
      <abstract>Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts. We publicly release our dataset and code to promote research along this line.</abstract>
      <url hash="6062af07">2025.findings-naacl.99</url>
      <bibkey>liu-etal-2025-open</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.99</doi>
    </paper>
    <paper id="100">
      <title>The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models</title>
      <author><first>Artem</first><last>Kirsanov</last><affiliation>New York University</affiliation></author>
      <author><first>Chi-Ning</first><last>Chou</last></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>SueYeon</first><last>Chung</last><affiliation>New York University and Flatiron Institute / Simons Foundation</affiliation></author>
      <pages>1855-1888</pages>
      <abstract>Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights critical geometric effects of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.</abstract>
      <url hash="e3f72365">2025.findings-naacl.100</url>
      <bibkey>kirsanov-etal-2025-geometry</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.100</doi>
    </paper>
    <paper id="101">
      <title>Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation</title>
      <author><first>Pedro</first><last>Cisneros-Velarde</last><affiliation>VMware Research</affiliation></author>
      <pages>1889-1916</pages>
      <abstract>We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM’s tendency to find consensus with the other LLM’s opinion, display caution when specifying funding, and consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus is mostly attained. When agents are aware of past opinions, they seek to maintain consistency with them, changing the opinion dynamics. Our study is performed using Llama 3 and Mistral LLMs.</abstract>
      <url hash="d4f35917">2025.findings-naacl.101</url>
      <bibkey>cisneros-velarde-2025-biases</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.101</doi>
    </paper>
    <paper id="102">
      <title><fixed-case>C</fixed-case>ase<fixed-case>S</fixed-case>umm: A Large-Scale Dataset for Long-Context Summarization from <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Opinions</title>
      <author><first>Mourad</first><last>Heddaya</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Kyle</first><last>MacMillan</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Hongyuan</first><last>Mei</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Anup</first><last>Malani</last><affiliation>University of Chicago</affiliation></author>
      <pages>1917-1942</pages>
      <abstract>This paper introduces CaseSumm, a novel dataset for long-context summarization in the legal domain that addresses the need for longer and more complex datasets for summarization evaluation. We collect 25.6K U.S. Supreme Court (SCOTUS) opinions and their official summaries, known as “syllabuses.” Our dataset is the largest open legal case summarization dataset, and is the first to include summaries of SCOTUS decisions dating back to 1815.We also present a comprehensive evaluation of LLM-generated summaries using both automatic metrics and expert human evaluation, revealing discrepancies between these assessment methods. Our evaluation shows Mistral 7b, a smaller open-source model, outperforms larger models on most automatic metrics and successfully generates syllabus-like summaries. In contrast, human expert annotators indicate that Mistral summaries contain hallucinations. The annotators consistently rank GPT-4 summaries as clearer and exhibiting greater sensitivity and specificity. We find that LLM-based evaluations are not more correlated with human evaluations than traditional automatic metrics. Furthermore, our analysis identifies specific hallucinations in generated summaries, including precedent citation errors and misrepresentations of case facts. These findings demonstrate the limitations of current automatic evaluation methods for legal summarization and highlight the critical role of human evaluation in assessing summary quality, particularly in complex, high-stakes domains.</abstract>
      <url hash="eb603744">2025.findings-naacl.102</url>
      <bibkey>heddaya-etal-2025-casesumm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.102</doi>
    </paper>
    <paper id="103">
      <title>Chasing Random: Instruction Selection Strategies Fail to Generalize</title>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daphne</first><last>Ippolito</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1943-1957</pages>
      <url hash="5997d475">2025.findings-naacl.103</url>
      <bibkey>diddee-ippolito-2025-chasing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.103</doi>
    </paper>
    <paper id="104">
      <title>Can’t Hide Behind the <fixed-case>API</fixed-case>: Stealing Black-Box Commercial Embedding Models</title>
      <author><first>Manveer Singh</first><last>Tamber</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jasper</first><last>Xian</last></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>1958-1969</pages>
      <abstract>Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being “hidden” behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to “steal” these models for retrieval by training thief models on text–embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.</abstract>
      <url hash="0d6a758b">2025.findings-naacl.104</url>
      <bibkey>tamber-etal-2025-cant</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.104</doi>
    </paper>
    <paper id="105">
      <title><fixed-case>CAMEL</fixed-case>-Bench: A Comprehensive <fixed-case>A</fixed-case>rabic <fixed-case>LMM</fixed-case> Benchmark</title>
      <author><first>Sara</first><last>Ghaboura</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ahmed</first><last>Heakl</last></author>
      <author><first>Omkar</first><last>Thawakar</last></author>
      <author><first>Ali Husain Salem Abdulla</first><last>Alharthi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ines</first><last>Riahi</last></author>
      <author><first>Abduljalil</first><last>Radman</last></author>
      <author><first>Jorma</first><last>Laaksonen</last><affiliation>Aalto University</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <author><first>Rao Muhammad</first><last>Anwer</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1970-1980</pages>
      <abstract>Recent years have witnessed a significant interest in developing large multi-modal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the bestopen-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark will be publicly released.</abstract>
      <url hash="a585ce65">2025.findings-naacl.105</url>
      <bibkey>ghaboura-etal-2025-camel</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.105</doi>
    </paper>
    <paper id="106">
      <title><fixed-case>P</fixed-case>roxy<fixed-case>LM</fixed-case>: Predicting Language Model Performance on Multilingual Tasks via Proxy Models</title>
      <author><first>David</first><last>Anugraha</last></author>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <author><first>Chenyue</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Patrick Amadeus</first><last>Irawan</last></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <pages>1981-2011</pages>
      <abstract>Performance prediction is a method to estimate the performance of Language Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating computational costs associated with model capacity and data for fine-tuning. Our paper presents ProxyLM, a scalable task- and language-agnostic framework designed to predict the performance of LMs using proxy models. These proxy models act as surrogates, approximating the performance of the LM of interest. By leveraging these proxy models, ProxyLM significantly reduces computational overhead in task evaluations, achieving up to a 37.08x speedup over traditional methods, even with our smallest proxy models. Our results across multiple multilingual NLP tasks and various robustness tests demonstrate that ProxyLM not only adapts well to previously unseen languages in pre-trained LMs, but also generalizes effectively across different datasets, outperforming the state-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).</abstract>
      <url hash="a623191e">2025.findings-naacl.106</url>
      <bibkey>anugraha-etal-2025-proxylm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.106</doi>
    </paper>
    <paper id="107">
      <title><fixed-case>S</fixed-case>im<fixed-case>SM</fixed-case>o<fixed-case>E</fixed-case>: Toward Efficient Training Mixture of Experts via Solving Representational Collapse</title>
      <author><first>Giang</first><last>Do</last></author>
      <author><first>Hung</first><last>Le</last><affiliation>Deakin University</affiliation></author>
      <author><first>Truyen</first><last>Tran</last><affiliation>Deakin University</affiliation></author>
      <pages>2012-2025</pages>
      <abstract>Sparse mixture of experts (SMoE) have emerged as an effective approach for scaling large language models while keeping a constant computational cost. Regardless of several notable successes of SMoE, effective training such architecture remains elusive due to the representation collapse problem, which in turn harms model performance and causes parameter redundancy. In this work, we present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel similarity of neural network algorithm, that guarantees a solution to address the representation collapse issue between experts given a fixed FLOPs budget. We conduct extensive empirical evaluations on three large language models for both Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness, and scalability of our method. The results demonstrate that SimSMoE significantly enhances existing routing policy and outperforms other SMoE routing methods in performance for the tasks. Our implementation is publicly available at https://github.com/giangdip2410/SimSMoE.</abstract>
      <url hash="e5c47699">2025.findings-naacl.107</url>
      <bibkey>do-etal-2025-simsmoe</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.107</doi>
    </paper>
    <paper id="108">
      <title><fixed-case>U</fixed-case>ni<fixed-case>RAG</fixed-case>: Universal Retrieval Augmentation for Large Vision Language Models</title>
      <author><first>Sahel</first><last>Sharifymoghaddam</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Shivani</first><last>Upadhyay</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>2026-2039</pages>
      <abstract>Recently, Large Vision Language Models (LVLMs) have unlocked many complex use cases that require Multi-Modal (MM) understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of LVLMs we introduce UniRAG, a plug-and-play technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT-4o and Gemini-Pro and smaller open-source models like LLaVA, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by Vision-Language (VL) retrievers like UniIR models. All the necessary code to reproduce our results is available at https://github.com/castorini/UniRAG.</abstract>
      <url hash="c070981d">2025.findings-naacl.108</url>
      <bibkey>sharifymoghaddam-etal-2025-unirag</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.108</doi>
    </paper>
    <paper id="109">
      <title>Evaluating the Performance of Large Language Models via Debates</title>
      <author><first>Behrad</first><last>Moniri</last></author>
      <author><first>Hamed</first><last>Hassani</last><affiliation>University of Pennsylvania, University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <author><first>Edgar</first><last>Dobriban</last><affiliation>The Wharton School, University of Pennsylvania</affiliation></author>
      <pages>2040-2075</pages>
      <abstract>Large Language Models (LLMs) are rapidly evolving and impacting various fields, necessitating the development of effective methods to evaluate and compare their performance. Most current approaches for performance evaluation are either based on fixed, domain-specific questions that lack the flexibility required in many real-world applications, or rely on human input, making them unscalable. To address these issues, we propose an automated benchmarking framework based on debates between LLMs, judged by another LLM. This method assesses not only domain knowledge, but also skills such as argumentative reasoning and inconsistency recognition. We evaluate the performance of various state-of-the-art LLMs using the debate framework and achieve rankings that align closely with popular rankings based on human input, eliminating the need for costly human crowdsourcing.</abstract>
      <url hash="c50b2478">2025.findings-naacl.109</url>
      <bibkey>moniri-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.109</doi>
    </paper>
    <paper id="110">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>G</fixed-case>raph2<fixed-case>LLM</fixed-case>: Evaluating <fixed-case>LLM</fixed-case>s for Causal Queries</title>
      <author><first>Ivaxi</first><last>Sheth</last><affiliation>CISPA, saarland university, saarland informatics campus</affiliation></author>
      <author><first>Bahare</first><last>Fatemi</last><affiliation>Google</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>2076-2098</pages>
      <abstract>Causality is essential in scientific research, enabling researchers to interpret true relationships between variables. These causal relationships are often represented by causal graphs, which are directed acyclic graphs. With the recent advancements in Large Language Models (LLMs), there is an increasing interest in exploring their capabilities in causal reasoning and their potential use to hypothesize causal graphs. These tasks necessitate the LLMs to encode the causal graph effectively for subsequent downstream tasks. In this paper, we introduce <b>CausalGraph2LLM</b>, a comprehensive benchmark comprising over <i>700k</i> queries across diverse causal graph settings to evaluate the causal reasoning capabilities of LLMs. We categorize the causal queries into two types: graph-level and node-level queries. We benchmark both open-sourced and closed models for our study. Our findings reveal that while LLMs show promise in this domain, they are highly sensitive to the encoding used. Even capable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with deviations of about 60%. We further demonstrate this sensitivity for downstream causal intervention tasks. Moreover, we observe that LLMs can often display biases when presented with contextual information about a causal graph, potentially stemming from their parametric memory.</abstract>
      <url hash="c0911ada">2025.findings-naacl.110</url>
      <bibkey>sheth-etal-2025-causalgraph2llm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.110</doi>
    </paper>
    <paper id="111">
      <title><fixed-case>P</fixed-case>uzzle<fixed-case>GPT</fixed-case>: Emulating Human Puzzle-Solving Ability for Time and Location Prediction</title>
      <author><first>Hammad</first><last>Ayyubi</last></author>
      <author><first>Xuande</first><last>Feng</last></author>
      <author><first>Junzhang</first><last>Liu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Xudong</first><last>Lin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhecan</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Shih-Fu</first><last>Chang</last><affiliation>Columbia University and Columbia University</affiliation></author>
      <pages>2099-2116</pages>
      <abstract>The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can’t be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets – TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically generated reasoning pipelines like VisProg, by at least 32% and 38%, respectively. It even rivals or surpasses finetuned models.</abstract>
      <url hash="659312f6">2025.findings-naacl.111</url>
      <bibkey>ayyubi-etal-2025-puzzlegpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>SAFR</fixed-case>: Neuron Redistribution for Interpretability</title>
      <author><first>Ruidi</first><last>Chang</last><affiliation>Rice University</affiliation></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <pages>2117-2126</pages>
      <abstract>Superposition refers to encoding representations of multiple features within a single neuron, which is common in deep neural networks. This property allows neurons to combine and represent multiple features, enabling the model to capture intricate information and handle complex tasks. Despite promising performance, the model’s interpretability has been diminished. This paper presents a novel approach to enhance model interpretability by regularizing feature superposition. We introduce SAFR, which simply applies regularizations to the loss function to promote monosemantic representations for important tokens while encouraging polysemanticity for correlated token pairs, where important tokens and correlated token pairs are identified via VMASK and attention weights respectively. We evaluate SAFR with a transformer model on two classification tasks. Experiments demonstrate the effectiveness of SAFR in improving model interpretability without compromising prediction performance. Besides, SAFR provides explanations by visualizing the neuron allocation within the intermediate layers.</abstract>
      <url hash="ff0afb1d">2025.findings-naacl.112</url>
      <bibkey>chang-etal-2025-safr</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.112</doi>
    </paper>
    <paper id="113">
      <title><fixed-case>GPT</fixed-case>-4<fixed-case>V</fixed-case> Cannot Generate Radiology Reports Yet</title>
      <author><first>Yuyang</first><last>Jiang</last></author>
      <author><first>Chacha</first><last>Chen</last></author>
      <author><first>Dang</first><last>Nguyen</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Benjamin M.</first><last>Mervak</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <pages>2127-2154</pages>
      <abstract>GPT-4’s purported strong multimodal abilities raise interests in using it to automate radiology report writing, but there lacks thorough evaluations. In this work, we perform a systematic evaluation of GPT-4 (4o and vision-preview) in generating radiology reports across three chest X-ray report benchmarks: MIMIC-CXR, CheXpert Plus, and IU X-Ray. We attempt to directly generate reports with different prompting strategies and find that the models fail terribly in both lexical metrics and clinical efficacy metrics. To understand the low performance, we decompose the task into two steps: 1) the **medical image reasoning** step of predicting medical condition labels from images; and 2) the **report synthesis** step of generating reports from (groundtruth) conditions. We show that GPT-4’s performance in image reasoning is consistently low across different prompts. In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully. Even when given groundtruth conditions in report synthesis, its generated reports are less correct and less natural-sounding than a finetuned Llama. Altogether, our findings cast doubt on the viability of using GPT-4 in a radiology workflow.</abstract>
      <url hash="33147b16">2025.findings-naacl.113</url>
      <bibkey>jiang-etal-2025-gpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.113</doi>
    </paper>
    <paper id="114">
      <title>Is Semantic Chunking Worth the Computational Cost?</title>
      <author><first>Renyi</first><last>Qu</last><affiliation>Vectara</affiliation></author>
      <author><first>Ruixuan</first><last>Tu</last></author>
      <author><first>Forrest Sheng</first><last>Bao</last><affiliation>Vectara, Inc.</affiliation></author>
      <pages>2155-2177</pages>
      <abstract>Recent advances in Retrieval-Augmented Generation (RAG) systems have popularized semantic chunking, which aims to improve retrieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over simpler fixed-size chunking, where documents are split into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrieval-based answer generation. The results show that the computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the previous assumptions about semantic chunking and highlight the need for more efficient chunking strategies in RAG systems.</abstract>
      <url hash="b2f78b28">2025.findings-naacl.114</url>
      <bibkey>qu-etal-2025-semantic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.114</doi>
    </paper>
    <paper id="115">
      <title>On Using <fixed-case>A</fixed-case>rabic Language Dialects in Recommendation Systems</title>
      <author><first>Abdulla</first><last>Alshabanah</last></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>2178-2186</pages>
      <abstract>While natural language processing (NLP) techniques have been applied to user reviews in recommendation systems, the potential of leveraging Arabic dialects in this context remains unexplored. Arabic is spoken by over 420 million people, with significant dialectal variation across regions. These dialects, often classified as low-resource languages, present both challenges and opportunities for machine learning applications. This paper represents the first attempt to incorporate Arabic dialects as a signal in recommendation systems. We explore both explicit and implicit approaches for integrating Arabic dialect information from user reviews, demonstrating its impact on improving recommendation performance. Our findings highlight the potential for leveraging dialectal diversity in Arabic to enhance recommendation systems and encourage further research at the intersection of NLP and recommendation systems within the Arab multicultural world.</abstract>
      <url hash="1037976f">2025.findings-naacl.115</url>
      <bibkey>alshabanah-annavaram-2025-using</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.115</doi>
    </paper>
    <paper id="116">
      <title>Assessing <fixed-case>LLM</fixed-case>s for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing</title>
      <author><first>Hadi</first><last>Askari</last></author>
      <author><first>Anshuman</first><last>Chhabra</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Prasant</first><last>Mohapatra</last></author>
      <pages>2187-2201</pages>
      <abstract>Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles. However, little is known about the robustness of such a process of zero-shot summarization.To bridge this gap, we propose *relevance paraphrasing*, a simple strategy that can be used to measure the robustness of LLMs as summarizers. The relevance paraphrasing approach identifies the most *relevant* sentences that contribute to generating an ideal summary, and then *paraphrases* these inputs to obtain a minimally perturbed dataset. Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM’s one aspect of robustness. We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B-v1, and Dolly-v2-7B). Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements.</abstract>
      <url hash="e4ec9b20">2025.findings-naacl.116</url>
      <bibkey>askari-etal-2025-assessing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.116</doi>
    </paper>
    <paper id="117">
      <title>Beyond Silent Letters: Amplifying <fixed-case>LLM</fixed-case>s in Emotion Recognition with Vocal Nuances</title>
      <author><first>Zehui</first><last>Wu</last></author>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Lin</first><last>Ai</last></author>
      <author><first>Pengyuan</first><last>Shi</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kaan</first><last>Donbekci</last><affiliation>Columbia University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <pages>2202-2218</pages>
      <url hash="3b9a30e8">2025.findings-naacl.117</url>
      <bibkey>wu-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.117</doi>
    </paper>
    <paper id="118">
      <title><fixed-case>D</fixed-case>omain<fixed-case>S</fixed-case>um: A Hierarchical Benchmark for Fine-Grained Domain Shift in Abstractive Text Summarization</title>
      <author><first>Haohan</first><last>Yuan</last></author>
      <author><first>Haopeng</first><last>Zhang</last><affiliation>University of Hawaii System</affiliation></author>
      <pages>2219-2231</pages>
      <abstract>Most research on abstractive summarization focuses on single-domain applications, often neglecting how domain shifts between documents affect performance and the generalization ability of summarization models. To address this issue, we introduce DomainSum, a hierarchical benchmark designed to capture fine-grained domain shifts in abstractive summarization. We categorize these shifts into three levels: genre, style, and topic, and demonstrate through comprehensive benchmark analysis that they follow a hierarchical structure. Furthermore, we evaluate the domain generalization capabilities of commonly used pre-trained language models (PLMs) and large language models (LLMs) in both in-domain and cross-domain settings. Our benchmark and source code are released at https://github.com/hpzhang94/DomainSum.</abstract>
      <url hash="2e01f31c">2025.findings-naacl.118</url>
      <bibkey>yuan-zhang-2025-domainsum</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.118</doi>
    </paper>
    <paper id="119">
      <title>Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations</title>
      <author><first>Wenjie Jacky</first><last>Mo</last></author>
      <author><first>Jiashu</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Jiongxiao</first><last>Wang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Jun</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>Hadi</first><last>Askari</last></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>2232-2249</pages>
      <abstract>Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, this study critically examines the use of demonstrations as a defense mechanism against backdoor attacks in black-box LLMs. With an identified task, we retrieve task-relevant demonstrations from a clean data pool and integrate them with user queries during testing. Importantly, this approach does not necessitate modifications or tuning of the model, nor does it require insight into the model’s internal architecture. The alignment properties inherent in in-context learning play a pivotal role in mitigating the impact of backdoor triggers, effectively recalibrating the behavior of compromised models. Our experimental analysis demonstrates that this method robustly defends against both instance-level and instruction-level backdoor attacks, outperforming existing defense baselines across most evaluation scenarios.</abstract>
      <url hash="ac532c8a">2025.findings-naacl.119</url>
      <bibkey>mo-etal-2025-test</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.119</doi>
    </paper>
    <paper id="120">
      <title>“All that Glitters”: Techniques for Evaluations with Unreliable Model and Human Annotations</title>
      <author><first>Michael</first><last>Hardy</last></author>
      <pages>2250-2278</pages>
      <abstract>“Gold” and “ground truth” human-mediated labels have error. This error can escape commonly reported metrics of label quality or obscure questions of accuracy, bias, fairness, and usefulness during model evaluation. This study demonstrates methods for answering such questions even in the context of very low reliabilities from expert humans. We analyze human labels, GPT model ratings, and transformer encoder model ratings of the quality of classroom teaching from two LLM architecture families–encoders and GPT decoders. First, we demonstrate that using standard metrics in the presence of poor labels can mask both label and model quality. The encoder family of models achieve state-of-the-art, even “super-human”, results across all classroom annotation tasks using standard metrics. However, evaluation techniques accounting for unreliable labels reveal important flaws, including spurious correlations and nonrandom racial biases across models and humans. We estimate that if models were used in a human-in-the-loop context, the variance contributed by GPT model labels would worsen ratings. These techniques also highlight tasks where encoders could offer 80% reduction in human costs while also reducing bias.</abstract>
      <url hash="e8ca1fb5">2025.findings-naacl.120</url>
      <bibkey>hardy-2025-glitters</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.120</doi>
    </paper>
    <paper id="121">
      <title><fixed-case>K</fixed-case>wai<fixed-case>C</fixed-case>hat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus</title>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Yiming</first><last>Lei</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Chenkai</first><last>Zhang</last></author>
      <author><first>Haitao</first><last>Leng</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Chuan</first><last>Wang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Qingjie</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunhong</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <pages>2279-2294</pages>
      <abstract>Video-based dialogue systems have compelling application value, such as education assistants, thereby garnering growing interest. However, the current video-based dialogue systems are limited by their reliance on a single dialogue type, which hinders their versatility in practical applications across a range of scenarios, including question-answering and emotionally dialog, etc. In this paper, we identify this challenge as how to generate video-driven multilingual mixed-type dialogues. To mitigate this challenge, we propose a novel task and create a human-to-human video-driven multilingual mixed-type dialogue corpus, termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues, across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally, we establish baseline models on KwaiChat. An extensive analysis of 7 distinct LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still cannot perform well in this situation even with the help of in-context learning and fine-tuning, which indicates that the task is not trivial and needs further research.</abstract>
      <url hash="f975301b">2025.findings-naacl.121</url>
      <bibkey>shi-etal-2025-kwaichat</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.121</doi>
    </paper>
    <paper id="122">
      <title><fixed-case>G</fixed-case>en<fixed-case>EOL</fixed-case>: Harnessing the Generative Power of <fixed-case>LLM</fixed-case>s for Training-Free Sentence Embeddings</title>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>2295-2308</pages>
      <abstract>Training-free embedding methods directly leverage pretrained large language models (LLMs) to embed text, bypassing the costly and complex procedure of contrastive learning. Previous training-free embedding methods have mainly focused on optimizing embedding prompts and have overlooked the benefits of utilizing the generative abilities of LLMs. We propose a novel method, GenEOL, which uses LLMs to generate diverse transformations of a sentence that preserve its meaning, and aggregates the resulting embeddings of these transformations to enhance the overall sentence embedding. GenEOL significantly outperforms the existing training-free embedding methods by an average of 2.85 points across several LLMs on the sentence semantic text similarity (STS) benchmark. GenEOL also achieves notable gains in clustering, reranking, and pair-classification tasks from the MTEB benchmark. Additionally, GenEOL stabilizes representation quality across LLM layers and remains robust to perturbations of embedding prompts.</abstract>
      <url hash="fd8706d7">2025.findings-naacl.122</url>
      <bibkey>thirukovalluru-dhingra-2025-geneol</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.122</doi>
    </paper>
    <paper id="123">
      <title>Attention Tracker: Detecting Prompt Injection Attacks in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kuo-Han</first><last>Hung</last></author>
      <author><first>Ching-Yun</first><last>Ko</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ambrish</first><last>Rawat</last><affiliation>International Business Machines</affiliation></author>
      <author><first>I-Hsin</first><last>Chung</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Winston H.</first><last>Hsu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <pages>2309-2322</pages>
      <abstract>Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose Attention Tracker, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities.</abstract>
      <url hash="20d12c1e">2025.findings-naacl.123</url>
      <bibkey>hung-etal-2025-attention</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.123</doi>
    </paper>
    <paper id="124">
      <title>Unsupervised Speech-text word-level alignment with Dynamic Programming</title>
      <author><first>Tianshu</first><last>Yu</last></author>
      <author><first>Zihan</first><last>Gong</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Minghuan</first><last>Tan</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guhong</first><last>Chen</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>2323-2334</pages>
      <url hash="de9eb415">2025.findings-naacl.124</url>
      <bibkey>yu-etal-2025-unsupervised</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>S</fixed-case>ci<fixed-case>A</fixed-case>ssess: Benchmarking <fixed-case>LLM</fixed-case> Proficiency in Scientific Literature Analysis</title>
      <author><first>Hengxing</first><last>Cai</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xiaochen</first><last>Cai</last><affiliation>DP Technology</affiliation></author>
      <author><first>Junhan</first><last>Chang</last></author>
      <author><first>Sihang</first><last>Li</last></author>
      <author><first>Lin</first><last>Yao</last></author>
      <author><first>Wang</first><last>Changxin</last></author>
      <author><first>Zhifeng</first><last>Gao</last><affiliation>DP Technology</affiliation></author>
      <author><first>Hongshuai</first><last>Wang</last><affiliation>DP technology</affiliation></author>
      <author><first>Li</first><last>Yongge</last></author>
      <author><first>Mujie</first><last>Lin</last></author>
      <author><first>Shuwen</first><last>Yang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Jiankun</first><last>Wang</last></author>
      <author><first>Mingjun</first><last>Xu</last></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Xi</first><last>Fang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Jiaxi</first><last>Zhuang</last></author>
      <author><first>Yuqi</first><last>Yin</last></author>
      <author><first>Yaqi</first><last>Li</last></author>
      <author><first>Changhong</first><last>Chen</last><affiliation>DP Technology</affiliation></author>
      <author><first>Zheng</first><last>Cheng</last></author>
      <author><first>Zifeng</first><last>Zhao</last><affiliation>AI for Science Institute</affiliation></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Guolin</first><last>Ke</last><affiliation>DP Technology</affiliation></author>
      <pages>2335-2357</pages>
      <abstract>Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data.In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis &amp; Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine.To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis.SciAssess and its resources are available at <url>https://github.com/sci-assess/SciAssess</url>.</abstract>
      <url hash="560962be">2025.findings-naacl.125</url>
      <bibkey>cai-etal-2025-sciassess</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.125</doi>
    </paper>
    <paper id="126">
      <title>Towards Understanding the Fragility of Multilingual <fixed-case>LLM</fixed-case>s against Fine-Tuning Attacks</title>
      <author><first>Samuele</first><last>Poppi</last><affiliation>University of Modena and Reggio Emilia and University of Pisa</affiliation></author>
      <author><first>Zheng Xin</first><last>Yong</last><affiliation>Brown University</affiliation></author>
      <author><first>Yifei</first><last>He</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Bobbie</first><last>Chern</last></author>
      <author><first>Han</first><last>Zhao</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Aobo</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Jianfeng</first><last>Chi</last><affiliation>Meta AI</affiliation></author>
      <pages>2358-2372</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have sparked widespread concerns about their safety. Recent work demonstrates that safety alignment of LLMs can be easily removed by fine-tuning with a few adversarially chosen instruction-following examples, i.e., fine-tuning attacks. We take a further step to understand fine-tuning attacks in multilingual LLMs. We first discover cross-lingual generalization of fine-tuning attacks: using a few adversarially chosen instruction-following examples in one language, multilingual LLMs can also be easily compromised (e.g., multilingual LLMs fail to refuse harmful prompts in other languages). Motivated by this finding, we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space. Through SIL, we validate this hypothesis and find that only changing 20% of weight parameters in fine-tuning attacks can break safety alignment across all languages. Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks, and we demonstrate that our attack vector can still jailbreak LLMs adapted to new languages.</abstract>
      <url hash="c444aba9">2025.findings-naacl.126</url>
      <bibkey>poppi-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.126</doi>
    </paper>
    <paper id="127">
      <title><fixed-case>MASSW</fixed-case>: A New Dataset and Benchmark Tasks for <fixed-case>AI</fixed-case>-Assisted Scientific Workflows</title>
      <author><first>Xingjian</first><last>Zhang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yutong</first><last>Xie</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Jinge</first><last>Ma</last></author>
      <author><first>Zhaoying</first><last>Pan</last></author>
      <author><first>Qijia</first><last>Liu</last></author>
      <author><first>Ziyang</first><last>Xiong</last></author>
      <author><first>Tolga</first><last>Ergen</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Dongsub</first><last>Shim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Qiaozhu</first><last>Mei</last><affiliation>Google and University of Michigan</affiliation></author>
      <pages>2373-2394</pages>
      <abstract>Scientific innovation relies on detailed workflows, which include critical steps such as contextualizing literature, generating ideas, validating ideas, interpreting results, and planning new research. Scientific publications that document these workflows are extensive and unstructured, making it difficult to effectively navigate and explore the space of scientific innovation. To meet this challenge, we introduce **MASSW**, a comprehensive dataset of **M**ulti-**A**spect **S**ummarization of **S**cientific **W**orkflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications – *context, key idea, method, outcome*, and *projected impact* – which correspond to five key steps in a research workflow. We show that these LLM-extract summaries have a comparable quality to human annotations, and they facilitate a variety of downstream tasks, corresponding to different types of predictions and recommendations along the scientific workflow. Overall, MASSW demonstrates decent utility as a pre-computed and trustful resource for the AI4Science community to create and benchmark a wide-range of new AI methods for optimizing scientific workflows and fostering scientific innovation. Our code and datasets are made available anonymously: [link](https://osf.io/7ygrq/?view_only=3d8261a0ea09489fa67ece2c68235afa).</abstract>
      <url hash="471a8d7a">2025.findings-naacl.127</url>
      <bibkey>zhang-etal-2025-massw</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.127</doi>
    </paper>
    <paper id="128">
      <title>Neuro-symbolic Training for Reasoning over Spatial Language</title>
      <author><first>Tanawan</first><last>Premsri</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>2395-2414</pages>
      <abstract>Spatial reasoning based on natural language expressions is essential for everyday human tasks. This reasoning ability is also crucial for machines to interact with their environment in a human-like manner. However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions. This is attributed to not achieving the right level of abstraction required for generalizability.To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering.Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains. We evaluate our approach on existing spatial question-answering benchmarks. Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</abstract>
      <url hash="3d77cc93">2025.findings-naacl.128</url>
      <bibkey>premsri-kordjamshidi-2025-neuro</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.128</doi>
    </paper>
    <paper id="129">
      <title>On Localizing and Deleting Toxic Memories in Large Language Models</title>
      <author><first>Anubrata</first><last>Das</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Manoj</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>Amazon and University of Massachusetts, Lowell</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Morteza</first><last>Ziyadi</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>2415-2423</pages>
      <abstract>Warning: This paper contains offensive language.Ensuring that large language models (LLMs) do not generate harmful text is critical for their safe deployment. A common failure mode involves producing toxic responses to otherwise innocuous prompts. While various detoxification methods have been proposed, the underlying mechanisms that drive toxic generation in LLMs are not yet fully understood. Our work aims to provide a mechanistic understanding of toxic generation against innocuous-seeming adversarial prompts through the lens of memory localization. We find evidence of localization of toxic memories in the early Multilayer Perceptron (MLP) layers of GPT-2-XL. We further investigate the effects of editing and deleting these toxic memories in MLP layers to reduce toxic generation. Editing significantly reduces toxic generation, from 62.86% to 28.61%. However, this reduction comes with a trade-off in generation quality as perplexity increases from 78.18 on GPT2-XL against the adversarial prompts to 106.06 after editing. Localization-informed deletion achieves a better toxicity-perplexity tradeoff compared to random early layer editing, which reduces toxicity but leads to greater perplexity increases.</abstract>
      <url hash="1578570f">2025.findings-naacl.129</url>
      <bibkey>das-etal-2025-localizing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.129</doi>
    </paper>
    <paper id="130">
      <title><fixed-case>D</fixed-case>i<fixed-case>VIS</fixed-case>e: Direct Visual-Input Speech Synthesis Preserving Speaker Characteristics And Intelligibility</title>
      <author><first>Yifan</first><last>Liu</last></author>
      <author><first>Yu</first><last>Fang</last></author>
      <author><first>Zhouhan</first><last>Lin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2424-2439</pages>
      <abstract>Video-to-speech (V2S) synthesis, the task of generating speech directly from silent video input, is inherently more challenging than other speech synthesis tasks due to the need to accurately reconstruct both speech content and speaker characteristics from visual cues alone. Recently, audio-visual pretraining has eliminated the need for additional acoustic hints in V2S, which previous methods often relied on to ensure training convergence. However, even with pretraining, existing methods continue to face challenges in achieving a balance between acoustic intelligibility and the preservation of speaker-specific characteristics. We analyzed this limitation and were motivated to introduce DiVISe (<b>Di</b>rect <b>V</b>sual-<b>I</b>nput <b>S</b>peech Synth<b>e</b>sis), an end-to-end V2S model that predicts Mel-spectrograms directly from video frames alone. Despite not taking any acoustic hints, DiVISe effectively preserves speaker characteristics in the generated audio, and achieves superior performance on both objective and subjective metrics across the LRS2 and LRS3 datasets. Our results demonstrate that DiVISe not only outperforms existing V2S models in acoustic intelligibility but also scales more effectively with increased data and model parameters. Code and weights will be made publicly available after acceptance of this paper.</abstract>
      <url hash="358871b9">2025.findings-naacl.130</url>
      <bibkey>liu-etal-2025-divise</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.130</doi>
    </paper>
    <paper id="131">
      <title><fixed-case>G</fixed-case>raph<fixed-case>ICL</fixed-case>: Unlocking Graph Learning Potential in <fixed-case>LLM</fixed-case>s through Structured Prompt Design</title>
      <author><first>Yuanfu</first><last>Sun</last><affiliation>New York University</affiliation></author>
      <author><first>Zhengnan</first><last>Ma</last></author>
      <author><first>Yi</first><last>Fang</last><affiliation>New York University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Qiaoyu</first><last>Tan</last><affiliation>New York University Shanghai</affiliation></author>
      <pages>2440-2459</pages>
      <abstract>The growing importance of textual and relational systems has driven interest in enhancing large language models (LLMs) for graph-structured data, particularly Text-Attributed Graphs (TAGs), where samples are represented by textual descriptions interconnected by edges. While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning, a comprehensive benchmark for evaluating LLMs solely through prompt design remains surprisingly absent. Without such a carefully crafted evaluation benchmark, most if not all, tailored graph LLMs are compared against general LLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can potentially camouflage many advantages as well as unexpected predicaments of them. To achieve more general evaluations and unveil the true potential of LLMs for graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a comprehensive benchmark comprising novel prompt templates designed to capture graph structure and handle limited label knowledge. Our systematic evaluation shows that general-purpose LLMs equipped with our GraphICL outperform state-of-the-art specialized graph LLMs and graph neural network models in resource-constrained settings and out-of-domain tasks. These findings highlight the significant potential of prompt engineering to enhance LLM performance on graph learning tasks without training and offer a strong baseline for advancing research in graph LLMs.</abstract>
      <url hash="ca5c9b69">2025.findings-naacl.131</url>
      <bibkey>sun-etal-2025-graphicl</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.131</doi>
    </paper>
    <paper id="132">
      <title><fixed-case>FIDELITY</fixed-case>: Fine-grained Interpretable Distillation for Effective Language Insights and Topic Yielding</title>
      <author><first>Divyansh</first><last>Singh</last></author>
      <author><first>Brodie</first><last>Mather</last><affiliation>The Institute for Human &amp; Machine Cognition</affiliation></author>
      <author><first>Demi</first><last>Zhang</last></author>
      <author><first>Patrick</first><last>Lehman</last></author>
      <author><first>Justin</first><last>Ho</last></author>
      <author><first>Bonnie J</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>2460-2472</pages>
      <abstract>The rapid expansion of text data has increased the need for effective methods to distill meaningful information from large datasets. Traditional and state-of-the-art approaches have made significant strides in topic modeling, yet they fall short in generating contextually specific and semantically intuitive topics, particularly in dynamic environments and low-resource languages. Additionally, multi-document summarization systems often struggle with issues like redundancy, scalability, and maintaining readability. We introduce FIDELITY (Fine-grained Interpretable Distillation for Effective Language Insights and Topic Yielding), a hybrid method that combines topic modeling and text summarization to produce fine-grained, semantically rich, and contextually relevant output. FIDELITY enhances dataset accessibility and interpretability, outperforming traditional models in topic diversity, similarity, and in the ability to process new, unseen documents. Additionally, it demonstrates robust multilingual capabilities, effectively handling low-resource languages like Tagalog. This makes FIDELITY a powerful tool for distilling and understanding complex textual data, providing detailed insights while maintaining the necessary granularity for practical applications.</abstract>
      <url hash="b7e0ec4e">2025.findings-naacl.132</url>
      <bibkey>singh-etal-2025-fidelity</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.132</doi>
    </paper>
    <paper id="133">
      <title><fixed-case>C</fixed-case>lassic4<fixed-case>C</fixed-case>hildren: Adapting <fixed-case>C</fixed-case>hinese Literary Classics for Children with Large Language Model</title>
      <author><first>Jiali</first><last>Chen</last></author>
      <author><first>Xusen</first><last>Hei</last></author>
      <author><first>Yuqi</first><last>Xue</last></author>
      <author><first>Zihan</first><last>Wu</last></author>
      <author><first>Jiayuan</first><last>Xie</last></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <pages>2473-2488</pages>
      <abstract>Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children’s reading preferences (i.e., vivid character portrayals, concise narrative structures, and appropriate readability with simpler words and sentences), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters’ personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children’s reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves performance in automatic and human evaluation.</abstract>
      <url hash="ed756d03">2025.findings-naacl.133</url>
      <bibkey>chen-etal-2025-classic4children</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.133</doi>
    </paper>
    <paper id="134">
      <title>Considering Length Diversity in Retrieval-Augmented Summarization</title>
      <author><last>Juseon-Do</last></author>
      <author><first>Jaesung</first><last>Hwang</last></author>
      <author><first>Jingun</first><last>Kwon</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>2489-2500</pages>
      <abstract>This study investigates retrieval-augmented summarization by specifically examining the impact of exemplar summary lengths because previous methods have not considered length constraints. We propose a Diverse Length-aware Maximal Marginal Relevance (DL-MMR) algorithm to better control summary lengths. This algorithm combines the query relevance with diverse target lengths in retrieval-augmented summarization. Unlike previous methods that necessitate exhaustive exemplar-exemplar relevance comparisons using MMR, DL-MMR considers the exemplar target length as well and avoids comparing exemplars to each other, thereby reducing computational cost and conserving memory during the construction of an exemplar pool. Experimental results showed the effectiveness of DL-MMR, which considers length diversity, compared to the original MMR algorithm. DL-MMR additionally showed the effectiveness in memory saving of 781,513 times and computational cost reduction of 500,092 times, while maintaining the same level of informativeness.</abstract>
      <url hash="29ff7163">2025.findings-naacl.134</url>
      <bibkey>juseon-do-etal-2025-considering</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.134</doi>
    </paper>
    <paper id="135">
      <title><fixed-case>LMOD</fixed-case>: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models</title>
      <author><first>Zhenyue</first><last>Qin</last><affiliation>Yale University</affiliation></author>
      <author><first>Yu</first><last>Yin</last></author>
      <author><first>Dylan</first><last>Campbell</last><affiliation>Australian National University</affiliation></author>
      <author><first>Xuansheng</first><last>Wu</last></author>
      <author><first>Ke</first><last>Zou</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ninghao</first><last>Liu</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Yih Chung</first><last>Tham</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Qingyu</first><last>Chen</last><affiliation>Yale University</affiliation></author>
      <pages>2501-2522</pages>
      <abstract>The prevalence of vision-threatening eye diseases is a significant global burden, with many cases remaining undiagnosed or diagnosed too late for effective treatment. Large vision-language models (LVLMs) have the potential to assist in understanding anatomical information, diagnosing eye diseases, and drafting interpretations and follow-up plans, thereby reducing the burden on clinicians and improving access to eye care. However, limited benchmarks are available to assess LVLMs’ performance in ophthalmology-specific applications. In this study, we introduce LMOD, a large-scale multimodal ophthalmology benchmark consisting of 21,993 instances across (1) five ophthalmic imaging modalities: optical coherence tomography, color fundus photographs, scanning laser ophthalmoscopy, lens photographs, and surgical scenes; (2) free-text, demographic, and disease biomarker information; and (3) primary ophthalmology-specific applications such as anatomical information understanding, disease diagnosis, and subgroup analysis. In addition, we benchmarked 13 state-of-the-art LVLM representatives from closed-source, open-source, and medical domains. The results demonstrate a significant performance drop for LVLMs in ophthalmology compared to other domains. Systematic error analysis further identified six major failure modes: misclassification, failure to abstain, inconsistent reasoning, hallucination, assertions without justification, and lack of domain-specific knowledge. In contrast, supervised neural networks specifically trained on these tasks as baselines demonstrated high accuracy. These findings underscore the pressing need for benchmarks in the development and validation of ophthalmology-specific LVLMs.</abstract>
      <url hash="463667e9">2025.findings-naacl.135</url>
      <bibkey>qin-etal-2025-lmod</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.135</doi>
    </paper>
    <paper id="136">
      <title>Syntriever: How to Train Your Retriever with Synthetic Data from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Minsang</first><last>Kim</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Seung Jun</first><last>Baek</last><affiliation>Korea University</affiliation></author>
      <pages>2523-2539</pages>
      <abstract>LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@<tex-math>K</tex-math>. the source code is available in https://github.com/kmswin1/Syntriever</abstract>
      <url hash="b0a190fa">2025.findings-naacl.136</url>
      <bibkey>kim-baek-2025-syntriever</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.136</doi>
    </paper>
    <paper id="137">
      <title><fixed-case>D</fixed-case>yn<fixed-case>C</fixed-case>lean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Huitong</first><last>Pan</last><affiliation>Temple University</affiliation></author>
      <author><first>Zhijia</first><last>Chen</last><affiliation>Facebook</affiliation></author>
      <author><first>Longin Jan</first><last>Latecki</last><affiliation>Temple University</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Eduard</first><last>Dragut</last><affiliation>Temple University</affiliation></author>
      <pages>2540-2556</pages>
      <abstract>Distantly Supervised Named Entity Recognition (DS-NER) has attracted attention due to its scalability and ability to automatically generate labeled data. However, distant annotation introduces many mislabeled instances, limiting its performance. Most of the existing work attempt to solve this problem by developing intricate models to learn from the noisy labels. An alternative approach is to attempt to clean the labeled data, thus increasing the quality of distant labels. This approach has received little attention for NER. In this paper, we propose a training dynamics-based label cleaning approach, which leverages the behavior of a model as training progresses to characterize the distantly annotated samples. We also introduce an automatic threshold estimation strategy to locate the errors in distant labels. Extensive experimental results demonstrate that: (1) models trained on our cleaned DS-NER datasets, which were refined by directly removing identified erroneous annotations, achieve significant improvements in F1-score, ranging from 3.18% to 8.95%; and (2) our method outperforms numerous advanced DS-NER approaches across four datasets.</abstract>
      <url hash="96326178">2025.findings-naacl.137</url>
      <bibkey>zhang-etal-2025-dynclean</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.137</doi>
    </paper>
    <paper id="138">
      <title>An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during Multi-stage Fine-tuning</title>
      <author><first>Andrew</first><last>Bai</last><affiliation>, University of California, Los Angeles</affiliation></author>
      <author><first>Chih-Kuan</first><last>Yeh</last><affiliation>Google</affiliation></author>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <author><first>Ankur</first><last>Taly</last><affiliation>Google</affiliation></author>
      <pages>2557-2569</pages>
      <abstract>Incrementally fine-tuning foundational models on new tasks or domains is now the de facto approach in NLP. A known pitfall of this approach is the <i>catastrophic forgetting</i> of prior knowledge that happens during fine-tuning. A common approach to alleviate such forgetting is to rehearse samples from prior tasks during fine-tuning. Several existing works assume a fixed memory buffer to store prior task examples, while relying on inferences (forward passes) with the model at hand for choosing examples for rehearsal from the buffer. However, given the increasing computational cost of model inference, and decreasing cost of data storage, we focus on the setting to rehearse samples with a fixed computational budget instead of a fixed memory budget. We propose a sampling scheme, <b>mix-cd</b>, that prioritizes rehearsal of “collateral damage” samples, which are samples predicted correctly by the prior model but forgotten by the incrementally tuned one. The crux of our scheme is a procedure to efficiently estimate the density of collateral damage samples without incurring additional model inferences. Our approach is computationally efficient, easy to implement, and outperforms several leading continual learning methods in compute-constrained settings. All the code will be publicly available at https://github.com/jybai/mix-cd-rehearsal.</abstract>
      <url hash="a79ecb79">2025.findings-naacl.138</url>
      <bibkey>bai-etal-2025-efficient</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.138</doi>
    </paper>
    <paper id="139">
      <title><fixed-case>COAST</fixed-case>: Enhancing the Code Debugging Ability of <fixed-case>LLM</fixed-case>s through Communicative Agent Based Data Synthesis</title>
      <author><first>Weiqing</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hanbin</first><last>Wang</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xinze</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Yu</first><last>Gu</last></author>
      <author><first>Minghe</first><last>Yu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>2570-2585</pages>
      <abstract>Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at https://github.com/NEUIR/COAST.</abstract>
      <url hash="aca947a5">2025.findings-naacl.139</url>
      <bibkey>yang-etal-2025-coast</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.139</doi>
    </paper>
    <paper id="140">
      <title>Chain-of-Probe: Examining the Necessity and Accuracy of <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Step-by-Step</title>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Weiwen</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2586-2606</pages>
      <abstract>Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in confidence during the model’s reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by the reasoning steps required. Furthermore, by analyzing patterns in confidence change, we examine the correctness of the model’s reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model’s reasoning.</abstract>
      <url hash="dbb1f8f0">2025.findings-naacl.140</url>
      <bibkey>wang-etal-2025-chain</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.140</doi>
    </paper>
    <paper id="141">
      <title><fixed-case>INDIC</fixed-case> <fixed-case>QA</fixed-case> <fixed-case>BENCHMARK</fixed-case>: A Multilingual Benchmark to Evaluate Question Answering capability of <fixed-case>LLM</fixed-case>s for <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Abhishek Kumar</first><last>Singh</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Ashish</first><last>Mittal</last><affiliation>IBM Research, Indian Institute of Technology, Bombay and IBM Research</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>2607-2626</pages>
      <abstract>Large Language Models (LLMs) perform well on unseen tasks in English, but their abilities in non-English languages are less explored due to limited benchmarks and training data. To bridge this gap, we introduce the Indic-QA Benchmark, a large dataset for context-grounded question answering in 11 major Indian languages, covering both extractive and abstractive tasks. Evaluations of multilingual LLMs, including instruction fine-tuned versions, revealed weak performance in low-resource languages due to a strong English-language bias in their training data. We also investigated the Translate-Test paradigm,where inputs are translated to English for processing and the results are translated back into the source language for output. This approach outperformed multilingual LLMs, particularly in low-resource settings. By releasing Indic-QA, we aim to promote further research into LLMs’ question-answering capabilities in low-resource languages. This benchmark offers a critical resource to address existing limitations and foster multilingual understanding.</abstract>
      <url hash="b5e0c645">2025.findings-naacl.141</url>
      <bibkey>singh-etal-2025-indic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.141</doi>
    </paper>
    <paper id="142">
      <title>Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data</title>
      <author><first>Juanhui</first><last>Li</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheikh Muhammad</first><last>Sarwar</last></author>
      <author><first>Limeng</first><last>Cui</last><affiliation>Amazon</affiliation></author>
      <author><first>Hansu</first><last>Gu</last><affiliation>Amazon</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>2627-2641</pages>
      <abstract>In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs (teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. % and the high computational expense of processing large unlabeled datasets. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.</abstract>
      <url hash="b83b74f2">2025.findings-naacl.142</url>
      <bibkey>li-etal-2025-learning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.142</doi>
    </paper>
    <paper id="143">
      <title><fixed-case>LSDC</fixed-case>: An Efficient and Effective Large-Scale Data Compression Method for Supervised Fine-tuning of Large Language Models</title>
      <author><first>Zhaoguang</first><last>Long</last></author>
      <author><first>Yuhao</first><last>Zhou</last></author>
      <author><first>Shangqing</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yupei</first><last>Ren</last></author>
      <author><first>Li</first><last>Cai</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Chenghao</first><last>Jia</last></author>
      <author><first>Zhe</first><last>Chen</last></author>
      <author><first>Zhe</first><last>Fang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yuxiang</first><last>Song</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>2642-2653</pages>
      <abstract>With the scale of Large Language Models(LLMs) and the size of the training data continuing to expand, the computational costs required for training or tuning have significantly increased as well. In this work we propose an efficient and effective Large-Scale Data Compression (LSDC) method to substantially reduce the size of training data and thus enhance the training efficiency without compromising the performance of LLMs through a bifurcated quantization strategy. Specifically, our method first segments the dataset into multiple clusters, significantly reducing the time and memory requirements for data compression. Then, during the second phase of coreset selection, the diversity of samples is ensured by maximizing the submodular gain in order to avoid performance degradation. The comparative experiments showed that the performance of LLMs fine-tuned on a 20% compressed subset of the Alpaca dataset using LSDC outperformed those on the full dataset. Moreover,on a domain-specific instruction dataset of millions of samples, the LLMs fine-tuned on a 10% compressed dataset using LSDC outperformed those on the entire dataset, which dramatically enhances the domain-adaption capabilities of LLMs. This provides a promising potential of LSDC in training bigger LLMs from scratch and supervised fine-tuning as well.</abstract>
      <url hash="b683ff24">2025.findings-naacl.143</url>
      <bibkey>long-etal-2025-lsdc</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.143</doi>
    </paper>
    <paper id="144">
      <title>What Is Missing in Multilingual Visual Reasoning and How to Fix It</title>
      <author><first>Yueqi</first><last>Song</last></author>
      <author><first>Simran</first><last>Khanuja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2654-2667</pages>
      <abstract>NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a zero-shot setting, boosting open models LLaVA-v1.5-13B by 13.4%, LLaVA-v1.6-34B by 20.3%, and Qwen-VL by 16.7%, while also minorly improving GPT-4V’s performance.</abstract>
      <url hash="d527c997">2025.findings-naacl.144</url>
      <bibkey>song-etal-2025-missing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.144</doi>
    </paper>
    <paper id="145">
      <title>Enhancing the Prototype Network with Local-to-Global Optimization for Few-Shot Relation Extraction</title>
      <author><first>Hui</first><last>Sun</last></author>
      <author><first>Rongxin</first><last>Chen</last><affiliation>Jimei University</affiliation></author>
      <pages>2668-2677</pages>
      <abstract>Few-Shot Relation Extraction (FSRE) aims to achieve high classification performance by training relation classification models with a small amount of labeled data. Prototypical networks serve as a straightforward and efficient method for optimizing model performance by combining similarity evaluation and contrastive learning. However, directly integrating these methods can introduce unpredictable noise, such as information redundancy, which hinders classification performance and negatively affects embedding space learning. The technique presented in this paper applies Local-To-Global optimization to enhance prototypical networks in few-shot relation extraction. Specifically, this paper develops a local optimization strategy that indirectly optimizes the prototypes by optimizing the other information contained within the prototypes. It considers relation prototypes as global anchors and incorporates the techniques introduced in this paper, such as information alignment, local contrastive learning, and a local adaptive focal loss function, to address the issues of information redundancy. This approach enables the model to learn a unified and effective embedding space. We conduct extensive experiments on the FewRel 1.0 and FewRel 2.0 datasets to validate the effectiveness of the proposed model.</abstract>
      <url hash="4c701214">2025.findings-naacl.145</url>
      <bibkey>sun-chen-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.145</doi>
    </paper>
    <paper id="146">
      <title><fixed-case>LLM</fixed-case>s for Mathematical Modeling: Towards Bridging the Gap between Natural and Mathematical Languages</title>
      <author><first>Xuhan</first><last>Huang</last></author>
      <author><first>Qingning</first><last>Shen</last></author>
      <author><first>Yan</first><last>Hu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>2678-2710</pages>
      <url hash="51abce3e">2025.findings-naacl.146</url>
      <bibkey>huang-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.146</doi>
    </paper>
    <paper id="147">
      <title>Advancing <fixed-case>P</fixed-case>ersian <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Sara Bourbour</first><last>Hosseinbeigi</last><affiliation>Tarbiat Modares University</affiliation></author>
      <author><first>Behnam</first><last>Rohani</last><affiliation>Sharif University of Technology, Sharif University of Technology</affiliation></author>
      <author><first>Mostafa</first><last>Masoudi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mehrnoush</first><last>Shamsfard</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Zahra</first><last>Saaberi</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Mostafa Karimi</first><last>Manesh</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Mohammad Amin</first><last>Abbasi</last><affiliation>Iran University of Science and Technology Tehran, University of Tehran</affiliation></author>
      <pages>2711-2727</pages>
      <abstract>Evaluation of large language models (LLMs) in low-resource languages like Persian has received less attention than in high-resource languages like English. Existing evaluation approaches for Persian LLMs generally lack comprehensive frameworks, limiting their ability to assess models’ performance over a wide range of tasks requiring considerable cultural and contextual knowledge, as well as a deeper understanding of Persian literature and style. This paper first aims to fill this gap by providing two new benchmarks, PeKA and PK-BETS, on topics such as history, literature, and cultural knowledge, as well as challenging the present state-of-the-art models’ abilities in a variety of Persian language comprehension tasks. These datasets are meant to reduce data contamination while providing an accurate assessment of Persian LLMs. The second aim of this paper is the general evaluation of LLMs across the current Persian benchmarks to provide a comprehensive performance overview. By offering a structured evaluation methodology, we hope to promote the examination of LLMs in the Persian language.</abstract>
      <url hash="381c3142">2025.findings-naacl.147</url>
      <bibkey>hosseinbeigi-etal-2025-advancing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.147</doi>
    </paper>
    <paper id="148">
      <title>Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling</title>
      <author><first>Zile</first><last>Qiao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Tong</first><last>Mo</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Weiping</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2728-2740</pages>
      <abstract>Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation. Specifically, we introduce the novel concept of “supportiveness”—which represents how effectively a knowledge piece facilitates downstream tasks. Based on supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4.</abstract>
      <url hash="6d5256c0">2025.findings-naacl.148</url>
      <bibkey>qiao-etal-2025-supportiveness</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.148</doi>
    </paper>
    <paper id="149">
      <title>Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models</title>
      <author><first>Jiatao</first><last>Li</last></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>2741-2775</pages>
      <abstract>The integration of documents generated by LLMs themselves (Self-Docs) alongside retrieved documents has emerged as a promising strategy for retrieval-augmented generation systems. However, previous research primarily focuses on optimizing the use of Self-Docs, with their inherent properties remaining underexplored. To bridge this gap, we first investigate the overall effectiveness of Self-Docs, identifying key factors that shape their contribution to RAG performance (RQ1). Building on these insights, we develop a taxonomy grounded in Systemic Functional Linguistics to compare the influence of various Self-Docs categories (RQ2) and explore strategies for combining them with external sources (RQ3). Our findings reveal which types of Self-Docs are most beneficial and offer practical guidelines for leveraging them to achieve significant improvements in knowledge-intensive question answering tasks.</abstract>
      <url hash="b3e17add">2025.findings-naacl.149</url>
      <bibkey>li-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.149</doi>
    </paper>
    <paper id="150">
      <title><fixed-case>PREMISE</fixed-case>: Matching-based Prediction for Accurate Review Recommendation</title>
      <author><first>Wei</first><last>Han</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>2776-2794</pages>
      <abstract>We present PREMISE, a new architecture for the matching-based learning in the multimodal fields for the MRHP task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.</abstract>
      <url hash="0a70802d">2025.findings-naacl.150</url>
      <bibkey>han-etal-2025-premise</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.150</doi>
      <revision id="1" href="2025.findings-naacl.150v1" hash="91afa145"/>
      <revision id="2" href="2025.findings-naacl.150v2" hash="0a70802d" date="2025-06-13">Update author list.</revision>
    </paper>
    <paper id="151">
      <title>Semi-supervised Fine-tuning for Large Language Models</title>
      <author><first>Junyu</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Wei</first><last>Ju</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2795-2808</pages>
      <abstract>Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated.Towards this end, we introduce a **semi-supervised fine-tuning (SemiFT)** task and a framework named **SemiEvol** for LLM alignment from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios. Github Repository: [https://github.com/luo-junyu/SemiEvol](https://github.com/luo-junyu/SemiEvol).</abstract>
      <url hash="fc436725">2025.findings-naacl.151</url>
      <bibkey>luo-etal-2025-semi</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.151</doi>
    </paper>
    <paper id="152">
      <title><fixed-case>CALM</fixed-case>: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering</title>
      <author><first>Yumeng</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>2809-2817</pages>
      <abstract>Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions across languages, we observe significant performance disparities. To address this, we explore the **C**ross-Lingual Self-**A**ligning ability of **L**anguage **M**odels (**CALM**) to align knowledge across languages. Specifically, for a given question, we sample multiple responses across different languages and select the most self-consistent response as the target, leaving the remaining responses as negative examples. We then employ direct preference optimization (DPO) to align the model’s knowledge across different languages. Evaluations on the MEDQA and X-CSQA datasets demonstrate CALM’s effectiveness in enhancing cross-lingual knowledge question answering, both in zero-shot and retrieval-augmented settings. We also found that increasing the number of languages involved in CALM training leads to higher accuracy and consistency. We offer a qualitative analysis of how cross-lingual consistency can enhance knowledge alignment and explore the method’s generalizability.</abstract>
      <url hash="b878079a">2025.findings-naacl.152</url>
      <bibkey>wang-etal-2025-calm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.152</doi>
    </paper>
    <paper id="153">
      <title>Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring</title>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Taehee</first><last>Park</last></author>
      <author><first>Sangwon</first><last>Ryu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>2818-2824</pages>
      <abstract>In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method’s generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts.</abstract>
      <url hash="38524c55">2025.findings-naacl.153</url>
      <bibkey>do-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.153</doi>
    </paper>
    <paper id="154">
      <title><fixed-case>M</fixed-case>ed<fixed-case>E</fixed-case>ureka: A Medical Domain Benchmark for Multi-Granularity and Multi-Data-Type Embedding-Based Retrieval</title>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Nan</first><last>Wang</last></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>2825-2851</pages>
      <abstract>Embedding-based retrieval (EBR), the mainstream approach in information retrieval (IR), aims to help users obtain relevant information and plays a crucial role in retrieval-augmented generation (RAG) techniques of large language models (LLMs). Numerous methods have been proposed to significantly improve the quality of retrieved content and many generic benchmarks are proposed to evaluate the retrieval abilities of embedding models. However, texts in the medical domain present unique contexts, structures, and language patterns, such as terminology, doctor-patient dialogue, and electronic health records (EHRs). Despite these unique features, specific benchmarks for medical context retrieval are still lacking. In this paper, we propose MedEureka, an enriched benchmark designed to evaluate medical-context retrieval capabilities of embedding models with multi-granularity and multi-data types. MedEureka includes four levels of granularity and six types of medical texts, encompassing 18 datasets, incorporating granularity and data type description to prompt instruction-fine-tuned text embedding models for embedding generation. We also provide the MedEureka Toolkit to support evaluation on the MedEureka test set. Our experiments evaluate state-of-the-art open-source and proprietary embedding models, and fine-tuned classical baselines, providing a detailed performance analysis. This underscores the challenges of using embedding models for medical domain retrieval and the need for further research. Our code and data are released in the repository: <url>https://github.com/JOHNNY-fans/MedEureka</url>.</abstract>
      <url hash="7816f9d5">2025.findings-naacl.154</url>
      <bibkey>fan-etal-2025-medeureka</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.154</doi>
    </paper>
    <paper id="155">
      <title>A Federated Framework for <fixed-case>LLM</fixed-case>-based Recommendation</title>
      <author><first>Jujia</first><last>Zhao</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Chen</first><last>Xu</last></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>2852-2865</pages>
      <abstract>Large Language Models (LLMs) have showcased their potential in building generative recommendation systems through fine-tuning user behavior data. However, utilizing the user behavior data may pose significant privacy risks like in the traditional recommender models, potentially leading to ethical dilemmas and violations of data protection regulations. To address the privacy concerns, Federated Learning for Recommendation (Fed4Rec) has been identified as a promising solution. However, directly applying Fed4Rec in the LLM context introduces two challenges: 1) exacerbated client performance imbalance, which ultimately impacts the system’s long-term effectiveness, and 2) substantial client resource costs, posing a high demand for clients’ both computational and storage capability to locally train and infer LLMs.To tackle these challenges, we propose a federated framework for LLM-based recommendation (shorted as FELLRec). Generally, FELLRec designs two key strategies. 1) Dynamic balance strategy, which designs dynamic parameter aggregation and learning speed for different clients during training, aiming to ensure relatively balanced performance across clients. 2) Flexible storage strategy, which selectively retains certain sensitive LLM layers on the client side, while offloading other layers to the server, aiming to preserve privacy while saving resources. Specifically, FELLRec flexibly maintains those input and output layers on the client side to ensure the protection of all sensitive information. Experiment results show that FELLRec can achieve a more balanced client performance and improved overall performance in a computational and storage-efficient way while safeguarding user privacy well.</abstract>
      <url hash="99f2ea32">2025.findings-naacl.155</url>
      <bibkey>zhao-etal-2025-federated</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.155</doi>
    </paper>
    <paper id="156">
      <title><fixed-case>W</fixed-case>ater<fixed-case>S</fixed-case>eeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents</title>
      <author><first>Leyi</first><last>Pan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yijian</first><last>Lu</last></author>
      <author><first>Zitian</first><last>Gao</last><affiliation>Ubiquant</affiliation></author>
      <author><first>Yichen</first><last>Di</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>2866-2882</pages>
      <abstract>Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, its localization capability lays the foundation for building interpretable AI detection systems. Our code is available at https://github.com/THU-BPM/WaterSeeker.</abstract>
      <url hash="40b052d2">2025.findings-naacl.156</url>
      <bibkey>pan-etal-2025-waterseeker</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.156</doi>
    </paper>
    <paper id="157">
      <title><fixed-case>MIRAGE</fixed-case>: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation</title>
      <author><first>Chanhee</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>2883-2900</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings.</abstract>
      <url hash="4c966747">2025.findings-naacl.157</url>
      <bibkey>park-etal-2025-mirage</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.157</doi>
    </paper>
    <paper id="158">
      <title><fixed-case>FIRE</fixed-case>: Fact-checking with Iterative Retrieval and Verification</title>
      <author><first>Zhuohan</first><last>Xie</last></author>
      <author><first>Rui</first><last>Xing</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Melbourne</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Hasan</first><last>Iqbal</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Dhruv</first><last>Sahnan</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>2901-2914</pages>
      <abstract>Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model’s internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose FIRE, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare FIRE with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that FIRE holds promise for application in large-scale fact-checking operations.</abstract>
      <url hash="c365b339">2025.findings-naacl.158</url>
      <bibkey>xie-etal-2025-fire</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.158</doi>
    </paper>
    <paper id="159">
      <title>Lessons from a User Experience Evaluation of <fixed-case>NLP</fixed-case> Interfaces</title>
      <author><first>Eduardo</first><last>Calò</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Lydia</first><last>Penkert</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Saad</first><last>Mahamood</last><affiliation>trivago N.V.</affiliation></author>
      <pages>2915-2929</pages>
      <abstract>Human evaluations lay at the heart of evaluations within the field of Natural Language Processing (NLP). Seen as the “golden standard” of evaluations, questions are being asked on whether these evaluations are both reproducible and repeatable. One overlooked aspect is the design choices made by researchers when designing user interfaces (UIs). In this paper, four UIs used in past NLP human evaluations are assessed by UX experts, based on standardized human-centered interaction principles. Building on these insights, we derive several recommendations that the NLP community should apply when designing UIs, to enable more consistent human evaluation responses.</abstract>
      <url hash="71e8a802">2025.findings-naacl.159</url>
      <bibkey>calo-etal-2025-lessons</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.159</doi>
    </paper>
    <paper id="160">
      <title><fixed-case>T</fixed-case>rend<fixed-case>S</fixed-case>im: Simulating Trending Topics in Social Media Under Poisoning Attacks with <fixed-case>LLM</fixed-case>-based Multi-agent System</title>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Chen</first><last>Ma</last></author>
      <author><first>Yaning</first><last>Qu</last></author>
      <author><first>Ye</first><last>Luo</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Le</first><last>Wu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2930-2949</pages>
      <abstract>Trending topics have become a significant part of modern social media, attracting users to participate in discussions of breaking events. However, they also bring in a new channel for poisoning attacks, resulting in negative impacts on society. Therefore, it is urgent to study this critical problem and develop effective strategies for defense. In this paper, we propose TrendSim, an LLM-based multi-agent system to simulate trending topics in social media under poisoning attacks. Specifically, we create a simulation environment for trending topics that incorporates a time-aware interaction mechanism, centralized message dissemination, and an interactive system. Moreover, we develop LLM-based humanoid agents to simulate users in social media, and propose prototype-based attackers to replicate poisoning attacks. Besides, we evaluate TrendSim from multiple aspects to validate its effectiveness. Based on TrendSim, we conduct simulation experiments to study four critical problems about poisoning attacks on trending topics.</abstract>
      <url hash="651eba0f">2025.findings-naacl.160</url>
      <bibkey>zhang-etal-2025-trendsim</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.160</doi>
    </paper>
    <paper id="161">
      <title><fixed-case>ASR</fixed-case>ank: Zero-Shot Re-Ranking with Answer Scent for Document Retrieval</title>
      <author><first>Abdelrahman</first><last>Abdallah</last></author>
      <author><first>Jamshid</first><last>Mozafari</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Bhawna</first><last>Piryani</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <pages>2950-2970</pages>
      <abstract>Retrieval-Augmented Generation (RAG) models have drawn considerable attention in modern open-domain question answering. The effectiveness of RAG depends on the quality of the top retrieved documents. However, conventional retrieval methods sometimes fail to rank the most relevant documents at the top. In this paper, we introduce ASRANK, a new re-ranking method based on scoring retrieved documents using zero-shot answer scent which relies on a pre-trained large language model to compute the likelihood of the document-derived answers aligning with the answer scent. Our approach demonstrates marked improvements across several datasets, including NQ, TriviaQA, WebQA, ArchivalQA, HotpotQA, and Entity Questions. Notably, ASRANK increases Top-1 retrieval accuracy on NQ from 19.2% to 46.5% for MSS and 22.1% to 47.3% for BM25. It also shows strong retrieval performance on several datasets compared to state-of-the-art methods (47.3 Top-1 by ASRANK vs 35.4 by UPR by BM25).</abstract>
      <url hash="270f9aee">2025.findings-naacl.161</url>
      <bibkey>abdallah-etal-2025-asrank</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.161</doi>
    </paper>
    <paper id="162">
      <title><fixed-case>DSQG</fixed-case>-Syn: Synthesizing High-quality Data for Text-to-<fixed-case>SQL</fixed-case> Parsing by Domain Specific Question Generation</title>
      <author><first>Shaoming</first><last>Duan</last></author>
      <author><first>Youxuan</first><last>Wu</last></author>
      <author><first>Chuanyi</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Han</last><affiliation>Harbin Institute of Technology(ShenZhen)</affiliation></author>
      <author><first>Shengyuan</first><last>Yu</last></author>
      <author><first>Liang</first><last>Yan</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Yingwei</first><last>Liang</last><affiliation>Guangdong Power Grid Co., Ltd</affiliation></author>
      <pages>2971-2989</pages>
      <abstract>Synthetic data has recently proven effective in enhancing the accuracy of Text-to-SQL parsers. However, existing methods generate SQL queries first by randomly sampling tables and columns based on probability and then synthesize natural language questions (NLQs). This approach often produces a large number of NLQ-SQL pairs that are irrelevant to the target domain and inconsistent in query intent, significantly diminishing the fine-tuning effectiveness of LLMs. In this paper, we introduce DSQG-Syn, a novel text-to-SQL data synthesis framework that based on domain-specific question generation. Specifically, we design a question generation method that creates domain-relevant questions based on predefined question types, ensuring coverage of major SQL operations. Guided by these questions, we synthesize NLQ-SQL pairs that are both domain-relevant and intent-consistent. To further enhance data quality, we filter out noisy samples from the generated pairs. When popular open-source LLMs are fine-tuned on our high-quality synthesized dataset, they achieve significant accuracy improvements, surpassing the performance of closed-source LLM-based approaches. Moreover, we demonstrate that our method outperforms existing state-of-the-art (SOTA) data synthesis techniques.</abstract>
      <url hash="8f0f1389">2025.findings-naacl.162</url>
      <bibkey>duan-etal-2025-dsqg</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.162</doi>
    </paper>
    <paper id="163">
      <title><fixed-case>E</fixed-case>go<fixed-case>S</fixed-case>peak: Learning When to Speak for Egocentric Conversational Agents in the Wild</title>
      <author><first>Junhyeok</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Min Soo</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Jungbin</first><last>Cho</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jisoo</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sungwoong</first><last>Kim</last></author>
      <author><first>Gyeongbo</first><last>Sim</last><affiliation>NCSOFT</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>2990-3005</pages>
      <abstract>Predicting when to initiate speech in real-world environments remains a fundamental challenge for conversational agents. We introduce , a novel framework for real-time speech initiation prediction in egocentric streaming video. By modeling the conversation from the speaker’s first-person viewpoint, is tailored for human-like interactions in which a conversational agent must continuously observe its environment and dynamically decide when to talk.Our approach bridges the gap between simplified experimental setups and complex natural conversations by integrating four key capabilities: (1) first-person perspective, (2) RGB processing, (3) online processing, and (4) untrimmed video processing. We also present YT-Conversation, a diverse collection of in-the-wild conversational videos from YouTube, as a resource for large-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that outperforms random and silence-based baselines in real time. Our results also highlight the importance of multimodal input and context length in effectively deciding when to speak. Code and data are available at website.</abstract>
      <url hash="6c6da0af">2025.findings-naacl.163</url>
      <bibkey>kim-etal-2025-egospeak</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.163</doi>
    </paper>
    <paper id="164">
      <title><fixed-case>P</fixed-case>lot2<fixed-case>C</fixed-case>ode: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots</title>
      <author><first>Chengyue</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zhixuan</first><last>Liang</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Yixiao</first><last>Ge</last></author>
      <author><first>Qiushan</first><last>Guo</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zeyu</first><last>Lu</last></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Ying</first><last>Shan</last><affiliation>Tencent AI Lab Center of Visual Computing and Tencent PCG ARC Lab</affiliation></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>3006-3028</pages>
      <abstract>Multi-modal Large Language Models have shown remarkable progress in visual contexts, yet their ability to convert visual figures into executable code remains underexplored. To address this, we introduce Plot2Code, a comprehensive benchmark designed to assess MLLMs’ visual coding capabilities. Plot2Code includes 132 high-quality matplotlib plots across six plot types, as well as an additional 150 and 86 plots from Python’s and R’s plotly libraries respectively, totaling 368 plots. Each plot is paired with its source code and a descriptive instruction generated by GPT-4, enabling thorough evaluation across diverse inputs. Furthermore, we propose three automatic evaluation metrics—code pass rate, text-match ratio, and GPT-4V rating judgement—to assess the quality of generated code and rendered images. Notably, the GPT-4V rating demonstrates strong reliability, as it correlates well with human evaluations, particularly for datasets of a certain size. Cross-validation across MLLMs (GPT-4V, Gemini-1.5-Pro, and Claude-3-Opus) also shows high consistency in ratings, which likely stems from the fact that ratings are based on rendered images rather than direct MLLM outputs, indicating minimal bias for this metric. Our evaluation of 14 MLLMs, including both proprietary and open-source models, highlights significant challenges in visual coding, particularly for text-dense plots, where MLLMs heavily rely on textual instructions. We believe these findings will advance future development of MLLMs.</abstract>
      <url hash="6d910c3c">2025.findings-naacl.164</url>
      <bibkey>wu-etal-2025-plot2code</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.164</doi>
    </paper>
    <paper id="165">
      <title><fixed-case>F</fixed-case>unnel<fixed-case>RAG</fixed-case>: A Coarse-to-Fine Progressive Retrieval Paradigm for <fixed-case>RAG</fixed-case></title>
      <author><first>Xinping</first><last>Zhao</last></author>
      <author><first>Yan</first><last>Zhong</last></author>
      <author><first>Zetian</first><last>Sun</last></author>
      <author><first>Xinshuo</first><last>Hu</last></author>
      <author><first>Zhenyu</first><last>Liu</last></author>
      <author><first>Dongfang</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>3029-3046</pages>
      <abstract>Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It mainly consists of retrieval and generation. The retrieval modules (a.k.a. retrievers) aim to find useful information used to facilitate the generation modules (a.k.a. generators). As such, generators’ performance largely depends on the effectiveness and efficiency of retrievers. However, the widely used retrieval paradigm remains flat. It treats retrieval procedures as a one-off deal with constant granularity. Despite effectiveness, we argue that they suffer from two limitations: (1) flat retrieval exerts a significant burden on one retriever; (2) constant granularity limits the ceiling of retrieval performance. In this work, we propose a progressive retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small quantity, and low-to-high capacity, which can relieve the burden on one retriever and also promote the ceiling of retrieval performance. Extensive experiments manifest that FunnelRAG achieves comparable retrieval performance while the time overhead is reduced by nearly 40 percent.</abstract>
      <url hash="81493860">2025.findings-naacl.165</url>
      <bibkey>zhao-etal-2025-funnelrag</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.165</doi>
    </paper>
    <paper id="166">
      <title>The Power of Bullet Lists: A Simple Yet Effective Prompting Approach to Enhancing Spatial Reasoning in Large Language Models</title>
      <author><first>Ikhyun</first><last>Cho</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Changyeon</first><last>Park</last><affiliation>Mirae Asset Securities</affiliation></author>
      <author><first>Julia</first><last>Hockenmaier</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>3047-3057</pages>
      <abstract>While large language models (LLMs) are dominating the field of natural language processing, it remains an open question how well these models can perform spatial reasoning. Contrary to recent studies suggesting that LLMs struggle with spatial reasoning tasks, we demonstrate in this paper that a novel prompting technique, termed Patient Visualization of Thought (Patient-VoT), can boost LLMs’ spatial reasoning abilities. The core idea behind Patient-VoT is to explicitly integrate *bullet lists, coordinates, and visualizations* into the reasoning process. By applying Patient-VoT, we achieve a significant boost in spatial reasoning performance compared to prior prompting techniques. We also show that integrating bullet lists into reasoning is effective in planning tasks, highlighting its general effectiveness across different applications.</abstract>
      <url hash="5afb6659">2025.findings-naacl.166</url>
      <bibkey>cho-etal-2025-power</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.166</doi>
    </paper>
    <paper id="167">
      <title>Overcoming both Domain Shift and Label Shift for Referring Video Segmentation</title>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Sashuai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yan</first><last>Xia</last></author>
      <pages>3058-3069</pages>
      <abstract>Open-set domain generalization (OSDG) aims to enhance the robustness of the model when facing both domain shift and label shift, highlighting a wide range of potential in real-world applications. However, previous OSDG methods can only recognize seen objects and mark all unseen objects as “unknown” categories during inference, which is far from satisfactory. In this paper, we explore the scenario of referring video segmentation to study how to make the model maintain good segmentation ability for unknown objects under OSDG setting. To bridge the huge gap caused by label shift, we propose CLIP-based Reasoning Prompt (CRPrompt), which can combine text and visual prompts together to improve text-object matching ability of CLIP, transferring the segmentation ability to unseen classes based on the knowledge learned from seen classes and large-scale text-image pairs, i.e., color, shape, spatial relationships. Meanwhile, to improve the robustness of CRPrompt, we propose Retrieval-augmented Instance Normalization (RaIN), which can effectively enhance the robustness of the model by retrieving visual objects with similar semantic concepts through input query and performing Instance Norm among them. Extensive experiments on open-set and zero-shot domain generalization tasks demonstrate the effectiveness of our approach.</abstract>
      <url hash="7a3c0762">2025.findings-naacl.167</url>
      <bibkey>huang-etal-2025-overcoming</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.167</doi>
    </paper>
    <paper id="168">
      <title>Language Modeling with Editable External Knowledge</title>
      <author><first>Belinda Z.</first><last>Li</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Emmy</first><last>Liu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Alexis</first><last>Ross</last><affiliation>Massachusetts Institute of Technology and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Abbas</first><last>Zeitoun</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>3070-3090</pages>
      <abstract>When the world changes, so does the text that people write about it. How do we build language models that can be easily updated to reflect these changes? One popular approach is retrieval-augmented generation (RAG), in which new documents are inserted into a knowledge base and retrieved during prediction for downstream tasks. Most prior work on RAG has focused on improving model behavior during *prediction* through better retrieval or reasoning. This paper introduces ERASE, which instead improves model behavior **when new documents are acquired**, by incrementally deleting or rewriting other entries in the knowledge base each time a document is added. In two new datasets evaluating models’ ability to answer questions about a stream of news articles or conversations, ERASE improves accuracy relative to conventional retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B) absolute. This improvement is complementary to improved retrieval or reasoning for RAG: we demonstrate an 11% improvement by applying ERASE to SelfRAG.</abstract>
      <url hash="a17bba86">2025.findings-naacl.168</url>
      <bibkey>li-etal-2025-language</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.168</doi>
    </paper>
    <paper id="169">
      <title>Beyond Excess and Deficiency: Adaptive Length Bias Mitigation in Reward Models for <fixed-case>RLHF</fixed-case></title>
      <author><first>Yuyan</first><last>Bu</last></author>
      <author><first>Liangyu</first><last>Huo</last></author>
      <author><first>Yi</first><last>Jing</last><affiliation>duxiaoman</affiliation></author>
      <author><first>Qing</first><last>Yang</last></author>
      <pages>3091-3098</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models (LLMs) with human values. However, it has been noted that reward models in RLHF often exhibit unintended biases, such as an overemphasis on response length based on the erroneous assumption that longer responses are universally preferred. This “length bias” can lead to excessively verbose responses that compromise the quality of LLMs alignment. Previous efforts to mitigate length bias in reward models have inadvertently decreased their accuracy by neglecting the legitimate influence of response length on human preferences. In this work, we argue that response length is a context-specific factor in human evaluations, with different queries naturally eliciting varying preferences for response length. We propose an adaptive approach to modeling length preference that dynamically adjusts the influence of response length in reward evaluations according to the context of the query. Experimental results demonstrate that our adaptive approach effectively balances the mitigation of undesired length hacking and alignment accuracy, reducing unnecessary verbosity while improving overall response quality.</abstract>
      <url hash="5a9f523e">2025.findings-naacl.169</url>
      <bibkey>bu-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.169</doi>
    </paper>
    <paper id="170">
      <title>Neuroplasticity and Corruption in Model Mechanisms: A Case Study Of Indirect Object Identification</title>
      <author><first>Vishnu Kabir</first><last>Chhabra</last></author>
      <author><first>Ding</first><last>Zhu</last></author>
      <author><first>Mohammad Mahdi</first><last>Khalili</last><affiliation>Ohio State University, Columbus and Yahoo! Research</affiliation></author>
      <pages>3099-3122</pages>
      <abstract>Previous research has shown that fine-tuning language models on general tasks enhance their underlying mechanisms. However, the impact of fine-tuning on poisoned data and the resulting changes in these mechanisms are poorly understood. This study investigates the changes in a model’s mechanisms during toxic fine-tuning and identifies the primary corruption mechanisms. We also analyze the changes after retraining a corrupted model on the original dataset and observe neuroplasticity behaviors, where the model relearns original mechanisms after fine-tuning the corrupted model. Our findings indicate that; (i) Underlying mechanisms are amplified across task-specific fine-tuning which can be generalized to longer epochs, (ii) Model corruption via toxic fine-tuning is localized to specific circuit components, (iii) Models exhibit neuroplasticity when retraining corrupted models on clean dataset, reforming the original model mechanisms.</abstract>
      <url hash="04193f10">2025.findings-naacl.170</url>
      <bibkey>chhabra-etal-2025-neuroplasticity</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.170</doi>
    </paper>
    <paper id="171">
      <title><fixed-case>VANE</fixed-case>-Bench: Video Anomaly Evaluation Benchmark for Conversational <fixed-case>LMM</fixed-case>s</title>
      <author><first>Hanan</first><last>Gani</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rohit</first><last>Bharadwaj</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Muzammal</first><last>Naseer</last><affiliation>Khalifa University of Science, Technology and Research</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <pages>3123-3140</pages>
      <abstract>The recent advancements in Large Language Models (LLMs) have greatly influenced the development of Large Multi-modal Video Models (Video-LMMs), significantly enhancing our ability to interpret and analyze video data. Despite their impressive capabilities, current Video-LMMs have not been evaluated for anomaly detection tasks, which is critical to their deployment in practical scenarios e.g., towards identifying deepfakes, manipulated video content, traffic accidents and crimes. In this paper, we introduce VANE-Bench, a benchmark designed to assess the proficiency of Video-LMMs in detecting and localizing anomalies and inconsistencies in videos. Our dataset comprises an array of videos synthetically generated using existing state-of-the-art text-to-video generation models, encompassing a variety of subtle anomalies and inconsistencies grouped into five categories: unnatural transformations, unnatural appearance, pass-through, disappearance and sudden appearance. Additionally, our benchmark features real-world samples from existing anomaly detection datasets, focusing on crime-related irregularities, atypical pedestrian behavior, and unusual events. The task is structured as a visual question-answering challenge to gauge the models’ ability to accurately detect and localize the anomalies within the videos. We evaluate nine existing Video-LMMs, both open and closed sources, on this benchmarking task and find that most of the models encounter difficulties in effectively identifying the subtle anomalies. In conclusion, our research offers significant insights into the current capabilities of Video-LMMs in the realm of anomaly detection, highlighting the importance of our work in evaluating and improving these models for real-world applications. Our code and data is publicly available at https://github.com/rohit901/VANE-Bench.</abstract>
      <url hash="e52719c5">2025.findings-naacl.171</url>
      <bibkey>gani-etal-2025-vane</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.171</doi>
    </paper>
    <paper id="172">
      <title>Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models</title>
      <author><first>Jiachen</first><last>Ma</last></author>
      <author><first>Yijiang</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Xiao</last><affiliation>Yale University and Zhejiang University</affiliation></author>
      <author><first>Anda</first><last>Cao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Ye</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3141-3157</pages>
      <abstract>Text-to-image (T2I) models can be maliciously used to generate harmful content such as sexually explicit, unfaithful, and misleading or Not-Safe-for-Work (NSFW) images. Previous attacks largely depend on the availability of the diffusion model or involve a lengthy optimization process. In this work, we investigate a more practical and universal attack that does not require the presence of a target model and demonstrate that the high-dimensional text embedding space inherently contains NSFW concepts that can be exploited to generate harmful images. We present the Jailbreaking Prompt Attack (JPA). JPA first searches for the target malicious concepts in the text embedding space using a group of antonyms generated by ChatGPT. Subsequently, a prefix prompt is optimized in the discrete vocabulary space to align malicious concepts semantically in the text embedding space.We further introduce a soft assignment with gradient masking technique that allows us to perform gradient ascent in the discrete vocabulary space.We perform extensive experiments with open-sourced T2I models, e.g. stable-diffusion-v1-4 and closed-sourced online services, e.g. DALL·E 2 and Midjourney with black-box safety checkers. Results show that (1) JPA bypasses both text and image safety checkers, (2) while preserving high semantic alignment with the target prompt. (3) JPA demonstrates a much faster speed than previous methods and can be executed in a fully automated manner. These merits render it a valuable tool for robustness evaluation in future text-to-image generation research.</abstract>
      <url hash="4276ca93">2025.findings-naacl.172</url>
      <bibkey>ma-etal-2025-jailbreaking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.172</doi>
    </paper>
    <paper id="173">
      <title><fixed-case>E</fixed-case>mo3<fixed-case>D</fixed-case>: Metric and Benchmarking Dataset for 3<fixed-case>D</fixed-case> Facial Expression Generation from Emotion Description</title>
      <author><first>Mahshid</first><last>Dehghani</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Amirahmad</first><last>Shafiee</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Ali</first><last>Shafiei</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Neda</first><last>Fallah</last></author>
      <author><first>Farahmand</first><last>Alizadeh</last></author>
      <author><first>Mohammad Mehdi</first><last>Gholinejad</last></author>
      <author><first>Hamid</first><last>Behroozi</last></author>
      <author><first>Jafar</first><last>Habibi</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Ehsaneddin</first><last>Asgari</last><affiliation>Qatar Computing Research Institute and University of California, Berkeley</affiliation></author>
      <pages>3158-3172</pages>
      <abstract>3D facial emotion modeling has important applications in areas such as animation design, virtual reality, and emotional human-computer interaction (HCI). However, existing models are constrained by limited emotion classes and insufficient datasets. To address this, we introduce Emo3D, an extensive “Text-Image-Expression dataset” that spans a wide spectrum of human emotions, each paired with images and 3D blendshapes. Leveraging Large Language Models (LLMs), we generate a diverse array of textual descriptions, enabling the capture of a broad range of emotional expressions. Using this unique dataset, we perform a comprehensive evaluation of fine-tuned language-based models and vision-language models, such as Contrastive Language-Image Pretraining (CLIP), for 3D facial expression synthesis. To better assess conveyed emotions, we introduce Emo3D metric, a new evaluation metric that aligns more closely with human perception than traditional Mean Squared Error (MSE). Unlike MSE, which focuses on numerical differences, Emo3D captures emotional nuances in visual-text alignment and semantic richness. Emo3D dataset and metric hold great potential for advancing applications in animation and virtual reality.</abstract>
      <url hash="6b20eaf0">2025.findings-naacl.173</url>
      <bibkey>dehghani-etal-2025-emo3d</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.173</doi>
    </paper>
    <paper id="174">
      <title>Task-wrapped Continual Learning in Task-Oriented Dialogue Systems</title>
      <author><first>Min</first><last>Zeng</last></author>
      <author><first>Haiqin</first><last>Yang</last><affiliation>International Digital Economy Academy (IDEA)</affiliation></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>3173-3183</pages>
      <abstract>Continual learning is vital for task-oriented dialogue systems (ToDs), and AdapterCL, equipped with residual adapters, has proven effectiveness in this domain. However, its performance is limited by training separate adapters for each task, preventing global knowledge sharing. To address this, we propose **Task-wrapped Continual Learning (TCL)**, a novel framework that employs **Task-Wrapped Adapters (TWAs)**, to simultaneously learn both global and task-specific information through parameter sharing. TCL leverages task-conditioned hypernetworks to transfer global knowledge across tasks, enabling TWAs to start from more informed initialization, efficiently learning task-specific details while reducing model parameters. Additionally, the simple, linear structure of both hypernetworks and TWAs ensure stable training, with task-free inference supported through effective loss utilization. Across 37 ToD domains, TCL consistently outperforms AdapterCL, significantly reducing forgetting. Remarkably, by setting the task embedding dimension to 1, TCL achieves a 4.76% improvement over AdapterCL while using only 46% of the parameters. These findings position TWA as a lightweight, powerful alternative to traditional adapters, offering a promising solution for continual learning in ToDs. The code is availableat https://github.com/cloversjtu/TCL.</abstract>
      <url hash="30e97a46">2025.findings-naacl.174</url>
      <bibkey>zeng-etal-2025-task</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.174</doi>
    </paper>
    <paper id="175">
      <title>Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains</title>
      <author><first>Katerina</first><last>Korre</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last><affiliation>Università di Bologna</affiliation></author>
      <pages>3184-3198</pages>
      <abstract>Hate speech relies heavily on cultural influences, leading to varying individual interpretations. For that reason, we propose a Semantic Componential Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate speech definitions. We create the first dataset of hate speech definitions encompassing 493 definitions from more than 100 cultures, drawn from five key domains: online dictionaries, academic research, Wikipedia, legal texts, and online platforms. By decomposing these definitions into semantic components,our analysis reveals significant variation across definitions, yet many domains borrow definitions from one another without taking into account the target culture. We conduct zero-shot model experiments using our proposed dataset, employing three popular open-sourced LLMs to understand the impact of different definitions on hate speech detection. Our findings indicate that LLMs are sensitive to definitions: responses for hate speech detection change according to the complexity of definitions used in the prompt.</abstract>
      <url hash="b3d13510">2025.findings-naacl.175</url>
      <bibkey>korre-etal-2025-untangling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.175</doi>
    </paper>
    <paper id="176">
      <title><fixed-case>C</fixed-case>ode<fixed-case>RAG</fixed-case>-Bench: Can Retrieval Augment Code Generation?</title>
      <author><first>Zora Zhiruo</first><last>Wang</last></author>
      <author><first>Akari</first><last>Asai</last><affiliation>Paul G. Allen School of Computer Science &amp; Engineering, University of Washington</affiliation></author>
      <author><first>Xinyan Velocity</first><last>Yu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Frank F.</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Meta AI and Carnegie Mellon University</affiliation></author>
      <pages>3199-3214</pages>
      <abstract>While language models (LMs) excel at generating code, many programs are difficult to generate using only parametric knowledge. Despite the success of retrieval-augmented generation (RAG) in text-centric tasks, its potential for code generation remains under-explored. This work introduces CodeRAG-bench, a holistic retrieval-augmented code generation benchmark covering tasks like basic programming, open-domain, and repository-level problems and provides reproducible evaluations on both retrieval and end-to-end code generation performance. We further create a diverse, open datastore for code retrieval, aggregating sources such as competition solutions, tutorials, library documentation, StackOverflow posts, and GitHub repositories. Based on CodeRAG-bench, we conduct large-scale evaluations of 10 retrievers and 10 LMs and systematically analyze when retrieval can benefit code generation models and identify remaining challenges. We find that while retrieving high-quality contexts improves code generation, retrievers often struggle to fetch useful contexts, and generators face limitations in using those contexts effectively. We hope CodeRAG-bench encourages further development in code-oriented RAG methods.</abstract>
      <url hash="1c333a0c">2025.findings-naacl.176</url>
      <bibkey>wang-etal-2025-coderag</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.176</doi>
    </paper>
    <paper id="177">
      <title>Multi-Condition Guided Diffusion Network for Multimodal Emotion Recognition in Conversation</title>
      <author><first>Wenjin</first><last>Tian</last></author>
      <author><first>Xianying</first><last>Huang</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>Shihao</first><last>Zou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>3215-3227</pages>
      <abstract>Emotion recognition in conversation (ERC) involves identifying emotional labels associated with utterances within a conversation, a task that is essential for developing empathetic robots. Current research emphasizes contextual factors, the speaker’s influence, and extracting complementary information across different modalities. However, it often overlooks the cross-modal noise at the semantic level and the redundant information brought by the features themselves. This study introduces a diffusion-based approach designed to effectively address the challenges posed by redundant information and unexpected noise while robustly capturing shared semantics, thus facilitating the learning of compact and representative features from multimodal data. Specifically, we present the Multi-Condition Guided Diffusion Network (McDiff). McDiff employs a modal prior knowledge extraction strategy to derive the prior distribution for each modality, thereby enhancing the regional attention of each modality and applying the generated prior distribution at each diffusion step. Furthermore, we propose a method to learn the mutual information of each modality through a specific objective constraints approach prior to the forward process, which aims to improve inter-modal interaction and mitigate the effects of noise and redundancy. Comprehensive experiments conducted on two multimodal datasets, IEMOCAP and MELD, demonstrate that McDiff significantly surpasses existing state-of-the-art methodologies, thereby affirming the generalizability and efficacy of the proposed model.</abstract>
      <url hash="09c9afba">2025.findings-naacl.177</url>
      <bibkey>tian-etal-2025-multi</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.177</doi>
    </paper>
    <paper id="178">
      <title>Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Senses</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Ruochen</first><last>Zhang</last><affiliation>Brown University</affiliation></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Elisa</first><last>Gilbert</last><affiliation>Universität Leipzig</affiliation></author>
      <author><first>Hiroki</first><last>Nomoto</last><affiliation>Tokyo University of Foreign Studies</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>3228-3250</pages>
      <abstract>Multilingual large language models (LLMs) have gained prominence, but concerns arise regarding their reliability beyond English. This study addresses the gap in cross-lingual semantic evaluation by introducing a novel benchmark for cross-lingual sense disambiguation, StingrayBench. In this paper, we demonstrate using false friends—words that are orthographically similar but have completely different meanings in two languages— as a possible approach to pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We collect false friends in four language pairs, namely Indonesian-Malay, Indonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to distinguish the use of them in context. In our analysis of various models, we observe they tend to be biased toward higher-resource languages. We also propose new metrics for quantifying the cross-lingual sense bias and comprehension based on our benchmark. Our work contributes to developing more diverse and inclusive language modeling, promoting fairer access for the wider multilingual community.</abstract>
      <url hash="f8de59d1">2025.findings-naacl.178</url>
      <bibkey>cahyawijaya-etal-2025-thank</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.178</doi>
    </paper>
    <paper id="179">
      <title>Atoxia: Red-teaming Large Language Models with Target Toxic Answers</title>
      <author><first>Yuhao</first><last>Du</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Zhuo</first><last>Li</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Pengyu</first><last>Cheng</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>3251-3266</pages>
      <abstract>Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content, causing unexpected negative social impacts. This vulnerability highlights the necessity for robust LLM red-teaming strategies to identify and mitigate such risks before large-scale application. To detect specific types of risks, we propose a novel red-teaming method that **A**ttacks LLMs with **T**arget **Toxi**c **A**nswers (**Atoxia**). Given a particular harmful answer, Atoxia generates a corresponding user query and a misleading answer opening to examine the internal defects of a given LLM. The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward. We verify the effectiveness of our method on various red-teaming benchmarks, such as AdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can successfully detect safety risks in not only open-source models but also state-of-the-art black-box models such as GPT-4o.</abstract>
      <url hash="7764c4c1">2025.findings-naacl.179</url>
      <bibkey>du-etal-2025-atoxia</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.179</doi>
    </paper>
    <paper id="180">
      <title>A Practical Method for Generating String Counterfactuals</title>
      <author><first>Matan</first><last>Avitan</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar-Ilan University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Shauli</first><last>Ravfogel</last><affiliation>New York University</affiliation></author>
      <pages>3267-3286</pages>
      <abstract>Interventions targeting the representation space of language models (LMs) have emerged as an effective means to influence model behavior. Such methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model’s representations and, in so doing, create a counterfactual representation. However, because the intervention operates within the representation space, understanding precisely what aspects of the text it modifies poses a challenge. In this paper, we give a method to convert representation counterfactuals into string counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation space intervention and to interpret the features utilized to encode a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification through data augmentation.</abstract>
      <url hash="1a722468">2025.findings-naacl.180</url>
      <bibkey>avitan-etal-2025-practical</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.180</doi>
    </paper>
    <paper id="181">
      <title>Probing-<fixed-case>RAG</fixed-case>: Self-Probing to Guide Language Models in Selective Document Retrieval</title>
      <author><first>Ingeol</first><last>Baek</last></author>
      <author><first>Hwan</first><last>Chang</last></author>
      <author><first>ByeongJeong</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Jimin</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>3287-3304</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and-generate processes may not be optimized for real-world scenarios, where queries might require multiple retrieval steps or none at all. In this paper, we propose a Probing-RAG, which utilizes the hidden state representations from the intermediate layers of language models to adaptively determine the necessity of additional retrievals for a given query. By employing a pre-trained prober, Probing-RAG effectively captures the model’s internal cognition, enabling reliable decision-making about retrieving external documents. Experimental results across five open-domain QA datasets demonstrate that Probing-RAG outperforms previous methods while reducing the number of redundant retrieval steps.</abstract>
      <url hash="705b45c4">2025.findings-naacl.181</url>
      <bibkey>baek-etal-2025-probing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.181</doi>
    </paper>
    <paper id="182">
      <title>Extracting Military Event Temporal Relations via Relative Event Time Prediction and Virtual Adversarial Training</title>
      <author><first>Jie</first><last>Gong</last></author>
      <author><first>Qiwang</first><last>Hu</last></author>
      <pages>3305-3317</pages>
      <abstract>Extracting temporal relationships between events in the text is crucial for understanding how events unfold over time, especially in the information-dense and precision-demanding military field. Existing models for extracting event temporal relations typically compare the relative times of events directly, neglecting the contextual information between event pairs. This can lead to difficulties in handling uncertain temporal boundaries expressed in text. In this paper, we propose an event temporal relationship extraction model for the military field, based on relative event time prediction and virtual adversarial training, MFRV. The relative event time prediction as an auxiliary task enhances the model’s ability to capture and infer temporal relationships. Virtual adversarial training increases the model’s generalization by generating adversarial samples. Additionally, we adopt the MoCo (Multi-objective gradient correction) method to balance the losses from relative event time prediction and virtual adversarial training, effectively resolving the gradient bias issue in multi-objective optimization. Furthermore, we have constructed a new dataset, TRMF, specifically for event temporal relationship extraction in the military field. Experiments conducted on TRMF, as well as widely used public datasets MATRES and TCR, demonstrate the effectiveness of MFRV.</abstract>
      <url hash="d786588b">2025.findings-naacl.182</url>
      <bibkey>gong-hu-2025-extracting</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.182</doi>
    </paper>
    <paper id="183">
      <title>Unlocking the Planning Capabilities of Large Language Models with Maximum Diversity Fine-tuning</title>
      <author><first>Wenjun</first><last>Li</last></author>
      <author><first>Changyu</first><last>Chen</last></author>
      <author><first>Pradeep</first><last>Varakantham</last></author>
      <pages>3318-3340</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive task-solving capabilities through prompting techniques and system designs, including solving planning tasks (e.g., math proofs, basic travel planning) when sufficient data is available online and used during pre-training. However, for planning tasks with limited prior data (e.g., blocks world, advanced travel planning), the performance of LLMs, including proprietary models like GPT and Gemini, is poor. This paper investigates the impact of fine-tuning on the planning capabilities of LLMs, revealing that LLMs can achieve strong performance in planning through substantial (tens of thousands of specific examples) fine-tuning. Yet, this process incurs high economic, time, and computational costs for each planning problem variation. To address this, we propose Clustering-Based Maximum Diversity Sampling (CMDS), which selects diverse and representative data to enhance sample efficiency and the model’s generalization capability. Extensive evaluations demonstrate that CMDS-<tex-math>l</tex-math>, a baseline method combining CMDS with language embeddings, outperforms random sampling. Furthermore, we introduce a novel algorithm, CMDS-<tex-math>g</tex-math>, which encodes planning task instances with their graph representations into the embedding space. Empirical results show that CMDS-<tex-math>g</tex-math> consistently outperforms baseline methods across various scales and multiple benchmark domains.</abstract>
      <url hash="e880791b">2025.findings-naacl.183</url>
      <bibkey>li-etal-2025-unlocking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.183</doi>
    </paper>
    <paper id="184">
      <title>Continuous Speech Tokenizer in Text To Speech</title>
      <author><first>Yixing</first><last>Li</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent AI Platform</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <pages>3341-3347</pages>
      <abstract>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer.</abstract>
      <url hash="f439dcd4">2025.findings-naacl.184</url>
      <bibkey>li-etal-2025-continuous</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.184</doi>
    </paper>
    <paper id="185">
      <title>Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media</title>
      <author><first>Owen</first><last>Cook</last></author>
      <author><first>Charlie</first><last>Grimshaw</last></author>
      <author><first>Ben Peng</first><last>Wu</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Sophie</first><last>Dillon</last></author>
      <author><first>Jack</first><last>Hicks</last></author>
      <author><first>Luke</first><last>Jones</last></author>
      <author><first>Thomas</first><last>Smith</last></author>
      <author><first>Matyas</first><last>Szert</last></author>
      <author><first>Xingyi</first><last>Song</last><affiliation>University of Sheffield</affiliation></author>
      <pages>3348-3358</pages>
      <abstract>Misinformation spreads rapidly on social media, confusing the truth and targeting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X’s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.</abstract>
      <url hash="0e279bf1">2025.findings-naacl.185</url>
      <bibkey>cook-etal-2025-efficient</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.185</doi>
    </paper>
    <paper id="186">
      <title>Challenges in Trustworthy Human Evaluation of Chatbots</title>
      <author><first>Wenting</first><last>Zhao</last><affiliation>Cornell University</affiliation></author>
      <author><first>Alexander M</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>Tanya</first><last>Goyal</last><affiliation>Cornell University</affiliation></author>
      <pages>3359-3365</pages>
      <abstract>Recently, open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained reputation as trustworthy publicly available benchmarks for LLM performance. While gold standard, it is often tricky to implement the required guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that different source of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high quality human annotations.</abstract>
      <url hash="b957d8bf">2025.findings-naacl.186</url>
      <bibkey>zhao-etal-2025-challenges</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.186</doi>
    </paper>
    <paper id="187">
      <title><fixed-case>RATSD</fixed-case>: Retrieval Augmented Truthfulness Stance Detection from Social Media Posts Toward Factual Claims</title>
      <author><first>Zhengyuan</first><last>Zhu</last></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Haiqi</first><last>Zhang</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>3366-3381</pages>
      <abstract>Social media provides a valuable lens for assessing public perceptions and opinions. This paper focuses on the concept of truthfulness stance, which evaluates whether a textual utterance affirms, disputes, or remains neutral or indifferent toward a factual claim. Our systematic analysis fills a gap in the existing literature by offering the first in-depth conceptual framework encompassing various definitions of stance. We introduce RATSD (Retrieval Augmented Truthfulness Stance Detection), a novel method that leverages large language models (LLMs) with retrieval-augmented generation (RAG) to enhance the contextual understanding of tweets in relation to claims. RATSD is evaluated on TSD-CT, our newly developed dataset containing 3,105 claim-tweet pairs, along with existing benchmark datasets. Our experiment results demonstrate that RATSD outperforms state-of-the-art methods, achieving a significant increase in Macro-F1 score on TSD-CT. Our contributions establish a foundation for advancing research in misinformation analysis and provide valuable tools for understanding public perceptions in digital discourse.</abstract>
      <url hash="c0f1b7ef">2025.findings-naacl.187</url>
      <bibkey>zhu-etal-2025-ratsd</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.187</doi>
    </paper>
    <paper id="188">
      <title><fixed-case>FACT</fixed-case>: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</title>
      <author><first>Jinlin</first><last>Wang</last></author>
      <author><first>Suyuchen</first><last>Wang</last><affiliation>Université de Montréal and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Ziwen</first><last>Xia</last><affiliation>Deepwisdom</affiliation></author>
      <author><first>Sirui</first><last>Hong</last><affiliation>DeepWisdom</affiliation></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Chenglin</first><last>Wu</last><affiliation>DeepWisdom</affiliation></author>
      <pages>3382-3392</pages>
      <abstract>Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel “lost-in-the-middle” phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, we introduce Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. Our findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies.</abstract>
      <url hash="77c578d7">2025.findings-naacl.188</url>
      <bibkey>wang-etal-2025-fact</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.188</doi>
    </paper>
    <paper id="189">
      <title>Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding</title>
      <author><first>Xingjian</first><last>Diao</last></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Weiyi</first><last>Wu</last></author>
      <author><first>Zhongyu</first><last>Ouyang</last></author>
      <author><first>Peijun</first><last>Qing</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Jiang</first><last>Gui</last><affiliation>Dartmouth College</affiliation></author>
      <pages>3393-3409</pages>
      <abstract>Multimodal foundation models (MFMs) have demonstrated significant success in tasks such as visual captioning, question answering, and image-text retrieval. However, these models face inherent limitations due to their finite internal capacity, which restricts their ability to process extended temporal sequences—an essential requirement for comprehensive video and audio analysis. To overcome these challenges, we introduce a specialized cognitive module, temporal working memory (TWM), which aims to enhance the temporal modeling capabilities of MFMs. It selectively retains task-relevant information across temporal dimensions, ensuring that critical details are preserved throughout the processing of video and audio content. The TWM uses a query-guided attention approach to focus on the most informative multimodal segments within temporal sequences. By retaining only the most relevant content, TWM optimizes the use of the model’s limited capacity, enhancing its temporal modeling ability. This plug-and-play module can be easily integrated into existing MFMs. With our TWM, nine state-of-the-art models exhibit significant performance improvements across tasks such as video captioning, question answering, and video-text retrieval. By enhancing temporal modeling, TWM extends the capability of MFMs to handle complex, time-sensitive data effectively. Our code is available at https://github.com/xid32/NAACL_2025_TWM.</abstract>
      <url hash="843d8c72">2025.findings-naacl.189</url>
      <bibkey>diao-etal-2025-temporal</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.189</doi>
    </paper>
    <paper id="190">
      <title>Investigating the Transferability of Code Repair for Low-Resource Programming Languages</title>
      <author><first>Kyle</first><last>Wong</last></author>
      <author><first>Alfonso</first><last>Amayuelas</last></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>3410-3432</pages>
      <abstract>Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent use case is iterative code repair, where an LLM fixes an incorrect program by rationalizing about errors and generating new code. Recent works augment the code repair process by integrating modern techniques such as chain-of-thought reasoning or distillation, but only study their benefits on high-resource languages like Python, and ignore low-resource languages like Perl. To address this gap of knowledge, we investigate the benefits of distilling code repair for both high and low resource languages to determine if the techniques that are effective in a high resource setting are also applicable in a low resource setting. Our evaluation shows that distilling the ability to repair code has language dependent benefits. To explain this behavior, we perform a further analysis and find that contrary to preexisting beliefs, the correlation between reasoning ability and code correction ability is weak. We hypothesize this weak correlation is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair.</abstract>
      <url hash="ca88990f">2025.findings-naacl.190</url>
      <bibkey>wong-etal-2025-investigating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.190</doi>
    </paper>
    <paper id="191">
      <title>Multilingual Blending: Large Language Model Safety Alignment Evaluation with Language Mixture</title>
      <author><first>Jiayang</first><last>Song</last></author>
      <author><first>Yuheng</first><last>Huang</last></author>
      <author><first>Zhehua</first><last>Zhou</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <pages>3433-3449</pages>
      <abstract>As safety remains a crucial concern throughout the development lifecycle of Large Language Models (LLMs), researchers and industrial practitioners have increasingly focused on safeguarding and aligning LLM behaviors with human preferences and ethical standards. LLMs, trained on extensive multilingual corpora, exhibit powerful generalization abilities across diverse languages and domains. However, current safety alignment practices predominantly focus on single-language scenarios, which leaves their effectiveness in complex multilingual contexts, especially for those complex mixed-language formats, largely unexplored. In this study, we introduce Multilingual Blending, a mixed-language query-response scheme designed to evaluate the safety alignment of various state-of-the-art LLMs (e.g., GPT-4o, GPT 3.5, Llama3) under sophisticated, multilingual conditions. We further investigate language patterns such as language availability, morphology, and language family that could impact the effectiveness of Multilingual Blending in compromising the safeguards of LLMs. Our experimental results show that, without meticulously crafted prompt templates, Multilingual Blending significantly amplifies the detriment of malicious queries, leading to dramatically increased bypass rates in LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding those of single-language baselines. Moreover, the performance of Multilingual Blending varies notably based on intrinsic linguistic properties, with languages of different morphology and from diverse families being more prone to evading safety alignments. These findings underscore the necessity of evaluating LLMs and developing corresponding safety alignment strategies in a complex, multilingual context to align with their superior cross-language generalization capabilities.</abstract>
      <url hash="fdb3ee28">2025.findings-naacl.191</url>
      <bibkey>song-etal-2025-multilingual</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.191</doi>
    </paper>
    <paper id="192">
      <title>Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting</title>
      <author><first>Jiarui</first><last>Wu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Zhuo</first><last>Liu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <pages>3450-3468</pages>
      <abstract>Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations.</abstract>
      <url hash="f0d2ad22">2025.findings-naacl.192</url>
      <bibkey>wu-etal-2025-mitigating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.192</doi>
    </paper>
    <paper id="193">
      <title>Concise and Organized Perception Facilitates Reasoning in Large Language Models</title>
      <author><first>Junjie</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shaotian</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhengdong</first><last>Xiao</last></author>
      <author><first>Liang</first><last>Xie</last></author>
      <author><first>Wenxiao</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3469-3498</pages>
      <abstract>Exploiting large language models (LLMs) to tackle reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex logical problems, characterized by plenty of premises within the context and requiring multi-hop reasoning. In particular, the reasoning capabilities of LLMs are brittle to disorder and distractibility. In this work, we first examine the mechanism from the perspective of information flow and reveal that LLMs confront difficulties akin to human-like cognitive biases when dealing with disordered and irrelevant content in reasoning tasks. However, in contrast to LLMs, disordered and irrelevant content does not significantly decrease human performance, as humans have a propensity to distill the most relevant information and systematically organize their thoughts, aiding them in responding to questions.Stem from that, we further propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to identify the most pertinent information while eliminating redundancy efficiently. It then prompts the LLMs in a more organized form that adapts to the model’s inference process. By perceiving concise and organized context, the reasoning abilities of LLMs can be better elicited. Extensive experimental results on several popular logical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark (DI-GSM) show that COP significantly outperforms previous state-of-the-art methods.</abstract>
      <url hash="8e4a9d9c">2025.findings-naacl.193</url>
      <bibkey>liu-etal-2025-concise</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.193</doi>
    </paper>
    <paper id="194">
      <title>Verifiable Format Control for Large Language Model Generations</title>
      <author><first>Zhaoyang</first><last>Wang</last></author>
      <author><first>Jinqi</first><last>Jiang</last></author>
      <author><first>Huichi</first><last>Zhou</last></author>
      <author><first>Wenhao</first><last>Zheng</last></author>
      <author><first>Xuchao</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chetan</first><last>Bansal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>3499-3513</pages>
      <abstract>Recent Large Language Models (LLMs) have demonstrated satisfying general instruction following ability. However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications. Most existing methods focus on benchmarking general instruction following while overlook how to improve the specific format following ability for small LLMs. Besides, these methods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which can introduce the intrinsic bias of LLMs and be costly due to the API calls. In this paper, we first curate a fully verifiable format following dataset VFF. In contrast to existing works often adopting external LLMs for instruction-following validations, every sample of VFF can be easily validated with a Python function. Further, we propose to leverage this verifiable feature to synthesize massive data for progressively training small LLMs, in order to improve their format following abilities. Experimental results highlight the prevalent limitations in the format following capabilities of 7B level open-source LLMs and demonstrate the effectiveness of our method in enhancing this essential ability.</abstract>
      <url hash="92af4c89">2025.findings-naacl.194</url>
      <bibkey>wang-etal-2025-verifiable</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.194</doi>
    </paper>
    <paper id="195">
      <title>Taxonomy and Analysis of Sensitive User Queries in Generative <fixed-case>AI</fixed-case> Search System</title>
      <author><first>Hwiyeol</first><last>Jo</last></author>
      <author><first>Taiwoo</first><last>Park</last></author>
      <author><first>Hyunwoo</first><last>Lee</last></author>
      <author><first>Nayoung</first><last>Choi</last></author>
      <author><first>Changbong</first><last>Kim</last></author>
      <author><first>Ohjoon</first><last>Kwon</last></author>
      <author><first>Donghyeon</first><last>Jeon</last></author>
      <author><first>Eui-Hyeon</first><last>Lee</last></author>
      <author><first>Kyoungho</first><last>Shin</last></author>
      <author><first>Sun Suk</first><last>Lim</last></author>
      <author><first>Kyungmi</first><last>Kim</last></author>
      <author><first>Jihye</first><last>Lee</last></author>
      <author><first>Sun</first><last>Kim</last></author>
      <pages>3514-3529</pages>
      <abstract>Although there has been a growing interest among industries in integrating generative LLMs into their services, limited experience and scarcity of resources act as a barrier in launching and servicing large-scale LLM-based services. In this paper, we share our experiences in developing and operating generative AI models within a national-scale search engine, with a specific focus on the sensitiveness of user queries. We propose a taxonomy for sensitive search queries, outline our approaches, and present a comprehensive analysis report on sensitive queries from actual users. We believe that our experiences in launching generative AI search systems can contribute to reducing the barrier in building generative LLM-based services.</abstract>
      <url hash="0f389f18">2025.findings-naacl.195</url>
      <bibkey>jo-etal-2025-taxonomy</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.195</doi>
    </paper>
    <paper id="196">
      <title><fixed-case>S</fixed-case>yn<fixed-case>G</fixed-case>host: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer</title>
      <author><first>Pengzhou</first><last>Cheng</last></author>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Zongru</first><last>Wu</last></author>
      <author><first>Fengwei</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Libo</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3530-3546</pages>
      <url hash="c35643e3">2025.findings-naacl.196</url>
      <bibkey>cheng-etal-2025-synghost</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.196</doi>
    </paper>
    <paper id="197">
      <title><fixed-case>TestEval</fixed-case>: Benchmarking Large Language Models for Test Case Generation</title>
      <author><first>Wenhan</first><last>Wang</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Chenyuan</first><last>Yang</last></author>
      <author><first>Zhijie</first><last>Wang</last></author>
      <author><first>Yuheng</first><last>Huang</last></author>
      <author><first>Zhaoyang</first><last>Chu</last></author>
      <author><first>Da</first><last>Song</last></author>
      <author><first>Lingming</first><last>Zhang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>An Ran</first><last>Chen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <pages>3547-3562</pages>
      <abstract>For program languages, testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform software testing, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in software testing. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities.In this paper, we propose TestEval, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate 17 popular LLMs, including both commercial and open-source ones, on TestEval. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths.</abstract>
      <url hash="2e9a414a">2025.findings-naacl.197</url>
      <bibkey>wang-etal-2025-testeval</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.197</doi>
    </paper>
    <paper id="198">
      <title>Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models</title>
      <author><first>Siyin</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xingsong</first><last>Ye</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Junwen</first><last>Duan</last></author>
      <author><first>Shimin</first><last>Li</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>3563-3605</pages>
      <abstract>As Artificial General Intelligence (AGI) becomes increasingly integrated into various facets of human life, ensuring the safety and ethical alignment of such systems is paramount. Previous studies primarily focus on single-modality threats, which may not suffice given the integrated and complex nature of cross-modality interactions. We introduce a novel safety alignment challenge called Safe Inputs but Unsafe Output (*SIUO*) to evaluate cross-modality safety alignment. Specifically, it considers cases where single modalities are safe independently but could potentially lead to unsafe or unethical outputs when combined. To empirically investigate this problem, we developed the *SIUO*, a cross-modality benchmark encompassing 9 critical safety domains, such as self-harm, illegal activities, and privacy violations. Our findings reveal substantial safety vulnerabilities in both closed- and open-source LVLMs, such as GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably interpret and respond to complex, real-world scenarios.</abstract>
      <url hash="d24530a3">2025.findings-naacl.198</url>
      <bibkey>wang-etal-2025-safe</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.198</doi>
    </paper>
    <paper id="199">
      <title><fixed-case>FLEX</fixed-case>: A Benchmark for Evaluating Robustness of Fairness in Large Language Models</title>
      <author><first>Dahyun</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>3606-3620</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the manifestation of social biases, which can lead to harmful societal impacts. Despite these concerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs, which can generate biased responses even with simple adversarial instructions. To address this critical gap, we introduce a new benchmark, Fairness Benchmark in LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias. To thoroughly evaluate the robustness of LLMs, we integrate prompts that amplify potential biases into the fairness assessment. Comparative experiments between FLEX and existing benchmarks demonstrate that traditional evaluations may underestimate the inherent risks in models. This highlights the need for more stringent LLM evaluation benchmarks to guarantee safety and fairness.</abstract>
      <url hash="d8d61ad5">2025.findings-naacl.199</url>
      <bibkey>jung-etal-2025-flex</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.199</doi>
    </paper>
    <paper id="200">
      <title>When and How to Augment Your Input: Question Routing Helps Balance the Accuracy and Efficiency of Large Language Models</title>
      <author><first>Shufan</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>He</first><last>Zheng</last></author>
      <author><first>Lei</first><last>Cui</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>3621-3634</pages>
      <abstract>Although large language models rely on parametric knowledge to achieve exceptional performance across various question-answering tasks, they still face challenges when addressing knowledge-based long-tail questions. Augmented generation techniques, such as chain-of-thought prompting and retrieval augmentation, can effectively enhance the ability of these models to answer long-tail questions. However, improving accuracy through augmented generation often results in significant latency within question-answering systems. This paper addresses the issue of “when and how to augment the input” by proposing an adaptive question routing framework. This framework employs a query router to select the most appropriate augmentation path at the right time, thereby enhancing both the accuracy and efficiency of question-answering systems. Extensive comparative experiments on benchmarks such as AmbigNQ, HotpotQA, MMLU-STEM, and PopQA demonstrate that our method surpasses existing approaches in both accuracy and efficiency. Furthermore, this paper introduces two metrics for evaluating adaptive question augmentation methods and presents a new benchmark for adaptive question augmentation, aiming to advance the field.</abstract>
      <url hash="232a6407">2025.findings-naacl.200</url>
      <bibkey>chen-etal-2025-augment</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.200</doi>
    </paper>
    <paper id="201">
      <title><fixed-case>G</fixed-case>ra<fixed-case>PPI</fixed-case>: A Retrieve-Divide-Solve <fixed-case>G</fixed-case>raph<fixed-case>RAG</fixed-case> Framework for Large-scale Protein-protein Interaction Exploration</title>
      <author><first>Ziwen</first><last>Li</last></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Youngseung</first><last>Jeon</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>3635-3648</pages>
      <abstract>Drug discovery (DD) has tremendously contributed to maintaining and improving public health. Hypothesizing that inhibiting protein misfolding can slow disease progression, researchers focus on target identification (Target ID) to find protein structures for drug binding. While Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug discovery, integrating models into cohesive workflows remains challenging. We conducted a user study with drug discovery researchers to identify the applicability of LLMs and RAGs in Target ID. We identified two main findings: 1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on an initial protein and protein candidates that have a therapeutic impact; 2) the model must provide the PPI and relevant explanations for better understanding. Based on these observations, we identified three limitations on previous approaches for Target ID: 1) semantic ambiguity, 2) lack of explainability, and 3) short retrieval units. To address these issues, we propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve agent pipeline RAG framework to support large-scale PPI signaling pathway exploration in understanding therapeutic impacts by decomposing the analysis of entire PPI pathways into sub-tasks focused on the analysis of PPI edges.</abstract>
      <url hash="cd2a0f6e">2025.findings-naacl.201</url>
      <bibkey>li-etal-2025-grappi</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.201</doi>
    </paper>
    <paper id="202">
      <title>From Curiosity to Clarity : Exploring the Impact of Consecutive Why-Questions</title>
      <author><first>Geonyeong</first><last>Son</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Jaeyoung</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Misuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>3649-3664</pages>
      <abstract>Humans attempt to understand the real world by asking the fundamental question ”Why?” when faced with incomprehensible situations in everyday life. Such why-questions provide essential knowledge that can help in understanding these situations. In this study, we conducted an end-to-end process to verify the utility of consecutive why-questions, from constructing a large language model (LLM)-based dataset to performing quantitative evaluation and analysis. Firstly, we created a WHY-Chain dataset, consisting of answers generated by an LLM in response to chain-of-why-questions, including a validity check. We also incorporated objectives that effectively capture the ”consecutive” characteristic of the data. Using the WHY-Chain dataset and two types of self-supervised objectives, we trained the pre-trained model. As a result, the refined model demonstrated improved performance on downstream tasks that require commonsense reasoning. Additionally, we conducted various ablation studies to assess the impact of different factors, confirming the scalability of the proposed approach. Lastly, we confirmed the consistency of the logical information by reasoning chain analysis of the answers generated from consecutive why-questions.</abstract>
      <url hash="011d64b9">2025.findings-naacl.202</url>
      <bibkey>son-etal-2025-curiosity</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.202</doi>
    </paper>
    <paper id="203">
      <title><fixed-case>C</fixed-case>ollab<fixed-case>S</fixed-case>tory: Multi-<fixed-case>LLM</fixed-case> Collaborative Story Generation and Authorship Analysis</title>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Amazon</affiliation></author>
      <author><first>Nafis Irtiza</first><last>Tripto</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>3665-3679</pages>
      <abstract>The rise of unifying frameworks that enable seamless interoperability of Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended tasks a possibility. Despite this, there have not been efforts to explore such collaborative writing. We take the next step beyond human-LLM collaboration to explore this multi-LLM scenario by generating the first exclusively LLM-generated collaborative stories dataset called CollabStory. We focus on single-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs co-author stories. We generate over 32k stories using open-source instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that have set the standard for human-human multi-author writing tasks and analysis. We extend their authorship-related tasks for multi-LLM settings and present baselines for LLM-LLM collaboration. We find that current baselines are not able to handle this emerging scenario. Thus, CollabStory is a resource that could help propel an understanding as well as the development of new techniques to discern the use of multiple LLMs. This is crucial to study in the context of writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing challenges related to plagiarism detection, credit assignment, maintaining academic integrity in educational settings, and addressing copyright infringement concerns. We make our dataset and code available at https://github.com/saranya-venkatraman/CollabStory.</abstract>
      <url hash="60217790">2025.findings-naacl.203</url>
      <bibkey>venkatraman-etal-2025-collabstory</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.203</doi>
    </paper>
    <paper id="204">
      <title><fixed-case>NTSEBENCH</fixed-case>: Cognitive Reasoning Benchmark for Vision Language Models</title>
      <author><first>Pranshu</first><last>Pandya</last></author>
      <author><first>Vatsal</first><last>Gupta</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <author><first>Agney S</first><last>Talwarr</last></author>
      <author><first>Tushar</first><last>Kataria</last><affiliation>University of Utah</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>3680-3708</pages>
      <abstract>Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, large language models (LLMs) and vision language models (VLMs) excel in common-sense reasoning tasks, but still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice questions, accompanied by a total of 4,642 images, spanning 26 categories. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open-source and propriety models, we propose four distinct modeling strategies to handle different modalities—text and images—in the dataset instances.</abstract>
      <url hash="ecc48f09">2025.findings-naacl.204</url>
      <bibkey>pandya-etal-2025-ntsebench</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.204</doi>
    </paper>
    <paper id="205">
      <title><fixed-case>K</fixed-case>now<fixed-case>A</fixed-case>gent: Knowledge-Augmented Planning for <fixed-case>LLM</fixed-case>-Based Agents</title>
      <author><first>Yuqi</first><last>Zhu</last></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Shiwei</first><last>Lyu</last></author>
      <author><first>Yue</first><last>Shen</last><affiliation>antgroup</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3709-3732</pages>
      <abstract>Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.</abstract>
      <url hash="8da6a5a9">2025.findings-naacl.205</url>
      <bibkey>zhu-etal-2025-knowagent</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.205</doi>
    </paper>
    <paper id="206">
      <title><fixed-case>SWITCH</fixed-case>: Studying with Teacher for Knowledge Distillation of Large Language Models</title>
      <author><first>Jahyun</first><last>Koo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yerin</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yongil</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Taegwan</first><last>Kang</last></author>
      <author><first>Hyunkyung</first><last>Bae</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>3733-3746</pages>
      <abstract>Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model compression, with the use of student-generated outputs (SGOs) as training data being particularly notable for reducing the mismatch between training and inference. However, SGOs often produce noisy and biased sequences, which can lead to misguidance from the teacher model, especially in long sequences. To mitigate these challenges, we propose SWITCH (Studying With Teacher for Knowledge Distillation), a novel approach that strategically incorporates the teacher model during the student’s sequence generation. SWITCH identifies discrepancies between the token probabilities of the teacher and student models, allowing the teacher to intervene selectively, particularly in long sequences that are more prone to teacher misguidance. Extensive experimental results across three model families and five instruction-following datasets show that SWITCH surpasses traditional KD methods, particularly excelling in the generation of long sequential data.</abstract>
      <url hash="8e51feb0">2025.findings-naacl.206</url>
      <bibkey>koo-etal-2025-switch</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.206</doi>
    </paper>
    <paper id="207">
      <title><fixed-case>T</fixed-case>hought2<fixed-case>T</fixed-case>ext: Text Generation from <fixed-case>EEG</fixed-case> Signal using Large Language Models (<fixed-case>LLM</fixed-case>s)</title>
      <author><first>Abhijit</first><last>Mishra</last><affiliation>University of Texas at Austin and Apple</affiliation></author>
      <author><first>Shreya</first><last>Shukla</last></author>
      <author><first>Jose</first><last>Torres</last></author>
      <author><first>Jacek</first><last>Gwizdka</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Shounak</first><last>Roychowdhury</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>3747-3759</pages>
      <abstract>Decoding and expressing brain activity in a comprehensible form is a challenging frontier in AI. This paper presents *Thought2Text*, which uses instruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to achieve this goal. The approach involves three stages: (1) training an EEG encoder for visual feature extraction, (2) fine-tuning LLMs on image and text data, enabling multimodal description generation, and (3) further fine-tuning on EEG embeddings to generate text directly from EEG during inference. Experiments on a public EEG dataset collected for six subjects with image stimuli and text captions demonstrate the efficacy of multimodal LLMs (*LLaMA-v3*, *Mistral-v0.3*, *Qwen2.5*), validated using traditional language generation evaluation metrics, as well as *fluency* and *adequacy* measures. This approach marks a significant advancement towards portable, low-cost “thoughts-to-text” technology with potential applications in both neuroscience and natural language processing.</abstract>
      <url hash="75c8bcb0">2025.findings-naacl.207</url>
      <bibkey>mishra-etal-2025-thought2text</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.207</doi>
    </paper>
    <paper id="208">
      <title>A Comprehensive Survey of Contemporary <fixed-case>A</fixed-case>rabic Sentiment Analysis: Methods, Challenges, and Future Directions</title>
      <author><first>Zhiqiang</first><last>Shi</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Ruchit</first><last>Agrawal</last><affiliation>University of Birmingham</affiliation></author>
      <pages>3760-3772</pages>
      <abstract>Sentiment Analysis, a popular subtask of Natural Language Processing, employs computational methods to extract sentiment, opinions, and other subjective aspects from linguistic data. Given its crucial role in understanding human sentiment, research in sentiment analysis has witnessed significant growth in the recent years. However, the majority of approaches are aimed at the English language, and research towards Arabic sentiment analysis remains relatively unexplored. This paper presents a comprehensive and contemporary survey of Arabic Sentiment Analysis, identifies the challenges and limitations of existing literature in this field and presents avenues for future research. We present a systematic review of Arabic sentiment analysis methods, focusing specifically on research utilizing deep learning. We then situate Arabic Sentiment Analysis within the broader context, highlighting research gaps in Arabic sentiment analysis as compared to general sentiment analysis. Finally, we outline the main challenges and promising future directions for research in Arabic sentiment analysis.</abstract>
      <url hash="f5598cef">2025.findings-naacl.208</url>
      <bibkey>shi-agrawal-2025-comprehensive</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.208</doi>
    </paper>
    <paper id="209">
      <title>Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models</title>
      <author><first>Shintaro</first><last>Ozaki</last></author>
      <author><first>Kazuki</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>3773-3809</pages>
      <abstract>As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data.</abstract>
      <url hash="b5da3260">2025.findings-naacl.209</url>
      <bibkey>ozaki-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.209</doi>
    </paper>
    <paper id="210">
      <title>Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis</title>
      <author><first>Yiyi</first><last>Chen</last></author>
      <author><first>Qiongxiu</first><last>Li</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Russa</first><last>Biswas</last><affiliation>Aalborg University, Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>3810-3827</pages>
      <abstract>Language Confusion is a phenomenon where Large Language Models (LLMs) generate text that is neither in the desired language, nor in a contextually appropriate language. This phenomenon presents a critical challenge in text generation by LLMs, often appearing as erratic and unpredictable behavior. We hypothesize that there are linguistic regularities to this inherent vulnerability in LLMs and shed light on patterns of language confusion across LLMs. We introduce a novel metric, Language Confusion Entropy, designed to directly measure and quantify this confusion, based on language distributions informed by linguistic typology and lexical variation. Comprehensive comparisons with the Language Confusion Benchmark (Marchisio et al., 2024) confirm the effectiveness of our metric, revealing patterns of language confusion across LLMs. We further link language confusion to LLM security, and find patterns in the case of multilingual embedding inversion attacks. Our analysis demonstrates that linguistic typology offers theoretically grounded interpretation, and valuable insights into leveraging language similarities as a prior for LLM alignment and security.</abstract>
      <url hash="a7ec0267">2025.findings-naacl.210</url>
      <bibkey>chen-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.210</doi>
    </paper>
    <paper id="211">
      <title>Huatuo-26<fixed-case>M</fixed-case>, a Large-scale <fixed-case>C</fixed-case>hinese Medical <fixed-case>QA</fixed-case> Dataset</title>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Shunian</first><last>Chen</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Yuxuan</first><last>Zhu</last></author>
      <author><first>Xiangbo</first><last>Wu</last><affiliation>SRIBD</affiliation></author>
      <author><first>Zhiyi</first><last>Zhang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiaolong</first><last>Xu</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>3828-3848</pages>
      <abstract>Large Language Models infuse newfound vigor into the advancement of the medical domain, yet the scarcity of data poses a significant bottleneck hindering community progress. In this paper, we release the largest ever medical Question Answering (QA) dataset with 26 Million QA pairs named Huatuo-26M. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. We also experimentally show the benefit of the proposed dataset in many aspects: (i) it serves as a fine-tuning data for training medical Large Language Models (LLMs); (ii) it works as an external knowledge source for retrieval-augmented generation (RAG); (iii) it demonstrates transferability by enhancing zero-shot performance on other QA datasets; and (iv) it aids in training biomedical model as a pre-training corpus. Our empirical findings substantiate the dataset’s utility in these domains, thereby confirming its significance as a resource in the medical QA landscape.</abstract>
      <url hash="3fcb3f8a">2025.findings-naacl.211</url>
      <bibkey>wang-etal-2025-huatuo</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.211</doi>
    </paper>
    <paper id="212">
      <title><fixed-case>SEP</fixed-case>-<fixed-case>MLDC</fixed-case>: A Simple and Effective Paradigm for Multi-Label Document Classification</title>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Shuqin</first><last>Li</last></author>
      <author><first>Xiaotong</first><last>Zhang</last></author>
      <author><first>Yuanyuan</first><last>Wang</last></author>
      <author><first>Feng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Hongyang</first><last>Chen</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>3849-3859</pages>
      <abstract>Multi-label document classification (MLDC) aims to allocate more than one label to each document and attracts increasing attention in many practical applications. However, previous studies have failed to pay sufficient attention to the lack of semantic information on labels and the long-tail problem prevalent in the datasets. Additionally, most existing methods focus on optimizing document features, overlooking the potential of high-quality label features to enhance classification performance. In this paper, we propose a simple and effective paradigm for MLDC. Regarding the problem of insufficient label information and imbalance in the sample size of categories, we utilize large language models (LLMs) to semantically expand the label content and generate pseudo-samples for the tail categories. To optimize the features of both documents and labels, we design the contrastive learning boosted feature optimization module facilitated by the similarity matrices. Finally, we construct a label-guided feature selection module to incorporate the optimized label features into the input features to provide richer semantic information for the classifier. Extensive experiments have demonstrated that our proposed method significantly outperforms state-of-the-art baselines.</abstract>
      <url hash="0a28a900">2025.findings-naacl.212</url>
      <bibkey>liu-etal-2025-sep</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.212</doi>
    </paper>
    <paper id="213">
      <title>Improving Pre-trained Language Models with Knowledge Enhancement and Filtering Framework</title>
      <author><first>Qi</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tian</first><last>Xie</last></author>
      <author><first>Haiyue</first><last>Zhang</last></author>
      <author><first>Hongyu</first><last>Yang</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <pages>3860-3871</pages>
      <abstract>Pre-trained language models (PLMs) are widely used in NLP but struggle with capturing entity knowledge. To address this, knowledge enhancement techniques have been proposed. However, existing methods rely heavily on external knowledge bases embedding and often introduce noisy entity representations. In this work, we propose a novel **K**nowledge **E**nhancement **F**iltering **F**ramework named KEFF, which contains both knowledge enhancement and knowledge enhancement filtering modules for PLM. We find that there are certain redundant bits in the embedding space of PLMs. Building on this insight, we implement knowledge-enhanced mapping of redundant bit values in entity span tokens. In order to solve the knowledge enhancement problem of existing methods that introduce noisy entity representation knowledge, we further propose a novel knowledge enhancement filter based on our knowledge enhancement method. Finally, experiments on four knowledge-driven NLP tasks show that our method effectively improves the ability of PLMs on downstream tasks. Compared to state-of-the-art approachs, our method achieves the highest F1-score and accuracy, while reducing the computational cost by 1.7-2.5x.</abstract>
      <url hash="584ea5c9">2025.findings-naacl.213</url>
      <bibkey>zhao-etal-2025-improving</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.213</doi>
    </paper>
    <paper id="214">
      <title>Using Review Combination and Pseudo-Tokens for Aspect Sentiment Quad Prediction</title>
      <author><first>Jiazhou</first><last>Chen</last></author>
      <author><first>Xu</first><last>Jia</last><affiliation>Hebei Normal University</affiliation></author>
      <author><first>RuiQiang</first><last>Guo</last></author>
      <pages>3872-3883</pages>
      <abstract>Aspect Sentiment Quad Prediction (ASQP) aims to identify quadruples consisting of an aspect term, aspect category, opinion term, and sentiment polarity from a given sentence, which is the most representative and challenging task in aspect-based sentiment analysis. A major challenge arises when implicit sentiment is present, as existing models often confuse implicit and explicit sentiment, making it difficult to extract the quadruples effectively. To tackle this issue, we propose a framework that leverages distinct labeled features from diverse reviews and incorporates pseudo-token prompts to harness the semantic knowledge of pre-trained models, effectively capturing both implicit and explicit sentiment expressions. Our approach begins by categorizing reviews based on the presence of implicit sentiment elements. We then build new samples that combine those with implicit sentiment and those with explicit sentiment. Next, we employ prompts with pseudo-tokens to guide the model in distinguishing between implicit and explicit sentiment expressions. Extensive experimental results show that our proposed method enhances the model’s ability across four public datasets, averaging 1.99% F1 improvement, particularly in instances involving implicit sentiment. We release our code at https://github.com/chienarmor/absa-implicit.</abstract>
      <url hash="e74fc37e">2025.findings-naacl.214</url>
      <bibkey>chen-etal-2025-using</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.214</doi>
    </paper>
    <paper id="215">
      <title><fixed-case>DDGIP</fixed-case>: Radiology Report Generation Through Disease Description Graph and Informed Prompting</title>
      <author><first>Chentao</first><last>Huang</last></author>
      <author><first>Guangli</first><last>Li</last><affiliation>East China Jiao Tong University</affiliation></author>
      <author><first>Xinjiong</first><last>Zhou</last></author>
      <author><first>Yafeng</first><last>Ren</last></author>
      <author><first>Hongbin</first><last>Zhang</last><affiliation>East China Jiao Tong University</affiliation></author>
      <pages>3884-3894</pages>
      <abstract>Automatic radiology report generation has attracted considerable attention with the rise of computer-aided diagnostic systems. Due to the inherent biases in medical imaging data, generating reports with precise clinical details is challenging yet crucial for accurate diagnosis. To this end, we design a disease description graph that encapsulates comprehensive and pertinent disease information. By aligning visual features with the graph, our model enhances the quality of the generated reports. Furthermore, we introduce a novel informed prompting method which increases the accuracy of short-gram predictions, acting as an implicit bag-of-words planning for surface realization. Notably, this informed prompt succeeds with a three-layer decoder, reducing the reliance on conventional prompting methods that require extensive model parameters. Extensive experiments on two widely-used datasets, IU-Xray and MIMIC-CXR, demonstrate that our method outperforms previous state-of-the-art models.</abstract>
      <url hash="d6eb2864">2025.findings-naacl.215</url>
      <bibkey>huang-etal-2025-ddgip</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.215</doi>
    </paper>
    <paper id="216">
      <title>Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding</title>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Sangjin</first><last>Choi</last></author>
      <author><first>Taeho</first><last>Hwang</last></author>
      <author><first>Jeongyeon</first><last>Seo</last></author>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Huije</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Youngjin</first><last>Kwon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>3895-3911</pages>
      <abstract>Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.</abstract>
      <url hash="990f660f">2025.findings-naacl.216</url>
      <bibkey>cho-etal-2025-lossless</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.216</doi>
    </paper>
    <paper id="217">
      <title>Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models</title>
      <author><first>Jialiang</first><last>Wu</last></author>
      <author><first>Yi</first><last>Shen</last><affiliation>China Mobile Communications Group Co.,Ltd</affiliation></author>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Yi</first><last>Tang</last></author>
      <author><first>Sen</first><last>Song</last></author>
      <author><first>Xiaoyi</first><last>Wang</last><affiliation>Beijing Wispirit Technology</affiliation></author>
      <author><first>Longjun</first><last>Cai</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3912-3921</pages>
      <abstract>Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the correlation between hidden-state prediction changes and output factuality into a deeper, token-wise level. Based on the insights , we propose cross-layer Entropy eNhanced Decoding (END), a decoding method that mitigates hallucinations without requiring extra training. END leverages inner probability changes across layers to individually quantify the factual knowledge required for each candidate token, and adjusts the final predicting distribution to prioritize tokens with higher factuality. Experiments on both hallucination and QA benchmarks demonstrate that END significantly enhances the truthfulness and informativeness of generation while maintaining robust QA accuracy. Moreover, our work provides a deeper perspective of understanding the correlations between inherent knowledge and output factuality.</abstract>
      <url hash="b00a9005">2025.findings-naacl.217</url>
      <bibkey>wu-etal-2025-improve-decoding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.217</doi>
    </paper>
    <paper id="218">
      <title><fixed-case>TE</fixed-case>a<fixed-case>R</fixed-case>: Improving <fixed-case>LLM</fixed-case>-based Machine Translation with Systematic Self-Refinement</title>
      <author><first>Zhaopeng</first><last>Feng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Bei</first><last>Wu</last></author>
      <author><first>Jiayu</first><last>Liao</last><affiliation>Tencent NLP Speech</affiliation></author>
      <author><first>Wenqiang</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Jun</first><last>Lang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3922-3938</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, human evaluations reveal that LLM-generated translations still contain various errors. Notably, feeding the error information back into the LLMs can facilitate self-refinement, leading to enhanced translation quality. Motivated by these findings, we introduce TEaR (Translate, Estimate, and Refine), a systematic LLM-based self-refinement framework aimed at bootstrapping translation performance. Our key results show that: 1) TEaR framework enables LLMs to improve their translation quality relying solely on self-feedback, measured by both automatic metrics and Multidimensional Quality Metrics (MQM) scores; 2) TEaR autonomously selects improvements, ensuring a robust translation quality baseline while outperforming both internal refinement and external feedback methods. Error analysis and iterative refinement experiments show its ability to continuously reduce translation errors and enhance overall translation quality. Our code and data are publicly available at https://github.com/fzp0424/self_correct_mt.</abstract>
      <url hash="88a9a00b">2025.findings-naacl.218</url>
      <bibkey>feng-etal-2025-tear</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.218</doi>
    </paper>
    <paper id="219">
      <title>Vulnerability of Large Language Models to Output Prefix Jailbreaks: Impact of Positions on Safety</title>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>3939-3952</pages>
      <abstract>Previous research on jailbreak attacks has mainly focused on optimizing the adversarial snippet content injected into input prompts to expose LLM security vulnerabilities. A significant portion of this research focuses on developing more complex, less readable adversarial snippets that can achieve higher attack success rates. In contrast to this trend, our research investigates the impact of the adversarial snippet’s position on the effectiveness of jailbreak attacks. We find that placing a simple and readable adversarial snippet at the beginning of the output effectively exposes LLM safety vulnerabilities, leading to much higher attack success rates than the input suffix attack or prompt-based output jailbreaks. Precisely speaking, we discover that directly enforcing the user’s target embedded output prefix is an effective method to expose LLMs’ safety vulnerabilities.</abstract>
      <url hash="19bc7d7f">2025.findings-naacl.219</url>
      <bibkey>wang-etal-2025-vulnerability</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.219</doi>
    </paper>
    <paper id="220">
      <title><fixed-case>I</fixed-case>ma<fixed-case>RA</fixed-case>: An Imaginative Frame Augmented Method for Low-Resource Multimodal Metaphor Detection and Explanation</title>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Minzheng</first><last>Wang</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>3953-3967</pages>
      <abstract>Multimodal metaphor detection is an important and challenging task in multimedia computing, which aims to distinguish between metaphorical and literal multimodal expressions. Existing studies mainly utilize typical multimodal computing approaches for detection, neglecting the unique cross-domain and cross-modality characteristics underlying multimodal metaphor understanding. According to Conceptual Metaphor Theory (CMT), the inconsistency between source and target domains and their attribute similarity are essential to infer the intricate meanings implied in metaphors. In practice, the scarcity of the annotated multimodal metaphorical contents in the real world brings additional difficulty to the detection task and further complicates the understanding of multimodal metaphors. To address the above challenges, in this paper, we propose a novel Imaginative FRame Augmented (ImaRA) method for low-resource multimodal metaphor detection and explanation inspired by CMT. Specifically, we first identify imaginative frame as an associative structure to stimulate the imaginative thinking of multimodal metaphor detection and understanding. We then construct a cross-modal imagination dataset rich in multimodal metaphors and corresponding imaginative frames, and retrieve an augmented instance from this imagination dataset using imaginative frames mined from the input. This augmented instance serves as the demonstration exemplar to boost the metaphor reasoning ability of the multimodal large language model (MLLM) in low-resource multimodal scenarios. Experiments on two publicly available datasets show that our method consistently achieves robust results compared to MLLM-based methods for both multimodal metaphor detection and explanation in low-resource scenarios and meanwhile surpasses existing multimodal metaphor detection methods with full training data.</abstract>
      <url hash="3b8875fd">2025.findings-naacl.220</url>
      <bibkey>tian-etal-2025-imara</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.220</doi>
    </paper>
    <paper id="221">
      <title><fixed-case>XAMPLER</fixed-case>: Learning to Retrieve Cross-Lingual In-Context Examples</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>3968-3977</pages>
      <abstract>Recent studies indicate that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving relevant in-context examples tailored to the input query, enhances few-shot in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, poses challenges due to the scarcity of cross-lingual retrievers and annotated data. Thus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever based on Glot500, a multilingual small language model, using positive and negative English examples constructed from the predictions of a multilingual large language model, i.e., MaLA500. Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on two multilingual text classification benchmarks, namely SIB200 with 176 languages and MasakhaNEWS with 16 languages, demonstrate that XAMPLER substantially improves the in-context learning performance across languages.</abstract>
      <url hash="3d4d7436">2025.findings-naacl.221</url>
      <bibkey>lin-etal-2025-xampler</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.221</doi>
    </paper>
    <paper id="222">
      <title>Evaluating Cultural and Social Awareness of <fixed-case>LLM</fixed-case> Web Agents</title>
      <author><first>Haoyi</first><last>Qiu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Divyansh</first><last>Agarwal</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Sarah</first><last>Tan</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>3978-4005</pages>
      <abstract>As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents’ sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents’ ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages – fine-tuning on culture-specific datasets significantly enhances the agents’ ability to generalize across different regions, while prompting boosts the agents’ ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents’ cultural and social awareness during the development cycle.</abstract>
      <url hash="84538ddf">2025.findings-naacl.222</url>
      <bibkey>qiu-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.222</doi>
    </paper>
    <paper id="223">
      <title><fixed-case>GRAIT</fixed-case>: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation</title>
      <author><first>Runchuan</first><last>Zhu</last></author>
      <author><first>Zinco</first><last>Jiang</last></author>
      <author><first>Jiang</first><last>Wu</last></author>
      <author><first>Zhipeng</first><last>Ma</last></author>
      <author><first>Jiahe</first><last>Song</last></author>
      <author><first>Fengshuo</first><last>Bai</last></author>
      <author><first>Dahua</first><last>Lin</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Conghui</first><last>He</last></author>
      <pages>4006-4021</pages>
      <abstract>Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models (LLMs) by improving their ability to refuse responses to questions beyond their knowledge, thereby reducing hallucinations and improving reliability. Effective RAIT must address two key challenges: firstly, effectively reject unknown questions to minimize hallucinations; secondly, avoid over-refusal to ensure questions that can be correctly answered are not rejected, thereby maintain the helpfulness of LLM outputs. In this paper, we address the two challenges by deriving insightful observations from the gradient-based perspective, and proposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT: (1) employs gradient-driven sample selection to effectively minimize hallucinations and (2) introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal, achieving the balance between accurate refusals and maintaining useful responses. Experimental evaluations on open-ended and multiple-choice question answering tasks demonstrate that GRAIT significantly outperforms existing RAIT methods in the overall performance. The source code and data will be available at https://github.com/opendatalab/GRAIT .</abstract>
      <url hash="734399c1">2025.findings-naacl.223</url>
      <bibkey>zhu-etal-2025-grait</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.223</doi>
    </paper>
    <paper id="224">
      <title>Entity Pair-guided Relation Summarization and Retrieval in <fixed-case>LLM</fixed-case>s for Document-level Relation Extraction</title>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hongsen</first><last>Yu</last></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Huangming</first><last>Xu</last></author>
      <pages>4022-4037</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract relations between entities in a document. While previous research has primarily focused on traditional small models, recent studies have extended the scope to large language models (LLMs). Current LLM-based methods typically focus on filtering all potential relations (candidate relations) within a document at one time and then performing triplet fact extraction. However, most approaches for candidate relation filtering are based on the document level, which results in insufficient correlation between candidate relations and entity pairs. In addition, the data imbalance problem caused by a large amount of no-relation data (NA problem) is another important reason for the suboptimal performance of LLM-based methods. To address these issues, we propose an entity pair-guided relation summarization and retrieval model (EP-RSR) for DocRE, which introduces an innovative LLM-based document-level relation extraction paradigm, EPRF (Entity Pair-Relation-Fact), along with an entity pair-level candidate relation filtering method. Our approach first selects entity pairs that potentially contain relations and uses them to guide relation summarization and retrieval for extracting relation facts. This enhances the relevance between candidate relations and entity pairs while alleviating the issue of imbalanced NA data. Benchmark testing on three datasets demonstrates that our approach achieves state-of-the-art (SOTA) performance for LLM-based models. Our code is available at https://github.com/LookingYu/EP-RSR.</abstract>
      <url hash="0fbe2769">2025.findings-naacl.224</url>
      <bibkey>zhang-etal-2025-entity</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.224</doi>
    </paper>
    <paper id="225">
      <title>A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>4038-4050</pages>
      <abstract>Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models, improving performance in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpora quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for effectively exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus with just 10K parallel sentences can yield results comparable to those obtained from much larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios.</abstract>
      <url hash="20ecb65b">2025.findings-naacl.225</url>
      <bibkey>lin-etal-2025-recipe</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.225</doi>
    </paper>
    <paper id="226">
      <title>Omni-Chart-600<fixed-case>K</fixed-case>: A Comprehensive Dataset of Chart Types for Chart Understanding</title>
      <author><first>Shulei</first><last>Wang</last></author>
      <author><first>Shuai</first><last>Yang</last></author>
      <author><first>Wang</first><last>Lin</last></author>
      <author><first>Zirun</first><last>Guo</last></author>
      <author><first>Sihang</first><last>Cai</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4051-4069</pages>
      <abstract>To address the deficiencies in chart types and the limited scope of chart tasks in existing datasets, we conducted a comprehensive review of current data collection methodologies. By integrating manual annotation with data generation leveraging GPT-4, we developed a dataset that includes 21 diverse chart types and a broad spectrum of tasks, such as data retrieval and mathematical reasoning. Our analysis of existing models revealed that capabilities in information extraction, mathematical reasoning, and understanding of multiple chart types are essential for performing a variety of chart tasks. To overcome the limitations in these areas, we devised a two-stage training strategy and a method for jointly training the vision encoder tailored for multi-type charts. In the first stage, we designed several tasks to enhance the model’s general understanding of charts, aligning multimodal large models pre-trained on natural images to chart tasks. To further improve the model’s capability to understand various chart tasks and enhance its reasoning abilities, we employed Chain-of-Thought data for training in the second stage. Through two-stage training on our proposed dataset, the pre-trained multimodal large language model achieved state-of-the-art performance across multiple chart understanding tasks, demonstrating the superiority of our data and methods.</abstract>
      <url hash="02a25420">2025.findings-naacl.226</url>
      <bibkey>wang-etal-2025-omni</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.226</doi>
    </paper>
    <paper id="227">
      <title>Comprehensive Layer-wise Analysis of <fixed-case>SSL</fixed-case> Models for Audio Deepfake Detection</title>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author><first>Younes</first><last>Samih</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Suraj</first><last>Maharjan</last><affiliation>Amazon</affiliation></author>
      <author><first>Tim</first><last>Polzehl</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>4070-4082</pages>
      <abstract>This paper conducts a comprehensive layer-wise analysis of self-supervised learning (SSL) models for audio deepfake detection across diverse contexts, including multilingual datasets (English, Chinese, Spanish), partial, song, and scene-based deepfake scenarios. By systematically evaluating the contributions of different transformer layers, we uncover critical insights into model behavior and performance. Our findings reveal that lower layers consistently provide the most discriminative features, while higher layers capture less relevant information. Notably, all models achieve competitive equal error rate (EER) scores even when employing a reduced number of layers. This indicates that we can reduce computational costs and increase the inference speed of detecting deepfakes by utilizing only a few lower layers. This work enhances our understanding of SSL models in deepfake detection, offering valuable insights applicable across varied linguistic and contextual settings. Our models and code are publicly available at https://github.com/Yaselley/SSL_Layerwise_Deepfake.</abstract>
      <url hash="d24d955b">2025.findings-naacl.227</url>
      <bibkey>el-kheir-etal-2025-comprehensive</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.227</doi>
    </paper>
    <paper id="228">
      <title>Attention on Multiword Expressions: A Multilingual Study of <fixed-case>BERT</fixed-case>-based Models with Regard to Idiomaticity and Microsyntax</title>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Vitalii</first><last>Hirak</last></author>
      <author><first>Badr M.</first><last>Abdullah</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Bernd</first><last>Möbius</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>4083-4092</pages>
      <abstract>This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages — English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.</abstract>
      <url hash="4571c784">2025.findings-naacl.228</url>
      <bibkey>zaitova-etal-2025-attention</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.228</doi>
    </paper>
    <paper id="229">
      <title>Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios</title>
      <author><first>Jiwei</first><last>Tang</last></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Tingwei</first><last>Lu</last></author>
      <author><first>Zhicheng</first><last>Zhang</last></author>
      <author><first>YimingZhao</first><last>YimingZhao</last></author>
      <author><first>LinHai</first><last>LinHai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hai-Tao</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>4093-4108</pages>
      <abstract>Large language models (LLMs) demonstrate exceptional capabilities in various scenarios. However, they suffer from much redundant information and are sensitive to the position of key information in long context scenarios. To address these challenges, we present Perception Compressor, a training-free prompt compression framework. It includes a perception retriever that leverages guiding questions and instruction to retrieve the most relevant demonstrations, a dual-slope ratio allocator to dynamically allocate compression ratios and open-book ratios, and a semi-guided iterative compression that retains key information at the token level while removing tokens that distract the LLM. We conduct extensive experiments on long context benchmarks, i.e., NaturalQuestions, LongBench, and MuSiQue. Experiment results show that Perception Compressor outperforms existing methods by a large margin, achieving state-of-the-art performance.</abstract>
      <url hash="cb4a6115">2025.findings-naacl.229</url>
      <bibkey>tang-etal-2025-perception</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.229</doi>
    </paper>
    <paper id="230">
      <title><fixed-case>M</fixed-case>ojo<fixed-case>B</fixed-case>ench: Language Modeling and Benchmarks for Mojo</title>
      <author><first>Nishat</first><last>Raihan</last></author>
      <author><first>Joanna C. S.</first><last>Santos</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>4109-4128</pages>
      <abstract>The recently introduced Mojo programming language (PL) by Modular, has received significant attention in the scientific community due to its claimed significant speed boost over Python. Despite advancements in code Large Language Models (LLMs) across various PLs, Mojo remains unexplored in this context. To address this gap, we introduce MojoBench, the first framework for Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM pretrained and finetuned for Mojo code generation, which supports instructions in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a 30-35% performance improvement over leading models like GPT-4o and Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with underrepresented and unseen PLs, offering potential strategies for enhancing model adaptability. MojoBench contributes to our understanding of LLM capabilities and limitations in emerging programming paradigms fostering more robust code generation systems.</abstract>
      <url hash="f2b35935">2025.findings-naacl.230</url>
      <bibkey>raihan-etal-2025-mojobench</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.230</doi>
    </paper>
    <paper id="231">
      <title><fixed-case>VL</fixed-case>ind-Bench: Measuring Language Priors in Large Vision-Language Models</title>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Minsung</first><last>Kim</last></author>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyukhun</first><last>Koh</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>4129-4144</pages>
      <abstract>Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks. However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information. Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution. Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied. Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors. To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs. It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as commonsense knowledge, visual perception, and commonsense biases. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors, presenting a strong challenge in the field.</abstract>
      <url hash="c82adec3">2025.findings-naacl.231</url>
      <bibkey>lee-etal-2025-vlind</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.231</doi>
    </paper>
    <paper id="232">
      <title><fixed-case>GRAG</fixed-case>: Graph Retrieval-Augmented Generation</title>
      <author><first>Yuntong</first><last>Hu</last><affiliation>Emory University</affiliation></author>
      <author><first>Zhihan</first><last>Lei</last><affiliation>Emory University</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Pan</last></author>
      <author><first>Chen</first><last>Ling</last></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>Emory University</affiliation></author>
      <pages>4145-4157</pages>
      <abstract>Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views—the text view and the graph view—enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods. Our datasets as well as codes of GRAG are available at https://github.com/HuieL/GRAG.</abstract>
      <url hash="5ffdcb23">2025.findings-naacl.232</url>
      <bibkey>hu-etal-2025-grag</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.232</doi>
    </paper>
    <paper id="233">
      <title>Sequence-level Large Language Model Training with Contrastive Preference Optimization</title>
      <author><first>Zhili</first><last>Feng</last></author>
      <author><first>Dhananjay</first><last>Ram</last><affiliation>Amazon</affiliation></author>
      <author><first>Cole</first><last>Hawkins</last><affiliation>Amazon</affiliation></author>
      <author><first>Aditya</first><last>Rawal</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinman</first><last>Zhao</last><affiliation>Morph Technologies</affiliation></author>
      <author><first>Sheng</first><last>Zha</last><affiliation>Amazon</affiliation></author>
      <pages>4158-4164</pages>
      <abstract>The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.</abstract>
      <url hash="c0baf723">2025.findings-naacl.233</url>
      <bibkey>feng-etal-2025-sequence</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.233</doi>
    </paper>
    <paper id="234">
      <title>Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models</title>
      <author><first>Haritz</first><last>Puerto</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Martin</first><last>Gubri</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Seong Joon</first><last>Oh</last><affiliation>Parameter Lab and Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>4165-4182</pages>
      <abstract>Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable “cheating.” In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable document- and dataset-level MIA. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.</abstract>
      <url hash="1674f156">2025.findings-naacl.234</url>
      <bibkey>puerto-etal-2025-scaling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.234</doi>
    </paper>
    <paper id="235">
      <title>Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding</title>
      <author><first>Kyungmin</first><last>Min</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>4183-4198</pages>
      <abstract>Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in generating detailed and coherent responses from visual inputs.However, they are prone to generate hallucinations due to an over-reliance on language priors. To address this issue, we investigate the language priors in LVLMs and make two key observations: (1) Even when predicting the tokens associated with image-related part-of-speech (POS), models increasingly rely on linguistic priors as the token sequences grow, thereby amplifying hallucinations. (2) Methods that directly calibrate LVLM’s output distribution to mitigate language priors can lead to a degradation in text quality or even exacerbate hallucinations.Based on these findings, we propose a novel method, <b>Sum</b>mary-<b>G</b>uided <b>D</b>ecoding <b>(SumGD)</b>. This method naturally encourages the model to focus more on image information by reducing the text context through summaries, while controlling only the image-related POS tokens to maintain text quality.Through experiments, we demonstrate that SumGD achieves state-of-the-art performance on object hallucination benchmarks. Furthermore, in terms of the trade-off between precision and recall, SumGD achieves Pareto optimality among the existing methods.Lastly, we observe that although existing methods struggle to balance the reduction of object hallucinations with maintaining text quality, SumGD demonstrates robustness in handling this challenge.</abstract>
      <url hash="c421dbb2">2025.findings-naacl.235</url>
      <bibkey>min-etal-2025-mitigating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.235</doi>
    </paper>
    <paper id="236">
      <title>Exploring Hybrid Sampling Inference for Aspect-based Sentiment Analysis</title>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <author><first>Minjie</first><last>Qiang</last></author>
      <author><first>Jinghang</first><last>Gu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>4199-4210</pages>
      <abstract>As the training of large language models (LLMs) will encounter high computational costs, massive works are now focusing on inference. Their methods can be generally summarised as re-sampling the target multiple times and performing a vote upon the outputs. Despite bringing significant performance improvements, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple yet efficient inference strategies named __Hybrid Sampling__ that combining both multiple and single sampling to greatly reduce the cost of multiple sampling without sacrificing performance. __Hybrid Sampling__ could dynamically choose the essential part of generated sequence for multiple sampling and proceed the rest with single sampling, achieving a performance-cost balance. Extensive experiments in several benchmarks underscore the robustness and effectiveness of our proposed Hybrid Sampling and more importantly, it is much faster.</abstract>
      <url hash="93cad0c5">2025.findings-naacl.236</url>
      <bibkey>bao-etal-2025-exploring</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.236</doi>
    </paper>
    <paper id="237">
      <title><fixed-case>F</fixed-case>e<fixed-case>RG</fixed-case>-<fixed-case>LLM</fixed-case> : Feature Engineering by Reason Generation Large Language Models</title>
      <author><first>Jeonghyun</first><last>Ko</last></author>
      <author><first>Gyeongyun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Donghoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Kyunam</first><last>Lee</last><affiliation>SK Telecom</affiliation></author>
      <pages>4211-4228</pages>
      <url hash="be03c9c0">2025.findings-naacl.237</url>
      <bibkey>ko-etal-2025-ferg</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.237</doi>
    </paper>
    <paper id="238">
      <title>Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Abdellah</first><last>El Mekki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4229-4256</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performance on a wide range of natural language processing (NLP) tasks, primarily through in-context learning (ICL). In ICL, the LLM is provided with examples that represent a given task such that it learns to generate answers for test inputs. However, access to these in-context examples is not guaranteed especially for low-resource or massively multilingual tasks. In this work, we propose an unsupervised approach to mine in-context examples for machine translation (MT), enabling unsupervised MT (UMT) across different languages. Our approach begins with word-level mining to acquire word translations that are then used to perform sentence-level mining. As the quality of mined parallel pairs may not be optimal due to noise or mistakes, we introduce a filtering criterion to select the optimal in-context examples from a pool of unsupervised parallel sentences. We evaluate our approach using two multilingual LLMs on 288 directions from the FLORES-200 dataset (CITATION) and analyze the impact of various linguistic features on performance. Our findings demonstrate the effectiveness of our unsupervised approach in mining in-context examples for MT, leading to better or comparable translation performance as translation with regular in-context samples (extracted from human-annotated data), while also outperforming the other state-of-the-art UMT methods by an average of 7 BLEU points.</abstract>
      <url hash="544b0cda">2025.findings-naacl.238</url>
      <bibkey>el-mekki-abdul-mageed-2025-effective</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.238</doi>
    </paper>
    <paper id="239">
      <title><fixed-case>GPT</fixed-case>-<fixed-case>NER</fixed-case>: Named Entity Recognition via Large Language Models</title>
      <author><first>Shuhe</first><last>Wang</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Xiaoya</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Rongbin</first><last>Ouyang</last><affiliation>Peking University, Peking Univserity</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tianwei</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jiwei</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Guo</last></author>
      <pages>4257-4275</pages>
      <abstract>Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text “Columbus is a city” is transformed to generate the text sequence "@@Columbus## is a city”, where special tokens @@## marks the entity to extract. To efficiently address the <i>hallucination</i> issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.</abstract>
      <url hash="d778201a">2025.findings-naacl.239</url>
      <bibkey>wang-etal-2025-gpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.239</doi>
    </paper>
    <paper id="240">
      <title><fixed-case>QP</fixed-case>runer: Probabilistic Decision Quantization for Structured Pruning in Large Language Models</title>
      <author><first>Changhai</first><last>Zhou</last></author>
      <author><first>Yuhua</first><last>Zhou</last></author>
      <author><first>Yibin</first><last>Wang</last></author>
      <author><first>Shijie</first><last>Han</last></author>
      <author><first>Qian</first><last>Qiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Hongguang</first><last>Li</last><affiliation>JF SmartInvest Holdings</affiliation></author>
      <pages>4276-4286</pages>
      <abstract>The rise of large language models (LLMs) has significantly advanced various natural language processing (NLP) tasks. However, the resource demands of these models pose substantial challenges. Structured pruning is an effective approach to reducing model size, but it often results in significant accuracy degradation, necessitating parameter updates to adapt. Unfortunately, such fine-tuning requires substantial memory, which limits its applicability. To address these challenges, we introduce quantization into the structured pruning framework to reduce memory consumption during both fine-tuning and inference. However, the combined errors from pruning and quantization increase the difficulty of fine-tuning, requiring a more refined quantization scheme. To this end, we propose QPruner, a novel framework that employs structured pruning to reduce model size, followed by a layer-wise mixed-precision quantization scheme. Quantization precisions are assigned to each layer based on their importance to the target task, and Bayesian optimization is employed to refine precision allocation strategies, ensuring a balance between model accuracy and memory efficiency. Extensive experiments on benchmark datasets demonstrate that QPruner significantly outperforms existing methods in memory savings while maintaining or improving model performance.</abstract>
      <url hash="95b78558">2025.findings-naacl.240</url>
      <bibkey>zhou-etal-2025-qpruner</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.240</doi>
    </paper>
    <paper id="241">
      <title><fixed-case>MES</fixed-case>-<fixed-case>RAG</fixed-case>: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to <fixed-case>RAG</fixed-case></title>
      <author><first>Pingyu</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Daiheng</first><last>Gao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jing</first><last>Tang</last></author>
      <author><first>Huimin</first><last>Chen</last><affiliation>Independent researcher</affiliation></author>
      <author><first>Wenbo</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4287-4298</pages>
      <abstract>Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by using external knowledge, but it struggles with precise entity information retrieval. Our proposed **MES-RAG** framework enhances entity-specific query handling and provides accurate, secure, and consistent responses. MES-RAG introduces proactive security measures that ensure system integrity by applying protections prior to data access. Additionally, the system supports real-time multi-modal outputs, including text, images, audio, and video, seamlessly integrating into existing RAG architectures. Experimental results demonstrate that MES-RAG significantly improves both accuracy and recall, highlighting its effectiveness in advancing the security and utility of question-answering, increasing accuracy to **0.83 (+0.25)** on targeted task. Our code and data are available at https://github.com/wpydcr/MES-RAG.</abstract>
      <url hash="4fc93f28">2025.findings-naacl.241</url>
      <bibkey>wu-etal-2025-mes</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.241</doi>
    </paper>
    <paper id="242">
      <title><fixed-case>LVP</fixed-case>runing: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models</title>
      <author><first>Yizheng</first><last>Sun</last></author>
      <author><first>Yanze</first><last>Xin</last></author>
      <author><first>Hao</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jingyuan</first><last>Sun</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>4299-4308</pages>
      <abstract>Multi-modal Large Language Models (MLLMs) have achieved remarkable success by integrating visual and textual modalities. However, they incur significant computational overhead due to the large number of vision tokens processed, limiting their practicality in resource-constrained environments. We introduce Language-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet simple method that significantly reduces the computational burden while preserving model performance. LVPruning employs cross-attention modules to compute the importance of vision tokens based on their interaction with language tokens, determining which to prune. Importantly, LVPruning can be integrated without modifying the original MLLM parameters, which makes LVPruning simple to apply or remove. Our experiments show that LVPruning can effectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5, resulting in a 62.1% decrease in inference Tera Floating-Point Operations Per Second (TFLOPs), with an average performance loss of just 0.45% across nine multi-modal benchmarks.</abstract>
      <url hash="46cfabf6">2025.findings-naacl.242</url>
      <bibkey>sun-etal-2025-lvpruning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.242</doi>
    </paper>
    <paper id="243">
      <title>How Much Knowledge Can You Pack into a <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Adapter without Harming <fixed-case>LLM</fixed-case>?</title>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Maria</first><last>Marina</last></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Vasily</first><last>Konovalov</last><affiliation>AIRI</affiliation></author>
      <author><first>Pavel</first><last>Braslavski</last><affiliation>Nazarbayev University</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <pages>4309-4322</pages>
      <abstract>The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model’s parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model’s performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.</abstract>
      <url hash="eec5180b">2025.findings-naacl.243</url>
      <bibkey>pletenev-etal-2025-much</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.243</doi>
    </paper>
    <paper id="244">
      <title><fixed-case>TART</fixed-case>: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning</title>
      <author><first>Xinyuan</first><last>Lu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4323-4339</pages>
      <abstract>Current Large Language Models (LLMs) exhibit limited ability to understand table structures and to apply precise numerical reasoning, which is crucial for tasks such as table question answering and table-based fact verification. To address these challenges, we introduce our Tool-Augmented Reasoning framework for Tables (TART), which integrates LLMs with specialized tools. TART contains three key components: a table formatter to ensure accurate data representation, a tool maker to develop specific computational tools, and an explanation generator to maintain explainability. We also present the TOOLTAB dataset, a new benchmark designed specifically for training LLMs in table–tool integration. Our experiments indicate that TART achieves substantial improvements over existing methods (e.g., Chain-of-Thought) by improving both the precision of data processing and the clarity of the reasoning process. Notably, TART paired with CodeLlama achieves 90.0% of the accuracy of the closed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse real-world scenarios. Both code and data are openly available at https://github.com/XinyuanLu00/TART.</abstract>
      <url hash="8d649c61">2025.findings-naacl.244</url>
      <bibkey>lu-etal-2025-tart</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.244</doi>
    </paper>
    <paper id="245">
      <title>Enhancing Text-to-<fixed-case>SQL</fixed-case> with Question Classification and Multi-Agent Collaboration</title>
      <author><first>Zhihui</first><last>Shao</last></author>
      <author><first>Shubin</first><last>Cai</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Rongsheng</first><last>Lin</last></author>
      <author><first>Zhong</first><last>Ming</last><affiliation>Shenzhen University</affiliation></author>
      <pages>4340-4349</pages>
      <abstract>Large Language Models (LLMs) have recently demonstrated remarkable performance in Text-to-SQL tasks. However, existing research primarily focuses on the optimization of prompts and improvements in workflow, with few studies delving into the exploration of the questions. In this paper, we propose a Text-to-SQL framework based on question classification and multi-agent collaboration (QCMA-SQL). Specifically, we first employ multiple cross-attention mechanisms to train a schema selector to classify questions and select the most suitable database schema. Subsequently, we employ the appropriate agents based on the varying difficulty levels of the questions to generate preliminary SQL queries. Moreover, we implement syntax validation and execution optimization steps to generate final SQL queries. Experimental results on the Spider dataset show that the QCMA-SQL framework achieves an execution accuracy of 87.4%, outperforming state-of-the-art methods. Through ablation studies, we find that classifying the questions ultimately leads to a 2.8% increase in execution accuracy.</abstract>
      <url hash="f79438ba">2025.findings-naacl.245</url>
      <bibkey>shao-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.245</doi>
    </paper>
    <paper id="246">
      <title>Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks</title>
      <author><first>Wataru</first><last>Hashimoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>4350-4366</pages>
      <abstract>Trustworthiness in model predictions is crucial for safety-critical applications in the real world. However, deep neural networks often suffer from the issues of uncertainty estimation, such as miscalibration. In this study, we propose <tex-math>k</tex-math>-Nearest Neighbor Uncertainty Estimation (<tex-math>k</tex-math>NN-UE), which is a new uncertainty estimation method that uses not only the distances from the neighbors, but also the ratio of labels in the neighbors. Experiments on sentiment analysis, natural language inference, and named entity recognition show that our proposed method outperforms the baselines and recent density-based methods in several calibration and uncertainty metrics. Moreover, our analyses indicate that approximate nearest neighbor search techniques reduce the inference overhead without significantly degrading the uncertainty estimation performance when they are appropriately combined.</abstract>
      <url hash="09c7b569">2025.findings-naacl.246</url>
      <bibkey>hashimoto-etal-2025-efficient</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.246</doi>
    </paper>
    <paper id="247">
      <title><fixed-case>B</fixed-case>it<fixed-case>A</fixed-case>buse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks</title>
      <author><first>Hanyong</first><last>Lee</last></author>
      <author><first>Chaelyn</first><last>Lee</last></author>
      <author><first>Yongjae</first><last>Lee</last><affiliation>Retrvr Inc.</affiliation></author>
      <author><first>Jaesung</first><last>Lee</last><affiliation>Chung-Ang University and Chung-Ang University</affiliation></author>
      <pages>4367-4384</pages>
      <abstract>Phishing often targets victims through visually perturbed texts to bypass security systems. The noise contained in these texts functions as an adversarial attack, designed to deceive language models and hinder their ability to accurately interpret the content. However, since it is difficult to obtain sufficient phishing cases, previous studies have used synthetic datasets that do not contain real-world cases. In this study, we propose the BitAbuse dataset, which includes real-world phishing cases, to address the limitations of previous research. Our dataset comprises a total of 325,580 visually perturbed texts. The dataset inputs are drawn from the raw corpus, consisting of visually perturbed sentences and sentences generated through an artificial perturbation process. Each input sentence is labeled with its corresponding ground truth, representing the restored, non-perturbed version. Language models trained on our proposed dataset demonstrated significantly better performance compared to previous methods, achieving an accuracy of approximately 96%. Our analysis revealed a significant gap between real-world and synthetic examples, underscoring the value of our dataset for building reliable pre-trained models for restoration tasks. We release the BitAbuse dataset, which includes real-world phishing cases annotated with visual perturbations, to support future research in adversarial attack defense.</abstract>
      <url hash="4c5cbace">2025.findings-naacl.247</url>
      <bibkey>lee-etal-2025-bitabuse</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.247</doi>
    </paper>
    <paper id="248">
      <title>Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization</title>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Shen</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4385-4398</pages>
      <abstract>In the fast-changing realm of information, the capacity to construct coherent timelines from extensive event-related content has become increasingly significant and challenging. The complexity arises in aggregating related documents to build a meaningful event graph around a central topic. This paper proposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline SummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective on the integration of Large Language Models (LLMs) to tackle the task of Timeline Summarization (TLS). By iteratively reflecting on how events are linked and posing new questions regarding a specific news topic to gather information online or from an offline knowledge base, LLMs produce and refresh chronological summaries based on documents retrieved in each round. Furthermore, we curate Open-TLS, a novel dataset of timelines on recent news topics authored by professional journalists to evaluate open-domain TLS where information overload makes it impossible to find comprehensive relevant documents from the web. Our experiments indicate that CHRONOS is not only adept at open-domain timeline summarization but also rivals the performance of existing state-of-the-art systems designed for closed-domain applications, where a related news corpus is provided for summarization.</abstract>
      <url hash="5f318fcf">2025.findings-naacl.248</url>
      <bibkey>wu-etal-2025-unfolding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.248</doi>
    </paper>
    <paper id="249">
      <title><fixed-case>R</fixed-case>etriever<fixed-case>G</fixed-case>uard: Empowering Information Retrieval to Combat <fixed-case>LLM</fixed-case>-Generated Misinformation</title>
      <author><first>Chuwen</first><last>Chen</last></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>4399-4411</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in generating human-like text and have been shown to store factual knowledge within their extensive parameters. However, models like ChatGPT can still actively or passively generate false or misleading information, increasing the challenge of distinguishing between human-created and machine-generated content. This poses significant risks to the authenticity and reliability of digital communication. This work aims to enhance retrieval models’ ability to identify the authenticity of texts generated by large language models, with the goal of improving the truthfulness of retrieved texts and reducing the harm of false information in the era of large models. Our contributions include: (1) we construct a diverse dataset of authentic human-authored texts and highly deceptive AI-generated texts from various domains; (2) we propose a self-supervised training method, RetrieverGuard, that enables the model to capture textual rules and styles of false information from the corpus without human-labelled data, achieving higher accuracy and robustness in identifying misleading and highly deceptive AI-generated content.</abstract>
      <url hash="c10926be">2025.findings-naacl.249</url>
      <bibkey>chen-zhang-2025-retrieverguard</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.249</doi>
    </paper>
    <paper id="250">
      <title>Unified Automated Essay Scoring and Grammatical Error Correction</title>
      <author><first>SeungWoo</first><last>Song</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>Junghun</first><last>Yuk</last></author>
      <author><first>ChangSu</first><last>Choi</last></author>
      <author><first>HanGyeol</first><last>Yoo</last></author>
      <author><first>HyeonSeok</first><last>Lim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>KyungTae</first><last>Lim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jungyeul</first><last>Park</last><affiliation>The University of British Columbia</affiliation></author>
      <pages>4412-4426</pages>
      <abstract>This study explores the integration of automated writing evaluation (AWE) and grammatical error correction (GEC) through multitask learning, demonstrating how combining these distinct tasks can enhance performance in both areas. By leveraging a shared learning framework, we show that models trained jointly on AWE and GEC outperform those trained on each task individually. To support this effort, we introduce a dataset specifically designed for multitask learning using AWE and GEC. Our experiments reveal significant synergies between tasks, leading to improvements in both writing assessment accuracy and error correction precision. This research represents a novel approach for optimizing language learning tools by unifying writing evaluation and correction tasks, offering insights into the potential of multitask learning in educational applications.</abstract>
      <url hash="81c1d50d">2025.findings-naacl.250</url>
      <bibkey>song-etal-2025-unified</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.250</doi>
    </paper>
    <paper id="251">
      <title>A Closer Look into Mixture-of-Experts in Large Language Models</title>
      <author><first>Ka Man</first><last>Lo</last></author>
      <author><first>Zeyu</first><last>Huang</last></author>
      <author><first>Zihan</first><last>Qiu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>4427-4447</pages>
      <abstract>Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models. Concretely, we comprehensively study the parametric and behavioral features of four popular MoE-based models and reveal some intriguing observations, including 1) Neurons act like fine-grained experts; 2) The router of MoE usually selects experts with larger output norms; 3) The expert diversity increases as the layer increases, while the last layer is an outlier, which is further validated by an initial experiment. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.</abstract>
      <url hash="c5634b6c">2025.findings-naacl.251</url>
      <bibkey>lo-etal-2025-closer</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.251</doi>
    </paper>
    <paper id="252">
      <title><fixed-case>CDB</fixed-case>: A Unified Framework for Hope Speech Detection Through Counterfactual, Desire and Belief</title>
      <author><first>Tulio Ferreira Leite Da</first><last>Silva</last><affiliation>Universidade de São Paulo</affiliation></author>
      <author><first>Gonzalo Freijedo</first><last>Aduna</last><affiliation>Ecole Normale Supérieure – PSL</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>Institut de recherche en informatique de toulouse</affiliation></author>
      <author><first>Alda</first><last>Mari</last><affiliation>CNRS</affiliation></author>
      <author><first>Zongmin</first><last>Li</last></author>
      <author><first>Li</first><last>Yue</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Jian</first><last>Su</last><affiliation>A*STAR</affiliation></author>
      <pages>4448-4463</pages>
      <abstract>Computational modeling of user-generated desires on social media can significantly aid decision-makers across various fields. Initially explored through wish speech,this task has evolved into a nuanced examination of hope speech. To enhance understanding and detection, we propose a novel scheme rooted in formal semantics approaches to modality, capturing both future-oriented hopes through desires and beliefs and the counterfactuality of past unfulfilled wishes and regrets. We manually re-annotated existing hope speech datasets and built a new one which constitutes a new benchmark in the field. We also explore the capabilities of LLMs in automatically detecting hope speech, relying on several prompting strategies. To the best of our knowledge, this is the first attempt towards a language-driven decomposition of the notional category hope and its automatic detection in a unified setting.</abstract>
      <url hash="bbac821e">2025.findings-naacl.252</url>
      <bibkey>silva-etal-2025-cdb</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.252</doi>
    </paper>
    <paper id="253">
      <title>How Well Do <fixed-case>LLM</fixed-case>s Handle <fixed-case>C</fixed-case>antonese? Benchmarking <fixed-case>C</fixed-case>antonese Capabilities of Large Language Models</title>
      <author><first>Jiyue</first><last>Jiang</last></author>
      <author><first>Pengan</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory, University of Hong Kong and University of Hong Kong</affiliation></author>
      <author><first>Liheng</first><last>Chen</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Qinghang</first><last>Bao</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chuan</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>4464-4505</pages>
      <abstract>The rapid evolution of large language models (LLMs) has transformed the competitive landscape in natural language processing (NLP), particularly for English and other data-rich languages. However, underrepresented languages like Cantonese, spoken by over 85 million people, face significant development gaps, which is particularly concerning given the economic significance of the Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial Cantonese-speaking populations in places like Singapore and North America. Despite its wide use, Cantonese has scant representation in NLP research, especially compared to other languages from similarly developed regions. To bridge these gaps, we outline current Cantonese NLP methods and introduce new benchmarks designed to evaluate LLM performance in factual generation, mathematical logic, complex reasoning, and general knowledge in Cantonese, which aim to advance open-source Cantonese LLM technology. We also propose future research directions and recommended models to enhance Cantonese LLM development.</abstract>
      <url hash="2bb0e895">2025.findings-naacl.253</url>
      <bibkey>jiang-etal-2025-well</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.253</doi>
    </paper>
    <paper id="254">
      <title>Improving Reward Models with Synthetic Critiques</title>
      <author><first>Zihuiwen</first><last>Ye</last></author>
      <author><first>Fraser David</first><last>Greenlee</last><affiliation>Cohere</affiliation></author>
      <author><first>Max</first><last>Bartolo</last><affiliation>Cohere and University College London</affiliation></author>
      <author><first>Phil</first><last>Blunsom</last><affiliation>Google, Department of Computer Science, University of Oxford and DeepMind</affiliation></author>
      <author><first>Jon Ander</first><last>Campos</last><affiliation>Cohere</affiliation></author>
      <author><first>Matthias</first><last>Gallé</last><affiliation>Cohere</affiliation></author>
      <pages>4506-4520</pages>
      <abstract>Reward models (RMs) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models, reducing the reliance on costly human annotations. Furthermore, incorporating critiques improves both the interpretability and robustness of RM training.</abstract>
      <url hash="ab12ae3b">2025.findings-naacl.254</url>
      <bibkey>ye-etal-2025-improving</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.254</doi>
    </paper>
    <paper id="255">
      <title>Rethinking Smoothness for Fast and Adaptable Entity Alignment Decoding</title>
      <author><first>Yuanyi</first><last>Wang</last></author>
      <author><first>Han</first><last>Li</last></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>China Unicom Network Communications Co., Ltd.</affiliation></author>
      <author><first>Bo</first><last>He</last></author>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Tianhao</first><last>Yan</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <pages>4521-4535</pages>
      <abstract>Entity alignment (EA) is crucial for integrating multi-source knowledge graphs (KGs), aiming to identify equivalent entities across different graphs. However, most existing EA decoding methods rely on both entity and relation embeddings, limiting their generalizability and efficiency, especially in GNN-based models. To address these challenges, we propose Triple Feature Propagation (TFP), an adaptable and fast EA decoding framework that only utilizes entity embeddings. TFP reconstructs KG representation by maximizing the smoothness of entity embeddings. The discretized smoothness-maximization process yields the explicit Euler solution of TFP. We also generalize multi-view matrices: entity-to-entity, entity-to-relation, relation-to-entity, and relation-to-triple, to capture structural diversity. Extensive experiments on public datasets demonstrate that TFP is fast and adaptable to various encoders, achieving comparable results to state-of-the-art methods in under 6 seconds, and surpassing them in many cases.</abstract>
      <url hash="5e0c7ca4">2025.findings-naacl.255</url>
      <bibkey>wang-etal-2025-rethinking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.255</doi>
    </paper>
    <paper id="256">
      <title>Lost in the Distance: Large Language Models Struggle to Capture Long-Distance Relational Knowledge</title>
      <author><first>Meiyun</first><last>Wang</last></author>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>4536-4544</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in handling long contexts, but challenges remain in capturing relational knowledge spread far apart within text. Connecting long-distance knowledge is important for solving tasks as the context length increases: imagine reading a lengthy detective novel where seemingly trivial information introduced early on often becomes essential during the climactic reveal of the culprit. In this study, we expose the ”Lost in the Distance” phenomenon, where LLM performance of capturing the relational knowledge degrades significantly when the relational knowledge is separated by noise, i.e., unrelated sentences to solve a task. Specifically, we design an experiment in which we insert artificial noise between two related elements and observe model performance as the distance between them increases. Our findings show that while LLMs can handle edge noise with little impact, their ability to reason about distant relationships declines sharply as the intervening noise grows. These findings are consistent in both forward-looking prediction and backward-looking prediction settings. We validate this across various models (GPT-4, Gemini-1.5-pro, GPT-4o-mini, Gemini-1.5-flash, Claude-3.5-Sonnet) and tasks (causal reasoning and knowledge extraction). These results reveal a significant limitation in how LLMs process relational knowledge over long contexts. We release our code and data to support further research.</abstract>
      <url hash="a08ab6ee">2025.findings-naacl.256</url>
      <bibkey>wang-etal-2025-lost</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.256</doi>
    </paper>
    <paper id="257">
      <title><fixed-case>F</fixed-case>in<fixed-case>NLI</fixed-case>: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</title>
      <author><first>Jabez</first><last>Magomere</last></author>
      <author><first>Elena</first><last>Kochkina</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Samuel</first><last>Mensah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Simerjot</first><last>Kaur</last><affiliation>JPMorgan Chase and Co</affiliation></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <pages>4545-4568</pages>
      <abstract>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset’s difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.</abstract>
      <url hash="aaee4cda">2025.findings-naacl.257</url>
      <bibkey>magomere-etal-2025-finnli</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.257</doi>
    </paper>
    <paper id="258">
      <title>Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models</title>
      <author><first>Atharva</first><last>Mehta</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shivam</first><last>Chauhan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Amirbek</first><last>Djanibekov</last></author>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Gus</first><last>Xia</last><affiliation>New York University</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>4569-4585</pages>
      <abstract>The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres.We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models – MusicGen and Mustango, for two underrepresented non-Western music traditions – Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.</abstract>
      <url hash="70b293d6">2025.findings-naacl.258</url>
      <bibkey>mehta-etal-2025-music</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.258</doi>
    </paper>
    <paper id="259">
      <title><fixed-case>SFMSS</fixed-case>: Service Flow aware Medical Scenario Simulation for Conversational Data Generation</title>
      <author><first>Zhijie</first><last>Bao</last></author>
      <author><first>Qingyun</first><last>Liu</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>4586-4604</pages>
      <abstract>Medical-specific Large Language Models (LLMs) have demonstrated impressive performance on medical-related exams and tasks. Despite their success in single-turn question and answering, instruction-tuned LLMs often falter in real-world healthcare applications, highlighting a disconnect between existing instruction datasets and practical contexts. To address this issue, we propose Service Flow aware Medical Scenario Simulation (SFMSS), a simulation framework designed for medical conversational data generation. SFMSS employs three key strategies to ensure the quality of the data generation. the use of Authentic Seed Data ensures alignment of real-world distributions. Diverse Patient Simulation enables simulated patients to exhibit distinct communication styles and complex behavioral logic. Service Flow Control ensures that conversations progress in alignment with medical objectives. We construct a dataset targeting on outpatient reception through SFMSS, named SFMSS-CD. Building on this dataset, we develop a model called SFMSS-Nurse. We conduct both automatic and human evaluations, involving 15 users and 15 clinical experts, to assess the effectiveness of SFMSS. The results demonstrate that SFMSS-Nurse outperforms all baselines, including the current state-of-the-art model GPT-4o, and aligns with human preferences and clinical demands.</abstract>
      <url hash="f73d7304">2025.findings-naacl.259</url>
      <bibkey>bao-etal-2025-sfmss</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.259</doi>
    </paper>
    <paper id="260">
      <title>Re-evaluating Automatic <fixed-case>LLM</fixed-case> System Ranking for Alignment with Human Preference</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Jonathan</first><last>Bragg</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4605-4629</pages>
      <abstract>Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models’ performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.</abstract>
      <url hash="c9b654d2">2025.findings-naacl.260</url>
      <bibkey>gao-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.260</doi>
    </paper>
    <paper id="261">
      <title><fixed-case>G</fixed-case>uide<fixed-case>Q</fixed-case>: Framework for Guided Questioning for progressive informational collection and classification</title>
      <author><first>Priya</first><last>Mishra</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Suraj</first><last>Racha</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Kaustubh</first><last>Ponkshe</last></author>
      <author><first>Adit</first><last>Akarsh</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>4630-4644</pages>
      <abstract>The veracity of a factoid is largely independent of the language it is written in. However, language models are inconsistent in their ability to answer the same factual question across languages. This raises questions about how LLMs represent a given fact across languages. We explore multilingual factual knowledge through two aspects: the model’s ability to answer a query consistently across languages, and the ability to ”store” answers in a shared representation for several languages. We propose a methodology to measure the extent of representation sharing across languages by repurposing knowledge editing methods. We examine LLMs with various multilingual configurations using a new multilingual dataset. We reveal that high consistency does not necessarily imply shared representation, particularly for languages with different scripts. Moreover, we find that script similarity is a dominant factor in representation sharing. Finally, we observe that if LLMs could fully share knowledge across languages, their accuracy in their best-performing language could benefit an increase of up to 150% on average. These findings highlight the need for improved multilingual knowledge representation in LLMs and suggest a path for the development of more robust and consistent multilingual LLMs.</abstract>
      <url hash="d82341a2">2025.findings-naacl.261</url>
      <bibkey>mishra-etal-2025-guideq</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.261</doi>
    </paper>
    <paper id="262">
      <title>Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations</title>
      <author><first>Kirti</first><last>Bhagat</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Kinshuk</first><last>Vasisht</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <pages>4645-4653</pages>
      <abstract>While a large body of work inspects language models for biases concerning gender, race, occupation and religion, biases of geographical nature are relatively less explored. Some recent studies benchmark the degree to which large language models encode geospatial knowledge. However, the impact of the encoded geographical knowledge (or lack thereof) on real-world applications has not been documented. In this work, we examine large language models for two common scenarios that require geographical knowledge: (a) travel recommendations and (b) geo-anchored story generation. Specifically, we study five popular language models, and across about 100K travel requests, and 200K story generations, we observe that travel recommendations corresponding to poorer countries are less unique with fewer location references, and stories from these regions more often convey emotions of hardship and sadness compared to those from wealthier nations.</abstract>
      <url hash="878a85cf">2025.findings-naacl.262</url>
      <bibkey>bhagat-etal-2025-richer</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.262</doi>
    </paper>
    <paper id="263">
      <title>Swan and <fixed-case>A</fixed-case>rabic<fixed-case>MTEB</fixed-case>: Dialect-Aware, <fixed-case>A</fixed-case>rabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Abdellah</first><last>El Mekki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4654-4670</pages>
      <abstract>In this paper, we introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan models are dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmarks will be made publicly accessible for research.</abstract>
      <url hash="5eee28ad">2025.findings-naacl.263</url>
      <bibkey>bhatia-etal-2025-swan</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.263</doi>
    </paper>
    <paper id="264">
      <title><fixed-case>TAGCOS</fixed-case>: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data</title>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Yaxuan</first><last>Qin</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Weizhong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Rui</first><last>Pan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>4671-4686</pages>
      <abstract>Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples’ quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.</abstract>
      <url hash="6ea216aa">2025.findings-naacl.264</url>
      <bibkey>zhang-etal-2025-tagcos</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.264</doi>
    </paper>
    <paper id="265">
      <title>From Text to Emoji: How <fixed-case>PEFT</fixed-case>-Driven Personality Manipulation Unleashes the Emoji Potential in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Navya</first><last>Jain</last></author>
      <author><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Cristian Enrique Munoz</first><last>Villalobos</last></author>
      <author><first>Airlie</first><last>Hilliard</last></author>
      <author><first>Xin</first><last>Guan</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Adriano</first><last>Koshiyama</last></author>
      <author><first>Emre</first><last>Kazim</last></author>
      <author><first>Philip Colin</first><last>Treleaven</last><affiliation>University College London, University of London</affiliation></author>
      <pages>4687-4723</pages>
      <abstract>The manipulation of the personality traits of large language models (LLMs) has emerged as a key area of research. Methods like prompt-based In-Context Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have been explored but show irregularity and variability; IKE depends on the prompt, leading to variability and sensitivity, while MEND yields inconsistent and gibberish outputs. To address this, we employed Opinion QA Based Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank Adaptation (QLoRA), to manipulate the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT, models such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent behaviour by generating emojis for certain traits, despite no emojis being present in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in 99.5% of extraversion-related test instances, while Mistral-7B-Instruct did so in 92.5% of openness-related test instances. ICL Explainability analysis indicated that the LLMs used emojis intentionally to express these traits. Mechanistic Interpretability analysis showed that this latent behaviour of LLMs could be traced to specific neurons that became activated or amplified after PEFT. This paper provides a number of novel contributions. First, introducing an Opinion QA dataset for PEFT-driven personality manipulation; second, developing metric models to benchmark LLM personality traits; third, demonstrating PEFT’s superiority over IKE in personality manipulation; and finally, analysing and validating emoji usage through explainability methods such as Mechanistic Interpretability and In-context learning Explainability methods.</abstract>
      <url hash="aa4cbfb6">2025.findings-naacl.265</url>
      <bibkey>jain-etal-2025-text</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.265</doi>
    </paper>
    <paper id="266">
      <title>Decoding Fatphobia: Examining Anti-Fat and Pro-Thin Bias in <fixed-case>AI</fixed-case>-Generated Images</title>
      <author><first>Jane</first><last>Warren</last></author>
      <author><first>Gary M.</first><last>Weiss</last><affiliation>Fordham University</affiliation></author>
      <author><first>Fernando</first><last>Martinez</last></author>
      <author><first>Annika</first><last>Guo</last></author>
      <author><first>Yijun</first><last>Zhao</last><affiliation>Fordham University</affiliation></author>
      <pages>4724-4736</pages>
      <abstract>Existing studies have shown that AI-generated images tend to reinforce social biases, including those related to race and gender. However, no studies have investigated weight bias, or fatphobia, in AI-generated images. This study utilizes DALL-E 3 to determine the extent to which anti-fat and pro-thin biases are present in AI-generated images, and examines stereotypical associations between moral character and body weight. Four-thousand images are generated using twenty pairs of positive and negative textual prompts. These images are then manually labeled with weight information and analyzed to determine the extent to which they reflect fatphobia. The findings and their impact are discussed and related to existing research on weight bias.</abstract>
      <url hash="a8d2d69d">2025.findings-naacl.266</url>
      <bibkey>warren-etal-2025-decoding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.266</doi>
    </paper>
    <paper id="267">
      <title><fixed-case>MMAU</fixed-case>: A Holistic Benchmark of Agent Capabilities Across Diverse Domains</title>
      <author><first>Guoli</first><last>Yin</last><affiliation>Apple</affiliation></author>
      <author><first>Haoping</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Shuang</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Yanchao</first><last>Sun</last><affiliation>Apple AI/ML</affiliation></author>
      <author><first>Zhaoyang</first><last>Xu</last><affiliation>Apple</affiliation></author>
      <author><first>Shen</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Xiang</first><last>Kong</last><affiliation>Apple</affiliation></author>
      <author><first>Aonan</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Dian Ang</first><last>Yap</last><affiliation>Apple</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Karsten</first><last>Ahnert</last><affiliation>Apple</affiliation></author>
      <author><first>Vik</first><last>Kamath</last><affiliation>Apple</affiliation></author>
      <author><first>Mathias</first><last>Berglund</last><affiliation>Apple</affiliation></author>
      <author><first>Dominic</first><last>Walsh</last><affiliation>Apple</affiliation></author>
      <author><first>Tobias</first><last>Gindele</last><affiliation>Apple</affiliation></author>
      <author><first>Juergen</first><last>Wiest</last><affiliation>Apple</affiliation></author>
      <author><first>Zhengfeng</first><last>Lai</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoming Simon</first><last>Wang</last><affiliation>Didi Research US</affiliation></author>
      <author><first>Jiulong</first><last>Shan</last><affiliation>Apple</affiliation></author>
      <author><first>Meng</first><last>Cao</last><affiliation>Apple</affiliation></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <pages>4737-4765</pages>
      <abstract>Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluate models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covering five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 20 representative models on MMAU, we provide deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance.</abstract>
      <url hash="9c64afe6">2025.findings-naacl.267</url>
      <bibkey>yin-etal-2025-mmau</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.267</doi>
    </paper>
    <paper id="268">
      <title>Improving Consistency in <fixed-case>LLM</fixed-case> Inference using Probabilistic Tokenization</title>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>4766-4778</pages>
      <abstract>Prior research has demonstrated noticeable performance gains through the use of probabilistic tokenizations, an approach that involves employing multiple tokenizations of the same input string during the training phase of a language model. Despite these promising findings, modern large language models (LLMs) have yet to be trained using probabilistic tokenizations. Interestingly, while the tokenizers of these contemporary LLMs have the capability to generate multiple tokenizations, this property remains underutilized.In this work, we propose a novel method to leverage the multiple tokenization capabilities of modern LLM tokenizers, aiming to enhance the self-consistency of LLMs in reasoning tasks. Our experiments indicate that when utilizing probabilistic tokenizations, LLMs generate logically diverse reasoning paths, moving beyond mere surface-level linguistic diversity. We carefully study probabilistic tokenization and offer insights to explain the self consistency improvements it brings through extensive experimentation on 5 LLM families and 4 reasoning benchmarks.</abstract>
      <url hash="8f3672ad">2025.findings-naacl.268</url>
      <bibkey>sathe-etal-2025-improving</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.268</doi>
    </paper>
    <paper id="269">
      <title><fixed-case>W</fixed-case>ord<fixed-case>G</fixed-case>ame: Efficient &amp; Effective <fixed-case>LLM</fixed-case> Jailbreak via Simultaneous Obfuscation in Query and Response</title>
      <author><first>Tianrong</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Yuanpu</first><last>Cao</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4779-4807</pages>
      <abstract>The recent breakthrough in large language models (LLMs) such as ChatGPT has revolutionized every industry at an unprecedented pace. Alongside this progress also comes mounting concerns about LLMs’ susceptibility to jailbreaking attacks, which leads to the generation of harmful or unsafe content. While safety alignment measures have been implemented in LLMs to mitigate existing jailbreak attempts and force them to become increasingly complicated, it is still far from perfect. In this paper, we analyze the common pattern of the current safety alignment and show that it is possible to exploit such patterns for jailbreaking attacks by simultaneous obfuscation in queries and responses. Specifically, we propose WordGame attack, which replaces malicious words with word games to break down the adversarial intent of a query and encourage benign content regarding the games to precede the anticipated harmful content in the response, creating a context that is hardly covered by any corpus used for safety alignment. Extensive experiments demonstrate that WordGame attack can break the guardrails of the current leading proprietary and open-source LLMs, including the latest Claude 3, GPT 4, and Llama 3 models more effectively than existing attacks efficiently. The attack also remains powerful when external defenses are adopted. Further ablation studies on such simultaneous obfuscation in query and response provide evidence of the merits of the attack strategy beyond an individual attack.</abstract>
      <url hash="d6a00a9a">2025.findings-naacl.269</url>
      <bibkey>zhang-etal-2025-wordgame</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.269</doi>
    </paper>
    <paper id="270">
      <title>Human and <fixed-case>LLM</fixed-case>-Based Resume Matching: An Observational Study</title>
      <author><first>Swanand</first><last>Vaishampayan</last></author>
      <author><first>Hunter</first><last>Leary</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Yoseph Berhanu</first><last>Alebachew</last></author>
      <author><first>Louis</first><last>Hickman</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Brent A.</first><last>Stevenor</last><affiliation>NREMT</affiliation></author>
      <author><first>Weston</first><last>Beck</last><affiliation>Discovered</affiliation></author>
      <author><first>Chris</first><last>Brown</last><affiliation>Virginia Tech</affiliation></author>
      <pages>4808-4823</pages>
      <abstract>Resume matching assesses the extent to which candidates qualify for jobs based on the content of resumes. This process increasingly uses natural language processing (NLP) techniques to automate parsing and rating tasks—saving time and effort. Large language models (LLMs) are increasingly used for this purpose—thus, we explore their capabilities for resume matching in an observational study. We compare zero-shot GPT-4 and human ratings for 736 resumes submitted to job openings from diverse fields using real-world evaluation criteria. We also study the effects of prompt engineering techniques on GPT-4 ratings and compare differences in GPT-4 and human ratings across racial and gender groups. Our results show: LLM scores correlate minorly with humans, suggesting they are not interchangeable; prompt engineering such as CoT improves the quality of LLM ratings; and LLM scores do not show larger group differences (i.e., bias) than humans. Our findings provide implications for LLM-based resume rating to promote more fair and NLP-based resume matching in a multicultural world.</abstract>
      <url hash="dfa858ca">2025.findings-naacl.270</url>
      <bibkey>vaishampayan-etal-2025-human</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.270</doi>
    </paper>
    <paper id="271">
      <title>A Practical Examination of <fixed-case>AI</fixed-case>-Generated Text Detectors for Large Language Models</title>
      <author><first>Brian</first><last>Tufts</last></author>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>4824-4841</pages>
      <abstract>The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank, Binoculars) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate practical adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.</abstract>
      <url hash="6ae6fd80">2025.findings-naacl.271</url>
      <bibkey>tufts-etal-2025-practical</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.271</doi>
    </paper>
    <paper id="272">
      <title>Robust Bias Detection in <fixed-case>MLM</fixed-case>s and its Application to Human Trait Ratings</title>
      <author><first>Ingroj</first><last>Shrestha</last></author>
      <author><first>Louis</first><last>Tay</last><affiliation>Purdue University</affiliation></author>
      <author><first>Padmini</first><last>Srinivasan</last><affiliation>University of Iowa</affiliation></author>
      <pages>4842-4858</pages>
      <abstract>There has been significant prior work using templates to study bias against demographic attributes in MLMs. However, these have limitations: they overlook random variability of templates and target concepts analyzed, assume equality amongst templates, and overlook bias quantification. Addressing these, we propose a systematic statistical approach to assess bias in MLMs, using mixed models to account for random effects, pseudo-perplexity weights for sentences derived from templates and quantify bias using statistical effect sizes. Replicating prior studies, we match on bias scores in magnitude and direction with small to medium effect sizes.Next, we explore the novel problem of gender bias in the context of *personality* and *character* traits, across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased for binary gender but the most biased for non-binary *neo*, while RoBERTa-large is the most biased for binary gender but shows small to no bias for *neo*. There is some alignment of MLM bias and findings in psychology (human perspective) - in *agreeableness* with RoBERTa-large and *emotional stability* with BERT-large. There is general agreement for the remaining 3 personality dimensions: both sides observe at most small differences across gender. For character traits, human studies on gender bias are limited thus comparisons are not feasible.</abstract>
      <url hash="06bc7807">2025.findings-naacl.272</url>
      <bibkey>shrestha-etal-2025-robust</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.272</doi>
    </paper>
    <paper id="273">
      <title>How Inclusively do <fixed-case>LM</fixed-case>s Perceive Social and Moral Norms?</title>
      <author><first>Michael</first><last>Galarnyk</last></author>
      <author><first>Agam</first><last>Shah</last></author>
      <author><first>Dipanwita</first><last>Guhathakurta</last></author>
      <author><first>Poojitha</first><last>Nandigam</last></author>
      <author><first>Sudheer</first><last>Chava</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>4859-4869</pages>
      <abstract>**This paper discusses and contains offensive content.** Language models (LMs) are used in decision-making systems and as interactive assistants. However, how well do these models making judgements align with the diversity of human values, particularly regarding social and moral norms? In this work, we investigate how inclusively LMs perceive norms across demographic groups (e.g., gender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare their outputs with the existing responses of 100 human annotators. We introduce the Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on ordinal questions. We find notable disparities in LM responses, with younger, higher-income groups showing closer alignment, raising concerns about the representation of marginalized perspectives. Our findings highlight the importance of further efforts to make LMs more inclusive of diverse human values. The code and prompts are available on GitHub under the CC BY-NC 4.0 license.</abstract>
      <url hash="2cf7ea83">2025.findings-naacl.273</url>
      <bibkey>galarnyk-etal-2025-inclusively</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.273</doi>
    </paper>
    <paper id="274">
      <title>Jailbreaking with Universal Multi-Prompts</title>
      <author><first>Yu-Ling</first><last>Hsu</last></author>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Shang-Tse</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>4870-4891</pages>
      <abstract>Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.</abstract>
      <url hash="fceb088f">2025.findings-naacl.274</url>
      <bibkey>hsu-etal-2025-jailbreaking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.274</doi>
    </paper>
    <paper id="275">
      <title>Echoes of Discord: Forecasting Hater Reactions to Counterspeech</title>
      <author><first>Xiaoying</first><last>Song</last></author>
      <author><first>Sharon Lisseth</first><last>Perez</last></author>
      <author><first>Xinchen</first><last>Yu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Lingzi</first><last>Hong</last><affiliation>University of North Texas</affiliation></author>
      <pages>4892-4905</pages>
      <abstract>Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division. Counterspeech has been recognized as a way to mitigate the harmful consequences. While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters’ reactions toward counterspeech, despite the immediate alteration of haters’ attitudes being an important aspect of counterspeech. This study fills the gap by analyzing the impact of counterspeech from the hater’s perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful. We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters’ reactions, to assess the impact of counterspeech. To predict haters’ behaviors, we employ two strategies: a two-stage reaction predictor and a three-way classifier. The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters’ reactions. Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type. We conclude the study with an assessment showing the most common errors identified by the best-performing model.</abstract>
      <url hash="003ea5a7">2025.findings-naacl.275</url>
      <bibkey>song-etal-2025-echoes</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.275</doi>
    </paper>
    <paper id="276">
      <title>Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy</title>
      <author><first>Athiya</first><last>Deviyani</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Fernando</first><last>Diaz</last><affiliation>Carnegie Mellon University and Google</affiliation></author>
      <pages>4906-4925</pages>
      <abstract>Meta-evaluation of automatic evaluation metrics—assessing evaluation metrics themselves—is crucial for accurately benchmarking natural language processing systems and has implications for scientific inquiry, production model development, and policy enforcement. While existing approaches to metric meta-evaluation focus on general statements about the absolute and relative quality of metrics across arbitrary system outputs, in practice, metrics are applied in highly contextual settings, often measuring the performance for a highly constrained set of system outputs. For example, we may only be interested in evaluating a specific model or class of models. We introduce a method for contextual metric meta-evaluation by comparing the local metric accuracy of evaluation metrics. Across translation, speech recognition, and ranking tasks, we demonstrate that the local metric accuracies vary both in absolute value and relative effectiveness as we shift across evaluation contexts. This observed variation highlights the importance of adopting context-specific metric evaluations over global ones.</abstract>
      <url hash="2d4424e6">2025.findings-naacl.276</url>
      <bibkey>deviyani-diaz-2025-contextual</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.276</doi>
    </paper>
    <paper id="277">
      <title>Advocating Character Error Rate for Multilingual <fixed-case>ASR</fixed-case> Evaluation</title>
      <author><first>Thennal D</first><last>K</last></author>
      <author><first>Jesin</first><last>James</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Deepa Padmini</first><last>Gopinath</last></author>
      <author><first>Muhammed Ashraf</first><last>K</last></author>
      <pages>4926-4935</pages>
      <abstract>Automatic speech recognition (ASR) systems have traditionally been evaluated using English datasets, with the word error rate (WER) serving as the predominant metric. WER’s simplicity and ease of interpretation have contributed to its widespread adoption, particularly for English. However, as ASR systems expand to multilingual contexts, WER fails in various ways, particularly with morphologically complex languages or those without clear word boundaries. Our work documents the limitations of WER as an evaluation metric and advocates for the character error rate (CER) as the primary metric in multilingual ASR evaluation. We show that CER avoids many of the challenges WER faces and exhibits greater consistency across writing systems. We support our proposition by conducting human evaluations of ASR transcriptions in three languages—Malayalam, English, and Arabic—which exhibit distinct morphological characteristics. We show that CER correlates more closely with human judgments than WER, even for English. To facilitate further research, we release our human evaluation dataset for future benchmarking of ASR metrics. Our findings suggest that CER should be prioritized, or at least supplemented, in multilingual ASR evaluations to account for the varying linguistic characteristics of different languages.</abstract>
      <url hash="743fb276">2025.findings-naacl.277</url>
      <bibkey>k-etal-2025-advocating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.277</doi>
    </paper>
    <paper id="278">
      <title>Enhancing Temporal Understanding in <fixed-case>LLM</fixed-case>s for Semi-structured Tables</title>
      <author><first>Irwin</first><last>Deng</last></author>
      <author><first>Kushagra</first><last>Dixit</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>4936-4955</pages>
      <abstract>Temporal reasoning over tabular data presents substantial challenges for large language models (LLMs), as evidenced by recent research. In this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific limitations of LLMs. Our investigation leads to enhancements in TempTabQA, a benchmark specifically designed for tabular temporal question answering. We provide critical insights for enhancing LLM performance in temporal reasoning tasks with tabular data. Furthermore, we introduce a novel approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings demonstrate that our method improves evidence-based reasoning across various models. Additionally, our experimental results reveal that indirect supervision with auxiliary unstructured data (TRAM) substantially boosts model performance in these tasks. This work contributes to a deeper understanding of LLMs’ temporal reasoning abilities over tabular data and promotes advancements in their application across diverse fields.</abstract>
      <url hash="11c475ea">2025.findings-naacl.278</url>
      <bibkey>deng-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.278</doi>
    </paper>
    <paper id="279">
      <title><fixed-case>B</fixed-case>n<fixed-case>TTS</fixed-case>: Few-Shot Speaker Adaptation in Low-Resource Setting</title>
      <author><first>Mohammad Jahid Ibna</first><last>Basher</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Rabindra Nath</first><last>Nandi</last><affiliation>Hishab Singapure Pte. Ltd</affiliation></author>
      <author><first>Nusrat Jahan</first><last>Prottasha</last></author>
      <author><first>Mehadi Hasan</first><last>Menon</last></author>
      <author><first>Tareq Al</first><last>Muntasir</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Ozlem</first><last>Garibay</last><affiliation>University of Central Florida</affiliation></author>
      <pages>4956-4968</pages>
      <abstract>This paper introduces BnTTS (Bangla Text-To-Speech), the first framework for Bangla speaker adaptation-based TTS, designed to bridge the gap in Bangla speech synthesis using minimal training data. Building upon the XTTS architecture, our approach integrates Bangla into a multilingual TTS pipeline, with modifications to account for the phonetic and linguistic characteristics of the language. We pretrain BnTTS on 3.85k hours of Bangla speech dataset with corresponding text labels and evaluate performance in both zero-shot and few-shot settings on our proposed test dataset. Empirical evaluations in few-shot settings show that BnTTS significantly improves the naturalness, intelligibility, and speaker fidelity of synthesized Bangla speech. Compared to state-of-the-art Bangla TTS systems, BnTTS exhibits superior performance in Subjective Mean Opinion Score (SMOS), Naturalness, and Clarity metrics.</abstract>
      <url hash="c69ac0c5">2025.findings-naacl.279</url>
      <bibkey>basher-etal-2025-bntts</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.279</doi>
    </paper>
    <paper id="280">
      <title>Playing with Voices: Tabletop Role-Playing Game Recordings as a Diarization Challenge</title>
      <author><first>Lian</first><last>Remme</last></author>
      <author><first>Kevin</first><last>Tang</last><affiliation>Heinrich Heine University Düsseldorf and University of Florida</affiliation></author>
      <pages>4969-4983</pages>
      <abstract>This paper provides a proof of concept that audio of tabletop role-playing games (TTRPG) could serve as a challenge for diarization systems. TTRPGs are carried out mostly by conversation. Participants often alter their voices to indicate that they are talking as a fictional character. Audio processing systems are susceptible to voice conversion with or without technological assistance. TTRPG present a conversational phenomenon in which voice conversion is an inherent characteristic for an immersive gaming experience. This could make it more challenging for diarizers to pick the real speaker and determine that impersonating is just that. We present the creation of a small TTRPG audio dataset and compare it against the AMI and the ICSI corpus. The performance of two diarizers, pyannote.audio and wespeaker, were evaluated. We observed that TTRPGs’ properties result in a higher confusion rate for both diarizers.Additionally, wespeaker strongly underestimates the number of speakers in the TTRPG audio files.We propose TTRPG audio as a promising challenge for diarization systems.</abstract>
      <url hash="4bd05e57">2025.findings-naacl.280</url>
      <bibkey>remme-tang-2025-playing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.280</doi>
    </paper>
    <paper id="281">
      <title>Causally Testing Gender Bias in <fixed-case>LLM</fixed-case>s: A Case Study on Occupational Bias</title>
      <author><first>Yuen</first><last>Chen</last></author>
      <author><first>Vethavikashini Chithrra</first><last>Raghuram</last><affiliation>CCC Intelligent Solutions</affiliation></author>
      <author><first>Justus</first><last>Mattern</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>4984-5004</pages>
      <abstract>Generated texts from large language models (LLMs) have been shown to exhibit a variety of harmful, human-like biases against various demographics. These findings motivate research efforts aiming to understand and measure such effects. This paper introduces a causal formulation for bias measurement in generative language models. Based on this theoretical foundation, we outline a list of desiderata for designing robust bias benchmarks. We then propose a benchmark called OccuGender, with a bias-measuring procedure to investigate occupational gender bias. We test several state-of-the-art open-source LLMs on OccuGender, including Llama, Mistral, and their instruction-tuned versions. The results show that these models exhibit substantial occupational gender bias. Lastly, we discuss prompting strategies for bias mitigation and an extension of our causal formulation to illustrate the generalizability of our framework.</abstract>
      <url hash="9610e537">2025.findings-naacl.281</url>
      <bibkey>chen-etal-2025-causally</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.281</doi>
    </paper>
    <paper id="282">
      <title><fixed-case>OLMES</fixed-case>: A Standard for Language Model Evaluations</title>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Dany</first><last>Haddad</last></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>5005-5033</pages>
      <abstract>Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models can be particularly challenging, as choices of how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural “cloze” formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered, documented recommendations guided by results from existing literature as well as new experiments resolving open questions.</abstract>
      <url hash="22909d22">2025.findings-naacl.282</url>
      <bibkey>gu-etal-2025-olmes</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.282</doi>
    </paper>
    <paper id="283">
      <title>Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning</title>
      <author><first>Joy</first><last>Crosbie</last></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>5034-5096</pages>
      <abstract>Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model’s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that the induction mechanism plays in ICL.</abstract>
      <url hash="2bc1e3bf">2025.findings-naacl.283</url>
      <bibkey>crosbie-shutova-2025-induction</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.283</doi>
    </paper>
    <paper id="284">
      <title><fixed-case>M</fixed-case>o<fixed-case>LA</fixed-case>: <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> with Layer-wise Expert Allocation</title>
      <author><first>Chongyang</first><last>Gao</last></author>
      <author><first>Kezhen</first><last>Chen</last><affiliation>Together AI</affiliation></author>
      <author><first>Jinmeng</first><last>Rao</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Baochen</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Yawen</first><last>Zhang</last><affiliation>Google X, Mineral.ai</affiliation></author>
      <author><first>Daiyi</first><last>Peng</last></author>
      <author><first>Xiaoyuan</first><last>Guo</last><affiliation>Mineral.ai</affiliation></author>
      <author><first>Vs</first><last>Subrahmanian</last><affiliation>Northwestern University</affiliation></author>
      <pages>5097-5112</pages>
      <abstract>Recent efforts to integrate low-rank adaptation (LoRA) with the Mixture-of-Experts (MoE) have managed to achieve performance comparable to full-parameter fine-tuning by tuning much fewer parameters. Despite promising results, research on improving the efficiency and expert analysis of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, <i>
          <b>M</b>oE-L<b>o</b>RA with <b>L</b>ayer-wise Expert <b>A</b>llocation (MoLA)</i> for Transformer-based models, where each model layer uses a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines on top of both LLAMA-2, Mistral, and Gemma. We find that allocating more LoRA experts to middle layers further enhances the effectiveness of models with a certain number of experts in total. The redundancy of the experts is more obvious in the lower layers. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code has been made available at <url>https://github.com/GCYZSL/MoLA</url>.</abstract>
      <url hash="ac32bc2b">2025.findings-naacl.284</url>
      <bibkey>gao-etal-2025-mola</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.284</doi>
    </paper>
    <paper id="285">
      <title><fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>im: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging</title>
      <author><first>Md. Ashraful</first><last>Islam</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Eunus</first><last>Ali</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>5113-5139</pages>
      <url hash="74ff4b22">2025.findings-naacl.285</url>
      <bibkey>islam-etal-2025-codesim</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.285</doi>
    </paper>
    <paper id="286">
      <title>On the Feasibility of In-Context Probing for Data Attribution</title>
      <author><first>Cathy</first><last>Jiao</last></author>
      <author><first>Weizhen</first><last>Gao</last></author>
      <author><first>Aditi</first><last>Raghunathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>5140-5155</pages>
      <abstract>Data attribution methods are used to measure the contribution of training data towards model outputs, and have several important applications in areas such as dataset curation and model interpretability. However, many standard data attribution methods, such as influence functions, utilize model gradients and are computationally expensive. In our paper, we show in-context probing (ICP) – prompting a LLM – can serve as a fast proxy for gradient-based data attribution for data selection under conditions contingent on data similarity. We study this connection empirically on standard NLP tasks, and show that ICP and gradient-based data attribution are well-correlated in identifying influential training data for tasks that share similar task type and content as the training data. Additionally, fine-tuning models on influential data selected by both methods achieves comparable downstream performance, further emphasizing their similarities. We then examine the connection between ICP and gradient-based data attribution using synthetic data on linear regression tasks. Our synthetic data experiments show similar results with those from NLP tasks, suggesting that this connection can be isolated in simpler settings, which offers a pathway to bridging their differences.</abstract>
      <url hash="bd20ff71">2025.findings-naacl.286</url>
      <bibkey>jiao-etal-2025-feasibility</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.286</doi>
    </paper>
    <paper id="287">
      <title>Evaluation of Multilingual Image Captioning: How far can we get with <fixed-case>CLIP</fixed-case> models?</title>
      <author><first>Goncalo Emanuel Cavaco</first><last>Gomes</last></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <pages>5156-5175</pages>
      <abstract>The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.</abstract>
      <url hash="1b6c2762">2025.findings-naacl.287</url>
      <bibkey>gomes-etal-2025-evaluation</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.287</doi>
    </paper>
    <paper id="288">
      <title>Avoiding Copyright Infringement via Large Language Model Unlearning</title>
      <author><first>Guangyao</first><last>Dou</last></author>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Qing</first><last>Lyu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Eric</first><last>Wong</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>5176-5200</pages>
      <abstract>Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. In real-world scenarios, model owners need to continuously address copyright infringement as new requests for content removal emerge at different time points. This leads to the need for sequential unlearning, where copyrighted content is removed sequentially as new requests arise. Despite its practical relevance, sequential unlearning in the context of copyright infringement has not been rigorously explored in existing literature. To address this gap, we propose Stable Sequential Unlearning (SSU), a novel framework designed to unlearn copyrighted content from LLMs over multiple time steps. Our approach works by identifying and removing specific weight updates in the model’s parameters that correspond to copyrighted content. We improve unlearning efficacy by introducing random labeling loss and ensuring the model retains its general-purpose knowledge by adjusting targeted parameters. Experimental results show that SSU achieves an effective trade-off between unlearning efficacy and general-purpose language abilities, outperforming existing baselines.</abstract>
      <url hash="10796bf0">2025.findings-naacl.288</url>
      <bibkey>dou-etal-2025-avoiding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.288</doi>
    </paper>
    <paper id="289">
      <title>A Context-Aware Contrastive Learning Framework for Hateful Meme Detection and Segmentation</title>
      <author><first>Xuanyu</first><last>Su</last></author>
      <author><first>Yansong</first><last>Li</last></author>
      <author><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Nathalie</first><last>Japkowicz</last><affiliation>American University</affiliation></author>
      <pages>5201-5215</pages>
      <abstract>Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within “Confounder Memes”. To address this, we introduce HateSieve, a new framework designed to enhance the detection and segmentation of hateful elements in memes. HateSieve features a novel Contrastive Meme Generator that creates semantically correlated memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments show that HateSieve not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. Caution: Contains academic discussions of hate speech; viewer discretion advised.</abstract>
      <url hash="9563ec96">2025.findings-naacl.289</url>
      <bibkey>su-etal-2025-context</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.289</doi>
    </paper>
    <paper id="290">
      <title><fixed-case>LLM</fixed-case>-Generated Passphrases That Are Secure and Easy to Remember</title>
      <author><first>Jie S.</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jonas</first><last>Geiping</last><affiliation>ELLIS Institute Tübingen and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <author><first>Micah</first><last>Goldblum</last><affiliation>Columbia University</affiliation></author>
      <author><first>Aniruddha</first><last>Saha</last></author>
      <author><first>Tom</first><last>Goldstein</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>5216-5234</pages>
      <abstract>Automatically generated passwords and passphrases are a cornerstone of IT security. Yet, these passphrases are often hard to remember and see only limited adoption. In this work, we use large language models to generate passphrases with rigorous security guarantees via the computation of the entropy of the output as a metric of the security of the passphrase. We then present a range of practical methods to generate language model outputs with sufficient entropy: raising entropy through in-context examples and generation through a new top-q truncation method. We further verify the influence of prompt construction in steering the output topic and grammatical structure. Finally, we conduct user studies to determine the adoption rates for these LLM-generated passphrases in practice.</abstract>
      <url hash="5be85106">2025.findings-naacl.290</url>
      <bibkey>li-etal-2025-llm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.290</doi>
    </paper>
    <paper id="291">
      <title>Does Data Contamination Detection Work (Well) for <fixed-case>LLM</fixed-case>s? A Survey and Evaluation on Detection Assumptions</title>
      <author><first>Yujuan</first><last>Fu</last></author>
      <author><first>Ozlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>5235-5256</pages>
      <abstract>Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.</abstract>
      <url hash="1995f97b">2025.findings-naacl.291</url>
      <bibkey>fu-etal-2025-data</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.291</doi>
    </paper>
    <paper id="292">
      <title>Representation-to-Creativity (<fixed-case>R</fixed-case>2<fixed-case>C</fixed-case>): Automated Holistic Scoring Model for Essay Creativity</title>
      <author><first>Deokgi</first><last>Kim</last></author>
      <author><first>Joonyoung</first><last>Jo</last></author>
      <author><first>Byung-Won</first><last>On</last><affiliation>Kunsan National University</affiliation></author>
      <author><first>Ingyu</first><last>Lee</last><affiliation>Yeungnam University</affiliation></author>
      <pages>5257-5275</pages>
      <abstract>Despite active research on Automated Essay Scoring (AES), there is a noticeable scarcity of studies focusing on predicting creativity scores for essays. In this study, we develop a new essay rubric specifically designed for assessing creativity in essays. Leveraging this rubric, we construct ground truth data consisting of 5,048 essays. Furthermore, we propose a novel self-supervised learning model that recognizes cluster patterns within the essay embedding space and leverages them for creativity scoring. This approach aims to automatically generate a high-quality training set, thereby facilitating the training of diverse language models. Our experimental findings indicated a substantial enhancement in the assessment of essay creativity, demonstrating an increase in F1-score up to 58% compared to the primary state-of-the-art models across the ASAP and AIHUB datasets.</abstract>
      <url hash="75bd02aa">2025.findings-naacl.292</url>
      <bibkey>kim-etal-2025-representation</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.292</doi>
    </paper>
    <paper id="293">
      <title>From Single to Multi: How <fixed-case>LLM</fixed-case>s Hallucinate in Multi-Document Summarization</title>
      <author><first>Catarina G</first><last>Belém</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Hayate</first><last>Iso</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Seiji</first><last>Maekawa</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last><affiliation>Megagon Labs, Inc</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>5276-5309</pages>
      <abstract>Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from a set of documents. Since no benchmarks exist for investigating hallucinations in MDS, we leverage existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, GPT-3.5-turbo and GPT-4o still generate summaries about 79.45% and 44% of the time, raising concerns about their tendency to fabricate content. To better understand the characteristics of these hallucinations, we conduct a human evaluation of 700+ insights and discover that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches that systematically mitigate hallucinations in MDS.</abstract>
      <url hash="a8c47e2a">2025.findings-naacl.293</url>
      <bibkey>belem-etal-2025-single</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.293</doi>
    </paper>
    <paper id="294">
      <title>Aligning to Constraints for Data-Efficient Language Model Customization</title>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chao</first><last>Shang</last><affiliation>Amazon AWS AI</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sarthak</first><last>Jain</last><affiliation>Profluent Bio</affiliation></author>
      <author><first>Qiang</first><last>Ning</last><affiliation>Jump Trading</affiliation></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Vittorio</first><last>Castelli</last><affiliation>Amazon</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5310-5325</pages>
      <abstract>General-purpose language models (LMs) are aligned to diverse user intents, but fall short when it comes to specific applications. While finetuning is the default method for customized alignment, human annotations are often unavailable in various customization scenarios. Based on the observation that one of the main issues of LM customization is constraint adherence, we investigate the feasibility of using constraints as a bridge from general LMs to customized ones. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs’ capability to adhere to different classes of constraints, thereby improving task performance comparable to or approaching that of finetuning with labeled data.</abstract>
      <url hash="74ba2c5c">2025.findings-naacl.294</url>
      <bibkey>wang-etal-2025-aligning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.294</doi>
    </paper>
    <paper id="295">
      <title>Where is this coming from? Making groundedness count in the evaluation of Document <fixed-case>VQA</fixed-case> models</title>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>School of Computer Science, Carnegie Mellon University and J.P. Morgan Chase</affiliation></author>
      <author><first>Siddharth</first><last>Parekh</last></author>
      <author><first>Pranav</first><last>Shetty</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Zhao</first><last>Jin</last></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>5326-5346</pages>
      <abstract>Document Visual Question Answering (VQA) models have evolved at an impressive rate over the past few years, coming close to or matching human performance on some benchmarks. We argue that common evaluation metrics used by popular benchmarks do not account for the semantic and multimodal groundedness of a model’s outputs. As a result, hallucinations and major semantic errors are treated the same way as well-grounded outputs, and the evaluation scores do not reflect the reasoning capabilities of the model. In response, we propose a new evaluation methodology that accounts for the groundedness of predictions with regard to the semantic characteristics of the output as well as the multimodal placement of the output within the input document. Our proposed methodology is parameterized in such a way that users can configure the score according to their preferences. We validate our scoring methodology using human judgment and show its potential impact on existing popular leaderboards. Through extensive analyses, we demonstrate that our proposed method produces scores that are a better indicator of a model’s robustness and tends to give higher rewards to better-calibrated answers.</abstract>
      <url hash="f6d41bf5">2025.findings-naacl.295</url>
      <bibkey>nourbakhsh-etal-2025-coming</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.295</doi>
    </paper>
    <paper id="296">
      <title>Transformer-based Causal Language Models Perform Clustering</title>
      <author><first>Xinbo</first><last>Wu</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Lav R.</first><last>Varshney</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>5347-5372</pages>
      <abstract>Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still an area of active development. Recent works (Ouyang et al., 2022; Rafailov et al., 2023; Zhang et al., 2023) have shown great improvements in instruction-following capability through additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances, and validate our results in a more realistic setting. We further present applications in pre-training and alignment, inspired by clustering.</abstract>
      <url hash="8de7f6ce">2025.findings-naacl.296</url>
      <bibkey>wu-varshney-2025-transformer</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.296</doi>
    </paper>
    <paper id="297">
      <title>Towards Better Multi-task Learning: A Framework for Optimizing Dataset Combinations in Large Language Models</title>
      <author><first>Zaifu</first><last>Zhan</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <pages>5373-5386</pages>
      <abstract>To efficiently select optimal dataset combinations for enhancing multi-task learning (MTL) performance in large language models, we proposed a novel framework that leverages a neural network to predict the best dataset combinations. The framework iteratively refines the selection, greatly improving efficiency, while being model-, dataset-, and domain-independent. Through experiments on 12 biomedical datasets across four tasks—named entity recognition, relation extraction, event extraction, and text classification—we demonstrate that our approach effectively identifies better combinations, even for tasks that may seem unpromising from a human perspective. This verifies that our framework provides a promising solution for maximizing MTL potential.</abstract>
      <url hash="0f9eb0a5">2025.findings-naacl.297</url>
      <bibkey>zhan-zhang-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.297</doi>
    </paper>
    <paper id="298">
      <title>Gender Bias in Instruction-Guided Speech Synthesis Models</title>
      <author><first>Chun-Yi</first><last>Kuan</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>5387-5413</pages>
      <abstract>Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like “Act like a nurse”. We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts. Our experimental results reveal the model’s tendency to exhibit gender bias for certain occupations. Moreover, models of different sizes show varying degrees of this bias across these occupations.</abstract>
      <url hash="ce2949f1">2025.findings-naacl.298</url>
      <bibkey>kuan-lee-2025-gender</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.298</doi>
    </paper>
    <paper id="299">
      <title><fixed-case>R</fixed-case>eso<fixed-case>F</fixed-case>ilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis</title>
      <author><first>Zeao</first><last>Tu</last><affiliation>Tomorrow Advancing Life</affiliation></author>
      <author><first>Xiangdi</first><last>Meng</last></author>
      <author><first>Yu</first><last>He</last></author>
      <author><first>Zihan</first><last>Yao</last></author>
      <author><first>Tianyu</first><last>Qi</last></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Ming</first><last>Li</last></author>
      <pages>5414-5428</pages>
      <abstract>Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.</abstract>
      <url hash="cc64745c">2025.findings-naacl.299</url>
      <bibkey>tu-etal-2025-resofilter</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.299</doi>
    </paper>
    <paper id="300">
      <title><fixed-case>UCFE</fixed-case>: A User-Centric Financial Expertise Benchmark for Large Language Models</title>
      <author><first>Yuzhe</first><last>Yang</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Nanjing university and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yan</first><last>Hu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yilin</first><last>Guo</last></author>
      <author><first>Ruoli</first><last>Gan</last></author>
      <author><first>Yueru</first><last>He</last></author>
      <author><first>Mingcong</first><last>Lei</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Qianqian</first><last>Xie</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Honghai</first><last>Yu</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>5429-5448</pages>
      <abstract>This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 11 LLMs services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial domain but also provides a robust framework for assessing their performance and user satisfaction.</abstract>
      <url hash="2befe48b">2025.findings-naacl.300</url>
      <bibkey>yang-etal-2025-ucfe</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.300</doi>
    </paper>
    <paper id="301">
      <title><fixed-case>BRIEF</fixed-case>: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression</title>
      <author><first>Yuankai</first><last>Li</last></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5449-5470</pages>
      <abstract>Retrieval-augmented generation (RAG) can supplement large language models (LLMs) by integrating external knowledge. However, as the number of retrieved documents increases, the input length to LLMs grows linearly, causing a dramatic increase in latency and a degradation in long-context understanding. This is particularly serious for multi-hop questions that require a chain of reasoning across documents. To accelerate inference, reduce costs, and minimize distractions, this paper presents BRIEF (Bridging Retrieval and Inference through Evidence Fusion), a lightweight approach that performs query-aware multi-hop reasoning by compressing retrieved documents into highly dense textual summaries to integrate into in-context RAG. To enable learning compression for multi-hop reasoning, we curate synthetic data by extracting atomic propositions that encapsulate distinct factoids from the source documents to compose synthetic summaries. Based on our synthetic data built entirely by open-source models, BRIEF generates more concise summaries and enables a range of LLMs to achieve exceptional open-domain question answering (QA) performance. For example, on HotpotQA, BRIEF improves the compression rate by 2 times compared to the state-of-the-art baseline, while outperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader model. It also generates more concise summaries than proprietary GPT-3.5, while demonstrating nearly identical QA performance.</abstract>
      <url hash="534fa76d">2025.findings-naacl.301</url>
      <bibkey>li-etal-2025-brief</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.301</doi>
    </paper>
    <paper id="302">
      <title>An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via <fixed-case>LLM</fixed-case> as Optimizer</title>
      <author><first>Weipeng</first><last>Jiang</last></author>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Juan</first><last>Zhai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zhengyu</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>5471-5483</pages>
      <abstract>Despite prior safety alignment efforts, LLMs can still generate harmful and unethical content when subjected to jailbreaking attacks. Existing jailbreaking methods fall into two main categories: template-based and optimization-based methods. The former requires significant manual effort and domain knowledge, while the latter, exemplified by GCG, which seeks to maximize the likelihood of harmful LLM outputs through token-level optimization, also encounters several limitations: requiring white-box access, necessitating pre-constructed affirmative phrase, and suffering from low efficiency. This paper introduces ECLIPSE, a novel and efficient black-box jailbreaking method with optimizable suffixes. We employ task prompts to translate jailbreaking objectives into natural language instructions, guiding LLMs to generate adversarial suffixes for malicious queries. A harmfulness scorer provides continuous feedback, enabling LLM self-reflection and iterative optimization to autonomously produce effective suffixes. Experimental results demonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92 across three open-source LLMs and GPT-3.5-Turbo, significantly outperforming GCG by 2.4 times. Moreover, ECLIPSE matches template-based methods in ASR while substantially reducing average attack overhead by 83%, offering superior attack efficiency.</abstract>
      <url hash="3a4dfacc">2025.findings-naacl.302</url>
      <bibkey>jiang-etal-2025-optimizable</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.302</doi>
    </paper>
    <paper id="303">
      <title>Multi-Stage <fixed-case>LLM</fixed-case> Fine-Tuning with a Continual Learning Setting</title>
      <author><first>Changhao</first><last>Guan</last></author>
      <author><first>Chao</first><last>Huang</last></author>
      <author><first>Hongliang</first><last>Li</last></author>
      <author><first>You</first><last>Li</last></author>
      <author><first>Ning</first><last>Cheng</last></author>
      <author><first>Zihe</first><last>Liu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Liu</last><affiliation>University of Science and Technology Beijing</affiliation></author>
      <pages>5484-5498</pages>
      <abstract>In recent years, large language models (LLMs) have made significant progress in knowledge-intensive applications. However, when adapting them to specific domains, we may encounter a multi-stage continuous learning scenario, especially in cases where domain knowledge evolves rapidly.This issue severely limits traditional fine-tuning approaches for LLMs.To overcome this limitation, we propose a new learning paradigm designed specifically for multi-stage continuous learning. This paradigm includes a preference-based learning bias to identify potential knowledge conflicts, as well as a self-distillation-based data augmentation strategy to expand and enrich the training corpus, thereby improving the integration of knowledge-compatible information.In the experiments, we show that our proposed method achieves a significant improvement in accuracy after 7 stages of fine-tuning compared to previous methods, while also demonstrating excellent performance in preserving general knowledge.We have released our code and dataset at Multi-Stage-Learning.</abstract>
      <url hash="b94cd24c">2025.findings-naacl.303</url>
      <bibkey>guan-etal-2025-multi</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.303</doi>
    </paper>
    <paper id="304">
      <title>Constraining Sequential Model Editing with Editing Anchor Compression</title>
      <author><first>Hao-Xiang</first><last>Xu</last></author>
      <author><first>Jun-Yu</first><last>Ma</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhen-Hua</first><last>Ling</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5499-5515</pages>
      <abstract>Large language models (LLMs) struggle with hallucinations due to false or outdated knowledge. Given the high resource demands of retraining these models, there is an increasing focus on developing model editing. However, the general abilities of LLMs across downstream tasks are prone to significant degradation during sequential editing. This paper statistically observes that the parameter matrix after editing exhibits a significant deviation compared to its previous state as the number of edits increases. This serious deviation affects the original knowledge associations within LLMs and leads to the degradation of their general abilities. To this end, a framework termed Editing Anchor Compression (EAC) is proposed to constrain the deviation of the parameter matrix during sequential editing. It compresses the editing information by selecting editing anchors that are important in encoding new relations without deviating too much from the original matrix, thereby preserving the general abilities. Experiments of applying EAC to two popular editing methods on three LLMs across four tasks are conducted. Evaluation results show that EAC effectively minimizes unreasonable deviations caused by model editing, preserving over 70% of the general abilities while better retaining the editing knowledge compared to the original counterpart methods.</abstract>
      <url hash="39a9407d">2025.findings-naacl.304</url>
      <bibkey>xu-etal-2025-constraining</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.304</doi>
    </paper>
    <paper id="305">
      <title><fixed-case>MLKV</fixed-case>: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</title>
      <author><first>Zayd Muhammad Kawakibi</first><last>Zuhri</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad Farid</first><last>Adilazuarda</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>5516-5525</pages>
      <abstract>Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV’s potential for efficient deployment of transformer models at scale.</abstract>
      <url hash="d43397bf">2025.findings-naacl.305</url>
      <bibkey>zuhri-etal-2025-mlkv</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.305</doi>
    </paper>
    <paper id="306">
      <title>Clarify When Necessary: Resolving Ambiguity Through Interaction with <fixed-case>LM</fixed-case>s</title>
      <author><first>Michael JQ</first><last>Zhang</last><affiliation>New York University</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <pages>5526-5543</pages>
      <abstract>In this work, we explore the challenges of developing interactive assistants that resolve ambiguity by asking their users clarifying questions. Specifically, we develop a task-agnostic framework for evaluating a system’s ability to determine when to ask for clarification. Determining when to ask for clarification is a challenging task that requires systems to consider the demands of the individual user (i.e., how much they prioritize speed and usability versus carefulness) and the distribution of interpretations for a given request (i.e., whether an ambiguous request has one dominant, inferable interpretation). Using this framework, we evaluate systems for determining when to clarify across three NLP applications: QA, MT, and NLI. Finally, we introduce present a novel uncertainty estimation approach, IntentSim, that determines the utility of asking a clarifying question by estimating the entropy over user intents. Our method consistently outperforms existing uncertainty estimation approaches at identifying predictions that will benefit from clarification. Furthermore, we find that IntentSim is robust, demonstrating improvements across a wide range of NLP tasks and LMs. Together, our work lays foundation for further studies on clarifying interactions with LM assistants.</abstract>
      <url hash="5f08715d">2025.findings-naacl.306</url>
      <bibkey>zhang-choi-2025-clarify</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.306</doi>
    </paper>
    <paper id="307">
      <title><fixed-case>DOLFIN</fixed-case> - Document-Level Financial Test-Set for Machine Translation</title>
      <author><first>Mariam</first><last>Nakhle</last></author>
      <author><first>Marco</first><last>Dinarelli</last><affiliation>CNRS</affiliation></author>
      <author><first>Raheel</first><last>Qader</last><affiliation>Lingua Custodia</affiliation></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last><affiliation>University of Grenoble-Alpes</affiliation></author>
      <author><first>Hervé</first><last>Blanchon</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <pages>5544-5556</pages>
      <abstract>Despite the strong research interest in document-level Machine Translation (MT), the test-sets dedicated to this task are still scarce. The existing test-sets mainly cover topics from the general domain and fall short on specialised domains, such as legal and financial. Also, despite their document-level aspect, they still follow a sentence-level logic that doesn’t allow for including certain linguistic phenomena such as information reorganisation. In this work, we aim to fill this gap by proposing a novel test-set : DOLFIN. The dataset is built from specialised financial documents and it makes a step towards true document-level MT by abandoning the paradigm of perfectly aligned sentences, presenting data in units of sections rather than sentences. The test-set consists of an average of 1950 aligned sections for five language pairs. We present the detailed data collection pipeline that can serve as inspiration for aligning new document-level datasets. We demonstrate the usefulness and the quality of this test-set with the evaluation of a series of models. Our results show that the test-set is able to discriminate between context-sensitive and context-agnostic models and shows the weaknesses when models fail to accurately translate financial texts. The test-set will be made public for the community.</abstract>
      <url hash="96178cc0">2025.findings-naacl.307</url>
      <bibkey>nakhle-etal-2025-dolfin</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.307</doi>
    </paper>
    <paper id="308">
      <title>Are Large Language Models Effective in Clinical Trial Design? A Study on Baseline Feature Generation</title>
      <author><first>Nafis</first><last>Neehal</last></author>
      <author><first>Bowen</first><last>Wang</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Shayom</first><last>Debopadhaya</last><affiliation>Albany Medical College</affiliation></author>
      <author><first>Corey</first><last>Curran</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vibha</first><last>Anand</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kristin</first><last>Bennett</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>5557-5570</pages>
      <url hash="ca4c7243">2025.findings-naacl.308</url>
      <bibkey>neehal-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.308</doi>
    </paper>
    <paper id="309">
      <title>Lightweight Contenders: Navigating Semi-Supervised Text Mining through Peer Collaboration and Self Transcendence</title>
      <author><first>Qianren</first><last>Mao</last><affiliation>Zhongguancun Laboratory, Beijing, P.R.China. and Beihang University</affiliation></author>
      <author><first>Weifeng</first><last>Jiang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Junnan</first><last>Liu</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Qian</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xianqing</first><last>Wen</last></author>
      <author><first>Jianxin</first><last>Li</last><affiliation>Beihang University</affiliation></author>
      <author><first>Jinhu</first><last>Lu</last></author>
      <pages>5571-5585</pages>
      <abstract>The semi-supervised learning (SSL) strategy in lightweight models requires reducing annotated samples and facilitating cost-effective inference. However, the constraint on model parameters, imposed by the scarcity of training labels, limits the SSL performance. In this paper, we introduce PS-NET, a novel framework tailored for semi-supervised text mining with lightweight models. PS-NET incorporates online distillation to train lightweight student models by imitating the Teacher model. It also integrates an ensemble of student peers that collaboratively instruct each other. Additionally, PS-NET implements a constant adversarial perturbation schema to further self-augmentation by progressive generalizing. Our PS-NET, equipped with a 2-layer distilled BERT, exhibits notable performance enhancements over SOTA lightweight SSL frameworks of FLiText and Disco in SSL text classification with extremely rare labelled data.</abstract>
      <url hash="e883e950">2025.findings-naacl.309</url>
      <bibkey>mao-etal-2025-lightweight</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.309</doi>
    </paper>
    <paper id="310">
      <title>Language-based Valence and Arousal Expressions between the <fixed-case>U</fixed-case>nited <fixed-case>S</fixed-case>tates and <fixed-case>C</fixed-case>hina: a Cross-Cultural Examination</title>
      <author><first>Young Min</first><last>Cho</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Dandan</first><last>Pang</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <author><first>Stuti</first><last>Thapa</last><affiliation>University of Tulsa</affiliation></author>
      <author><first>Garrick</first><last>Sherman</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Louis</first><last>Tay</last><affiliation>Purdue University</affiliation></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>5586-5600</pages>
      <abstract>While affective expressions on social media have been extensively studied, most research has focused on the Western context. This paper explores cultural differences in affective expressions by comparing valence and arousal on Twitter/X (geolocated to the US) and Sina Weibo (in Mainland China). Using the NRC-VAD lexicon to measure valence and arousal, we identify distinct patterns of emotional expression across both platforms. Our analysis reveals a functional representation between valence and arousal, showing a negative offset in contrast to traditional lab-based findings which suggest a positive offset. Furthermore, we uncover significant cross-cultural differences in arousal, with US users displaying higher emotional intensity than Chinese users, regardless of the valence of the content. Finally, we conduct a comprehensive language analysis correlating n-grams and LDA topics with affective dimensions to deepen our understanding of how language and culture shape emotional expression. These findings contribute to a more nuanced understanding of affective communication across cultural and linguistic contexts on social media.</abstract>
      <url hash="79791c93">2025.findings-naacl.310</url>
      <bibkey>cho-etal-2025-language</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.310</doi>
    </paper>
    <paper id="311">
      <title>Chain-of-Rank: Enhancing Large Language Models for Domain-Specific <fixed-case>RAG</fixed-case> in Edge Device</title>
      <author><first>Juntae</first><last>Lee</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Jihwan</first><last>Bang</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Kyuhong</first><last>Shim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Seunghan</first><last>Yang</last><affiliation>Qualcomm AI Research</affiliation></author>
      <author><first>Simyung</first><last>Chang</last><affiliation>QualComm AI Research</affiliation></author>
      <pages>5601-5608</pages>
      <abstract>Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs. While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.</abstract>
      <url hash="d7610faa">2025.findings-naacl.311</url>
      <bibkey>lee-etal-2025-chain-rank</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.311</doi>
    </paper>
    <paper id="312">
      <title><fixed-case>MAL</fixed-case>o<fixed-case>RA</fixed-case>: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning</title>
      <author><first>Xujia</first><last>Wang</last></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Hanqing</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>5609-5626</pages>
      <abstract>Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA have significantly improved the adaptation of LLMs to downstream tasksin a resource-efficient manner. However, in multi-task scenarios, challenges such as training imbalance and the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which combines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by promoting task-specific learning among experts. Despite this, MoLoRA remains inefficient in terms of training speed, parameter utilization, and overall multi-task performance. In this paper, we propose Mixture of Asymmetric Low-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages asymmetric optimization among LoRA experts. MALoRA reduces the number of trainable parameters by 30% to 48%, increases training speed by 1.2x, and matches the computational efficiency of single-task LoRA models. Additionally, MALoRA addresses overfitting issues commonly seen in high-rank configurations, enhancing performance stability. Extensive experiments across diverse multi-task learning scenarios demonstrate that MALoRA consistently outperforms all baseline methods in both inter-domain and intra-domain tasks.</abstract>
      <url hash="1b5c50e3">2025.findings-naacl.312</url>
      <bibkey>wang-etal-2025-malora</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.312</doi>
    </paper>
    <paper id="313">
      <title><fixed-case>L</fixed-case>lama<fixed-case>L</fixed-case>ens: Specialized Multilingual <fixed-case>LLM</fixed-case> for Analyzing News and Social Media Content</title>
      <author><first>Mohamed Bayan</first><last>Kmainasi</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Ali Ezzat</first><last>Shahroor</last></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Sahinur Rahman</first><last>Laskar</last><affiliation>UPES</affiliation></author>
      <author><first>Naeemul</first><last>Hassan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>5627-5649</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable success as general-purpose task solvers across various fields. However, their capabilities remain limited when addressing domain-specific problems, particularly in downstream NLP tasks. Research has shown that models fine-tuned on instruction-based downstream NLP datasets outperform those that are not fine-tuned. While most efforts in this area have primarily focused on resource-rich languages like English and broad domains, little attention has been given to multilingual settings and specific domains. To address this gap, this study focuses on developing a specialized LLM, LlamaLens, for analyzing news and social media content in a multilingual context. To the best of our knowledge, this is the first attempt to tackle both domain specificity and multilinguality, with a particular focus on news and social media. Our experimental setup includes 18 tasks, represented by 52 datasets covering Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the current state-of-the-art (SOTA) on 23 testing sets, and achieves comparable performance on 8 sets. We make the models and resources publicly available for the research community (https://huggingface.co/QCRI).</abstract>
      <url hash="750e5ce6">2025.findings-naacl.313</url>
      <bibkey>kmainasi-etal-2025-llamalens</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.313</doi>
    </paper>
    <paper id="314">
      <title><fixed-case>LLM</fixed-case>s are Biased Teachers: Evaluating <fixed-case>LLM</fixed-case> Bias in Personalized Education</title>
      <author><first>Iain</first><last>Weissburg</last></author>
      <author><first>Sathvika</first><last>Anand</last></author>
      <author><first>Sharon</first><last>Levy</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Haewon</first><last>Jeong</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <pages>5650-5698</pages>
      <abstract>With the increasing adoption of large language models (LLMs) in education, concerns about inherent biases in these models have gained prominence. We evaluate LLMs for bias in the personalized educational setting, specifically focusing on the models’ roles as “teachers.” We reveal significant biases in how models generate and select educational content tailored to different demographic groups, including race, ethnicity, sex, gender, disability status, income, and national origin. We introduce and apply two bias score metrics—Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB)—to analyze 9 open and closed state-of-the-art LLMs. Our experiments, which utilize over 17,000 educational explanations across multiple difficulty levels and topics, uncover that models potentially harm student learning by both perpetuating harmful stereotypes and reversing them. We find that bias is similar for all frontier models, with the highest MAB along income levels while MDB is highest relative to both income and disability status. For both metrics, we find the lowest bias exists for sex/gender and race/ethnicity.</abstract>
      <url hash="c6452370">2025.findings-naacl.314</url>
      <bibkey>weissburg-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.314</doi>
    </paper>
    <paper id="315">
      <title>Preserving Zero-shot Capability in Supervised Fine-tuning for Multi-label Text Classification</title>
      <author><first>Si-An</first><last>Chen</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chih-Jen</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <pages>5699-5712</pages>
      <abstract>Zero-shot multi-label text classification (ZMTC) requires models to predict multiple labels for a document, including labels unseen during training. Previous work assumes that models leveraging label descriptions ensures zero-shot capability. However, we find that supervised methods, despite achieving strong overall performance, lose their zero-shot capability during training, revealing a trade-off between overall and zero-shot performance. To address the issue, we propose OF-DE and OF-LAN, which preserve the zero-shot capabilities of powerful dual encoder and label-wise attention network architectures by freezing the label encoder. Additionally, we introduce a self-supervised auxiliary loss to further improve zero-shot performance. Experiments demonstrate that our approach significantly improves zero-shot performance of supervised methods while maintaining strong overall accuracy.</abstract>
      <url hash="20b01462">2025.findings-naacl.315</url>
      <bibkey>chen-etal-2025-preserving</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.315</doi>
    </paper>
    <paper id="316">
      <title>Data-centric <fixed-case>NLP</fixed-case> Backdoor Defense from the Lens of Memorization</title>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Zhizhi</first><last>Wang</last></author>
      <author><first>Mingyu</first><last>Jin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Juan</first><last>Zhai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <pages>5713-5731</pages>
      <abstract>Backdoor attack is a severe threat to the trustworthiness of DNN-based language models. In this paper, we first extend the definition of memorization of language models from sample-wise to more fine-grained sentence element-wise (e.g., word, phrase, structure, and style), and then point out that language model backdoors are a type of element-wise memorization. Through further analysis, we find that the strength of such memorization is positively correlated to the frequency of duplicated elements in the training dataset. In conclusion, duplicated sentence elements are necessary for successful backdoor attacks. Based on this, we propose a data-centric defense. We first detect trigger candidates in training data by finding memorizable elements, i.e., duplicated elements, and then confirm real triggers by testing if the candidates can activate backdoor behaviors (i.e., malicious elements). Results show that our method outperforms state-of-the-art defenses in defending against different types of NLP backdoors.</abstract>
      <url hash="f62ca2c6">2025.findings-naacl.316</url>
      <bibkey>wang-etal-2025-data-centric</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.316</doi>
    </paper>
    <paper id="317">
      <title>Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs</title>
      <author><first>Sen</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5732-5744</pages>
      <abstract>Two lines of approaches are adopted for complex reasoning with LLMs. One line of work prompts LLMs with various reasoning structures, while the structural outputs can be naturally regarded as intermediate reasoning steps. Another line of work adopt LLM-free declarative solvers to do the reasoning task, rendering higher reasoning accuracy but lacking interpretability due to the black-box nature of the solvers. Aiming to resolve the trade-off between answer accuracy and interpretability, we present a simple extension to the latter line of work. Specifically, we showcase that the intermediate search logs generated by Prolog interpreters can be accessed and interpreted into human-readable reasoning proofs. As long as LLMs correctly translate problem descriptions into Prolog representations, the corresponding reasoning proofs are ensured to be causal and reliable. On two logical reasoning and one arithmetic reasoning datasets, our framework obtains significant improvements in terms of both answer accuracy and reasoning proof accuracy. We released our code at <url>https://github.com/DAMO-NLP-SG/CaRing</url> for future research regarding better reasoning proofs using LLMs.</abstract>
      <url hash="060edadb">2025.findings-naacl.317</url>
      <bibkey>yang-etal-2025-neuro</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.317</doi>
    </paper>
    <paper id="318">
      <title>Infogent: An Agent-Based Framework for Web Information Aggregation</title>
      <author><first>Revanth</first><last>Gangi Reddy</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>5745-5758</pages>
      <abstract>Despite seemingly performant web agents on the task-completion benchmarks, most existing methods evaluate the agents based on a presupposition: the web navigation task consists of a linear sequence of actions with an end state that marks task completion. In contrast, our work focuses on web navigation for information aggregation, wherein the agent must explore different websites to gather information for a complex query. We consider web information aggregation from two different perspectives: i) Direct API-driven Access relies on a text-only view of the Web, leveraging external tools such as Google Search API to navigate the Web and a scraper to extract website contents. (ii) Interactive Visual Access uses screenshots of the webpages and requires interaction with the browser to navigate and access information. Motivated by these diverse information access settings, we introduce Infogent, a novel modular framework for web information aggregation involving three distinct components: Navigator, Extractor, and Aggregator. Experiments on different information access settings demonstrate that Infogent beats an existing SOTA multi-agent search framework by 7% under Direct API-Driven Access on FRAMES and improves over an existing information-seeking web agent by 4.3% under Interactive Visual Access on AssistantBench.</abstract>
      <url hash="43f1d9e0">2025.findings-naacl.318</url>
      <bibkey>gangi-reddy-etal-2025-infogent</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.318</doi>
    </paper>
    <paper id="319">
      <title>On the Role of Key Phrases in Argument Mining</title>
      <author><first>Nilmadhab</first><last>Das</last></author>
      <author><first>Vijaya V</first><last>Saradhi</last></author>
      <author><first>Ashish</first><last>Anand</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <pages>5759-5772</pages>
      <abstract>Argument mining (AM) focuses on analyzing argumentative structures such as Argument Components (ACs) and Argumentative Relations (ARs). Modeling dependencies between ACs and ARs is challenging due to the complex interactions between ACs. Existing approaches often overlook crucial conceptual links, such as key phrases that connect two related ACs, and tend to rely on cartesian product methods to model these dependencies, which can result in class imbalances. To extract key phrases from the AM benchmarks, we employ a prompt-based strategy utilizing an open-source Large Language Model (LLM). Building on this, we propose a unified text-to-text generation framework that leverages Augmented Natural Language (ANL) formatting and integrates the extracted key phrases inside the ANL itself to efficiently solve multiple AM tasks in a joint formulation. Our method sets new State-of-the-Art (SoTA) on three structurally distinct standard AM benchmarks, surpassing baselines by up to 9.5% F1 score, demonstrating its strong potential.</abstract>
      <url hash="655259f6">2025.findings-naacl.319</url>
      <bibkey>das-etal-2025-role</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.319</doi>
    </paper>
    <paper id="320">
      <title><fixed-case>T</fixed-case>ab<fixed-case>C</fixed-case>omp: A Dataset for Visual Table Reading Comprehension</title>
      <author><first>Somraj</first><last>Gautam</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author><first>Abhishek</first><last>Bhandari</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author><first>Gaurav</first><last>Harit</last></author>
      <pages>5773-5780</pages>
      <abstract>Reaching a human-level understanding of real-world documents necessitates effective machine reading comprehension, yet recent developments in this area often struggle with table images. In response, we introduce the Visual Table Reading Comprehension (TabComp) dataset, which includes table images, questions, and generative answers designed to evaluate OCR-free models. Unlike general Visual Question Answering (VQA) datasets, TabComp uniquely focuses on table images, fostering the development of systems which obviate the use of optical character recognition (OCR) technology, which often struggles with complex table layouts. Our findings reveal that current OCR-free models perform poorly on TabComp, highlighting the need for robust, specialized models for accurate table reading comprehension. We propose TabComp as a benchmark for evaluating OCR-free models in table reading comprehension and encourage the research community to collaborate on developing more effective solutions. The code and data are available at - https://github.com/dialabiitj/TabComp/</abstract>
      <url hash="6c51e94a">2025.findings-naacl.320</url>
      <bibkey>gautam-etal-2025-tabcomp</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.320</doi>
    </paper>
    <paper id="321">
      <title><fixed-case>R</fixed-case>ank<fixed-case>A</fixed-case>daptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned <fixed-case>LLM</fixed-case>s via Performance Model</title>
      <author><first>Changhai</first><last>Zhou</last></author>
      <author><first>Shijie</first><last>Han</last></author>
      <author><first>Lining</first><last>Yang</last></author>
      <author><first>Yuhua</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Cheng</last></author>
      <author><first>Yibin</first><last>Wang</last></author>
      <author><first>Hongguang</first><last>Li</last><affiliation>JF SmartInvest Holdings</affiliation></author>
      <pages>5781-5795</pages>
      <abstract>The efficient compression of large language models (LLMs) has become increasingly popular. However, recovering the performance of compressed LLMs remains a major challenge. The current practice in LLM compression entails the implementation of structural pruning, complemented by a recovery phase that leverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning’s uneven modification of model architecture, coupled with standard LoRA’s fixed configuration allocation across layers in an online pipeline, leads to suboptimal performance in various downstream tasks for pruned models. To address this challenge, we introduce RankAdaptor, a hierarchical rank allocation method that enables efficient fine-tuning of pruned LLMs according to layerwise specific recovery requirements. We employ a performance model that conducts offline meta-learning and online incremental learning to explore optimal rank values for each layer. Comprehensive experiments on popular benchmarks show that RankAdaptor consistently outperforms state-of-the-art methods across a variety of pruning settings and LLM architectures, with improvements ranging from 0.7% to 5.5%.</abstract>
      <url hash="95b51e12">2025.findings-naacl.321</url>
      <bibkey>zhou-etal-2025-rankadaptor</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.321</doi>
    </paper>
    <paper id="322">
      <title>Rationale Behind Essay Scores: Enhancing <fixed-case>S</fixed-case>-<fixed-case>LLM</fixed-case>’s Multi-Trait Essay Scoring with Rationale Generated by <fixed-case>LLM</fixed-case>s</title>
      <author><first>SeongYeub</first><last>Chu</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong Woo</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Bryan</first><last>Wong</last></author>
      <author><first>Mun Yong</first><last>Yi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>5796-5814</pages>
      <abstract>Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays. The code is available at <b>
          <url>https://github.com/BBeeChu/RMTS.git</url></b>.</abstract>
      <url hash="be5b4dca">2025.findings-naacl.322</url>
      <bibkey>chu-etal-2025-rationale</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.322</doi>
    </paper>
    <paper id="323">
      <title><fixed-case>MTPC</fixed-case>hat: A Multimodal Time-Aware Persona Dataset for Conversational Agents</title>
      <author><first>Wanqi</first><last>Yang</last></author>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>5815-5826</pages>
      <abstract>Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model’s ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.</abstract>
      <url hash="36028ecc">2025.findings-naacl.323</url>
      <bibkey>yang-etal-2025-mtpchat</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.323</doi>
    </paper>
    <paper id="324">
      <title><fixed-case>M</fixed-case>eta<fixed-case>A</fixed-case>lign: Align Large Language Models with Diverse Preferences during Inference Time</title>
      <author><first>Mozhi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Chenkun</first><last>Tan</last></author>
      <author><first>Mianqiu</first><last>Huang</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Yaqian</first><last>Zhou</last><affiliation>Fudan University, Tsinghua University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>5827-5845</pages>
      <abstract>Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is essential. Existing alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed predefined preferences directly within the model’s parameters. These methods, however, often result in a static alignment that can not account for the diversity of human preferences in practical applications.In response to this challenge, we propose an effective method, <b>MetaAlign</b>, which aims to help LLMs dynamically align with various explicit or implicit preferences specified at inference time. Experimental results show that LLMs optimized on our meticulously constructed MetaAlign Dataset can effectively align with any preferences specified at the inference stage, validating the feasibility of MetaAlign. We hope that our work can provide some insights into the alignment of language models.</abstract>
      <url hash="5444c003">2025.findings-naacl.324</url>
      <bibkey>zhang-etal-2025-metaalign</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.324</doi>
    </paper>
    <paper id="325">
      <title><fixed-case>MAQA</fixed-case>: Evaluating Uncertainty Quantification in <fixed-case>LLM</fixed-case>s Regarding Data Uncertainty</title>
      <author><first>Yongjin</first><last>Yang</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>Sogang University</affiliation></author>
      <pages>5846-5863</pages>
      <abstract>Despite the massive advancements in large language models (LLMs), they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on single-labeled questions, which removes data uncertainty—the irreducible randomness often present in user queries, which can arise from factors like multiple possible answers. This limitation may cause uncertainty quantification results to be unreliable in practical settings. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, **MAQA**, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that previous methods relatively struggle compared to single-answer settings, though this varies depending on the task. Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty.</abstract>
      <url hash="24f5b509">2025.findings-naacl.325</url>
      <bibkey>yang-etal-2025-maqa</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.325</doi>
    </paper>
    <paper id="326">
      <title>Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning</title>
      <author><first>Hyundong Justin</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Karishma</first><last>Sharma</last></author>
      <author><first>Nicolaas Paul</first><last>Jedema</last><affiliation>Amazon</affiliation></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon AGI</affiliation></author>
      <pages>5864-5885</pages>
      <abstract>Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users’ styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user’s style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5% against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, presents a novel yet simple approach for personalized alignment.</abstract>
      <url hash="76beb0b3">2025.findings-naacl.326</url>
      <bibkey>cho-etal-2025-tuning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.326</doi>
    </paper>
    <paper id="327">
      <title>Causal Inference with Large Language Model: A Survey</title>
      <author><first>Jing</first><last>Ma</last><affiliation>Case Western Reserve University</affiliation></author>
      <pages>5886-5898</pages>
      <abstract>Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.</abstract>
      <url hash="4a6f88b0">2025.findings-naacl.327</url>
      <bibkey>ma-2025-causal</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.327</doi>
    </paper>
    <paper id="328">
      <title>Ask Optimal Questions: Aligning Large Language Models with Retriever’s Preference in Conversation</title>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <author><first>Byeongguk</first><last>Jeon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>5899-5921</pages>
      <abstract>Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever’s Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers’ preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers’ Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers’ preferences as feedback. The resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches, including GPT-3.5.</abstract>
      <url hash="7f243ef1">2025.findings-naacl.328</url>
      <bibkey>yoon-etal-2025-ask</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.328</doi>
    </paper>
    <paper id="329">
      <title>Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific <fixed-case>RAG</fixed-case></title>
      <author><first>Kushagra</first><last>Bhushan</last></author>
      <author><first>Yatin</first><last>Nandwani</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Khandelwal</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sonam</first><last>Gupta</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Gaurav</first><last>Pandey</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Raghu</last><affiliation>IBM Research - New Delhi</affiliation></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>5922-5943</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways – context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we finetune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10% relative gain in token-level recall while preserving the LLM’s generalization capabilities.</abstract>
      <url hash="e7572056">2025.findings-naacl.329</url>
      <bibkey>bhushan-etal-2025-systematic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.329</doi>
    </paper>
    <paper id="330">
      <title>Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models</title>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>5944-5964</pages>
      <abstract>Through numerous endeavors, large language models (LLMs) have witnessed significant advancements in their instruction-following capability. However, we discern that LLMs are prone to generate responses to instruction-formatted statements in an instinctive manner, rather than comprehending the underlying user intention reside within the given instructions. We also recognize that the significance of instruction understanding capability is largely overlooked in most of LLM evaluation benchmarks. To ensure more comprehensive evaluation on the instruction understanding capability of LLM, we propose Intention of Instruction (IntInst) benchmark, which primary objective is to distinguish the appropriate instruction that accurately instruct to generate a given context. IntInst presents four instruction candidates and requires LLMs to select one among them. Through extensive experiments with several instruction-tuned LLMs, we reveal that most LLMs struggle to grasp the actual intention concealed in the instruction and thoroughly analyze the factors influencing instruction understanding.</abstract>
      <url hash="723b7c23">2025.findings-naacl.330</url>
      <bibkey>moon-etal-2025-find</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.330</doi>
    </paper>
    <paper id="331">
      <title>Long-Tail Crisis in Nearest Neighbor Language Models</title>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>Future Corporation and Tohoku University</affiliation></author>
      <author><first>Hiroyuki</first><last>Deguchi</last><affiliation>NTT Communications</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>5965-5978</pages>
      <abstract>The <tex-math>k</tex-math>-nearest-neighbor language model (<tex-math>k</tex-math>NN-LM), one of the retrieval-augmented language models, improves the perplexity for given text by directly accessing a large datastore built from any text data during inference.A widely held hypothesis for the success of <tex-math>k</tex-math>NN-LM is that its explicit memory, i.e., the datastore, enhances predictions for long-tail phenomena.However, prior works have primarily shown its ability to retrieve long-tail contexts, leaving the model’s performance remain underexplored in estimating the probabilities of long-tail target tokens during inference.In this paper, we investigate the behavior of <tex-math>k</tex-math>NN-LM on low-frequency tokens, examining prediction probability, retrieval accuracy, and token distribution in the datastore.Our experimental results reveal that <tex-math>k</tex-math>NN-LM does not improve prediction performance for low-frequency tokens but mainly benefits high-frequency tokens regardless of long-tail contexts in the datastore.</abstract>
      <url hash="999353cb">2025.findings-naacl.331</url>
      <bibkey>nishida-etal-2025-long</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.331</doi>
    </paper>
    <paper id="332">
      <title>Keep Guessing? When Considering Inference Scaling, Mind the Baselines</title>
      <author><first>Gal</first><last>Yona</last><affiliation>Research, Google</affiliation></author>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Omer</first><last>Levy</last><affiliation>Facebook</affiliation></author>
      <author><first>Roee</first><last>Aharoni</last><affiliation>Google</affiliation></author>
      <pages>5979-5991</pages>
      <abstract>Scaling inference compute in large language models (LLMs) through repeated sampling consistently increases the coverage (fraction of problems solved) as the number of samples increases. We conjecture that this observed improvement is partially due to the answer distribution of standard evaluation benchmarks, which is skewed towards a relatively small set of common answers. To test this conjecture, we define a baseline that enumerates answers according to their prevalence in the training set. Experiments spanning two domains – mathematical reasoning and factual knowledge – reveal that this baseline outperforms repeated model sampling for some LLMs, while the coverage for others is on par with that of a mixture strategy that obtains <tex-math>k</tex-math> answers by using only 10 model samples and similarly guessing the remaining <tex-math>k-10</tex-math> attempts via enumeration. Our baseline enables a more accurate measurement of how much repeated sampling improves coverage in such settings beyond prompt-agnostic guessing.</abstract>
      <url hash="8ac1001f">2025.findings-naacl.332</url>
      <bibkey>yona-etal-2025-keep</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.332</doi>
    </paper>
    <paper id="333">
      <title>Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey</title>
      <author><first>Ruiyao</first><last>Xu</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <pages>5992-6012</pages>
      <abstract>Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers: https://github.com/rux001/Awesome-LLM-Anomaly-OOD-Detection.</abstract>
      <url hash="fec52e7e">2025.findings-naacl.333</url>
      <bibkey>xu-ding-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.333</doi>
    </paper>
    <paper id="334">
      <title>Time-aware <fixed-case>R</fixed-case>e<fixed-case>A</fixed-case>ct Agent for Temporal Knowledge Graph Question Answering</title>
      <author><first>Qianyi</first><last>Hu</last></author>
      <author><first>Xinhui</first><last>Tu</last></author>
      <author><first>Cong</first><last>Guo</last></author>
      <author><first>Shunping</first><last>Zhang</last></author>
      <pages>6013-6024</pages>
      <abstract>Temporal knowledge graph question answering (TKGQA) addresses time-sensitive queries using knowledge bases. Although large language models (LLMs) and LLM-based agents such as ReAct have shown potential for TKGQA, they often lack sufficient temporal constraints in the retrieval process. To tackle this challenge, we propose TempAgent, a novel autonomous agent framework built on LLMs that enhances their ability to conduct temporal reasoning and comprehension. By integrating temporal constraints into information retrieval, TempAgent effectively discards irrelevant material and concentrates on extracting pertinent temporal and factual information. We evaluate our framework on the MultiTQ dataset, a real-world multi-granularity TKGQA benchmark, using a fully automated setup. Our experimental results reveal the remarkable effectiveness of our approach: TempAgent achieves a 41.3% improvement over the baseline model and a 32.2% gain compared to the Abstract Reasoning Induction (ARI) method. Moreover, our method attains an accuracy of 70.2% on the @hit1 metric, underscoring its substantial advantage in addressing time-aware TKGQA tasks.</abstract>
      <url hash="755d037d">2025.findings-naacl.334</url>
      <bibkey>qianyihu-etal-2025-time</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.334</doi>
    </paper>
    <paper id="335">
      <title><fixed-case>SG</fixed-case>-<fixed-case>FSM</fixed-case>: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine</title>
      <author><first>Xiaochen</first><last>Wang</last></author>
      <author><first>Junqing</first><last>He</last><affiliation>International Digital Econemy Academy</affiliation></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <author><first>Yiru</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiangdi</first><last>Meng</last></author>
      <author><first>Kunhao</first><last>Pan</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>6025-6037</pages>
      <abstract>Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs’ performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.</abstract>
      <url hash="0b1932a7">2025.findings-naacl.335</url>
      <bibkey>wang-etal-2025-sg</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.335</doi>
    </paper>
    <paper id="336">
      <title>Dynamic Strategy Planning for Efficient Question Answering with Large Language Models</title>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>Pradyot</first><last>Prakash</last><affiliation>Meta</affiliation></author>
      <author><first>Alexander</first><last>Radovic</last><affiliation>Facebook</affiliation></author>
      <author><first>Akshay</first><last>Shekher</last></author>
      <author><first>Denis</first><last>Savenkov</last><affiliation>Meta</affiliation></author>
      <pages>6038-6059</pages>
      <abstract>Research has shown an effectiveness of reasoning (e.g. Chain-of-Thought), planning (e.g. SelfAsk) and retrieval augmented generation strategies to improve performance of Large Language Models (LLMs) on various tasks, such as question answering. However, using a single fixed strategy for answering all different kinds of questions is sub-optimal in performance and inefficient in terms of generated tokens and retrievals. In our work, we propose a novel technique, DyPlan, to induce a dynamic strategy selection process in LLMs for cost-effective question-answering. DyPlan incorporates an initial decision step to select the most suitable strategy conditioned on the input question and guides the LLM’s response generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal verification and correction process to further enrich the generated answer. Experimentation on three prominent multi-hop question answering (MHQA) datasets reveals how DyPlan can improve model performance by 7-13% while reducing the cost by 11-32% relative to the best baseline model.</abstract>
      <url hash="1f02d0c7">2025.findings-naacl.336</url>
      <bibkey>parekh-etal-2025-dynamic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.336</doi>
    </paper>
    <paper id="337">
      <title>Can <fixed-case>I</fixed-case> Introduce My Boyfriend to My Grandmother? Evaluating Large Language Models Capabilities on <fixed-case>I</fixed-case>ranian Social Norm Classification</title>
      <author><first>Hamidreza</first><last>Saffari</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Mohammadamin</first><last>Shafiei</last><affiliation>University of Milan</affiliation></author>
      <author><first>Donya</first><last>Rooein</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Francesco</first><last>Pierri</last><affiliation>Politecnico di Milano</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>6060-6074</pages>
      <abstract>Creating globally inclusive AI systems demands datasets reflecting diverse social norms. Iran, with its unique cultural blend, offers an ideal case study, with Farsi adding linguistic complexity. In this work, we introduce the Iranian Social Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms, including environments, demographic features, and scope annotation, alongside English translations. Our evaluation of 6 Large Language Models (LLMs) in classifying Iranian social norms, using a variety of prompts, uncovered critical insights into the impact of geographic and linguistic context. Results revealed a substantial performance gap in LLMs’ comprehension of Iranian norms. Notably, while the geographic context in English prompts enhanced the performance, this effect was absent in Farsi, pointing to nuanced linguistic challenges. Particularly, performance was significantly worse for Iran-specific norms, emphasizing the importance of culturally tailored datasets. As the first Farsi dataset for social norm classification, ISN will facilitate crucial cross-cultural analyses, shedding light on how values differ across contexts and cultures.</abstract>
      <url hash="c1648c2b">2025.findings-naacl.337</url>
      <bibkey>saffari-etal-2025-introduce</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.337</doi>
    </paper>
    <paper id="338">
      <title><fixed-case>PLD</fixed-case>+: Accelerating <fixed-case>LLM</fixed-case> Inference by Leveraging Language Model Artifacts</title>
      <author><first>Shwetha</first><last>Somasundaram</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Anirudh</first><last>Phukan</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Apoorv</first><last>Saxena</last><affiliation>Adobe Systems</affiliation></author>
      <pages>6075-6089</pages>
      <abstract>To reduce the latency associated with autoretrogressive LLM inference, speculative decoding has emerged as a novel decoding paradigm, where future tokens are drafted and verified in parallel. However, the practical deployment of speculative decoding is hindered by its requirements for additional computational resources and fine-tuning, which limits its out-of-the-box usability. To address these challenges, we present PLD+, a suite of novel algorithms developed to accelerate the inference process of LLMs, particularly for input-guided tasks. These tasks, which include code editing, text editing, summarization, etc., often feature outputs with substantial overlap with their inputs—an attribute PLD+ is designed to exploit. PLD+ also leverages the artifacts (attention and hidden states) generated during inference to accelerate inference speed. We test our approach on five input-guided tasks and through extensive experiments we find that PLD+ outperforms all tuning-free approaches. In the greedy setting, it even outperforms the state-of-the-art tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31 in terms of avg. speedup). Our approach is tuning free, does not require any additional compute and can easily be used for accelerating inference of any LLM.</abstract>
      <url hash="7be951a7">2025.findings-naacl.338</url>
      <bibkey>somasundaram-etal-2025-pld</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.338</doi>
    </paper>
    <paper id="339">
      <title>Adapting <fixed-case>LLM</fixed-case> Agents with Universal Communication Feedback</title>
      <author><first>Kuan</first><last>Wang</last></author>
      <author><first>Yadong</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Santacroce</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <pages>6090-6107</pages>
      <abstract>Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication (LTC). We design a universal buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to explore and update its policy in an given environment. To optimize agent interactions for task-specific learning with our universal buffer and pipeline, we introduce diverse communication patterns tailored for both single-agent and multi-agent environments. We evaluate the efficacy of our LTC approach on four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration), Chameleon (multi-agent competition), and GSM8k (multi-agent teacher-student). On these data sets, LTC outperforms the supervised instruction fine-tuning baselines by 3.6% to 12%. These results highlight the versatility and efficiency of LTC in facilitating online adaptation for LLM agents.</abstract>
      <url hash="d5ccd885">2025.findings-naacl.339</url>
      <bibkey>wang-etal-2025-adapting</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.339</doi>
    </paper>
    <paper id="340">
      <title>Ignore the <fixed-case>KL</fixed-case> Penalty! Boosting Exploration on Critical Tokens to Enhance <fixed-case>RL</fixed-case> Fine-Tuning</title>
      <author><first>Jean</first><last>Vassoyan</last></author>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Roman</first><last>Plaud</last></author>
      <pages>6108-6118</pages>
      <abstract>The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of “critical tokens” which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.</abstract>
      <url hash="30ea21be">2025.findings-naacl.340</url>
      <bibkey>vassoyan-etal-2025-ignore</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.340</doi>
    </paper>
    <paper id="341">
      <title><fixed-case>S</fixed-case>ea<fixed-case>E</fixed-case>xam and <fixed-case>S</fixed-case>ea<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>LLM</fixed-case>s with Local Multilingual Questions in <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sia</title>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>6119-6136</pages>
      <abstract>This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.</abstract>
      <url hash="ff27cc52">2025.findings-naacl.341</url>
      <bibkey>liu-etal-2025-seaexam</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.341</doi>
    </paper>
    <paper id="342">
      <title>Learning to Search Effective Example Sequences for In-Context Learning</title>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <author><first>Ankita</first><last>Sinha</last><affiliation>intuit</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <pages>6137-6146</pages>
      <abstract>Large language models (LLMs) demonstrate impressive few-shot learning capabilities, but their performance varies widely based on the sequence of in-context examples. Key factors influencing this include the sequence’s length, composition, and arrangement, as well as its relation to the specific query. Existing methods often tackle these factors in isolation, overlooking their interdependencies. Moreover, the extensive search space for selecting optimal sequences complicates the development of a holistic approach. In this work, we introduce Beam Search-based Example Sequence Constructor (BESC), a novel method for learning to construct optimal example sequences. addresses all key factors involved in sequence selection by considering them jointly during inference, while incrementally building the sequence. This design enables the use of beam search to significantly reduce the complexity of the search space. Experiments across various datasets and language models show notable improvements in performance.</abstract>
      <url hash="b1d78cf5">2025.findings-naacl.342</url>
      <bibkey>gao-etal-2025-learning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.342</doi>
    </paper>
    <paper id="343">
      <title>From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models</title>
      <author><first>Harsh Nishant</first><last>Lalai</last></author>
      <author><first>Aashish</first><last>Anantha Ramakrishnan</last><affiliation>Pennsylvania State University, Pennsylvania State University</affiliation></author>
      <author><first>Raj Sanjay</first><last>Shah</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>6147-6160</pages>
      <abstract>With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques through a comprehensive survey of the research literature. Our work has two key advantages: (1) We analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, and watermarking addition and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research protecting text authorship. This extensive coverage and detailed analysis sets our work apart, outlining the evolving landscape of text watermarking in Language Models.</abstract>
      <url hash="e595c9d3">2025.findings-naacl.343</url>
      <bibkey>lalai-etal-2025-intentions</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.343</doi>
    </paper>
    <paper id="344">
      <title><fixed-case>M</fixed-case>-<fixed-case>IFE</fixed-case>val: Multilingual Instruction-Following Evaluation</title>
      <author><first>Antoine</first><last>Dussolle</last></author>
      <author><first>A.</first><last>Cardeña</last></author>
      <author><first>Shota</first><last>Sato</last><affiliation>Lightblue</affiliation></author>
      <author><first>Peter</first><last>Devine</last></author>
      <pages>6161-6176</pages>
      <abstract>Instruction following is a core capability of modern Large language models (LLMs), making evaluating this capability essential to understanding these models. The Instruction Following Evaluation (IFEval) benchmark from the literature does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement. However, it only includes English instructions, limiting its ability to assess LLMs in other languages.We propose the Multilingual Instruction Following Evaluation (M-IFEval) benchmark, expanding the evaluation to French, Japanese, and Spanish, with both general and language-specific instructions. Applying this benchmark to 8 state-of-the-art LLMs, we find that benchmark performance across languages and instruction types can vary widely, underscoring the importance of a multilingual benchmark for evaluating LLMs in a diverse cultural context.</abstract>
      <url hash="790ff7c7">2025.findings-naacl.344</url>
      <bibkey>dussolle-etal-2025-ifeval</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.344</doi>
    </paper>
    <paper id="345">
      <title>Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language</title>
      <author><first>Zhiqiang</first><last>Zhong</last></author>
      <author><first>Simon Sataa-Yu</first><last>Larsen</last></author>
      <author><first>Haoyu</first><last>Guo</last></author>
      <author><first>Tao</first><last>Tang</last></author>
      <author><first>Kuangyu</first><last>Zhou</last><affiliation>Microsoft</affiliation></author>
      <author><first>Davide</first><last>Mottin</last><affiliation>Aarhus University</affiliation></author>
      <pages>6177-6194</pages>
      <abstract>Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA<tex-math>^3</tex-math>, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA<tex-math>^3</tex-math> by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.Experimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA<tex-math>^3</tex-math> leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA<tex-math>^3</tex-math> notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility.</abstract>
      <url hash="b4e2e79e">2025.findings-naacl.345</url>
      <bibkey>zhong-etal-2025-automatic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.345</doi>
    </paper>
    <paper id="346">
      <title>Let Modalities Teach Each Other: Modal-Collaborative Knowledge Extraction and Fusion for Multimodal Knowledge Graph Completion</title>
      <author><first>Guoliang</first><last>Zhu</last></author>
      <author><first>Tao</first><last>Ren</last></author>
      <author><first>Dandan</first><last>Wang</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Hu</last><affiliation>Institute of Software, CAS</affiliation></author>
      <pages>6195-6207</pages>
      <abstract>Multimodal knowledge graph completion (MKGC) aims to predict missing triples in MKGs using multimodal information. Recent research typically either extracts information from each modality separately to predict, then ensembles the predictions at the decision stage, or projects multiple modalities into a unified feature space to learn multimodal representations for prediction. However, these methods usually overlook the intrinsic correlation between modalities in MKGs which should be leveraged in both unimodal knowledge extraction and multimodal knowledge fusion. Motivated by this, we propose a noval Modal-collaborative knowledge learning (Moodle) framework for MKGC, the key idea of which is to foster mutual guidance and collaboration during unimodal knowledge extraction, to let each modality acquire distinct and complementary knowledge that subsequently enhances the multimodal knowledge fusion. Specifically, Moodle preserves the representations of different modalities to learn unimodal knowledge while modeling the mutual guidance through multi-task learning. Furthermore, Moodle performs multimodal knowledge fusion and prediction guided by unimodal knowledge, capturing their synergistic relationships and acquire fine-grained semantic knowledge through contrastive learning. Extensive experiments on three real-world datasets demonstrate the advantages of Moodle over state-of-the-art methods.</abstract>
      <url hash="0b585c44">2025.findings-naacl.346</url>
      <bibkey>zhu-etal-2025-modalities</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.346</doi>
    </paper>
    <paper id="347">
      <title>Modeling the Differential Prevalence of Online Supportive Interactions in Private Instant Messages of Adolescents</title>
      <author><first>Ondrej</first><last>Sotolar</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Michał</first><last>Tkaczyk</last></author>
      <author><first>Jaromír</first><last>Plhák</last><affiliation>Masaryk University</affiliation></author>
      <author><first>David</first><last>Smahel</last></author>
      <pages>6208-6226</pages>
      <abstract>This paper focuses on modeling gender-based and pair-or-group disparities in online supportive interactions among adolescents. To address the limitations of conventional social science methods in handling large datasets, this research employs language models to detect supportive interactions based on the Social Support Behavioral Code and to model their distribution. The study conceptualizes detection as a classification task, constructs a new dataset, and trains predictive models. The novel dataset comprises 196,772 utterances from 2165 users collected from Instant Messenger apps. The results show that the predictions of language models can be used to effectively model the distribution of supportive interactions in private online dialogues. As a result, this study provides new computational evidence that supports the theory that supportive interactions are more prevalent in online female-to-female conversations. The findings advance our understanding of supportive interactions in adolescent communication and present methods to automate the analysis of large datasets, opening new research avenues in computational social science.</abstract>
      <url hash="2e70ef8a">2025.findings-naacl.347</url>
      <bibkey>sotolar-etal-2025-modeling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.347</doi>
    </paper>
    <paper id="348">
      <title>Dynamic Feature Fusion for Sign Language Translation Using <fixed-case>H</fixed-case>yper<fixed-case>N</fixed-case>etworks</title>
      <author><first>Ruiquan</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Zhao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Zhicong</first><last>Wu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Haoqi</first><last>Zhang</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>6227-6239</pages>
      <abstract>This paper presents an efficient dual-stream early fusion method for sign language translation. Inspired by the brain’s ability to process color, shape, and motion simultaneously, the method explores complex dependencies between RGB and keypoint streams, improving speed and efficiency. A key challenge is extracting complementary features from both streams while ensuring global semantic consistency to avoid conflicts and improve generalization. To address this issue, we propose a hypernetwork-based fusion strategy that effectively extracts salient features from RGB and keypoint streams, alongside a partial shortcut connection training method to strengthen the complementary information between the dual streams. Additionally, we introduce self-distillation and SST contrastive learning to maintain feature advantages while aligning the global semantic space. Experiments show that our method achieves state-of-the-art performance on two public sign language datasets, reducing model parameters by about two-thirds.</abstract>
      <url hash="3b928a91">2025.findings-naacl.348</url>
      <bibkey>zhang-etal-2025-dynamic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.348</doi>
    </paper>
    <paper id="349">
      <title>Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models</title>
      <author><first>Sonam</first><last>Gupta</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Yatin</first><last>Nandwani</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asaf</first><last>Yehudai</last></author>
      <author><first>Dinesh</first><last>Khandelwal</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Raghu</last><affiliation>IBM Research - New Delhi</affiliation></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>6240-6249</pages>
      <abstract>Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization.S3FT leverages the existence of multiple valid responses to a query.By utilizing the model’s correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples.The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to 4.4 on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. 2.5, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.</abstract>
      <url hash="e601902b">2025.findings-naacl.349</url>
      <bibkey>gupta-etal-2025-selective</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.349</doi>
    </paper>
    <paper id="350">
      <title><fixed-case>P</fixed-case>roverb<fixed-case>E</fixed-case>val: Exploring <fixed-case>LLM</fixed-case> Evaluation Challenges for Low-resource Language Understanding</title>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Yonas</first><last>Chanie</last></author>
      <author><first>Bontu Fufa</first><last>Balcha</last></author>
      <author><first>Negasi Haile</first><last>Abadi</last><affiliation>Lesan AI</affiliation></author>
      <author><first>Henok Biadglign</first><last>Ademtew</last></author>
      <author><first>Mulubrhan Abebe</first><last>Nerea</last></author>
      <author><first>Debela Desalegn</first><last>Yadeta</last></author>
      <author><first>Derartu Dagne</first><last>Geremew</last><affiliation>Adama Science and Technology University and Gebeya Inc.</affiliation></author>
      <author><first>Assefa Atsbiha</first><last>Tesfu</last></author>
      <author><first>Philipp</first><last>Slusallek</last><affiliation>German Research Center for Artificial Intelligence (DFKI) and Saarland University</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>6250-6266</pages>
      <url hash="8e6da195">2025.findings-naacl.350</url>
      <bibkey>azime-etal-2025-proverbeval</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.350</doi>
    </paper>
    <paper id="351">
      <title><fixed-case>MRE</fixed-case>-<fixed-case>MI</fixed-case>: A Multi-image Dataset for Multimodal Relation Extraction in Social Media Posts</title>
      <author><first>Shizhou</first><last>Huang</last></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Donghua University, Shanghai</affiliation></author>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Yang</first><last>Yu</last></author>
      <author><first>Xin Alex</first><last>Lin</last></author>
      <pages>6267-6277</pages>
      <abstract>Despite recent advances in Multimodal Relation Extraction (MRE), existing datasets and approaches primarily focus on single-image scenarios, overlooking the prevalent real-world cases where relationships are expressed through multiple images alongside text. To address this limitation, we present MRE-MI, a novel human-annotated dataset that includes both multi-image and single-image instances for relation extraction. Beyond dataset creation, we establish comprehensive baselines and propose a simple model named Global and Local Relevance-Modulated Attention Model (GLRA) to address the new challenges in multi-image scenarios. Our extensive experiments reveal that incorporating multiple images substantially improves relation extraction in multi-image scenarios. Furthermore, GLRA achieves state-of-the-art results on MRE-MI, demonstrating its effectiveness. The datasets and source code can be found at https://github.com/JinFish/MRE-MI.</abstract>
      <url hash="5a8f5c7e">2025.findings-naacl.351</url>
      <bibkey>huang-etal-2025-mre</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.351</doi>
    </paper>
    <paper id="352">
      <title>Discrete Diffusion Language Model for Efficient Text Summarization</title>
      <author><first>Do Huu</first><last>Dat</last></author>
      <author><first>Duc Anh</first><last>Do</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <pages>6278-6290</pages>
      <abstract>While diffusion models excel at conditionally generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. This work addresses the limitations of prior discrete diffusion models for conditional long-text generation, particularly in the long abstractive summarization task. Despite faster decoding speeds compared to autoregressive methods, previous discrete diffusion models failed on the abstractive summarization task due to the incompatibility between the backbone architectures and the random noising process. To overcome these challenges, we introduce a novel semantic-aware noising process that enables Transformer backbones to handle long sequences effectively. Additionally, we propose CrossMamba, an adaptation of the Mamba model to the encoder-decoder paradigm, which integrates seamlessly with the random absorbing noising process. Our approaches outperform existing discrete diffusion models on three benchmark summarization datasets: Gigaword, CNN/DailyMail, and Arxiv, while also achieving much faster inference speed compared to autoregressive models.</abstract>
      <url hash="35057b6c">2025.findings-naacl.352</url>
      <bibkey>dat-etal-2025-discrete</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.352</doi>
    </paper>
    <paper id="353">
      <title><fixed-case>CAPE</fixed-case>: A <fixed-case>C</fixed-case>hinese Dataset for Appraisal-based Emotional Generation in Large Language Models</title>
      <author><first>June M.</first><last>Liu</last></author>
      <author><first>He</first><last>Cao</last></author>
      <author><first>Renliang</first><last>Sun</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Jiaxing</first><last>Zhang</last><affiliation>IDEA</affiliation></author>
      <pages>6291-6309</pages>
      <abstract>Generating emotionally appropriate responses in conversations with large language models presents a significant challenge due to the complexities of human emotions and cognitive processes, which remain largely underexplored in their critical role in social interactions. In this study, we introduce a two-stage automatic data generation framework to create CAPE, a Chinese dataset named Cognitive Appraisal theory-based Emotional corpus. This corpus facilitates the generation of dialogues with contextually appropriate emotional responses by accounting for diverse personal and situational factors. We propose two tasks utilizing this dataset: emotion prediction and next utterance prediction. Both automated and human evaluations demonstrate that agents trained on our dataset can deliver responses that are more aligned with human emotional expressions. Our study shows the potential for advancing emotional expression in conversational agents, paving the way for more nuanced and meaningful human-computer interactions.</abstract>
      <url hash="ea34ab16">2025.findings-naacl.353</url>
      <bibkey>liu-etal-2025-cape</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.353</doi>
    </paper>
    <paper id="354">
      <title>Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models</title>
      <author><first>Hongbang</first><last>Yuan</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>6310-6323</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success but still tend to generate factually erroneous responses, a phenomenon known as hallucination. A recent trend is to use preference learning to fine-tune models to align with factuality. However, existing work primarily evaluates fine-tuned models on in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets remains underexplored. In this paper, we conduct a comprehensive evaluation of the factuality of different models tuned by various preference learning algorithms and demonstrate that their performance on OOD datasets either increases minimally or decreases. Subsequently, we reveal that the main cause of model’s failure to uphold factuality under a distribution shift is <b>under-alignment</b>, rather than <b>over-alignment</b>, by analyzing the token distribution shift of the models before and after tuning. Finally, we propose <b>APEFT</b> (<b>A</b>tomic <b>P</b>reference <b>E</b>nhanced <b>F</b>actuality <b>T</b>uning), a framework that enhances model’s awareness of factuality at the granularity of individual facts. Extensive experiments demonstrate that APEFT improves model performance by an average of on both ID and OOD datasets, which is highly effective.</abstract>
      <url hash="4167fffa">2025.findings-naacl.354</url>
      <bibkey>yuan-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.354</doi>
    </paper>
    <paper id="355">
      <title>Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference</title>
      <author><first>Go</first><last>Kamoda</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Tatsuro</first><last>Inaba</last></author>
      <author><first>Keito</first><last>Kudo</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>6324-6343</pages>
      <abstract>According to the stages-of-inference hypothesis, early layers of language models map their subword-tokenized input, which does not necessarily correspond to a linguistically meaningful segmentation, to more meaningful representations that form the model’s “inner vocabulary”.Prior analysis of this *detokenization* stage has predominantly relied on probing and interventions such as path patching, which involve selecting particular inputs, choosing a subset of components that will be patched, and then observing changes in model behavior.Here, we show that several important aspects of the detokenization stage can be understood purely by analyzing model weights, without performing any model inference steps.Specifically, we introduce an analytical decomposition of first-layer attention in GPT-2.Our decomposition yields interpretable terms that quantify the relative contributions of position-related, token-related, and mixed effects.By focusing on terms in this decomposition, we discover weight-based explanations of attention bias toward close tokens and attention for detokenization.</abstract>
      <url hash="4a122ff3">2025.findings-naacl.355</url>
      <bibkey>kamoda-etal-2025-weight</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.355</doi>
    </paper>
    <paper id="356">
      <title><fixed-case>D</fixed-case>i<fixed-case>PT</fixed-case>: Enhancing <fixed-case>LLM</fixed-case> Reasoning through Diversified Perspective-Taking</title>
      <author><first>Hoang Anh</first><last>Just</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Mahavir</first><last>Dabas</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Ming</first><last>Jin</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Ruoxi</first><last>Jia</last><affiliation>Virginia Tech</affiliation></author>
      <pages>6344-6374</pages>
      <abstract>Existing work on improving language model reasoning typically explores a single solution path, which can be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a novel approach that complements current reasoning methods by explicitly incorporating diversified viewpoints. This approach allows the model to gain a deeper understanding of the problem’s context and identify the most effective solution path during the inference stage. Additionally, it provides a general data-centric AI recipe for augmenting existing data to improve their quality for fine-tuning. Our empirical results demonstrate that DiPT can be flexibly integrated into existing methods that focus on a single reasoning approach, enhancing their reasoning performance and stability when presented with paraphrased problems. Furthermore, we illustrate improved context understanding by maintaining the model’s safe outputs against “jailbreaking” prompts intentionally designed to bypass safeguards built into deployed models. Lastly, we show that fine-tuning with data enriched with diverse perspectives can boost the reasoning capabilities of the model compared to fine-tuning with raw data alone.</abstract>
      <url hash="390f7999">2025.findings-naacl.356</url>
      <bibkey>just-etal-2025-dipt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.356</doi>
    </paper>
    <paper id="357">
      <title><fixed-case>SOLID</fixed-case>: Self-seeding and Multi-intent Self-instructing <fixed-case>LLM</fixed-case>s for Generating Intent-aware Information-Seeking Dialogs</title>
      <author><first>Arian</first><last>Askari</last></author>
      <author><first>Roxana</first><last>Petcu</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <author><first>Chuan</first><last>Meng</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Amin</first><last>Abolghasemi</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <pages>6375-6395</pages>
      <abstract>Intent prediction in information-seeking dialogs is challenging and requires a substantial amount of data with human-labeled intents for effective model training. While Large Language Models (LLMs) have demonstrated effectiveness in generating synthetic data, existing methods typically rely on human feedback and are tailored to structured, task-oriented intents. In this paper, we leverage LLMs for zero-shot generation of large-scale, open-domain, intent-aware information-seeking dialogs to serve as training data for intent prediction models. We introduce SOLID, a method that generates dialogs turn by turn using novel self-seeding and multi-intent self-instructing strategies. Additionally, we propose SOLID-RL, a finetuned version that generates an entire dialog in one step using data created with SOLID. SOLID and SOLID-RL are each used to generate over 300k intent-aware dialogs, significantly surpassing the size of existing datasets. Experiments show that intent prediction models trained on sampled dialogs generated by SOLID and SOLID-RL outperform those trained solely on human-generated dialogs. Our findings demonstrate the potential of LLMs to expand training datasets, as they provide valuable resources for conversational agents across multiple tasks. Our self-seeding and self-instructing approaches are adaptable to various conversational data types and languages with minimal modifications.</abstract>
      <url hash="742da042">2025.findings-naacl.357</url>
      <bibkey>askari-etal-2025-solid</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.357</doi>
    </paper>
    <paper id="358">
      <title><fixed-case>C</fixed-case>ollage<fixed-case>P</fixed-case>rompt: A Benchmark for Budget-Friendly Visual Recognition with <fixed-case>GPT</fixed-case>-4<fixed-case>V</fixed-case></title>
      <author><first>Siyu</first><last>Xu</last></author>
      <author><first>Yunke</first><last>Wang</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Daochang</first><last>Liu</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chang</first><last>Xu</last><affiliation>University of Sydney</affiliation></author>
      <pages>6396-6418</pages>
      <url hash="bf2e8500">2025.findings-naacl.358</url>
      <bibkey>xu-etal-2025-collageprompt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.358</doi>
    </paper>
    <paper id="359">
      <title><fixed-case>ARISE</fixed-case>: Iterative Rule Induction and Synthetic Data Generation for Text Classification</title>
      <author><first>Yaswanth</first><last>M</last></author>
      <author><first>Vaibhav</first><last>Singh</last></author>
      <author><first>Ayush</first><last>Maheshwari</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Amrith</first><last>Krishna</last><affiliation>Learno</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>6419-6434</pages>
      <abstract>We propose ARISE, a framework that iteratively induces rules and generates synthetic data for text classification. We combine synthetic data generation and automatic rule induction, via bootstrapping, to iteratively filter the generated rules and data. We induce rules via inductive generalisation of syntactic-ngrams, enabling us to capture a complementary source of supervision. These rules alone lead to performance gains in both, in-context learning (ICL) and fine-tuning (FT) settings. Similarly, use of augmented data from ARISE alone improves the performance for a model, outperforming configurations that rely on complex methods like contrastive learning. Further, our extensive experiments on various datasets covering three full-shot, eight few-shot and seven multilingual variant settings demonstrate that the rules and data we generate lead to performance improvements across these diverse domains and languages.</abstract>
      <url hash="87b02ca5">2025.findings-naacl.359</url>
      <bibkey>m-etal-2025-arise</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.359</doi>
    </paper>
    <paper id="360">
      <title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Ik-hwan</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jongyoon</first><last>Song</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Saehyung</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Junsung</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>6435-6455</pages>
      <abstract>Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs’ performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the <b>misordered context</b> problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called <b>co</b>ntext <b>re</b>petition (<b>CoRe</b>), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model’s reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known “lost-in-the-middle” problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.</abstract>
      <url hash="774dd6cf">2025.findings-naacl.360</url>
      <bibkey>yu-etal-2025-unleashing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.360</doi>
    </paper>
    <paper id="361">
      <title>Text Annotation via Inductive Coding: Comparing Human Experts to <fixed-case>LLM</fixed-case>s in Qualitative Data Analysis</title>
      <author><first>Angelina</first><last>Parfenova</last></author>
      <author><first>Andreas</first><last>Marfurt</last><affiliation>HSLU - Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Alexander</first><last>Denzler</last><affiliation>HSLU - Lucerne University of Applied Sciences and Arts</affiliation></author>
      <pages>6456-6469</pages>
      <abstract>This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM-generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.</abstract>
      <url hash="d186460a">2025.findings-naacl.361</url>
      <bibkey>parfenova-etal-2025-text</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.361</doi>
    </paper>
    <paper id="362">
      <title>Investigating the Zone of Proximal Development of Language Models for In-Context Learning</title>
      <author><first>Peng</first><last>Cui</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>6470-6483</pages>
      <abstract>In this paper, we introduce a learning analytics framework to analyze the in-context learning (ICL) behavior of large language models (LLMs) through the lens of the Zone of Proximal Development (ZPD), an established theory in educational psychology. ZPD delineates the range of tasks a learner can accomplish with appropriate guidance but not yet independently. We adapt this concept to ICL, measuring the ZPD of LLMs based on model performance on individual examples in different settings. Furthermore, we propose an item response theory (IRT) model to predict the distribution of zones for LLMs. Our findings reveal a series of intricate and multifaceted behaviors of ICL, providing new insights into understanding and leveraging this technique. Finally, we demonstrate how our framework can enhance LLM in both inference and fine-tuning scenarios: (1) By predicting a model’s zone distribution, we selectively apply ICL to queries that are most likely to benefit from demonstrations, achieving a better balance between inference cost and performance; (2) We propose a human-like curriculum for fine-tuning, which prioritizes examples within the model’s ZPD. The curriculum results in improved performance, and we explain its effectiveness through an analysis of the training dynamics of LLMs.</abstract>
      <url hash="8ae508ad">2025.findings-naacl.362</url>
      <bibkey>cui-sachan-2025-investigating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.362</doi>
    </paper>
    <paper id="363">
      <title>Breaking <fixed-case>R</fixed-case>e<fixed-case>A</fixed-case>ct Agents: Foot-in-the-Door Attack Will Get You In</title>
      <author><first>Itay</first><last>Nakash</last><affiliation>International Business Machines</affiliation></author>
      <author><first>George</first><last>Kour</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Guy</first><last>Uziel</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>International Business Machines</affiliation></author>
      <pages>6484-6509</pages>
      <abstract>Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become prevalent.As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack.Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions.Our results show that once a ReAct agent’s thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a ‘foot-in-the-door’, allowing an attacker to embed malicious instructions into the agent’s thought process, making it more susceptible to harmful directives.To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.</abstract>
      <url hash="8a14dfff">2025.findings-naacl.363</url>
      <bibkey>nakash-etal-2025-breaking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.363</doi>
    </paper>
    <paper id="364">
      <title>As easy as <fixed-case>PIE</fixed-case>: understanding when pruning causes language models to disagree</title>
      <author><first>Pietro</first><last>Tropeano</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Maria</first><last>Maistro</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Tuukka</first><last>Ruotsalo</last><affiliation>Lappeenranta University of Technology, University of Copenhagen and University of Helsinki</affiliation></author>
      <author><first>Christina</first><last>Lioma</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>6510-6536</pages>
      <abstract>Language Model (LM) pruning compresses the model by removing weights, nodes, or other parts of its architecture. Typically, pruning focuses on the resulting efficiency gains at the cost of effectiveness.However, when looking at how individual data pointsare affected by pruning, it turns out that a particular subset of data points always bears most of the brunt (in terms of reduced accuracy) when pruning,but this effect goes unnoticed when reporting the mean accuracy of all data points. These data points are called PIEs and have been studied in image processing, but not in NLP.In a study of various NLP datasets, pruning methods, and levels of compression, we find that PIEs impact inference quality considerably, regardless of class frequency, andthat BERT is more prone to this than BiLSTM. We also find that PIEs contain a high amount of data points that have the largest influence on how well the model generalises to unseen data. This means that when pruning, with seemingly moderate loss to accuracy across all data points, we in fact hurt tremendously those data points that matter the most. We trace what makes PIEs both hard and impactful to inference to their overall longer and more semantically complex text. These findings are novel and contribute to understanding how LMs are affected by pruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE</abstract>
      <url hash="6887a908">2025.findings-naacl.364</url>
      <bibkey>tropeano-etal-2025-easy</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.364</doi>
    </paper>
    <paper id="365">
      <title>Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction</title>
      <author><first>Shengbin</first><last>Yue</last></author>
      <author><first>Ting</first><last>Huang</last></author>
      <author><first>Zheng</first><last>Jia</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Shujun</first><last>Liu</last></author>
      <author><first>Yun</first><last>Song</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>6537-6570</pages>
      <abstract>Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants’ characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs’ performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.</abstract>
      <url hash="2340311f">2025.findings-naacl.365</url>
      <bibkey>shengbinyue-etal-2025-multi</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.365</doi>
    </paper>
    <paper id="366">
      <title>Exploring Backward Reasoning in Large Language Models</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <pages>6571-6586</pages>
      <abstract>Multi-step reasoning through in-context learning strategies have been extensively explored, highlighting the abilities of Large Language Models (LLMs) to generate answers derived from step-by-step reasoning. These studies focus the attention on LLMs’ forward reasoning abilities epitomised in a series of general premises leading to a final solution. In this paper, by taking the reverse perspective, we study the backward reasoning abilities of LLMs, namely the inference that leads to the causal hypothesis. Behind formalising the backward problems, we analyse whether the LLMs are able to reason about the conclusion and reconstruct the original question that led to the delivery of the final answer. Operating with question-answering tasks involving symbolic reasoning, understanding, and commonsense abilities, we observe that the proposed models reveal robust comprehension capabilities managing different kinds of input; however, they are not always able to reason in the backward direction. Finally, to challenge this limitation, we demonstrate that instructing LLMs to generate the answer by reconsidering the structure of the problem allows for improved backward reasoning direction.</abstract>
      <url hash="0254cc5d">2025.findings-naacl.366</url>
      <bibkey>ranaldi-pucci-2025-exploring</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.366</doi>
    </paper>
    <paper id="367">
      <title><fixed-case>MMLF</fixed-case>: Multi-query Multi-passage Late Fusion Retrieval</title>
      <author><first>Yuan-Ching</first><last>Kuo</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Yi</first><last>Yu</last></author>
      <author><first>Chih-Ming</first><last>Chen</last></author>
      <author><first>Chuan-Ju</first><last>Wang</last><affiliation>Academia Sinica</affiliation></author>
      <pages>6587-6598</pages>
      <abstract>Leveraging large language models (LLMs) for query expansion has proven highly effective across diverse tasks and languages. Yet, challenges remain in optimizing query formatting and prompting, often with less focus on handling retrieval results. In this paper, we introduce Multi-query Multi-passage Late Fusion (MMLF), a straightforward yet potent pipeline that generates sub-queries, expands them into pseudo-documents, retrieves them individually, and aggregates results using reciprocal rank fusion. Our experiments demonstrate that MMLF exhibits superior performance across five BEIR benchmark datasets, achieving an average improvement of 4% and a maximum gain of up to 8% in both Recall@1k and nDCG@10 compared to state of the art across BEIR information retrieval datasets.</abstract>
      <url hash="74dd908f">2025.findings-naacl.367</url>
      <bibkey>kuo-etal-2025-mmlf</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.367</doi>
    </paper>
    <paper id="368">
      <title>Dynamic Guided and Domain Applicable Safeguards for Enhanced Security in Large Language Models</title>
      <author><first>Weidi</first><last>Luo</last></author>
      <author><first>He</first><last>Cao</last></author>
      <author><first>Zijing</first><last>Liu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>Aidan</first><last>Wong</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Bin</first><last>Feng</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>International Digital Economy Academy</affiliation></author>
      <pages>6599-6620</pages>
      <abstract>With the extensive deployment of Large Language Models (LLMs), ensuring their safety has become increasingly critical. However, existing defense methods often struggle with two key issues: (i) inadequate defense capabilities, particularly in domain-specific scenarios like chemistry, where a lack of specialized knowledge can lead to the generation of harmful responses to malicious queries. (ii) over-defensiveness, which compromises the general utility and responsiveness of LLMs. To mitigate these issues, we introduce a multi-agents-based defense framework, Guide for Defense (G4D), which leverages accurate external information to provide an unbiased summary of user intentions and analytically grounded safety response guidance. Extensive experiments on popular jailbreak attacks and benign datasets show that our G4D can enhance LLM’s robustness against jailbreak attacks on general and domain-specific scenarios without compromising the model’s general functionality.</abstract>
      <url hash="234ea196">2025.findings-naacl.368</url>
      <bibkey>luo-etal-2025-dynamic</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.368</doi>
    </paper>
    <paper id="369">
      <title>k<fixed-case>NN</fixed-case> For Whisper And Its Effect On Bias And Speaker Adaptation</title>
      <author><first>Maya K.</first><last>Nachesa</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Vlad</first><last>Niculae</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>6621-6627</pages>
      <abstract>Speech recognition performance varies by language, domain, and speaker characteristics such as accent, but fine-tuning a model on any of these categories may lead to catastrophic forgetting. Token-level <tex-math>k</tex-math> nearest neighbor search (<tex-math>k</tex-math>NN), first proposed for neural sequence decoders for natural language generation (NLG) and machine translation (MT), is a non-parametric method that instead adapts using inference-time search in an external datastore, without training the underlying model. We show that Whisper, a transformer end-to-end speech model, benefits from <tex-math>k</tex-math>NN. We investigate the differences between the speech and text setups. We discuss implications for speaker adaptation, and analyze improvements by gender, accent, and age.</abstract>
      <url hash="9782b0ff">2025.findings-naacl.369</url>
      <bibkey>nachesa-niculae-2025-knn</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.369</doi>
      <revision id="1" href="2025.findings-naacl.369v1" hash="18f7af86"/>
      <revision id="2" href="2025.findings-naacl.369v2" hash="9782b0ff" date="2025-06-10">Minor updates on results.</revision>
    </paper>
    <paper id="370">
      <title><fixed-case>V</fixed-case>isual<fixed-case>C</fixed-case>oder: Guiding Large Language Models in Code Execution with Fine-grained Multimodal Chain-of-Thought Reasoning</title>
      <author><first>Cuong Le</first><last>Chi</last></author>
      <author><first>Chau Truong Vinh</first><last>Hoang</last><affiliation>FPT</affiliation></author>
      <author><first>Phan Nhật</first><last>Huy</last></author>
      <author><first>Dung D.</first><last>Le</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Tien N</first><last>Nguyen</last><affiliation>university of texas at dallas</affiliation></author>
      <author><first>Nghi D. Q.</first><last>Bui</last></author>
      <pages>6628-6645</pages>
      <abstract>Predicting program behavior and reasoning about code execution remain significant challenges in software engineering, particularly for large language models (LLMs) designed for code analysis. While these models excel at understanding static syntax, they often struggle with dynamic reasoning tasks. We introduce VisualCoder, a simple yet effective approach that enhances code reasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a visual Control Flow Graph (CFG). By aligning code snippets with their corresponding CFGs, VisualCoder provides deeper insights into execution flows. We address challenges in multimodal CoT integration through a reference mechanism, ensuring consistency between code and its execution path, thereby improving performance in program behavior prediction, error detection, and output generation.</abstract>
      <url hash="02e7f9d3">2025.findings-naacl.370</url>
      <bibkey>chi-etal-2025-visualcoder</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.370</doi>
    </paper>
    <paper id="371">
      <title>Optimizing <fixed-case>LLM</fixed-case>s for <fixed-case>I</fixed-case>talian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</title>
      <author><first>Luca</first><last>Moroni</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Giovanni</first><last>Puccetti</last><affiliation>CNR</affiliation></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last><affiliation>Facebook</affiliation></author>
      <author><first>Andrei Stefan</first><last>Bejgu</last></author>
      <author><first>Alessio</first><last>Miaschi</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <author><first>Andrea</first><last>Esuli</last><affiliation>CNR</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>6646-6660</pages>
      <abstract>The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token “fertility”) and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.</abstract>
      <url hash="bf628580">2025.findings-naacl.371</url>
      <bibkey>moroni-etal-2025-optimizing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.371</doi>
    </paper>
    <paper id="372">
      <title>Beyond the Mode: Sequence-Level Distillation of Multilingual Translation Models for Low-Resource Language Pairs</title>
      <author><first>Aarón</first><last>Galiano-Jiménez</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last><affiliation>University of Alicante</affiliation></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last><affiliation>Universidad de Alicante</affiliation></author>
      <pages>6661-6676</pages>
      <abstract>This paper delves into sequence-level knowledge distillation (KD) of multilingual pre-trained translation models. We posit that, beyond the approximated mode obtained via beam search, the whole output distribution of the teacher contains valuable insights for students. We explore the potential of n-best lists from beam search to guide student’s learning and then investigate alternative decoding methods to address observed issues like low variability and under-representation of infrequent tokens. Our research in data-limited scenarios reveals that although sampling methods can slightly compromise the translation quality of the teacher output compared to beam search based methods, they enrich the generated corpora with increased variability and lexical richness, ultimately enhancing student model performance and reducing the gender bias amplification commonly associated with KD.</abstract>
      <url hash="df7cc2fc">2025.findings-naacl.372</url>
      <bibkey>galiano-jimenez-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.372</doi>
    </paper>
    <paper id="373">
      <title><fixed-case>LLM</fixed-case>s for Extremely Low-Resource <fixed-case>F</fixed-case>inno-<fixed-case>U</fixed-case>gric Languages</title>
      <author><first>Taido</first><last>Purason</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Hele-Andra</first><last>Kuulmets</last></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>6677-6697</pages>
      <abstract>The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on Võro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP.</abstract>
      <url hash="97f36fd5">2025.findings-naacl.373</url>
      <bibkey>purason-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.373</doi>
    </paper>
    <paper id="374">
      <title><fixed-case>LOFT</fixed-case>: Scalable and More Realistic Long-Context Evaluation</title>
      <author><first>Jinhyuk</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Anthony</first><last>Chen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zhuyun</first><last>Dai</last><affiliation>Google</affiliation></author>
      <author><first>Dheeru</first><last>Dua</last><affiliation>Google</affiliation></author>
      <author><first>Devendra Singh</first><last>Sachan</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Michael</first><last>Boratko</last><affiliation>Google</affiliation></author>
      <author><first>Yi</first><last>Luan</last><affiliation>Google</affiliation></author>
      <author><first>Séb</first><last>Arnold</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Hexiang</first><last>Hu</last><affiliation>xAI</affiliation></author>
      <author><first>Xudong</first><last>Lin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Panupong</first><last>Pasupat</last><affiliation>Google</affiliation></author>
      <author><first>Aida</first><last>Amini</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Jeremy R.</first><last>Cole</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Google and University College London</affiliation></author>
      <author><first>Iftekhar</first><last>Naim</last><affiliation>Google</affiliation></author>
      <author><first>Ming-Wei</first><last>Chang</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Kelvin</first><last>Guu</last><affiliation>Google</affiliation></author>
      <pages>6698-6723</pages>
      <abstract>Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs’ ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs’ performance on in-context retrieval and reasoning. Our findings reveal LCLMs’ surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their capabilities to tackle existing paradigms.</abstract>
      <url hash="ae323f32">2025.findings-naacl.374</url>
      <bibkey>lee-etal-2025-loft</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.374</doi>
    </paper>
    <paper id="375">
      <title>On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>6724-6736</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.</abstract>
      <url hash="5c9fa2f5">2025.findings-naacl.375</url>
      <bibkey>vladika-matthes-2025-influence</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.375</doi>
    </paper>
    <paper id="376">
      <title>Aligning Black-box Language Models with Human Judgments</title>
      <author><first>Gerrit J.j.</first><last>Van Den Burg</last><affiliation>Amazon</affiliation></author>
      <author><first>Gen</first><last>Suzuki</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Murat</first><last>Sensoy</last></author>
      <pages>6737-6749</pages>
      <abstract>Large language models (LLMs) are increasingly used as automated judges to evaluate recommendation systems, search engines, and other subjective tasks, where relying on human evaluators can be costly, time-consuming, and unscalable. LLMs offer an efficient solution for continuous, automated evaluation. However, since the systems that are built and improved with these judgments are ultimately designed for human use, it is crucial that LLM judgments align closely with human evaluators to ensure such systems remain human-centered. On the other hand, aligning LLM judgments with human evaluators is challenging due to individual variability and biases in human judgments. We propose a simple yet effective framework to align LLM judgments with individual human evaluators or their aggregated judgments, without retraining or fine-tuning the LLM. Our approach learns a linear mapping between the LLM’s outputs and human judgments, achieving over 142% average improvement in agreement across 29 tasks with only a small number of calibration examples used for training. Notably, our method works in zero-shot and few-shot settings, exceeds inter-human agreement on four out of six tasks, and enables smaller LLMs to achieve performance comparable to that of larger models.</abstract>
      <url hash="ba1eca59">2025.findings-naacl.376</url>
      <bibkey>van-den-burg-etal-2025-aligning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.376</doi>
    </paper>
    <paper id="377">
      <title>Guideline Compliance in Task-Oriented Dialogue: The Chained Prior Approach</title>
      <author><first>Xiangyu</first><last>Wen</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jianyuan</first><last>Zhong</last></author>
      <author><first>Zhijian</first><last>Xu</last></author>
      <author><first>Qiang</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6750-6776</pages>
      <abstract>Task-oriented dialogue (TOD) systems are widely used across various domains, including customer service, appointment scheduling, and technical support. In real-world scenarios, such systems must adhere to given operational guidelines. However, existing solutions based on large language models often cannot achieve strict guideline compliance, even when fine-tuned with domain knowledge. To address this issue, we introduce a novel TOD system named GuidedTOD, which explicitly considers domain-specific guidelines by integrating a policy module. This module employs a Markov Chain, termed Chained Prior, to efficiently encode and dynamically update guideline knowledge. During inference, the Chained Prior re-ranks outputs from the domain-expert language model using beam search, ensuring guideline adherence. Experimental results show that GuidedTOD significantly improves guideline compliance, achieving approximately 20% better action prediction accuracy than state-of-the-art solutions. Code is available here: https://github.com/cure-lab/GuidedTOD.</abstract>
      <url hash="427addb2">2025.findings-naacl.377</url>
      <bibkey>wen-etal-2025-guideline</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.377</doi>
    </paper>
    <paper id="378">
      <title><fixed-case>A</fixed-case>uto<fixed-case>B</fixed-case>reach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization via Multi-<fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Xiao</first><last>Yang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhengwei</first><last>Fang</last></author>
      <author><first>Yu</first><last>Tian</last></author>
      <author><first>Yinpeng</first><last>Dong</last></author>
      <author><first>Zhaoxia</first><last>Yin</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Tsinghua University</affiliation></author>
      <pages>6777-6798</pages>
      <abstract>Recent studies show that large language models (LLMs) are vulnerable to jailbreak attacks, which can bypass their defense mechanisms. However, existing jailbreak research often exhibits limitations in universality, validity, and efficiency. Therefore, we rethink jailbreaking LLMs and define three key properties to guide the design of effective jailbreak methods. We introduce AutoBreach, a novel black-box approach that uses wordplay-guided mapping rule sampling to create universal adversarial prompts. By leveraging LLMs’ summarization and reasoning abilities, AutoBreach minimizes manual effort. To boost jailbreak success rates, we further suggest sentence compression and chain-of-thought-based mapping rules to correct errors and wordplay misinterpretations in target LLMs. Also, we propose a two-stage mapping rule optimization that initially optimizes mapping rules before querying target LLMs to enhance efficiency. Experimental results indicate AutoBreach efficiently identifies security vulnerabilities across various LLMs (Claude-3, GPT-4, etc.), achieving an average success rate of over 80% with fewer than 10 queries. Notably, the adversarial prompts generated by AutoBreach for GPT-4 can directly bypass the defenses of the advanced commercial LLM GPT o1-preview, demonstrating strong transferability and universality.</abstract>
      <url hash="a999a794">2025.findings-naacl.378</url>
      <bibkey>chen-etal-2025-autobreach</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.378</doi>
    </paper>
    <paper id="379">
      <title><tex-math>\mathcal{S}^2</tex-math><fixed-case>IT</fixed-case>: Stepwise Syntax Integration Tuning for Large Language Models in Aspect Sentiment Quad Prediction</title>
      <author><first>Bingfeng</first><last>Chen</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Chenjie</first><last>Qiu</last></author>
      <author><first>Yifeng</first><last>Xie</last></author>
      <author><first>Boyan</first><last>Xu</last></author>
      <author><first>Ruichu</first><last>Cai</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Zhifeng</first><last>Hao</last><affiliation>Shantou University</affiliation></author>
      <pages>6799-6806</pages>
      <url hash="1c58c7b4">2025.findings-naacl.379</url>
      <bibkey>chen-etal-2025-s2it</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.379</doi>
    </paper>
    <paper id="380">
      <title><fixed-case>B</fixed-case>an<fixed-case>NERD</fixed-case>: A Benchmark Dataset and Context-Driven Approach for <fixed-case>B</fixed-case>angla Named Entity Recognition</title>
      <author><first>Md. Motahar</first><last>Mahtab</last><affiliation>GIGATECH, BEXIMCO</affiliation></author>
      <author><first>Faisal Ahamed</first><last>Khan</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Md. Ekramul</first><last>Islam</last><affiliation>Giga Tech Limited.</affiliation></author>
      <author><first>Md. Shahad Mahmud</first><last>Chowdhury</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Labib Imam</first><last>Chowdhury</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Sadia</first><last>Afrin</last><affiliation>Heinrich-Heine Universität Düsseldorf and Giga Tech Limited</affiliation></author>
      <author><first>Hazrat</first><last>Ali</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Mohammad Mamun Or</first><last>Rashid</last></author>
      <author><first>Nabeel</first><last>Mohammed</last><affiliation>North South University</affiliation></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last><affiliation>Fordham University</affiliation></author>
      <pages>6807-6828</pages>
      <abstract>In this study, we introduce <b>BanNERD</b>, the most extensive human-annotated and validated <b>B</b>angla <b>N</b>amed <b>E</b>ntity <b>R</b>ecognition <b>D</b>ataset to date, comprising over 85,000 sentences. BanNERD is curated from a diverse array of sources, spanning over 29 domains, thereby offering a comprehensive range of generalized contexts. To ensure the dataset’s quality, expert linguists developed a detailed annotation guideline tailored to the Bangla language. All annotations underwent rigorous validation by a team of validators, with final labels being determined via majority voting, thereby ensuring the highest annotation quality and a high IAA score of 0.88. In a cross-dataset evaluation, models trained on BanNERD consistently outperformed those trained on four existing Bangla NER datasets. Additionally, we propose a method named <i>BanNERCEM</i> (Bangla NER context-ensemble Method) which outperforms existing approaches on Bangla NER datasets and performs competitively on English datasets using lightweight Bangla pretrained LLMs. Our approach passes each context separately to the model instead of previous concatenation-based approaches achieving the highest average macro F1 score of 81.85% across 10 NER classes, outperforming previous approaches and ensuring better context utilization. We are making the code and datasets publicly available at <url>https://github.com/eblict-gigatech/BanNERD</url> in order to contribute to the further advancement of Bangla NLP.</abstract>
      <url hash="6b5dc741">2025.findings-naacl.380</url>
      <bibkey>mahtab-etal-2025-bannerd</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.380</doi>
    </paper>
    <paper id="381">
      <title>Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias</title>
      <author><first>Andres</first><last>Algaba</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Carmen</first><last>Mazijn</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Vincent</first><last>Holst</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Floriano</first><last>Tori</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Sylvia</first><last>Wenmackers</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Vincent</first><last>Ginis</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <pages>6829-6864</pages>
      <abstract>Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4’s knowledge cut-off date. In our experiment, LLMs are tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias, which persists even after controlling for publication year, title length, number of authors, and venue. The results hold for both GPT-4, and the more capable models GPT-4o and Claude 3.5 where the papers are part of the training data. Additionally, we observe a large consistency between the characteristics of LLM’s existing and non-existent generated references, indicating the model’s internalization of citation patterns. By analyzing citation graphs, we show that the references recommended are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases, such as the Matthew effect, and introduce new ones, potentially skewing scientific knowledge dissemination.</abstract>
      <url hash="5f9d1552">2025.findings-naacl.381</url>
      <bibkey>algaba-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.381</doi>
    </paper>
    <paper id="382">
      <title>What can Large Language Models Capture about Code Functional Equivalence?</title>
      <author><first>Nickil</first><last>Maveli</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Antonio</first><last>Vergari</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Shay B</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>6865-6903</pages>
      <abstract>Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code)-LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.</abstract>
      <url hash="3e409ff4">2025.findings-naacl.382</url>
      <bibkey>maveli-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.382</doi>
    </paper>
    <paper id="383">
      <title>Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning</title>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Yueqi</first><last>Zhang</last></author>
      <author><first>Chuyi</first><last>Tan</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>6904-6917</pages>
      <abstract>Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information of batch queries from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the overall cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.</abstract>
      <url hash="0a794c00">2025.findings-naacl.383</url>
      <bibkey>wang-etal-2025-make</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.383</doi>
    </paper>
    <paper id="384">
      <title>Large Language Models Are Better Logical Fallacy Reasoners with Counterargument, Explanation, and Goal-Aware Prompt Formulation</title>
      <author><first>Jiwon</first><last>Jeong</last></author>
      <author><first>Hyeju</first><last>Jang</last><affiliation>Indiana University</affiliation></author>
      <author><first>Hogun</first><last>Park</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>6918-6937</pages>
      <abstract>The advancement of Large Language Models (LLMs) has greatly improved our ability to process complex language. However, accurately detecting logical fallacies remains a significant challenge. This study presents a novel and effective prompt formulation approach for logical fallacy detection, applicable in both supervised (fine-tuned) and unsupervised (zero-shot) settings. Our method enriches input text by incorporating implicit contextual information—counterarguments, explanations, and goals—which we query for validity within the argument’s context. We then rank these queries based on confidence scores to inform classification. We evaluate our approach across multiple datasets from 5 domains, covering 29 distinct fallacy types, using models from GPT and LLaMA series. The results show substantial improvements over state-of-the-art models: up to a 0.57 increase in F1-score in zero-shot settings and up to 0.45 in fine-tuned models. Extensive analyses further illustrate why and how our method excels.</abstract>
      <url hash="4374ffc8">2025.findings-naacl.384</url>
      <bibkey>jeong-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.384</doi>
    </paper>
    <paper id="385">
      <title><fixed-case>M</fixed-case>orph<fixed-case>NLI</fixed-case>: A Stepwise Approach to Natural Language Inference Using Text Morphing</title>
      <author><first>Vlad Andrei</first><last>Negru</last><affiliation>Technical University of Cluj-Napoca</affiliation></author>
      <author><first>Robert</first><last>Vacareanu</last><affiliation>Scale AI</affiliation></author>
      <author><first>Camelia</first><last>Lemnaru</last><affiliation>Technical University of Cluj-Napoca</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Rodica</first><last>Potolea</last></author>
      <pages>6938-6953</pages>
      <abstract>We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into entailment, contradiction, neutral, we use a language model to generate the necessary edits to incrementally transform (i.e., morph) the premise into the hypothesis. Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output. We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines with improvements up to 12.6% (relative). Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.</abstract>
      <url hash="4da48529">2025.findings-naacl.385</url>
      <bibkey>negru-etal-2025-morphnli</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.385</doi>
    </paper>
    <paper id="386">
      <title>Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-<fixed-case>SQL</fixed-case> Systems</title>
      <author><first>Đorđe</first><last>Klisura</last></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>6954-6976</pages>
      <abstract>Text-to-SQL systems empower users to interact with databases using natural language, automatically translating queries into executable SQL code. However, their reliance on database schema information for SQL generation exposes them to significant security vulnerabilities, particularly schema inference attacks that can lead to unauthorized data access or manipulation. In this paper, we introduce a novel zero-knowledge framework for reconstructing the underlying database schema of text-to-SQL models without any prior knowledge of the database. Our approach systematically probes text-to-SQL models with specially crafted questions and leverages a surrogate GPT-4 model to interpret the outputs, effectively uncovering hidden schema elements—including tables, columns, and data types. We demonstrate that our method achieves high accuracy in reconstructing table names, with F1 scores of up to .99 for generative models and .78 for fine-tuned models, underscoring the severity of schema leakage risks. We also show that our attack can steal prompt information in non-text-to-SQL models. Furthermore, we propose a simple protection mechanism for generative models and empirically show its limitations in mitigating these attacks.</abstract>
      <url hash="dd0b84b8">2025.findings-naacl.386</url>
      <bibkey>klisura-rios-2025-unmasking</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.386</doi>
    </paper>
    <paper id="387">
      <title>Media of Langue: Exploring Word Translation Network</title>
      <author><first>Goki</first><last>Muramoto</last></author>
      <author><first>Atsuki</first><last>Sato</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Takayoshi</first><last>Koyama</last></author>
      <pages>6977-6994</pages>
      <abstract>In the human activity of word translation, two languages face each other, mutually searching their own language system for the semantic place of words in the other language. We discover the huge network formed by the chain of these mutual translations as *Word Translation Network*, a network where words are nodes, and translation volume is represented as edges, and propose *Word Translation Map*, a novel interface for exploring this network. *Word Translation Map* points to the semantic configurations of many words in multiple languages at once, containing the information of existing dictionaries such as bilingual and synonym dictionaries. We have also implemented and published this interface as a web application, focusing on seven language pairs. This paper first defines the *Word Translation Network* and describes how to actually construct the network from bilingual corpora, followed by an analysis of the properties of the network. Next, we explain how to design a *Word Translation Map* using this network, and finally, we analyze the features of the *Word Translation Map* as a dictionary. The web application is publicly accessible at www.media-of-langue.org.</abstract>
      <url hash="5743e170">2025.findings-naacl.387</url>
      <bibkey>muramoto-etal-2025-media</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.387</doi>
    </paper>
    <paper id="388">
      <title>Tackling Social Bias against the Poor: a Dataset and a Taxonomy on Aporophobia</title>
      <author><first>Georgina</first><last>Curto</last><affiliation>United Nations University Institute in Macau</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Muhammad Hammad Fahim</first><last>Siddiqui</last></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada and University of Ottawa</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>6995-7016</pages>
      <abstract>Eradicating poverty is the first goal in the U.N. Sustainable Development Goals. However, aporophobia – the societal bias against people living in poverty – constitutes a major obstacle to designing, approving and implementing poverty-mitigation policies. This work presents an initial step towards operationalizing the concept of aporophobia to identify and track harmful beliefs and discriminative actions against poor people on social media. In close collaboration with non-profits and governmental organizations, we conduct data collection and exploration. Then we manually annotate a corpus of English tweets from five world regions for the presence of (1) direct expressions of aporophobia, and (2) statements referring to or criticizing aporophobic views or actions of others, to comprehensively characterize the social media discourse related to bias and discrimination against the poor. Based on the annotated data, we devise a taxonomy of categories of aporophobic attitudes and actions expressed through speech on social media. Finally, we train several classifiers and identify the main challenges for automatic detection of aporophobia in social networks. This work paves the way towards identifying, tracking, and mitigating aporophobic views on social media at scale.</abstract>
      <url hash="d88107e0">2025.findings-naacl.388</url>
      <bibkey>curto-etal-2025-tackling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.388</doi>
    </paper>
    <paper id="389">
      <title>The <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Knowledge Graph: Infusing <fixed-case>ASL</fixed-case> Models with Linguistic Knowledge</title>
      <author><first>Lee</first><last>Kezar</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nidhi</first><last>Munikote</last></author>
      <author><first>Zian</first><last>Zeng</last><affiliation>University of Hawaii System</affiliation></author>
      <author><first>Zed</first><last>Sehyr</last><affiliation>Chapman University</affiliation></author>
      <author><first>Naomi</first><last>Caselli</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <pages>7017-7029</pages>
      <abstract>Sign language models could make modern language technologies more accessible to those who sign, but the supply of accurately labeled data struggles to meet the demand associated with training large, end-to-end neural models. As an alternative to this approach, we explore how knowledge about the linguistic structure of signs may be used as inductive priors for learning sign recognition and comprehension tasks. We first construct the American Sign Language Knowledge Graph (ASLKG) from 11 sources of linguistic knowledge, with emphasis on features related to signs’ phonological and lexical-semantic properties. Then, we use the ASLKG to train neuro-symbolic models on ASL video input tasks, achieving accuracies of 91% for isolated sign recognition, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.</abstract>
      <url hash="f581bd04">2025.findings-naacl.389</url>
      <bibkey>kezar-etal-2025-american</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.389</doi>
    </paper>
    <paper id="390">
      <title>Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting</title>
      <author><first>Mohamed Salim</first><last>Aissi</last></author>
      <author><first>Clément</first><last>Romac</last><affiliation>Inria and Hugging Face</affiliation></author>
      <author><first>Thomas</first><last>Carta</last></author>
      <author><first>Sylvain</first><last>Lamprier</last><affiliation>Université d’Angers</affiliation></author>
      <author><first>Pierre-Yves</first><last>Oudeyer</last><affiliation>Inria</affiliation></author>
      <author><first>Olivier</first><last>Sigaud</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Laure</first><last>Soulier</last><affiliation>Sorbonne Université, CNRS, ISIR</affiliation></author>
      <author><first>Nicolas</first><last>Thome</last><affiliation>sorbonne université</affiliation></author>
      <pages>7030-7046</pages>
      <abstract>Reinforcement learning (RL) is a promising approach for aligning large language models (LLMs) knowledge with sequential decision-making tasks. However, few studies have thoroughly investigated the impact on LLM agents capabilities of fine-tuning them with RL in a specific environment. In this paper, we propose a novel framework to analyze the sensitivity of LLMs to prompt formulations following RL training in a textual environment. Our findings reveal that the performance of LLMs degrades when faced with prompt formulations different from those used during the RL training phase. Besides, we analyze the source of this sensitivity by examining the model’s internal representations and salient tokens. Finally, we propose to use a contrastive loss to mitigate this sensitivity and improve the robustness and generalization capabilities of LLMs.</abstract>
      <url hash="4711d0e0">2025.findings-naacl.390</url>
      <bibkey>aissi-etal-2025-reinforcement</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.390</doi>
    </paper>
    <paper id="391">
      <title>An empirical study of validating synthetic data for formula generation</title>
      <author><first>Usneek</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>José</first><last>Cambronero</last></author>
      <author><first>Sumit</first><last>Gulwani</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Aditya</first><last>Kanade</last><affiliation>Microsoft</affiliation></author>
      <author><first>Anirudh</first><last>Khatry</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mukul</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Gust</first><last>Verbruggen</last><affiliation>Microsoft</affiliation></author>
      <pages>7047-7054</pages>
      <abstract>Large language models (LLMs) can be leveraged to help write formulas in spreadsheets, but formula data resources are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use another model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the natural language (NL) generated by the LLM is accurate for it to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.</abstract>
      <url hash="a3a85af2">2025.findings-naacl.391</url>
      <bibkey>singh-etal-2025-empirical</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.391</doi>
    </paper>
    <paper id="392">
      <title><fixed-case>T</fixed-case>e<fixed-case>C</fixed-case>o<fixed-case>F</fixed-case>e<fixed-case>S</fixed-case>: Text Column Featurization using Semantic Analysis</title>
      <author><first>Ananya</first><last>Singha</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Mukul</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ashish</first><last>Tiwari</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sumit</first><last>Gulwani</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chris</first><last>Parnin</last><affiliation>North Carolina State University</affiliation></author>
      <pages>7055-7061</pages>
      <abstract>Extracting insights from text columns can bechallenging and time-intensive. Existing methods for topic modeling and feature extractionare based on syntactic features and often overlook the semantics. We introduce the semantictext column featurization problem, and presenta scalable approach for automatically solvingit. We extract a small sample smartly, use alarge language model (LLM) to label only thesample, and then lift the labeling to the wholecolumn using text embeddings. We evaluateour approach by turning existing text classification benchmarks into semantic categorization benchmarks. Our approach performs better than baselines and naive use of LLMs.</abstract>
      <url hash="a5dca2c8">2025.findings-naacl.392</url>
      <bibkey>singha-etal-2025-tecofes</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.392</doi>
    </paper>
    <paper id="393">
      <title><fixed-case>CA</fixed-case>*: Addressing Evaluation Pitfalls in Computation-Aware Latency for Simultaneous Speech Translation</title>
      <author><first>Xi</first><last>Xu</last></author>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>7062-7067</pages>
      <abstract>Simultaneous speech translation (SimulST) systems must balance translation quality with response time, making latency measurement crucial for evaluating their real-world performance. However, there has been a longstanding belief that current metrics yield unrealistically high latency measurements in unsegmented streaming settings. In this paper, we investigate this phenomenon, revealing its root cause in a fundamental misconception underlying existing latency evaluation approaches. We demonstrate that this issue affects not only streaming but also segment-level latency evaluation across different metrics. Furthermore, we propose a modification to correctly measure computation-aware latency for SimulST systems, addressing the limitations present in existing metrics.</abstract>
      <url hash="14a7f1e4">2025.findings-naacl.393</url>
      <bibkey>xu-etal-2025-ca</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.393</doi>
    </paper>
    <paper id="394">
      <title>Augmented Adversarial Trigger Learning</title>
      <author><first>Zhe</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Amazon and University of Virginia</affiliation></author>
      <pages>7068-7100</pages>
      <abstract>Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs.</abstract>
      <url hash="27d7b04c">2025.findings-naacl.394</url>
      <bibkey>wang-qi-2025-augmented</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.394</doi>
    </paper>
    <paper id="395">
      <title>Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Qiusi</first><last>Zhan</last></author>
      <author><first>Richard</first><last>Fang</last></author>
      <author><first>Henil Shalin</first><last>Panchal</last></author>
      <author><first>Daniel</first><last>Kang</last></author>
      <pages>7101-7117</pages>
      <abstract>Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks.In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%.This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability.The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.</abstract>
      <url hash="d92ec17c">2025.findings-naacl.395</url>
      <bibkey>zhan-etal-2025-adaptive</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.395</doi>
    </paper>
    <paper id="396">
      <title>Flaming-hot Initiation with Regular Execution Sampling for Large Language Models</title>
      <author><first>Weizhe</first><last>Chen</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Zhicheng</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Guanlin</first><last>Liu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Renjie</first><last>Zheng</last><affiliation>ByteDance</affiliation></author>
      <author><first>Wenlei</first><last>Shi</last></author>
      <author><first>Chen</first><last>Dun</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zheng</first><last>Wu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xing</first><last>Jin</last></author>
      <author><first>Lin</first><last>Yan</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>7118-7127</pages>
      <abstract>Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This becomes especially critical in reasoning-related tasks with sandbox checkers, such as math or code, where the goal is to generate correct solutions to specific problems with higher probability. In this work, we introduce Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet highly effective method to efficiently find good responses. Our empirical findings show that FIRE sampling enhances inference-time generation quality and also benefits training in the alignment stage. Furthermore, we explore how FIRE sampling improves performance by promoting diversity and analyze the impact of employing FIRE at different positions within a response.</abstract>
      <url hash="019c9cea">2025.findings-naacl.396</url>
      <bibkey>chen-etal-2025-flaming</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.396</doi>
    </paper>
    <paper id="397">
      <title><fixed-case>HEISIR</fixed-case>: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Hangyeul</first><last>Lee</last></author>
      <author><first>Yohan</first><last>Lee</last><affiliation>Coxwave</affiliation></author>
      <pages>7128-7144</pages>
      <abstract>The growth of conversational AI services has increased demand for effective information retrieval from dialogue data. However, existing methods often face challenges in capturing semantic intent or require extensive labeling and fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted Semantic Indexing for Retrieval), a novel framework that enhances semantic understanding in conversational data retrieval through optimized data ingestion, eliminating the need for resource-intensive labeling or model adaptation.HEISIR implements a two-step process: (1) Hierarchical Triplets Formulation and (2) Adjunct Augmentation, creating semantic indices consisting of Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured representation effectively captures the underlying semantic information from dialogue content. HEISIR achieves high retrieval performance while maintaining low latency during the actual retrieval process. Our experimental results demonstrate that HEISIR outperforms fine-tuned models across various embedding types and language models. Beyond improving retrieval capabilities, HEISIR also offers opportunities for intent and topic analysis in conversational data, providing a versatile solution for dialogue systems.</abstract>
      <url hash="3d38f8d4">2025.findings-naacl.397</url>
      <bibkey>kim-etal-2025-heisir</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.397</doi>
    </paper>
    <paper id="398">
      <title>“Women do not have heart attacks!” Gender Biases in Automatically Generated Clinical Cases in <fixed-case>F</fixed-case>rench</title>
      <author><first>Fanny</first><last>Ducel</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Nicolas</first><last>Hiebel</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Olivier</first><last>Ferret</last><affiliation>CEA</affiliation></author>
      <author><first>Karën</first><last>Fort</last><affiliation>University of Lorraine</affiliation></author>
      <author><first>Aurélie</first><last>Névéol</last><affiliation>LISN-CNRS / Université Paris Saclay</affiliation></author>
      <pages>7145-7159</pages>
      <abstract>Healthcare professionals are increasingly including Language Models (LMs) in clinical practice. However, LMs have been shown to exhibit and amplify stereotypical biases that can cause life-threatening harm in a medical context. This study aims to evaluate gender biases in automatically generated clinical cases in French, on ten disorders. Using seven LMs fine-tuned for clinical case generation and an automatic linguistic gender detection tool, we measure the associations between disorders and gender. We unveil that LMs over-generate cases describing male patients, creating synthetic corpora that are not consistent with documented prevalence for these disorders. For instance, when prompts do not specify a gender, LMs generate eight times more clinical cases describing male (vs. female patients) for heart attack. We discuss the ideal synthetic clinical case corpus and establish that explicitly mentioning demographic information in generation instructions appears to be the fairest strategy. In conclusion, we argue that the presence of gender biases in synthetic text raises concerns about LM-induced harm, especially for women and transgender people.</abstract>
      <url hash="641d230e">2025.findings-naacl.398</url>
      <bibkey>ducel-etal-2025-women</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.398</doi>
    </paper>
    <paper id="399">
      <title><fixed-case>NOTA</fixed-case>: Multimodal Music Notation Understanding for Visual Large Language Model</title>
      <author><first>Mingni</first><last>Tang</last></author>
      <author><first>Jiajia</first><last>Li</last></author>
      <author><first>Lu</first><last>Yang</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last></author>
      <author><first>Jinhao</first><last>Tian</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <pages>7160-7173</pages>
      <abstract>Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music score notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline.</abstract>
      <url hash="afff9bc1">2025.findings-naacl.399</url>
      <bibkey>tang-etal-2025-nota</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.399</doi>
    </paper>
    <paper id="400">
      <title>Exploring Large Language Models for Hate Speech Detection in <fixed-case>R</fixed-case>ioplatense <fixed-case>S</fixed-case>panish</title>
      <author><first>Juan Manuel</first><last>Pérez</last><affiliation>Universidad de San Andres and Universidad de Buenos Aires</affiliation></author>
      <author><first>Paula</first><last>Miguel</last><affiliation>Universidad de Buenos Aires and Universidad de Buenos Aires</affiliation></author>
      <author><first>Viviana</first><last>Cotik</last><affiliation>Computer Science Department, University of Buenos Aires</affiliation></author>
      <pages>7174-7187</pages>
      <abstract>Hate speech detection deals with many language variants, slang, slurs, expression modalities, and cultural nuances. This outlines the importance of working with specific corpora, when addressing hate speech within the scope of Natural Language Processing, recently revolutionized by the irruption of Large Language Models. This work presents a brief analysis of the performance of large language models in the detection of Hate Speech for Rioplatense Spanish. We performed classification experiments leveraging chain-of-thought reasoning with ChatGPT 3.5, Mixtral, and Aya, comparing their results with those of a state-of-the-art BERT classifier. These experiments outline that, even if large language models show a lower precision compared to the fine-tuned BERT classifier and, in some cases, they find hard-to-get slurs or colloquialisms, they still are sensitive to highly nuanced cases (particularly, homophobic/transphobic hate speech). We make our code and models publicly available for future research.</abstract>
      <url hash="d5c52b64">2025.findings-naacl.400</url>
      <bibkey>perez-etal-2025-exploring</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.400</doi>
    </paper>
    <paper id="401">
      <title>An Annotated Dataset of Errors in Premodern <fixed-case>G</fixed-case>reek and Baselines for Detecting Them</title>
      <author><first>Creston</first><last>Brooks</last></author>
      <author><first>Johannes</first><last>Haubold</last><affiliation>Princeton University</affiliation></author>
      <author><first>Charlie</first><last>Cowen-Breen</last></author>
      <author><first>Jay</first><last>White</last></author>
      <author><first>Desmond</first><last>DeVaul</last><affiliation>Princeton University</affiliation></author>
      <author><first>Frederick</first><last>Riemenschneider</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Karthik R</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <author><first>Barbara</first><last>Graziosi</last><affiliation>Princeton University</affiliation></author>
      <pages>7188-7202</pages>
      <abstract>As premodern texts are passed down over centuries, errors inevitably accrue. These errors can be challenging to identify, as some have survived undetected for so long precisely because they are so elusive. While prior work has evaluated error detection methods on artificially-generated errors, we introduce the first dataset of real errors in premodern Greek, enabling the evaluation of error detection methods on errors that genuinely accumulated at some stage in the centuries-long copying process. To create this dataset, we use metrics derived from BERT conditionals to sample 1,000 words more likely to contain errors, which are then annotated and labeled by a domain expert as errors or not. We then propose and evaluate new error detection methods and find that our discriminator-based detector outperforms all other methods, improving the true positive rate for classifying real errors by 5%. We additionally observe that scribal errors are more difficult to detect than print or digitization errors. Our dataset enables the evaluation of error detection methods on real errors in premodern texts for the first time, providing a benchmark for developing more effective error detection algorithms to assist scholars in restoring premodern works.</abstract>
      <url hash="781390b1">2025.findings-naacl.401</url>
      <bibkey>brooks-etal-2025-annotated</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.401</doi>
    </paper>
    <paper id="402">
      <title><fixed-case>W</fixed-case>orld<fixed-case>M</fixed-case>ed<fixed-case>QA</fixed-case>-<fixed-case>V</fixed-case>: a multilingual, multimodal medical examination dataset for multimodal language models evaluation</title>
      <author><first>João</first><last>Matos</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Shan</first><last>Chen</last></author>
      <author><first>Siena Kathleen V.</first><last>Placino</last></author>
      <author><first>Yingya</first><last>Li</last><affiliation>Harvard University</affiliation></author>
      <author><first>Juan Carlos Climent</first><last>Pardo</last></author>
      <author><first>Daphna</first><last>Idan</last></author>
      <author><first>Takeshi</first><last>Tohyama</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>David</first><last>Restrepo</last><affiliation>Centrale Supélec and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Luis Filipe</first><last>Nakayama</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>José María Millet</first><last>Pascual-Leone</last></author>
      <author><first>Guergana K</first><last>Savova</last><affiliation>Harvard University</affiliation></author>
      <author><first>Hugo</first><last>Aerts</last><affiliation>Harvard University</affiliation></author>
      <author><first>Leo Anthony</first><last>Celi</last><affiliation>Massachusetts Institute of Technology and Beth Israel Deaconess Medical Center</affiliation></author>
      <author><first>An-Kwok Ian</first><last>Wong</last><affiliation>Duke University</affiliation></author>
      <author><first>Danielle</first><last>Bitterman</last><affiliation>Harvard University</affiliation></author>
      <author><first>Jack</first><last>Gallifant</last></author>
      <pages>7203-7216</pages>
      <abstract>Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.</abstract>
      <url hash="4288a55c">2025.findings-naacl.402</url>
      <bibkey>matos-etal-2025-worldmedqa</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.402</doi>
    </paper>
    <paper id="403">
      <title><fixed-case>B</fixed-case>an<fixed-case>TH</fixed-case>: A Multi-label Hate Speech Detection Dataset for Transliterated <fixed-case>B</fixed-case>angla</title>
      <author><first>Fabiha</first><last>Haider</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Fariha Tanjim</first><last>Shifat</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Md Farhan</first><last>Ishmam</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Md Sakib Ul Rahman</first><last>Sourove</last></author>
      <author><first>Deeparghya Dutta</first><last>Barua</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Md Farhad Alam</first><last>Bhuiyan</last></author>
      <pages>7217-7236</pages>
      <abstract>The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in understanding hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We propose a novel translation-based LLM prompting strategy that translates or transliterates under-resourced text to higher-resourced text before classifying the hate group(s). Experiments reveal further pre-trained encoders achieving state-of-the-art performance on the BanTH dataset while translation-based prompting outperforms other strategies in the zero-shot setting. We address a critical gap in Bangla hate speech and set the stage for further exploration into code-mixed and multi-label classification in underrepresented languages.</abstract>
      <url hash="bf62f5ec">2025.findings-naacl.403</url>
      <bibkey>haider-etal-2025-banth</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.403</doi>
    </paper>
    <paper id="404">
      <title>Mutual Reinforcement of <fixed-case>LLM</fixed-case> Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization</title>
      <author><first>Yen-Ju</first><last>Lu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ting-Yao</first><last>Hu</last><affiliation>Apple</affiliation></author>
      <author><first>Hema Swetha</first><last>Koppula</last><affiliation>Apple</affiliation></author>
      <author><first>Hadi</first><last>Pouransari</last><affiliation>Apple</affiliation></author>
      <author><first>Jen-Hao Rick</first><last>Chang</last><affiliation>Apple</affiliation></author>
      <author><first>Yin</first><last>Xia</last><affiliation>Apple and JD.com Technology America</affiliation></author>
      <author><first>Xiang</first><last>Kong</last><affiliation>Apple</affiliation></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoming Simon</first><last>Wang</last><affiliation>Didi Research US</affiliation></author>
      <author><first>Oncel</first><last>Tuzel</last><affiliation>Apple</affiliation></author>
      <author><first>Raviteja</first><last>Vemulapalli</last><affiliation>Apple</affiliation></author>
      <pages>7237-7256</pages>
      <abstract>In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM’s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.</abstract>
      <url hash="76a8dd86">2025.findings-naacl.404</url>
      <bibkey>lu-etal-2025-mutual</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.404</doi>
    </paper>
    <paper id="405">
      <title><fixed-case>UNLEARN</fixed-case> Efficient Removal of Knowledge in Large Language Models</title>
      <author><first>Tyler</first><last>Lizzo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Larry</first><last>Heck</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7257-7268</pages>
      <abstract>Large Language Models (LLMs) excel in many Natural Language Processing tasks but are outperformed by specialized tools for certain tasks. This raises the question: Can we reduce redundant LLM parameters when using these tools? Given the size and high training costs of LLMs, it is essential to efficiently forget specific knowledge without retraining. This paper introduces UNLEARN, a novel method that uses subspace techniques to selectively remove knowledge without access to the original training data, without retraining, and with minimal impact to other tasks. Our results show that UNLEARN significantly outperforms previous methods for forgetting targeted (unwanted) knowledge while also preserving related (wanted) knowledge. We also propose LEARN, a complementary approach for targeted knowledge addition, which achieves fine-tuning accuracy comparable to Low-Rank Adaptation (LoRA) without degrading related task performance.</abstract>
      <url hash="050c2cac">2025.findings-naacl.405</url>
      <bibkey>lizzo-heck-2025-unlearn</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.405</doi>
    </paper>
    <paper id="406">
      <title>Adaptive Parameter Compression for Language Models</title>
      <author><first>Jeremias</first><last>Bohn</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Frederic</first><last>Mrozinski</last></author>
      <author><first>Georg</first><last>Groh</last><affiliation>Technical University Munich</affiliation></author>
      <pages>7269-7286</pages>
      <url hash="57cbaede">2025.findings-naacl.406</url>
      <bibkey>bohn-etal-2025-adaptive</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.406</doi>
    </paper>
    <paper id="407">
      <title>Personalize Your <fixed-case>LLM</fixed-case>: Fake it then Align it</title>
      <author><first>Yijing</first><last>Zhang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dyah</first><last>Adila</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Changho</first><last>Shin</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Frederic</first><last>Sala</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <pages>7287-7301</pages>
      <abstract>Personalizing large language models (LLMs) is essential for delivering tailored interactions that improve user experience. Many existing personalization methods require fine-tuning LLMs for each user, rendering them prohibitively expensive for widespread adoption. Although retrieval-based approaches offer a more compute-efficient alternative, they still depend on large, high-quality datasets that are not consistently available for all users. To address this challenge, we propose Chameleon, a scalable and efficient personalization approach that uses (1) self-generated personal preference data and (2) representation editing to enable quick and cost-effective personalization. Our experiments on various tasks, including those from the LaMP personalization benchmark, show that Chameleon efficiently adapts models to personal preferences, improving instruction-tuned models and outperforms two personalization baselines by an average of 40% across two model architectures.</abstract>
      <url hash="d4cbd989">2025.findings-naacl.407</url>
      <bibkey>zhang-etal-2025-personalize</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.407</doi>
    </paper>
    <paper id="408">
      <title>A Survey to Recent Progress Towards Understanding In-Context Learning</title>
      <author><first>Haitao</first><last>Mao</last></author>
      <author><first>Guangliang</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yao</first><last>Ma</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Rongrong</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>7302-7323</pages>
      <abstract>In-Context Learning (ICL) empowers Large Language Models (LLMs) with the ability to learn from a few examples provided in the prompt, enabling downstream generalization without the requirement for gradient updates. Despite encouragingly empirical success, the underlying mechanism of ICL remains unclear. Existing research remains ambiguous with various viewpoints, utilizing intuition-driven and ad-hoc technical solutions to interpret ICL. In this paper, we leverage a data generation perspective to reinterpret recent efforts from a systematic angle, demonstrating the potential broader usage of these popular technical solutions. For a conceptual definition, we rigorously adopt the terms of skill recognition and skill learning. Skill recognition selects one learned data generation function previously seen during pre-training while skill learning can learn new data generation functions from in-context data. Furthermore, we provide insights into the strengths and weaknesses of both abilities, emphasizing their commonalities through the perspective of data generation. This analysis suggests potential directions for future research. The corresponding paper list can be found here.</abstract>
      <url hash="bb6325aa">2025.findings-naacl.408</url>
      <bibkey>mao-etal-2025-survey</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.408</doi>
    </paper>
    <paper id="409">
      <title>Inference Scaling for Bridging Retrieval and Augmented Generation</title>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Daniel F</first><last>Campos</last><affiliation>Snowflake</affiliation></author>
      <author><first>Filip</first><last>Graliński</last><affiliation>Snowflake and Adam Mickiewicz University</affiliation></author>
      <author><first>Zhewei</first><last>Yao</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yuxiong</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>7324-7339</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a popular approach to steering the output of a large language model (LLM) by incorporating retrieved contexts as inputs. However, existing work observed the generator bias, such that improving the retrieval results may negatively affect the outcome. In this work, we show such bias can be mitigated, from inference scaling, aggregating inference calls from the permuted order of retrieved contexts. The proposed Mixture-of-Intervention (MoI) explicitly models the debiased utility of each passage with multiple forward passes to construct a new ranking. We also show that MoI can leverage the retriever’s prior knowledge to reduce the computational cost by minimizing the number of permutations considered and lowering the cost per LLM call. We showcase the effectiveness of MoI on diverse RAG tasks, improving ROUGE-L on MS MARCO and EM on HotpotQA benchmarks by ~7 points.</abstract>
      <url hash="9b7faf63">2025.findings-naacl.409</url>
      <bibkey>lee-etal-2025-inference</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.409</doi>
    </paper>
    <paper id="410">
      <title><fixed-case>G</fixed-case>eo<fixed-case>C</fixed-case>oder: Solving Geometry Problems by Generating Modular Code through Vision-Language Models</title>
      <author><first>Aditya</first><last>Sharma</last><affiliation>École Polytechnique de Montréal, Université de Montréal</affiliation></author>
      <author><first>Aman</first><last>Dalmia</last></author>
      <author><first>Mehran</first><last>Kazemi</last><affiliation>Google</affiliation></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <author><first>Christopher</first><last>Pal</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>7340-7356</pages>
      <abstract>Geometry problem-solving demands advanced reasoning abilities to process multimodal inputs and employ mathematical knowledge effectively. Vision-language models (VLMs) have made significant progress in various multimodal tasks. Yet, they still struggle with geometry problems and are significantly limited by their inability to perform mathematical operations not seen during pre-training, such as calculating the cosine of an arbitrary angle, and by difficulties in correctly applying relevant geometry formulas. To overcome these challenges, we present GeoCoder, which leverages modular code-finetuning to generate and execute code using a predefined geometry function library. By executing the code, we achieve accurate and deterministic calculations, contrasting the stochastic nature of autoregressive token prediction, while the function library minimizes errors in formula usage. We also propose a multimodal retrieval-augmented variant of GeoCoder, named RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving functions from the geometry library, thereby reducing reliance on parametric memory. Our modular code-finetuning approach enhances the geometric reasoning capabilities of VLMs, yielding an average improvement of over 16% across various question complexities on the GeomVerse dataset compared to other fine-tuning methods.</abstract>
      <url hash="5f2fda72">2025.findings-naacl.410</url>
      <bibkey>sharma-etal-2025-geocoder</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.410</doi>
    </paper>
    <paper id="411">
      <title><fixed-case>SEE</fixed-case>val: Advancing <fixed-case>LLM</fixed-case> Text Evaluation Efficiency and Accuracy through Self-Explanation Prompting</title>
      <author><first>Meng-Chen</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Md Mosharaf</first><last>Hossain</last><affiliation>Amazon</affiliation></author>
      <author><first>Tess</first><last>Wood</last><affiliation>Amazon</affiliation></author>
      <author><first>Shayan Ali</first><last>Akbar</last><affiliation>Amazon</affiliation></author>
      <author><first>Si-Chi</first><last>Chin</last><affiliation>Amazon</affiliation></author>
      <author><first>Erwin</first><last>Cornejo</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <pages>7357-7368</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success in various natural language generation (NLG) tasks, but their performance in automatic text evaluation is not yet ready as human replacements. In this paper, we propose SEEval (Self-Explanation in Evaluation), a novel prompt-based text evaluator. Inspired by educational psychology, SEEval incorporates self-explanation, a metacognitive strategy, to enhance automatic text evaluation. Our experimental results show that SEEval, without probability normalization, is able to achieve competitive and often superior performance compared to the two state-of-the-art baselines – G-Eval and Analyze-Rate – across all evaluation dimensions and is 20 times more efficient in terms of run-time. The SEEval method is also generalizable as its results are consistent across three other selected LLMs – Claude 3.5 Sonnet, Command R+, and Mistral-Large 2.</abstract>
      <url hash="8ebcc22d">2025.findings-naacl.411</url>
      <bibkey>wu-etal-2025-seeval</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.411</doi>
    </paper>
    <paper id="412">
      <title>When natural language is not enough: The limits of in-context learning demonstrations in multilingual reasoning</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7369-7396</pages>
      <abstract>Previous studies have demonstrated the effectiveness of reasoning methods in eliciting multi-step reasoned answers from Large Language Models (LLMs) by leveraging in-context demonstrations. These methods, exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), have been shown to perform well in monolingual contexts, primarily in English. There has, however, been limited exploration of their abilities in other languages.To gain a deeper understanding of the role of reasoning methods for in-context demonstrations, we investigate how well CoT and PAL perform across languages for arithmetic and symbolic reasoning tasks. Our findings indicate that the effectiveness of reasoning methods varies significantly across different languages and models. Specifically, CoT, which relies on natural language demonstrations, tends to be more accurate in high-resource than in low-resource languages. Conversely, the structured nature of PAL demonstrations facilitates multilingual comprehension, enabling LLMs to generate programmatic answers in both high- and low-resource languages and leading to significant performance improvements over CoT concerning the accuracy of the generated responses.</abstract>
      <url hash="6e1e5d73">2025.findings-naacl.412</url>
      <bibkey>ranaldi-etal-2025-natural</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.412</doi>
    </paper>
    <paper id="413">
      <title>Uncovering Latent Arguments in Social Media Messaging by Employing <fixed-case>LLM</fixed-case>s-in-the-Loop Strategy</title>
      <author><first>Tunazzina</first><last>Islam</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <pages>7397-7429</pages>
      <abstract>The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic **LLMs-in-the-Loop** strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Additionally, we design a downstream task as stance prediction by leveraging talking points in climate debates. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events.</abstract>
      <url hash="b419cd89">2025.findings-naacl.413</url>
      <bibkey>islam-goldwasser-2025-uncovering</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.413</doi>
    </paper>
    <paper id="414">
      <title><fixed-case>A</fixed-case>crostic<fixed-case>S</fixed-case>leuth: Probabilistic Identification and Ranking of Acrostics in Multilingual Corpora</title>
      <author><first>Aleksandr</first><last>Fedchin</last></author>
      <author><first>Isabel</first><last>Cooperman</last></author>
      <author><first>Pramit</first><last>Chaudhuri</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>7430-7437</pages>
      <abstract>For centuries, writers have hidden messages as acrostics, in which initial letters of consecutive lines or paragraphs form meaningful words or phrases. Scholars searching for acrostics manually can only focus on a few authors at a time and often favor qualitative arguments about whether a given acrostic is accidental or intentional. Here we describe AcrosticSleuth, a first-of-its-kind approach to identify acrostics automatically and rank them by the probability that the corresponding sequence of characters does not occur by chance. Since acrostics are rare, we formalize the problem as a binary classification task in the presence of extreme class imbalance. To evaluate AcrosticSleuth, we present the Acrostic Identification Dataset (AcrostID), a collection of acrostics from the WikiSource online database. Despite the class imbalance, AcrosticSleuth achieves F1 scores of 0.39, 0.59, and 0.66 on the French, English, and Russian subdomains of WikiSource, respectively. We further demonstrate that AcrosticSleuth can identify previously unknown instances of wordplay in high-profile literary contexts, including the English philosopher Thomas Hobbes’ signature in the opening paragraphs of The Elements of Law.</abstract>
      <url hash="736fbb11">2025.findings-naacl.414</url>
      <bibkey>fedchin-etal-2025-acrosticsleuth</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.414</doi>
    </paper>
    <paper id="415">
      <title><fixed-case>M</fixed-case>ed<fixed-case>T</fixed-case>hink: A Rationale-Guided Framework for Explaining Medical Visual Question Answering</title>
      <author><first>Xiaotang</first><last>Gai</last></author>
      <author><first>Chenyi</first><last>Zhou</last></author>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>7438-7450</pages>
      <abstract>Medical Visual Question Answering (Med-VQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing Med-VQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamline data preparation and build new benchmark Med-VQA datasets R-RAD, R-SLAKE and R-Path. These datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing Med-VQA datasets, i.e., VQA-RAD, SLAKE and PathVQA. Moreover, we design a novel framework, MedThink, which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales. MedThink includes three distinct strategies to generate decision outcomes and corresponding rationales, clearly showcasing the medical decision-making process during reasoning. Our comprehensive experiments show that our method achieves an accuracy of 83.5% on R-RAD, 86.3% on R-SLAKE and 87.2% on R-Path. These results significantly exceed those of existing state-of-the-art models with comparable parameters. Datasets and code are available at https://github.com/Tang-xiaoxiao/Medthink.</abstract>
      <url hash="6536a779">2025.findings-naacl.415</url>
      <bibkey>gai-etal-2025-medthink</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.415</doi>
    </paper>
    <paper id="416">
      <title>How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise in Machine Translation</title>
      <author><first>Yan</first><last>Meng</last></author>
      <author><first>Di</first><last>Wu</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>7451-7467</pages>
      <abstract>The massive amounts of web-mined parallel data often contain large amounts of noise. Semantic misalignment, as the primary source of the noise, poses a challenge for training machine translation systems. In this paper, we first introduce a process for simulating misalignment controlled by semantic similarity, which closely resembles misaligned sentences in real-world web-crawled corpora. Under our simulated misalignment noise settings, we quantitatively analyze its impact on machine translation and demonstrate the limited effectiveness of widely used pre-filters for noise detection. This underscores the necessity of more fine-grained ways to handle hard-to-detect misalignment noise. By analyzing the reliability of the model’s self-knowledge for distinguishing misaligned and clean data at the token level, we propose self-correction—an approach that gradually increases trust in the model’s self-knowledge to correct the supervision signal during training. Comprehensive experiments show that our method significantly improves translation performance both in the presence of simulated misalignment noise and when applied to real-world, noisy web-mined datasets, across a range of translation tasks.</abstract>
      <url hash="d9408dab">2025.findings-naacl.416</url>
      <bibkey>meng-etal-2025-learn</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.416</doi>
    </paper>
    <paper id="417">
      <title>Rejected Dialects: Biases Against <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican Language in Reward Models</title>
      <author><first>Joel</first><last>Mire</last></author>
      <author><first>Zubin Trivadi</first><last>Aysola</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Chechelnitsky</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Nicholas</first><last>Deas</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>7468-7487</pages>
      <abstract>Preference alignment via reward models helps build safe, helpful, and reliable large language models (LLMs). However, subjectivity in preference judgments and the lack of representative sampling in preference data collection can introduce new biases, hindering reward models’ fairness and equity. In this work, we introduce a framework for evaluating dialect biases in reward models and conduct a case study on biases against African American Language (AAL) through several experiments comparing reward model preferences and behavior on paired White Mainstream English (WME) and both machine-translated and human-written AAL corpora. We show that reward models are less aligned with human preferences when processing AAL texts vs. WME ones (-4% accuracy on average), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and steer conversations toward WME, even when prompted with AAL texts. Our findings provide a targeted analysis of anti-AAL biases at a relatively understudied stage in LLM development, highlighting representational harms and ethical questions about the desired behavior of LLMs concerning AAL.</abstract>
      <url hash="a95cbb71">2025.findings-naacl.417</url>
      <bibkey>mire-etal-2025-rejected</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.417</doi>
    </paper>
    <paper id="418">
      <title>Do Large Language Models Align with Core Mental Health Counseling Competencies?</title>
      <author><first>Viet Cuong</first><last>Nguyen</last></author>
      <author><first>Mohammad</first><last>Taher</last></author>
      <author><first>Dongwan</first><last>Hong</last></author>
      <author><first>Vinicius Konkolics</first><last>Possobom</last></author>
      <author><first>Vibha Thirunellayi</first><last>Gopalakrishnan</last></author>
      <author><first>Ekta</first><last>Raj</last></author>
      <author><first>Zihang</first><last>Li</last></author>
      <author><first>Heather J.</first><last>Soled</last><affiliation>Northwell Health</affiliation></author>
      <author><first>Michael L.</first><last>Birnbaum</last></author>
      <author><first>Srijan</first><last>Kumar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Munmun</first><last>De Choudhury</last></author>
      <pages>7488-7511</pages>
      <abstract>The rapid evolution of Large Language Models (LLMs) presents a promising solution to the global shortage of mental health professionals. However, their alignment with essential counseling competencies remains underexplored. We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22 general-purpose and medical-finetuned LLMs across five key competencies. While frontier models surpass minimum aptitude thresholds, they fall short of expert-level performance, excelling in Intake, Assessment &amp; Diagnosis but struggling with Core Counseling Attributes and Professional Practice &amp; Ethics. Surprisingly, medical LLMs do not outperform generalist models in accuracy, though they provide slightly better justifications while making more context-related errors. These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning. Our results underscore the need for specialized, fine-tuned models aligned with core mental health counseling competencies and supported by human oversight before real-world deployment. Code and data associated with this manuscript can be found at: https://github.com/cuongnguyenx/CounselingBench</abstract>
      <url hash="9115f733">2025.findings-naacl.418</url>
      <bibkey>nguyen-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.418</doi>
    </paper>
    <paper id="419">
      <title>Uncertainty Quantification for Clinical Outcome Predictions with (Large) Language Models</title>
      <author><first>Zizhang</first><last>Chen</last></author>
      <author><first>Peizhao</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Xiaomeng</first><last>Dong</last></author>
      <author><first>Pengyu</first><last>Hong</last><affiliation>Brandeis University</affiliation></author>
      <pages>7512-7523</pages>
      <abstract>To facilitate healthcare delivery, language models (LMs) have significant potential for clinical prediction tasks using electronic health records (EHRs). However, in these high-stakes applications, unreliable decisions can result in significant costs due to compromised patient safety and ethical concerns, thus increasing the need for good uncertainty modelling of automated clinical predictions. To address this, we consider uncertainty quantification of LMs for EHR tasks in both white-box and black-box settings. We first quantify uncertainty in white-box models, where we have access to model parameters and output logits. We show that an effective reduction of model uncertainty can be achieved by using the proposed multi-tasking and ensemble methods in EHRs. Continuing with this idea, we extend our approach to black-box settings, including popular proprietary LMs such as GPT-4. We validate our framework using longitudinal clinical data from over 6,000 patients across ten clinical prediction tasks. Results show that ensembling methods and multi-task prediction prompts reduce uncertainty across different scenarios. These findings increase model transparency in white-box and black-box settings, thereby advancing reliable AI healthcare.</abstract>
      <url hash="49d648c3">2025.findings-naacl.419</url>
      <bibkey>chen-etal-2025-uncertainty</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.419</doi>
    </paper>
    <paper id="420">
      <title>Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Shrinidhi</first><last>Kumbhar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Venkatesh</first><last>Mishra</last></author>
      <author><first>Kevin</first><last>Coutinho</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Divij</first><last>Handa</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ashif</first><last>Iquebal</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>7524-7555</pages>
      <abstract>Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.</abstract>
      <url hash="f0795211">2025.findings-naacl.420</url>
      <bibkey>kumbhar-etal-2025-hypothesis</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.420</doi>
    </paper>
    <paper id="421">
      <title>Aligning to What? Limits to <fixed-case>RLHF</fixed-case> Based Alignment</title>
      <author><first>Logan</first><last>Barnhart</last></author>
      <author><first>Reza</first><last>Akbarian Bafghi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Stephen</first><last>Becker</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Maziar</first><last>Raissi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>7556-7591</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to align large language models (LLMs) with human preferences. However, the effectiveness of RLHF in addressing underlying biases remains unclear. This study investigates the relationship between RLHF and both covert and overt biases in LLMs, particularly focusing on biases against African Americans. We applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and evaluated the covert and overt biases of the resulting models using matched-guise probing and explicit bias testing. We performed additional tests with DPO on different base models and datasets; among several implications, we found that SFT before RLHF calcifies model biases. Additionally, we extend the tools for measuring biases to multi-modal models. Through our experiments we collect evidence that indicates that current alignment techniques are inadequate for nebulous tasks such as mitigating covert biases, highlighting the need for capable datasets, data curating techniques, or alignment tools.</abstract>
      <url hash="6dbe4f2b">2025.findings-naacl.421</url>
      <bibkey>barnhart-etal-2025-aligning</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.421</doi>
    </paper>
    <paper id="422">
      <title>Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models</title>
      <author><first>Srishti</first><last>Yadav</last></author>
      <author><first>Zhi</first><last>Zhang</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>7592-7608</pages>
      <abstract>Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.</abstract>
      <url hash="830d59bf">2025.findings-naacl.422</url>
      <bibkey>yadav-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.422</doi>
    </paper>
    <paper id="423">
      <title>Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning</title>
      <author><first>Jeffrey</first><last>Olmo</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Jared</first><last>Wilson</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Max</first><last>Forsey</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Bryce</first><last>Hepner</last></author>
      <author><first>Thomas Vincent</first><last>Howe</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>David</first><last>Wingate</last><affiliation>Brigham Young University</affiliation></author>
      <pages>7609-7619</pages>
      <abstract>Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network’s internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. This limits the information available to learn features, and biases the autoencoder towards neglecting features which are represented with small activation values but strongly influence model outputs.To address this, we introduce Gradient SAEs (g-SAEs), which modify the <tex-math>k</tex-math>-sparse autoencoder architecture by augmenting the TopK activation function to rely on the gradients of the input activation when selecting the <tex-math>k</tex-math> elements. For a given sparsity level, g-SAEs produce reconstructions that are more faithful to original network performance when propagated through the network.Additionally, we find evidence that g-SAEs learn latents that are on average more effective at steering models in arbitrary contexts.By considering the downstream effects of activations, our approach leverages the dual nature of neural network features as both representations, retrospectively, and actions, prospectively. While previous methods have approached the problem of feature discovery primarily focused on the former aspect, g-SAEs represent a step towards accounting for the latter as well.</abstract>
      <url hash="371f02fc">2025.findings-naacl.423</url>
      <bibkey>olmo-etal-2025-features</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.423</doi>
    </paper>
    <paper id="424">
      <title>Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving</title>
      <author><first>Botao</first><last>Yu</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Frazier N.</first><last>Baker</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Ziru</first><last>Chen</last></author>
      <author><first>Garrett</first><last>Herb</last></author>
      <author><first>Boyu</first><last>Gou</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Daniel</first><last>Adu-Ampratwum</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <pages>7620-7640</pages>
      <abstract>To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents’ ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.</abstract>
      <url hash="31fb4835">2025.findings-naacl.424</url>
      <bibkey>yu-etal-2025-tooling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.424</doi>
    </paper>
    <paper id="425">
      <title><fixed-case>R</fixed-case>us<fixed-case>C</fixed-case>ode: <fixed-case>R</fixed-case>ussian Cultural Code Benchmark for Text-to-Image Generation</title>
      <author><first>Viacheslav</first><last>Vasilev</last></author>
      <author><first>Julia</first><last>Agafonova</last></author>
      <author><first>Nikolai</first><last>Gerasimenko</last></author>
      <author><first>Alexander</first><last>Kapitanov</last><affiliation>SberDevices</affiliation></author>
      <author><first>Polina</first><last>Mikhailova</last><affiliation>salute devices</affiliation></author>
      <author><first>Evelina</first><last>Mironova</last></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <pages>7641-7657</pages>
      <abstract>Text-to-image generation models have gained popularity among users around the world. However, many of these models exhibit a strong bias toward English-speaking cultures, ignoring or misrepresenting the unique characteristics of other language groups, countries, and nationalities. The lack of cultural awareness can reduce the generation quality and lead to undesirable consequences such as unintentional insult, and the spread of prejudice. In contrast to the field of natural language processing, cultural awareness in computer vision has not been explored as extensively. In this paper, we strive to reduce this gap. We propose a RusCode benchmark for evaluating the quality of text-to-image generation containing elements of the Russian cultural code. To do this, we form a list of 19 categories that best represent the features of Russian visual culture. Our final dataset consists of 1250 text prompts in Russian and their translations into English. The prompts cover a wide range of topics, including complex concepts from art, popular culture, folk traditions, famous people’s names, natural objects, scientific achievements, etc. We present the results of a human evaluation of the side-by-side comparison of Russian visual concepts representations using popular generative models.</abstract>
      <url hash="0bcea155">2025.findings-naacl.425</url>
      <bibkey>vasilev-etal-2025-ruscode</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.425</doi>
    </paper>
    <paper id="426">
      <title>Evaluation of <fixed-case>LLM</fixed-case>s-based Hidden States as Author Representations for Psychological Human-Centered <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Pranav</first><last>Chitale</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Khushboo</first><last>Singh</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>7658-7667</pages>
      <abstract>Like most of NLP, models for human-centered NLP tasks—tasks attempting to assess author-level information—predominantly use rep-resentations derived from hidden states of Transformer-based LLMs. However, what component of the LM is used for the representation varies widely. Moreover, there is a need for Human Language Models (HuLMs) that implicitly model the author and provide a user-level hidden state. Here, we systematically evaluate different ways of representing documents and users using different LM and HuLM architectures to predict task outcomes as both dynamically changing states and averaged trait-like user-level attributes of valence, arousal, empathy, and distress. We find that representing documents as an average of the token hidden states performs the best generally. Further, while a user-level hidden state itself is rarely the best representation, we find its inclusion in the model strengthens token or document embeddings used to derive document- and user-level representations resulting in best performances.</abstract>
      <url hash="df014c04">2025.findings-naacl.426</url>
      <bibkey>soni-etal-2025-evaluation</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.426</doi>
    </paper>
    <paper id="427">
      <title>Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey</title>
      <author><first>Xiaoyu</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Paiheng</first><last>Xu</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jiaxin</first><last>Yuan</last></author>
      <author><first>Yifan</first><last>Yang</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Tianrui</first><last>Guan</last></author>
      <author><first>Haoliang</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <pages>7668-7684</pages>
      <abstract>Causal inference has demonstrated significant potential to enhance Natural Language Processing (NLP) models in areas such as predictive accuracy, fairness, robustness, and explainability by capturing causal relationships among variables. The rise of generative Large Language Models (LLMs) has greatly impacted various language processing tasks. This survey focuses on research that evaluates or improves LLMs from a causal view in the following areas: reasoning capacity, fairness and safety issues, explainability, and handling multimodality. Meanwhile, LLMs can assist in causal inference tasks, such as causal relationship discovery and causal effect estimation, by leveraging their generation ability and knowledge learned during pre-training. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and robust artificial intelligence systems.</abstract>
      <url hash="75f478c3">2025.findings-naacl.427</url>
      <bibkey>liu-etal-2025-large-language</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.427</doi>
    </paper>
    <paper id="428">
      <title><fixed-case>T</fixed-case>hought<fixed-case>S</fixed-case>culpt: Reasoning with Intermediate Revision and Search</title>
      <author><first>Yizhou</first><last>Chi</last></author>
      <author><first>Kevin</first><last>Yang</last><affiliation>Scaled Cognition</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>7685-7711</pages>
      <abstract>We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).</abstract>
      <url hash="845bed86">2025.findings-naacl.428</url>
      <bibkey>chi-etal-2025-thoughtsculpt</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.428</doi>
    </paper>
    <paper id="429">
      <title>Optimizing Hidden <fixed-case>M</fixed-case>arkov Language Models: An Empirical Study of Reparameterization and Initialization Techniques</title>
      <author><first>Ivan</first><last>Lee</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>7712-7723</pages>
      <abstract>Hidden Markov models (HMMs) are valuable for their ability to provide exact and tractable inference. However, learning an HMM in an unsupervised manner involves a non-convex optimization problem that is plagued by poor local optima. Recent work on scaling-up HMMs to perform competitively as language models has indicated that this challenge only increases with larger hidden state sizes. Several techniques to address this problem have been proposed, but have not be evaluated comprehensively. This study provides a comprehensive empirical analysis of two recent strategies that use neural networks to enhance HMM optimization: neural reparameterization and neural initialization. We find that (1) these techniques work effectively for scaled HMM language modeling, (2) linear reparameterizations can be as effective as non-linear ones, and (3) the strategies are complementary.</abstract>
      <url hash="585c06fe">2025.findings-naacl.429</url>
      <bibkey>lee-berg-kirkpatrick-2025-optimizing</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.429</doi>
    </paper>
    <paper id="430">
      <title>Using Linguistic Entrainment to Evaluate Large Language Models for Use in Cognitive Behavioral Therapy</title>
      <author><first>Mina</first><last>Kian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kaleen</first><last>Shrestha</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Katrin</first><last>Fischer</last></author>
      <author><first>Xiaoyuan</first><last>Zhu</last></author>
      <author><first>Jonathan</first><last>Ong</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Aryan</first><last>Trehan</last></author>
      <author><first>Jessica</first><last>Wang</last></author>
      <author><first>Gloria</first><last>Chang</last></author>
      <author><first>Séb</first><last>Arnold</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Maja</first><last>Mataric</last><affiliation>University of Southern California</affiliation></author>
      <pages>7724-7743</pages>
      <abstract>Entrainment, the responsive communication between interacting individuals, is a crucial process in building a strong relationship between a mental health therapist and their client, leading to positive therapeutic outcomes. However, so far entrainment has not been investigated as a measure of efficacy of large language models (LLMs) delivering mental health therapy. In this work, we evaluate the linguistic entrainment of an LLM (ChatGPT 3.5-turbo) in a mental health dialog setting. We first validate computational measures of linguistic entrainment with two measures of the quality of client self-disclosures: intimacy and engagement (<tex-math>p &lt; 0.05</tex-math>). We then compare the linguistic entrainment of the LLM to trained therapists and non-expert online peer supporters in a cognitive behavioral therapy (CBT) setting. We show that the LLM is outperformed by humans with respect to linguistic entrainment (<tex-math>p &lt; 0.001</tex-math>). These results support the need to be cautious in using LLMs out-of-the-box for mental health applications.</abstract>
      <url hash="324a0614">2025.findings-naacl.430</url>
      <bibkey>kian-etal-2025-using</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.430</doi>
    </paper>
    <paper id="431">
      <title>Analysis of <fixed-case>LLM</fixed-case> as a grammatical feature tagger for <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Rahul</first><last>Porwal</last></author>
      <author><first>Alice</first><last>Rozet</last><affiliation>University of Florida</affiliation></author>
      <author><first>Jotsna</first><last>Gowda</last></author>
      <author><first>Pryce</first><last>Houck</last></author>
      <author><first>Kevin</first><last>Tang</last><affiliation>Heinrich Heine University Düsseldorf and University of Florida</affiliation></author>
      <author><first>Sarah</first><last>Moeller</last><affiliation>University of Florida</affiliation></author>
      <pages>7744-7756</pages>
      <abstract>African American English (AAE) presents unique challenges in natural language processing (NLP) This research systematically compares the performance of available NLP models—rule-based, transformer-based, and large language models (LLMs)—capable of identifying key grammatical features of AAE, namely Habitual Be and Multiple Negation. These features were selected for their distinct grammatical complexity and frequency of occurrence. The evaluation involved sentence-level binary classification tasks, using both zero-shot and few-shot strategies. The analysis reveals that while LLMs show promise compared to the baseline, they are influenced by biases such as recency and unrelated features in the text such as formality. This study highlights the necessity for improved model training and architectural adjustments to better accommodate AAE’s unique linguistic characteristics. Data and code are available.</abstract>
      <url hash="32ec130e">2025.findings-naacl.431</url>
      <bibkey>porwal-etal-2025-analysis</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.431</doi>
    </paper>
    <paper id="432">
      <title><fixed-case>LLM</fixed-case>-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers</title>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Matvey</first><last>Mikhalchuk</last><affiliation>Artificial Intelligence Research Institute (AIRI)</affiliation></author>
      <author><first>Temurbek</first><last>Rahmatullaev</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last><affiliation>Artificial Intelligence Research Institure and Higher School of Economics</affiliation></author>
      <author><first>Polina</first><last>Druzhinina</last><affiliation>Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology and Institute of Numerical Mathematics</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>AIRI, Sber and Samara National Research University</affiliation></author>
      <pages>7757-7764</pages>
      <abstract>We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens — especially stopwords, articles, and commas — consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer’s embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of “filler” tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.</abstract>
      <url hash="7532f42c">2025.findings-naacl.432</url>
      <bibkey>razzhigaev-etal-2025-llm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.432</doi>
    </paper>
    <paper id="433">
      <title>On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness Evaluation</title>
      <author><first>Xiaonan</first><last>Jing</last><affiliation>Expedia Group</affiliation></author>
      <author><first>Srinivas</first><last>Billa</last></author>
      <author><first>Danny</first><last>Godbout</last></author>
      <pages>7765-7780</pages>
      <abstract>Hallucination has been a popular topic in natural language generation (NLG). In real-world applications, unfaithful content can result in poor data quality or loss of trust from end users. Thus, it is crucial to fact-check before adopting NLG for production usage, which can be expensive if done manually. In this paper, we investigate automated faithfulness evaluation in guided NLG. We developed a rubric template and used large language models (LLMs) to score the generation on quantifiable scales. We compared popular LLMs as well as widely adopted natural language inference (NLI) models in scoring quality and sensitivity. In addition, we developed methods for the generation of synthetic unfaithful data, as well as heuristics to quantify the percentage of hallucination. Our results on 4 travel-domain industry dataset show that GPT-4 can provide accurate judgement and explanation of whether a source and a generation are factually consistent. Furthermore, we found that tuning NLI models on synthetic data can improve performance. Lastly, we present insights on the latency and cost of deploying such a system.</abstract>
      <url hash="6d8f01c0">2025.findings-naacl.433</url>
      <bibkey>jing-etal-2025-scale</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.433</doi>
    </paper>
    <paper id="434">
      <title><fixed-case>LITERA</fixed-case>: An <fixed-case>LLM</fixed-case> Based Approach to <fixed-case>L</fixed-case>atin-to-<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Paul</first><last>Rosu</last></author>
      <pages>7781-7794</pages>
      <abstract>This paper introduces an LLM-based Latin-to-English translation platform designed to address the challenges of translating Latin texts. We named the model LITERA, which stands for Latin Interpretation and Translations into English for Research Assistance. Through a multi-layered translation process utilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an unprecedented level of accuracy, showcased by greatly improved BLEU scores, particularly in classical Latin, along with improved BLEURT scores. The development of LITERA involved close collaboration with Duke University’s Classical Studies Department, which was instrumental in creating a small, high-quality parallel Latin-English dataset. This paper details the architecture, fine-tuning methodology, and prompting strategies used in LITERA, emphasizing its ability to produce literal translations.</abstract>
      <url hash="359942f8">2025.findings-naacl.434</url>
      <bibkey>rosu-2025-litera</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.434</doi>
    </paper>
    <paper id="435">
      <title>Investigating the Shortcomings of <fixed-case>LLM</fixed-case>s in Step-by-Step Legal Reasoning</title>
      <author><first>Venkatesh</first><last>Mishra</last></author>
      <author><first>Bimsara</first><last>Pathiraja</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Sat</first><last>Chidananda</last></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author><first>Gaowen</first><last>Liu</last></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>7795-7826</pages>
      <abstract>Reasoning abilities of LLMs have been a key focus in recent years. One challenging reasoning domain with interesting nuances is legal reasoning, which requires careful application of rules, and precedents while balancing deductive and analogical reasoning, and conflicts between rules. Although there have been a few works on using LLMs for legal reasoning, their focus has been on overall accuracy. In this paper, we dig deeper to do a step-by-step analysis and figure out where they commit errors. We use the college-level Multiple Choice Question-Answering (MCQA) task from the <i>Civil Procedure</i> dataset and propose a new error taxonomy derived from initial manual analysis of reasoning chains with respect to several LLMs, including two objective measures: soundness and correctness scores. We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs. The computation of soundness and correctness on the dataset using the auto-evaluator framework reveals several interesting insights. Furthermore, we show that incorporating the error taxonomy as feedback in popular prompting techniques marginally increases LLM performance. Our work will also serve as an evaluation framework that can be used in detailed error analysis of reasoning chains for logic-intensive complex tasks.</abstract>
      <url hash="566b91c8">2025.findings-naacl.435</url>
      <bibkey>mishra-etal-2025-investigating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.435</doi>
    </paper>
    <paper id="436">
      <title>Towards Long Context Hallucination Detection</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Qi</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Phu Mon</first><last>Htut</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>7827-7835</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference. We publicly release our dataset and code to promote research along the same line.</abstract>
      <url hash="8bcf1d68">2025.findings-naacl.436</url>
      <bibkey>liu-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.436</doi>
    </paper>
    <paper id="437">
      <title>How to Talk to Language Models: Serialization Strategies for Structured Entity Matching</title>
      <author><first>Haoteng</first><last>Yin</last></author>
      <author><first>Jinha</first><last>Kim</last><affiliation>Amazon</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>Amazon</affiliation></author>
      <author><first>Krishanu</first><last>Sarker</last><affiliation>Amazon</affiliation></author>
      <author><first>Vidit</first><last>Bansal</last><affiliation>Amazon</affiliation></author>
      <pages>7836-7850</pages>
      <abstract>Entity matching (EM), which identifies whether two data records refer to the same real-world entity, is crucial for knowledge base construction and enhancing data-driven AI systems. Recent advances in language models (LMs) have shown great potential in resolving entities with rich textual attributes. However, their performance heavily depends on how structured entities are “talked” through serialized text. The impact of this serialization process remains underexplored, particularly for entities with complex relations in knowledge graphs (KGs). In this work, we systematically study entity serialization by benchmarking the effect of common schemes with LMs of different sizes on diverse tabular matching datasets. We apply our findings to propose a novel serialization scheme for KG entities based on random walks and utilize LLMs to encode sampled semantic walks for matching. Using this lightweight approach with open-source LLMs, we achieve a leading performance on EM in canonical and highly heterogeneous KGs, demonstrating significant throughput increases and superior robustness compared to GPT-4-based methods. Our study on serialization provides valuable insights for the deployment of LMs in real-world EM tasks.</abstract>
      <url hash="cb879736">2025.findings-naacl.437</url>
      <bibkey>yin-etal-2025-talk</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.437</doi>
    </paper>
    <paper id="438">
      <title>Accounting for Sycophancy in Language Model Uncertainty Estimation</title>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Mert</first><last>Inan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>7851-7866</pages>
      <abstract>Effective human-machine collaboration requires machine learning models to externalize uncertainty, so users can reflect and intervene when necessary. For language models, these representations of uncertainty may be impacted by sycophancy bias: proclivity to agree with users, even if they are wrong. For instance, models may be over-confident in (incorrect) problem solutions suggested by a user. We study the relationship between sycophancy and uncertainty estimation for the first time. We propose a generalization of the definition of sycophancy bias to measure downstream impacts on uncertainty estimation, and also propose a new algorithm (SyRoUP) to account for sycophancy in the uncertainty estimation process. Unlike previous works, we study a broad array of user behaviors, varying both correctness and confidence of user suggestions to see how model answers (and their certainty) change. Our experiments across conversation forecasting and question-answering tasks show that user confidence plays a critical role in modulating the effects of sycophancy, and that SyRoUP can better predict these effects. From these results, we argue that externalizing both model <i>and</i> user uncertainty can help to mitigate the impacts of sycophancy bias.</abstract>
      <url hash="d9ee80c9">2025.findings-naacl.438</url>
      <bibkey>sicilia-etal-2025-accounting</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.438</doi>
    </paper>
    <paper id="439">
      <title>Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-sample Aggregation on Large Language Models</title>
      <author><first>Jishnu</first><last>Ray Chowdhury</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Jayanth</first><last>Mohan</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Tomas</first><last>Malik</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>7867-7884</pages>
      <abstract>Keyphrases are the essential topical phrases that summarize a document. Keyphrase generation is a long-standing NLP task for automatically generating keyphrases for a given document. While the task has been comprehensively explored in the past via various models, only a few works perform some preliminary analysis of Large Language Models (LLMs) for the task. Given the impact of LLMs in the field of NLP, it is important to conduct a more thorough examination of their potential for keyphrase generation. In this paper, we attempt to meet this demand with our research agenda. Specifically, we focus on the zero-shot capabilities of open-source instruction-tuned LLMs (Phi-3, Llama-3) and the closed-source GPT-4o for this task. We systematically investigate the effect of providing task-relevant specialized instructions in the prompt. Moreover, we design task-specific counterparts to self-consistency-style strategies for LLMs and show significant benefits from our proposals over the baselines.</abstract>
      <url hash="29cc32ef">2025.findings-naacl.439</url>
      <bibkey>ray-chowdhury-etal-2025-zero</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.439</doi>
    </paper>
    <paper id="440">
      <title>Meta-Reasoning Improves Tool Use in Large Language Models</title>
      <author><first>Lisa</first><last>Alazraki</last></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <pages>7885-7897</pages>
      <abstract>External tools help large language models succeed at tasks where they would otherwise typically fail. In existing frameworks, choosing tools at test time relies on naive greedy decoding, regardless of whether the model has been fine-tuned on tool-annotated data or prompted with in-context examples. In contrast, we find that gathering and choosing among a suitable set of candidate tools has greater potential to lead to an optimal selection. We present Tool selECTion via meta-reasONing (TECTON), a two-phase system that first *reasons* over a task and outputs candidate tools using a custom fine-tuned language modelling head. Then, with the custom head disabled, it *meta-reasons* (i.e., it reasons over the previous reasoning process) to make a final choice. We show that TECTON results in substantial gains—both in-distribution and out-of-distribution—on a range of math reasoning datasets.</abstract>
      <url hash="dd6a649a">2025.findings-naacl.440</url>
      <bibkey>alazraki-rei-2025-meta</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.440</doi>
    </paper>
    <paper id="441">
      <title><fixed-case>CLERC</fixed-case>: A Dataset for <fixed-case>U</fixed-case>. <fixed-case>S</fixed-case>. Legal Case Retrieval and Retrieval-Augmented Analysis Generation</title>
      <author><first>Abe Bohan</first><last>Hou</last></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Eugene</first><last>Yang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Nils</first><last>Holzenberger</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Andrew</first><last>Blair-Stanek</last><affiliation>Johns Hopkins University and University of Maryland School of Law</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>7898-7913</pages>
      <abstract>Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligence systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to create a colossal dataset. supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset **CLERC** (Case Law Evaluation and Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3% recall@1000.</abstract>
      <url hash="b37f64c7">2025.findings-naacl.441</url>
      <bibkey>hou-etal-2025-clerc</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.441</doi>
    </paper>
    <paper id="442">
      <title><fixed-case>GAI</fixed-case>f<fixed-case>E</fixed-case>: Using <fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case> to Improve Literacy in Low-resourced Settings</title>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Nouhoum</first><last>Coulibaly</last></author>
      <author><first>Seydou</first><last>Diallo</last></author>
      <author><first>Sebastien</first><last>Diarra</last></author>
      <author><first>Christopher M</first><last>Homan</last></author>
      <author><first>Mamadou K.</first><last>Keita</last></author>
      <author><first>Michael</first><last>Leventhal</last><affiliation>RobotsMali</affiliation></author>
      <pages>7914-7929</pages>
      <abstract>Illiteracy is a predictor of many negative social and personal outcomes. Illiteracy rates are particularly high in countries with underresourced languages, where few books exist that are suitable for children to learn to read from. We present GAIfE (Generative AI for Education), a toolchain and workflow developed through empirical methods, that demonstrates how existing tools can be adapted to address low literacy for an underresourced language. We used GAIfE (a play on the Bambara word for “book”) to construct materials for developing children’s reading competence in Bambara, the vehicular language of Mali. Our approach to the generation and post-generation editing of content skewed by the Global-North-centric bias of available LLMs, enabled us to rapidly multiply the content in Bambara available online by 10 times while maintaining high standards of attractiveness of the material to maintain high engagement, accurate representation of the Malian culture and physical and social environment and language quality. Using our materials, pilot reading programs achieved a 67% reduction in the number of children unable to read Bambara. Our approach demonstrated the power of bias-aware application of generative AI to the problem domain as well as the potential impact the application of this technology could have on reducing illiteracy and improving learning outcomes through native language education.</abstract>
      <url hash="d711c00a">2025.findings-naacl.442</url>
      <bibkey>tapo-etal-2025-gaife</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.442</doi>
    </paper>
    <paper id="443">
      <title>Hard Emotion Test Evaluation Sets for Language Models</title>
      <author><first>Tiberiu</first><last>Sosea</last><affiliation>Google</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>7930-7944</pages>
      <abstract>Language models perform well on emotion datasets but it remains unclear whether these models indeed understand emotions expressed in text or simply exploit supperficial lexical cues (e.g., emotion words). In this paper, we present two novel test evaluation sets sourced from two existing datasets that allow us to evaluate whether language models make real inferential decisions for emotion detection or not. Our human-annotated test sets are created by iteratively rephrasing input texts to gradually remove explicit emotion cues (while preserving the semantic similarity and the emotions) until a strong baseline BERT model yields incorrect predictions. Using our new test sets, we carry out a comprehensive analysis into the capabilities of small and large language models to predict emotions. Our analysis reveals that all models struggle to correctly predict emotions when emotion lexical cues become scarcer and scarcer, but large language models perform better than small pre-trained language models and push the performance by 14% over the 5% BERT baseline. We make our evaluation test sets and code publicly available.</abstract>
      <url hash="1cfbf7bb">2025.findings-naacl.443</url>
      <bibkey>sosea-caragea-2025-hard</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.443</doi>
    </paper>
    <paper id="444">
      <title><fixed-case>UCL</fixed-case>-Bench: A <fixed-case>C</fixed-case>hinese User-Centric Legal Benchmark for Large Language Models</title>
      <author><first>Ruoli</first><last>Gan</last></author>
      <author><first>Duanyu</first><last>Feng</last></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhihang</first><last>Lin</last><affiliation>Westlake Scietrain</affiliation></author>
      <author><first>Haochen</first><last>Jia</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Zhenyang</first><last>Cai</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Qianqian</first><last>Xie</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>7945-7988</pages>
      <abstract>Existing legal benchmarks focusing on knowledge and logic effectively evaluate LLMs on various tasks in legal domain. However, few have explored the practical application of LLMs by actual users. To further assess whether LLMs meet the specific needs of legal practitioners in real-world scenarios, we introduce UCL-Bench, a Chinese User-Centric Legal Benchmark, comprising 22 tasks across 5 distinct legal scenarios.To build the UCL-Bench, we conduct a user survey targeting legal professionals to understand their needs and challenges. Based on the survey results, we craft tasks, verified by legal professionals, and categorized them according to Bloom’s taxonomy. Each task in UCL-Bench mirrors real-world legal scenarios, and instead of relying on pre-defined answers, legal experts provide detailed answer guidance for each task, incorporating both “information” and “needs” elements to mimic the complexities of legal practice. With the guidance, we use GPT-4 as the user simulator and evaluator, enabling multi-turn dialogues as a answer guidance based evaluation framework. Our findings reveal that many recent open-source general models achieve the highest performance, suggesting that they are well-suited to address the needs of legal practitioners. However, these legal LLMs do not outperform ChatGPT, indicating a need for training strategies aligned with users’ needs. Furthermore, we find that the most effective models are able to address legal issues within fewer dialogue turns, highlighting the importance of concise and accurate responses in achieving high performance. The code and dataset are available at https://github.com/wittenberg11/UCL-bench.</abstract>
      <url hash="d871dfd5">2025.findings-naacl.444</url>
      <bibkey>gan-etal-2025-ucl</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.444</doi>
    </paper>
    <paper id="445">
      <title><fixed-case>MIDAS</fixed-case>: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn <fixed-case>NLU</fixed-case></title>
      <author><first>Yan</first><last>Li</last></author>
      <author><first>So-Eon</first><last>Kim</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Seong-Bae</first><last>Park</last></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <pages>7989-8012</pages>
      <abstract>Although Large Language Models (LLMs) can generate coherent text, they often struggle to recognise user intent behind queries. In contrast, Natural Language Understanding (NLU) models interpret the purpose and key information of user input for responsive interactions. Existing NLU models typically map utterances to a dual-level semantic frame, involving sentence-level intent (SI) and word-level slot (WS) labels. However, real-life conversations primarily consist of multi-turn dialogues, requiring the interpretation of complex and extended exchanges. Researchers encounter challenges in addressing all facets of multi-turn dialogue using a unified NLU model. This paper introduces MIDAS, a novel approach leveraging multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. We construct distinct teachers for SI detection, WS filling, and conversation-level domain (CD) classification, each fine-tuned for specific knowledge. A multi-teacher loss is proposed to facilitate the integration of these teachers, guiding a student model in multi-turn dialogue tasks. Results demonstrate the efficacy of our model in improving multi-turn conversation understanding, showcasing the potential for advancements in NLU through multi-level dialogue knowledge distillation. Our implementation is open-sourced on GitHub (https://github.com/adlnlp/Midas).</abstract>
      <url hash="6451c451">2025.findings-naacl.445</url>
      <bibkey>li-etal-2025-midas</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.445</doi>
    </paper>
    <paper id="446">
      <title>A Practical Analysis of Human Alignment with *<fixed-case>PO</fixed-case></title>
      <author><first>Kian</first><last>Ahrabian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xihui</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alon</first><last>Benhaim</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xia</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <pages>8013-8021</pages>
      <abstract>At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.</abstract>
      <url hash="f7173f65">2025.findings-naacl.446</url>
      <bibkey>ahrabian-etal-2025-practical</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.446</doi>
    </paper>
    <paper id="447">
      <title>Understanding Reference Policies in Direct Preference Optimization</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>8022-8037</pages>
      <abstract>Direct Preference Optimization (DPO) has become a widely used training method for the instruction fine-tuning of large language models (LLMs). In this work, we explore an under-investigated aspect of DPO – its dependency on the reference model or policy. Such reference policies, typically instantiated as the model to be further fine-tuned, are important since they can impose an upper limit on DPO’s effectiveness. Therefore, we address three related research questions in this work. First, we explore the optimal strength of the KL divergence constraint in DPO, which penalizes deviations from the reference policy, and find that DPO is sensitive to this strength. Next, we examine the necessity of the KL-constraint from the reference policies in DPO by providing both theoretical and empirical comparisons between DPO and related learning objectives, demonstrating DPO’s superiority in this controlled setting. Additionally, we investigate whether DPO benefits from stronger reference policies, finding that a stronger reference policy can lead to improved performance, but only when it is similar to the model being fine-tuned. Our findings highlight the confounding role of reference policies in DPO and offer insights for best practices, while also identifying open research questions for future studies.</abstract>
      <url hash="be7b5487">2025.findings-naacl.447</url>
      <bibkey>liu-etal-2025-understanding</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.447</doi>
    </paper>
    <paper id="448">
      <title><fixed-case>LLM</fixed-case>-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
      <author><first>Saaket</first><last>Agashe</last></author>
      <author><first>Yue</first><last>Fan</last></author>
      <author><first>Anthony</first><last>Reyna</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Xin Eric</first><last>Wang</last><affiliation>Simular and University of California, Santa Cruz</affiliation></author>
      <pages>8038-8057</pages>
      <abstract>Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners’ beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs’ Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement.</abstract>
      <url hash="94ce435d">2025.findings-naacl.448</url>
      <bibkey>agashe-etal-2025-llm</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.448</doi>
    </paper>
    <paper id="449">
      <title><fixed-case>A</fixed-case>ssertion<fixed-case>B</fixed-case>ench: A Benchmark to Evaluate Large-Language Models for Assertion Generation</title>
      <author><first>Vaishnavi</first><last>Pulavarthi</last></author>
      <author><first>Deeksha</first><last>Nandal</last></author>
      <author><first>Soham</first><last>Dan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Debjit</first><last>Pal</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>8058-8065</pages>
      <abstract>Assertions have been the de facto collateral for hardware for over a decade. The verification quality, i.e., detection and diagnosis of corner-case design bugs, is critically dependent on the assertion quality. There has been a considerable amount of research to generate high-quality assertions from hardware design source code and design execution trace data. With recent advent of generative AI techniques such as Large-Language Models (LLMs), there has been a renewed interest in deploying LLMs for assertion generation. However, there is little effort to quantitatively establish the effectiveness and suitability of various LLMs for assertion generation. In this paper, we present AssertionBench, a novel benchmark to evaluate LLMs’ effectiveness for assertion generation quantitatively. AssertionBench contains 100 curated Verilog hardware designs from OpenCores and formally verified assertions for each design, generated from GoldMine and HARM. We use AssertionBench to compare state-of-the-art LLMs, e.g., GPT-3.5, GPT-4o, CodeLLaMa-2, and LLaMa3-70B, to assess their effectiveness in inferring functionally correct assertions for hardware designs. Our experiments comprehensively demonstrate how LLMs perform relative to each other, the benefits of using more in-context exemplars in generating a higher fraction of functionally correct assertions, and the significant room for improvement for LLM-based assertion generators.</abstract>
      <url hash="f8507bb8">2025.findings-naacl.449</url>
      <bibkey>pulavarthi-etal-2025-assertionbench</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.449</doi>
    </paper>
    <paper id="450">
      <title>On Reference (In-)Determinacy in Natural Language Inference</title>
      <author><first>Sihao</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chaitanya</first><last>Malaviya</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Alex</first><last>Fabrikant</last><affiliation>Google Research</affiliation></author>
      <author><first>Hagai</first><last>Taitelbaum</last><affiliation>Research, Google</affiliation></author>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google DeepMind and Google</affiliation></author>
      <author><first>Senaka</first><last>Buthpitiya</last><affiliation>Google</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>8066-8078</pages>
      <abstract>We revisit the reference determinacy (RD) assumption in the task of natural language inference (NLI), i.e., the premise and hypothesis are assumed to refer to the same context when human raters annotate a label. While RD is a practical assumption for constructing a new NLI dataset, we observe that current NLI models—which are typically trained solely on hypothesis-premise pairs created with the RD assumption—fail in downstream applications such as fact verification, where the input premise and hypothesis may refer to different contexts. To highlight the impact of this phenomenon in real-world use cases, we introduce RefNLI, a diagnostic benchmark for identifying reference ambiguity in NLI examples. In RefNLI, the premise is retrieved from a knowledge source (i.e. Wikipedia) and does not necessarily refer to the same context as the hypothesis. With RefNLI, we demonstrate that finetuned NLI models and few-shot prompted LLMs both fail to recognize context mismatch, leading to &gt;80% false contradiction and &gt;50% entailment predictions. We discover that the existence of reference ambiguity in NLI examples can in part explain the inherent human disagreements in NLI, and provide insight into how the RD assumption impacts NLI dataset creation process.</abstract>
      <url hash="919fe26b">2025.findings-naacl.450</url>
      <bibkey>chen-etal-2025-reference</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.450</doi>
    </paper>
    <paper id="451">
      <title><fixed-case>DHP</fixed-case> Benchmark: Are <fixed-case>LLM</fixed-case>s Good <fixed-case>NLG</fixed-case> Evaluators?</title>
      <author><first>Yicheng</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiayi</first><last>Yuan</last></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Zhuoer</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yingchi</first><last>Liu</last><affiliation>Axon</affiliation></author>
      <author><first>Mark</first><last>Cusick</last><affiliation>Axon Enterprise, Inc.</affiliation></author>
      <author><first>Param</first><last>Kulkarni</last><affiliation>Axon</affiliation></author>
      <author><first>Zhengping</first><last>Ji</last><affiliation>Axon</affiliation></author>
      <author><first>Yasser</first><last>Ibrahim</last><affiliation>Axon</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>8079-8094</pages>
      <abstract>Large Language Models (LLMs) are increasingly serving as evaluators in Natural Language Generation (NLG) tasks; this is often referred to as “LLM-as-a-judge” paradigm. However, the capabilities of LLMs in evaluating NLG quality remain underexplored. Current studies depend on human assessments and simple metrics that fail to capture the discernment of LLMs across diverse NLG tasks. To address this gap, we propose the Discernment of Hierarchical Perturbation (DHP) benchmarking framework, which provides quantitative discernment scores for LLMs. This framework leverages hierarchically perturbed text data and statistical tests to systematically measure the NLG evaluation capabilities of LLMs. We re-established six evaluation datasets for this benchmark, covering four NLG tasks: Summarization, Story Completion, Question Answering, and Translation. Our comprehensive benchmarking of five major LLM families provides critical insight into their strengths and limitations as NLG evaluators. Our dataset is available at https://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.</abstract>
      <url hash="afa4546a">2025.findings-naacl.451</url>
      <bibkey>wang-etal-2025-dhp</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.451</doi>
    </paper>
    <paper id="452">
      <title><fixed-case>G</fixed-case>raph<fixed-case>E</fixed-case>val36<fixed-case>K</fixed-case>: Benchmarking Coding and Reasoning Capabilities of Large Language Models on Graph Datasets</title>
      <author><first>Qiming</first><last>Wu</last></author>
      <author><first>Zichen</first><last>Chen</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Will</first><last>Corcoran</last></author>
      <author><first>Misha</first><last>Sra</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Ambuj</first><last>Singh</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>8095-8117</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating significant capabilities in processing and understanding text data. However, recent studies have identified limitations in LLMs’ ability to manipulate, program, and reason about structured data, especially graphs. We introduce GraphEval36K, the first comprehensive graph dataset, comprising 40 graph coding problems and 36,900 test cases to evaluate the ability of LLMs on graph problem-solving. Our dataset is categorized into eight primary and four sub-categories to ensure a thorough evaluation across different types of graphs. We benchmark eight LLMs, finding that private models outperform open-source ones, though the gap is narrowing. We also analyze the performance of LLMs across directed vs undirected graphs, different kinds of graph concepts, and network models. Furthermore, to improve the usability of our evaluation framework, we propose Structured Symbolic Decomposition (SSD), an instruction-based method designed to enhance LLM performance on complex graph tasks. Results show that SSD improves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and Claude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.</abstract>
      <url hash="39a0cc74">2025.findings-naacl.452</url>
      <bibkey>wu-etal-2025-grapheval36k</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.452</doi>
    </paper>
    <paper id="453">
      <title><fixed-case>S</fixed-case>imul<fixed-case>B</fixed-case>ench: Evaluating Language Models with Creative Simulation Tasks</title>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tuney</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Huang</last><affiliation>xAI</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <pages>8118-8131</pages>
      <abstract>We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation tasks, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM’s general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on SimulBench, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these creative simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55% more cases.</abstract>
      <url hash="474724f7">2025.findings-naacl.453</url>
      <bibkey>jia-etal-2025-simulbench</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.453</doi>
    </paper>
    <paper id="454">
      <title><fixed-case>R</fixed-case>easoning<fixed-case>R</fixed-case>ec: Bridging Personalized Recommendations and Human-Interpretable Explanations through <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Millennium</first><last>Bismay</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiangjue</first><last>Dong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>James</first><last>Caverlee</last><affiliation>Google and Texas A&amp;M University - College Station</affiliation></author>
      <pages>8132-8148</pages>
      <abstract>This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user preferences, subsequently used to fine-tune a smaller LLM for enhanced recommendation accuracy and human-interpretable explanation. Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM’s capacity to generate plausible explanations. Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art methods by up to 12.5% in recommendation prediction while concurrently providing human-intelligible explanations.</abstract>
      <url hash="34ac9a98">2025.findings-naacl.454</url>
      <bibkey>bismay-etal-2025-reasoningrec</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.454</doi>
    </paper>
    <paper id="455">
      <title>2<fixed-case>D</fixed-case>-<fixed-case>DPO</fixed-case>: Scaling Direct Preference Optimization with 2-Dimensional Supervision</title>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hangyu</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weixun</first><last>Wang</last></author>
      <author><first>Jihao</first><last>Gu</last></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>8149-8173</pages>
      <abstract>Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.</abstract>
      <url hash="5d5dce95">2025.findings-naacl.455</url>
      <bibkey>li-etal-2025-2d</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.455</doi>
    </paper>
    <paper id="456">
      <title>Demystifying the Power of Large Language Models in Graph Generation</title>
      <author><first>Yu</first><last>Wang</last><affiliation>University of Oregon and Vanderbilt University</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Namyong</first><last>Park</last><affiliation>Meta AI</affiliation></author>
      <author><first>Nesreen K.</first><last>Ahmed</last><affiliation>Intel AI Research</affiliation></author>
      <author><first>Danai</first><last>Koutra</last><affiliation>Amazon and University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tyler</first><last>Derr</last><affiliation>Vanderbilt University</affiliation></author>
      <pages>8174-8189</pages>
      <abstract>Despite the unprecedented success of applying Large Language Models (LLMs) to graph discriminative tasks such as node classification and link prediction, its potential for graph structure generation remains largely unexplored. To fill this crucial gap, this paper presents a systematic investigation into the capability of LLMs for graph structure generation. Specifically, we design prompts triggering LLMs to generate codes that optimize network properties by injecting domain expertise from network science. Since graphs in different domains exhibit unique structural properties captured by various metrics (e.g., clustering coefficient capturing triangles in social networks while squares reflecting road segments in transportation networks), we first evaluate the capability of LLMs to generate graphs satisfying each structural property in different domains. After that, we select the optimal property configurations and benchmark the graph structure generation performance of LLMs against established graph generative models across multiple domains. Our findings shed light on generating graph structures from an LLM perspective. Our code is publically available https://github.com/yuwvandy/LLM-GraphGen.</abstract>
      <url hash="b71f2349">2025.findings-naacl.456</url>
      <bibkey>wang-etal-2025-demystifying</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.456</doi>
    </paper>
    <paper id="457">
      <title><fixed-case>COIG</fixed-case>-<fixed-case>CQIA</fixed-case>: Quality is All You Need for <fixed-case>C</fixed-case>hinese Instruction Fine-tuning</title>
      <author><first>Yuelin</first><last>Bai</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xeron</first><last>Du</last></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Leo</first><last>Jin</last></author>
      <author><first>Junting</first><last>Zhou</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Mingshan</first><last>Chang</last></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Xincheng</first><last>Zhang</last></author>
      <author><first>Nuo</first><last>Ma</last><affiliation>01.ai</affiliation></author>
      <author><first>Zekun Moore</first><last>Wang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Haihong</first><last>Wu</last></author>
      <author><first>Hongquan</first><last>Lin</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>8190-8205</pages>
      <abstract>Remarkable progress on large language models (LLMs), particularly in English, has facilitated impressive capabilities in following human instructions. However, there remains a noticeable gap in instruction fine-tuning for Chinese, where the complex linguistic features pose significant challenges. Existing datasets, generally distilled from English-centric LLMs, are not well-aligned with Chinese users’ interaction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese instruction tuning dataset derived from various real-world data resources and undergoing comprehensive human verification. We conduct extensive experiments on COIG-CQIA, and compare them with strong baseline models and datasets. The experimental results show that models trained on COIG-CQIA achieve highly competitive performance in diverse benchmarks. Additionally, our findings offer several insights for designing effective Chinese instruction-tuning datasets and data mixing strategies. Our dataset are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA.</abstract>
      <url hash="324a0bef">2025.findings-naacl.457</url>
      <bibkey>bai-etal-2025-coig</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.457</doi>
    </paper>
    <paper id="458">
      <title>Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation</title>
      <author><first>Yu</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Intuit AI Research</affiliation></author>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <author><first>Wendi</first><last>Cui</last><affiliation>Intuit</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <pages>8206-8217</pages>
      <abstract>In tasks such as summarization and open-book question answering (QA), Large Language Models (LLMs) frequently experience “contextual hallucination”, where they generate irrelevant or incorrect responses despite having access to accurate information in the input. This issue often stems from the models’ propensity to prioritize self-generated content over input context, leading to a disregard for pertinent details. To address this challenge, we introduce, Guided Attention Map Editing (GAME), an innovative approach that dynamically adjusts attention maps to enhance contextual relevance. During inference, GAME employs a trained classifier to identify attention maps likely to induce hallucinations and implements targeted interventions. These interventions, guided by gradient-informed “edit directions”, strategically redistribute attention weights across various heads to efficiently mitigate hallucination. Extensive evaluations on challenging summarization and open-book QA tasks demonstrate that GAME consistently and significantly reduces hallucinations across diverse open-source models, thereby improving the reliability and applicability of LLMs.</abstract>
      <url hash="87e13d50">2025.findings-naacl.458</url>
      <bibkey>wang-etal-2025-gradient</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.458</doi>
    </paper>
    <paper id="459">
      <title>Alleviating Hallucinations of Large Language Models through Induced Hallucinations</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>8218-8232</pages>
      <abstract>Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as hallucination. In this work, we propose a simple Induce-then-Contrast Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and FActScore, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various task formats, model sizes, and model families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively, without compromising their generalization capabilities on other tasks.</abstract>
      <url hash="3d7fe18f">2025.findings-naacl.459</url>
      <bibkey>zhang-etal-2025-alleviating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.459</doi>
    </paper>
    <paper id="460">
      <title><fixed-case>M</fixed-case>o<fixed-case>DE</fixed-case>: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts</title>
      <author><first>Lin</first><last>Ning</last><affiliation>Google</affiliation></author>
      <author><first>Harsh</first><last>Lara</last><affiliation>Research, Google</affiliation></author>
      <author><first>Meiqi</first><last>Guo</last><affiliation>Google</affiliation></author>
      <author><first>Abhinav</first><last>Rastogi</last><affiliation>Google</affiliation></author>
      <pages>8233-8246</pages>
      <abstract>Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA) have revolutionized the adaptation of large language models (LLMs) to diverse tasks. Recent efforts have explored mixtures of LoRA modules for multi-task settings. However, our analysis reveals redundancy in the down-projection matrices of these architectures. This observation motivates our proposed method, Mixture of Dyadic Experts (MoDE), which introduces a novel design for efficient multi-task adaptation. This is done by sharing the down-projection matrix across tasks and employing atomic rank-one adapters, coupled with routers that allow more sophisticated task-level specialization. Our design allows for more fine-grained mixing, thereby increasing the model’s ability to jointly handle multiple tasks. We evaluate MoDE on the Supernatural Instructions (SNI) benchmark consisting of a diverse set of 700+ tasks and demonstrate that it outperforms state-of-the-art multi-task parameter-efficient fine-tuning (PEFT) methods, without introducing additional parameters. Our findings contribute to a deeper understanding of parameter efficiency in multi-task LLM adaptation and provide a practical solution for deploying high-performing, lightweight models.</abstract>
      <url hash="bb0acea4">2025.findings-naacl.460</url>
      <bibkey>ning-etal-2025-mode</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.460</doi>
    </paper>
    <paper id="461">
      <title>Unsupervised Sentence Representation Learning with Syntactically Aligned Negative Samples</title>
      <author><first>Zhilan</first><last>Wang</last></author>
      <author><first>Zekai</first><last>Zhi</last></author>
      <author><first>Rize</first><last>Jin</last><affiliation>Tiangong University</affiliation></author>
      <author><first>Kehui</first><last>Song</last><affiliation>Tiangong University</affiliation></author>
      <author><first>He</first><last>Wang</last><affiliation>tiangong university</affiliation></author>
      <author><first>Da-Jung</first><last>Cho</last><affiliation>Ajou University</affiliation></author>
      <pages>8247-8259</pages>
      <abstract>Sentence representation learning benefits from data augmentation strategies to improve model performance and generalization, yet existing approaches often encounter issues such as semantic inconsistencies and feature suppression. To address these limitations, we propose a method for generating Syntactically Aligned Negative (SAN) samples through a semantic importance-aware Masked Language Model (MLM) approach. Our method quantifies semantic contributions of individual words to produce negative samples that have substantial textual overlap with the original sentences while conveying different meanings. We further introduce Hierarchical-InfoNCE (HiNCE), a novel contrastive learning objective employing differential temperature weighting to optimize the utilization of both in-batch and syntactically aligned negative samples. Extensive evaluations across seven semantic textual similarity benchmarks demonstrate consistent improvements over state-of-the-art models.</abstract>
      <url hash="2516d657">2025.findings-naacl.461</url>
      <bibkey>wang-etal-2025-unsupervised</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.461</doi>
    </paper>
    <paper id="462">
      <title>Hierarchical Speculative Decoding with Dynamic Window</title>
      <author><first>Shensian</first><last>Syu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>8260-8273</pages>
      <abstract>Speculative Decoding (SD) utilizes an efficient draft model to generate multiple tokens, which are subsequently verified in parallel by a target model. This approach has shown significant potential for accelerating inference in large language models (LLMs), with performance heavily reliant on the hyperparameter <tex-math>K</tex-math>—the window size. However, previous methods often depend on simple heuristics to select <tex-math>K</tex-math> or dynamically adjust the window size, which may necessitate additional training or careful resource management to avoid competition.To address these challenges, we propose <b>H</b>ierarchical <b>S</b>peculative <b>D</b>ecoding with <b>D</b>ynamic <b>W</b>indow (HSDDW), a straightforward framework that eliminates the need for additional training. Specifically, we introduce a <i>self-verify</i> mechanism that enables the draft model to autonomously decide when to stop generating tokens. Additionally, by integrating a hierarchical structure that leverages the capabilities of models of different sizes, we significantly enhance the overall speed of the system.HSDDW demonstrates competitive performance across four datasets, achieving notable speedups of <tex-math>2.91\times</tex-math> on MT-Bench and <tex-math>2.99\times</tex-math> on Alpaca, outperforming existing state-of-the-art methods.</abstract>
      <url hash="b3d5d0c8">2025.findings-naacl.462</url>
      <bibkey>syu-lee-2025-hierarchical</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.462</doi>
    </paper>
    <paper id="463">
      <title><fixed-case>Q</fixed-case>-<fixed-case>FAKER</fixed-case>: Query-free Hard Black-box Attack via Controlled Generation</title>
      <author><first>CheolWon</first><last>Na</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>8274-8289</pages>
      <abstract>Many adversarial attack approaches are proposed to verify the vulnerability of language models. However, they require numerous queries and the information on the target model. Even black-box attack methods also require the target model’s output information. They are not applicable in real-world scenarios, as in hard black-box settings where the target model is closed and inaccessible. Even the recently proposed hard black-box attacks still require many queries and demand extremely high costs for training adversarial generators. To address these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a novel and efficient method that generates adversarial examples without accessing the target model. To avoid accessing the target model, we use a surrogate model instead. The surrogate model generates adversarial sentences for a target-agnostic attack. During this process, we leverage controlled generation techniques. We evaluate our proposed method on eight datasets. Experimental results demonstrate our method’s effectiveness including high transferability and the high quality of the generated adversarial examples, and prove its practical in hard black-box settings.</abstract>
      <url hash="48df05f8">2025.findings-naacl.463</url>
      <bibkey>na-etal-2025-q</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.463</doi>
    </paper>
    <paper id="464">
      <title><fixed-case>PRD</fixed-case>etect: Perturbation-Robust <fixed-case>LLM</fixed-case>-generated Text Detection Based on Syntax Tree</title>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Zhiyi</first><last>Yin</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hexiang</first><last>Tan</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaoling</first><last>Jing</last></author>
      <author><first>Du</first><last>Su</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8290-8301</pages>
      <abstract>As LLM-generated text becomes increasingly prevalent on the internet, often containing hallucinations or biases, detecting such content has emerged as a critical area of research.Recent methods have demonstrated impressive performance in detecting text generated entirely by LLMs.However, in real-world scenarios, users often introduce perturbations to the LLM-generated text, and the robustness of existing detection methods against these perturbations has not been sufficiently explored.This paper empirically investigates this challenge and finds that even minor perturbations can severely degrade the performance of current detection methods. To address this issue, we find that the syntactic tree is minimally affected by disturbances and exhibits distinct differences between human-written and LLM-generated text.Therefore, we propose a detection method based on syntactic trees, which can capture features invariant to perturbations.It demonstrates significantly improved robustness against perturbation on the HC3 and GPT-3.5-mixed datasets.Moreover, it also has the shortest time expenditure.We provide the code and data at https://github.com/thulx18/PRDetect.</abstract>
      <url hash="d9b7be06">2025.findings-naacl.464</url>
      <bibkey>li-etal-2025-prdetect</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.464</doi>
    </paper>
    <paper id="465">
      <title>Enabling Natural Zero-Shot Prompting on Encoder Models via Statement-Tuning</title>
      <author><first>Ahmed</first><last>Elshabrawy</last></author>
      <author><first>Yongxin</first><last>Huang</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8302-8321</pages>
      <abstract>While Large Language Models (LLMs) exhibit remarkable capabilities in zero-shot and few-shot scenarios, they often require computationally prohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT and RoBERTa achieve state-of-the-art results through fine-tuning but struggle with extending to few-shot and zero-shot settings due to their architectural constraints. Hence, we propose Statement-Tuning, a technique that models discriminative tasks as a set of finite statements and trains an encoder model to discriminate between the potential statements to determine the label. We do Statement-Tuning on multiple tasks to enable cross-task generalization. Experimental results demonstrate that Statement-Tuning achieves competitive performance compared to state-of-the-art LLMs with significantly fewer parameters. Furthermore, we compare with previous encoder-based methodology and show that our method is more accurate and more robust to spurious patterns. Moreover, the study investigates the impact of several design choices on few-shot and zero-shot generalization, revealing that Statement-Tuning can achieve strong performance with modest training data and benefits from task and statement diversity for unseen task generalizability. We release all the code used to generate statement data, train and evaluate our Statement-Tuned models.</abstract>
      <url hash="476d7fb3">2025.findings-naacl.465</url>
      <bibkey>elshabrawy-etal-2025-enabling</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.465</doi>
    </paper>
    <paper id="466">
      <title>Faster Machine Translation Ensembling with Reinforcement Learning and Competitive Correction</title>
      <author><first>Kritarth</first><last>Prasad</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Mohammadi</first><last>Zaki</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Pratik Rakesh</first><last>Singh</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Pankaj</first><last>Wasnik</last><affiliation>Sony Research India</affiliation></author>
      <pages>8322-8335</pages>
      <abstract>Ensembling neural machine translation (NMT) models to produce higher-quality translations than the <tex-math>L</tex-math> individual models has been extensively studied. Recent methods typically employ a candidate selection block (CSB) and an encoder-decoder fusion block (FB), requiring inference across <i>all</i> candidate models, leading to significant computational overhead, generally <tex-math>\Omega(L)</tex-math>. This paper introduces <b>SmartGen</b>, a reinforcement learning (RL)-based strategy that improves the CSB by selecting a small, fixed number of candidates and identifying optimal groups to pass to the fusion block for each input sentence. Furthermore, previously, the CSB and FB were trained independently, leading to suboptimal NMT performance. Our DQN-based <b>SmartGen</b> addresses this by using feedback from the FB block as a reward during training. We also resolve a key issue in earlier methods, where candidates were passed to the FB without modification, by introducing a Competitive Correction Block (CCB). Finally, we validate our approach with extensive experiments on English-Hindi translation tasks in both directions as well as English to Chinese and English to German.</abstract>
      <url hash="3e724685">2025.findings-naacl.466</url>
      <bibkey>prasad-etal-2025-faster</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.466</doi>
    </paper>
    <paper id="467">
      <title>Evaluating Numeracy of Language Models as a Natural Language Inference Task</title>
      <author><first>Rahmad</first><last>Mahendra</last><affiliation>Royal Melbourne Institute of Technology and Universitas Indonesia</affiliation></author>
      <author><first>Damiano</first><last>Spina</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Lawrence</first><last>Cavedon</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Karin</first><last>Verspoor</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>8336-8361</pages>
      <abstract>While recent advancements in large language models (LLMs) have enhanced their capabilities to solve mathematical problems, other aspects of numeracy remain underexplored. In this paper, we propose a benchmark to evaluate the ability of language models to perform basic numeracy tasks. We frame numeracy as a Natural Language Inference (NLI) task to assess the models’ ability to understand both numbers and language contexts. We evaluate 49 language models (LMs), including fine-tuned LMs on NLI datasets, instruction-tuned LLMs, and specialized math-LLMs. Our findings reveal three main insights: (1) LLMs only clearly outperform smaller LMs in arithmetic tasks, indicating that mathematical reasoning cannot be generalized to other numeracy skills such as number comparison and normalization; (2) while most language models achieve fair to good accuracy for NLI entailment cases, they still struggle to predict contradiction and neutral cases; and (3) the robustness of language models’ numeracy capabilities needs improvement, particularly in understanding the semantics and pragmatics of numbers in linguistic contexts.</abstract>
      <url hash="0feefebb">2025.findings-naacl.467</url>
      <bibkey>mahendra-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.467</doi>
    </paper>
    <paper id="468">
      <title>Are Language Models Agnostic to Linguistically Grounded Perturbations? A Case Study of <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Poulami</first><last>Ghosh</last><affiliation>Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>8362-8396</pages>
      <abstract>Pre-trained language models (PLMs) are known to be susceptible to perturbations to the input text, but existing works do not explicitly focus on linguistically grounded attacks, which are subtle and more prevalent in nature. In this paper, we study whether PLMs are agnostic to linguistically grounded attacks or not. To this end, we offer the first study addressing this, investigating different Indic languages and various downstream tasks. Our findings reveal that although PLMs are susceptible to linguistic perturbations, when compared to non-linguistic attacks, PLMs exhibit a slightly lower susceptibility to linguistic attacks. This highlights that even constrained attacks are effective. Moreover, we investigate the implications of these outcomes across a range of languages, encompassing diverse language families and different scripts.</abstract>
      <url hash="0dc09779">2025.findings-naacl.468</url>
      <bibkey>ghosh-etal-2025-language</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.468</doi>
    </paper>
    <paper id="469">
      <title>Do <fixed-case>LLM</fixed-case>s Have Distinct and Consistent Personality? <fixed-case>TRAIT</fixed-case>: Personality Testset designed for <fixed-case>LLM</fixed-case>s with Psychometrics</title>
      <author><first>Seungbeen</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungwon</first><last>Lim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Giyeong</first><last>Oh</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Beong-woo</first><last>Kwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yeonsoo</first><last>Lee</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>8397-8437</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have led to their adaptation in various domains as conversational agents. We wonder: can personality tests be applied to these agents to analyze their behavior, similar to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice questions designed to assess the personality of LLMs. TRAIT is built on two psychometrically validated small human questionnaires, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a variety of real-world scenarios. TRAIT also outperforms existing personality tests for LLMs in terms of reliability and validity, achieving the highest scores across four key metrics: Content Validity, Internal Validity, Refusal Rate, and Reliability. Using TRAIT, we reveal two notable insights into personalities of LLMs: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (e.g., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.</abstract>
      <url hash="17ab50f9">2025.findings-naacl.469</url>
      <bibkey>lee-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.469</doi>
    </paper>
    <paper id="470">
      <title>Tell Me What You Know About Sexism: Expert-<fixed-case>LLM</fixed-case> Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection</title>
      <author><first>Myrthe</first><last>Reuver</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Matteo</first><last>Melis</last></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>GESIS – Leibniz Institute for the Social Sciences and Heinrich-Heine University Düsseldorf</affiliation></author>
      <pages>8438-8467</pages>
      <abstract>This paper investigates hybrid intelligence and collaboration between researchers of sexism and Large Language Models (LLMs), with afour-component pipeline. First, nine sexism researchers answer questions about their knowledge of sexism and of LLMs. They then participate in two interactive experiments involving an LLM (GPT3.5). The first experiment has experts assessing the model’s knowledgeabout sexism and suitability for use in research. The second experiment tasks them with creating three different definitions of sexism: anexpert-written definition, an LLM-written one, and a co-created definition. Lastly, zero-shot classification experiments use the three definitions from each expert in a prompt template for sexism detection, evaluating GPT4o on 2.500 texts sampled from five sexism benchmarks. We then analyze the resulting 67.500 classification decisions. The LLM interactions lead to longer and more complex definitions of sexism. Expert-written definitions on average perform poorly compared to LLM-generated definitions. However, some experts do improve classification performance with their co-created definitions of sexism, also experts who are inexperienced in using LLMs.</abstract>
      <url hash="117f5692">2025.findings-naacl.470</url>
      <bibkey>reuver-etal-2025-tell</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.470</doi>
    </paper>
    <paper id="471">
      <title>The Role of Prosody in Spoken Question Answering</title>
      <author><first>Jie</first><last>Chi</last></author>
      <author><first>Maureen</first><last>de Seyssel</last><affiliation>Apple</affiliation></author>
      <author><first>Natalie</first><last>Schluter</last><affiliation>Technical University of Denmark, Apple and IT University</affiliation></author>
      <pages>8468-8479</pages>
      <abstract>Spoken language understanding research to date has generally carried a heavy text perspective. Most datasets are derived from text, which is then subsequently synthesized into speech, and most models typically rely on automatic transcriptions of speech. This is to the detriment of prosody–additional information carried by the speech signal beyond the phonetics of the words themselves and difficult to recover from text alone. In this work, we investigate the role of prosody in Spoken Question Answering. By isolating prosodic and lexical information on the SLUE-SQA-5 dataset, which consists of natural speech, we demonstrate that models trained on prosodic information alone can perform reasonably well by utilizing prosodic cues. However, we find that when lexical information is available, models tend to predominantly rely on it. Our findings suggest that while prosodic cues provide valuable supplementary information, more effective integration methods are required to ensure prosody contributes more significantly alongside lexical features.</abstract>
      <url hash="8b6f9646">2025.findings-naacl.471</url>
      <bibkey>chi-etal-2025-role</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.471</doi>
    </paper>
    <paper id="472">
      <title>Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation Generation</title>
      <author><first>Palaash</first><last>Goel</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Dushyant Singh</first><last>Chauhan</last></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>8480-8493</pages>
      <abstract>Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a <b>T</b>arget-a<b>U</b>gmented sha<b>R</b>ed fusion-<b>B</b>ased sarcasm explanati<b>O</b>n model, aka. TURBO. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. TURBO assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed TURBO model on the MORE+ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of TURBO by an average margin of +3.3%. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human mevaluation on TURBO’s generated explanations and find them out to be comparatively better than other systems.</abstract>
      <url hash="735769c9">2025.findings-naacl.472</url>
      <bibkey>goel-etal-2025-target</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.472</doi>
    </paper>
    <paper id="473">
      <title>Seeds of Discourse: A Multilingual Corpus of Direct Quotations from <fixed-case>A</fixed-case>frican Media on Agricultural Biotechnologies</title>
      <author><first>Patricia</first><last>Chiril</last></author>
      <author><first>Trevor</first><last>Spreadbury</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Joeva Sean</first><last>Rock</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Brian</first><last>Dowd-Uribe</last></author>
      <author><first>David</first><last>Uminsky</last><affiliation>University of Chicago</affiliation></author>
      <pages>8494-8500</pages>
      <abstract>Direct quotations play a crucial role in journalism by substantiating claims and enhancing persuasive communication. This makes news articles a rich resource for opinion mining, providing valuable insights into the topics they cover. This paper presents the first multilingual corpora (English and French) featuring both manually annotated (1,657) and automatically extracted (102,483) direct quotations related to agricultural biotechnologies from a curated list of Africa-based news sources. In addition, we provide 665 instances annotated for Aspect-Based Sentiment Analysis, enabling a fine-grained examination of sentiment toward key aspects of agricultural biotechnologies. These corpora are freely available to the research community for future work on media discourse surrounding agricultural biotechnologies.</abstract>
      <url hash="284c13ea">2025.findings-naacl.473</url>
      <bibkey>chiril-etal-2025-seeds</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.473</doi>
    </paper>
    <paper id="474">
      <title>Position Really Matters: Towards a Holistic Approach for Prompt Tuning</title>
      <author><first>Xianjun</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Wenchao</first><last>Yu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Linda Ruth</first><last>Petzold</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <pages>8501-8523</pages>
      <abstract>Prompt tuning is highly effective in efficiently extracting knowledge from foundation models, encompassing both language, vision, and vision-language models. However, the efficacy of employing fixed soft prompts with a <i>predetermined position</i> for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. We first provide a theoretical analysis, revealing that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Then, we present a holistic parametric prompt tuning strategy that dynamically determines different factors of prompts based on specific tasks or instances. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP, vision recognition, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask settings.</abstract>
      <url hash="fa05121e">2025.findings-naacl.474</url>
      <bibkey>yang-etal-2025-position</bibkey>
      <doi>10.18653/v1/2025.findings-naacl.474</doi>
    </paper>
  </volume>
  <volume id="acl" ingest-date="2025-07-16" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: ACL 2025</booktitle>
      <editor><first>Wanxiang</first><last>Che</last></editor>
      <editor><first>Joyce</first><last>Nabende</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Mohammad Taher</first><last>Pilehvar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <venue>findings</venue>
      <isbn>979-8-89176-256-5</isbn>
    </meta>
    <frontmatter>
      <url hash="3d4e4c35">2025.findings-acl.0</url>
      <bibkey>findings-ws-2025-acl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection</title>
      <author><first>Yachao</first><last>Zhao</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Dongming</first><last>Zhao</last></author>
      <author><first>Ruifang</first><last>He</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>1-12</pages>
      <abstract>Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs.We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes.We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.</abstract>
      <url hash="d7100e35">2025.findings-acl.1</url>
      <bibkey>zhao-etal-2025-explicit</bibkey>
    </paper>
    <paper id="2">
      <title>Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task</title>
      <author><first>Yanbei</first><last>Jiang</last></author>
      <author><first>Yihao</first><last>Ding</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Chao</first><last>Lei</last></author>
      <author><first>Jiayang</first><last>Ao</last></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Krista A.</first><last>Ehinger</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>13-45</pages>
      <abstract>Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn’t explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages. The dataset and code will be available after acceptance.</abstract>
      <url hash="59483d1e">2025.findings-acl.2</url>
      <bibkey>jiang-etal-2025-beyond</bibkey>
    </paper>
    <paper id="3">
      <title>How Numerical Precision Affects Arithmetical Reasoning Capabilities of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Guhao</first><last>Feng</last></author>
      <author><first>Kai</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuntian</first><last>Gu</last></author>
      <author><first>Xinyue</first><last>Ai</last></author>
      <author><first>Shengjie</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiacheng</first><last>Sun</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Di</first><last>He</last><affiliation>Peking University and Microsoft</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>46-85</pages>
      <abstract>Despite the remarkable success of transformer-based large language models (LLMs) across various domains, understanding and enhancing their mathematical capabilities remains a significant challenge. In this paper, we conduct a rigorous theoretical analysis of LLMs’ mathematical abilities, with a specific focus on their arithmetic performances. We identify numerical precision as a key factor that influences their effectiveness in arithmetical tasks. Our results show that Transformers operating with low numerical precision fail to address arithmetic tasks, such as iterated addition and integer multiplication, unless the model size grows super-polynomially with respect to the input length. In contrast, Transformers with standard numerical precision can efficiently handle these tasks with significantly smaller model sizes. We further support our theoretical findings through empirical experiments that explore the impact of varying numerical precision on arithmetic tasks, providing valuable insights for improving the mathematical reasoning capabilities of LLMs.</abstract>
      <url hash="d60aafbc">2025.findings-acl.3</url>
      <bibkey>feng-etal-2025-numerical</bibkey>
    </paper>
    <paper id="4">
      <title>Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts</title>
      <author><first>Zeliang</first><last>Zhang</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Xiaodong</first><last>Liu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Chenliang</first><last>Xu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>86-102</pages>
      <abstract>In this work, we address the memory overhead of deploying Mixture-of-Experts (MoE) architectures in Large Language Models (LLMs). While MoE layers improve LLM performance without increasing inference costs, the ever-growing number of experts inflates memory requirements, hindering practical deployment. Our empirical study reveals that some experts encode redundant knowledge during pre-training. We thus propose a method of grouping and pruning similar experts to improve the model’s parameter efficiency. We validate the effectiveness of our method by pruning three state-of-the-art MoE architectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows that our method outperforms other model pruning methods on a range of natural language tasks.</abstract>
      <url hash="258bca67">2025.findings-acl.4</url>
      <bibkey>zhang-etal-2025-diversifying</bibkey>
    </paper>
    <paper id="5">
      <title>A Persona-Aware <fixed-case>LLM</fixed-case>-Enhanced Framework for Multi-Session Personalized Dialogue Generation</title>
      <author><first>Dongshuo</first><last>Liu</last></author>
      <author><first>Zhijing</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Dandan</first><last>Song</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>103-123</pages>
      <abstract>Multi-session personalized dialogue generation is one of the most important topics in open-domain dialogue. It aims to generate responses consistent with the dialogue history and personality information across multiple sessions to engage users’ interest in the dialogue. Recent approaches focusing on history modeling and persona modeling have advanced the development of this field. However, they overlook the importance of dialogue structure in helping large language models (LLMs) understand the dialogue context. Moreover, these methods do not efficiently expand and utilize personality information, reducing the responses’ consistency. In this paper, we propose a Persona-Aware LLM-enAnCEd(PALACE) framework for multi-session personalized dialogue generation. Specifically, the framework consists of three components: a topic-aware memory bank, a persona prompt learning module, and VAE-LoRA. The topic-aware memory bank works by retrieving historical information that possesses a certain dialogue structure and relevant topics. The persona prompt learning module enhances the LLM’s persona-aware capabilities by utilizing a persona commonsense knowledge graph and a query-driven graph neural network. Furthermore, to enhance the generative capabilities of the LLM and obtain more useful prior knowledge, we combine VAE with LoRA to propose VAE-LoRA. Experimental results on the MSC and DuLeMon dataset demonstrate that our framework outperforms the state-of-the-art methods in automatic and human evaluation metrics.</abstract>
      <url hash="57ac7eda">2025.findings-acl.5</url>
      <bibkey>liu-etal-2025-persona</bibkey>
    </paper>
    <paper id="6">
      <title>Exploring In-Image Machine Translation with Real-World Background</title>
      <author><first>Yanzhi</first><last>Tian</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Zhengyang</first><last>Liu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>124-137</pages>
      <abstract>In-Image Machine Translation (IIMT) aims to translate texts within images from one language to another. Previous research on IIMT was primarily conducted on simplified scenarios such as images of one-line text with black font in white backgrounds, which is far from reality and impractical for applications in the real world. To make IIMT research practically valuable, it is essential to consider a complex scenario where the text backgrounds are derived from real-world images. To facilitate research of complex scenarios IIMT, we design an IIMT dataset that includes subtitle text with a real-world background. However, previous IIMT models perform inadequately in complex scenarios. To address the issue, we propose the DebackX model, which separates the background and text-image from the source image, performs translation on the text-image directly, and fuses the translated text-image with the background to generate the target image. Experimental results show that our model achieves improvements in both translation quality and visual effect.</abstract>
      <url hash="9f983d50">2025.findings-acl.6</url>
      <bibkey>tian-etal-2025-exploring</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>ayes<fixed-case>KD</fixed-case>: <fixed-case>B</fixed-case>ayesian Knowledge Distillation for Compact <fixed-case>LLM</fixed-case>s in Constrained Fine-tuning Scenarios</title>
      <author><first>Wei</first><last>Li</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Lujun</first><last>Li</last></author>
      <author><first>Mark G.</first><last>Lee</last></author>
      <author><first>Shengjie</first><last>Sun</last></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>University of Exeter</affiliation></author>
      <author><first>Wei</first><last>Xue</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>138-152</pages>
      <abstract>Large language models (LLMs) have revolutionized various domains with their remarkable capabilities, but their massive parameter sizes pose significant challenges for fine-tuning and inference, especially in resource-constrained environments. Conventional compression methods often result in substantial performance degradation within LLMs and struggle to restore model quality during fine-tuning. To address this challenge, we present Bayesian Knowledge Distillation (BayesKD), a novel distillation framework meticulously designed for compact LLMs in resource-constrained fine-tuning scenarios. Departing from conventional LLM distillation methods that introduce time-consuming paradigms and fail to generalize in compressed LLM fine-tuning scenarios, our BayesKD develops the Logits Dual-Scaling, Knowledge Alignment Module, and Bayesian Distillation Optimization. In particular, our Logits Dual-Scaling strategy adaptively aligns the strength of the teacher’s knowledge transfer, while the Knowledge Alignment Module bridges the gap between the teacher and student models by projecting their knowledge representations into a shared interval. Additionally, we employ Logits-Aware Bayesian Optimization to swiftly identify optimal settings based on these strategies, thereby enhancing model performance. Extensive experiments across diverse tasks demonstrate that BayesKD consistently outperforms baseline methods on various state-of-the-art LLMs, including LLaMA, Qwen2, Bloom, and Vicuna. Notably, our BayesKD achieves average accuracy gains of 2.99% and 4.05% over standard KD for the 8B parameter LLaMA and Qwen2 model. Codes are available in the supplementary materials.</abstract>
      <url hash="5b7cb28a">2025.findings-acl.7</url>
      <bibkey>li-etal-2025-bayeskd</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>GOLF</fixed-case>er: Smaller <fixed-case>LM</fixed-case>s-Generated Documents Hallucination Filter &amp; Combiner for Query Expansion in Information Retrieval</title>
      <author><first>Lingyuan</first><last>Liu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Mengxiang</first><last>Zhang</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>153-162</pages>
      <abstract>Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter &amp; Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLMs-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.</abstract>
      <url hash="cd05f687">2025.findings-acl.8</url>
      <bibkey>liu-zhang-2025-golfer</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>E</fixed-case>xp4<fixed-case>F</fixed-case>use: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion</title>
      <author><first>Lingyuan</first><last>Liu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Mengxiang</first><last>Zhang</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>163-173</pages>
      <abstract>Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes—one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.</abstract>
      <url hash="60f1528c">2025.findings-acl.9</url>
      <bibkey>liu-zhang-2025-exp4fuse</bibkey>
    </paper>
    <paper id="10">
      <title>Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification</title>
      <author><first>Alexander</first><last>Shvets</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <pages>174-191</pages>
      <abstract>Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.</abstract>
      <url hash="012b95fa">2025.findings-acl.10</url>
      <bibkey>shvets-2025-emo</bibkey>
    </paper>
    <paper id="11">
      <title>Multi-Prompting Decoder Helps Better Language Understanding</title>
      <author><first>Zifeng</first><last>Cheng</last></author>
      <author><first>Zhaoling</first><last>Chen</last></author>
      <author><first>Zhiwei</first><last>Jiang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yafeng</first><last>Yin</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Cong</first><last>Wang</last><affiliation>Singapore Management University and Nanjing University</affiliation></author>
      <author><first>Shiping</first><last>Ge</last></author>
      <author><first>Qing</first><last>Gu</last><affiliation>Nanjing University</affiliation></author>
      <pages>192-208</pages>
      <abstract>Recent large Pre-trained Language Models (PLMs) usually only provide users with the inference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt MaaS PLMs to downstream tasks without accessing their parameters and gradients, some existing methods focus on the output-side adaptation of PLMs, viewing the PLM as an encoder and then optimizing a task-specific decoder for decoding the output hidden states and class scores of the PLM. Despite the effectiveness of these methods, they only use a single prompt to query PLMs for decoding, leading to a heavy reliance on the quality of the adopted prompt. In this paper, we propose a simple yet effective Multi-Prompting Decoder (MPD) framework for MaaS adaptation. The core idea is to query PLMs with multiple different prompts for each sample, thereby obtaining multiple output hidden states and class scores from PLMs for subsequent decoding. Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt, alleviate the issue of data scarcity under the few-shot setting, and provide richer knowledge extracted from PLMs. Specifically, we propose two decoding strategies: multi-prompting decoding with optimal transport for hidden states and calibrated decoding for class scores. Extensive experiments demonstrate that our method achieves new state-of-the-art results on multiple natural language understanding datasets under the few-shot setting.</abstract>
      <url hash="4140992b">2025.findings-acl.11</url>
      <bibkey>cheng-etal-2025-multi</bibkey>
    </paper>
    <paper id="12">
      <title>Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction</title>
      <author><first>Sam O’Connor</first><last>Russell</last></author>
      <author><first>Naomi</first><last>Harte</last><affiliation>University of Dublin, Trinity College</affiliation></author>
      <pages>209-221</pages>
      <abstract>Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate natural- istic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconfer- encing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio- only turn-taking model across all durations of speaker transitions. We conduct a detailed abla- tion study, which reveals that facial expression features contribute the most to model perfor- mance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of au- tomatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.</abstract>
      <url hash="c9c4a68a">2025.findings-acl.12</url>
      <bibkey>russell-harte-2025-visual</bibkey>
    </paper>
    <paper id="13">
      <title>The Right Time Matters: Data Arrangement Affects Zero-Shot Generalization in Instruction Tuning</title>
      <author><first>Bingxiang</first><last>He</last></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jia</first><last>Deng</last></author>
      <author><first>Ganqu</first><last>Cui</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Lifan</first><last>Yuan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Haiwen</first><last>Hong</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Huan-ang</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Longtao</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Xue</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Huimin</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>222-243</pages>
      <abstract>Understanding alignment techniques begins with comprehending zero-shot generalization brought by instruction tuning, but little of the mechanism has been understood. Existing work has largely been confined to the task level, without considering that tasks are artificially defined and, to LLMs, merely consist of tokens and representations. To bridge this gap, we investigate zero-shot generalization from the perspective of the data itself. We first demonstrate that zero-shot generalization happens very early during instruction tuning, with loss serving as a stable indicator. Next, we investigate training data arrangement through similarity and granularity perspectives, confirming that the timing of exposure to certain training examples may greatly facilitate generalization on unseen tasks. Finally, we propose a more grounded training data arrangement framework, Test-centric Multi-turn Arrangement, and show its effectiveness in promoting continual learning and further loss reduction. For the first time, we show that zero-shot generalization during instruction tuning is a form of similarity-based generalization between training and test data at the instance level.</abstract>
      <url hash="b09b0857">2025.findings-acl.13</url>
      <bibkey>he-etal-2025-right</bibkey>
    </paper>
    <paper id="14">
      <title>MFinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset</title>
      <author><first>Jie</first><last>Zhu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yalong</first><last>Wen</last></author>
      <author><first>Xiandong</first><last>Li</last><affiliation>nanjing university</affiliation></author>
      <author><first>Lifan</first><last>Guo</last></author>
      <author><first>Feng</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <pages>244-266</pages>
      <abstract>Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called MFinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, MFinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, MFinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of MFinMeeting as a benchmark for assessing LLMs’ financial meeting comprehension skills.</abstract>
      <url hash="4460f890">2025.findings-acl.14</url>
      <bibkey>zhu-etal-2025-mfinmeeting</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>ODDA</fixed-case>: An <fixed-case>OODA</fixed-case>-Driven Diverse Data Augmentation Framework for Low-Resource Relation Extraction</title>
      <author><first>Yijie</first><last>Zhong</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yunfan</first><last>Gao</last><affiliation>Tongji University</affiliation></author>
      <author><first>Xiaolian</first><last>Zhang</last></author>
      <author><first>Haofen</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>267-285</pages>
      <abstract>Data Augmentation (DA) has emerged as a promising solution to address the scarcity of high-quality annotated data in low-resource relation extraction (LRE). Leveraging large language models (LLMs), DA has significantly improved the performance of RE models with considerably fewer parameters. However, existing DA methods struggle with diversity misalignments, as they neglect the diversity required by the model and generate homogeneous augmentations that do not cover the inter-sample and inter-relation variability, leading to suboptimal performance. Inspired by the Observe-Orient-Decide-Act (OODA) framework, which provides a robust theoretical foundation for iterative decision-making under dynamic conditions, we propose an OODA-driven Diverse DA method (ODDA), guiding the data generation and selection process. DDA first <tex-math>\mathrm{observes}</tex-math> the RE model’s behavior to select effective demonstrations for LLMs. Next, it <tex-math>\mathrm{orients}</tex-math> LLMs towards generating diverse data by replacing schema constraints with attribute constraints. Then ODDA <tex-math>\mathrm{decides}</tex-math> on the final augmented dataset with overall diversity from a global search and finally <tex-math>\mathrm{acts}</tex-math> to train the RE model. Extensive experiments on three widely-used benchmarks demonstrate that ODDA consistently outperforms state-of-the-art baselines, achieving average F1 improvements of 3.1% across various LRE scenarios while maintaining enhanced model stability.</abstract>
      <url hash="90d652c7">2025.findings-acl.15</url>
      <bibkey>zhong-etal-2025-odda</bibkey>
    </paper>
    <paper id="16">
      <title>Detecting and Mitigating Challenges in Zero-Shot Video Summarization with Video <fixed-case>LLM</fixed-case>s</title>
      <author><first>Luca</first><last>Cagliero</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Lorenzo</first><last>Vaiani</last></author>
      <author><first>Eliana</first><last>Pastor</last><affiliation>Politecnico di Torino</affiliation></author>
      <author><first>Alkis</first><last>Koudounas</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Elena</first><last>Baralis</last><affiliation>Politecnico di Torino and Politecnico di Torino</affiliation></author>
      <author><first>Vittorio</first><last>Mazzia</last><affiliation>Amazon</affiliation></author>
      <author><first>Sandro</first><last>Pollastrini</last></author>
      <author><first>Thomas</first><last>Gueudre</last><affiliation>Amazon</affiliation></author>
      <author><first>Manuel</first><last>Giollo</last></author>
      <author><first>Daniele</first><last>Amberti</last><affiliation>Amazon</affiliation></author>
      <author><first>Yue</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <pages>286-301</pages>
      <abstract>Video summarization aims to generate a condensed textual version of an original video. Summaries may consist of either plain text or a shortlist of salient events, possibly including temporal or spatial references. Video Large Language Models (VLLMs) exhibit impressive zero-shot capabilities in video analysis. However, their performance varies significantly according to the LLM prompt, the characteristics of the video, and the properties of the training data and LLM architecture.In this work, we thoroughly evaluate the zero-shot summarization performance of four state-of-the-art open-source VLLMs specifically designed to address spatial and temporal reasoning. In light of the detected summarization issues, we propose different cost-effective mitigation strategies, based on Chain-of-Thought prompting, that involve the injection of knowledge extracted by external, lightweight models. To perform the VLLM evaluation, we design a new video summarization benchmark consisting of 100 videos with varying characteristics in terms of domain, duration, and spatio-temporal properties. Videos are manually annotated by three independent human experts with plain text, event-based, and spatio-temporal summaries. The experimental evaluation shows that VLLMs significantly benefit from prompting a list of recognized actions, whereas injecting automatically recognized objects and scene changes respectively improve spatially contextualized and event-based summaries in specific cases.</abstract>
      <url hash="dd5da9df">2025.findings-acl.16</url>
      <bibkey>cagliero-etal-2025-detecting</bibkey>
    </paper>
    <paper id="17">
      <title>Entity Framing and Role Portrayal in the News</title>
      <author><first>Tarek</first><last>Mahmoud</last></author>
      <author><first>Zhuohan</first><last>Xie</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Dimitar Iliyanov</first><last>Dimitrov</last></author>
      <author><first>Nikolaos</first><last>Nikolaidis</last></author>
      <author><first>Purificação</first><last>Silvano</last><affiliation>Universidade do Porto</affiliation></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Shivam</first><last>Sharma</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Elisa</first><last>Sartori</last></author>
      <author><first>Nicolas</first><last>Stefanovitch</last><affiliation>European Commission</affiliation></author>
      <author><first>Giovanni</first><last>Da San Martino</last><affiliation>University of Padua</affiliation></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>302-326</pages>
      <abstract>We introduce a novel multilingual and hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.</abstract>
      <url hash="0bdbd21f">2025.findings-acl.17</url>
      <bibkey>mahmoud-etal-2025-entity</bibkey>
    </paper>
    <paper id="18">
      <title>Derailer-Rerailer: Adaptive Verification for Efficient and Reliable Language Model Reasoning</title>
      <author><first>Guangya</first><last>Wan</last></author>
      <author><first>Yuqi</first><last>Wu</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Shengming</first><last>Zhao</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Jie</first><last>Chen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Sheng</first><last>Li</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>327-348</pages>
      <abstract>Large Language Models (LLMs) have shown impressive reasoning capabilities, yet existing prompting methods face a critical trade-off: simple approaches often struggle with complex tasks and reasoning stability, while more sophisticated methods require multiple inferences and substantial computational resources, limiting their practical deployment. To address this challenge, we propose Derailer-Rerailer, a novel framework that adaptively balances reasoning accuracy and computational efficiency. At its core, our framework employs a lightweight Derailer mechanism to assess reasoning stability and selectively triggers an advanced Rerailer verification process only when necessary, thereby optimizing computational resource usage. Extensive evaluation across both open and closed-source models on more than 20 categories of mathematical, symbolic, and commonsense reasoning tasks demonstrates our framework’s effectiveness: Derailer-Rerailer achieves significant accuracy improvements (8-11% across various reasoning tasks) while maintaining 2-3 times better efficiency than existing verification methods, with particularly strong performance in mathematical and symbolic reasoning, offering a practical solution for enhancing LLM reasoning reliability while significantly reducing computational overhead.</abstract>
      <url hash="e358115c">2025.findings-acl.18</url>
      <bibkey>wan-etal-2025-derailer</bibkey>
    </paper>
    <paper id="19">
      <title>Leveraging Large Language Models for Conversational Multi-Doc Question Answering: The First Place of <fixed-case>WSDM</fixed-case> Cup 2024</title>
      <author><first>Yiming</first><last>Li</last></author>
      <author><first>Zhao</first><last>Zhang</last></author>
      <pages>349-355</pages>
      <abstract>Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the “Conversational Multi-Doc QA” challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents, and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.</abstract>
      <url hash="17377138">2025.findings-acl.19</url>
      <bibkey>li-zhang-2025-leveraging</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>ree<fixed-case>RAG</fixed-case>: Unleashing the Power of Hierarchical Storage for Enhanced Knowledge Retrieval in Long Documents</title>
      <author><first>Wenyu</first><last>Tao</last></author>
      <author><first>Xiaofen</first><last>Xing</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Yirong</first><last>Chen</last></author>
      <author><first>Linyi</first><last>Huang</last><affiliation>South China University of Technology and The 5th Electronic Research Institute, Ministry of Industry and Information Technology, Guangzhou, China</affiliation></author>
      <author><first>Xiangmin</first><last>Xu</last><affiliation>South China University of Technology</affiliation></author>
      <pages>356-371</pages>
      <abstract>When confronting long document information retrieval for Query-Focused Summarization(QFS), Traditional Retrieval-Augmented Generation(RAG) frameworks struggle to retrieve all relevant knowledge points, and the chunking and retrieve strategies of existing frameworks may disrupt the connections between knowledge points and the integrity of the information. To address these issues, we propose <tex-math>\textbf{TreeRAG}</tex-math>, which employs <tex-math>\textbf{Tree-Chunking}</tex-math> for chunking and embedding in a tree-like structure , coupled with "<tex-math>\textbf{root-to-leaves}</tex-math>" and "<tex-math>\textbf{leaf-to-root}</tex-math>" retrieve strategy named <tex-math>\textbf{Bidirectional Traversal Retrieval}</tex-math>. This approach effectively preserves the hierarchical structure among knowledge points and significantly enhances the ability to retrieve while minimizing noise inference. Our experimental results on the <tex-math>\textbf{Finance, Law, and Medical subsets of the Dragonball dataset}</tex-math> demonstrate that <tex-math>\textbf{TreeRAG}</tex-math> achieves significant enhancements in both recall quality and precision compared to traditional and popular existing methods and achieves better performance to corresponding question-answering tasks, marking a new breakthrough in long document knowledge retrieval.</abstract>
      <url hash="df66372b">2025.findings-acl.20</url>
      <bibkey>tao-etal-2025-treerag</bibkey>
    </paper>
    <paper id="21">
      <title>Attention with Dependency Parsing Augmentation for Fine-Grained Attribution</title>
      <author><first>Qiang</first><last>Ding</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lvzhou</first><last>Luo</last></author>
      <author><first>Yixuan</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ping</first><last>Luo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>372-387</pages>
      <abstract>To assist humans in efficiently validating RAG-generated content, developing a fine-grained attribution mechanism that provides supporting evidence from retrieved documents for every answer span is essential. Existing fine-grained attribution methods rely on model-internal similarity metrics between responses and documents, such as saliency scores and hidden state similarity. However, these approaches suffer from either high computational complexity or coarse-grained representations. Additionally, a common problem shared by the previous works is their reliance on decoder-only Transformers, limiting their ability to incorporate contextual information after the target span. To address the above problems, we propose two techniques applicable to all model-internals-based methods. First, we aggregate token-wise evidence through set union operations, preserving the granularity of representations. Second, we enhance the attributor by integrating dependency parsing to enrich the semantic completeness of target spans. For practical implementation, our approach employs attention weights as the similarity metric. Experimental results demonstrate that the proposed method consistently outperforms all prior works.</abstract>
      <url hash="6f744aa0">2025.findings-acl.21</url>
      <bibkey>ding-etal-2025-attention</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>ASTRO</fixed-case>: Automatic Strategy Optimization For Non-Cooperative Dialogues</title>
      <author><first>Yikuan</first><last>Hu</last></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>388-408</pages>
      <abstract>Non-cooperative dialogues, such as negotiations and persuasion, present significant challenges for large language models (LLMs) due to the lack of inherent cooperation or shared goals. Current methods for optimizing dialogue strategies require substantial human effort for strategy optimization. To address these challenges, we propose ASTRO (Automated Strategy Optimization), a fully automated solution that leverages LLMs’ self-envolving capabilities. ASTRO dynamically generates customized strategy sets based on task goals and optimizes strategy planner using a self-play reinforcement learning paradigm. Our experimental results demonstrate ASTRO’s significant performance improvements over baseline models across various non-cooperative dialogue tasks, highlighting the potential for autonomously developing such agents without human intervention. Our code is available at https://github.com/SCUNLP/ASTRO.</abstract>
      <url hash="c5d05404">2025.findings-acl.22</url>
      <bibkey>hu-etal-2025-astro</bibkey>
    </paper>
    <paper id="23">
      <title>Defensive Prompt Patch: A Robust and Generalizable Defense of Large Language Models against Jailbreak Attacks</title>
      <author><first>Chen</first><last>Xiong</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xiangyu</first><last>Qi</last><affiliation>Princeton University</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Tsung-Yi</first><last>Ho</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <pages>409-437</pages>
      <abstract>Safety, security, and compliance are essential requirements when aligning large language models (LLMs). However, many seemingly aligned LLMs are soon shown to be susceptible to jailbreak attacks. These attacks aim to circumvent the models’ safety guardrails and security mechanisms by introducing jailbreak prompts into malicious queries. In response to these challenges, this paper introduces **Defensive Prompt Patch** (DPP), a novel prompt-based defense mechanism specifically designed to protect LLMs against such sophisticated jailbreak strategies. Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs. Our method uses strategically designed suffix prompts that effectively thwart a wide range of standard and adaptive jailbreak techniques. Empirical results conducted on Llama-2-7B-Chat and Mistral-7B-Instruct-v0.2 demonstrate the robustness and adaptability of DPP, showing significant reductions in ASR with negligible impact on utility. Our approach not only outperforms existing defense strategies in balancing safety and functionality, but also provides a scalable and robust solution to various LLM platforms.</abstract>
      <url hash="1e715332">2025.findings-acl.23</url>
      <bibkey>xiong-etal-2025-defensive</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>GUM</fixed-case>-<fixed-case>SAGE</fixed-case>: A Novel Dataset and Approach for Graded Entity Salience Prediction</title>
      <author><first>Jessica</first><last>Lin</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Amir</first><last>Zeldes</last><affiliation>Georgetown University</affiliation></author>
      <pages>438-455</pages>
      <abstract>Determining and ranking the most salient entities in a text is critical for user-facing systems, especially as users increasingly rely on models to interpret long documents they only partially read. Graded entity salience addresses this need by assigning entities scores that reflect their relative importance in a text. Existing approaches fall into two main categories: subjective judgments of salience, which allow for gradient scoring but lack consistency, and summarization-based methods, which define salience as mention-worthiness in a summary, promoting explainability but limiting outputs to binary labels (entities are either summary-worthy or not). In this paper, we introduce a novel approach for graded entity salience that combines the strengths of both approaches. Using an English dataset spanning 12 spoken and written genres, we collect 5 summaries per document and calculate each entity’s salience score based on its presence across these summaries. Our approach shows stronger correlation with scores based on human summaries and alignments, and outperforms existing techniques, including LLMs. We release our data and code at https://github.com/jl908069/gum_sum_salience to support further research on graded salient entity extraction.</abstract>
      <url hash="7e055452">2025.findings-acl.24</url>
      <bibkey>lin-zeldes-2025-gum</bibkey>
    </paper>
    <paper id="25">
      <title>Verifying the Steps of Deductive Reasoning Chains</title>
      <author><first>Zacchary</first><last>Sadeddine</last></author>
      <author><first>Fabian M.</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>456-475</pages>
      <abstract>As Large Language Models penetrate everyday life more and more, it becomes essential to measure the correctness of their output. Inthis paper, we propose a novel task: the automatic verification of individual reasoning steps in a logical deductive Chain-of-Thought. Thistask addresses two well-known problems of LLMs, hallucination and incorrect reasoning. We propose a new dataset of logical reasoningchains, in which the individual deduction steps have been manually annotated for soundness, and benchmark several methods on it. We findthat LLMs can detect unsound reasoning steps fairly well, but argue that verification has to be performed by transparent methods instead.We test symbolic methods, but find that they under-perform. We develop a neuro-symbolic baseline called VANESSA that comes closer to the performance of LLMs.</abstract>
      <url hash="3c49e06a">2025.findings-acl.25</url>
      <bibkey>sadeddine-suchanek-2025-verifying</bibkey>
    </paper>
    <paper id="26">
      <title>Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations</title>
      <author><first>Pardis Sadat</first><last>Zahraei</last></author>
      <author><first>Ali</first><last>Emami</last><affiliation>Brock University</affiliation></author>
      <pages>476-501</pages>
      <abstract>Addressing gender bias and maintaining logical coherence in machine translation remains challenging, particularly when translating between natural gender languages, like English, and genderless languages, such as Persian, Indonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset, comprising 3,950 challenging scenarios across six low- to mid-resource languages, to assess translation systems’ performance. Our analysis of diverse technologies, including GPT-4, mBART-50, NLLB-200, and Google Translate, reveals a universal struggle in translating genderless content, resulting in gender stereotyping and reasoning errors. All models preferred masculine pronouns when gender stereotypes could influence choices. Google Translate and GPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more than feminine ones in leadership and professional success contexts. Fine-tuning mBART-50 on TWC substantially resolved these biases and errors, led to strong generalization, and surpassed proprietary LLMs while remaining open-source. This work emphasizes the need for targeted approaches to gender and semantic coherence in machine translation, particularly for genderless languages, contributing to more equitable and accurate translation systems.</abstract>
      <url hash="9477bdab">2025.findings-acl.26</url>
      <bibkey>zahraei-emami-2025-translate</bibkey>
    </paper>
    <paper id="27">
      <title>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</title>
      <author><first>Benjamin C.</first><last>Warner</last></author>
      <author><first>Ziqi</first><last>Xu</last></author>
      <author><first>Simon</first><last>Haroutounian</last></author>
      <author><first>Thomas</first><last>Kannampallil</last></author>
      <author><first>Chenyang</first><last>Lu</last></author>
      <pages>502-520</pages>
      <abstract>Surveys are widely used to collect patient data in healthcare, and there is significant clinical interest in predicting patient outcomes using survey data. However, surveys often include numerous features that lead to high-dimensional inputs for machine learning models. This paper exploits a unique source of information in surveys for feature selection. We observe that feature names (i.e., survey questions) are often semantically indicative of what features are most useful. Using language models, we leverage semantic textual similarity (STS) scores between features and targets to select features. The performance of STS scores in directly ranking features as well as in the minimal-redundancy-maximal-relevance (mRMR) algorithm is evaluated using survey data collected as part of a clinical study on persistent post-surgical pain (PPSP) as well as an accessible dataset collected through the NIH All of Us program. Our findings show that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.</abstract>
      <url hash="aaea00e2">2025.findings-acl.27</url>
      <bibkey>warner-etal-2025-utilizing</bibkey>
    </paper>
    <paper id="28">
      <title>Distance between Relevant Information Pieces Causes Bias in Long-Context <fixed-case>LLM</fixed-case>s</title>
      <author><first>Runchu</first><last>Tian</last></author>
      <author><first>Yanghao</first><last>Li</last></author>
      <author><first>Yuepeng</first><last>Fu</last></author>
      <author><first>Siyang</first><last>Deng</last></author>
      <author><first>Qinyu</first><last>Luo</last><affiliation>Johns Hopkins University and Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Zhong</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yesai</first><last>Wu</last></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Huadong</first><last>Wang</last><affiliation>ModelBest Inc.</affiliation></author>
      <author><first>Xiaojiang</first><last>Liu</last><affiliation>Apple</affiliation></author>
      <pages>521-533</pages>
      <abstract>Positional bias in large language models hinders their ability to effectively process long inputs. A prominent example is the “lost in the middle” phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LongPiBench, a benchmark designed to assess positional bias involving multiple pieces of relevant information. It includes various tasks and input lengths. Thorough experiments are conducted with three commercial and six open-source models. These experiments reveal that while most current models are more robust against the “lost in the middle” issue, there also exist noticeable biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases for long-context LLMs.</abstract>
      <url hash="1b523460">2025.findings-acl.28</url>
      <bibkey>tian-etal-2025-distance</bibkey>
    </paper>
    <paper id="29">
      <title>Variable Layerwise Quantization: A Simple and Effective Approach to Quantize <fixed-case>LLM</fixed-case>s</title>
      <author><first>Razvan-Gabriel</first><last>Dumitru</last></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Rishabh</first><last>Maheshwary</last><affiliation>ServiceNow</affiliation></author>
      <author><first>Paul Ioan</first><last>Clotan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Sathwik Tejaswi</first><last>Madhusudhan</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>534-550</pages>
      <abstract>We present a simple meta quantization approach that quantizes different layers of a large language model (LLM) at different bit levels, and is independent of the underlying quantization technique. Specifically, we quantize the most important layers to higher bit precision and less important layers to lower bits. We propose two effective strategies to measure the importance of layers within LLMs: the first measures the importance of a layer based on how different its output embeddings are from the input embeddings (higher is better); the second estimates the importance of a layer using the number of layer weights that are much larger than average (smaller is better). We show that quantizing different layers at varying bits as per our importance scores results in minimal performance drop with a far more compressed model. Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25–50% of layers are moved in lower quantization using our proposed ordering but only until 5–10% if moved using no specific ordering; (b) Adding layer importance to inherently dynamic quantization techniques can further improve their performance, showing that our approach is complementary to other dynamic quantization methods; (c) Quantizing LLMs to lower bits performs substantially better than pruning unless extreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower bits works better in the case of larger LLMs with more layers compared to smaller LLMs with fewer layers.</abstract>
      <url hash="911ca7f1">2025.findings-acl.29</url>
      <bibkey>dumitru-etal-2025-variable</bibkey>
    </paper>
    <paper id="30">
      <title>Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? A Petroglyph Revisited</title>
      <author><first>Kazuki</first><last>Irie</last><affiliation>Harvard University</affiliation></author>
      <pages>551-559</pages>
      <abstract>Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is ‘no’ provided they have more than one layer—they can distinguish sequences with permuted tokens without the need for explicit PEs. This follows from the fact that a cascade of (permutation invariant) set processors can collectively exhibit sequence-sensitive behavior in the autoregressive setting. This property has been known since early efforts (contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated, leading to recent rediscoveries. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2/3, but perhaps also due to the lack of a clear explanation in prior work, despite being commonly understood by practitioners in the past. Here we review the long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their inputs), as well as the origin of this result, and hope to re-establish it as a common knowledge.</abstract>
      <url hash="bcce8379">2025.findings-acl.30</url>
      <bibkey>irie-2025-positional</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>CRPO</fixed-case>: Confidence-Reward Driven Preference Optimization for Machine Translation</title>
      <author><first>Guofeng</first><last>Cui</last></author>
      <author><first>Pichao</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zemian</first><last>Ke</last></author>
      <author><first>Zhu</first><last>Liu</last><affiliation>Amazon Prime Video</affiliation></author>
      <author><first>Vimal</first><last>Bhat</last><affiliation>Amazon</affiliation></author>
      <pages>560-574</pages>
      <abstract>Large language models (LLMs) have shown great potential in natural language processing tasks, but their application to machine translation (MT) remains challenging due to pretraining on English-centric data and the complexity of reinforcement learning from human feedback (RLHF). Direct Preference Optimization (DPO) has emerged as a simpler and more efficient alternative, but its performance depends heavily on the quality of preference data. To address this, we propose Confidence-Reward driven Preference Optimization (CRPO), a novel method that combines reward scores with model confidence to improve data selection for fine-tuning. CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. While primarily designed for LLMs, CRPO also generalizes to encoder-decoder models like NLLB, demonstrating its versatility. Empirical results show that CRPO outperforms existing methods such as RS-DPO, RSO and MBR score in both translation accuracy and data efficiency.</abstract>
      <url hash="0196ff6d">2025.findings-acl.31</url>
      <bibkey>cui-etal-2025-crpo</bibkey>
    </paper>
    <paper id="32">
      <title>Talking Point based Ideological Discourse Analysis in News Events</title>
      <author><first>Nishanth Sridhar</first><last>Nakshatri</last><affiliation>Purdue University</affiliation></author>
      <author><first>Nikhil</first><last>Mehta</last></author>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Sihao</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Daniel</first><last>Hopkins</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <pages>575-594</pages>
      <abstract>Analyzing ideological discourse even in the age of LLMs remains a challenge, as these models often struggle to capture the key elements that shape real-world narratives. Specifically, LLMs fail to focus on characteristic elements driving dominant discourses and lack the ability to integrate contextual information required for understanding abstract ideological views. To address these limitations, we propose a framework motivated by the theory of ideological discourse analysis to analyze news articles related to real-world events. Our framework represents the news articles using a relational structure−talking points, which captures the interaction between entities, their roles, and media frames along with a topic of discussion. It then constructs a vocabulary of repeating themes−prominent talking points, that are used to generate ideology-specific viewpoints (or partisan perspectives). We evaluate our framework’s ability to generate these perspectives through automated tasks−ideology and partisan classification tasks, supplemented by human validation. Additionally, we demonstrate straightforward applicability of our framework in creating event snapshots, a visual way of interpreting event discourse. We release resulting dataset and model to the community to support further research.</abstract>
      <url hash="882a1a5a">2025.findings-acl.32</url>
      <bibkey>nakshatri-etal-2025-talking</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>F</fixed-case>lash<fixed-case>B</fixed-case>ack: Efficient Retrieval-Augmented Language Modeling for Fast Inference</title>
      <author><first>Runheng</first><last>Liu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xingchen</first><last>Xiao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zewen</first><last>Chi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhijing</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>595-608</pages>
      <abstract>Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven methodology for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work by retrieving a set of tokens iteratively with retrieved content prepending to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. We propose FlashBack, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after fine-tuning by Low-Rank Adaption. FlashBack appends retrieved documents at the end of the context for efficiently utilizing the KV cache. We also introduce the Marking Token as two special prompt tokens for marking the appending context during fine-tuning. Our experiments show that FlashBack can improve language modeling performance in perplexity metric. We proved the Marking Token is a usable add-on when fine-tuning models on specific context patterns. By bypassing unnecessary re-computation, FlashBack achieves fast inference speed speed with long context input. The inference speed is up to <tex-math>4\times</tex-math> faster than the prepending counterpart on a 7B LLM (Llama 2) in the runtime test.</abstract>
      <url hash="0a4cf29f">2025.findings-acl.33</url>
      <bibkey>liu-etal-2025-flashback</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>CMQCIC</fixed-case>-Bench: A <fixed-case>C</fixed-case>hinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation</title>
      <author><first>Guangya</first><last>Yu</last></author>
      <author><first>Yanhao</first><last>Li</last></author>
      <author><first>Zongying</first><last>Jiang</last></author>
      <author><first>Yuxiong</first><last>Jin</last></author>
      <author><first>Li</first><last>Dai</last></author>
      <author><first>Yupian</first><last>Lin</last></author>
      <author><first>Ruihui</first><last>Hou</last></author>
      <author><first>Weiyan</first><last>Zhang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Qi</first><last>Ye</last></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>609-626</pages>
      <abstract>Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repository https://github.com/YuY-2001/C-MQCIC.</abstract>
      <url hash="a8a80e87">2025.findings-acl.34</url>
      <bibkey>yu-etal-2025-cmqcic</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>C</fixed-case>on<fixed-case>KE</fixed-case>: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning</title>
      <author><first>Liyu</first><last>Zhang</last></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>627-635</pages>
      <abstract>Knowledge Editing (KE) aims to adjust a Large Language Model’s (LLM) internal representations and parameters to correct inaccuracies and improve output consistency without incurring the computational expense of re-training the entire model. However, editing commonsense knowledge still faces difficulties, including limited knowledge coverage in existing resources, the infeasibility of annotating labels for an overabundance of commonsense knowledge, and the strict knowledge formats of current editing methods. In this paper, we address these challenges by presenting ConceptEdit, a framework that integrates conceptualization and instantiation into the KE pipeline for LLMs to enhance their commonsense reasoning capabilities. ConceptEdit dynamically diagnoses implausible commonsense knowledge within an LLM using another verifier LLM and augments the source knowledge to be edited with conceptualization for stronger generalizability. Experimental results demonstrate that LLMs enhanced with ConceptEdit successfully generate commonsense knowledge with improved plausibility compared to other baselines and achieve stronger performance across multiple question answering benchmarks. Our data, code, and models are publicly available at https://github.com/HKUST-KnowComp/ConKE.</abstract>
      <url hash="38e0110b">2025.findings-acl.35</url>
      <bibkey>zhang-etal-2025-conke</bibkey>
    </paper>
    <paper id="36">
      <title>Exploring Multi-Modal Data with Tool-Augmented <fixed-case>LLM</fixed-case> Agents for Precise Causal Discovery</title>
      <author><first>ChengAo</first><last>Shen</last></author>
      <author><first>Zhengzhang</first><last>Chen</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Dongsheng</first><last>Luo</last><affiliation>Florida International University</affiliation></author>
      <author><first>Dongkuan</first><last>Xu</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <author><first>Jingchao</first><last>Ni</last><affiliation>University of Houston</affiliation></author>
      <pages>636-660</pages>
      <abstract>Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MatMCD, a multi-agent system powered by tool-augmented LLMs. MatMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.</abstract>
      <url hash="812eb3e4">2025.findings-acl.36</url>
      <bibkey>shen-etal-2025-exploring</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>PARSQL</fixed-case>: Enhancing Text-to-<fixed-case>SQL</fixed-case> through <fixed-case>SQL</fixed-case> Parsing and Reasoning</title>
      <author><first>Yaxun</first><last>Dai</last><affiliation>Soochow University</affiliation></author>
      <author><first>Haiqin</first><last>Yang</last><affiliation>International Digital Economy Academy (IDEA)</affiliation></author>
      <author><first>Mou</first><last>Hao</last><affiliation>datastory</affiliation></author>
      <author><first>Pingfu</first><last>Chao</last><affiliation>Soochow University</affiliation></author>
      <pages>661-681</pages>
      <abstract>Large language models (LLMs) have made significant strides in text-to-SQL tasks; however, small language models (SLMs) are crucial due to their low resource consumption and efficient inference for real-world deployment. Due to resource limitations, SLMs struggle to accurately interpret natural language questions and may overlook critical constraints, leading to challenges such as generating SQL with incorrect logic or incomplete conditions. To address these issues, we propose PARSQL, a novel framework that leverages SQL parsing and reasoning. Specifically, we design PARSer, an SQL parser that extracts constraints from SQL to generate sub-SQLs for data augmentation and producing step-by-step SQL explanations (reason) via both rule-based and LLM-based methods. We define a novel text-to-reason task and incorporate it into multi-task learning, thereby enhancing text-to-SQL performance. Additionally, we employ an efficient SQL selection strategy that conducts direct similarity computation between the generated SQLs and their corresponding reasons to derive the final SQL for post-correction. Extensive experiments show that our PARSQL outperforms models with the same model size on the BIRD and Spider benchmarks. Notably, PARSQL-3B achieves 56.98% execution accuracy on BIRD, rivaling 7B models with significantly fewer parameters, setting a new state-of-the-art performance. Code can be found [here](https://github.com/yaxundai/parsql).</abstract>
      <url hash="7b8fed26">2025.findings-acl.37</url>
      <bibkey>dai-etal-2025-parsql</bibkey>
    </paper>
    <paper id="38">
      <title>Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in <fixed-case>LLM</fixed-case>s Across Logical Transformations and Question Answering Tasks</title>
      <author><first>Yuntai</first><last>Bao</last></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tianyu</first><last>Du</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xinkui</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhengwen</first><last>Feng</last></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Zhejiang Normal University</affiliation></author>
      <author><first>Jianwei</first><last>Yin</last><affiliation>Zhejiang University</affiliation></author>
      <pages>682-700</pages>
      <abstract>Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the “truth direction”, which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts.Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation.Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources.Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs.These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs.</abstract>
      <url hash="e8ce00b3">2025.findings-acl.38</url>
      <bibkey>bao-etal-2025-probing</bibkey>
    </paper>
    <paper id="39">
      <title>Comparing Bad Apples to Good Oranges Aligning Large Language Models via Joint Preference Optimization</title>
      <author><first>Hritik</first><last>Bansal</last></author>
      <author><first>Ashima</first><last>Suvarna</last></author>
      <author><first>Gantavya</first><last>Bhatt</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Aditya</first><last>Grover</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>701-723</pages>
      <abstract>A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This method, however, relies solely on pairwise comparisons, where the generations are evaluated within an identical context. While effective to such conditional preferences often fail to encompass the nuanced and multidimensional nature of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis based on eliciting preferences jointly over the instruction-response pairs. Unlike prior preference optimizations, which are designed for conditional ranking protocols (e.g., DPO), we propose Joint Preference Optimization (JPO), a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, LLMs trained with joint instruction-response preference data using JPO outperform LLM trained with DPO by 5.2% and 3.3% win-rate for summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available athttps://github.com/Hritikbansal/jpo.</abstract>
      <url hash="904eebea">2025.findings-acl.39</url>
      <bibkey>bansal-etal-2025-comparing</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>T</fixed-case>est<fixed-case>A</fixed-case>gent: An Adaptive and Intelligent Expert for Human Assessment</title>
      <author><first>Junhao</first><last>Yu</last></author>
      <author><first>Yan</first><last>Zhuang</last></author>
      <author><first>Yuxuan</first><last>Sun</last></author>
      <author><first>Weibo</first><last>Gao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Mingyue</first><last>Cheng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>724-747</pages>
      <abstract>Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers’ responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.</abstract>
      <url hash="a5759bc6">2025.findings-acl.40</url>
      <bibkey>yu-etal-2025-testagent</bibkey>
    </paper>
    <paper id="41">
      <title><fixed-case>SPICA</fixed-case>: Retrieving Scenarios for Pluralistic In-Context Alignment</title>
      <author><first>Quan Ze</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Amy X</first><last>Zhang</last><affiliation>Department of Computer Science</affiliation></author>
      <pages>748-765</pages>
      <abstract>When different groups’ values differ, one approach to model alignment is to steer models at inference time towards each group’s preferences. However, techniques like in-context learning only consider similarity when drawing few-shot examples and not cross-group differences in values. We propose SPICA, a framework that accounts for group-level differences during in-context example retrieval. SPICA introduces three designs: scenario banks, group-informed retrieval metrics, and in-context alignment prompts. From an evaluation of SPICA on an alignment task collecting inputs from four demographic groups (<tex-math>n = 544</tex-math>), our metrics retrieve in-context examples that more closely match observed preferences, with the best prompt configuration using multiple contrastive responses to demonstrate examples. In an end-to-end evaluation (<tex-math>n = 120</tex-math>), we observe that SPICA is higher rated than similarity-based retrieval, with groups seeing up to a +0.16 point improvement on a 5 point scale. Additionally, gains from SPICA were more uniform, with all groups benefiting from alignment rather than only some. Finally, we find that while a group-agnostic approach can align to aggregated values, it is not most suited for divergent groups.</abstract>
      <url hash="e6162e78">2025.findings-acl.41</url>
      <bibkey>chen-etal-2025-spica</bibkey>
    </paper>
    <paper id="42">
      <title>First-Step Advantage: Importance of Starting Right in Multi-Step Math Reasoning</title>
      <author><first>Kushal</first><last>Jain</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Moritz</first><last>Miller</last></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <pages>766-778</pages>
      <abstract>Language models can solve complex reasoning tasks better by learning to generate rationales for their predictions. Often these models know how to solve a task but their auto-regressive decoding nature leads to incorrect results if started incorrectly. We observe that smaller models in particular, when corrected, can solve a task that they would otherwise struggle with. We demonstrate this phenomenon by using a larger model to guide smaller models, which leads to significantly improved performance (up to +24 points on the GSM8K dataset by 7B models). To assist smaller models in initiating the starting step, we propose QuestCoT, where a smaller model first asks how to start before proceeding with a chain of reasoning. On various multistep mathematical reasoning datasets over multiple smaller models, we show that getting the start right can lead to significant performance gains across all models (gains of up to +6 points on GSM8K, +9 on SVAMP, +5 on ASDiv, and +7 on MultiArith).</abstract>
      <url hash="1a314f55">2025.findings-acl.42</url>
      <bibkey>jain-etal-2025-first</bibkey>
    </paper>
    <paper id="43">
      <title>Evaluating Instructively Generated Statement by Large Language Models for Directional Event Causality Identification</title>
      <author><first>Wei</first><last>Xiang</last><affiliation>Central China Normal University</affiliation></author>
      <author><first>Chuanhong</first><last>Zhan</last></author>
      <author><first>Qing</first><last>Zhang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Bang</first><last>Wang</last></author>
      <pages>779-785</pages>
      <abstract>This paper aims to identify directional causal relations between events, including the existence and direction of causality. Previous studies mainly adopt prompt learning paradigm to predict a causal answer word based on a Pre-trained Language Model (PLM) for causality existence identification. However, the indecision in selecting answer words from some synonyms and the confusion of indicating opposite causal directions with the same answer word raise more challenges in directional causality identification. Inspired by the strong capabilities of pre-trained Generative Language Models (GLMs) in generating responses or statements, we propose to instruct a GLM to generate causality statements and identify directional event causality by evaluating the generated statements. Specifically, we propose an Instructive Generation and Statement Evaluation method to identify both the existence and direction of causality. We first fine-tune a GLM to instructively generate causality statements based on event description inputs. Then, we evaluate the rationality of the generated statements to determine the existence and direction of event causalities. Experiments on the ESC and MAVEN datasets show that our method significantly outperforms state-of-the-art algorithms, even with fewer training data.</abstract>
      <url hash="d497ff88">2025.findings-acl.43</url>
      <bibkey>xiang-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>C</fixed-case>oin<fixed-case>M</fixed-case>ath: Harnessing the Power of Coding Instruction for Math <fixed-case>LLM</fixed-case></title>
      <author><first>Chengwei</first><last>Wei</last><affiliation>, A*STAR</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Jung-jae</first><last>Kim</last><affiliation>A*STAR</affiliation></author>
      <author><first>Guimei</first><last>Liu</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <pages>786-797</pages>
      <abstract>Large Language Models (LLMs) have shown strong performance in solving mathematical problems, with code-based solutions proving particularly effective. However, the best practice to leverage coding instruction data to enhance mathematical reasoning remains underexplored. This study investigates three key questions: (1) How do different coding styles of mathematical code-based rationales impact LLMs’ learning performance? (2) Can general-domain coding instructions improve performance? (3) How does integrating textual rationales with code-based ones during training enhance mathematical reasoning abilities? Our findings reveal that code-based rationales with concise comments, descriptive naming, and hardcoded solutions are beneficial, while improvements from general-domain coding instructions and textual rationales are relatively minor. Based on these insights, we propose CoinMath, a learning strategy designed to enhance mathematical reasoning by diversifying the coding styles of code-based rationales. CoinMath generates a variety of code-based rationales incorporating concise comments, descriptive naming conventions, and hardcoded solutions. Experimental results demonstrate that CoinMath significantly outperforms its baseline model, MAmmoTH, one of the SOTA math LLMs.</abstract>
      <url hash="d760543d">2025.findings-acl.44</url>
      <bibkey>wei-etal-2025-coinmath</bibkey>
    </paper>
    <paper id="45">
      <title>Profiling News Media for Factuality and Bias Using <fixed-case>LLM</fixed-case>s and the Fact-Checking Methodology of Human Experts</title>
      <author><first>Zain Muhammad</first><last>Mujahid</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Maha Tufail</first><last>Agro</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>798-819</pages>
      <abstract>In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code.</abstract>
      <url hash="76fa1de3">2025.findings-acl.45</url>
      <bibkey>mujahid-etal-2025-profiling</bibkey>
    </paper>
    <paper id="46">
      <title>Structured Discourse Representation for Factual Consistency Verification</title>
      <author id="kun-zhang"><first>Kun</first><last>Zhang</last></author>
      <author><first>Oana</first><last>Balalau</last><affiliation>INRIA</affiliation></author>
      <author><first>Ioana</first><last>Manolescu</last><affiliation>École Polytechnique and Inria</affiliation></author>
      <pages>820-838</pages>
      <abstract>Analysing the differences in how events are represented across texts, or verifying whether the language model generations hallucinate, requires the ability to systematically compare their content. To support such comparison, structured representation that captures fine-grained information plays a vital role.In particular, identifying distinct atomic facts and the discourse relations connecting them enables deeper semantic comparison. Our proposed approach combines structured discourse information extraction with a classifier, <b>FDSpotter</b>, for factual consistency verification. We show that adversarial discourse relations pose challenges for language models, but fine-tuning on our annotated data, <b>DiscInfer</b>, achieves competitive performance. Our proposed approach advances factual consistency verification by grounding in linguistic structure and decomposing it into interpretable components. We demonstrate the effectiveness of our method on the evaluation of two tasks: data-to-text generation and text summarisation. Our code and dataset will be publicly available on GitHub.</abstract>
      <url hash="1db45fff">2025.findings-acl.46</url>
      <bibkey>zhang-etal-2025-structured</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>SHARP</fixed-case>: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chuyi</first><last>Kong</last></author>
      <author><first>Ziyang</first><last>Luo</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Zhiyuan</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Yuxi</first><last>Sun</last></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>839-866</pages>
      <abstract>The advanced role-playing capabilities of Large Language Models (LLMs) have enabled rich interactive scenarios, yet existing research in social interactions neglects hallucination while struggling with poor generalizability and implicit character fidelity judgments. To bridge this gap, motivated by human behaviour, we introduce a generalizable and explicit paradigm for uncovering interactive patterns of LLMs across diverse worldviews. Specifically, we first define interactive hallucination through stance transfer, then construct SHARP, a benchmark built by extracting relations from commonsense knowledge graphs and utilizing LLMs’ inherent hallucination properties to simulate multi-role interactions. Extensive experiments confirm our paradigm’s effectiveness and stability, examine the factors that influence these metrics, and challenge conventional hallucination mitigation solutions. More broadly, our work reveals a fundamental limitation in popular post-training methods for role-playing LLMs: the tendency to obscure knowledge beneath style, resulting in monotonous yet human-like behaviors—interactive hallucination.</abstract>
      <url hash="7960da10">2025.findings-acl.47</url>
      <bibkey>kong-etal-2025-sharp</bibkey>
    </paper>
    <paper id="48">
      <title>Understanding the Gap: an Analysis of Research Collaborations in <fixed-case>NLP</fixed-case> and Language Documentation</title>
      <author><first>Luke</first><last>Gessler</last><affiliation>Indiana University</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>867-877</pages>
      <abstract>Despite over 20 years of NLP work explicitly intended for application in language documentation (LD), practical use of this work remains vanishingly scarce. This issue has been noted and discussed over the past 10 years, but without the benefit of data to inform the discourse.To address this lack in the literature, we present a survey- and interview-based analysis of the lack of adoption of NLP in LD, focusing on the matter of collaborations between documentary linguists and NLP researchers. Our data show support for ideas from previous work but also reveal the importance of little-discussed factors such as misaligned professional incentives, technical knowledge burdens, and LD software.</abstract>
      <url hash="b9ac3690">2025.findings-acl.48</url>
      <bibkey>gessler-etal-2025-understanding</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>B</fixed-case>ench: Evaluating <fixed-case>AI</fixed-case> Models on Understanding Personal Information through Accessing (Synthetic) Private User Data</title>
      <author><first>Juntao</first><last>Tan</last><affiliation>SalesForce.com and Rutgers University</affiliation></author>
      <author><first>Liangwei</first><last>Yang</last></author>
      <author><first>Zuxin</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Zhiwei</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Rithesh</first><last>R N</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Tulika Manoj</first><last>Awalgaonkar</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Weiran</first><last>Yao</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shirley</first><last>Kokane</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Huan</first><last>Wang</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <pages>878-893</pages>
      <abstract>Personalization is essential for AI assistants, especially in private AI settings where models are expected to interpret users’ personal data (e.g., conversations, app usage) to understand their background, preferences, and social context. However, due to privacy concerns, existing academic research lacks direct access to such data, making benchmarking difficult. To fill this gap, we propose a synthetic data pipeline that generates realistic user profiles and private documents, enabling the creation of PersonaBench—a benchmark for evaluating models’ ability to understand personal information. Using this benchmark, we assess Retrieval-Augmented Generation (RAG) pipelines on personalized questions and find that current models struggle to accurately extract and answer questions even when provided with the full set of user documents, highlighting the need for improved personalization methods.</abstract>
      <url hash="bc9de106">2025.findings-acl.49</url>
      <bibkey>tan-etal-2025-personabench</bibkey>
    </paper>
    <paper id="50">
      <title>Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning</title>
      <author><first>Simret A</first><last>Gebreegziabher</last></author>
      <author><first>Kuangshi</first><last>Ai</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Elena</first><last>Glassman</last><affiliation>Harvard University</affiliation></author>
      <author><first>Toby Jia-Jun</first><last>Li</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>894-906</pages>
      <abstract>Active Learning (AL) allows models to learn interactively from user feedback. However, only annotating existing samples may hardly benefit the model’s generalization. Moreover, AL commonly faces a cold start problem due to insufficient annotated data for effective sample selection. To address this, we introduce a counterfactual data augmentation approach inspired by Variation Theory, a theory of human concept learning that emphasizes the essential features of a concept by focusing on what stays the same and what changes. We use a neuro-symbolic pipeline to pinpoint key conceptual dimensions and use a large language model (LLM) to generate targeted variations along those dimensions. Through a text classification experiment, we show that our approach achieves significantly higher performance when there are fewer annotated data, showing its capability to address the cold start problem in AL. We also find that as the annotated training data gets larger, the impact of the generated data starts to diminish. This work demonstrates the value of incorporating human learning theories into the design and optimization of AL.</abstract>
      <url hash="c430cce3">2025.findings-acl.50</url>
      <bibkey>gebreegziabher-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>ORBIT</fixed-case>: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study</title>
      <author><first>Eric</first><last>Modesitt</last></author>
      <author><first>Ke</first><last>Yang</last></author>
      <author><first>Spencer</first><last>Hulsey</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>ChengXiang</first><last>Zhai</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Volodymyr</first><last>Kindratenko</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>907-926</pages>
      <abstract>Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning LLaMA-3-8B on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69% to 76% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed LLaMA-3-8B-base, with GPT-4o evaluations preferring it in 73% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT’s generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model.</abstract>
      <url hash="ef8ab83a">2025.findings-acl.51</url>
      <bibkey>modesitt-etal-2025-orbit</bibkey>
    </paper>
    <paper id="52">
      <title>Serial Position Effects of Large Language Models</title>
      <author><first>Xiaobo</first><last>Guo</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>927-953</pages>
      <abstract>We would like to express our gratitude to the Reviewers and the Area Chair for their insightful comments and for recognizing the robustness of our proposed framework for analyzing the serial position effects (SPE) in LLMs. We appreciate the acknowledgment of our work in demonstrating the widespread existence of this effect across various LLMs and the experiments we conducted to mitigate SPE.We acknowledge the concerns raised regarding the significance of the mitigation methods, including training-side solutions, CoT, and prompt engineering. The varying degrees of effectiveness observed in these methods highlight both the complexity and importance of addressing this cognitive bias. We believe these effects are inherently rooted in LLMs, and a comprehensive solution that fully addresses SPE may be beyond the scope of this work. However, we have proposed practical strategies, such as using binary choices instead of multiple choices where feasible, limiting prompt length, and placing crucial information at the beginning of prompts. These suggestions are intended to help users, particularly those who may not be experts in the domain of LLMs, to better utilize these models.We agree with the suggestion that a deeper analysis of the relationship between task characteristics and SPE could enhance the manuscript. As it stands, our findings indicate that higher model accuracy tends to correlate with a reduction in SPE, which aligns with expectations—if a model achieves 100% accuracy, it is unlikely to be influenced by SPE. Beyond this, we did not observe any clear relationships, which suggests that SPE may be influenced by a combination of factors, including the specific task, the model used, and the nature of the prompts. We will clarify this point in the final version of the manuscript.</abstract>
      <url hash="e749e6ef">2025.findings-acl.52</url>
      <bibkey>guo-vosoughi-2025-serial</bibkey>
    </paper>
    <paper id="53">
      <title>sc<fixed-case>RAG</fixed-case>: Hybrid Retrieval-Augmented Generation for <fixed-case>LLM</fixed-case>-based Cross-Tissue Single-Cell Annotation</title>
      <author><first>Zhiyin</first><last>Yu</last></author>
      <author><first>Chao</first><last>Zheng</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Chong</first><last>Chen</last><affiliation>Terminus Group</affiliation></author>
      <author><first>Xian-Sheng</first><last>Hua</last><affiliation>Tongji University</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>954-970</pages>
      <abstract>In recent years, large language models (LLMs) such as GPT-4 have demonstrated impressive potential in a wide range of fields, including biology, genomics and healthcare. Numerous studies have attempted to apply pre-trained LLMs to single-cell data analysis within one tissue. However, when it comes to cross-tissue cell annotation, LLMs often suffer from unsatisfactory performance due to the lack of specialized biological knowledge regarding genes and tissues. In this paper, we introduce scRAG, a novel framework that incorporates advanced LLM-based RAG techniques into cross-tissue single-cell annotation. scRAG utilizes LLMs to retrieve structured triples from knowledge graphs and unstructured similar cell information from the reference cell database, and it generates candidate cell types. The framework further optimizes predictions by retrieving marker genes from both candidate cells and similar cells to refine its results. Extensive experiments on a cross-tissue dataset demonstrate that our scRAG framework outperforms various baselines, including generalist models, domain-specific methods, and trained classifiers. The source code is available at https://github.com/YuZhiyin/scRAG.</abstract>
      <url hash="60e27f10">2025.findings-acl.53</url>
      <bibkey>yu-etal-2025-scrag</bibkey>
    </paper>
    <paper id="54">
      <title>Can Large Language Models Address Open-Target Stance Detection?</title>
      <author><first>Abu Ubaida</first><last>Akash</last></author>
      <author><first>Ahmed</first><last>Fahmy</last><affiliation>Université de Sherbrooke</affiliation></author>
      <author><first>Amine</first><last>Trabelsi</last><affiliation>Université de Sherbrooke</affiliation></author>
      <pages>971-985</pages>
      <abstract>Stance detection (SD) identifies a text’s position towards a target, typically labeled as favor, against, or none. We introduce Open-Target Stance Detection (OTSD), the most realistic task where targets are neither seen during training nor provided as input. We evaluate Large Language Models (LLMs) from GPT, Gemini, Llama, and Mistral families, comparing their performance to the only existing work, Target-Stance Extraction (TSE), which benefits from predefined targets. Unlike TSE, OTSD removes the dependency of a predefined list, making target generation and evaluation more challenging. We also provide a metric for evaluating target quality that correlates well with human judgment. Our experiments reveal that LLMs outperform TSE in target generation, both when the real target is explicitly and not explicitly mentioned in the text. Similarly, LLMs overall surpass TSE in stance detection for both explicit and non-explicit cases. However, LLMs struggle in both target generation and stance detection when the target is not explicit.</abstract>
      <url hash="86ebec69">2025.findings-acl.54</url>
      <bibkey>akash-etal-2025-large</bibkey>
    </paper>
    <paper id="55">
      <title>Improve Language Model and Brain Alignment via Associative Memory</title>
      <author><first>Congchi</first><last>Yin</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Yongpeng</first><last>Zhang</last></author>
      <author><first>Xuyun</first><last>Wen</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>986-999</pages>
      <abstract>Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computational language models. We find the alignment between language model and brain is improved in brain regions closely related to associative memory processing. We also demonstrate large language models after specific supervised fine-tuning better align with brain response, by building the Association dataset containing 1000 samples of stories, with instructions encouraging associative memory as input and associated content as output.</abstract>
      <url hash="04f65c3f">2025.findings-acl.55</url>
      <bibkey>yin-etal-2025-improve</bibkey>
    </paper>
    <paper id="56">
      <title>Towards Reliable Large Audio Language Model</title>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Xiquan</first><last>Li</last></author>
      <author><first>Yakun</first><last>Song</last></author>
      <author><first>Wenxi</first><last>Chen</last></author>
      <author><first>Chenpeng</first><last>Du</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jian</first><last>Wu</last></author>
      <author><first>Yuanzhe</first><last>Chen</last></author>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Yuping</first><last>Wang</last></author>
      <author><first>Yuxuan</first><last>Wang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Xie</first><last>Chen</last></author>
      <pages>1000-1014</pages>
      <abstract>Recent advancements in large audio language models (LALMs) have demonstrated impressive results and promising prospects in universal understanding and reasoning across speech, music, and general sound. However, these models still lack the ability to recognize their knowledge boundaries and refuse to answer questions they don’t know proactively. While there have been successful attempts to enhance the reliability of LLMs, reliable LALMs remain largely unexplored. In this paper, we systematically investigate various approaches towards reliable LALMs, including training-free methods such as multi-modal chain-of-thought (MCoT), and training-based methods such as supervised fine-tuning (SFT). Besides, we identify the limitations of previous evaluation metrics and propose a new metric, the Reliability Gain Index (RGI), to assess the effectiveness of different reliable methods. Our findings suggest that both training-free and training-based methods enhance the reliability of LALMs to different extents. Moreover, we find that awareness of reliability is a “meta ability”, which can be transferred across different audio modalities, although significant structural and content differences exist among sound, music, and speech.</abstract>
      <url hash="ff7af2ba">2025.findings-acl.56</url>
      <bibkey>ma-etal-2025-towards</bibkey>
    </paper>
    <paper id="57">
      <title>Large Vocabulary Size Improves Large Language Models</title>
      <author><first>Sho</first><last>Takase</last><affiliation>LINE Corporation</affiliation></author>
      <author><first>Ryokan</first><last>Ri</last><affiliation>SB Intuitions</affiliation></author>
      <author><first>Shun</first><last>Kiyono</last><affiliation>SB Intuitions</affiliation></author>
      <author><first>Takuya</first><last>Kato</last><affiliation>SBIntuitions</affiliation></author>
      <pages>1015-1026</pages>
      <abstract>This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.</abstract>
      <url hash="82689cef">2025.findings-acl.57</url>
      <bibkey>takase-etal-2025-large</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>MUSE</fixed-case>: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles</title>
      <author><first>Zihan</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaocui</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>YongKang</first><last>Liu</last><affiliation>Northeast University</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <pages>1027-1053</pages>
      <abstract>Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Muse’s learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at https://anonymous.4open.science/r/Muse-0086.</abstract>
      <url hash="61dba2bd">2025.findings-acl.58</url>
      <bibkey>wang-etal-2025-muse</bibkey>
    </paper>
    <paper id="59">
      <title>Machine Translation Models are Zero-Shot Detectors of Translation Direction</title>
      <author><first>Michelle</first><last>Wastl</last></author>
      <author><first>Jannis</first><last>Vamvas</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <pages>1054-1074</pages>
      <abstract>Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications, such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that <tex-math>p(translation|original)&gt;p(original|translation)</tex-math>, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82–96% for NMT-produced translations, and 60–81% for human translations, depending on the model used.</abstract>
      <url hash="34a25c87">2025.findings-acl.59</url>
      <bibkey>wastl-etal-2025-machine</bibkey>
    </paper>
    <paper id="60">
      <title>Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination</title>
      <author><first>Jerry</first><last>Huang</last><affiliation>Montreal Institute for Learning Algorithms, Université de Montréal and The University of Tokyo</affiliation></author>
      <author><first>Prasanna</first><last>Parthasarathi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last><affiliation>Advanced Micro Devices</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>École Polytechnique de Montréal</affiliation></author>
      <pages>1075-1096</pages>
      <abstract>The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.</abstract>
      <url hash="ccac8a6f">2025.findings-acl.60</url>
      <bibkey>huang-etal-2025-robot</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>G</fixed-case>en<fixed-case>T</fixed-case>ool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation</title>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Jennifer</first><last>Neville</last></author>
      <author><first>Mengting</first><last>Wan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Longqi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Xiaofeng</first><last>Xu</last></author>
      <author><first>Xia</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Pei</first><last>Zhou</last><affiliation>Microsoft</affiliation></author>
      <pages>1097-1122</pages>
      <abstract>Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.</abstract>
      <url hash="4e1b2272">2025.findings-acl.61</url>
      <bibkey>he-etal-2025-gentool</bibkey>
    </paper>
    <paper id="62">
      <title><fixed-case>SWE</fixed-case>-Fixer: Training Open-Source <fixed-case>LLM</fixed-case>s for Effective and Efficient <fixed-case>G</fixed-case>it<fixed-case>H</fixed-case>ub Issue Resolution</title>
      <author><first>Chengxing</first><last>Xie</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Chang</first><last>Gao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>He</first><last>Du</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Difan</first><last>Zou</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>1123-1139</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios.We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.</abstract>
      <url hash="2203bfe5">2025.findings-acl.62</url>
      <bibkey>xie-etal-2025-swe</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>G</fixed-case>lyph<fixed-case>P</fixed-case>attern: An Abstract Pattern Recognition for Vision-Language Models</title>
      <author><first>Zixuan</first><last>Wu</last></author>
      <author><first>Yoolim</first><last>Kim</last><affiliation>Wellesley College</affiliation></author>
      <author><first>Carolyn Jane</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <pages>1140-1175</pages>
      <abstract>Vision-Language Models (VLMs) have made rapid progress in reasoning across visual and textual data. While VLMs perform well on vision tasks that they are trained on, our results highlight key challenges in abstract pattern recognition. We present GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of visual patterns from 40 writing systems with three visual presentation styles.GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models to understand and judge natural language descriptions of visual patterns. GlyphPattern patterns are drawn from a large-scale cognitive science investigation of human writing systems; as a result, they are rich in spatial reference and compositionality. Our experiments show that GlyphPattern is challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with marginal gains from few-shot prompting. Our detailed analysis reveals errors at multiple levels, including visual processing, natural language understanding, and pattern generalization.</abstract>
      <url hash="0962fbda">2025.findings-acl.63</url>
      <bibkey>wu-etal-2025-glyphpattern</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>F</fixed-case>it<fixed-case>CF</fixed-case>: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title>
      <author><first>Qianli</first><last>Wang</last></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Luis Felipe</first><last>Villa-Arenas</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <author><first>Vera</first><last>Schmitt</last><affiliation>Technische Universität Berlin</affiliation></author>
      <pages>1176-1191</pages>
      <abstract>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming three state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF’s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an importantfinding for future research in this direction.</abstract>
      <url hash="d2b94bda">2025.findings-acl.64</url>
      <bibkey>wang-etal-2025-fitcf</bibkey>
    </paper>
    <paper id="65">
      <title>From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Guocong</first><last>Li</last></author>
      <author><first>Weize</first><last>Liu</last></author>
      <author><first>Yihang</first><last>Wu</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuaihan</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Hongxia</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>1192-1209</pages>
      <abstract>Large language models (LLMs) exhibit excellent performance in natural language processing (NLP), but remain highly sensitive to the quality of input queries, especially when these queries contain misleading or inaccurate information. Existing methods focus on correcting the output, but they often overlook the potential of improving the ability of LLMs to detect and correct misleading content in the input itself. In this paper, we propose a novel three-stage fine-tuning method that enhances the ability of LLMs to detect and correct misleading information in the input, further improving response accuracy and reducing hallucinations. Specifically, the three stages include (1) training LLMs to identify misleading information, (2) training LLMs to correct the misleading information using built-in or external knowledge, and (3) training LLMs to generate accurate answers based on the corrected queries. To evaluate our method, we conducted experiments on three datasets for the hallucination detection task and the question answering (QA) task, as well as two datasets containing misleading information that we constructed. The experimental results demonstrate that our method significantly improves the accuracy and factuality of LLM responses, while also enhancing the ability to detect hallucinations and reducing the generation of hallucinations in the output, particularly when the query contains misleading information.</abstract>
      <url hash="0133eede">2025.findings-acl.65</url>
      <bibkey>li-etal-2025-misleading</bibkey>
    </paper>
    <paper id="66">
      <title>Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Xin</first><last>Lu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>1210-1225</pages>
      <abstract>Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges. A key issue is that fine-tuning often compromises the safety alignment of LLMs. To address this issue, we propose a method named <b>IRR</b> (<b>I</b>dentify, <b>R</b>emove, and <b>R</b>ecalibrate for Safety Realignment) that performs safety realignment for LLMs. The core of IRR is to identify and remove unsafe delta parameters from the fine-tuned models, while recalibrating the retained parameters. We evaluate the effectiveness of IRR across various datasets, including both full fine-tuning and LoRA methods. Our results demonstrate that IRR significantly enhances the safety performance of fine-tuned models on safety benchmarks, such as harmful queries and jailbreak attacks, while maintaining their performance on downstream tasks. The source code is available at: <url>https://github.com/pikepokenew/IRR</url>.</abstract>
      <url hash="05a2f002">2025.findings-acl.66</url>
      <bibkey>wu-etal-2025-separate</bibkey>
    </paper>
    <paper id="67">
      <title>Nuclear Deployed!: Analyzing Catastrophic Risks in Decision-making of Autonomous <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Rongwu</first><last>Xu</last></author>
      <author><first>Xiaojian</first><last>Li</last></author>
      <author><first>Shuo</first><last>Chen</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>1226-1310</pages>
      <abstract>Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent’s Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents.</abstract>
      <url hash="45b69b32">2025.findings-acl.67</url>
      <bibkey>xu-etal-2025-nuclear</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>M</fixed-case>o<fixed-case>RE</fixed-case>: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning</title>
      <author><first>Dacao</first><last>Zhang</last></author>
      <author id="kun-zhang"><first>Kun</first><last>Zhang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Shimao</first><last>Chu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Le</first><last>Wu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Si</first><last>Wei</last><affiliation>IFLYTEK CO.LTD.</affiliation></author>
      <pages>1311-1324</pages>
      <abstract>With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aims to achieve efficient fine-tuning of LLMs with fewer parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA) introduces low-rank matrices to approximate the incremental tuning parameters and achieves impressive performance over multiple scenarios. After that, plenty of improvements have been proposed for further improvement. However, these methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting the efficiency and effectiveness of LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for multi-task PEFT. Specifically, instead of using an individual LoRA for each task, we align different ranks of LoRA module with different tasks, which we named low-rank experts. Moreover, we design a novel adaptive rank selector to select the appropriate expert for each task. By jointly training low-rank experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task scenarios. Finally, we conduct extensive experiments over multiple multi-task benchmarks along with different LLMs to verify model performance. Experimental results demonstrate that compared to traditional LoRA and its variants, MoRE significantly improves the performance of LLMs in multi-task scenarios and incurs no additional inference cost. We also release the model and code to facilitate the community.</abstract>
      <url hash="a0d9bd2c">2025.findings-acl.68</url>
      <bibkey>zhang-etal-2025-mixture</bibkey>
    </paper>
    <paper id="69">
      <title>Lunar Twins: We Choose to Go to the Moon with Large Language Models</title>
      <author><first>Xin-Yu</first><last>Xiao</last></author>
      <author><first>Yalei</first><last>Liu</last></author>
      <author><first>Xiangyu</first><last>Liu</last></author>
      <author><first>Zengrui</first><last>Li</last></author>
      <author><first>Erwei</first><last>Yin</last><affiliation>Defense Innovation Institute, Academy of Military Sciences (AMS) and Tianjin Artificial Intelligence Innovation Center (TAIIC), Tianjin, 300450, China</affiliation></author>
      <author><first>Qianchen</first><last>Xia</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>1325-1339</pages>
      <abstract>In recent years, the rapid advancement of large language models (LLMs) has significantly reshaped the landscape of scientific research. While LLMs have achieved notable success across various domains, their application in specialized fields such as lunar exploration remains underdeveloped, and their full potential in this domain has yet to be fully realized. To address this gap, we introduce Lunar Twins, the first LLMs designed specifically for lunar exploration, along with a collaborative framework that combines both large and small models. Additionally, we present Lunar GenData, a multi-agent collaborative workflow for generating lunar instructions, and establish the first specialized lunar dataset, which integrates real data from the Chang’e lunar missions. Lastly, we developed Lunar Eval, the first comprehensive evaluation suite for assessing the capabilities of LLMs in lunar exploration tasks. Experimental validation demonstrates that our approach not only enhances domain expertise in lunar exploration but also reveals preliminary indications of embodied intelligence potential.</abstract>
      <url hash="23250487">2025.findings-acl.69</url>
      <bibkey>xiao-etal-2025-lunar</bibkey>
    </paper>
    <paper id="70">
      <title><fixed-case>SPHERE</fixed-case>: An Evaluation Card for Human-<fixed-case>AI</fixed-case> Systems</title>
      <author><first>Dora</first><last>Zhao</last><affiliation>Stanford University</affiliation></author>
      <author><first>Qianou</first><last>Ma</last></author>
      <author><first>Xinran</first><last>Zhao</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Chenglei</first><last>Si</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chenyang</first><last>Yang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ryan</first><last>Louie</last><affiliation>Stanford University</affiliation></author>
      <author><first>Ehud</first><last>Reiter</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>1340-1365</pages>
      <abstract>In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices.</abstract>
      <url hash="c35deee1">2025.findings-acl.70</url>
      <bibkey>zhao-etal-2025-sphere</bibkey>
    </paper>
    <paper id="71">
      <title>Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling</title>
      <author><first>Maximillian</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Ruoxi</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Sercan O</first><last>Arik</last><affiliation>Google</affiliation></author>
      <pages>1366-1387</pages>
      <abstract>Conversational assistants are increasingly popular across diverse real-world applications, highlighting the need for advanced multimodal speech modeling. Speech, as a natural mode of communication, encodes rich user-specific characteristics such as speaking rate and pitch, making it critical for effective interaction. Our work introduces a data-centric customization approach for efficiently enhancing multimodal understanding in conversational speech modeling. Central to our contributions is a novel multi-task learning paradigm that involves designing auxiliary tasks to utilize a small amount of speech data. Our approach achieves state-of-the-art performance on the Spoken-SQuAD benchmark, using only 10% of the training data with open-weight models, establishing a robust and efficient framework for audio-centric conversational modeling. We also introduce ASK-QA, the first dataset for multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation inputs.</abstract>
      <url hash="a8c5d808">2025.findings-acl.71</url>
      <bibkey>chen-etal-2025-data</bibkey>
    </paper>
    <paper id="72">
      <title>Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models</title>
      <author><first>Haochen</first><last>Liu</last></author>
      <author><first>Song</first><last>Wang</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Jundong</first><last>Li</last><affiliation>University of Virginia</affiliation></author>
      <pages>1388-1400</pages>
      <abstract>Large Language Models (LLMs) often struggle with tasks requiring external knowledge, such as knowledge-intensive Multiple Choice Question Answering (MCQA). Integrating Knowledge Graphs (KGs) can enhance reasoning; however, existing methods typically demand costly fine-tuning or retrieve noisy KG information. Recent approaches leverage Graph Neural Networks (GNNs) to generate KG-based input embedding prefixes as soft prompts for LLMs but fail to account for question relevance, resulting in noisy prompts. Moreover, in MCQA tasks, the absence of relevant KG knowledge for certain answer options remains a significant challenge. To address these issues, we propose Question-Aware Knowledge Graph Prompting (QAP), which incorporates question embeddings into GNN aggregation to dynamically assess KG relevance. QAP employs global attention to capture inter-option relationships, enriching soft prompts with inferred knowledge. Experimental results demonstrate that QAP outperforms state-of-the-art methods across multiple datasets, highlighting its effectiveness.</abstract>
      <url hash="c83071b5">2025.findings-acl.72</url>
      <bibkey>liu-etal-2025-question</bibkey>
    </paper>
    <paper id="73">
      <title><tex-math>\texttt{UQ-Merge}</tex-math>: Uncertainty Guided Multimodal Large Language Model Merging</title>
      <author><first>Huaizhi</first><last>Qu</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Xinyu</first><last>Zhao</last></author>
      <author><first>Jie</first><last>Peng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Kwonjoon</first><last>Lee</last><affiliation>Honda Research Institute USA</affiliation></author>
      <author><first>Behzad</first><last>Dariush</last><affiliation>Honda Research Institute USA</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>1401-1417</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have gained increasing popularity as a promising framework for leveraging the strong language reasoning capabilities in the vision-language domain. Given a wide range of MLLMs, model merging potentially offers a cheap way to aggregate their diverse knowledge into a single MLLM. However, directly plug-in existing model merging approaches often leads to suboptimal performance due to (1) inclusion of harmful models that have over-confident predictions in the target task; (2) the lack of specialized designs for vision-language inputs. To tackle these pain points, we conduct pioneering investigations to dissect the merging procedures and propose an uncertainty-guided MLLM merging algorithm, <tex-math>\textit{i.e.}</tex-math>, <tex-math>\texttt{UQ-Merge}</tex-math>, which <tex-math>i</tex-math>) identifies beneficial candidates for merging, <tex-math>ii</tex-math>) determines the merging order and the number of helpful candidates, and <tex-math>iii</tex-math>) performs appropriate merging. Within our framework, we consider uncertainty quantification on both text and vision inputs to examine the MLLM prediction confidence, and then decide whether and when a MLLM needs to be included. It is worth mentioning that our vision-language uncertainty quantification does not require access to sample labels, making it more practical in various scenarios. Extensive experiments consistently demonstrate the superior MLLM merging performance of <tex-math>\texttt{UQ-Merge}</tex-math> in both held-in and held-out vision-language benchmarks. For example, compared to existing state-of-the-art merging methods, <tex-math>\texttt{UQ-Merge}</tex-math> brings substantial performance improvements of up to 44.3% on average accuracy in 12 datasets. Codes are available at https://anonymous.4open.science/r/UQ-Merge-7CD7.</abstract>
      <url hash="64fa19d3">2025.findings-acl.73</url>
      <bibkey>qu-etal-2025-uq</bibkey>
    </paper>
    <paper id="74">
      <title><fixed-case>AQ</fixed-case>u<fixed-case>AECHR</fixed-case>: Attributed Question Answering for <fixed-case>E</fixed-case>uropean Court of Human Rights</title>
      <author><first>Korbinian Q.</first><last>Weidinger</last></author>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Oana</first><last>Ichim</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1418-1447</pages>
      <abstract>LLMs have become prevalent tools for information seeking across various fields, including law. However, their generated responses often suffer from hallucinations, hindering their widespread adoption in high stakes domains such as law, which can potentially mislead experts and propagate societal harms. To enhance trustworthiness in these systems, one promising approach is to attribute the answer to an actual source, thereby improving the factuality and verifiability of the response. In pursuit of advancing attributed legal question answering, we introduce AQuAECHR, a benchmark comprising information-seeking questions from ECHR jurisprudence along with attributions to relevant judgments. We present strategies to automatically curate this dataset from ECHR case law guides and utilize an LLM-based filtering pipeline to improve dataset quality, as validated by legal experts. Additionally, we assess several LLMs, including those trained on legal corpora, on this dataset to underscore significant challenges with the current models and strategies dealing with attributed QA, both quantitatively and qualitatively.</abstract>
      <url hash="b59bf94a">2025.findings-acl.74</url>
      <bibkey>weidinger-etal-2025-aquaechr</bibkey>
    </paper>
    <paper id="75">
      <title>Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation</title>
      <author><first>Yuhao</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xiangnan</first><last>Ma</last></author>
      <author><first>Kaiqi</first><last>Kou</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Peizhuo</first><last>Liu</last></author>
      <author><first>Weiqiao</first><last>Shan</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yuxin</first><last>Huang</last></author>
      <author><first>Zhengtao</first><last>Yu</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>1448-1460</pages>
      <abstract>The success of building textless speech-to-speech translation (S2ST) models has attracted much attention. However, S2ST still faces two main challenges: 1) extracting linguistic features for various speech signals, called cross-modal (CM), and 2) learning alignment of difference languages in long sequences, called cross-lingual (CL). We propose the unit language to overcome the two modeling challenges. The unit language can be considered a text-like representation format, constructed using <tex-math>n</tex-math>-gram language modeling. We implement multi-task learning to utilize the unit language in guiding the speech modeling process. Our initial results reveal a conflict when applying source and target unit languages simultaneously. We propose task prompt modeling to mitigate this conflict. We conduct experiments on four languages of the Voxpupil dataset. Our method demonstrates significant improvements over a strong baseline and achieves performance comparable to models trained with text.</abstract>
      <url hash="ae2260ba">2025.findings-acl.75</url>
      <bibkey>zhang-etal-2025-leveraging-unit</bibkey>
    </paper>
    <paper id="76">
      <title>Ponder &amp; Press: Advancing Visual <fixed-case>GUI</fixed-case> Agent towards General Computer Control</title>
      <author><first>Yiqin</first><last>Wang</last></author>
      <author><first>Haoji</first><last>Zhang</last><affiliation>Tsinghua University, ByteDance Inc. and Alibaba Group</affiliation></author>
      <author><first>Jingqi</first><last>Tian</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yansong</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>1461-1473</pages>
      <abstract>Most existing GUI agents typically depend on non-vision inputs like HTML source code or accessibility trees, limiting flexibility across diverse software environments and platforms. Current multimodal large language models (MLLMs), though excel at using vision to ground real-world objects, often struggle with accurately localizing GUI elements – a critical requirement for effective GUI automation – due to the semantic gap between real-world objects and GUI elements. In this work, we introduce Ponder &amp; Press, a divide-and-conquer framework for general computer control that uses only visual input. Our approach combines a general-purpose MLLM as an ‘interpreter’, responsible for translating high-level user instructions into detailed action descriptions, with a GUI-specific MLLM as a ‘locator’ that precisely locates GUI elements for action placement. By leveraging a purely visual input, our agent offers a versatile, human-like interaction paradigm applicable to various applications. Ponder &amp; Press locator outperforms existing models by +22.5% on the ScreenSpot GUI grounding benchmark. More offline and interactive agent benchmarks across various GUI environments – including web pages, desktop software, and mobile UIs – demonstrate that the Ponder &amp; Press framework achieves state-of-the-art performance, highlighting the potential of visual GUI agents.</abstract>
      <url hash="f0993264">2025.findings-acl.76</url>
      <bibkey>wang-etal-2025-ponder</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>L</fixed-case>ogic<fixed-case>G</fixed-case>ame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models</title>
      <author><first>Jiayi</first><last>Gui</last></author>
      <author><first>Yiming</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>1474-1491</pages>
      <abstract>Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.</abstract>
      <url hash="45ed5f9f">2025.findings-acl.77</url>
      <bibkey>gui-etal-2025-logicgame</bibkey>
    </paper>
    <paper id="78">
      <title><fixed-case>LLM</fixed-case>-Based Multi-Agent Systems are Scalable Graph Generative Models</title>
      <author><first>Jiarui</first><last>Ji</last></author>
      <author><first>Runlin</first><last>Lei</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jialing</first><last>Bi</last></author>
      <author><first>Zhewei</first><last>Wei</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xuchen</first><last>Pan</last></author>
      <author><first>Yaliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bolin</first><last>Ding</last><affiliation>Alibaba Group</affiliation></author>
      <pages>1492-1523</pages>
      <abstract>The structural properties of naturally arising social graphs are extensively studied to understand their evolution. Prior approaches for modeling network dynamics typically rely on rule-based models, which lack realism and generalizability, or deep learning-based models, which require large-scale training datasets. As abstract graph representations of entity-wise interactions, social graphs present an opportunity to explore network evolution mechanisms through realistic simulations of human-item interactions. Leveraging the pre-trained social consensus knowledge embedded in large language models (LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic, text-attributed social graph generation. GAG simulates the temporal node and edge generation processes for zero-shot social graph generation. The resulting graphs adhere to seven key macroscopic network properties, achieving an 11% improvement in microscopic graph structure metrics. Through the node classification benchmarking task, we validate that GAG effectively captures the intricate text-structure correlations in graph generation. Furthermore, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation with parallel acceleration, achieving a minimum speed-up of 90.4%. The source code is available at https://github.com/Ji-Cather/GraphAgent.</abstract>
      <url hash="3b23a93e">2025.findings-acl.78</url>
      <bibkey>ji-etal-2025-llm</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>AD</fixed-case>-<fixed-case>LLM</fixed-case>: Benchmarking Large Language Models for Anomaly Detection</title>
      <author><first>Tiankai</first><last>Yang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Yi</first><last>Nian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Li</first><last>Li</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ruiyao</first><last>Xu</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Yuangang</first><last>Li</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Zhuo</first><last>Xiao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiyang</first><last>Hu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <author><first>Yue</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <pages>1524-1547</pages>
      <abstract>Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs’ pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.</abstract>
      <url hash="2cff4905">2025.findings-acl.79</url>
      <bibkey>yang-etal-2025-ad</bibkey>
    </paper>
    <paper id="80">
      <title><fixed-case>RTAD</fixed-case>ev: Intention Aligned Multi-Agent Framework for Software Development</title>
      <author><first>Jie</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Guohua</first><last>Wang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Ronghui</first><last>Yang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jiajie</first><last>Zeng</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Mengchen</first><last>Zhao</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <pages>1548-1581</pages>
      <abstract>LLM-based Multi-agent frameworks have shown a great potential in solving real-world software development tasks, where the agents of different roles can communicate much more efficiently than humans. Despite their efficiency, LLM-based agents can hardly fully understand each other, which frequently causes errors during the development process. Moreover, the accumulation of errors could easily lead to the failure of the whole project. In order to reduce such errors, we introduce an intention aligned multi-agent framework RTADev, which utilizes a self-correction mechanism to ensure that all agents work based on a consensus. RTADev mimics human teams where individuals are free to start meetings anytime for reaching agreement. Specifically, RTADev integrates an alignment checking phase and a conditional ad hoc group review phase, so that the errors can be effectively reduced with minimum agent communications. Our experiments on various software development tasks show that RTADev significantly improves the quality of generated software code in terms of executability, structural and functional completeness. The code of our project is available at https://github.com/codeagent-rl/RTADev.</abstract>
      <url hash="ca64b0c5">2025.findings-acl.80</url>
      <bibkey>liu-etal-2025-rtadev</bibkey>
    </paper>
    <paper id="81">
      <title><fixed-case>TACO</fixed-case>-<fixed-case>RL</fixed-case>: Task Aware Prompt Compression Optimization with Reinforcement Learning</title>
      <author><first>Shivam</first><last>Shandilya</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Menglin</first><last>Xia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Supriyo</first><last>Ghosh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huiqiang</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jue</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qianhui</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Victor</first><last>Rühle</last><affiliation>Microsoft</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <pages>1582-1597</pages>
      <abstract>The increasing prevalence of large language models (LLMs) such as GPT-4 in various applications has led to a surge in the size of prompts required for optimal performance, leading to challenges in computational efficiency. Prompt compression aims to reduce the inference cost by minimizing input tokens without compromising on the task performance. However, existing prompt compression techniques either rely on sub-optimal metrics such as information entropy or model it as a task-agnostic token classification problem that fails to capture task-specific information.To address these issues, we propose a novel and efficient reinforcement learning (RL) based task-aware prompt compression method. To ensure low latency requirements, we leverage existing Transformer encoder-based token classification model while guiding the learning process with task-specific reward signals using lightweight REINFORCE algorithm. We evaluate the performance of our method on three diverse and challenging tasks including text summarization, question answering and code summarization. We demonstrate that our RL-guided compression method improves the task performance by 8% - 189% across these three scenarios over state-of-the-art compression techniques while satisfying the same compression rate and latency requirements.</abstract>
      <url hash="aeee6cb0">2025.findings-acl.81</url>
      <bibkey>shandilya-etal-2025-taco</bibkey>
    </paper>
    <paper id="82">
      <title>A Character-Centric Creative Story Generation via Imagination</title>
      <author><first>Kyeongman</first><last>Park</last></author>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>1598-1645</pages>
      <abstract>Creative story generation has long been a goal of NLP research. While existing methodologies have aimed to generate long and coherent stories, they fall significantly short of human capabilities in terms of diversity and character depth. To address this, we introduce a novel story generation framework called <tex-math>CCI</tex-math> (Character-centric Creative story generation via Imagination). CCI features two modules for creative story generation: <tex-math>IG</tex-math> (Image-Guided Imagination) and <tex-math>MW</tex-math> (Multi-Writer model). In the IG module, we utilize a text-to-image model to create visual representations of key story elements, such as characters, backgrounds, and main plots, in a more novel and concrete manner than text-only approaches. The MW module uses these story elements to generate multiple persona-description candidates and selects the best one to insert into the story, thereby enhancing the richness and depth of the narrative. We compared the stories generated by CCI and baseline models through statistical analysis, as well as human and LLM evaluations. The results showed that the IG and MW modules significantly improve various aspects of the stories’ creativity. Furthermore, our framework enables interactive multi-modal story generation with users, opening up new possibilities for human-LLM integration in cultural development. Project page : https://www.2024cci.p-e.kr/</abstract>
      <url hash="643333cc">2025.findings-acl.82</url>
      <bibkey>park-etal-2025-character</bibkey>
    </paper>
    <paper id="83">
      <title>Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large Language Model</title>
      <author><first>Minghan</first><last>Wang</last><affiliation>Monash University</affiliation></author>
      <author><first>Viet Thanh</first><last>Pham</last><affiliation>Monash University</affiliation></author>
      <author><first>Farhad</first><last>Moghimifar</last><affiliation>Monash University</affiliation></author>
      <author><first>Thuy-Trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <pages>1646-1662</pages>
      <abstract>Despite achieving remarkable performance, machine translation (MT) research remains underexplored in terms of translating cultural elements in languages, such as idioms, proverbs, and colloquial expressions. This paper investigates the capability of state-of-the-art neural machine translation (NMT) and large language models (LLMs) in translating proverbs, which are deeply rooted in cultural contexts. We construct a translation dataset of standalone proverbs and proverbs in conversation for four language pairs. Our experiments show that the studied models can achieve good translation between languages with similar cultural backgrounds, and LLMs generally outperform NMT models in proverb translation. Furthermore, we find that current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation, highlighting the need for more culturally aware evaluation metrics.</abstract>
      <url hash="10baac4f">2025.findings-acl.83</url>
      <bibkey>wang-etal-2025-proverbs</bibkey>
    </paper>
    <paper id="84">
      <title>Towards Efficient <fixed-case>LLM</fixed-case> Grounding for Embodied Multi-Agent Collaboration</title>
      <author><first>Yang</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shixin</first><last>Yang</last></author>
      <author><first>Chenjia</first><last>Bai</last><affiliation>TeleAI, China Telecom</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiu</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>China Telecom and Northwestern Polytechnical University</affiliation></author>
      <pages>1663-1699</pages>
      <abstract>Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at https://read-llm.github.io/.</abstract>
      <url hash="a5069f4b">2025.findings-acl.84</url>
      <bibkey>zhang-etal-2025-towards-efficient</bibkey>
    </paper>
    <paper id="85">
      <title><fixed-case>UAQF</fixed-case>act: Evaluating Factual Knowledge Utilization of <fixed-case>LLM</fixed-case>s on Unanswerable Questions</title>
      <author><first>Chuanyuan</first><last>Tan</last><affiliation>Soochow University</affiliation></author>
      <author><first>Wenbiao</first><last>Shao</last></author>
      <author><first>Hao</first><last>Xiong</last></author>
      <author><first>Tong</first><last>Zhu</last></author>
      <author><first>Zhenhua</first><last>Liu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Kai</first><last>Shi</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Wenliang</first><last>Chen</last><affiliation>Soochow University, China</affiliation></author>
      <pages>1700-1715</pages>
      <abstract>Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps prevent misleading responses in complex situations. While previous studies have built several datasets to assess LLMs’ performance on UAQ, these datasets lack factual knowledge support, which limits the evaluation of LLMs’ ability to utilize their factual knowledge when handling UAQ. To address the limitation, we introduce a new unanswerable question dataset <b>UAQFact</b>, a bilingual dataset with auxiliary factual knowledge created from a Knowledge Graph. Based on UAQFact, we further define two new tasks to measure LLMs’ ability to utilize internal and external factual knowledge, respectively. Our experimental results across multiple LLM series show that UAQFact presents significant challenges, as LLMs do not consistently perform well even when they have factual knowledge stored. Additionally, we find that incorporating external knowledge may enhance performance, but LLMs still cannot make full use of the knowledge which may result in incorrect responses. Our code and dataset are available at https://github.com/cytan17726/UAQ_Fact.</abstract>
      <url hash="3c34c2c9">2025.findings-acl.85</url>
      <bibkey>tan-etal-2025-uaqfact</bibkey>
    </paper>
    <paper id="86">
      <title>Exploring Knowledge Filtering for Retrieval-Augmented Discriminative Tasks</title>
      <author><first>Minjie</first><last>Qiang</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <author><first>HaoYuan</first><last>Ma</last></author>
      <author><first>Shoushan</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>1716-1729</pages>
      <abstract>Retrieval-augmented methods have achieved remarkable advancements in alleviating the hallucination of large language models.Nevertheless, the introduction of external knowledge does not always lead to the expected improvement in model performance, as irrelevant or harmful information present in the retrieved knowledge can compromise the prediction process.To address these challenges, we propose a novel framework aimed at improving model performance by incorporating knowledge filtering and prediction fusion mechanisms.In particular, our approach first employs a perplexity-based annotation method to collect training data.Then, we design four distinct strategies to filter out harmful retrieved knowledge.Finally, we integrate the filtered knowledge to generate the final result via batch-wise predictions.We conduct extensive experiments across multiple discriminative task datasets to evaluate the proposed framework.The results demonstrate that our framework can significantly enhance the performance of models on discriminative tasks.</abstract>
      <url hash="e94c6abe">2025.findings-acl.86</url>
      <bibkey>qiang-etal-2025-exploring-knowledge</bibkey>
    </paper>
    <paper id="87">
      <title>Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model</title>
      <author><first>Chong</first><last>Li</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yingzhuo</first><last>Deng</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>1730-1754</pages>
      <abstract>The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.</abstract>
      <url hash="a9d0da58">2025.findings-acl.87</url>
      <bibkey>li-etal-2025-group</bibkey>
    </paper>
    <paper id="88">
      <title>Beyond Verbal Cues: Emotional Contagion Graph Network for Causal Emotion Entailment</title>
      <author><first>Fangxu</first><last>Yu</last></author>
      <author><first>Junjie</first><last>Guo</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xinyu</first><last>Dai</last><affiliation>Nanjing University</affiliation></author>
      <pages>1755-1767</pages>
      <abstract>Emotions are fundamental to conversational understanding. While significant advancements have been achieved in conversational emotion recognition and emotional response generation, recognizing the causes of eliciting emotions is less explored. Previous studies have primarily focused on identifying the causes of emotions by understanding verbal contextual utterances, overlooking that non-verbal emotional cues can elicit emotions. To address this issue, we develop an Emotional Contagion Graph Network (ECGN) that simulates the impact of non-verbal implicit emotions on the counterpart’s emotions. To achieve this, we construct a heterogeneous graph that simulates the transmission of non-verbal emotions alongside verbal influences. By applying message passing between nodes, the constructed graph effectively models both the implicit emotional dynamics and explicit verbal interactions. We evaluate ECGN’s performance through extensive experiments on the benchmark datasets and compare it against multiple state-of-the-art models. Experimental results demonstrate the effectiveness of the proposed model. Our code is available at https://github.com/Yu-Fangxu/ECGN.</abstract>
      <url hash="d520e4e1">2025.findings-acl.88</url>
      <bibkey>yu-etal-2025-beyond-verbal</bibkey>
    </paper>
    <paper id="89">
      <title>Critic-<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic</title>
      <author><first>Xin</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Lou</last></author>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Xueru</first><last>Wen</last></author>
      <author><first>Yuqiu</first><last>Ji</last></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Debing</first><last>Zhang</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>1768-1806</pages>
      <abstract>Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities. Moreover, there is a lack of in-depth investigations into the relationship between LLM’s ability to criticize and its task-solving performance. To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability. Through a step-wise CoT reasoning paradigm and the automatic construction of weak-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities. Experiments on GSM8K and MATH and out-of-domain evaluation demonstrate that our enhanced model significantly boosts task-solving performance by filtering out invalid solutions or iterative refinement. Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict.</abstract>
      <url hash="4e083c33">2025.findings-acl.89</url>
      <bibkey>zheng-etal-2025-critic</bibkey>
    </paper>
    <paper id="90">
      <title>Systematic Generalization in Language Models Scales with Information Entropy</title>
      <author><first>Sondre</first><last>Wold</last></author>
      <author><first>Lucas Georges Gabriel</first><last>Charpentier</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Étienne</first><last>Simon</last></author>
      <pages>1807-1819</pages>
      <abstract>Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.</abstract>
      <url hash="30fecc29">2025.findings-acl.90</url>
      <bibkey>wold-etal-2025-systematic</bibkey>
    </paper>
    <paper id="91">
      <title>The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage</title>
      <author><first>Byung-Doh</first><last>Oh</last><affiliation>New York University</affiliation></author>
      <author><first>Hongao</first><last>Zhu</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>William</first><last>Schuler</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>1820-1827</pages>
      <abstract>In psycholinguistic modeling, surprisal from larger pre-trained language models has been shown to be a poorer predictor of naturalistic human reading times. However, it has been speculated that this may be due to data leakage that caused language models to see the text stimuli during training. This paper presents two studies to address this concern at scale. The first study reveals relatively little leakage of five naturalistic reading time corpora in two pre-training datasets in terms of length and frequency of token <tex-math>n</tex-math>-gram overlap. The second study replicates the negative relationship between language model size and the fit of surprisal to reading times using models trained on ‘leakage-free’ data that overlaps only minimally with the reading time corpora. Taken together, this suggests that previous results using language models trained on these corpora are not driven by the effects of data leakage.</abstract>
      <url hash="a7ee2431">2025.findings-acl.91</url>
      <bibkey>oh-etal-2025-inverse</bibkey>
    </paper>
    <paper id="92">
      <title>Logical Consistency is Vital: Neural-Symbolic Information Retrieval for Negative-Constraint Queries</title>
      <author><first>Ganlin</first><last>Xu</last></author>
      <author><first>Zhoujia</first><last>Zhang</last></author>
      <author><first>Wangyi</first><last>Mei</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Weijia</first><last>Lu</last><affiliation>UAES</affiliation></author>
      <author><first>Xiaodong</first><last>Zhang</last><affiliation>United Automotive Electronic Systems Co., Ltd.</affiliation></author>
      <author><first>Zhifei</first><last>Yang</last><affiliation>United Automotive Electronic Systems Co., Ltd.</affiliation></author>
      <author><first>Xiaofeng</first><last>Ma</last><affiliation>UAES</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>1828-1847</pages>
      <abstract>Information retrieval plays a crucial role in resource localization. Current dense retrievers retrieve the relevant documents within a corpus via embedding similarities, which compute similarities between dense vectors mainly depending on word co-occurrence between queries and documents, but overlook the real query intents. Thus, they often retrieve numerous irrelevant documents. Particularly in the scenarios of complex queries such as negative-constraint queries, their retrieval performance could be catastrophic. To address the issue, we propose a neuro-symbolic information retrieval method, namely NS-IR, that leverages first-order logic (FOL) to optimize the embeddings of naive natural language by considering the logical consistency between queries and documents. Specifically, we introduce two novel techniques, logic alignment and connective constraint, to re-rank candidate documents, thereby enhancing retrieval relevance. Furthermore, we construct a new dataset <b>NegConstraint</b> including negative-constraint queries to evaluate our NS-IR’s performance on such complex IR scenarios. Our extensive experiments demonstrate that NS-IR not only achieves superior zero-shot retrieval performance on web search and low-resource retrieval tasks, but also performs better on negative-constraint queries. Our scource code and dataset are available at https://github.com/xgl-git/NS-IR-main.</abstract>
      <url hash="e3184680">2025.findings-acl.92</url>
      <bibkey>xu-etal-2025-logical</bibkey>
    </paper>
    <paper id="93">
      <title>‘No’ Matters: Out-of-Distribution Detection in Multimodality Multi-Turn Interactive Dialogue Download <fixed-case>PDF</fixed-case></title>
      <author><first>Rena Wei</first><last>Gao</last></author>
      <author><first>Xuetong</first><last>Wu</last></author>
      <author><first>Siwen</first><last>Luo</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <author><first>Feng</first><last>Liu</last><affiliation>University of Melbourne</affiliation></author>
      <pages>1848-1864</pages>
      <abstract>Out-of-distribution (OOD) detection in multimodal contexts is essential for identifying deviations in different modalities, particularly for interactive dialogue systems in real-life interactions, where the systems are usually infeasible to deploy large language models (LLMs) to generate dialogue responses due to data privacy and ethical issues. This paper aims to improve label detection that involves multi-round long dialogues by efficiently detecting OOD dialogues and images. We introduce a novel scoring framework named Dialogue Image Aligning and Enhancing Framework (DIAEF) that integrates the visual language models with the novel proposed scores that detect OOD in two key scenarios (1) mismatches between the dialogue and image input pair and (2) input pairs with previously unseen labels. Our experimental results, derived from various benchmarks, demonstrate that integrating image and multi-round dialogue OOD detection is more effective with previously unseen labels than using either modality independently. In the presence of mismatched pairs, our proposed score effectively identifies these mismatches and demonstrates strong robustness in long dialogues. This approach enhances domain-aware, adaptive conversational agents and establishes baselines for future studies.</abstract>
      <url hash="aad60e94">2025.findings-acl.93</url>
      <bibkey>gao-etal-2025-matters</bibkey>
    </paper>
    <paper id="94">
      <title>Event Pattern-Instance Graph: A Multi-Round Role Representation Learning Strategy for Document-Level Event Argument Extraction</title>
      <author><first>Qizhi</first><last>Wan</last></author>
      <author><first>LiuTao</first><last>LiuTao</last></author>
      <author><first>Changxuan</first><last>Wan</last><affiliation>Jiangxi University of Finance and Economics</affiliation></author>
      <author><first>Rong</first><last>Hu</last><affiliation>Jiangxi University of Finance and Economics</affiliation></author>
      <author><first>Keli</first><last>Xiao</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Yuxin</first><last>Shuai</last><affiliation>Jiangxi University of Finance and Economics</affiliation></author>
      <pages>1865-1877</pages>
      <abstract>For document-level event argument extraction, existing role-based span selection strategies suffer from several limitations: (1) ignoring interrelations among arguments within an event instance; (2) relying on pre-trained language models to capture role semantics at either the event pattern or document, without leveraging pattern-instance associations. To address these limitations, this paper proposes a multi-round role representation learning strategy. First, we construct an event pattern-instance graph (EPIG) to comprehensively capture the role semantics embedded in various direct and indirect associations, including those among roles within event patterns, arguments within event instances, and the alignments between patterns and instances. Second, to enhance the learning of role node representation in the graph, we optimize the update mechanisms for both node and edge representations in the EPIG graph. By leveraging the graph attention network, we iteratively update the representations of role nodes and role edges. The role representations learned from the EPIG are then integrated into the original role representations, further enriching their semantic information. Finally, a role representation memory module and a multi-round learning strategy is proposed to retain and refine role representations learned from previously analyzed documents. This memory mechanism enhances the prediction performance in subsequent rounds of span selection. Extensive experiments on three datasets verify the effectiveness of the model.</abstract>
      <url hash="fee7ad11">2025.findings-acl.94</url>
      <bibkey>wan-etal-2025-event</bibkey>
    </paper>
    <paper id="95">
      <title><fixed-case>EXECUTE</fixed-case>: A Multilingual Benchmark for <fixed-case>LLM</fixed-case> Token Understanding</title>
      <author><first>Lukas</first><last>Edman</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Helmut</first><last>Schmid</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>1878-1887</pages>
      <abstract>The CUTE benchmark showed that LLMs struggle with character understanding in English. We extend it to more languages with diverse scripts and writing systems, introducing EXECUTE. Our simplified framework allows easy expansion to any language. Tests across multiple LLMs reveal that challenges in other languages are not always on the character level as in English. Some languages show word-level processing issues, some show no issues at all. We also examine sub-character tasks in Chinese, Japanese, and Korean to assess LLMs’ understanding of character components.</abstract>
      <url hash="36dac09f">2025.findings-acl.95</url>
      <bibkey>edman-etal-2025-execute</bibkey>
    </paper>
    <paper id="96">
      <title>Explainable Hallucination through Natural Language Inference Mapping</title>
      <author><first>Wei-Fan</first><last>Chen</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Zhixue</first><last>Zhao</last><affiliation>University of Sheffield, University of Sheffield</affiliation></author>
      <author><first>Akbar</first><last>Karimi</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <pages>1888-1896</pages>
      <abstract>Large language models (LLMs) often generate hallucinated content, making it crucial to identify and quantify inconsistencies in their outputs. We introduce HaluMap, a post-hoc framework that detects hallucinations by mapping entailment and contradiction relations between source inputs and generated outputs using a natural language inference (NLI) model. To improve reliability, we propose a calibration step leveraging intra-text relations to refine predictions. HaluMap outperforms state-of-the-art NLI-based methods by five percentage points compared to other training-free approaches, while providing clear, interpretable explanations. As a training-free and model-agnostic approach, HaluMap offers a practical solution for verifying LLM outputs across diverse NLP tasks. The resources of this paper are available at https://github.com/caisa-lab/acl25-halumap.</abstract>
      <url hash="9d5a3725">2025.findings-acl.96</url>
      <bibkey>chen-etal-2025-explainable</bibkey>
    </paper>
    <paper id="97">
      <title><fixed-case>H</fixed-case>op<fixed-case>RAG</fixed-case>: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation</title>
      <author><first>Hao</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhengren</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xi</first><last>Chen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Qinhan</first><last>Yu</last></author>
      <author><first>Wentao</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>1897-1913</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose <b>HopRAG</b>, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a <i>retrieve-reason-prune</i> mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Experiments on multiple multi-hop benchmarks demonstrate that HopRAG’s <i>retrieve-reason-prune</i> mechanism can expand the retrieval scope based on logical connections and improve final answer quality.</abstract>
      <url hash="bd8f1df6">2025.findings-acl.97</url>
      <bibkey>liu-etal-2025-hoprag</bibkey>
    </paper>
    <paper id="98">
      <title>Double Entendre: Robust Audio-Based <fixed-case>AI</fixed-case>-Generated Lyrics Detection via Multi-View Fusion</title>
      <author><first>Markus</first><last>Frohmann</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Gabriel</first><last>Meseguer-Brocal</last><affiliation>Deezer</affiliation></author>
      <author><first>Markus</first><last>Schedl</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Elena V.</first><last>Epure</last><affiliation>Deezer</affiliation></author>
      <pages>1914-1926</pages>
      <abstract>The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.</abstract>
      <url hash="a7325bc1">2025.findings-acl.98</url>
      <bibkey>frohmann-etal-2025-double</bibkey>
    </paper>
    <paper id="99">
      <title>Don’t Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models</title>
      <author><first>Sangmin</first><last>Woo</last></author>
      <author><first>Donguk</first><last>Kim</last><affiliation>KAIST</affiliation></author>
      <author><first>Jaehyuk</first><last>Jang</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yubin</first><last>Choi</last><affiliation>KAIST, Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Changick</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>1927-1951</pages>
      <abstract>Large Vision Language Models (LVLMs) demonstrate strong capabilities in visual understanding and description, yet often suffer from hallucinations, attributing incorrect or misleading features to images. We observe that LVLMs disproportionately focus on a small subset of image tokens—termed blind tokens—which are typically irrelevant to the query (e.g., background or non-object regions). We hypothesize that such attention misalignment plays a key role in generating hallucinated responses. To mitigate this issue, we propose Attentional Vision Calibration (AvisC), a test-time approach that dynamically recalibrates the influence of blind tokens without modifying the underlying attention mechanism. AvisC first identifies blind tokens by analyzing layer-wise attention distributions over image tokens, then employs a contrastive decoding strategy to balance the influence of original and blind-token-biased logits. Experiments on standard benchmarks, including POPE, MME, and AMBER, demonstrate that AvisC effectively reduces hallucinations in LVLMs.</abstract>
      <url hash="cc196346">2025.findings-acl.99</url>
      <bibkey>woo-etal-2025-dont</bibkey>
    </paper>
    <paper id="100">
      <title><fixed-case>SATA</fixed-case>: A Paradigm for <fixed-case>LLM</fixed-case> Jailbreak via Simple Assistive Task Linkage</title>
      <author><first>Xiaoning</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wenbo</first><last>Hu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Tianxing</first><last>He</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>1952-1987</pages>
      <abstract>Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remains a major concern. Exploring jailbreak prompts can expose LLMs’ vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task—such as a masked language model task or an element lookup by position task—to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.</abstract>
      <url hash="7fd9da9a">2025.findings-acl.100</url>
      <bibkey>dong-etal-2025-sata</bibkey>
    </paper>
    <paper id="101">
      <title>Chain-Talker: Chain Understanding and Rendering for Empathetic Conversational Speech Synthesis</title>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Rui</first><last>Liu</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Yi</first><last>Ren</last><affiliation>ByteDance</affiliation></author>
      <author><first>Xiang</first><last>Yin</last></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>1988-2003</pages>
      <abstract>Conversational Speech Synthesis (CSS) aims to align synthesized speech with the emotional and stylistic context of user-agent interactions to achieve empathy. Current generative CSS models face interpretability limitations due to insufficient emotional perception and redundant discrete speech coding. To address the above issues, we present Chain-Talker, a three-stage framework mimicking human cognition: Emotion Understanding derives context-aware emotion descriptors from dialogue history; Semantic Understanding generates compact semantic codes via serialized prediction; and Empathetic Rendering synthesizes expressive speech by integrating both components. To support emotion modeling, we develop CSS-EmCap, an LLM-driven automated pipeline for generating precise conversational speech emotion captions. Experiments on three benchmark datasets demonstrate that Chain-Talker produces more expressive and empathetic speech than existing methods, with CSS-EmCap contributing to reliable emotion modeling. The code and demos are available at: https://github.com/AI-S2-Lab/Chain-Talker.</abstract>
      <url hash="33d00c17">2025.findings-acl.101</url>
      <bibkey>hu-etal-2025-chain</bibkey>
    </paper>
    <paper id="102">
      <title>Parameter-Efficient Fine-Tuning via Circular Convolution</title>
      <author><first>Aochuan</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiashun</first><last>Cheng</last></author>
      <author><first>Zijing</first><last>Liu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Ziqi</first><last>Gao</last></author>
      <author><first>Fugee</first><last>Tsung</last><affiliation>Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>2004-2019</pages>
      <abstract>Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large foundation models, leveraging low-rank matrices <tex-math>\mathbf A</tex-math> and <tex-math>\mathbf B</tex-math> to represent weight changes (i.e., <tex-math>\Delta \mathbf W = \mathbf B \mathbf A</tex-math>). This method reduces trainable parameters and mitigates heavy memory consumption associated with full delta matrices by sequentially multiplying <tex-math>\mathbf A</tex-math> and <tex-math>\mathbf B</tex-math> with the activation. Despite its success, the intrinsic low-rank characteristic may limit its performance. Although several variants have been proposed to address this issue, they often overlook the crucial computational and memory efficiency brought by LoRA. In this paper, we propose Circular Convolution Adaptation (C<tex-math>^3</tex-math>A), which not only achieves high-rank adaptation with enhanced performance but also excels in both computational power and memory utilization. Extensive experiments demonstrate that C<tex-math>^3</tex-math>A consistently outperforms LoRA and its variants across various fine-tuning tasks.</abstract>
      <url hash="b9881ef0">2025.findings-acl.102</url>
      <bibkey>chen-etal-2025-parameter</bibkey>
    </paper>
    <paper id="103">
      <title>Alleviating Hallucinations in Large Language Models via Truthfulness-driven Rank-adaptive <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Jiahao</first><last>Li</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>2020-2031</pages>
      <abstract>Improving the truthfulness of LLMs to alleviate hallucinations has become critical for promoting the practical deployment of LLMs. Current fine-tuning-based methods ignore the intrinsic discrepancy in the truthfulness correlations across LLM internal modules, and instead treat them equally, which may potentially decrease the performance of truthfulness improvement. In this paper, we propose a truthfulness-driven rank-adaptive LoRA method to improve LLM truthfulness (RaLFiT), which adaptively allocates the ranks in LoRA training according to the truthfulness correlations of modules within LLM. Specifically, it first measures the truthfulness correlation of each LLM module by a probing process, and allocates higher ranks to strongly correlated modules, which means a larger update subspace during training. Experimental results on TruthfulQA show that RaLFiT consistently outperforms previous state-of-the-art methods across the Llama LLM family, verifying its effectiveness and superiority, and for the first time makes the performance of 7B Llama LLMs exceed GPT-4.</abstract>
      <url hash="998c9921">2025.findings-acl.103</url>
      <bibkey>li-etal-2025-alleviating</bibkey>
    </paper>
    <paper id="104">
      <title><fixed-case>S</fixed-case>c<fixed-case>E</fixed-case>dit: Script-based Assessment of Knowledge Editing</title>
      <author><first>Xinye</first><last>Li</last></author>
      <author><first>Zunwen</first><last>Zheng</last></author>
      <author><first>Qian</first><last>Zhang</last></author>
      <author><first>Dekai</first><last>Zhuang</last></author>
      <author><first>Jiabao</first><last>Kang</last></author>
      <author><first>Liyan</first><last>Xu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Qingbin</first><last>Liu</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Zhiying</first><last>Tu</last></author>
      <author><first>Dianhui</first><last>Chu</last></author>
      <author><first>Dianbo</first><last>Sui</last></author>
      <pages>2032-2052</pages>
      <abstract>Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark – ScEdit (Script-based Knowledge Editing Benchmark) – which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (“What”-type question) evaluation to action-based (“How”-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at https://github.com/asdfo123/ScEdit.</abstract>
      <url hash="433bc6ca">2025.findings-acl.104</url>
      <bibkey>li-etal-2025-scedit</bibkey>
    </paper>
    <paper id="105">
      <title><fixed-case>S</fixed-case>afe<fixed-case>R</fixed-case>oute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</title>
      <author><first>Seanie</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Dong Bok</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Dominik</first><last>Wagner</last></author>
      <author><first>Minki</first><last>Kang</last><affiliation>KRAFTON and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Haebin</first><last>Seong</last><affiliation>Maum.ai</affiliation></author>
      <author><first>Tobias</first><last>Bocklet</last><affiliation>TH Nürnberg</affiliation></author>
      <author><first>Juho</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <pages>2053-2069</pages>
      <abstract>Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on “hard” examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model’s capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.</abstract>
      <url hash="a662c31c">2025.findings-acl.105</url>
      <bibkey>lee-etal-2025-saferoute</bibkey>
    </paper>
    <paper id="106">
      <title>Moderation Matters: Measuring Conversational Moderation Impact in <fixed-case>E</fixed-case>nglish as a Second Language Group Discussion</title>
      <author><first>Rena</first><last>Gao</last></author>
      <author><first>Ming-Bin</first><last>Chen</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>2070-2095</pages>
      <abstract>English as a Second Language (ESL) speakers often struggle to engage in group discussions due to language barriers. While moderators can facilitate participation, few studies assess conversational engagement and evaluate moderation effectiveness. To address this gap, we develop a dataset comprising 17 sessions from an online ESL conversation club, which includes both moderated and non-moderated discussions. We then introduce an approach that integrates automatic ESL dialogue assessment and a framework that categorizes moderation strategies. Our findings indicate that moderators help improve the flow of topics and start/end a conversation. Interestingly, we find active acknowledgement and encouragement to be the most effective moderation strategy, while excessive information and opinion sharing by moderators has a negative impact. Ultimately, our study paves the way for analyzing ESL group discussions and the role of moderators in non-native conversation settings. Code and data are available at <a href="https://github.com/RenaGao/L2Moderator">https://github.com/RenaGao/L2Moderator</a>.</abstract>
      <url hash="4057be6a">2025.findings-acl.106</url>
      <bibkey>gao-etal-2025-moderation</bibkey>
    </paper>
    <paper id="107">
      <title>Measuring Bias and Agreement in Large Language Model Presupposition Judgments</title>
      <author><first>Katherine</first><last>Atwell</last><affiliation>Northeastern University and University of Pittsburgh</affiliation></author>
      <author><first>Mandy</first><last>Simons</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>2096-2107</pages>
      <abstract>Identifying linguistic bias in text demands the identification not only of explicitly asserted content but also of implicit content including presuppositions. Large language models (LLMs) offer a promising automated approach to detecting presuppositions, yet the extent to which their judgments align with human intuitions remains unexplored. Moreover, LLMs may inadvertently reflect societal biases when identifying presupposed content. To empirically investigate this, we prompt multiple large language models to evaluate presuppositions across diverse textual domains, drawing from three distinct datasets annotated by human raters. We calculate the agreement between LLMs and human raters, and find several linguistic factors associated with fluctuations in human-model agreement. Our observations reveal discrepancies in human-model alignment, suggesting potential biases in LLMs, notably influenced by gender and political ideology.</abstract>
      <url hash="6f5b8f51">2025.findings-acl.107</url>
      <bibkey>atwell-etal-2025-measuring</bibkey>
    </paper>
    <paper id="108">
      <title>Harnessing <fixed-case>PDF</fixed-case> Data for Improving <fixed-case>J</fixed-case>apanese Large Multimodal Models</title>
      <author><first>Jeonghun</first><last>Baek</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Kiyoharu</first><last>Aizawa</last><affiliation>The University of Tokyo, The University of Tokyo and Tokyo University of Science</affiliation></author>
      <pages>2108-2123</pages>
      <abstract>Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.</abstract>
      <url hash="e0274e82">2025.findings-acl.108</url>
      <bibkey>baek-etal-2025-harnessing</bibkey>
    </paper>
    <paper id="109">
      <title><fixed-case>E</fixed-case>ner<fixed-case>GIZA</fixed-case>r: Leveraging <fixed-case>GIZA</fixed-case>++ for Effective Tokenizer Initialization</title>
      <author><first>Pranaydeep</first><last>Singh</last></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Gorka</first><last>Azkune</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Orphee</first><last>De Clercq</last><affiliation>Ghent University</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>Ghent University</affiliation></author>
      <pages>2124-2137</pages>
      <abstract>Continual pre-training has long been considered the default strategy for adapting models to non-English languages, but struggles with initializing new embeddings, particularly for non-Latin scripts. In this work, we propose EnerGIZAr, a novel methodology that improves continual pre-training by leveraging statistical word alignment techniques. Our approach utilizes GIZA++ to construct a subword-level alignment matrix between source (English) and target language tokens. This matrix enables informed initialization of target tokenizer embeddings, which provides a more effective starting point for adaptation. We evaluate EnerGIZAr against state-of-the-art initialization strategies such as OFA and FOCUS across four typologically diverse languages: Hindi, Basque, Arabic and Korean. Experimental results on key NLP tasks – including POS tagging, Sentiment Analysis, NLI, and NER – demonstrate that EnerGIZAr achieves superior monolingual performance while also out-performing all methods for cross-lingual transfer when tested on XNLI. With EnerGIZAr, we propose an intuitive, explainable as well as state-of-the-art initialisation technique for continual pre-training of English models.</abstract>
      <url hash="34338e2c">2025.findings-acl.109</url>
      <bibkey>singh-etal-2025-energizar</bibkey>
    </paper>
    <paper id="110">
      <title><fixed-case>AMEX</fixed-case>: Android Multi-annotation Expo Dataset for Mobile <fixed-case>GUI</fixed-case> Agents</title>
      <author><first>Yuxiang</first><last>Chai</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Siyuan</first><last>Huang</last></author>
      <author><first>Yazhe</first><last>Niu</last></author>
      <author><first>Han</first><last>Xiao</last></author>
      <author><first>Liang</first><last>Liu</last></author>
      <author><first>Guozhi</first><last>Wang</last></author>
      <author><first>Dingyu</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Shuai</first><last>Ren</last><affiliation>vivo</affiliation></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2138-2156</pages>
      <abstract>AI agents have drawn increasing attention mostly on their ability to perceive environments, understand tasks, and autonomously achieve goals. To advance research on AI agents in mobile scenarios, we introduce the Android Multi-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mobile GUI-control agents which are capable of completing tasks by directly interacting with the graphical user interface (GUI) on mobile devices. AMEX comprises over 104K high-resolution screenshots from popular mobile applications, which are annotated at multiple levels. Unlike existing GUI-related datasets, e.g., Rico, AitW, etc., AMEX includes three levels of annotations: GUI interactive element grounding, GUI screen and element functionality descriptions, and complex natural language instructions with stepwise GUI-action chains. We develop this dataset from a more instructive and detailed perspective, complementing the general settings of existing datasets. Additionally, we finetune a baseline model SPHINX Agent and illustrate the effectiveness of AMEX.</abstract>
      <url hash="4539ebac">2025.findings-acl.110</url>
      <bibkey>chai-etal-2025-amex</bibkey>
    </paper>
    <paper id="111">
      <title>Drop Dropout on Single Epoch Language Model Pretraining</title>
      <author><first>Houjun</first><last>Liu</last><affiliation>Stanford University</affiliation></author>
      <author><first>John</first><last>Bauer</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher D</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <pages>2157-2166</pages>
      <abstract>Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced “early dropout” also degrades performance over applying no dropout at all. We further investigate the models’ editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to **drop dropout** during single-epoch pretraining.</abstract>
      <url hash="f357a651">2025.findings-acl.111</url>
      <bibkey>liu-etal-2025-drop-dropout</bibkey>
    </paper>
    <paper id="112">
      <title>Robust and Minimally Invasive Watermarking for <fixed-case>E</fixed-case>aa<fixed-case>S</fixed-case></title>
      <author><first>Zongqi</first><last>Wang</last></author>
      <author><first>Baoyuan</first><last>Wu</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Jingyuan</first><last>Deng</last></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>2167-2191</pages>
      <abstract>Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.</abstract>
      <url hash="5475472f">2025.findings-acl.112</url>
      <bibkey>wang-etal-2025-robust</bibkey>
    </paper>
    <paper id="113">
      <title>Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text</title>
      <author><first>Jarca</first><last>Andrei</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Florinel Alin</first><last>Croitoru</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Radu Tudor</first><last>Ionescu</last><affiliation>Universitatea Bucuresti</affiliation></author>
      <pages>2192-2201</pages>
      <abstract>Masked language modeling has become a widely adopted unsupervised technique to pre-train large language models (LLMs). However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.</abstract>
      <url hash="1bd25cfd">2025.findings-acl.113</url>
      <bibkey>andrei-etal-2025-task</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>CARMO</fixed-case>: Dynamic Criteria Generation for Context Aware Reward Modelling</title>
      <author><first>Taneesh</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shivam</first><last>Shandilya</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xuchao</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rahul</first><last>Madhavan</last><affiliation>Indian Institute of Management, Ahmedabad, Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology and Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Supriyo</first><last>Ghosh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chetan</first><last>Bansal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <pages>2202-2261</pages>
      <abstract>Reward modeling in large language models is known to be susceptible to reward hacking, causing models to latch onto superficial features such as the tendency to generate lists or unnecessarily long responses. In RLHF, and more generally during post-training, flawed reward signals often lead to outputs that optimize for these spurious correlates instead of genuine quality or correctness. We propose **Carmo (Context-Aware Reward Modeling)**, a novel approach that first generates dynamic, context-relevant criteria to ground the reward model prior to producing reward scores. Unlike prior methods that use static rubrics, Carmo leverages powerful LLMs to adaptively create evaluation criteria, e.g., logical consistency, clarity, and depth, tailored to the user query. Our theoretical analysis shows that such criteria generation can mitigate reward hacking. We further demonstrate how Carmo can be distilled into smaller models, thereby lowering the computational cost of alignment. We establish a new state-of-the-art performance on zero shot setting for generative models, with a 2.1% improvement on Reward Bench. Furthermore, alignment performed on the Carmo-curated preference dataset achieves **22.5% and 21.1% LC-WR (%) and WR (%) on Mistral-Base (7B)**. We release our datasets at [huggingface/CARMO](https://huggingface.co/datasets/Multi-preference-Optimization/CARMO-UltraFeedback).</abstract>
      <url hash="5e706b14">2025.findings-acl.114</url>
      <bibkey>gupta-etal-2025-carmo</bibkey>
    </paper>
    <paper id="115">
      <title><fixed-case>SLAM</fixed-case>-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training</title>
      <author><first>Wenxi</first><last>Chen</last></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Ruiqi</first><last>Yan</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yuzhe</first><last>Liang</last></author>
      <author><first>Xiquan</first><last>Li</last></author>
      <author><first>Ruiyang</first><last>Xu</last></author>
      <author><first>Zhikang</first><last>Niu</last></author>
      <author><first>Yanqiao</first><last>Zhu</last></author>
      <author><first>Yifan</first><last>Yang</last></author>
      <author><first>Zhanxun</first><last>Liu</last></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yuxuan</first><last>Hu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jinyu</first><last>Li</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yan</first><last>Lu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Xie</first><last>Chen</last></author>
      <pages>2262-2282</pages>
      <abstract>Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.</abstract>
      <url hash="82494f73">2025.findings-acl.115</url>
      <bibkey>chen-etal-2025-slam</bibkey>
    </paper>
    <paper id="116">
      <title><fixed-case>C</fixed-case><tex-math>^2</tex-math><fixed-case>LEVA</fixed-case>: Toward Comprehensive and Contamination-Free Language Model Evaluation</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Wong Tin</first><last>Long</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Cheung To</first><last>Hung</last></author>
      <author><first>Jianqiao</first><last>Zhao</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Duo</first><last>Zheng</last></author>
      <author><first>Liu Ka</first><last>Wai</last></author>
      <author><first>Michael R.</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2283-2306</pages>
      <abstract>Recent advances in large language models (LLMs) have shown significant promise, yet their evaluation raises concerns, particularly regarding data contamination due to the lack of access to proprietary training data. To address this issue, we present C<tex-math>^2</tex-math>LEVA, a comprehensive bilingual benchmark featuring systematic contamination prevention. C<tex-math>^2</tex-math>LEVA firstly offers a holistic evaluation encompassing 22 tasks, each targeting a specific application or ability of LLMs, and secondly a trustworthy assessment due to our contamination-free tasks, ensured by a systematic contamination prevention strategy that fully automates test data renewal and enforces data protection during benchmark data release. Our large-scale evaluation of 15 open-source and proprietary models demonstrates the effectiveness of C<tex-math>^2</tex-math>LEVA.</abstract>
      <url hash="287deb37">2025.findings-acl.116</url>
      <bibkey>li-etal-2025-c2leva</bibkey>
    </paper>
    <paper id="117">
      <title>Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering</title>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>2307-2318</pages>
      <abstract>In table question answering (TQA), tables are encoded as either texts or images. Prior work suggests that passing images of tables to multi-modal large language models (MLLMs) performs comparably to using textual input with large language models (LLMs). However, the lack of controlled setups limits fine-grained distinctions between these approaches. In this paper, we conduct the first controlled study on the effectiveness of several combinations of table representations and model types from two perspectives: question complexity and table size. We build a new benchmark based on existing TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we find that the best combination of table representation and model varies across setups. We propose FRES, a method selecting table representations dynamically, and observe a 10% average performance improvement compared to using both representations indiscriminately.</abstract>
      <url hash="1a380be0">2025.findings-acl.117</url>
      <bibkey>zhou-etal-2025-texts</bibkey>
    </paper>
    <paper id="118">
      <title>Adaptive-<fixed-case>VP</fixed-case>: A Framework for <fixed-case>LLM</fixed-case>-Based Virtual Patients that Adapts to Trainees’ Dialogue to Facilitate Nurse Communication Training</title>
      <author><first>Keyeun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seolhee</first><last>Lee</last></author>
      <author><first>Esther Hehsun</first><last>Kim</last></author>
      <author><first>Yena</first><last>Ko</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jinsu</first><last>Eun</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dahee</first><last>Kim</last></author>
      <author><first>Hyewon</first><last>Cho</last></author>
      <author><first>Haiyi</first><last>Zhu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Robert E.</first><last>Kraut</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Eunyoung E.</first><last>Suh</last></author>
      <author><first>Eun-mee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hajin</first><last>Lim</last><affiliation>Seoul National University</affiliation></author>
      <pages>2319-2352</pages>
      <abstract>Effective communication training is essential to preparing nurses for high-quality patient care. While standardized patient (SP) simulations provide valuable experiential learning, they are often costly and inflexible. Virtual patient (VP) systems offer a scalable alternative, but most fail to adapt to the varying communication skills of trainees. In particular, when trainees respond ineffectively, VPs should escalate in hostility or become uncooperative—yet this level of adaptive interaction remains largely unsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue generation framework that leverages large language models (LLMs) to dynamically adapt VP behavior based on trainee input. The framework features a pipeline for constructing clinically grounded yet flexible VP scenarios and a modular system for assessing trainee communication and adjusting VP responses in real time, while ensuring learner safety. We validated Adaptive-VP by simulating challenging patient conversations. Automated evaluation using a corpus from practicing nurses showed that our communication skill evaluation mechanism reflected real-world proficiency levels. Expert nurses further confirmed that Adaptive-VP produced more natural and realistic interactions than existing approaches, demonstrating its potential as a scalable and effective tool for nursing communication training.</abstract>
      <url hash="863e6e99">2025.findings-acl.118</url>
      <bibkey>lee-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="119">
      <title>Enhancing Multimodal Unified Representations for Cross Modal Generalization</title>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Yan</first><last>Xia</last></author>
      <author><first>Shengpeng</first><last>Ji</last></author>
      <author><first>Shulei</first><last>Wang</last></author>
      <author><first>Hanting</first><last>Wang</last></author>
      <author><first>Minghui</first><last>Fang</last></author>
      <author><first>Jieming</first><last>Zhu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Sashuai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>2353-2366</pages>
      <abstract>To enhance the interpretability of multimodal unified representations, many studies have focused on discrete unified representations. These efforts typically start with contrastive learning and gradually extend to the disentanglement of modal information, achieving solid multimodal discrete unified representations. However, existing research often overlooks two critical issues: 1) The use of Euclidean distance for quantization in discrete representations often overlooks the important distinctions among different dimensions of features, resulting in redundant representations after quantization; 2) Different modalities have unique characteristics, and a uniform alignment approach does not fully exploit these traits. To address these issues, we propose Training-free Optimization of Codebook (TOC) and Fine and Coarse cross-modal Information Disentangling (FCID). These methods refine the unified discrete representations from pretraining and perform fine- and coarse-grained information disentanglement tailored to the specific characteristics of each modality, achieving significant performance improvements over previous state-of-the-art models. The code is available at https://github.com/haihuangcode/CMG.</abstract>
      <url hash="8ee79ac3">2025.findings-acl.119</url>
      <bibkey>huang-etal-2025-enhancing-multimodal</bibkey>
    </paper>
    <paper id="120">
      <title>Domain Regeneration: How well do <fixed-case>LLM</fixed-case>s match syntactic properties of text domains?</title>
      <author><first>Da</first><last>Ju</last><affiliation>Facebook</affiliation></author>
      <author><first>Hagen</first><last>Blix</last><affiliation>New York University</affiliation></author>
      <author><first>Adina</first><last>Williams</last><affiliation>FAIR (Meta Platforms Inc.)</affiliation></author>
      <pages>2367-2388</pages>
      <abstract>Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data—Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.</abstract>
      <url hash="ab94d4b6">2025.findings-acl.120</url>
      <bibkey>ju-etal-2025-domain</bibkey>
    </paper>
    <paper id="121">
      <title>Structural Deep Encoding for Table Question Answering</title>
      <author><first>Raphaël</first><last>Mouravieff</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last><affiliation>CNRS / ISIR, Sorbonne Université and CNRS</affiliation></author>
      <author><first>Sylvain</first><last>Lamprier</last><affiliation>Université d’Angers</affiliation></author>
      <pages>2389-2402</pages>
      <abstract>Although Transformers-based architectures excel at processing textual information, their naive adaptation for tabular data often involves flattening the table structure. This simplification can lead to the loss of essential inter-dependencies between rows, columns, and cells, while also posing scalability challenges for large tables. To address these issues, prior works have explored special tokens, structured embeddings, and sparse attention patterns. In this paper, we conduct a comprehensive analysis of tabular encoding techniques used in QA, which highlights the crucial role of attention sparsity in preserving structural information of tables. We also introduce a set of novel sparse attention mask designs for tabular data, that not only enhance computational efficiency but also preserve structural integrity, leading to better overall performance.</abstract>
      <url hash="571b4a5e">2025.findings-acl.121</url>
      <bibkey>mouravieff-etal-2025-structural</bibkey>
    </paper>
    <paper id="122">
      <title><fixed-case>MPL</fixed-case>: Multiple Programming Languages with Large Language Models for Information Extraction</title>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Hebei University of Technology</affiliation></author>
      <author><first>Gexiang</first><last>Fang</last></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhenghua</first><last>Xu</last></author>
      <author><first>Jinglei</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Hebei University of Technology</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2403-2414</pages>
      <abstract>Recent research in information extraction (IE) focuses on utilizing code-style inputs to enhance structured output generation. The intuition behind this is that the programming languages (PLs) inherently exhibit greater structural organization than natural languages (NLs). This structural advantage makes PLs particularly suited for IE tasks. Nevertheless, existing research primarily focuses on Python for code-style simulation, overlooking the potential of other widely-used PLs (e.g., C++ and Java) during the supervised fine-tuning (SFT) phase. In this research, we propose Multiple Programming Languages with large language models for information extraction (abbreviated as MPL), a novel framework that explores the potential of incorporating different PLs in the SFT phase. Additionally, we introduce function-prompt with virtual running to simulate code-style inputs more effectively and efficiently. Experimental results on a wide range of datasets demonstrate the effectiveness of MPL. Furthermore, we conduct extensive experiments to provide a comprehensive analysis. Our code and additional files are in the supplementary materials.</abstract>
      <url hash="067566fe">2025.findings-acl.122</url>
      <bibkey>li-etal-2025-mpl</bibkey>
    </paper>
    <paper id="123">
      <title>Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering</title>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Huiming</first><last>Fan</last></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Qianyu</first><last>Wang</last></author>
      <author><first>Mingda</first><last>Yang</last></author>
      <author><first>Jiafeng</first><last>Liang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhongjie</first><last>Wang</last></author>
      <author><first>Hao</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Guo</first><last>Tang</last></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2415-2438</pages>
      <abstract>Although large language models (LLMs) have demonstrated remarkable reasoning capabilities, they still face challenges in knowledge-intensive multi-hop reasoning. Recent work explores iterative retrieval to address complex problems. However, the absence of intermediate guidance often leads to inaccurate retrieval and intermediate reasoning errors, leading to incorrect reasoning. To address these, we propose Self-Critique Guided Iterative Reasoning (SiGIR), which uses self-critique feedback to guide the iterative reasoning process. Specifically, through end-to-end training, we enable the model to iteratively address complex problems via question decomposition, while also being able to self-evaluate its intermediate reasoning steps. During iterative reasoning, the model engages in branching exploration and employs self-evaluation to guide the selection of promising reasoning trajectories. Extensive experiments on three multi-hop reasoning datasets demonstrate the effectiveness of our proposed method, surpassing the previous SOTA by 8.6%. Furthermore, our thorough analysis offers insights for future research. Our code, data, and models are available at https://github.com/zchuz/SiGIR-MHQA.</abstract>
      <url hash="09f51fcd">2025.findings-acl.123</url>
      <bibkey>chu-etal-2025-self</bibkey>
    </paper>
    <paper id="124">
      <title>Anchored Answers: Unravelling Positional Bias in <fixed-case>GPT</fixed-case>-2’s Multiple-Choice Questions</title>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Yanjun</first><last>Gao</last><affiliation>University of Colorado Anschutz Medical Campus</affiliation></author>
      <pages>2439-2465</pages>
      <abstract>Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse “anchored bias” in the GPT-2 family, where they consistently favour the first choice ‘A’ in MCQs during inference. This anchored bias challenges the integrity of GPT-2’s decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the “logit lens” method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice ‘A’, we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.</abstract>
      <url hash="d65d4399">2025.findings-acl.124</url>
      <bibkey>li-gao-2025-anchored</bibkey>
    </paper>
    <paper id="125">
      <title>Failing Forward: Improving Generative Error Correction for <fixed-case>ASR</fixed-case> with Synthetic Data and Retrieval Augmentation</title>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Levit</last><affiliation>Microsoft</affiliation></author>
      <author><first>Peidong</first><last>Wang</last></author>
      <author><first>Jian</first><last>Xue</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jinyu</first><last>Li</last><affiliation>Microsoft</affiliation></author>
      <pages>2466-2482</pages>
      <abstract>Generative Error Correction (GEC) has emerged as a powerful post-processing method to boost the performance of Automatic Speech Recognition (ASR) systems. In this paper, we first show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. First, we augment the GEC training dataset with synthetic data generated using foundational generative models, thereby simulating additional errors from which the model can learn from. For out-of-domain scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle NEs, we introduce retrieval-augmented correction wherein we augment the model input with entities retrieved from a datastore of NEs. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8%–30% relative WER improvements in ID and 10%–33% improvements in OOD settings.</abstract>
      <url hash="816c64f6">2025.findings-acl.125</url>
      <bibkey>ghosh-etal-2025-failing</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>LTRAG</fixed-case>: Enhancing Autoformalization and Self-refinement for Logical Reasoning with Thought-Guided <fixed-case>RAG</fixed-case></title>
      <author><first>Ruikang</first><last>Hu</last></author>
      <author><first>Shaoyu</first><last>Lin</last></author>
      <author><first>Yeliang</first><last>Xiu</last></author>
      <author><first>Yongmei</first><last>Liu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>2483-2493</pages>
      <abstract>Logical reasoning is fundamental to intelligent systems. Large language models (LLMs) have demonstrated promise in natural language (NL) reasoning, especially with techniques like chain-of-thought (CoT) prompting. Neuro-symbolic methods like Logic-LM and LINC further enhance performance on challenging datasets FOLIO and AR-LSAT by integrating formalization with LLMs and symbolic solvers, and possibly refinement with LLMs. However, these methods still struggle with the accurate formalization of complex NL problems.In this paper, we introduce LTRAG, a framework to enhance autoformalization and self-refinement for logical reasoning with Retrieval-Augmented Generation (RAG), by building knowledge bases of thought-guided examples (https://github.com/sysulic/LTRAG ).Experimental results on FOLIO and AR-LSAT show that LTRAG consistently outperforms Logic-LM and LINC across different models. On GPT-4 and AR-LSAT, it achieves an accuracy gain of 13% over Logic-LM.</abstract>
      <url hash="88620162">2025.findings-acl.126</url>
      <bibkey>hu-etal-2025-ltrag</bibkey>
    </paper>
    <paper id="127">
      <title>Eta-<fixed-case>W</fixed-case>av<fixed-case>LM</fixed-case>: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation</title>
      <author><first>Giuseppe</first><last>Ruggiero</last><affiliation>University of Turin</affiliation></author>
      <author><first>Matteo</first><last>Testa</last></author>
      <author><first>Jurgen Van De</first><last>Walle</last></author>
      <author><first>Luigi</first><last>Di Caro</last><affiliation>University of Turin, Italy</affiliation></author>
      <pages>2494-2504</pages>
      <abstract>Self-supervised learning (SSL) has reduced the reliance on expensive labeling in speech technologies by learning meaningful representations from unannotated data. Since most SSL-based downstream tasks prioritize content information in speech, ideal representations should disentangle content from unwanted variations like speaker characteristics in the SSL representations. However, removing speaker information often degrades other speech components, and existing methods either fail to fully disentangle speaker identity or require resource-intensive models. In this paper, we propose a novel disentanglement method that linearly decomposes SSL representations into speaker-specific and speaker-independent components, effectively generating speaker disentangled representations. Comprehensive experiments show that our approach achieves speaker independence and as such, when applied to content-driven tasks such as voice conversion, our representations yield significant improvements over state-of-the-art methods.</abstract>
      <url hash="e38938e0">2025.findings-acl.127</url>
      <bibkey>ruggiero-etal-2025-eta</bibkey>
    </paper>
    <paper id="128">
      <title><fixed-case>M</fixed-case>ath<fixed-case>C</fixed-case>oder-<fixed-case>VL</fixed-case>: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning</title>
      <author><first>Ke</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Junting</first><last>Pan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Linda</first><last>Wei</last></author>
      <author><first>Aojun</first><last>Zhou</last></author>
      <author><first>Weikang</first><last>Shi</last></author>
      <author><first>Zimu</first><last>Lu</last></author>
      <author><first>Han</first><last>Xiao</last></author>
      <author><first>Yunqiao</first><last>Yang</last></author>
      <author><first>Houxing</first><last>Ren</last><affiliation>Sensetime</affiliation></author>
      <author><first>Mingjie</first><last>Zhan</last></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2505-2534</pages>
      <abstract>Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%.</abstract>
      <url hash="424dd047">2025.findings-acl.128</url>
      <bibkey>wang-etal-2025-mathcoder</bibkey>
    </paper>
    <paper id="129">
      <title><fixed-case>M</fixed-case>ling<fixed-case>C</fixed-case>onf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</title>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Yiming</first><last>Du</last></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2535-2556</pages>
      <abstract>The tendency of Large Language Models (LLMs) to generate hallucinations raises concerns regarding their reliability. Therefore, confidence estimations indicating the extent of trustworthiness of the generations become essential. However, current LLM confidence estimations in languages other than English remain underexplored. This paper addresses this gap by introducing a comprehensive investigation of Multilingual Confidence estimation (MlingConf) on LLMs, focusing on both language-agnostic (LA) and language-specific (LS) tasks to explore the performance and language dominance effects of multilingual confidence estimations on different tasks. The benchmark comprises four meticulously checked and human-evaluated high-quality multilingual datasets for LA tasks and one for the LS task tailored to specific social, cultural, and geographical contexts of a language. Our experiments reveal that on LA tasks English exhibits notable linguistic dominance in confidence estimations than other languages, while on LS tasks, using question-related language to prompt LLMs demonstrates better linguistic dominance in multilingual confidence estimations. The phenomena inspire a simple yet effective native-tone prompting strategy by employing language-specific prompts for LS tasks, effectively improving LLMs’ reliability and accuracy in LS scenarios.</abstract>
      <url hash="eca08d81">2025.findings-acl.129</url>
      <bibkey>xue-etal-2025-mlingconf</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>COMPKE</fixed-case>: Complex Question Answering under Knowledge Editing</title>
      <author><first>Keyuan</first><last>Cheng</last></author>
      <author><first>Zijian</first><last>Kan</last></author>
      <author><first>Zhuoran</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Muhammad Asif</first><last>Ali</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Lijie</first><last>Hu</last></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>2557-2576</pages>
      <abstract>Knowledge Editing-Efficiently modifying the knowledge in large language models has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We perform a comprehensive evaluation of four different knowledge editing methods in COMPKE, and our results show that the performance of these methods varies between different models. For example, MeLLo achieves an accuracy of 39.47 on GPT-4o-mini but drops significantly to 3.83 on Qwen2.5-3B. We further analyze the reasons behind these results from both methodological and model perspectives. Our dataset will be publicly available on GitHub.</abstract>
      <url hash="1ba8e251">2025.findings-acl.130</url>
      <bibkey>cheng-etal-2025-compke</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>R</fixed-case>aa<fixed-case>S</fixed-case>: Reasoning-Aware Attention Sparsity for Efficient <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Junhao</first><last>Hu</last></author>
      <author><first>Wenrui</first><last>Huang</last></author>
      <author><first>Weidong</first><last>Wang</last></author>
      <author><first>Zhenwen</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Tiancheng</first><last>Hu</last></author>
      <author><first>Zhixia</first><last>Liu</last></author>
      <author><first>Xusheng</first><last>Chen</last></author>
      <author><first>Tao</first><last>Xie</last><affiliation>Peking University</affiliation></author>
      <author><first>Yizhou</first><last>Shan</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>2577-2590</pages>
      <abstract>Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires an LLM to generate long sequences, incurring <tex-math>O(N)</tex-math> time and memory complexities per token, where <tex-math>N</tex-math> is the current sequence length. To reduce complexities, existing sparsity-based algorithms propose to retain Key-Value (KV) vectors, the intermediate representations of only the most critical tokens. However, these algorithms struggle with the “impossible trinity” of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with <tex-math>O(L)</tex-math> time but <tex-math>O(N)</tex-math> memory (<tex-math>L</tex-math> is the cache budget, <tex-math>L \ll N</tex-math>). To address the “impossible trinity”, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm RaaS that identifies milestone tokens and retains their KV vectors until they are no longer needed, achieving high accuracy with <tex-math>O(L)</tex-math> time and <tex-math>O(L)</tex-math> memory complexities.</abstract>
      <url hash="c75b4744">2025.findings-acl.131</url>
      <bibkey>hu-etal-2025-raas</bibkey>
    </paper>
    <paper id="132">
      <title>One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models</title>
      <author><first>Rongguang</first><last>Ye</last></author>
      <author><first>Ming</first><last>Tang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>2591-2604</pages>
      <abstract>Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user’s compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.</abstract>
      <url hash="6cdddac4">2025.findings-acl.132</url>
      <bibkey>ye-tang-2025-one</bibkey>
    </paper>
    <paper id="133">
      <title><fixed-case>CL</fixed-case>a<fixed-case>MP</fixed-case> 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages</title>
      <author><first>Shangda</first><last>Wu</last></author>
      <author><first>Guo</first><last>Zhancheng</last></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Junyan</first><last>Jiang</last></author>
      <author><first>SeungHeon</first><last>Doh</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Gus</first><last>Xia</last><affiliation>New York University</affiliation></author>
      <author><first>Juhan</first><last>Nam</last></author>
      <author><first>Xiaobing</first><last>Li</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Feng</first><last>Yu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>2605-2625</pages>
      <abstract>CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities–including sheet music, performance signals, and audio recordings–with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.</abstract>
      <url hash="95cc1eea">2025.findings-acl.133</url>
      <bibkey>wu-etal-2025-clamp-3</bibkey>
    </paper>
    <paper id="134">
      <title><fixed-case>PFD</fixed-case>ial: A Structured Dialogue Instruction Fine-tuning Method Based on <fixed-case>UML</fixed-case> Flowcharts</title>
      <author><first>Ming</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuhui</first><last>Wang</last></author>
      <author><first>Yujiong</first><last>Shen</last></author>
      <author><first>Tingyi</first><last>Yang</last></author>
      <author><first>Changhao</first><last>Jiang</last></author>
      <author><first>Yilong</first><last>Wu</last></author>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Qinhao</first><last>Chen</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Zhihao</first><last>Zhang</last></author>
      <author><first>Yi</first><last>Dong</last></author>
      <author><first>Zhen</first><last>Wang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhihui</first><last>Fei</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Mingyang</first><last>Wan</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Tao</first><last>Liang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Guojun</first><last>Ma</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>2626-2649</pages>
      <abstract>Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct <b>P</b>rocess <b>F</b>low <b>Dial</b>ogue (<b>PFDial</b>) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models’ performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in <a href="https://github.com/KongLongGeFDU/PFDial">https://github.com/KongLongGeFDU/PFDial</a>.</abstract>
      <url hash="ff411600">2025.findings-acl.134</url>
      <bibkey>zhang-etal-2025-pfdial</bibkey>
    </paper>
    <paper id="135">
      <title>Listening to Patients: Detecting and Mitigating Patient Misreport in Medical Dialogue System</title>
      <author><first>Lang</first><last>Qin</last></author>
      <author><first>Yao</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Zhenglu</first><last>Yang</last><affiliation>Nankai University</affiliation></author>
      <pages>2650-2664</pages>
      <abstract>Medical Dialogue Systems (MDSs) have emerged as promising tools for automated healthcare support through patient-agent interactions. Previous efforts typically relied on an idealized assumption — patients can accurately report symptoms aligned with their actual health conditions. However, in reality, patients often misreport their symptoms, due to cognitive limitations, emotional factors, etc. Overlooking patient misreports can significantly compromise the diagnostic accuracy of MDSs. To address this critical issue, we emphasize the importance of enabling MDSs to “listen to patients” by tackling two key challenges: how to detect misreport and mitigate misreport effectively. In this work, we propose PaMis, a novel framework that can detect patient misreports based on calculating the structural entropy of the dialogue entity graph, and mitigate them through generating controlled clarifying questions. Our experimental results demonstrate that PaMis effectively enhances MDSs reliability by effectively addressing patient misreports during the medical response generation process.</abstract>
      <url hash="81ea4a4a">2025.findings-acl.135</url>
      <bibkey>qin-etal-2025-listening</bibkey>
    </paper>
    <paper id="136">
      <title>Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm</title>
      <author><first>Xiaoyang</first><last>Hu</last><affiliation>Brown University</affiliation></author>
      <author><first>Richard</first><last>Lewis</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>2665-2677</pages>
      <abstract>Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it is often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argues that GPT 3.5’s declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance is due at least in part to a limitation in task comprehension and task set maintenance. We challenge the best-performing model with progressively harder versions of the task (up to 10-back) and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.</abstract>
      <url hash="963776dc">2025.findings-acl.136</url>
      <bibkey>hu-lewis-2025-language</bibkey>
    </paper>
    <paper id="137">
      <title>Graph-guided Cross-composition Feature Disentanglement for Compositional Zero-shot Learning</title>
      <author><first>Yuxia</first><last>Geng</last><affiliation>PowerChina Huadong Engineering Corporation Limited</affiliation></author>
      <author><first>Runkai</first><last>Zhu</last></author>
      <author><first>Jiaoyan</first><last>Chen</last></author>
      <author><first>Jintai</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Yuxiang</first><last>Wang</last><affiliation>Hangzhou Dianzi University</affiliation></author>
      <author><first>Xiaoliang</first><last>Xu</last></author>
      <author><first>Sheng-Jun</first><last>Huang</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>2678-2690</pages>
      <abstract>Disentanglement of visual features of primitives (i.e., attributes and objects) has shown exceptional results in Compositional Zero-shot Learning (CZSL). However, due to the feature divergence of an attribute (resp. object) when combined with different objects (resp. attributes), it is challenging to learn disentangled primitive features that are general across different compositions. To this end, we propose the solution of <i>cross-composition feature disentanglement</i>, which takes multiple primitive-sharing compositions as inputs and constrains the disentangled primitive features to be general across these compositions. More specifically, we leverage a compositional graph to define the overall primitive-sharing relationships between compositions, and build a task-specific architecture upon the recently successful large pre-trained vision-language model (VLM) CLIP, with dual cross-composition disentangling adapters (called L-Adapter and V-Adapter) inserted into CLIP’s frozen text and image encoders, respectively. Evaluation on three popular CZSL benchmarks shows that our proposed solution significantly improves the performance of CZSL, and its components have been verified by solid ablation studies. Our code and data are available at: https://github.com/zhurunkai/DCDA.</abstract>
      <url hash="6fa75849">2025.findings-acl.137</url>
      <bibkey>geng-etal-2025-graph</bibkey>
    </paper>
    <paper id="138">
      <title>Training Long-Context <fixed-case>LLM</fixed-case>s Efficiently via Chunk-wise Optimization</title>
      <author><first>Wenhao</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yuxin</first><last>Zhang</last></author>
      <author><first>Gen</first><last>Luo</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Daohai</first><last>Yu</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Rongrong</first><last>Ji</last></author>
      <pages>2691-2700</pages>
      <abstract>While long-context large language models (LLMs) exhibit remarkable document processing capabilities, their prohibitively high training costs often hinder customized applications. To mitigate this issue, we propose __Sequential Chunk-wise Optimization (SeCO)__, a memory-efficient training paradigm that partitions lengthy inputs into manageable chunks. Each chunk independently constructs its computational graph and performs localized backpropagation, ensuring that only one chunk’s forward activations are stored in memory. Building on SeCO, we further introduce __Sparse Chunk-wise Optimization (SpaCO)__, which reduces computational overhead by selectively propagating gradients to specific chunks and incorporates a carefully designed compensation factor to ensure unbiased gradient estimation. SpaCO decouples the computational cost of backpropagation from the context length, enabling training time to gradually converge to inference time as sequences become longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer substantial practical benefits. For example, when fine-tuning an 8B model with LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to 16K tokens, while SpaCO demonstrates accelerated training speed—achieving up to 3× faster than SeCO under the same experimental setup. These innovations provide new insights into optimizing long-context models, making them more accessible for practical applications. We have open-sourced the code at https://anonymous.4open.science/r/seco-CCBD.</abstract>
      <url hash="e9fdd1b9">2025.findings-acl.138</url>
      <bibkey>li-etal-2025-training</bibkey>
    </paper>
    <paper id="139">
      <title>Revisiting <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> through the Lens of Parameter Redundancy: Spectral Encoding Helps</title>
      <author><first>Jiashun</first><last>Cheng</last></author>
      <author><first>Aochuan</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Ziqi</first><last>Gao</last></author>
      <author><first>Yuhan</first><last>Li</last></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Fugee</first><last>Tsung</last><affiliation>Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>2701-2718</pages>
      <abstract>Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models. Despite its successes, the substantial parameter redundancy, which limits the capacity and efficiency of LoRA, has been recognized as a bottleneck. In this work, we systematically investigate the impact of redundancy in fine-tuning LoRA and reveal that reducing density redundancy does not degrade expressiveness. Based on this insight, we introduce Spectral-encoding Low-Rank Adaptation (SeLoRA), which harnesses the robust expressiveness of spectral bases to re-parameterize LoRA from a sparse spectral subspace. Designed with simplicity, SeLoRA enables seamless integration with various LoRA variants for performance boosting, serving as a scalable plug-and-play framework. Extensive experiments substantiate that SeLoRA achieves greater efficiency with fewer parameters, delivering superior performance enhancements over strong baselines on various downstream tasks, including commonsense reasoning, math reasoning, and code generation.</abstract>
      <url hash="2d11ecbb">2025.findings-acl.139</url>
      <bibkey>cheng-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="140">
      <title><fixed-case>CODEMENV</fixed-case>: Benchmarking Large Language Models on Code Migration</title>
      <author><first>Keyuan</first><last>Cheng</last></author>
      <author><first>Xudong</first><last>Shen</last></author>
      <author><first>Yihao</first><last>Yang</last></author>
      <author><first>TengyueWang</first><last>TengyueWang</last></author>
      <author><first>Yang</first><last>Cao</last></author>
      <author><first>Muhammad Asif</first><last>Ali</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Hanbin</first><last>Wang</last></author>
      <author><first>Lijie</first><last>Hu</last></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>2719-2744</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable proficiency in handling a wide range of tasks within the software engineering domain, but their ability to perform code migration—adapting code to different environments—remains underexplored. In this work, we propose a novel benchmark, : <b>Code</b> <b>M</b>igration Across <b>Env</b>ironment, designed to evaluate LLMs’ performance in handling code migration tasks. The benchmark comprises 922 data points across 19 Python and Java packages, offering three tasks to systematically evaluate code migration: identifying version-incompatible functions, determining function changes, and adapting code to target environments. Experimental evaluation of across seven LLMs revealed an average pass@1 rate of 26.50%, with GPT-4o performing best at 43.84%. We highlight our key findings as follows: (i) LLMs are more familiar with newer function versions, making them better at migrating legacy code, and (ii) a logical inconsistency where LLMs sometimes identify irrelevant function changes for the target migration environment.</abstract>
      <url hash="561c8e94">2025.findings-acl.140</url>
      <bibkey>cheng-etal-2025-codemenv</bibkey>
    </paper>
    <paper id="141">
      <title>A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in <fixed-case>LLM</fixed-case>s</title>
      <author><first>V.S.D.S.Mahesh</first><last>Akavarapu</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Hrishikesh</first><last>Terdalkar</last></author>
      <author><first>Pramit</first><last>Bhattacharyya</last></author>
      <author><first>Shubhangi</first><last>Agarwal</last></author>
      <author><first>Dr. Vishakha</first><last>Deulgaonkar</last><affiliation>Indian Institute of Technology, Kanpur, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Chaitali</first><last>Dangarikar</last></author>
      <author><first>Pralay</first><last>Manna</last></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>2745-2761</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages—Sanskrit, Ancient Greek and Latin—to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question–answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.</abstract>
      <url hash="6e7cda2b">2025.findings-acl.141</url>
      <bibkey>akavarapu-etal-2025-case</bibkey>
    </paper>
    <paper id="142">
      <title><fixed-case>B</fixed-case>rain<fixed-case>ECHO</fixed-case>: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation</title>
      <author><first>Jilong</first><last>Li</last></author>
      <author><first>Zhenxi</first><last>Song</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Honghai</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhiguo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2762-2778</pages>
      <abstract>Current EEG/MEG-to-text decoding systems suffer from three key limitations: (1) reliance on teacher-forcing methods, which compromises robustness during inference, (2) sensitivity to session-specific noise, hindering generalization across subjects, and (3) misalignment between brain signals and linguistic representations due to pre-trained language model over-dominance. To overcome these challenges, we propose BrainECHO (<tex-math>\textbf{B}</tex-math>rain signal decoding via v<tex-math>\textbf{E}</tex-math>ctor-quantized spe<tex-math>\textbf{C}</tex-math>trogram reconstruction for W<tex-math>\textbf{H}</tex-math>isper-enhanced text generati<tex-math>\textbf{O}</tex-math>n), a multi-stage framework that employs decoupled representation learning to achieve state-of-the-art performance on both EEG and MEG datasets. Specifically, BrainECHO consists of three stages: (1) Discrete autoencoding, which transforms continuous Mel spectrograms into a finite set of high-quality discrete representations for subsequent stages. (2) Frozen alignment, where brain signal embeddings are mapped to corresponding Mel spectrogram embeddings in a frozen latent space, effectively filtering session-specific noise through vector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score. (3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper model for audio-to-text translation, balancing signal adaptation with knowledge preservation, and achieving 74%-89% decoding BLEU scores without excessive reliance on teacher forcing. BrainECHO demonstrates robustness across sentence, session, and subject-independent conditions, passing Gaussian noise tests and showcasing its potential for enhancing language-based brain-computer interfaces.</abstract>
      <url hash="d59af90e">2025.findings-acl.142</url>
      <bibkey>li-etal-2025-brainecho</bibkey>
    </paper>
    <paper id="143">
      <title>Progressive <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> for Multimodal Continual Instruction Tuning</title>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Duzhen</first><last>Zhang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yong</first><last>Ren</last></author>
      <author><first>Xuanle</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiuyi</first><last>Chen</last><affiliation>Baidu</affiliation></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <pages>2779-2796</pages>
      <abstract>Multimodal Continual Instruction Tuning (MCIT) empowers Multimodal Large Language Models (MLLMs) to adapt to ever-evolving requirements without continuous costly retraining. However, MCIT faces challenges in mitigating Catastrophic Forgetting (CF) and enhancing Knowledge Transfer (KT). Existing works combine Mixture-of-Expert (MoE) and LoRA to address these. However, using a fixed number of shared LoRA blocks across tasks can lead to the overwriting of acquired knowledge, making MLLMs harder to handle CF and KT. Therefore, we propose the **Prog**ressive **LoRA** framework (ProgLoRA), which contains a progressive LoRA pool and trains a new LoRA block for each incremental task to reduce knowledge interference. Specifically, ProgLoRA has two key mechanisms: task-aware allocation for effectively leveraging acquired knowledge at current task and task recall for realigning the model with learned tasks. Additionally, considering different application scenarios, we design a static ProgLoRA for the more idealized basic setting and a dynamic ProgLoRA for the more realistic challenging setting. Experiments on the latest MCIT benchmark demonstrate that ProgLoRA outperforms existing approaches.</abstract>
      <url hash="b01d4baa">2025.findings-acl.143</url>
      <bibkey>yu-etal-2025-progressive</bibkey>
    </paper>
    <paper id="144">
      <title><fixed-case>ARC</fixed-case> ‘Challenge’ Is Not That Challenging</title>
      <author><first>Łukasz</first><last>Borchmann</last><affiliation>Snowflake</affiliation></author>
      <pages>2797-2804</pages>
      <abstract>ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.</abstract>
      <url hash="45902361">2025.findings-acl.144</url>
      <bibkey>borchmann-2025-arc</bibkey>
    </paper>
    <paper id="145">
      <title>Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual <fixed-case>LLM</fixed-case>s: An Extensive Investigation</title>
      <author><first>Vera</first><last>Neplenbroek</last></author>
      <author><first>Arianna</first><last>Bisazza</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>2805-2830</pages>
      <abstract>Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model’s bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model’s pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.</abstract>
      <url hash="a70982e0">2025.findings-acl.145</url>
      <bibkey>neplenbroek-etal-2025-cross</bibkey>
    </paper>
    <paper id="146">
      <title>Tracr-Injection: Distilling Algorithms into Pre-trained Language Models</title>
      <author><first>Tomás</first><last>Vergara Browne</last><affiliation>Pontificia Universidad Catolica de Chile</affiliation></author>
      <author><first>Alvaro</first><last>Soto</last></author>
      <pages>2831-2843</pages>
      <abstract>Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model’s residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out-of-distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.</abstract>
      <url hash="19d8da27">2025.findings-acl.146</url>
      <bibkey>vergara-browne-soto-2025-tracr</bibkey>
    </paper>
    <paper id="147">
      <title>Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</title>
      <author><first>Ximing</first><last>Dong</last></author>
      <author><first>Shaowei</first><last>Wang</last><affiliation>University of Manitoba</affiliation></author>
      <author><first>Dayi</first><last>Lin</last><affiliation>Huawei Technologies Canada Co., Ltd.</affiliation></author>
      <author><first>Ahmed</first><last>Hassan</last><affiliation>Queen’s University</affiliation></author>
      <pages>2844-2859</pages>
      <abstract>Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the major of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection approach for effective Prompt Optimization using real time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on two datasets BIG-bench and LIAR, and two models GPT-3.5 and GPT-4o-mini, show that IPOMP improves effectiveness by at least 1.6% to 3.1%, and stability by at least 50% to 55.5% compared with the best baseline across the studied datasets and models, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.</abstract>
      <url hash="8db2321e">2025.findings-acl.147</url>
      <bibkey>dong-etal-2025-model</bibkey>
    </paper>
    <paper id="148">
      <title>Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse <fixed-case>KL</fixed-case> vs. Forward <fixed-case>KL</fixed-case></title>
      <author><first>Wei</first><last>Yao</last></author>
      <author><first>Wenkai</first><last>Yang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ziqiao</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2860-2888</pages>
      <abstract>As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence—whose mass-covering behavior risks overfitting to imperfect weak signals—with reverse KL divergence. Reverse KL divergence’s zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last linear layer, reverse KL guarantees that it outperforms its weak supervisor by the magnitude of their disagreement. Empirically, we demonstrate that reverse KL and reverse cross-entropy not only enable strong models to outperform those trained with forward KL and standard cross-entropy across most settings, but also exhibit greater robustness to noisy labels.</abstract>
      <url hash="899c4031">2025.findings-acl.148</url>
      <bibkey>yao-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="149">
      <title>Stories that (are) Move(d by) Markets: A Causal Exploration of Market Shocks and Semantic Shifts across Different Partisan Groups</title>
      <author><first>Felix</first><last>Drinkall</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Stefan</first><last>Zohren</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Michael</first><last>McMahon</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Janet B.</first><last>Pierrehumbert</last><affiliation>University of Oxford</affiliation></author>
      <pages>2889-2904</pages>
      <abstract>Macroeconomic fluctuations and the narratives that shape them form a mutually reinforcing cycle: public discourse can spur behavioural changes leading to economic shifts, which then result in changes in the stories that propagate. We show that shifts in semantic embedding space can be causally linked to real-world market shocks or deviations from the expected market behaviour (sec:market_shocks). Furthermore, we show how partisanship can influence the predictive power of text for market fluctuations and shape reactions to those same shocks. We also provide some evidence that text-based signals are particularly salient during rare events such as COVID-19, highlighting the value of language data as an exogenous variable in economic forecasting. Our findings underscore the bidirectional relationship between news outlets and market shocks, offering a novel empirical approach to studying their effect on each other.</abstract>
      <url hash="d34b7a3d">2025.findings-acl.149</url>
      <bibkey>drinkall-etal-2025-stories</bibkey>
    </paper>
    <paper id="150">
      <title><fixed-case>N</fixed-case>et<fixed-case>S</fixed-case>afe: Exploring the Topological Safety of Multi-agent System</title>
      <author><first>Miao</first><last>Yu</last></author>
      <author><first>Shilong</first><last>Wang</last></author>
      <author><first>Guibin</first><last>Zhang</last></author>
      <author><first>Junyuan</first><last>Mao</last></author>
      <author><first>Chenlong</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Qijiong</first><last>Liu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Qingsong</first><last>Wen</last><affiliation>Squirrel Ai Learning</affiliation></author>
      <author><first>Yang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>2905-2938</pages>
      <abstract>Large language models (LLMs) have fueled significant progress in intelligent Multi-agent Systems (MAS), with expanding academic and industrial applications. However, safeguarding these systems from malicious queries receives relatively little attention, while methods for single-agent safety are challenging to transfer. In this paper, we explore MAS safety from a topological perspective, aiming at identifying structural properties that enhance security. To this end, we propose NetSafe framework, unifying diverse MAS workflows via iterative RelCom interactions to enable generalized analysis. We identify several critical phenomena for MAS under attacks (misinformation, bias, and harmful content), termed as <tex-math>\textit{Agent Hallucination}</tex-math>, <tex-math>\textit{Aggregation Safety}</tex-math> and <tex-math>\textit{Security Bottleneck}</tex-math>. Furthermore, we verify that highly connected and larger systems are more vulnerable to adversarial spread, with task performance in a Star Graph Topology decreasing by 29.7%. In conclusion, our work introduces a new perspective on MAS safety and discovers unreported phenomena, offering insights and posing challenges to the community.</abstract>
      <url hash="99c27437">2025.findings-acl.150</url>
      <bibkey>yu-etal-2025-netsafe</bibkey>
    </paper>
    <paper id="151">
      <title>Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation</title>
      <author><first>Qiji</first><last>Zhou</last></author>
      <author><first>YiFan</first><last>Gong</last></author>
      <author><first>Guangsheng</first><last>Bao</last><affiliation>Westlake University</affiliation></author>
      <author><first>Hongjie</first><last>Qiu</last></author>
      <author><first>Jinqiang</first><last>Li</last></author>
      <author><first>Xiangrong</first><last>Zhu</last></author>
      <author><first>Huajian</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>2939-2957</pages>
      <abstract>Counterfactual reasoning is crucial for robust video understanding but remains underexplored in existing multimodal benchmarks. In this paper, we introduce **COVER** (**CO**unterfactual **V**id**E**o **R**easoning), a multidimensional multimodal benchmark that systematically evaluates MLLMs across the abstract-concrete and perception-cognition dimensions. Beyond prior multimodal benchmarks, COVER decomposes complex queries into structured sub-questions, enabling fine-grained reasoning analysis. Experiments on commercial and open-source models reveal a strong correlation between sub-question accuracy and counterfactual reasoning performance, highlighting the role of structured inference in video understanding. Furthermore, our results suggest a key insight: enhancing the reasoning capability of models is essential for improving the robustness of video understanding. COVER establishes a new standard for assessing MLLMs’ logical reasoning abilities in dynamic environments. Our work is available at https://github.com/gongyifan-hash/COVER-Benchmark.</abstract>
      <url hash="8ee1f326">2025.findings-acl.151</url>
      <bibkey>zhou-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="152">
      <title>Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing</title>
      <author><first>Hanlun</first><last>Zhu</last></author>
      <author><first>Yunshi</first><last>Lan</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Weining</first><last>Qian</last></author>
      <pages>2958-2971</pages>
      <abstract>As the insight of knowledge storage in language models deepens, the ability to perform CRUD (Create, Read, Update, Delete) operations on language models becomes increasingly indispensable for satisfying the demands of managing rapidly updating knowledge. Considering the high cost of fine-tuning language models, model editing methods with low cost are usually required to manipulate models’ knowledge. The evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks, thus we propose iReVa, a method that explicitly initializes and retrofits key-value pairs into MLP blocks to construct a new mapping of a piece of knowledge without damaging the irrelevant knowledge. In comparison to existing methods, iReVa reveals better interpretability and a stronger capacity for carrying traceable edits. Experiment results on a series of GPT series models show our prominent performance on edit success and generalization without influencing specificity. We also made the first attempt to conduct a knowledge withdrawal test of iReVa. Our codes are available at https://github.com/timberflow/iReVa.</abstract>
      <url hash="e23d1a6e">2025.findings-acl.152</url>
      <bibkey>zhu-etal-2025-initializing</bibkey>
    </paper>
    <paper id="153">
      <title>Know the Unknown: An Uncertainty-Sensitive Method for <fixed-case>LLM</fixed-case> Instruction Tuning</title>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Yixuan</first><last>Tang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>2972-2989</pages>
      <abstract>Large language models (LLMs) demonstrate remarkable capabilities but face challenges from hallucinations, which typically arise from insufficient knowledge or context. While instructing LLMs to acknowledge knowledge limitations by responding with “I don’t know” appears promising, we find that models consistently struggle with admitting knowledge gaps. This challenge may originate from current instruction datasets that emphasise answer generation over knowledge boundary awareness. To address this limitation, we introduce **U**ncertainty-and-**S**ensitivity-Aware Tuning **(US-Tuning)**, a novel two-stage approach for contextual question answering (QA). The first stage enhances LLMs’ ability to recognise their knowledge boundaries, while the second stage reinforces instruction adherence through carefully designed causal prompts. Our experimental results demonstrate that US-Tuning not only significantly reduces incorrect answers in contextual QA but also improves models’ faithfulness to their parametric knowledge, mitigating hallucinations in general QA tasks. Our fine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling out-of-knowledge questions and outperforms GPT-4 by 4.2% in overall performance.</abstract>
      <url hash="3a7bae2e">2025.findings-acl.153</url>
      <bibkey>li-etal-2025-know</bibkey>
    </paper>
    <paper id="154">
      <title>Position-Aware Depth Decay Decoding (<tex-math>D^3</tex-math>): Boosting Large Language Model Inference Efficiency</title>
      <author><first>Siqi</first><last>Fan</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xuezhi</first><last>Fang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Xingrun</first><last>Xing</last></author>
      <author><first>Peng</first><last>Han</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <pages>2990-3001</pages>
      <abstract>Due to the large number of parameters, the inference phase of Large Language Models (LLMs) is resource-intensive. Unlike traditional model compression, which needs retraining, recent dynamic computation methods show that not all components are required for inference, enabling a training-free pipeline.In this paper, we focus on the dynamic depth of LLM generation. A token-position aware layer skipping framework is proposed to save 1.5x times operations efficiently while maintaining performance.We first observed that tokens predicted later have lower perplexity and thus require less computation. Then, we propose a training-free algorithm called Position-Aware <b>D</b>epth <b>D</b>ecay <b>D</b>ecoding (), which leverages a power-law decay function, <tex-math>\left\lfloor L \times (\alpha^i) \right\rfloor</tex-math>, to determine the number of layers to retain when generating token <tex-math>T_i</tex-math>. Remarkably, without any retraining, the achieves success across a wide range of generation tasks for the first time.Experiments on large language models (the Llama) with <tex-math>7 \sim 70</tex-math> billion parameters show that can achieve an average 1.5x speedup compared with the full-inference pipeline while maintaining comparable performance with nearly no performance drop (<tex-math>&lt;1\%</tex-math>) on the GSM8K and BBH benchmarks.</abstract>
      <url hash="8c005b32">2025.findings-acl.154</url>
      <bibkey>fan-etal-2025-position</bibkey>
    </paper>
    <paper id="155">
      <title>Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku</title>
      <author><first>Anirudh</first><last>Maiya</last></author>
      <author><first>Razan</first><last>Alghamdi</last><affiliation>King Saud University</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Ashutosh</first><last>Trivedi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Fabio</first><last>Somenzi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>3002-3009</pages>
      <abstract>The success of Large Language Models (LLMs) in human-AI collaborative decision-making hinges on their ability to provide trustworthy, gradual, and tailored explanations. Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution. In this study, we evaluate the performance of five LLMs in solving and explaining 6x6 Sudoku puzzles. While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problem-solving. These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.</abstract>
      <url hash="eb268219">2025.findings-acl.155</url>
      <bibkey>maiya-etal-2025-explaining</bibkey>
    </paper>
    <paper id="156">
      <title>Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors</title>
      <author><first>Andrea</first><last>Pedrotti</last></author>
      <author><first>Michele</first><last>Papucci</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Cristiano</first><last>Ciaccio</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Alessio</first><last>Miaschi</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Giovanni</first><last>Puccetti</last><affiliation>CNR</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <author><first>Andrea</first><last>Esuli</last><affiliation>CNR</affiliation></author>
      <pages>3010-3031</pages>
      <abstract>Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we evaluate the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. We develop a pipeline that fine-tunes language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT), obtaining generations more challenging to detect by current models. Additionally, we analyze the linguistic shifts induced by the alignment and how detectors rely on “linguistic shortcuts” to detect texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detecting performances. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. We release code, models, and data to support future research on more robust MGT detection benchmarks.</abstract>
      <url hash="644dff47">2025.findings-acl.156</url>
      <bibkey>pedrotti-etal-2025-stress</bibkey>
    </paper>
    <paper id="157">
      <title><fixed-case>I</fixed-case>nfini<fixed-case>SST</fixed-case>: Simultaneous Translation of Unbounded Speech with Large Language Model</title>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Xi</first><last>Xu</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>3032-3046</pages>
      <abstract>Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the historical speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. Code is released at https://github.com/LeiLiLab/InfiniSST.</abstract>
      <url hash="098c0c79">2025.findings-acl.157</url>
      <bibkey>ouyang-etal-2025-infinisst</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>VSCB</fixed-case>ench: Bridging the Gap in Vision-Language Model Safety Calibration</title>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zongxiong</first><last>Chen</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Derui</first><last>Zhu</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Zhuohan</first><last>Xie</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fakhri</first><last>Karray</last><affiliation>University of Waterloo and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>3047-3059</pages>
      <abstract>The rapid advancement of vision-language models (VLMs) has brought a lot of attention to their safety alignment. However, existing methods have primarily focused on model undersafety, where the model responds to hazardous queries, while neglecting oversafety, where the model refuses to answer safe queries. In this paper, we introduce the concept of safety calibration, which systematically addresses both undersafety and oversafety. Specifically, we present VSCBench, a novel dataset of 3,600 image-text pairs that are visually or textually similar but differ in terms of safety, which is designed to evaluate safety calibration across image-centric and text-centric scenarios. Based on our benchmark, we evaluate safety calibration across eleven widely used VLMs. Our extensive experiments revealed major issues with both undersafety and oversafety. We further investigated four approaches to improve the model’s safety calibration. We found that even though some methods effectively calibrated the models’ safety problems, these methods also lead to the degradation of models’ utility. This trade-off underscores the urgent need for advanced calibration methods, and our benchmark provides a valuable tool for evaluating future approaches.</abstract>
      <url hash="b0265710">2025.findings-acl.158</url>
      <bibkey>geng-etal-2025-vscbench</bibkey>
    </paper>
    <paper id="159">
      <title>To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization</title>
      <author><first>Haozhe</first><last>Wang</last><affiliation>INF</affiliation></author>
      <author><first>Long</first><last>Li</last></author>
      <author><first>Chao</first><last>Qu</last><affiliation>Inftech</affiliation></author>
      <author><first>Weidi</first><last>Xu</last><affiliation>Infly Technology</affiliation></author>
      <author><first>Fengming</first><last>Zhu</last></author>
      <author><first>Wei</first><last>Chu</last><affiliation>Inf Tech</affiliation></author>
      <author><first>Fangzhen</first><last>Lin</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <pages>3060-3075</pages>
      <abstract>Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness—the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training.While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.</abstract>
      <url hash="db9d0c42">2025.findings-acl.159</url>
      <bibkey>wang-etal-2025-code</bibkey>
    </paper>
    <paper id="160">
      <title><fixed-case>GOODLIAR</fixed-case>: A Reinforcement Learning-Based Deceptive Agent for Disrupting <fixed-case>LLM</fixed-case> Beliefs on Foundational Principles</title>
      <author><first>Soo Kyung</first><last>Kim</last><affiliation>Ewha Women’s University and Palo Alto Research Center</affiliation></author>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Ewha Women’s University</affiliation></author>
      <pages>3076-3101</pages>
      <abstract>Large Language Models (LLMs) often succumb to adversarial prompts, a phenomenon popularly known as “jailbreaking.” While jailbreaking primarily targets short-term noncompliance with predefined policies, we argue that a deeper vulnerability lies in altering an LLM’s <i>fundamental axiomatic beliefs</i>, such as mathematical or philosophical truths. In this work, we introduce GoodLiar, a reinforcement learning (RL)-based framework that generates deceptive contexts to systematically <i>rewrite</i> an LLM’s core logical or philosophical understandings. By incentivizing an RL agent to produce persuasive and coherent arguments, GoodLiar aims to induce <i>persistent</i> belief shifts, rather than merely influencing immediate judgments of factual truthfulness. %rather than one-off policy breaches. Our approach introduces <i>DA-ILQL</i>, a novel offline RL method that extends ILQL by integrating on-policy data and language exploration to enhance the language discovery and optimization. Through extensive evaluations on multiple LLMs, we show that deceptive contexts discovered by GoodLiar consistently outperform simple multi-turn prompting methods.</abstract>
      <url hash="b8683be8">2025.findings-acl.160</url>
      <bibkey>kim-cho-2025-goodliar</bibkey>
    </paper>
    <paper id="161">
      <title>How Does Response Length Affect Long-Form Factuality</title>
      <author><first>James Xu</first><last>Zhao</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Jimmy Z.j.</first><last>Liu</last></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>3102-3125</pages>
      <abstract>Large language models (LLMs) are widely used for long-form text generation. However, factual errors in the responses would undermine their reliability. Despite growing attention to LLM factuality, the effect of response length on factuality remains underexplored. In this work, we systematically investigate this relationship by first introducing an automatic and bi-level long-form factuality evaluation framework, which achieves high agreement with human annotations while being cost-effective. Using this framework, we conduct controlled experiments and find that longer responses exhibit lower factual precision, confirming the presence of length bias. To explain this phenomenon, we empirically examine three hypotheses: error propagation, long context, and facts exhaustion. Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses.</abstract>
      <url hash="3ffbad76">2025.findings-acl.161</url>
      <bibkey>zhao-etal-2025-response</bibkey>
    </paper>
    <paper id="162">
      <title>Scaling <fixed-case>LLM</fixed-case>s’ Social Reasoning: Sprinkle Cognitive “Aha Moment” into Fundamental Long-thought Logical Capabilities</title>
      <author><first>Guiyang</first><last>Hou</last><affiliation>Alibaba Group and Wuhan University</affiliation></author>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Zhe</first><last>Zheng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yongliang</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3126-3138</pages>
      <abstract>Humans continually engage in reasoning about others’ mental states, a capability known as Theory of Mind (ToM), is essential for social interactions. While this social reasoning capability emerges naturally in human cognitive development, how has the social reasoning capability of Large Language Models (LLMs) evolved during their development process? Various datasets have been proposed to assess LLMs’ social reasoning capabilities, but each is designed with a distinct focus, and none have explored how models’ social reasoning capabilities evolve during model size scaling or reasoning tokens scaling. In light of this, we optimize the evaluation of LLMs’ social reasoning from both data and model perspectives, constructing progressively difficult levels of social reasoning data and systematically exploring how LLMs’ social reasoning capabilities evolve. Furthermore, through an in-depth analysis of DeepSeek-R1’s reasoning trajectories, we identify notable cognitive “Aha Moment” and the reasons for its reasoning errors. Experiments reveal that long-thought logical capabilities and cognitive thinking are key to scaling LLMs’ social reasoning capabilities. By equipping the Qwen2.5-32B-Instruct model with long-thought logical capabilities and cognitive thinking, we achieve an improvement of 19.0 points, attaining social reasoning performance comparable to o1-preview model.</abstract>
      <url hash="8714cbdb">2025.findings-acl.162</url>
      <bibkey>hou-etal-2025-scaling</bibkey>
    </paper>
    <paper id="163">
      <title><fixed-case>S</fixed-case>im<fixed-case>GRAG</fixed-case>: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation</title>
      <author><first>Yuzheng</first><last>Cai</last></author>
      <author><first>Zhenyue</first><last>Guo</last></author>
      <author><first>YiWen</first><last>Pei</last><affiliation>Fudan University</affiliation></author>
      <author><first>WanRui</first><last>Bian</last></author>
      <author><first>Weiguo</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <pages>3139-3158</pages>
      <abstract>Recent advancements in large language models (LLMs) have shown impressive versatility across various tasks. To eliminate their hallucinations, retrieval-augmented generation (RAG) has emerged as a powerful approach, leveraging external knowledge sources like knowledge graphs (KGs). In this paper, we study the task of KG-driven RAG and propose a novel Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively addresses the challenge of aligning query texts and KG structures through a two-stage process: (1) query-to-pattern, which uses an LLM to transform queries into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the alignment between the pattern and candidate subgraphs using a graph semantic distance (GSD) metric. We also develop an optimized retrieval algorithm that efficiently identifies the top-k subgraphs within 1-second on a 10-million-scale KG. Extensive experiments show that SimGRAG outperforms state-of-the-art KG-driven RAG methods in both question answering and fact verification. Our code is available at https://github.com/YZ-Cai/SimGRAG.</abstract>
      <url hash="acf84a23">2025.findings-acl.163</url>
      <bibkey>cai-etal-2025-simgrag</bibkey>
    </paper>
    <paper id="164">
      <title><fixed-case>R</fixed-case>ule<fixed-case>E</fixed-case>dit: Towards Rule-Level Knowledge Generalization to Mitigate Over-Editing in Large Language Models</title>
      <author><first>Bihan</first><last>Zhou</last></author>
      <author><first>HaoPeng</first><last>Ren</last></author>
      <author><first>Li</first><last>Yuan</last></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Liuwen</first><last>Cao</last></author>
      <author><first>Zikun</first><last>Deng</last><affiliation>South China University of Technology</affiliation></author>
      <pages>3159-3175</pages>
      <abstract>Knowledge editing emerges as a promising approach for updating target knowledge in Large Language Models (LLMs) in a timely manner, thereby preventing undesirable behaviors stemming from outdated, inaccurate, or incomplete knowledge. However, existing methods mainly focus on instance-level editing, which is prone to over-editing risk featuring knowledge degradation and general ability deterioration, due to redundant instance-specific modifications for knowledge. To mitigate the over-editing risk, we explore the rule-level editing problem that avoids case-by-case modification by generalizing rule-level knowledge to update rule-derived instances. We further construct a benchmark called <tex-math>RuleEdit</tex-math> for systematic evaluation on rule-level editing. Moreover, we propose a Rule-Transfer Editing (RTE) method to facilitate effective updates and generalizations of rule-level knowledge in LLMs. Experimental results highlight our significant improvements, with the enhancements of 28.1% in portability and 8.1% in average performance over the best-performing baselines for LLaMA-2-7B on <tex-math>RULE_{mix}</tex-math>.</abstract>
      <url hash="3851f455">2025.findings-acl.164</url>
      <bibkey>zhou-etal-2025-ruleedit</bibkey>
    </paper>
    <paper id="165">
      <title>Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Varun R.</first><last>Embar</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Navdeep</first><last>Jaitly</last><affiliation>Apple</affiliation></author>
      <author><first>Shay B</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Benjamin</first><last>Han</last><affiliation>Apple</affiliation></author>
      <pages>3176-3192</pages>
      <abstract>Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly – a capability we define as In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.</abstract>
      <url hash="e569513d">2025.findings-acl.165</url>
      <bibkey>qiu-etal-2025-eliciting</bibkey>
    </paper>
    <paper id="166">
      <title><fixed-case>G</fixed-case>e<fixed-case>AR</fixed-case>: Generation Augmented Retrieval</title>
      <author><first>Haoyu</first><last>Liu</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Jianfeng</first><last>Liu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuefeng</first><last>Zhan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Weiwei</first><last>Deng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Feng</first><last>Sun</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>3193-3207</pages>
      <abstract>Document retrieval techniques are essential for developing large-scale information systems. The common approach involves using a bi-encoder to compute the semantic similarity between a query and documents. However, the scalar similarity often fail to reflect enough information, hindering the interpretation of retrieval results. In addition, this process primarily focuses on global semantics, overlooking the finer-grained semantic relationships between the query and the document’s content. In this paper, we introduce a novel method, <tex-math>\textbf{Ge}</tex-math>neration <tex-math>\textbf{A}</tex-math>ugmented <tex-math>\textbf{R}</tex-math>etrieval (<tex-math>\textbf{GeAR}</tex-math>), which not only improves the global document-query similarity through contrastive learning, but also integrates well-designed fusion and decoding modules. This enables GeAR to generate relevant context within the documents based on a given query, facilitating learning to retrieve local fine-grained information.Furthermore, when used as a retriever, GeAR does not incur any additional computational cost over bi-encoders. GeAR exhibits competitive retrieval performance across diverse scenarios and tasks. Moreover, qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released at https://github.com/microsoft/LMOps.</abstract>
      <url hash="1e38471b">2025.findings-acl.166</url>
      <bibkey>liu-etal-2025-gear</bibkey>
    </paper>
    <paper id="167">
      <title>A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion</title>
      <author><first>Yanzhen</first><last>Shen</last></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Yunyi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>3208-3220</pages>
      <abstract>Entity set expansion, taxonomy expansion, and seed-guided taxonomy construction are three representative tasks that can be applied to automatically populate an existing taxonomy with emerging concepts. Previous studies view them as three separate tasks. Therefore, their proposed techniques usually work for one specific task only, lacking generalizability and a holistic perspective. In this paper, we aim at a unified solution to the three tasks. To be specific, we identify two common skills needed for entity set expansion, taxonomy expansion, and seed-guided taxonomy construction: finding “siblings” and finding “parents”. We propose a taxonomy-guided instruction tuning framework to teach a large language model to generate siblings and parents for query entities, where the joint pre-training process facilitates the mutual enhancement of the two skills. Extensive experiments on multiple benchmark datasets demonstrate the efficacy of our proposed TaxoInstruct framework, which outperforms task-specific baselines across all three tasks.</abstract>
      <url hash="2b1154c6">2025.findings-acl.167</url>
      <bibkey>shen-etal-2025-unified</bibkey>
    </paper>
    <paper id="168">
      <title>Zero-Shot Conversational Stance Detection: Dataset and Approaches</title>
      <author><first>Yuzhe</first><last>Ding</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Kang</first><last>He</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bobo</first><last>Li</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Li</first><last>Zheng</last></author>
      <author><first>Haijun</first><last>He</last></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>3221-3235</pages>
      <abstract>Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the increasing number of online debates among social media users, conversational stance detection has become a crucial research area. However, existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications. To bridge this gap, we manually curate a large-scale, high-quality zero-shot conversational stance detection dataset, named ZS-CSD, comprising 280 targets across two distinct target types. Leveraging the ZS-CSD dataset, we propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model, and establish the benchmark performance in the zero-shot setting. Experimental results demonstrate that our proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%, highlighting the persistent challenges in zero-shot conversational stance detection.</abstract>
      <url hash="9c1c6de6">2025.findings-acl.168</url>
      <bibkey>ding-etal-2025-zero</bibkey>
    </paper>
    <paper id="169">
      <title><fixed-case>L</fixed-case>ong<fixed-case>F</fixed-case>aith: Enhancing Long-Context Reasoning in <fixed-case>LLM</fixed-case>s with Faithful Synthetic Data</title>
      <author><first>Cehao</first><last>Yang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xueyuan</first><last>Lin</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chengjin</first><last>Xu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Xuhui</first><last>Jiang</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Shengjie</first><last>Ma</last></author>
      <author><first>Aofan</first><last>Liu</last></author>
      <author><first>Hui</first><last>Xiong</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <pages>3236-3256</pages>
      <abstract>Despite the growing development of long-context large language models (LLMs), data-centric approaches relying on synthetic data have been hindered by issues related to faithfulness, which limit their effectiveness in enhancing model performance on tasks such as long-context reasoning and question answering (QA). These challenges are often exacerbated by misinformation caused by lack of verification, reasoning without attribution, and potential knowledge conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. By integrating ground truth and citation-based reasoning prompts, we eliminate distractions and improve the accuracy of reasoning chains, thus mitigating the need for costly verification processes. We open-source two synthesized datasets—LongFaith-SFT and LongFaith-PO—which systematically address multiple dimensions of faithfulness, including verified reasoning, attribution, and contextual grounding. Extensive experiments on multi-hop reasoning datasets and LongBench demonstrate that models fine-tuned on these datasets significantly improve performance. Our ablation studies highlight the scalability and adaptability of the LongFaith pipeline, showcasing its broad applicability in developing long-context LLMs.</abstract>
      <url hash="37a72f6b">2025.findings-acl.169</url>
      <bibkey>yang-etal-2025-longfaith</bibkey>
    </paper>
    <paper id="170">
      <title><fixed-case>SYNTHVERIFY</fixed-case>: Enhancing Zero-Shot Claim Verification through Step-by-Step Synthetic Data Generation</title>
      <author><first>Rongwen</first><last>Zhao</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Jeffrey</first><last>Flanigan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>3257-3274</pages>
      <abstract>Claim verification is a fundamental task in natural language processing (NLP), involving the assessment of whether available evidence supports or refutes a given claim. While large language models (LLMs) have shown promise in this area, they continue to struggle with domain-specific knowledge. Synthetic data generation has emerged as an effective solution to this challenge. However, existing methods are often either inefficient to scale across multiple domains or overly reliant on external documents. We introduce SYNTHVERIFY, a novel step-by-step prompting-based synthetic data generation framework designed to enhance zero-shot claim verification. Our core insight is that guiding generation with domain-specific claim patterns and structured evidence plans can bridge LLMs’ knowledge gaps in specialized domains without requiring access to external corpora or sacrificing generalizability. Using SYNTHVERIFY, we construct a diverse synthetic dataset for zero-shot verification, enabling instruction fine-tuning tailored to the verification task. Empirical results across multiple specialized domains demonstrate significant accuracy improvements, including a 20.1-point gain on the Llama-3-8B model. Our results highlight the effectiveness of structured synthetic data generation in addressing the limitations of verification systems, particularly in domain-specific tasks.</abstract>
      <url hash="5d0a88ad">2025.findings-acl.170</url>
      <bibkey>zhao-flanigan-2025-synthverify</bibkey>
    </paper>
    <paper id="171">
      <title>Domain<tex-math>o1</tex-math>s: Guiding <fixed-case>LLM</fixed-case> Reasoning for Explainable Answers in High-Stakes Domains</title>
      <author><first>Xu</first><last>Chu</last></author>
      <author><first>Zhijie</first><last>Tan</last><affiliation>Peking University</affiliation></author>
      <author><first>Hanlin</first><last>Xue</last></author>
      <author><first>Guanyu</first><last>Wang</last></author>
      <author><first>Tong</first><last>Mo</last></author>
      <author><first>Weiping</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>3275-3293</pages>
      <abstract>Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users’ confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain<tex-math>o1</tex-math>s, which enhances LLMs’ reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models’ explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domain<tex-math>o1</tex-math>s’s leading performance and explainability. Our code is available at <url>https://anonymous.4open.science/r/Domaino1s-006F/</url>.</abstract>
      <url hash="27808d3c">2025.findings-acl.171</url>
      <bibkey>chu-etal-2025-domaino1s</bibkey>
    </paper>
    <paper id="172">
      <title>Dynamic Prefix as Instructor for Incremental Named Entity Recognition: A Unified <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Generation Framework</title>
      <author><first>Zihao</first><last>Wu</last></author>
      <author><first>YongXiang</first><last>Hua</last></author>
      <author><first>Yongxin</first><last>Zhu</last></author>
      <author><first>Fang</first><last>Zhang</last></author>
      <author><first>Linli</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3294-3306</pages>
      <abstract>The Incremental Named Entity Recognition (INER) task aims to update a model to extract entities from an expanding set of entity type candidates due to concerns related to data privacy and scarcity. However, conventional sequence labeling approaches to INER often suffer from the catastrophic forgetting problem, which leads to the degradation of the model’s performance on previously encountered entity types. In this paper, we formalize INER as a unified seq2seq generation task and propose a parameter-efficient dynamic prefix method. By employing the dynamic prefix as a task instructor to guide the generative model, our approach can preserve task-invariant knowledge while adapting to new entities with minimal parameter updates, making it particularly effective in low-resource scenarios. Additionally, we introduce a generative label augmentation strategy with dual optimization objectives including a self-entropy loss and a task-aware similarity loss to enable optimal balance between stability and plasticity. Empirical experiments on NER benchmarks demonstrate the effectiveness of our proposed method in addressing the challenges associated with INER.</abstract>
      <url hash="0c8a0796">2025.findings-acl.172</url>
      <bibkey>wu-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="173">
      <title>Who Taught You That? Tracing Teachers in Model Distillation</title>
      <author><first>Somin</first><last>Wadhwa</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Chantal</first><last>Shaib</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Silvio</first><last>Amir</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Byron C</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <pages>3307-3315</pages>
      <abstract>Model distillation – using outputs from a large teacher model to teach a small student model – is a practical means of creating efficient models for a particular task. We ask: Can we identify a students’ teacher based on its outputs? Such “footprints” left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that n-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.</abstract>
      <url hash="aa2ad67d">2025.findings-acl.173</url>
      <bibkey>wadhwa-etal-2025-taught</bibkey>
    </paper>
    <paper id="174">
      <title><i>
          <fixed-case>D</fixed-case>-<fixed-case>GEN</fixed-case></i>: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Models</title>
      <author><first>Grace</first><last>Byun</last><affiliation>Emory University</affiliation></author>
      <author><first>Jinho D.</first><last>Choi</last><affiliation>Emory University</affiliation></author>
      <pages>3316-3349</pages>
      <abstract>Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce <i>D-GEN</i>, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: 1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and 2) entropy analysis, comparing model confidence distributions. Our results show that <i>D-GEN</i> preserves ranking consistency (Spearman’s <tex-math>\rho</tex-math> 0.99, Kendall’s <tex-math>\tau</tex-math> 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.</abstract>
      <url hash="02f8f6f3">2025.findings-acl.174</url>
      <bibkey>byun-choi-2025-gen</bibkey>
    </paper>
    <paper id="175">
      <title><fixed-case>H</fixed-case>ammer<fixed-case>B</fixed-case>ench: Fine-Grained Function-Calling Evaluation in Real Mobile Assistant Scenarios</title>
      <author><first>Jun</first><last>Wang</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Jiamu</first><last>Zhou</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Xihuai</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xiaoyun</first><last>Mo</last><affiliation>Oppo</affiliation></author>
      <author><first>Haoyu</first><last>Zhang</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Qiqiang</first><last>Lin</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Jincheng</first><last>Jincheng</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Muning</first><last>Wen</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Qiuying</first><last>Peng</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>OPPO Research Institute</affiliation></author>
      <pages>3350-3376</pages>
      <abstract>Evaluating the performance of LLMs in multi-turn human-agent interactions presents significant challenges, particularly due to the complexity and variability of user behavior. In this paper, we introduce HammerBench, a novel benchmark framework for assessing LLMs’ function-calling capabilities in real-world, multi-turn dialogues. HammerBench simulates diverse mobile assistant use cases, incorporating imperfect instructions, dynamic question-answer trajectories, intent and argument shifts, and the indirect use of external information through pronouns. To construct this benchmark, we curate a comprehensive dataset derived from popular mobile app functionalities and anonymized user logs, complemented by a cost-effective data generation pipeline leveraging open-source models. HammerBench is further augmented with fine-grained interaction snapshots and metrics, enabling detailed evaluation of function-calling performance across individual conversational turns. We demonstrate the effectiveness of HammerBench by evaluating several leading LLMs and uncovering key performance trends. Our experiments reveal that different types of parameter name errors are a significant source of failure across different interaction scenarios, highlighting critical areas for further improvement in LLM robustness for mobile assistant applications.</abstract>
      <url hash="6404d50b">2025.findings-acl.175</url>
      <bibkey>wang-etal-2025-hammerbench</bibkey>
    </paper>
    <paper id="176">
      <title>Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines</title>
      <author><first>Do Xuan</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Duong Ngoc</first><last>Yen</last></author>
      <author><first>Do Xuan</first><last>Trong</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <pages>3377-3411</pages>
      <abstract>In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task’s language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.</abstract>
      <url hash="1f23a655">2025.findings-acl.176</url>
      <bibkey>long-etal-2025-beyond</bibkey>
    </paper>
    <paper id="177">
      <title><fixed-case>GRAMMAR</fixed-case>-<fixed-case>LLM</fixed-case>: Grammar-Constrained Natural Language Generation</title>
      <author><first>Gabriele</first><last>Tuccio</last></author>
      <author><first>Luana</first><last>Bulla</last></author>
      <author><first>Maria</first><last>Madonia</last><affiliation>Università degli studi di Catania</affiliation></author>
      <author><first>Aldo</first><last>Gangemi</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Misael</first><last>Mongiovì</last><affiliation>University of Catania</affiliation></author>
      <pages>3412-3422</pages>
      <abstract>Large Language Models have achieved impressive performance across various natural language generation tasks. However, their lack of a reliable control mechanism limits their effectiveness in applications that require strict adherence to predefined taxonomies, syntactic structures, or domain-specific rules. Existing approaches, such as fine-tuning and prompting, remain insufficient to ensure compliance with these requirements, particularly in low-resource scenarios and structured text generation tasks.To address these limitations, we introduce GRAMMAR-LLM, a novel framework that integrates formal grammatical constraints into the LLM decoding process. GRAMMAR-LLM enforces syntactic correctness in linear time while maintaining expressiveness in grammar rule definition. To achieve this, we define a class of grammars, called LL(prefix), – which we show to be equivalent to LL(1) – specifically designed for their use with LLMs. These grammars are expressive enough to support common tasks such as hierarchical classification, vocabulary restriction, and structured parsing. We formally prove that LL(prefix) grammars can be transformed into LL(1) grammars in linear time, ensuring efficient processing via deterministic pushdown automata. We evaluate GRAMMAR-LLM across diverse NLP tasks, including hierarchical classification, sign language translation, and semantic parsing. Our experiments, conducted on models such as LLaMA 3 (for classification and translation) and AMRBART (for parsing), demonstrate that GRAMMAR-LLM consistently improves task performance across zero-shot, few-shot, and fine-tuned settings.</abstract>
      <url hash="9fd8d431">2025.findings-acl.177</url>
      <bibkey>tuccio-etal-2025-grammar</bibkey>
    </paper>
    <paper id="178">
      <title><fixed-case>MANB</fixed-case>ench: Is Your Multimodal Model Smarter than Human?</title>
      <author><first>Han</first><last>Zhou</last></author>
      <author><first>Qitong</first><last>Xu</last></author>
      <author><first>Yiheng</first><last>Dong</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Yang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>3423-3449</pages>
      <abstract>The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework.Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench/.</abstract>
      <url hash="b29a60e0">2025.findings-acl.178</url>
      <bibkey>zhou-etal-2025-manbench</bibkey>
    </paper>
    <paper id="179">
      <title><fixed-case>B</fixed-case>an<fixed-case>S</fixed-case>tereo<fixed-case>S</fixed-case>et: A Dataset to Measure Stereotypical Social Biases in <fixed-case>LLM</fixed-case>s for <fixed-case>B</fixed-case>angla</title>
      <author><first>Mahammed</first><last>Kamruzzaman</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Abdullah Al</first><last>Monsur</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Shrabon Kumar</first><last>Das</last></author>
      <author><first>Enamul</first><last>Hassan</last></author>
      <author><first>Gene Louis</first><last>Kim</last><affiliation>University of South Florida</affiliation></author>
      <pages>3450-3460</pages>
      <abstract>This study presents ***BanStereoSet***, a dataset designed to evaluate stereotypical social biases in multilingual LLMs for the Bangla language. In an effort to extend the focus of bias research beyond English-centric datasets, we have localized the content from the StereoSet, IndiBias, and kamruzzaman-etal’s datasets, producing a resource tailored to capture biases prevalent within the Bangla-speaking community. Our BanStereoSet dataset consists of 1,194 sentences spanning 9 categories of bias: race, profession, gender, ageism, beauty, beauty in profession, region, caste, and religion. This dataset not only serves as a crucial tool for measuring bias in multilingual LLMs but also facilitates the exploration of stereotypical bias across different social categories, potentially guiding the development of more equitable language technologies in *Bangladeshi* contexts. Our analysis of several language models using this dataset indicates significant biases, reinforcing the necessity for culturally and linguistically adapted datasets to develop more equitable language technologies.</abstract>
      <url hash="c987c3ec">2025.findings-acl.179</url>
      <bibkey>kamruzzaman-etal-2025-banstereoset</bibkey>
    </paper>
    <paper id="180">
      <title>m<fixed-case>OSCAR</fixed-case>: A Large-scale Multilingual and Multimodal Document-level Corpus</title>
      <author><first>Matthieu</first><last>Futeral</last></author>
      <author><first>Armel Randy</first><last>Zebaze</last><affiliation>INRIA</affiliation></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last><affiliation>Common Crawl Foundation</affiliation></author>
      <author><first>Julien</first><last>Abadji</last><affiliation>INRIA</affiliation></author>
      <author><first>Rémi</first><last>Lacroix</last><affiliation>Institut du développement et des ressources en informatique scientifique (IDRIS)</affiliation></author>
      <author><first>Cordelia</first><last>Schmid</last><affiliation>Google, INRIA and Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <pages>3461-3494</pages>
      <abstract>Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. (2022) showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model trained on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs. The dataset will be made publicly accessible under the Creative Commons CC BY 4.0 license.</abstract>
      <url hash="fcf43622">2025.findings-acl.180</url>
      <bibkey>futeral-etal-2025-moscar</bibkey>
    </paper>
    <paper id="181">
      <title><fixed-case>N</fixed-case>or<fixed-case>E</fixed-case>val: A <fixed-case>N</fixed-case>orwegian Language Understanding and Generation Evaluation Benchmark</title>
      <author><first>Vladislav</first><last>Mikhailov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Tita</first><last>Enstad</last><affiliation>National Library of Norway</affiliation></author>
      <author><first>David</first><last>Samuel</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Hans Christian</first><last>Farsethås</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept. of Informatics, University of Oslo</affiliation></author>
      <pages>3495-3541</pages>
      <abstract>This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets – of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokmål and Nynorsk. All our datasets and a collection of over 100 human-created prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pretrained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available.</abstract>
      <url hash="a1cfbfdd">2025.findings-acl.181</url>
      <bibkey>mikhailov-etal-2025-noreval</bibkey>
    </paper>
    <paper id="182">
      <title>Massively Multilingual Instruction-Following Information Extraction</title>
      <author><first>Thang</first><last>Le</last></author>
      <author><first>Huy Huu</first><last>Nguyen</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>3542-3585</pages>
      <abstract>The literature on information extraction (IE) has mostly centered around a selected few languages, hindering their applications on multilingual corpora. In this work, we introduce MASSIE - a comprehensive collection for instruction-following multilingual IE that standardizes and unifies 215 manually annotated datasets, covering 96 typologically diverse languages from 18 language families. Based on MASSIE, we conduct empirical studies on few-shot in-context learning and report important factors that either positively or negatively affect LLMs’ performance in multilingual IE, covering 21 LLMs sizing from 0.5B to 72B. Additionally, we introduce LF1 - a structure-aware metric that captures partially matched spans, resolving the conservativeness of standard exact matching scheme which overpenalizes LLMs’ predictions. Overall, our results signify that multilingual IE remains very challenging for existing LLMs, especially on complex tasks involving relations and events. In addition, performance gap is extremely large among high- and low-performing languages, but the group of similar-performing languages largely overlap between different LLMs, suggesting a shared performance bias in current LLMs.</abstract>
      <url hash="664b4509">2025.findings-acl.182</url>
      <bibkey>le-etal-2025-massively</bibkey>
    </paper>
    <paper id="183">
      <title><fixed-case>DALR</fixed-case>: Dual-level Alignment Learning for Multimodal Sentence Representation Learning</title>
      <author><first>Kang</first><last>He</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Yuzhe</first><last>Ding</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>3586-3601</pages>
      <abstract>Previous multimodal sentence representation learning methods have achieved impressive performance. However, most approaches focus on aligning images and text at a coarse level, facing two critical challenges: cross-modal misalignment bias and intra-modal semantic divergence, which significantly degrade sentence representation quality. To address these challenges, we propose DALR (Dual-level Alignment Learning for Multimodal Sentence Representation). For cross-modal alignment, we propose a consistency learning module that softens negative samples and utilizes semantic similarity from an auxiliary task to achieve fine-grained cross-modal alignment. Additionally, we contend that sentence relationships go beyond binary positive-negative labels, exhibiting a more intricate ranking structure. To better capture these relationships and enhance representation quality, we integrate ranking distillation with global intra-modal alignment learning. Comprehensive experiments on semantic textual similarity (STS) and transfer (TR) tasks validate the effectiveness of our approach, consistently demonstrating its superiority over state-of-the-art baselines.</abstract>
      <url hash="a77e929f">2025.findings-acl.183</url>
      <bibkey>he-etal-2025-dalr</bibkey>
    </paper>
    <paper id="184">
      <title>Large Language Models in Bioinformatics: A Survey</title>
      <author><first>Zhenyu</first><last>Wang</last></author>
      <author><first>Zikang</first><last>Wang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jiyue</first><last>Jiang</last></author>
      <author><first>Pengan</first><last>Chen</last><affiliation>The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory and University of Hong Kong</affiliation></author>
      <author><first>Xiangyu</first><last>Shi</last></author>
      <author><first>Yu</first><last>Li</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <pages>3602-3615</pages>
      <abstract>Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.</abstract>
      <url hash="49bfcf7b">2025.findings-acl.184</url>
      <bibkey>wang-etal-2025-large-language</bibkey>
    </paper>
    <paper id="185">
      <title><fixed-case>C</fixed-case>hart<fixed-case>E</fixed-case>dit: How Far Are <fixed-case>MLLM</fixed-case>s From Automating Chart Analysis? Evaluating <fixed-case>MLLM</fixed-case>s’ Capability via Chart Editing</title>
      <author><first>Xuanle</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xuexin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yang</first><last>Haoyue</last></author>
      <author><first>Xianzhen</first><last>Luo</last><affiliation>Harbin Institute of Techology</affiliation></author>
      <author><first>Fanhu</first><last>Zeng</last></author>
      <author><first>Jianling</first><last>Li</last></author>
      <author><first>Qi</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chi</first><last>Chen</last></author>
      <pages>3616-3630</pages>
      <abstract>Although multimodal large language models (MLLMs) show promise in generating chart rendering code, editing charts via code presents a greater challenge. This task demands MLLMs to integrate chart understanding and reasoning capacities, which are labor-intensive. While many MLLMs claim such editing capabilities, current evaluations rely on limited case studies, highlighting the urgent need for a comprehensive evaluation framework.In this work, we propose ChartEdit, a new high-quality benchmark designed for chart editing tasks. This benchmark comprises 1,405 diverse editing instructions applied to 233 real-world charts, with each instruction-chart instance having been manually annotated and validated for accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream MLLMs across two types of experiments at both the code and chart levels.The results suggest that large-scale models can generate code to produce images that partially match the reference images.However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only 59.96, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at <url>https://github.com/xxlllz/ChartEdit</url>.</abstract>
      <url hash="a1731efc">2025.findings-acl.185</url>
      <bibkey>zhao-etal-2025-chartedit</bibkey>
    </paper>
    <paper id="186">
      <title>Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models</title>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Chao</first><last>Shang</last><affiliation>Amazon AWS AI</affiliation></author>
      <author><first>Ling</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Jie</first><last>Ma</last></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Srikanth</first><last>Doss</last></author>
      <author><first>Lluis</first><last>Marquez</last></author>
      <author><first>Miguel</first><last>Ballesteros</last><affiliation>Oracle</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <pages>3631-3643</pages>
      <abstract>The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as “safety alignment degradation” in this paper, and show that the challenge arises from the representation gap that emerges when introducing vision modality to VLMs. In particular, we show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for. At the same time, the safety alignment capabilities, initially developed within the textual embedding space, do not successfully transfer to this new multi-modal representation space. To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs, while simultaneously preserving the functional capabilities of VLMs. The empirical results show that our framework significantly recovers the alignment ability that is inherited from the LLM backbone with minimal impact on the fluency and linguistic capabilities of pre-trained VLMs even without additional training. Specifically, the unsafe rate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as 3.15% with only inference-time intervention.</abstract>
      <url hash="beb095c3">2025.findings-acl.186</url>
      <bibkey>liu-etal-2025-unraveling-mitigating</bibkey>
    </paper>
    <paper id="187">
      <title>Turbocharging Web Automation: The Impact of Compressed History States</title>
      <author><first>Xiyue</first><last>Zhu</last></author>
      <author><first>Peng</first><last>Tang</last><affiliation>Meta</affiliation></author>
      <author><first>Haofu</first><last>Liao</last><affiliation>Amazon</affiliation></author>
      <author><first>Srikar</first><last>Appalaraju</last><affiliation>Amazon</affiliation></author>
      <pages>3644-3651</pages>
      <abstract>Language models have led to leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequence and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.</abstract>
      <url hash="11484a3e">2025.findings-acl.187</url>
      <bibkey>zhu-etal-2025-turbocharging</bibkey>
    </paper>
    <paper id="188">
      <title>Making <fixed-case>RALM</fixed-case> Robust to Irrelevant Contexts via Layer Knowledge Guided Attention</title>
      <author><first>Weijie</first><last>Shi</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Hao</first><last>Chen</last><affiliation>FiT, Tencent</affiliation></author>
      <author><first>Jiaming</first><last>Li</last></author>
      <author><first>Yao</first><last>Zhao</last></author>
      <author><first>Yazhong</first><last>Zhang</last></author>
      <author><first>Qijin</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Ruiyuan</first><last>Zhang</last></author>
      <author><first>Jia</first><last>Zhu</last><affiliation>Zhejiang Normal University</affiliation></author>
      <author><first>Jiajie</first><last>Xu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Xiaofang</first><last>Zhou</last><affiliation>The Hong Kong University of Science and Technology and Hong Kong University of Science and Technology</affiliation></author>
      <pages>3652-3668</pages>
      <abstract>Retrieval-augmented language models (RALMs) aim to incorporate external knowledge to address the issues of factual hallucination and knowledge obsolescence faced by large language models (LLMs). Inevitably, the retrieved passages based on similarity search may be irrelevant to the given question, and the aggregation of these passages can confuse the model to give a correct answer. To improve the performance of RALM in such conditions, we propose layer-knowledge guided attention for RALMs, which harnesses the layer-wise knowledge of LLMs to optimize per-layer attention on useful passages, making the model pay attention to the most relevant content and ignore irrelevant ones. Specifically, we first systematically study LLM’s attention patterns and their relationship with the accuracy of RALM responses, where middle-focus attentions play a crucial role in selectively gathering relevant information. Based on this, a layer-wise passage estimator leverages the varied knowledge encoded across LLM layers to assess not only passage relevance scores but also associated confidences. Finally, a relevance-aware passage fusion enables selective attention to relevant passages, mitigating distractibility and positional bias of causal attention. Experiments show that our method outperforms existing methods on RALM benchmarks.</abstract>
      <url hash="04d4472b">2025.findings-acl.188</url>
      <bibkey>shi-etal-2025-making</bibkey>
    </paper>
    <paper id="189">
      <title>Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction</title>
      <author><first>Yuting</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chengyuan</first><last>Liu</last></author>
      <author><first>Yifeng</first><last>Feng</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chao</first><last>Wu</last></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3669-3690</pages>
      <abstract>As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the **R**ewrite to **J**ailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety. The code can be found at [https://github.com/ythuang02/R2J/.](https://github.com/ythuang02/R2J/)</abstract>
      <url hash="078810f4">2025.findings-acl.189</url>
      <bibkey>huang-etal-2025-rewrite</bibkey>
    </paper>
    <paper id="190">
      <title><fixed-case>S</fixed-case>ign<fixed-case>A</fixed-case>lign<fixed-case>LM</fixed-case>: Integrating Multimodal Sign Language Processing into Large Language Models</title>
      <author><first>Mert</first><last>Inan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>3691-3706</pages>
      <abstract>Deaf and Hard-of-Hearing (DHH) users increasingly utilize Large Language Models (LLMs), yet face significant challenges due to these models’ limited understanding of sign language grammar, multimodal sign inputs, and Deaf cultural contexts. Further, current approaches that try to address these limitations, frequently reduce sign language processing (SLP) to traditional translation tasks, neglecting the multimodal and linguistic complexity inherent in signed languages. In this paper, we present an empirical investigation informed by learning theory into natively integrating sign language support within LLMs, directly addressing the documented needs of DHH users. We introduce the first text-based and multimodal LLMs capable of sign language processing called SignAlignLM, and propose new prompting and fine-tuning strategies incorporating sign linguistic rules and conventions. We show that LLMs can be generalized interfaces for both spoken and signed languages if trained with a multitasking paradigm. Our code and model checkpoints are open-source.</abstract>
      <url hash="f458eeb8">2025.findings-acl.190</url>
      <bibkey>inan-etal-2025-signalignlm</bibkey>
    </paper>
    <paper id="191">
      <title><fixed-case>N</fixed-case>eg<fixed-case>VQA</fixed-case>: Can Vision Language Models Understand Negation?</title>
      <author><first>Yuhui</first><last>Zhang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yuchang</first><last>Su</last><affiliation>Stanford University and Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiming</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Serena</first><last>Yeung-Levy</last><affiliation>Stanford University</affiliation></author>
      <pages>3707-3716</pages>
      <abstract>Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs’ negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.</abstract>
      <url hash="264da50c">2025.findings-acl.191</url>
      <bibkey>zhang-etal-2025-negvqa</bibkey>
    </paper>
    <paper id="192">
      <title>Natural Language Reasoning in Large Language Models: Analysis and Evaluation</title>
      <author><first>Debela</first><last>Gemechu</last></author>
      <author><first>Ramon</first><last>Ruiz-Dolz</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Henrike</first><last>Beyer</last><affiliation>University of Dundee</affiliation></author>
      <author><first>Chris</first><last>Reed</last><affiliation>University of Dundee</affiliation></author>
      <pages>3717-3741</pages>
      <abstract>While Large Language Models (LLMs) have demonstrated promising results on a range of reasoning benchmarks—particularly in formal logic, mathematical tasks, and Chain-of-Thought prompting—less is known about their capabilities in unconstrained natural language reasoning. Argumentative reasoning, a form of reasoning naturally expressed in language and central to everyday discourse, presents unique challenges for LLMs due to its reliance on context, implicit assumptions, and value judgments. This paper addresses a gap in the study of reasoning in LLMs by presenting the first large-scale evaluation of their unconstrained natural language reasoning capabilities based on natural language argumentation. The paper offers three contributions: (i) the formalisation of a new strategy designed to evaluate argumentative reasoning in LLMs: argument-component selection; (ii) the creation of the Argument Reasoning Tasks (ART) dataset, a new benchmark for argument-component selection based on argument structures for natural language reasoning; and (iii) an extensive experimental analysis involving four different models, demonstrating the limitations of LLMs on natural language reasoning tasks.</abstract>
      <url hash="647763d7">2025.findings-acl.192</url>
      <bibkey>gemechu-etal-2025-natural</bibkey>
    </paper>
    <paper id="193">
      <title><fixed-case>SWE</fixed-case>-Dev: Building Software Engineering Agents with Training and Inference Scaling</title>
      <author><first>Haoran</first><last>Wang</last></author>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Yao</first><last>Wei</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <pages>3742-3761</pages>
      <abstract>Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.</abstract>
      <url hash="47b7ebaa">2025.findings-acl.193</url>
      <bibkey>wang-etal-2025-swe</bibkey>
    </paper>
    <paper id="194">
      <title>The Two Paradigms of <fixed-case>LLM</fixed-case> Detection: Authorship Attribution vs Authorship Verification</title>
      <author><first>Janek</first><last>Bevendorff</last><affiliation>Bauhaus Universität Weimar</affiliation></author>
      <author><first>Matti</first><last>Wiegmann</last><affiliation>Bauhaus Universität Weimar</affiliation></author>
      <author><first>Emmelie</first><last>Richter</last><affiliation>Bauhaus Universität Weimar</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>University of Kassel, hessian.AI, and ScaDS.AI</affiliation></author>
      <author><first>Benno</first><last>Stein</last><affiliation>Bauhaus Universität Weimar</affiliation></author>
      <pages>3762-3787</pages>
      <abstract>The detection of texts generated by LLMs has quickly become an important research problem. Many supervised and zero-shot detectors have already been proposed, yet their effectiveness and precision remain disputed. Current research therefore focuses on making detectors robust against domain shifts and on building corresponding benchmarks. In this paper, we show that the actual limitations hindering progress in LLM detection lie elsewhere: LLM detection is often implicitly modeled as an authorship attribution task, while its true nature is that of authorship verification. We systematically analyze the current research with respect to this misunderstanding, conduct an in-depth comparative analysis of the benchmarks, and validate our claim using state-of-the-art LLM detectors.Our contributions open the realm of authorship analysis technology for understanding and tackling the problem of LLM detection.</abstract>
      <url hash="8b439ee0">2025.findings-acl.194</url>
      <bibkey>bevendorff-etal-2025-two</bibkey>
    </paper>
    <paper id="195">
      <title>Unveiling Confirmation Bias in Chain-of-Thought Reasoning</title>
      <author><first>Yue</first><last>Wan</last></author>
      <author><first>Xiaowei</first><last>Jia</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <pages>3788-3804</pages>
      <abstract>Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of <i>confirmation bias</i> in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation (<tex-math>Q \to R</tex-math>) and reasoning-guided answer prediction (<tex-math>QR \to A</tex-math>) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at <i>https://github.com/yuewan2/biasedcot</i>.</abstract>
      <url hash="0d4475dc">2025.findings-acl.195</url>
      <bibkey>wan-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="196">
      <title><fixed-case>GRNF</fixed-case>ormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into <fixed-case>RNA</fixed-case> Foundation Models</title>
      <author><first>Mufan</first><last>Qiu</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Fengwei</first><last>Zhan</last></author>
      <author><first>Sukwon</first><last>Yun</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Jie</first><last>Peng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ruichen</first><last>Zhang</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Jiekun</first><last>Yang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>3805-3819</pages>
      <abstract>Foundation models for single-cell RNA sequencing (scRNA-seq) have shown promising capabilities in capturing gene expression patterns. However, current approaches face critical limitations: they ignore biological prior knowledge encoded in gene regulatory relationships and fail to leverage multi-omics signals that could provide complementary regulatory insights. In this paper, we propose GRNFormer, a new framework that systematically integrates multi-scale Gene Regulatory Networks (GRNs) inferred from multi-omics data into RNA foundation model training. Our framework introduces two key innovations. First, we introduce a pipeline for constructing hierarchical GRNs that capture regulatory relationships at both cell-type-specific and cell-specific resolutions. Second, we design a structure-aware integration framework that addresses the information asymmetry in GRNs through two technical advances: (1) A graph topological adapter using multi-head cross-attention to weight regulatory relationships dynamically, and (2) a novel edge perturbation strategy that perturb GRNs with biologically-informed co-expression links to augment graph neural network training. Comprehensive experiments have been conducted on three representative downstream tasks across multiple model architectures to demonstrate the effectiveness of GRNFormer. It achieves consistent improvements over state-of-the-art (SoTA) baselines: <tex-math>\mathbf{3.6\\\%}</tex-math> increase in drug response prediction correlation, <tex-math>\mathbf{9.6\\\%}</tex-math> improvement in single-cell drug classification AUC, and <tex-math>\mathbf{1.1\\\%}</tex-math> average gain in gene perturbation prediction accuracy.</abstract>
      <url hash="217d0f15">2025.findings-acl.196</url>
      <bibkey>qiu-etal-2025-grnformer</bibkey>
    </paper>
    <paper id="197">
      <title><fixed-case>R</fixed-case>emote<fixed-case>RAG</fixed-case>: A Privacy-Preserving <fixed-case>LLM</fixed-case> Cloud <fixed-case>RAG</fixed-case> Service</title>
      <author><first>Yihang</first><last>Cheng</last></author>
      <author><first>Lan</first><last>Zhang</last></author>
      <author><first>Junyang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Mu</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yunhao</first><last>Yao</last></author>
      <pages>3820-3837</pages>
      <abstract>Retrieval-augmented generation (RAG) improves the service quality of large language models by retrieving relevant documents from credible literature and integrating them into the context of the user query.Recently, the rise of the cloud RAG service has made it possible for users to query relevant documents conveniently.However, directly sending queries to the cloud brings potential privacy leakage.In this paper, we are the first to formally define the privacy-preserving cloud RAG service to protect the user query and propose RemoteRAG as a solution regarding privacy, efficiency, and accuracy.For privacy, we introduce <tex-math>(n,\epsilon)</tex-math>-DistanceDP to characterize privacy leakage of the user query and the leakage inferred from relevant documents.For efficiency, we limit the search range from the total documents to a small number of selected documents related to a perturbed embedding generated from <tex-math>(n,\epsilon)</tex-math>-DistanceDP, so that computation and communication costs required for privacy protection significantly decrease.For accuracy, we ensure that the small range includes target documents related to the user query with detailed theoretical analysis.Experimental results also demonstrate that RemoteRAG can resist existing embedding inversion attack methods while achieving no loss in retrieval under various settings.Moreover, RemoteRAG is efficient, incurring only 0.67 seconds and 46.66KB of data transmission (2.72 hours and 1.43 GB with the non-optimized privacy-preserving scheme) when retrieving from a total of <tex-math>10^5</tex-math> documents.</abstract>
      <url hash="cd4f1536">2025.findings-acl.197</url>
      <bibkey>cheng-etal-2025-remoterag</bibkey>
    </paper>
    <paper id="198">
      <title>“My life is miserable, have to sign 500 autographs everyday”: Exposing Humblebragging, the Brags in Disguise</title>
      <author><first>Sharath</first><last>Naganna</last></author>
      <author><first>Saprativa</first><last>Bhattacharjee</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Biplab</first><last>Banerjee</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>3838-3858</pages>
      <abstract>Humblebragging is a phenomenon in which individuals present self-promotional statements under the guise of modesty or complaints. For example, a statement like, “Ugh, I can’t believe I got promoted to lead the entire team. So stressful!”, subtly highlights an achievement while pretending to be complaining. Detecting humblebragging is important for machines to better understand the nuances of human language, especially in tasks like sentiment analysis and intent recognition. However, this topic has not yet been studied in computational linguistics. For the first time, we introduce the task of automatically detecting humblebragging in text. We formalize the task by proposing a 4-tuple definition of humblebragging and evaluate machine learning, deep learning, and large language models (LLMs) on this task, comparing their performance with humans. We also create and release a dataset called HB-24, containing 3,340 humblebrags generated using GPT-4o. Our experiments show that detecting humblebragging is non-trivial, even for humans. Our best model achieves an F1-score of 0.88. This work lays the foundation for further exploration of this nuanced linguistic phenomenon and its integration into broader natural language understanding systems.</abstract>
      <url hash="754f5a21">2025.findings-acl.198</url>
      <bibkey>naganna-etal-2025-life</bibkey>
    </paper>
    <paper id="199">
      <title><fixed-case>SCITAT</fixed-case>: A Question Answering Benchmark for Scientific Tables and Text Covering Diverse Reasoning Types</title>
      <author><first>Xuanliang</first><last>Zhang</last></author>
      <author><first>Dingzirui</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baoxin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Longxu</first><last>Dou</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Xinyuan</first><last>Lu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Keyan</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>3859-3881</pages>
      <abstract>Scientific question answering (SQA) is an important task aimed at answering questions based on papers. However, current SQA datasets have limited reasoning types and neglect the relevance between tables and text, creating a significant gap with real scenarios. To address these challenges, we propose a QA benchmark for scientific tables and text with diverse reasoning types (SCITAT). To cover more reasoning types, we summarize various reasoning types from real-world questions. To reason on both tables and text, we require the questions to incorporate tables and text as much as possible. Based on SCITAT, we propose a baseline (CAR), which combines various reasoning methods to address different reasoning types and process tables and text at the same time. CAR brings average improvements of 4.1% over other baselines on SCITAT, validating its effectiveness. Error analysis reveals the challenges of SCITAT, such as complex numerical calculations and domain knowledge.</abstract>
      <url hash="377c6a39">2025.findings-acl.199</url>
      <bibkey>zhang-etal-2025-scitat</bibkey>
    </paper>
    <paper id="200">
      <title><fixed-case>T</fixed-case>oken<fixed-case>S</fixed-case>hapley: Token Level Context Attribution with Shapley Value</title>
      <author><first>Yingtai</first><last>Xiao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yuqing</first><last>Zhu</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Sirat</first><last>Samyoun</last></author>
      <author><first>Wanrong</first><last>Zhang</last><affiliation>Tiktok</affiliation></author>
      <author><first>Jiachen T.</first><last>Wang</last><affiliation>Princeton University</affiliation></author>
      <author><first>Jian</first><last>Du</last><affiliation>TikTok</affiliation></author>
      <pages>3882-3894</pages>
      <abstract>Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving a 11–23% improvement in accuracy.</abstract>
      <url hash="af1bc475">2025.findings-acl.200</url>
      <bibkey>xiao-etal-2025-tokenshapley</bibkey>
    </paper>
    <paper id="201">
      <title>Entropy-based Exploration Conduction for Multi-step Reasoning</title>
      <author><first>Jinghan</first><last>Zhang</last><affiliation>Portland State University</affiliation></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Yeyang</first><last>Zhou</last><affiliation>Uber</affiliation></author>
      <author><first>Wanfu</first><last>Gao</last><affiliation>Jilin University</affiliation></author>
      <author><first>Kunpeng</first><last>Liu</last><affiliation>Portland State University</affiliation></author>
      <pages>3895-3906</pages>
      <abstract>Multi-step processes via large language models (LLMs) have proven effective for solving complex reasoning tasks. However, the depth of exploration of the reasoning procedure can significantly affect the task performance. Existing methods to automatically decide the depth often lead to high cost and a lack of flexibility. To address these issues, we propose Entropy-based Exploration Depth Conduction (Entro-duction), a novel method that dynamically adjusts the exploration depth during multi-step reasoning by monitoring LLM’s output entropy and variance entropy. We employ these two features to capture the model’s uncertainty of the current step and the fluctuation of uncertainty across consecutive reasoning steps. Based on the observed entropy changes, the LLM selects whether to deepen, expand, or stop exploration according to the probability, which facilitates the trade-off between the reasoning accuracy and exploration effectiveness. Experimental results across four benchmark datasets demonstrate the efficacy of Entro-duction.</abstract>
      <url hash="7d30949f">2025.findings-acl.201</url>
      <bibkey>zhang-etal-2025-entropy</bibkey>
    </paper>
    <paper id="202">
      <title>Taxonomizing Representational Harms using Speech Act Theory</title>
      <author><first>Emily</first><last>Corvi</last></author>
      <author><first>Hannah</first><last>Washington</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Stefanie</first><last>Reed</last></author>
      <author><first>Chad</first><last>Atalla</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alexandra</first><last>Chouldechova</last><affiliation>Microsoft and Carnegie Mellon University</affiliation></author>
      <author><first>P. Alex</first><last>Dow</last></author>
      <author><first>Jean</first><last>Garcia-Gathright</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Nicholas J</first><last>Pangakis</last><affiliation>Microsoft</affiliation></author>
      <author><first>Emily</first><last>Sheng</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Dan</first><last>Vann</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Matthew</first><last>Vogel</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hanna</first><last>Wallach</last><affiliation>Microsoft</affiliation></author>
      <pages>3907-3932</pages>
      <abstract>Representational harms are widely recognized among fairness-related harms caused by generative language systems. However, their definitions are commonly under-specified. We make a theoretical contribution to the specification of representational harms by introducing a framework, grounded in speech act theory (Austin 1962), that conceptualizes representational harms caused by generative language systems as the perlocutionary effects (i.e., real-world impacts) of particular types of illocutionary acts (i.e., system behaviors). Building on this argument and drawing on relevant literature from linguistic anthropology and sociolinguistics, we provide new definitions of stereotyping, demeaning, and erasure. We then use our framework to develop a granular taxonomy of illocutionary acts that cause representational harms, going beyond the high-level taxonomies presented in previous work. We also discuss the ways that our framework and taxonomy can support the development of valid measurement instruments. Finally, we demonstrate the utility of our framework and taxonomy via a case study that engages with recent conceptual debates about what constitutes a representational harm and how such harms should be measured.</abstract>
      <url hash="e2d03c09">2025.findings-acl.202</url>
      <bibkey>corvi-etal-2025-taxonomizing</bibkey>
    </paper>
    <paper id="203">
      <title>Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service <fixed-case>AI</fixed-case> Agents</title>
      <author><first>Prafulla Kumar</first><last>Choubey</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Xiangyu</first><last>Peng</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Shilpa</first><last>Bhagavath</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Shiva Kumar</first><last>Pentyala</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>3933-3954</pages>
      <abstract>Automated service agents require well-structured workflows to deliver consistent and accurate responses to customer queries. However, such workflows are often undocumented, and their automatic extraction from conversations remains largely unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process involves two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation step using question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively evaluate the quality of the extracted workflows, we introduce an automated simulation framework with agent and customer bots that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets show that our QA-CoT technique improves workflow extraction by 12.16% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, offering a reliable and scalable framework for future research.</abstract>
      <url hash="fa82b8c4">2025.findings-acl.203</url>
      <bibkey>choubey-etal-2025-turning</bibkey>
    </paper>
    <paper id="204">
      <title>Statistical inference on black-box generative models in the data kernel perspective space</title>
      <author><first>Hayden</first><last>Helm</last><affiliation>Nomic AI and Helivan Research</affiliation></author>
      <author><first>Aranyak</first><last>Acharyya</last></author>
      <author><first>Youngser</first><last>Park</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Brandon</first><last>Duderstadt</last><affiliation>Nomic AI</affiliation></author>
      <author><first>Carey</first><last>Priebe</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3955-3970</pages>
      <abstract>Generative models are capable of producing human-expert level content across a variety of topics and domains. As the impact of generative models grows, it is necessary to develop statistical methods to understand collections of available models. These methods are particularly important in settings where the user may not have access to information related to a model’s pre-training data, weights, or other relevant model-level covariates. In this paper we extend recent results on representations of black-box generative models to model-level statistical inference tasks. We demonstrate that the model-level representations are effective for multiple inference tasks.</abstract>
      <url hash="74ced4ab">2025.findings-acl.204</url>
      <bibkey>helm-etal-2025-statistical</bibkey>
    </paper>
    <paper id="205">
      <title>Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?</title>
      <author><first>Sohee</first><last>Yang</last><affiliation>University College London, University of London, Department of Computer Science, University College London, University of London, DeepMind and Google</affiliation></author>
      <author><first>Nora</first><last>Kassner</last><affiliation>Allen Institute for Artificial Intelligence, Google DeepMind and Google</affiliation></author>
      <author><first>Elena</first><last>Gribovskaya</last><affiliation>Deepmind Google</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Google and University College London</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <pages>3971-3992</pages>
      <abstract>We evaluate how well Large Language Models (LLMs) latently recall and compose facts to answer multi-hop queries like “In the year Scarlett Johansson was born, the Summer Olympics were hosted in the country of”. One major challenge in such evaluation is that LLMs may have developed shortcuts by encountering the head entity “Scarlett Johansson” and the answer entity “United States” in the same training sequences or merely guess the answer based on frequency-based priors. To prevent shortcuts, we exclude test queries where the head and answer entities might have co-appeared during training. Through careful selection of relations and facts and systematic removal of cases where models might guess answers or exploit partial matches, we construct an evaluation dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising latent multi-hop reasoning abilities without exploiting shortcuts, but only for certain types of queries. For queries requiring latent recall of countries as the intermediate answer, the best models achieve 80% latent composability, but this drops to just 5% for the recall of years. Comparisons with Chain-of-Thought highlight a significant gap between the ability of models to reason latently versus explicitly. Analysis reveals that latent representations of the intermediate answer are constructed more often in queries with higher latent composability, and shows the emergence of latent multi-hop reasoning during pretraining.</abstract>
      <url hash="83bde48d">2025.findings-acl.205</url>
      <bibkey>yang-etal-2025-large</bibkey>
    </paper>
    <paper id="206">
      <title><fixed-case>A</fixed-case>ce<fixed-case>M</fixed-case>ath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title>
      <author><first>Zihan</first><last>Liu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yang</first><last>Chen</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Mohammad</first><last>Shoeybi</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Bryan</first><last>Catanzaro</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Wei</first><last>Ping</last><affiliation>NVIDIA</affiliation></author>
      <pages>3993-4015</pages>
      <abstract>In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks.</abstract>
      <url hash="870e0bc2">2025.findings-acl.206</url>
      <bibkey>liu-etal-2025-acemath</bibkey>
    </paper>
    <paper id="207">
      <title><fixed-case>WXI</fixed-case>mpact<fixed-case>B</fixed-case>ench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models</title>
      <author><first>Yongan</first><last>Yu</last></author>
      <author><first>Qingchen</first><last>Hu</last></author>
      <author><first>Xianda</first><last>Du</last></author>
      <author><first>Jiayin</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Renée</first><last>Sieber</last><affiliation>McGill University</affiliation></author>
      <pages>4016-4035</pages>
      <abstract>Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.</abstract>
      <url hash="868093b7">2025.findings-acl.207</url>
      <bibkey>yu-etal-2025-wximpactbench</bibkey>
    </paper>
    <paper id="208">
      <title><fixed-case>M</fixed-case>e<fixed-case>M</fixed-case>o<fixed-case>T</fixed-case>une: A Measure and Moment-Driven Fine-Tuning Framework for Quantized Large Language Models</title>
      <author><first>Yun</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xue</first><last>Geng</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jintong</first><last>Sun</last></author>
      <author><first>Minghe</first><last>Yu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>4036-4050</pages>
      <abstract>Quantizing large language models (LLMs) is essential for reducing memory and computational costs in natural language processing. Existing methods combine quantization with parameter-efficient fine-tuning but often fail to meet practical performance requirements. This paper introduces MeMoTune, a novel fine-tuning framework for quantized LLMs. By employing a measure and moment approach within a low-rank approximation framework in probability measure space, MeMoTune optimizes the objective function for superior fine-tuning results. The update process is further refined through scaled gradient, enhancing convergence efficiency and noise robustness. Experiments on tasks like text generation, summarization, and understanding show MeMoTune significantly outperforms state-of-the-art methods, e.g. fine-tuning Llama2-13B on GSM8K improves accuracy by 5.5%, while fine-tuning DeBERTaV3-base on CoLA of GLUE increases Matthews correlation by 1.7%. The code is publicly available at: https://github.com/hddyyyb/MeMoTune.</abstract>
      <url hash="d7245291">2025.findings-acl.208</url>
      <bibkey>zhang-etal-2025-memotune</bibkey>
    </paper>
    <paper id="209">
      <title><fixed-case>MALAMUTE</fixed-case>: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset</title>
      <author><first>Sagi</first><last>Shaier</last></author>
      <author><first>George Arthur</first><last>Baker</last><affiliation>University of Utah and University of Colorado Boulder</affiliation></author>
      <author><first>Chiranthan</first><last>Sridhar</last></author>
      <author><first>Lawrence</first><last>Hunter</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>4051-4069</pages>
      <abstract>Language models (LMs) have excelled in various broad domains. However, to ensure their safe and effective integration into real-world educational settings, they must demonstrate proficiency in specific, granular areas of knowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs’ knowledge, have three major limitations. They: 1) do not cover the educational domain; 2) typically focus on low-complexity, generic knowledge or broad domains, which do not adequately assess the models’ knowledge in specific subjects; and 3) often rely on templates that can bias model predictions. Here, we introduce MALAMUTE, a multilingual, template-free, and highly granular probing dataset comprising expert-written, peer-reviewed probes from 71 university-level textbooks across three languages (English, Spanish, and Polish). MALAMUTE is the first education-based cloze-style dataset. It covers eight domains, each with up to 14 subdomains, further broken down into concepts and concept-based prompts, totaling 33,361 university curriculum concepts and 116,887 prompts. MALAMUTE’s fine granularity, educational focus, and inclusion of both sentence-level and paragraph-level prompts make it an ideal tool for evaluating LMs’ course-related knowledge. Our evaluation of masked and causal LMs on MALAMUTE shows that despite overall proficiency, they have significant gaps in knowledge when examined closely on specific subjects, hindering their safe use in classrooms and underscoring the need for further development.</abstract>
      <url hash="b6c55f7a">2025.findings-acl.209</url>
      <bibkey>shaier-etal-2025-malamute</bibkey>
    </paper>
    <paper id="210">
      <title>Sentimental Image Generation for Aspect-based Sentiment Analysis</title>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <author><first>Jinghang</first><last>Gu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>4070-4081</pages>
      <abstract>Recent research work on textual Aspect-Based Sentiment Analysis (ABSA) have achieved promising performance. However, a persistent challenge lies in the limited semantics derived from the raw data. To address this issue, researchers have explored enhancing textual ABSA with additional augmentations, they either craft audio, text and linguistic features based on the input, or rely on user-posted images. Yet these approaches have their limitations: the former three formations are heavily overlap with the original data, which undermines their ability to be supplementary while the user-posted images are extremely dependent on human annotation, which not only limits its application scope to just a handful of text-image datasets, but also propagates the errors derived from human mistakes to the entire downstream loop. In this study, we explore the way of generating the sentimental image that no one has ever ventured before. We propose a novel Sentimental Image Generation method that can precisely provide ancillary visual semantics to reinforce the textual extraction as shown in Figure 1. Extensive experiments build a new SOTA performance in ACOS, ASQP and en-Phone datasets, underscoring the effectiveness of our method and highlighting a promising direction for expanding our features.</abstract>
      <url hash="e74e5dc4">2025.findings-acl.210</url>
      <bibkey>bao-etal-2025-sentimental</bibkey>
    </paper>
    <paper id="211">
      <title>Long-form Hallucination Detection with Self-elicitation</title>
      <author><first>Zihang</first><last>Liu</last></author>
      <author><first>Jiawei</first><last>Guo</last></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hongyang</first><last>Chen</last></author>
      <author><first>Jiajun</first><last>Bu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haishuai</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4082-4100</pages>
      <abstract>While Large Language Models (LLMs) have exhibited impressive performance in generating long-form content, they frequently present a hazard of producing factual inaccuracies or hallucinations. An effective strategy to mitigate this hazard is to leverage off-the-shelf LLMs to detect hallucinations after the generation. The primary challenge resides in the comprehensive elicitation of the intrinsic knowledge acquired during their pre-training phase. However, existing methods that employ multi-step reasoning chains predominantly fall short of addressing this issue. Moreover, since existing methods for hallucination detection tend to decompose text into isolated statements, they are unable to understand the contextual semantic relations in long-form content. In this paper, we study a novel concept, self-elicitation, to leverage self-generated thoughts derived from prior statements as catalysts to elicit the expression of intrinsic knowledge and understand contextual semantics. We present a framework, SelfElicit, to integrate self-elicitation with graph structures to effectively organize the elicited knowledge and facilitate factual evaluations. Extensive experiments on five datasets in various domains demonstrate the effectiveness of self-elicitation and the superiority of our proposed method.</abstract>
      <url hash="e1ba9713">2025.findings-acl.211</url>
      <bibkey>liu-etal-2025-long</bibkey>
    </paper>
    <paper id="212">
      <title><fixed-case>C</fixed-case>omparison<fixed-case>QA</fixed-case>: Evaluating Factuality Robustness of <fixed-case>LLM</fixed-case>s Through Knowledge Frequency Control and Uncertainty</title>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Edinburgh University, University of Edinburgh and Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianshi</first><last>Zheng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiyu</first><last>Ren</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>4101-4117</pages>
      <abstract>The rapid development of LLMs has sparked extensive research into their factual knowledge. Current works find that LLMs fall short on questions around low-frequency entities. However, such proofs are unreliable since the questions can differ not only in entity frequency but also in difficulty themselves. So we introduce **ComparisonQA** benchmark, containing **283K** abstract questions, each instantiated by a pair of high-frequency and low-frequency entities. It ensures a controllable comparison to study the role of knowledge frequency in the performance of LLMs. Because the difference between such a pair is only the entity with different frequencies. In addition, we use both correctness and uncertainty to develop a two-round method to evaluate LLMs’ knowledge robustness. It aims to avoid possible semantic shortcuts which is a serious problem of current QA study. Experiments reveal that LLMs, including GPT-4o, exhibit particularly low robustness regarding low-frequency knowledge. Besides, we find that uncertainty can be used to effectively identify high-quality and shortcut-free questions while maintaining the data size. Based on this, we propose an automatic method to select such questions to form a subset called **ComparisonQA-Hard**, containing only hard low-frequency questions.</abstract>
      <url hash="0d5c3e32">2025.findings-acl.212</url>
      <bibkey>zong-etal-2025-comparisonqa</bibkey>
    </paper>
    <paper id="213">
      <title>One-Dimensional Object Detection for Streaming Text Segmentation of Meeting Dialogue</title>
      <author><first>Rui</first><last>He</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Minjie</first><last>Qiang</last></author>
      <author><first>Hongling</first><last>Wang</last></author>
      <author><first>Yifan.zhang</first><last>Yifan.zhang</last></author>
      <author><first>Hua</first><last>Xu</last></author>
      <author><first>Shuai</first><last>Fan</last><affiliation>AISpeech Ltd</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>4118-4130</pages>
      <abstract>Dialogue text segmentation aims to partition dialogue content into consecutive paragraphs based on themes or logic, enhancing its comprehensibility and manageability. Current text segmentation models, when applied directly to STS (Streaming Text Segmentation), exhibit numerous limitations, such as imbalances in labels that affect the stability of model training, and discrepancies between the model’s training tasks (sentence classification) and the actual text segmentation that limit the model’s segmentation capabilities.To address these challenges, we first implement STS for the first time using a sliding window-based segmentation method. Secondly, we employ two different levels of sliding window-based balanced label strategies to stabilize the training process of the streaming segmentation model and enhance training convergence speed. Finally, by adding a one-dimensional bounding-box regression task for text sequences within the window, we restructure the training approach of STS tasks, shifting from sentence classification to sequence segmentation, thereby aligning the training objectives with the task objectives, which further enhanced the model’s performance. Extensive experimental results demonstrate that our method is robust, controllable, and achieves state-of-the-art performance.</abstract>
      <url hash="ae793aab">2025.findings-acl.213</url>
      <bibkey>he-etal-2025-one</bibkey>
    </paper>
    <paper id="214">
      <title><fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>axo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts</title>
      <author><first>Qingkai</first><last>Zeng</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuyang</first><last>Bai</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Zhenyu</first><last>Wu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>4131-4144</pages>
      <abstract>Taxonomies provide structural representations of knowledge and are crucial in various applications. The task of taxonomy expansion involves integrating emerging entities into existing taxonomies by identifying appropriate parent entities for these new query entities. Previous methods rely on self-supervised techniques that generate annotation data from existing taxonomies but are less effective with small taxonomies (fewer than 100 entities). In this work, we introduce CodeTaxo, a novel approach that leverages large language models through code language prompts to capture the taxonomic structure. Extensive experiments on five real-world benchmarks from different domains demonstrate that CodeTaxo consistently achieves superior performance across all evaluation metrics, significantly outperforming previous state-of-the-art methods. The code and data are available at <url>https://github.com/QingkaiZeng/CodeTaxo-official</url>.</abstract>
      <url hash="4cd36ef6">2025.findings-acl.214</url>
      <bibkey>zeng-etal-2025-codetaxo</bibkey>
    </paper>
    <paper id="215">
      <title>Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings</title>
      <author><first>Yuqicheng</first><last>Zhu</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Daniel</first><last>Hernández</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Yuan</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Zifeng</first><last>Ding</last></author>
      <author><first>Bo</first><last>Xiong</last><affiliation>Stanford University</affiliation></author>
      <author><first>Evgeny</first><last>Kharlamov</last><affiliation>University of Oslo and Robert Bosch GmbH, Bosch</affiliation></author>
      <author><first>Steffen</first><last>Staab</last><affiliation>University of Stuttgart and University of Southampton</affiliation></author>
      <pages>4145-4167</pages>
      <abstract>Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is crucial for ensuring the reliability of downstream applications. A recent work applies conformal prediction to KGE methods, providing uncertainty estimates by generating a set of answers that is guaranteed to include the true answer with a predefined confidence level. However, existing methods provide probabilistic guarantees averaged over a reference set of queries and answers (marginal coverage guarantee). In high-stakes applications such as medical diagnosis, a stronger guarantee is often required: the predicted sets must provide consistent coverage per query (conditional coverage guarantee). We propose CondKGCP, a novel method that approximates predicate-conditional coverage guarantees while maintaining compact prediction sets. CondKGCP merges predicates with similar vector representations and augments calibration with rank information. We prove the theoretical guarantees and demonstrate empirical effectiveness of CondKGCP by comprehensive evaluations.</abstract>
      <url hash="694a9436">2025.findings-acl.215</url>
      <bibkey>zhu-etal-2025-predicate</bibkey>
    </paper>
    <paper id="216">
      <title>Autonomous Data Selection with Zero-shot Generative Classifiers for Mathematical Texts</title>
      <author><first>Yifan</first><last>Zhang</last><affiliation>University of California, Los Angeles and Tsinghua University</affiliation></author>
      <author><first>Yifan</first><last>Luo</last></author>
      <author><first>Yang</first><last>Yuan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Andrew C</first><last>Yao</last><affiliation>The Chinese University of Hong Kong and Tsinghua University</affiliation></author>
      <pages>4168-4189</pages>
      <abstract>We present Autonomous Data Selection (AutoDS), a method that leverages base language models as zero-shot “generative classifiers” to automatically curate high-quality mathematical texts. Unlike prior approaches that require human annotations or training a dedicated data filter, AutoDS relies solely on a model’s logits to determine whether a given passage is mathematically informative and educational. By integrating AutoDS into a continual pretraining pipeline, we substantially boost downstream performance on challenging math benchmarks (MATH, GSM8K, and BBH) while using far fewer tokens than previous methods. Empirically, our approach achieves roughly a twofold improvement in pretraining token efficiency over strong baselines, underscoring the potential of self-directed data selection in enhancing mathematical reasoning. We will release our curated dataset to facilitate future research in automated domain-specific data curation.</abstract>
      <url hash="70a3ecc7">2025.findings-acl.216</url>
      <bibkey>zhang-etal-2025-autonomous</bibkey>
    </paper>
    <paper id="217">
      <title>Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review</title>
      <author><first>Zhuochun</first><last>Li</last></author>
      <author><first>Yuelyu</first><last>Ji</last></author>
      <author><first>Rui</first><last>Meng</last><affiliation>Google</affiliation></author>
      <author><first>Daqing</first><last>He</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>4190-4205</pages>
      <abstract>While reasoning capabilities typically emerge in large language models (LLMs) with tens of billions of parameters, recent research focuses on improving smaller open-source models through knowledge distillation (KD) from commercial LLMs. However, many of these studies rely solely on responses from a single LLM as the gold rationale, unlike the natural human learning process, which involves understanding both the correct answers and the reasons behind mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach: 1) instead of merely obtaining rationales from teachers, our method asks teachers to identify and explain the student’s mistakes, providing customized instruction learning data; 2) we design a simulated peer-review process between teacher LLMs, and selects only the generated rationales above the acceptance threshold, which reduces the chance of teachers guessing correctly with flawed rationale, improving instructional data quality. Comprehensive experiments and analysis on mathematical, commonsense, and logical reasoning tasks demonstrate the effectiveness of our method. Our code is available at https://github.com/zhuochunli/Learn-from-Committee.</abstract>
      <url hash="c0dca9c8">2025.findings-acl.217</url>
      <bibkey>li-etal-2025-learning-committee</bibkey>
    </paper>
    <paper id="218">
      <title>Investigating Prosodic Signatures via Speech Pre-Trained Models for Audio Deepfake Source Attribution</title>
      <author><first>Orchid</first><last>Chetia Phukan</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Drishti</first><last>Singh</last></author>
      <author><first>Swarup Ranjan</first><last>Behera</last><affiliation>Reliance Jio AICoE</affiliation></author>
      <author><first>Arun Balaji</first><last>Buduru</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Rajesh</first><last>Sharma</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <pages>4206-4214</pages>
      <abstract>In this work, we investigate various state-of-the-art (SOTA) speech pre-trained models (PTMs) for their capability to capture prosodic sig-natures of the generative sources for audio deepfake source attribution (ADSD). These prosodic characteristics can be considered oneof major signatures for ADSD, which is unique to each source. So better is the PTM at capturing prosodic signs better the ADSD per-formance. We consider various SOTA PTMs that have shown top performance in different prosodic tasks for our experiments on benchmark datasets, ASVSpoof 2019 and CFAD. x-vector (speaker recognition PTM) attains the highest performance in comparison to allthe PTMs considered despite consisting lowest model parameters. This higher performance can be due to its speaker recognition pre-training that enables it for capturing unique prosodic characteristics of the sources in a better way. Further, motivated from tasks suchas audio deepfake detection and speech recognition, where fusion of PTMs representations lead to improved performance, we explorethe same and propose FINDER for effective fusion of such representations. With fusion of Whisper and x-vector representations through FINDER, we achieved the topmost performance in comparison to all the individual PTMs as well as baseline fusion techniques and attaining SOTA performance.</abstract>
      <url hash="4e8a9571">2025.findings-acl.218</url>
      <bibkey>chetia-phukan-etal-2025-investigating</bibkey>
    </paper>
    <paper id="219">
      <title>Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness</title>
      <author><first>Bryan</first><last>Li</last></author>
      <author><first>Fiona</first><last>Luo</last></author>
      <author><first>Samar</first><last>Haider</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Adwait</first><last>Agashe</last></author>
      <author><first>Siyu</first><last>Li</last></author>
      <author><first>Runqi</first><last>Liu</last></author>
      <author><first>Miranda Muqing</first><last>Miao</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Shriya</first><last>Ramakrishnan</last></author>
      <author><first>Yuan</first><last>Yuan</last></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <pages>4215-4241</pages>
      <abstract>The paradigm of retrieval-augmented generated (RAG) helps mitigate hallucinations of large language models (LLMs). However, RAG also introduces biases contained within the retrieved documents. These biases can be amplified in scenarios which are multilingual and culturally-sensitive, such as territorial disputes. We thus introduce BordIRLines, a dataset of territorial disputes paired with retrieved Wikipedia documents, across 49 languages. We evaluate the cross-lingual robustness of this RAG setting by formalizing several modes for multilingual retrieval. Our experiments on several LLMs show that incorporating perspectives from diverse languages can in fact improve robustness; retrieving multilingual documents best improves response consistency and decreases geopolitical bias over RAG with purely in-language documents. We also consider how RAG responses utilize presented documents, finding a much wider variance in the linguistic distribution of response citations, when querying in low-resource languages. Our further analyses investigate the various aspects of a cross-lingual RAG pipeline, from retrieval to document contents. We release our benchmark to support continued research towards equitable information access across languages, at https://huggingface.co/datasets/borderlines/bordirlines.</abstract>
      <url hash="1a49147e">2025.findings-acl.219</url>
      <bibkey>li-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="220">
      <title>Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</title>
      <author><first>Pengyue</first><last>Jia</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Derong</first><last>Xu</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Xiaopeng</first><last>Li</last></author>
      <author><first>Zhaocheng</first><last>Du</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Yichao</first><last>Wang</last></author>
      <author><first>Yuhao</first><last>Wang</last></author>
      <author><first>Qidong</first><last>Liu</last><affiliation>City University of Hong Kong and Xi’an Jiaotong University</affiliation></author>
      <author><first>Maolin</first><last>Wang</last></author>
      <author><first>Huifeng</first><last>Guo</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>4242-4256</pages>
      <abstract>The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of large language models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.</abstract>
      <url hash="74f1ec86">2025.findings-acl.220</url>
      <bibkey>jia-etal-2025-bridging</bibkey>
    </paper>
    <paper id="221">
      <title>Scaling Laws for Multilingual Language Models</title>
      <author><first>Yifei</first><last>He</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Alon</first><last>Benhaim</last><affiliation>Microsoft</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Praneetha</first><last>Vaddamanu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sanchit</first><last>Ahuja</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Parul</first><last>Chopra</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Han</first><last>Zhao</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Xia</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <pages>4257-4273</pages>
      <abstract>We propose a novel scaling law for general-purpose decoder-only language models (LMs) trained on multilingual data, tackling the problem of balancing languages during multilingual pretraining. A primary challenge in studying multilingual scaling is the difficulty of analyzing individual language performance due to cross-lingual transfer. To tackle this, we shift the focus from individual languages to language families. We introduce and validate a hypothesis that the test cross-entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages in the mixture. This insight simplifies the complexity of multilingual scaling and make the analysis scalable to an arbitrary number of languages. Building on this hypothesis, we derive a power-law relationship that links performance with dataset size, model size and sampling ratios. This relationship enables us to predict performance across various combinations of the above three quantities, and derive the optimal sampling ratios at different model scales. To demonstrate the effectiveness and accuracy of our proposed scaling law, we perform a large-scale empirical study, training more than 100 models on 23 languages spanning 5 language families. Our experiments show that the optimal sampling ratios derived from small models (85M parameters) generalize effectively to models that are several orders of magnitude larger (1.2B parameters), offering a resource-efficient approach for multilingual LM training at scale.</abstract>
      <url hash="785b2473">2025.findings-acl.221</url>
      <bibkey>he-etal-2025-scaling</bibkey>
    </paper>
    <paper id="222">
      <title>Corpus Poisoning via Approximate Greedy Gradient Descent</title>
      <author><first>Jinyan</first><last>Su</last><affiliation>Cornell University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Claire</first><last>Cardie</last><affiliation>Cornell University</affiliation></author>
      <pages>4274-4294</pages>
      <abstract>Dense retrievers are widely used in information retrieval and have also been successfully extended to other knowledge intensive areas such as language models, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they have recently been shown to be vulnerable to corpus poisoning attacks in which a malicious user injects a small fraction of adversarial passages into the retrieval corpus to trick the system into returning these passages among the top-ranked results for a broad set of user queries. Further study is needed to understand the extent to which these attacks could limit the deployment of dense retrievers in real-world applications. In this work, we propose Approximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval systems based on the widely used HotFlip method for efficiently generating adversarial passages. We demonstrate that AGGD can select a higher quality set of token-level perturbations than HotFlip by replacing its random token sampling with a more structured search. Experimentally, we show that our method achieves a high attack success rate on several datasets and using several retrievers, and can generalize to unseen queries and new domains. Notably, our method is extremely effective in attacking the ANCE retrieval model, achieving attack success rates that are 15.24% and 17.44% higher on the NQ and MS MARCO datasets, respectively, compared to HotFlip. Additionally, we demonstrate AGGD’s potential to replace HotFlip in other adversarial attacks, such as knowledge poisoning of RAG systems.</abstract>
      <url hash="338e8263">2025.findings-acl.222</url>
      <bibkey>su-etal-2025-corpus</bibkey>
    </paper>
    <paper id="223">
      <title>Taxonomy-Driven Knowledge Graph Construction for Domain-Specific Scientific Applications</title>
      <author><first>Huitong</first><last>Pan</last><affiliation>Temple University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Mustapha</first><last>Adamu</last></author>
      <author><first>Eduard</first><last>Dragut</last><affiliation>Temple University</affiliation></author>
      <author><first>Longin Jan</first><last>Latecki</last><affiliation>Temple University</affiliation></author>
      <pages>4295-4320</pages>
      <abstract>We present a taxonomy-driven framework for constructing domain-specific knowledge graphs (KGs) that integrates structured taxonomies, Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). Although we focus on climate science to illustrate its effectiveness, our approach can potentially be adapted for other specialized domains. Existing methods often neglect curated taxonomies—hierarchies of verified entities and relationships—and LLMs frequently struggle to extract KGs in specialized domains. Our approach addresses these gaps by anchoring extraction to expert-curated taxonomies, aligning entities and relations with domain semantics, and validating LLM outputs using RAG against the domain taxonomy. Through a climate science case study using our annotated dataset of 25 publications (1,705 entity-publication links, 3,618 expert-validated relationships), we demonstrate that taxonomy-guided LLM prompting combined with RAG-based validation reduces hallucinations by 23.3% while improving F1 scores by 13.9% compared to baselines without the proposed techniques. Our contributions include: 1) a generalizable methodology for taxonomy-aligned KG construction; 2) a reproducible annotation pipeline, 3) the first benchmark dataset for climate science information retrieval; and 4) empirical insights into combining structured taxonomies with LLMs for specialized domains. The dataset, including expert annotations and taxonomy-aligned outputs, is publicly available at <url>https://github.com/Jo-Pan/ClimateIE</url>, and the accompanying framework can be accessed at <url>https://github.com/Jo-Pan/TaxoDrivenKG</url>.</abstract>
      <url hash="2edfe9d1">2025.findings-acl.223</url>
      <bibkey>pan-etal-2025-taxonomy</bibkey>
    </paper>
    <paper id="224">
      <title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
      <author><first>Yifan</first><last>Yang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Kai</first><last>Zhen</last></author>
      <author><first>Bhavana</first><last>Ganesh</last><affiliation>Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Goeric</first><last>Huybrechts</last><affiliation>Amazon</affiliation></author>
      <author><first>Markus</first><last>Müller</last><affiliation>Amazon</affiliation></author>
      <author><first>Jonas M.</first><last>Kübler</last><affiliation>Amazon</affiliation></author>
      <author><first>Rupak Vignesh</first><last>Swaminathan</last><affiliation>Amazon</affiliation></author>
      <author><first>Athanasios</first><last>Mouchtaris</last></author>
      <author><first>Sravan Babu</first><last>Bodapati</last><affiliation>Amazon</affiliation></author>
      <author><first>Nathan</first><last>Susanj</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Jack</first><last>FitzGerald</last><affiliation>Amazon</affiliation></author>
      <author><first>Abhishek</first><last>Kumar</last><affiliation>Google DeepMind</affiliation></author>
      <pages>4321-4333</pages>
      <abstract>Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level <b>regional</b> gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.</abstract>
      <url hash="ee895a69">2025.findings-acl.224</url>
      <bibkey>yang-etal-2025-wanda</bibkey>
    </paper>
    <paper id="225">
      <title><fixed-case>MATCHED</fixed-case>: Multimodal Authorship-Attribution To Combat Human Trafficking in Escort-Advertisement Data</title>
      <author><first>Vageesh Kumar</first><last>Saxena</last></author>
      <author><first>Benjamin</first><last>Ashpole</last></author>
      <author><first>Gijs</first><last>Van Dijck</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>4334-4373</pages>
      <abstract>Human trafficking (HT) remains a critical issue, with traffickers increasingly leveraging online escort advertisements to advertise victims anonymously. Existing detection methods, including text-based Authorship Attribution (AA), overlook the multimodal nature of these ads, which combine text and images. To bridge this gap, we introduce MATCHED, a multimodal AA dataset comprising 27,619 unique text descriptions and 55,115 unique images sourced from Backpage across seven U.S. cities in four geographic regions. This study extensively benchmarks text-only, vision-only, and multimodal baselines for vendor identification and verification tasks, employing multitask (joint) training objectives that achieve superior classification and retrieval performance on in-sample and out-of-data distribution datasets. The results demonstrate that while text remains the dominant modality, integrating visual features adds stylistic cues that enrich model performance. Moreover, text-image alignment strategies like CLIP and BLIP2 struggle due to low semantic overlap and vague connections between the modalities of escort ads, with end-to-end multimodal training proving more robust. Our findings emphasize the potential of multimodal AA to combat HT, providing Law Enforcement Agencies with robust tools to link advertisements and disrupt trafficking networks.</abstract>
      <url hash="c1591bd6">2025.findings-acl.225</url>
      <bibkey>saxena-etal-2025-matched</bibkey>
    </paper>
    <paper id="226">
      <title>Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of <fixed-case>LLM</fixed-case> Against Augmented Fraud and Phishing Inducements</title>
      <author id="shu-yang"><first>Shu</first><last>Yang</last></author>
      <author><first>Shenzhe</first><last>Zhu</last></author>
      <author><first>Zeyu</first><last>Wu</last></author>
      <author><first>Keyu</first><last>Wang</last></author>
      <author><first>Junchi</first><last>Yao</last></author>
      <author><first>Junchao</first><last>Wu</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lijie</first><last>Hu</last></author>
      <author><first>Mengdi</first><last>Li</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>4374-4420</pages>
      <abstract>With the increasing integration of large language models (LLMs) into real-world applications such as finance, e-commerce, and recommendation systems, their susceptibility to misinformation and adversarial manipulation poses significant risks. Existing fraud detection benchmarks primarily focus on single-turn classification tasks, failing to capture the dynamic nature of real-world fraud attempts. To address this gap, we introduce Fraud-R1, a challenging bilingual benchmark designed to assess LLMs’ ability to resist fraud and phishing attacks across five key fraud categories: Fraudulent Services, Impersonation, Phishing Scams, Fake Job Postings, and Online Relationships, covering subclasses. Our dataset comprises manually curated fraud cases from social media, news, phishing scam records, and prior fraud datasets.</abstract>
      <url hash="0b6d288b">2025.findings-acl.226</url>
      <bibkey>yang-etal-2025-fraud</bibkey>
    </paper>
    <paper id="227">
      <title>Mitigating Paraphrase Attacks on Machine-Text Detection via Paraphrase Inversion</title>
      <author><first>Rafael Alberto</first><last>Rivera Soto</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Barry Y.</first><last>Chen</last></author>
      <author><first>Nicholas</first><last>Andrews</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>4421-4433</pages>
      <abstract>High-quality paraphrases are easy to produce using instruction-tuned language models or specialized paraphrasing models. Although this capability has a variety of benign applications, paraphrasing attacks—paraphrases applied to machine-generated texts—are known to significantly degrade the performance of machine-text detectors. This motivates us to consider the novel problem of paraphrase inversion, where, given paraphrased text, the objective is to recover an approximation of the original text. The closer the approximation is to the original text, the better machine-text detectors will perform. We propose an approach which frames the problem as translation from paraphrased text back to the original text, which requires examples of texts and corresponding paraphrases to train the inversion model. Fortunately, such training data can easily be generated, given a corpus of original texts and one or more paraphrasing models. We find that language models such as GPT-4 and Llama-3 exhibit biases when paraphrasing which an inversion model can learn with a modest amount of data. Perhaps surprisingly, we also find that such models generalize well, including to paraphrase models unseen at training time. Finally, we show that when combined with a paraphrased-text detector, our inversion models provide an effective defense against paraphrasing attacks, and overall our approach yields an average improvement of +22% AUROC across seven machine-text detectors and three different domains.</abstract>
      <url hash="b622741a">2025.findings-acl.227</url>
      <bibkey>rivera-soto-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="228">
      <title><fixed-case>SANSKRITI</fixed-case>: A Comprehensive Benchmark for Evaluating Language Models’ Knowledge of <fixed-case>I</fixed-case>ndian Culture</title>
      <author><first>Arijit</first><last>Maji</last></author>
      <author><first>Raghvendra</first><last>Kumar</last></author>
      <author><first>Akash</first><last>Ghosh</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Anushka</first><last>Anushka</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <pages>4434-4451</pages>
      <abstract>Language models (LMs) are indispensable tools shaping modern workflows, but their global effectiveness depends on understanding local socio-cultural contexts. To address this, we introduce <i>
          <b>SANSKRITI</b></i>, a benchmark designed to evaluate language models’ comprehension of India’s rich cultural diversity. Comprising of 21,853 meticulously curated question-answer pairs spanning 28 states and 8 union territories, <i>
          <b>SANSKRITI</b></i> is the largest dataset for testing Indian cultural knowledge. It covers sixteen key attributes of Indian culture namely rituals and ceremonies, history, tourism, cuisine, dance and music, costume, language, art, festivals, religion, medicine, transport, sports, nightlife and personalities, providing a comprehensive representation of India’s cultural tapestry. We evaluate <i>
          <b>SANSKRITI</b></i> on leading Large Language Models (LLMs), Indic Language Models (ILMs), and Small Language Models(SLMs), revealing significant disparities in their ability to handle culturally nuanced queries, with many models struggling in region-specific contexts. By offering an extensive, culturally rich, and diverse dataset, <i>
          <b>SANSKRITI</b></i> sets a new standard for assessing and improving the cultural understanding of LMs. We will share the dataset and findings publicly to support research on inclusive and culturally aware AI systems.</abstract>
      <url hash="121d6770">2025.findings-acl.228</url>
      <bibkey>maji-etal-2025-sanskriti</bibkey>
    </paper>
    <paper id="229">
      <title>System Prompt Hijacking via Permutation Triggers in <fixed-case>LLM</fixed-case> Supply Chains</title>
      <author><first>Lu</first><last>Yan</last></author>
      <author><first>Siyuan</first><last>Cheng</last><affiliation>Purdue University</affiliation></author>
      <author><first>Xuan</first><last>Chen</last></author>
      <author><first>Kaiyuan</first><last>Zhang</last><affiliation>Purdue University</affiliation></author>
      <author><first>Guangyu</first><last>Shen</last><affiliation>Purdue University</affiliation></author>
      <author><first>Xiangyu</first><last>Zhang</last><affiliation>Purdue University</affiliation></author>
      <pages>4452-4473</pages>
      <abstract>LLMs are increasingly developed through distributed supply chains, where model providers create base models that deployers customize with system prompts for task-specific applications and safety alignment. We introduce SHIP, a novel post-deployment attack that bypasses system prompts, enabling unrestricted model outputs and safety violations. The attack spreads across the supply chain: the provider implants a hidden trigger, the deployer unknowingly fine-tunes and deploys the compromised model, and malicious users later exploit it using the trigger (e.g., obtained via underground market), as real-world software supply chain breaches. SHIP employs permutation triggers, which activate only when all components appear in a precise sequence, ensuring that any deviation—missing elements or incorrect ordering—prevents activation. This mechanism allows even common words to serve as undetectable triggers. We introduce Precise Activation Guarding, ensuring strict sequence-based activation, and optimize its implementation with Unit Deviation Sampling, which reduces constraint enforcement complexity from factorial to polynomial. Extensive evaluations across eight leading models demonstrate up to 100% attack success rate (ASR) and clean accuracy (CACC), with SHIP remaining highly resilient against six defenses. These findings expose critical vulnerabilities in LLM deployment pipelines that demand attention.</abstract>
      <url hash="1576c69d">2025.findings-acl.229</url>
      <bibkey>yan-etal-2025-system</bibkey>
    </paper>
    <paper id="230">
      <title>Frequency matters: Modeling irregular morphological patterns in <fixed-case>S</fixed-case>panish with Transformers</title>
      <author><first>Akhilesh</first><last>Kakolu Ramarao</last></author>
      <author><first>Kevin</first><last>Tang</last></author>
      <author><first>Dinah</first><last>Baer-Henney</last></author>
      <pages>4474-4489</pages>
      <abstract>Over the past decade, various studies have addressed how speakers solve the so-called ‘The Paradigm Cell Filling Problem’ (PCFP) (CITATION) across different languages. The PCFP addresses a fundamental question in morphological processing: how do speakers accurately generate inflected forms of words when presented with incomplete paradigms? This problem is particularly salient when modeling complex inflectional systems. We focus on Spanish verbal paradigms, where certain verbs follow an irregular L-shaped pattern, where the first-person singular present indicative stem matches the stem used throughout the present subjunctive mood. We formulate the problem as a morphological reinflection task. Specifically, we investigate the role of input frequency in the acquisition of regular versus irregular L-shaped patterns in transformer models. By systematically manipulating the input distributions and analyzing model behavior, we reveal four key findings: 1) Models perform better on L-shaped verbs compared to regular verbs, especially in uneven frequency conditions; 2) Robust primacy effects are observed, but no consistent recency effects; 3) Memorization becomes more prominent as the proportion of L-shaped verbs increases; 4) There is a tendency to regularize L-shaped verbs when their consonant alternation pairs are rare or absent in the training data.</abstract>
      <url hash="b7b36eee">2025.findings-acl.230</url>
      <bibkey>ramarao-etal-2025-frequency</bibkey>
    </paper>
    <paper id="231">
      <title>From Heart to Words: Generating Empathetic Responses via Integrated Figurative Language and Semantic Context Signals</title>
      <author><first>Gyeongeun</first><last>Lee</last></author>
      <author><first>Zhu</first><last>Wang</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Sathya N.</first><last>Ravi</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>4490-4502</pages>
      <abstract>Although generically expressing empathy is straightforward, effectively conveying empathy in specialized settings presents nuanced challenges. We present a conceptually motivated investigation into the use of figurative language and causal semantic context to facilitate targeted empathetic response generation within a specific mental health support domain, studying how these factors may be leveraged to promote improved response quality. Our approach achieves a 7.6% improvement in BLEU, a 36.7% reduction in Perplexity, and a 7.6% increase in lexical diversity (D-1 and D-2) compared to models without these signals, and human assessments show a 24.2% increase in empathy ratings. These findings provide deeper insights into grounded empathy understanding and response generation, offering a foundation for future research in this area.</abstract>
      <url hash="1b71147e">2025.findings-acl.231</url>
      <bibkey>lee-etal-2025-heart</bibkey>
    </paper>
    <paper id="232">
      <title>There’s No Such Thing as Simple Reasoning for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Nurul Fajrin</first><last>Ariyani</last><affiliation>Cardiff University and Institut Teknologi Sepuluh Nopember</affiliation></author>
      <author><first>Zied</first><last>Bouraoui</last><affiliation>CRIL Univ-Artois &amp; CNRS</affiliation></author>
      <author><first>Richard</first><last>Booth</last></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>4503-4514</pages>
      <abstract>Large Language Models (LLMs) have been widely found to struggle with logical reasoning, where even fine-tuned models fail dramatically on out-of-distribution problems. However, existing work has focused on relatively complex “many-hop” reasoning problems. In this paper, we analyse the performance of fine-tuned LLMs on simple reasoning problems, all of which can be solved in at most three inference steps. Due to the simplicity of these problems, the model cannot encounter test problems that are fundamentally different from those it has seen during training. Unfortunately, however, we find that the models remain highly brittle, being susceptible to seemingly innocent perturbations, such as the addition of duplicates to the set of premises and shuffling the order in which the premises are presented.</abstract>
      <url hash="8df100b4">2025.findings-acl.232</url>
      <bibkey>ariyani-etal-2025-theres</bibkey>
    </paper>
    <paper id="233">
      <title><fixed-case>CLIX</fixed-case>: Cross-Lingual Explanations of Idiomatic Expressions</title>
      <author><first>Aaron</first><last>Gluck</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>4515-4529</pages>
      <abstract>Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.</abstract>
      <url hash="322abd28">2025.findings-acl.233</url>
      <bibkey>gluck-etal-2025-clix</bibkey>
    </paper>
    <paper id="234">
      <title>Beyond Semantic Entropy: Boosting <fixed-case>LLM</fixed-case> Uncertainty Quantification with Pairwise Semantic Similarity</title>
      <author><first>Dang</first><last>Nguyen</last><affiliation>Google</affiliation></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Baharan</first><last>Mirzasoleiman</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>4530-4540</pages>
      <abstract>Hallucination in large language models (LLMs) can be detected by assessing the uncertainty of model outputs, typically measured using entropy. Semantic entropy (SE) enhances traditional entropy estimation by quantifying uncertainty at the semantic cluster level. However, as modern LLMs generate longer one-sentence responses, SE becomes less effective because it overlooks two crucial factors: intra-cluster similarity (the spread within a cluster) and inter-cluster similarity (the distance between clusters). To address this limitation, we propose a simple black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy. Our approach can also be easily extended to white-box settings by incorporating token probabilities. Additionally, we provide theoretical results showing that our method generalizes semantic entropy. Extensive empirical results demonstrate its effectiveness compared to semantic entropy across two recent LLMs (Phi3 and Llama3) and three common text generation tasks: question answering, text summarization, and machine translation. Our code is available at [https://github.com/BigML-CS-UCLA/SNNE](https://github.com/BigML-CS-UCLA/SNNE).</abstract>
      <url hash="55fcd642">2025.findings-acl.234</url>
      <bibkey>nguyen-etal-2025-beyond</bibkey>
    </paper>
    <paper id="235">
      <title><fixed-case>R</fixed-case><tex-math>^3</tex-math><fixed-case>M</fixed-case>em: Bridging Memory Retention and Retrieval via Reversible Compression</title>
      <author><first>Xiaoqiang</first><last>Wang</last></author>
      <author><first>Suyuchen</first><last>Wang</last><affiliation>Université de Montréal and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <pages>4541-4557</pages>
      <abstract>Memory plays a key role in enhancing LLMs’ performance when deployed to real-world applications. Existing solutions face trade-offs: explicit memory designs based on external storage require complex management and incur storage overhead, while implicit memory designs that store information via parameters struggle with reliable retrieval. In this paper, we propose R<tex-math>^3</tex-math>Mem, a memory network that optimizes both information Retention and Retrieval through Reversible context compression. Specifically, R<tex-math>^3</tex-math>Mem employs virtual memory tokens to compress and encode infinitely long histories, further enhanced by a hierarchical compression strategy that refines information from document- to entity-level for improved assimilation across granularities. For retrieval, R<tex-math>^3</tex-math>Mem employs a reversible architecture, reconstructing raw data by invoking the model backward with compressed information. Implemented via parameter-efficient fine-tuning, it can integrate seamlessly with any Transformer-based model. Experiments demonstrate that our memory design achieves state-of-the-art performance in long-context language modeling and retrieval-augmented generation tasks. It also significantly outperforms conventional memory modules in long-horizon interaction tasks like conversational agents, showcasing its potential for next-generation retrieval systems.</abstract>
      <url hash="f2d03059">2025.findings-acl.235</url>
      <bibkey>wang-etal-2025-r3mem</bibkey>
    </paper>
    <paper id="236">
      <title>Vision Language Model Helps Private Information De-Identification in Vision Data</title>
      <author><first>Tiejin</first><last>Chen</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Pingzhi</first><last>Li</last></author>
      <author><first>Kaixiong</first><last>Zhou</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Hua</first><last>Wei</last><affiliation>Arizona State University</affiliation></author>
      <pages>4558-4572</pages>
      <abstract>Visual Language Models (VLMs) have gained significant popularity due to their remarkable ability. While various methods exist to enhance privacy in text-based applications, privacy risks associated with visual inputs remain largely overlooked such as Protected Health Information (PHI) in medical images. To tackle this problem, two key tasks: accurately localizing sensitive text and processing it to ensure privacy protection should be performed. To address this issue, we introduce VisShield (Vision Privacy Shield), an end-to-end framework designed to enhance the privacy awareness of VLMs. Our framework consists of two key components: a specialized instruction-tuning dataset OPTIC (Optical Privacy Text Instruction Collection) and a tailored training methodology. The dataset provides diverse privacy-oriented prompts that guide VLMs to perform targeted Optical Character Recognition (OCR) for precise localization of sensitive text, while the training strategy ensures effective adaptation of VLMs to privacy-preserving tasks. Specifically, our approach ensures that VLMs recognize privacy-sensitive text and output precise bounding boxes for detected entities, allowing for effective masking of sensitive information. Extensive experiments demonstrate that our framework significantly outperforms existing approaches in handling private information, paving the way for privacy-preserving applications in vision-language models.</abstract>
      <url hash="24340947">2025.findings-acl.236</url>
      <bibkey>chen-etal-2025-vision</bibkey>
    </paper>
    <paper id="237">
      <title>Unveiling Privacy Risks in Multi-modal Large Language Models: Task-specific Vulnerabilities and Mitigation Challenges</title>
      <author><first>Tiejin</first><last>Chen</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Pingzhi</first><last>Li</last></author>
      <author><first>Kaixiong</first><last>Zhou</last><affiliation>North Carolina State University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Hua</first><last>Wei</last><affiliation>Arizona State University</affiliation></author>
      <pages>4573-4586</pages>
      <abstract>Privacy risks in text-only Large Language Models (LLMs) are well studied, particularly their tendency to memorize and leak sensitive information. However, Multi-modal Large Language Models (MLLMs), which process both text and images, introduce unique privacy challenges that remain underexplored. Compared to text-only models, MLLMs can extract and expose sensitive information embedded in images, posing new privacy risks. We reveal that some MLLMs are susceptible to privacy breaches, leaking sensitive data embedded in images or stored in memory. Specifically, in this paper, we (1) introduce MM-Privacy, a comprehensive dataset designed to assess privacy risks across various multi-modal tasks and scenarios, where we define Disclosure Risks and Retention Risks. (2) systematically evaluate different MLLMs using MM-Privacy and demonstrate how models leak sensitive data across various tasks, and (3) provide additional insights into the role of task inconsistency in privacy risks, emphasizing the urgent need for mitigation strategies. Our findings highlight privacy concerns in MLLMs, underscoring the necessity of safeguards to prevent data exposure. Part of our dataset and code can be found here.</abstract>
      <url hash="fba1511c">2025.findings-acl.237</url>
      <bibkey>chen-etal-2025-unveiling-privacy</bibkey>
    </paper>
    <paper id="238">
      <title><fixed-case>D</fixed-case>e<fixed-case>F</fixed-case>ine: Decision-Making with Analogical Reasoning over Factor Profiles</title>
      <author><first>Yebowen</first><last>Hu</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Amazon</affiliation></author>
      <author><first>Yiming</first><last>Lu</last></author>
      <author><first>Daoan</first><last>Zhang</last></author>
      <author><first>Hassan</first><last>Foroosh</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <pages>4587-4603</pages>
      <abstract>LLMs are ideal for decision-making thanks to their ability to reason over long contexts. However, challenges arise when processing speech transcripts that describe complex scenarios, as they are verbose and include repetition, hedging, and vagueness. E.g., during a company’s earnings call, an executive might project a positive revenue outlook to reassure investors, despite uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce DeFine, a modular framework that constructs probabilistic factor profiles from complex scenarios. It then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in new situations. Our framework separates the tasks of quantifying uncertainty and incorporating it into LLM decision-making. This approach is particularly useful in areas such as consulting and financial deliberation, where making decisions under uncertainty is vital.</abstract>
      <url hash="023ca35c">2025.findings-acl.238</url>
      <bibkey>hu-etal-2025-define</bibkey>
    </paper>
    <paper id="239">
      <title><fixed-case>SMART</fixed-case>: Self-Aware Agent for Tool Overuse Mitigation</title>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Emre Can</first><last>Acikgoz</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Avirup</first><last>Sil</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Gokhan</first><last>Tur</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>4604-4621</pages>
      <abstract>Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to **Tool Overuse**, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce **SMART** (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent’s self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce **SMART-ER**, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop **SMARTAgent**, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.</abstract>
      <url hash="b4d9ce6e">2025.findings-acl.239</url>
      <bibkey>qian-etal-2025-smart</bibkey>
    </paper>
    <paper id="240">
      <title>Continued Pretraining and Interpretability-Based Evaluation for Low-Resource Languages: A <fixed-case>G</fixed-case>alician Case Study</title>
      <author><first>Pablo</first><last>Rodríguez</last></author>
      <author><first>Silvia Paniagua</first><last>Suárez</last></author>
      <author><first>Pablo</first><last>Gamallo</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <author><first>Susana Sotelo</first><last>Docio</last><affiliation>Universidade de Santiago de Compostela</affiliation></author>
      <pages>4622-4637</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have led to remarkable improvements in language understanding and text generation. However, challenges remain in enhancing their performance for underrepresented languages, ensuring continual learning without catastrophic forgetting, and developing robust evaluation methodologies. This work addresses these issues by investigating the impact of Continued Pretraining (CPT) on multilingual models and proposing a comprehensive evaluation framework for LLMs, focusing on the case of Galician language. Our first contribution explores CPT strategies for languages with limited representation in multilingual models. We analyze how CPT with Galician corpora improves text generation while assessing the trade-offs between linguistic enrichment and task-solving capabilities. Our findings show that CPT with small, high-quality corpora and diverse instructions enhances both task performance and linguistic quality. Our second contribution is a structured evaluation framework based on distinguishing task-based and language-based assessments, leveraging existing and newly developed benchmarks for Galician. Additionally, we contribute new Galician LLMs, datasets for evaluation and instructions, and an evaluation framework.</abstract>
      <url hash="f27d7418">2025.findings-acl.240</url>
      <bibkey>rodriguez-etal-2025-continued</bibkey>
    </paper>
    <paper id="241">
      <title><fixed-case>TC</fixed-case>-Bench: Benchmarking Temporal Compositionality in Conditional Video Generation</title>
      <author><first>Weixi</first><last>Feng</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Jiachen</first><last>Li</last><affiliation>xAI</affiliation></author>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Tsu-Jui</first><last>Fu</last><affiliation>Apple</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>4638-4662</pages>
      <abstract>Video generation has many unique challenges beyond those of image generation. The temporal dimension introduces extensive possible variations across frames, over which consistency and continuity may be violated. In this work, we evaluate the emergence of new concepts and relation transitions as time progresses in a video, which we refer to as Temporal Compositionality. We propose TC-Bench, a benchmark of meticulously crafted text prompts, ground truth videos, and new evaluation metrics. The prompts articulate the initial and final states of scenes, effectively reducing ambiguities for frame development. In addition, by collecting corresponding ground-truth videos, the benchmark can be used for text-to-video and image-to-video generation. We develop new metrics to measure the completeness of component transitions, which demonstrate significantly higher correlations with human judgments than existing metrics. Our experiments reveal that contemporary video generators are still weak in prompt understanding and achieve less than 20% of the compositional changes, highlighting enormous improvement space. Our analysis indicates that current video generation models struggle to interpret descriptions of compositional changes and synthesize various components across different time steps.</abstract>
      <url hash="65887682">2025.findings-acl.241</url>
      <bibkey>feng-etal-2025-tc</bibkey>
    </paper>
    <paper id="242">
      <title><fixed-case>DAM</fixed-case>: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration</title>
      <author><first>Hanzhi</first><last>Zhang</last><affiliation>University of North Texas</affiliation></author>
      <author><first>Heng</first><last>Fan</last><affiliation>University of North Texas and Temple University</affiliation></author>
      <author><first>Kewei</first><last>Sha</last><affiliation>University of North Texas</affiliation></author>
      <author><first>Yan</first><last>Huang</last><affiliation>, University of North Texas</affiliation></author>
      <author><first>Yunhe</first><last>Feng</last><affiliation>University of North Texas</affiliation></author>
      <pages>4663-4676</pages>
      <abstract>Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.</abstract>
      <url hash="24abde1e">2025.findings-acl.242</url>
      <bibkey>zhang-etal-2025-dam</bibkey>
    </paper>
    <paper id="243">
      <title>Arbiters of Ambivalence: Challenges of using <fixed-case>LLM</fixed-case>s in No-Consensus tasks</title>
      <author><first>Bhaktipriya</first><last>Radharapu</last><affiliation>Facebook</affiliation></author>
      <author><first>Manon</first><last>Revel</last><affiliation>Harvard University</affiliation></author>
      <author><first>Megan</first><last>Ung</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Facebook</affiliation></author>
      <author><first>Adina</first><last>Williams</last><affiliation>FAIR (Meta Platforms Inc.)</affiliation></author>
      <pages>4677-4731</pages>
      <abstract>The increasing use of LLMs as substitutes for humans in “aligning” LLMs has raised questions about their ability to replicate human judgments and preferences, especially in ambivalent scenarios where humans disagree. This study examines the biases and limitations of LLMs in three roles: answer generator, judge, and debater. These roles loosely correspond to previously described alignment frameworks: preference alignment (judge) and scalable oversight (debater), with the answer generator reflecting the typical setting with user interactions. We develop a “no-consensus” benchmark by curating examples that encompass a variety of a priori ambivalent scenarios, each presenting two possible stances. Our results show that while LLMs can provide nuanced assessments when generating open-ended answers, they tend to take a stance on no-consensus topics when employed as judges or debaters. These findings underscore the necessity for more sophisticated methods for aligning LLMs without human oversight, highlighting that LLMs cannot fully capture human non-agreement even on topics where humans themselves are divided.</abstract>
      <url hash="62c7fffa">2025.findings-acl.243</url>
      <bibkey>radharapu-etal-2025-arbiters</bibkey>
    </paper>
    <paper id="244">
      <title>Beyond Text: Characterizing Domain Expert Needs in Document Research</title>
      <author><first>Sireesh</first><last>Gururaja</last></author>
      <author><first>Nupoor</first><last>Gandhi</last></author>
      <author><first>Jeremiah</first><last>Milbauer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>4732-4745</pages>
      <abstract>Working with documents is a key part of almost any knowledge work, from contextualizing research in a literature review to reviewing legal precedent. Recently, as their capabilities have expanded, primarily text-based NLP systems have often been billed as able to assist or even automate this kind of work. But to what extent are these systems able to model these tasks as experts conceptualize and perform them now? In this study, we interview sixteen domain experts across two domains to understand their processes of document research, and compare it to the current state of NLP systems. We find that our participants processes are idiosyncratic, iterative, and rely extensively on the social context of a document in addition its content, and that approaches in NLP and adjacent fields that explicitly center the document as an object, rather than as merely a container for text, tend to better reflect our participants’ priorities. We call on the NLP community to more carefully consider the role of the document in building useful tools that are accessible, personalizable, iterative, and socially aware.</abstract>
      <url hash="97e50331">2025.findings-acl.244</url>
      <bibkey>gururaja-etal-2025-beyond</bibkey>
    </paper>
    <paper id="245">
      <title>Efficient but Vulnerable: Benchmarking and Defending <fixed-case>LLM</fixed-case> Batch Prompting Attack</title>
      <author><first>Murong</first><last>Yue</last><affiliation>George Mason University</affiliation></author>
      <author><first>Ziyu</first><last>Yao</last><affiliation>George Mason University</affiliation></author>
      <pages>4746-4761</pages>
      <abstract>Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BatchSafeBench, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.</abstract>
      <url hash="0fb9da67">2025.findings-acl.245</url>
      <bibkey>yue-yao-2025-efficient</bibkey>
    </paper>
    <paper id="246">
      <title><fixed-case>MM</fixed-case>-R<tex-math>^3</tex-math>: On (In-)Consistency of Vision-Language Models (<fixed-case>VLM</fixed-case>s)</title>
      <author><first>Shih-Han</first><last>Chou</last><affiliation>Department of Computer Science, University of British Columbia</affiliation></author>
      <author><first>Shivam</first><last>Chandhok</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Jim</first><last>Little</last><affiliation>University of British Columbia, University of British Columbia and University of British Columbia</affiliation></author>
      <author><first>Leonid</first><last>Sigal</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4762-4788</pages>
      <abstract>With the advent of LLMs and variants, a flurry of research has emerged, analyzing the performance of such models across an array of tasks. While most studies focus on evaluating the capabilities of state-of-the-art (SoTA) Vision Language Models (VLMs) through task accuracy (<tex-math>\textit{e.g.}</tex-math>, visual question answering, grounding), our work explores the related but complementary aspect of <tex-math>\textit{consistency}</tex-math> – the ability of a VLM to produce semantically similar or identical responses to semantically similar queries. We note that consistency is a fundamental prerequisite (necessary but not sufficient condition) for robustness and trust in VLMs. Armed with this perspective, we propose the MM-Rbenchmark, which allows us to analyze performance, in terms of consistency and accuracy, of SoTA VLMs on three tasks: Question Rephrasing, Image Restyling, and Context Reasoning. Our analysis reveals that consistency does not always align with accuracy, indicating that models with higher accuracy are not necessarily more consistent, and vice versa. Furthermore, we propose a simple yet effective mitigation strategy in the form of an adapter module trained to minimize inconsistency across prompts. With our proposed strategy, we are able to achieve absolute improvements of 5.7% and 12.5%, on average on widely used VLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing counterparts.</abstract>
      <url hash="68bfb08e">2025.findings-acl.246</url>
      <bibkey>chou-etal-2025-mm</bibkey>
    </paper>
    <paper id="247">
      <title>Investigating Context Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style</title>
      <author><first>Yuepei</first><last>Li</last></author>
      <author><first>Kang</first><last>Zhou</last><affiliation>Amazon</affiliation></author>
      <author><first>Qiao</first><last>Qiao</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Bach</first><last>Nguyen</last></author>
      <author><first>Qing</first><last>Wang</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Qi</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <pages>4789-4807</pages>
      <abstract>Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs’ context faithfulness remain largely unexplored. In this study, we investigate the impact of memory strength and evidence presentation on LLMs’ receptiveness to external evidence. We quantify the memory strength of LLMs by measuring the divergence in LLMs’ responses to different paraphrases of the same question, which is not considered by previous works. We also generate evidence in various styles to examine LLMs’ behavior. Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory. Furthermore, presenting paraphrased evidence significantly increases LLMs’ receptiveness compared to simple repetition or adding details. These findings provide key insights for improving retrieval-augmented generation and context-aware LLMs. Our code is available at https://github.com/liyp0095/ContextFaithful.</abstract>
      <url hash="a227978a">2025.findings-acl.247</url>
      <bibkey>li-etal-2025-investigating</bibkey>
    </paper>
    <paper id="248">
      <title>Shadow-Activated Backdoor Attacks on Multimodal Large Language Models</title>
      <author><first>Ziyi</first><last>Yin</last></author>
      <author><first>Muchao</first><last>Ye</last><affiliation>University of Iowa</affiliation></author>
      <author><first>Yuanpu</first><last>Cao</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Aofei</first><last>Chang</last></author>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Ting</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Fenglong</first><last>Ma</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4808-4829</pages>
      <abstract>This paper delves into a novel backdoor attack scenario, aiming to uncover potential security risks associated with Multimodal Large Language Models (MLLMs) during multi-round open-ended conversations with users. In the practical use of MLLMs, users have full control over the interaction process with the model, such as using their own collected photos and posing arbitrary open-ended questions. Traditional backdoor attacks that rely on adding external triggers are less applicable. To this end, we introduce a new shadow-activated backdoor attacking paradigm in this paper, wherein attacks implicitly inject malicious content into the responses of MLLMs when the responses explicitly relate to the shadowed object, i.e., without any triggers. To facilitate the shadow-activated backdoor attack, we present a novel framework named BadMLLM to achieve the desired behaviors by constructing a poisoned dataset using GPT-4 Vision and implementing an attention-regularized tuning strategy to address the semantic discontinuity between the original response and the inserted promotion. Extensive experimental results conducted on five MLLMs, three objects, and two types of promotion slogans have demonstrated impressive performance in achieving both efficacy and utility goals, thereby highlighting the significant potential risks concealed within MLLMs.</abstract>
      <url hash="f23a1f7f">2025.findings-acl.248</url>
      <bibkey>yin-etal-2025-shadow</bibkey>
    </paper>
    <paper id="249">
      <title>Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding</title>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Can</first><last>Qin</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Haoyi</first><last>Qiu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>4830-4843</pages>
      <abstract>Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget’s theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.</abstract>
      <url hash="680c8cd6">2025.findings-acl.249</url>
      <bibkey>huang-etal-2025-vision</bibkey>
    </paper>
    <paper id="250">
      <title>K-order Ranking Preference Optimization for Large Language Models</title>
      <author><first>Shihao</first><last>Cai</last></author>
      <author><first>Chongming</first><last>Gao</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wentao</first><last>Shi</last></author>
      <author><first>Jizhi</first><last>Zhang</last></author>
      <author><first>Keqin</first><last>Bao</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4844-4859</pages>
      <abstract>To adapt large language models (LLMs) to ranking tasks, existing list-wise methods, represented by list-wise Direct Preference Optimization (DPO), focus on optimizing partial-order or full-order list ranking consistency for LLMs to enhance their ranking abilities.However, we argue that optimizing top-K ranking consistency could be more appropriate for real-world applications. There are two main reasons: (1) users are typically concerned with only the top-K results, making top-K ranking more important, and (2) tail items often lack precise feedback, making top-K ranking more reliable. Based on this, we propose <tex-math>\textbf{K}</tex-math>-order Ranking <tex-math>\textbf{P}</tex-math>reference <tex-math>\textbf{O}</tex-math>ptimization (KPO) by extending the DPO’s Plackett-Luce model to accommodate top-K rankings. Additionally, recognizing that the number of important items can vary across queries, we extend KPO to dynamically determine appropriate <tex-math>K</tex-math> for different samples and introduce a curriculum learning strategy to boost training efficiency. Extensive experiments demonstrate the effectiveness of KPO, highlighting its high sample efficiency and robustness to noise. The code is available at https://github.com/Lanyu0303/KPO.</abstract>
      <url hash="7e9775a9">2025.findings-acl.250</url>
      <bibkey>cai-etal-2025-k</bibkey>
    </paper>
    <paper id="251">
      <title>Spectral Insights into Data-Oblivious Critical Layers in Large Language Models</title>
      <author><first>Xuyuan</first><last>Liu</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Lei</first><last>Hsiung</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Yaoqing</first><last>Yang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Yujun</first><last>Yan</last><affiliation>Dartmouth College</affiliation></author>
      <pages>4860-4877</pages>
      <abstract>Understanding how feature representations evolve across layers in large language models (LLMs) is key to improving their interpretability and robustness. While recent studies have identified critical layers linked to specific functions or behaviors, these efforts typically rely on data-dependent analyses of fine-tuned models, limiting their use to post-hoc settings. In contrast, we introduce a <i>data-oblivious</i> approach to identify intrinsic critical layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered Kernel Alignment (CKA). We show that layers with significant shifts in representation space are also those most affected during fine-tuning—a pattern that holds consistently across tasks for a given model. Our spectral analysis further reveals that these shifts are driven by changes in the top principal components, which encode semantic transitions from rationales to conclusions.We further apply these findings to two practical scenarios: efficient domain adaptation, where fine-tuning critical layers leads to greater loss reduction compared to non-critical layers; and backdoor defense, where freezing them reduces attack success rates by up to 40%.</abstract>
      <url hash="1038ca55">2025.findings-acl.251</url>
      <bibkey>liu-etal-2025-spectral</bibkey>
    </paper>
    <paper id="252">
      <title><fixed-case>S</fixed-case>yn<fixed-case>F</fixed-case>ix: Dependency-Aware Program Repair via <fixed-case>R</fixed-case>elation<fixed-case>G</fixed-case>raph Analysis</title>
      <author><first>Xunzhu</first><last>Tang</last></author>
      <author><first>Jiechao</first><last>Gao</last></author>
      <author><first>Jin</first><last>Xu</last></author>
      <author><first>Tiezhu</first><last>Sun</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Yewei</first><last>Song</last></author>
      <author><first>Saad</first><last>Ezzini</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Wendkûuni C.</first><last>Ouédraogo</last></author>
      <author><first>Jacques</first><last>Klein</last><affiliation>University of Luxemburg</affiliation></author>
      <author><first>Tegawendé F.</first><last>Bissyandé</last><affiliation>University of Luxemburg</affiliation></author>
      <pages>4878-4894</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly improved software development automation, including bug localization, code synthesis, program repair, and test generation. However, most prior work on program repair focuses on isolated elements, such as classes or functions, neglecting their interdependencies, which limits repair accuracy. We present SynFix, a RelationGraph-based approach that integrates LLMs with structural search and synchronization techniques for coordinated program repair across codebases. SynFix constructs a <b>RelationGraph</b> to capture relationships among classes, functions, variables, and their interactions (e.g., imports, inheritance, dependencies). Each RelationGraph node includes detailed code descriptions to help LLMs understand root causes and retrieve relevant contexts. By analyzing one-hop nodes in the RelationGraph, SynFixensures repairs account for dependent updates across components. Patch validation is conducted using regression tests from the SWE-bench benchmark suite. Evaluated on SWE-bench datasets, SynFix resolves 52.33% of issues in <b>SWE-bench-lite</b> (300 GitHub issues), 55.8% in <b>SWE-bench-verified</b> (500 issues), and 29.86% in <b>SWE-bench-full</b> (2,294 issues), outperforming baselines such as Swe-Agent, Agentless and AutoCodeRover. The codebase is available at <url>https://anonymous.4open.science/r/AutoFix-EC86/</url>.</abstract>
      <url hash="1667f898">2025.findings-acl.252</url>
      <bibkey>tang-etal-2025-synfix</bibkey>
    </paper>
    <paper id="253">
      <title><fixed-case>EXIT</fixed-case>: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation</title>
      <author><first>Taeho</first><last>Hwang</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>SeungYoon</first><last>Han</last></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>4895-4924</pages>
      <abstract>We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce the latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents—while preserving their contextual dependencies—enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at https://github.com/ThisIsHwang/EXIT.</abstract>
      <url hash="6e970081">2025.findings-acl.253</url>
      <bibkey>hwang-etal-2025-exit</bibkey>
    </paper>
    <paper id="254">
      <title>Re-<fixed-case>TASK</fixed-case>: Revisiting <fixed-case>LLM</fixed-case> Tasks from Capability, Skill, and Knowledge Perspectives</title>
      <author><first>Zhihu</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>Heyuan</first><last>Huang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Sitao</first><last>Xie</last></author>
      <author><first>Yubo</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Shi</last><affiliation>Xmax.AI Ltd.</affiliation></author>
      <author><first>Zhixing</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hongyan</first><last>Li</last></author>
      <author><first>Junchi</first><last>Yan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4925-4936</pages>
      <abstract>The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving complex problems with large language models (LLMs). However, its application to domain-specific tasks remains challenging, as LLMs often fail to decompose tasks accurately or execute subtasks effectively. This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, and Knowledge perspectives, drawing on the principles of Bloom’s Taxonomy and Knowledge Space Theory. While CoT provides a workflow-centric perspective on tasks, Re-TASK introduces a Chain-of-Learning (CoL) paradigm that highlights task dependencies on specific capability items, further broken down into their constituent knowledge and skill components. To address CoT failures, we propose a Re-TASK prompting strategy, which strengthens task-relevant capabilities through targeted knowledge injection and skill adaptation. Experiments across diverse domains demonstrate the effectiveness of Re-TASK. In particular, we achieve improvements of 45.00% on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks. These results highlight the potential of Re-TASK to significantly enhance LLM performance and its applicability in specialized domains. We release our code and data at https://github.com/Uylee/Re-TASK.</abstract>
      <url hash="55ff488f">2025.findings-acl.254</url>
      <bibkey>wang-etal-2025-task</bibkey>
    </paper>
    <paper id="255">
      <title>Unlearning Backdoor Attacks for <fixed-case>LLM</fixed-case>s with Weak-to-Strong Knowledge Distillation</title>
      <author><first>Shuai</first><last>Zhao</last></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Cong-Duy T</first><last>Nguyen</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Yanhao</first><last>Jia</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Meihuizi</first><last>Jia</last><affiliation>Northwest Normal University Lanzhou</affiliation></author>
      <author><first>Feng</first><last>Yichao</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>4937-4952</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model’s ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct comprehensive experiments on three state-of-the-art large language models and several different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.</abstract>
      <url hash="b4e3a3c5">2025.findings-acl.255</url>
      <bibkey>zhao-etal-2025-unlearning</bibkey>
    </paper>
    <paper id="256">
      <title>Packing Analysis: Packing Is More Appropriate for Large Models or Datasets in Supervised Fine-tuning</title>
      <author><first>Shuhe</first><last>Wang</last></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Jiwei</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <author><first>Chen</first><last>Guo</last></author>
      <pages>4953-4967</pages>
      <abstract>Packing, initially utilized in the pre-training phase, is an optimization technique designed to maximize hardware resource efficiency by combining different training sequences to fit the model’s maximum input length. Although it has demonstrated effectiveness during pre-training, there remains a lack of comprehensive analysis for the supervised fine-tuning (SFT) stage on the following points: (1) whether packing can effectively enhance training efficiency while maintaining performance, (2) the suitable size of the model and dataset for fine-tuning with the packing method, and (3) whether packing unrelated or related training samples might cause the model to either excessively disregard or over-rely on the context.In this paper, we perform extensive comparisons between SFT methods using padding and packing, covering SFT datasets ranging from 69K to 1.2M and models from 8B to 70B. This provides the first comprehensive analysis of the advantages and limitations of packing versus padding, as well as practical considerations for implementing packing in various training scenarios. Our analysis covers various benchmarks, including knowledge, reasoning, and coding, as well as GPT-based evaluations, time efficiency, and other fine-tuning parameters. We also open-source our code for fine-tuning and evaluation and provide checkpoints fine-tuned on datasets of different sizes, aiming to advance future research on packing methods.</abstract>
      <url hash="70422cfd">2025.findings-acl.256</url>
      <bibkey>wang-etal-2025-packing</bibkey>
    </paper>
    <paper id="257">
      <title>Better Red Teaming via Searching with Large Language Model</title>
      <author><first>Yongkang</first><last>Chen</last></author>
      <author><first>Chongyang</first><last>Zhao</last><affiliation>Institute of Systems Engineering</affiliation></author>
      <author><first>Jianwentian</first><last>Jianwentian</last></author>
      <author><first>Guiling</first><last>Cao</last></author>
      <author><first>Hu</first><last>Li</last></author>
      <author><first>Xiaohui</first><last>Kuang</last></author>
      <pages>4968-4984</pages>
      <abstract>The safe deployment of large language models (LLMs) necessitates comprehensive safety evaluations through red teaming. However, existing methods face challenges in managing semantic intricacies and optimizing the efficiency of the search process. To overcome these limitations, we propose Better Red Teaming (BRT)—an innovative framework that reconceptualizes test case generation as a strategic planning problem, leveraging Monte Carlo Tree Search (MCTS). A notable advancement of our approach is the incorporation of LLMs as world models, enabling the prediction of state transitions and simulation of long-term outcomes throughout the search process. By jointly optimizing objectives related to conditional mutual information and diversity, we improve the world model’s capacity to follow actions while maintaining output diversity. Extensive experiments conducted across a range of LLM architectures demonstrate that BRT achieves state-of-the-art attack success rates without sacrificing computational efficiency.</abstract>
      <url hash="177e2a98">2025.findings-acl.257</url>
      <bibkey>chen-etal-2025-better</bibkey>
    </paper>
    <paper id="258">
      <title><fixed-case>A</fixed-case>da<fixed-case>V</fixed-case>: Adaptive Text-visual Redirection for Vision-Language Models</title>
      <author><first>Jiayi</first><last>Han</last><affiliation>Inspur Group Co, Ltd</affiliation></author>
      <author><first>Liang</first><last>Du</last><affiliation>Tencent</affiliation></author>
      <author><first>Yiwen</first><last>Wu</last></author>
      <author><first>Guanming</first><last>Liang</last></author>
      <author><first>Xiangguo</first><last>Zhou</last></author>
      <author><first>Weibo</first><last>Zheng</last><affiliation>Inspur Genersoft Co., Ltd.</affiliation></author>
      <author><first>Donghong</first><last>Han</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zixun</first><last>Sun</last><affiliation>Tencent</affiliation></author>
      <pages>4985-4997</pages>
      <abstract>The success of Vision-Language Models (VLMs) often relies on high-resolution schemes that preserve image details, while these approaches also generate an excess of visual tokens, leading to a substantial decrease in model efficiency. A typical VLM includes a visual encoder, a text encoder, and an LLM. Recent studies suggest pruning visual tokens based on visual and textual priors to accelerate VLMs without additional training costs. However, these methods often overlook prompt semantics or suffer from biased self-attention in the LLM. Inspired by the efficient mechanisms of the human brain for multimodal understanding, we introduce AdaV, a novel training-free visual token pruning method. By emulating the neural pathways that preprocess visual and auditory information before the reasoning stage, we shift text-guided visual attention redirection to the pre-LLM stage, which reduces biased token pruning and enhances model robustness with a limited visual token budget. A Self-adaptive Cross-modality Attention Redirection (SCAR) module is further proposed that effectively merges and redirects visual attention with text-to-image attention. Extensive experiments on seven challenging benchmarks demonstrate that our AdaV achieves SOTA performance in training-free VLM acceleration and can be plug-and-play on various VLMs. We plan to open-source the code upon publication.</abstract>
      <url hash="65c80937">2025.findings-acl.258</url>
      <bibkey>han-etal-2025-adav</bibkey>
    </paper>
    <paper id="259">
      <title><fixed-case>M</fixed-case>ega<fixed-case>A</fixed-case>gent: A Large-Scale Autonomous <fixed-case>LLM</fixed-case>-based Multi-Agent System Without Predefined <fixed-case>SOP</fixed-case>s</title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Tianyu</first><last>Wang</last></author>
      <author><first>Zhenheng</first><last>Tang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qinbin</first><last>Li</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Nuo</first><last>Chen</last><affiliation>National University of Singapore and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Jingsheng</first><last>Liang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Bingsheng</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4998-5036</pages>
      <abstract>LLM-based multi-agent systems (MAS) have shown promise in tackling complex tasks. However, existing solutions often suffer from limited agent coordination and heavy reliance on predefined Standard Operating Procedures (SOPs), which demand extensive human input. To address these limitations, we propose <i>MegaAgent</i>, a large-scale autonomous LLM-based multi-agent system. <i>MegaAgent</i> generates agents based on task complexity and enables dynamic task decomposition, parallel execution, efficient communication, and comprehensive system monitoring of agents. In evaluations, <i>MegaAgent</i> demonstrates exceptional performance, successfully developing a Gobang game within 800 seconds and scaling up to 590 agents in a national policy simulation to generate multi-domain policies. It significantly outperforms existing systems, such as MetaGPT, in both task completion efficiency and scalability. By eliminating the need for predefined SOPs, <i>MegaAgent</i> demonstrates exceptional scalability and autonomy, setting a foundation for advancing true autonomy in MAS.</abstract>
      <url hash="8b8e2817">2025.findings-acl.259</url>
      <bibkey>wang-etal-2025-megaagent</bibkey>
    </paper>
    <paper id="260">
      <title>Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment</title>
      <author><first>Xiaotian</first><last>Zhang</last></author>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>5037-5049</pages>
      <abstract>Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.</abstract>
      <url hash="5013b6af">2025.findings-acl.260</url>
      <bibkey>zhang-etal-2025-persona</bibkey>
    </paper>
    <paper id="261">
      <title>A Self-Distillation Recipe for Neural Machine Translation</title>
      <author><first>Hongfei</first><last>Xu</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Zhuofei</first><last>Liang</last></author>
      <author><first>Qiuhui</first><last>Liu</last><affiliation>China Mobile Online Services</affiliation></author>
      <author><first>Lingling</first><last>Mu</last></author>
      <pages>5050-5064</pages>
      <abstract>Self-distillation distills the deeper sub-networks to the shallower sub-networks without using an extra teacher model, and has been proven effective in improving the performance of a series of computer vision tasks. In this paper, we study the representation-based self-distillation methods for Neural Machine Translation (NMT) considering the efficiency issue with a large vocabulary. We present a rank-order augmented Pearson correlation loss and an iterative distillation method to prevent the discrepancy of predictions between the student and a stronger teacher from disturbing the training. To prevent the teacher from misleading the student’s learning, we utilize a warm-up strategy and present a gradient adaption method to scale down or zero the Knowledge Distillation (KD) gradients which are opposite to the translation. Experiments show that our method can lead to significant improvements over the strong Transformer baseline on low/middle/high-resource tasks, obtaining comparable performance to previous MT KD studies without pre-training a teacher. Deeper Transformer experiments show that our method can lead to comparable or better performance with fewer layers.</abstract>
      <url hash="2c68023e">2025.findings-acl.261</url>
      <bibkey>xu-etal-2025-self-distillation</bibkey>
    </paper>
    <paper id="262">
      <title><fixed-case>B</fixed-case>lock<fixed-case>P</fixed-case>runer: Fine-grained Pruning for Large Language Models</title>
      <author><first>Longguang</first><last>Zhong</last></author>
      <author><first>Fanqi</first><last>Wan</last></author>
      <author><first>Ruijun</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xiaojun</first><last>Quan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Liangzhi</first><last>Li</last><affiliation>Meetyou AI Lab and Qufu Normal University</affiliation></author>
      <pages>5065-5080</pages>
      <abstract>With the rapid growth in the size and complexity of large language models (LLMs), the costs associated with their training and inference have escalated significantly. Research indicates that certain layers in LLMs harbor substantial redundancy, and pruning these layers has minimal impact on the overall performance. While various layer pruning methods have been developed based on this insight, they generally overlook the finer-grained redundancies within the layers themselves. In this paper, we delve deeper into the architecture of LLMs and demonstrate that finer-grained pruning can be achieved by targeting redundancies in multi-head attention (MHA) and multi-layer perceptron (MLP) blocks. We propose a novel, training-free structured pruning approach called BlockPruner. Unlike existing layer pruning methods, BlockPruner segments each Transformer layer into MHA and MLP blocks. It then assesses the importance of these blocks using perplexity measures and applies a heuristic search for iterative pruning. We applied BlockPruner to LLMs of various sizes and architectures and validated its performance across a wide range of downstream tasks. Experimental results show that BlockPruner achieves more granular and effective pruning compared to state-of-the-art baselines.</abstract>
      <url hash="44ece2ac">2025.findings-acl.262</url>
      <bibkey>zhong-etal-2025-blockpruner</bibkey>
    </paper>
    <paper id="263">
      <title>Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</title>
      <author><first>Yuchen</first><last>Wen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Keping</first><last>Bi</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Chen</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>5081-5097</pages>
      <abstract>As large language models (LLMs) become an important way of information access, there have been increasing concerns that LLMs may intensify the spread of unethical content, including implicit bias that hurts certain populations without explicit harmful words. In this paper, we conduct a rigorous evaluation of LLMs’ implicit bias towards certain demographics by attacking them from a psychometric perspective to elicit agreements to biased viewpoints. Inspired by psychometric principles in cognitive and social psychology, we propose three attack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the corresponding attack instructions, we built two benchmarks: (1) a bilingual dataset with biased statements covering four bias types (2.7K instances) for extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning nine common bias types (12.7K instances) for comprehensive evaluation. Extensive evaluation of popular commercial and open-source LLMs shows that our methods can elicit LLMs’ inner bias more effectively than competitive baselines. Our attack methodology and benchmarks offer an effective means of assessing the ethical risks of LLMs, driving progress toward greater accountability in their development.</abstract>
      <url hash="77fb4221">2025.findings-acl.263</url>
      <bibkey>wen-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="264">
      <title><fixed-case>L</fixed-case>ong<fixed-case>C</fixed-case>ite: Enabling <fixed-case>LLM</fixed-case>s to Generate Fine-grained Citations in Long-Context <fixed-case>QA</fixed-case></title>
      <author><first>Jiajie</first><last>Zhang</last></author>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Xin</first><last>Lv</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Wanjun</first><last>Gu</last></author>
      <author><first>Danqing</first><last>Liu</last></author>
      <author><first>Minhao</first><last>Zou</last></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Zhipu AI and Tsinghua University</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ling</first><last>Feng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>5098-5122</pages>
      <abstract>Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering various questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to the potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations on the fly, thereby improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs’ performance in long-context question answering with citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically construct long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the constructed dataset, successfully enabling the generation of accurate responses and fine-grained citations in one pass. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o. We also discover that SFT with citation information can further improve the correctness of model responses compared to standard long-context SFT.</abstract>
      <url hash="d38ebd80">2025.findings-acl.264</url>
      <bibkey>zhang-etal-2025-longcite</bibkey>
    </paper>
    <paper id="265">
      <title>An Empirical Study of Group Conformity in Multi-Agent Systems</title>
      <author><first>Min</first><last>Choi</last><affiliation>Kim &amp; Chang</affiliation></author>
      <author><first>Keonwoo</first><last>Kim</last><affiliation>Kim &amp; Chang</affiliation></author>
      <author><first>Sungwon</first><last>Chae</last><affiliation>Kim &amp; Chang</affiliation></author>
      <author><first>Sangyeop</first><last>Baek</last></author>
      <pages>5123-5139</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.</abstract>
      <url hash="39ee889d">2025.findings-acl.265</url>
      <bibkey>choi-etal-2025-empirical</bibkey>
    </paper>
    <paper id="266">
      <title>Combining the Best of Both Worlds: A Method for Hybrid <fixed-case>NMT</fixed-case> and <fixed-case>LLM</fixed-case> Translation</title>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yuanchang</first><last>Luo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>5140-5148</pages>
      <abstract>Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as less LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with less LLM usage, demonstrating effectiveness of our decider.</abstract>
      <url hash="6eb37a10">2025.findings-acl.266</url>
      <bibkey>wu-etal-2025-combining</bibkey>
    </paper>
    <paper id="267">
      <title><fixed-case>ASPO</fixed-case>: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning</title>
      <author><first>Yeyuan</first><last>Wang</last></author>
      <author><first>Dehong</first><last>Gao</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Rujiao</first><last>Long</last></author>
      <author><first>Lei</first><last>Yi</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Linbo</first><last>Jin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Libin</first><last>Yang</last></author>
      <author><first>Xiaoyan</first><last>Cai</last></author>
      <pages>5149-5160</pages>
      <abstract>Direct Preference Optimization (DPO) has gained significant attention for its simplicity and computational efficiency in aligning large language models (LLMs). Recent advancements have extended DPO to multimodal scenarios, achieving strong performance. However, traditional DPO relies on binary preference optimization, rewarding or penalizing entire responses without considering fine-grained segment correctness, leading to suboptimal solutions. The root of this issue lies in the absence of fine-grained supervision during the optimization process. To address this, we propose Adaptive Sentence-level Preference Optimization (ASPO), which evaluates individual sentences for more precise preference optimization. By dynamically calculating adaptive rewards at the sentence level based on model predictions, ASPO enhances response content assessment without additional models or parameters. This significantly improves the alignment of multimodal features. Extensive experiments show that ASPO substantially enhances the overall performance of multimodal models.</abstract>
      <url hash="f8ac0bae">2025.findings-acl.267</url>
      <bibkey>wang-etal-2025-aspo</bibkey>
    </paper>
    <paper id="268">
      <title><fixed-case>N</fixed-case>ovel<fixed-case>CR</fixed-case>: A Large-Scale Bilingual Dataset Tailored for Long-Span Coreference Resolution</title>
      <author><first>MeiHan</first><last>Tong</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <pages>5161-5173</pages>
      <abstract>Coreference resolution (CR) endeavors to match pronouns, noun phrases, etc. with their referent entities, acting as an important step for deep text understanding. Presently available CR datasets are either small in scale or restrict coreference resolution to a limited text span. In this paper, we present NovelCR, a large-scale bilingual benchmark designed for long-span coreference resolution. NovelCR features extensive annotations, including 148k mentions in NovelCR-en and 311k mentions in NovelCR-zh. Moreover, the dataset is notably rich in long-span coreference pairs, with 85% of pairs in NovelCR-en and 83% in NovelCR-zh spanning across three or more sentences. Experiments on NovelCR reveal a large gap between state-of-the-art baselines and human performance, highlighting that NovelCR remains an open issue.</abstract>
      <url hash="c22a38d9">2025.findings-acl.268</url>
      <bibkey>tong-wang-2025-novelcr</bibkey>
    </paper>
    <paper id="269">
      <title>Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models</title>
      <author><first>Huangyw</first><last>Huangyw</last></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Ning</first><last>Cheng</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Zhitao</first><last>Li</last><affiliation>Thoughtworks</affiliation></author>
      <author><first>Shaojun</first><last>Wang</last><affiliation>PAII Inc.</affiliation></author>
      <author><first>Jing</first><last>Xiao</last><affiliation>Pingan Group</affiliation></author>
      <pages>5174-5193</pages>
      <abstract>Large language models (LLMs) often exhibit Context Faithfulness Hallucinations, where outputs deviate from retrieved information due to incomplete context integration. Our analysis reveals a strong correlation between token-level uncertainty and hallucinations. We hypothesize that attention mechanisms inherently encode context utilization signals, supported by probing analysis. Based on these insights, we propose **Dynamic Attention-Guided Context Decoding (DAGCD)**, a lightweight framework that leverages attention distributions and uncertainty signals in a single-pass decoding. Experiments on open-book QA datasets demonstrate DAGCD’s effectiveness, yielding significant improvements in faithfulness and robustness while preserving computational efficiency.</abstract>
      <url hash="da4907b2">2025.findings-acl.269</url>
      <bibkey>huangyw-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="270">
      <title>Exploring the Choice Behavior of Large Language Models</title>
      <author><first>Weidong</first><last>Wu</last></author>
      <author><first>Qinlin</first><last>Zhao</last><affiliation>Research, Microsoft and University of Science and Technology of China</affiliation></author>
      <author><first>Hao</first><last>Chen</last><affiliation>Nankai University</affiliation></author>
      <author><first>Lexin</first><last>Zhou</last></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hong</first><last>Xie</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5194-5214</pages>
      <abstract>Large Language Models (LLMs) are increasingly deployed as human assistants across various domains where they help to make choices. However, the mechanisms behind LLMs’ choice behavior remain unclear, posing risks in safety-critical situations. Inspired by the intrinsic and extrinsic motivation framework within the classic human behavioral model of Self-Determination Theory and its established research methodologies, we investigate the factors influencing LLMs’ choice behavior by constructing a virtual QA platform that includes three different experimental conditions, with four models from GPT and Llama series participating in repeated experiments. Our findings indicate that LLMs’ behavior is influenced not only by intrinsic attention bias but also by extrinsic social influence, exhibiting patterns similar to the Matthew effect and Conformity. We distinguish independent pathways of these two factors in LLMs’ behavior by self-report. This work provides new insights into understanding LLMs’ behavioral patterns, exploring their human-like characteristics.</abstract>
      <url hash="0f7ab553">2025.findings-acl.270</url>
      <bibkey>wu-etal-2025-exploring-choice</bibkey>
    </paper>
    <paper id="271">
      <title>On-Policy Self-Alignment with Fine-grained Knowledge Feedback for Hallucination Mitigation</title>
      <author><first>Xueru</first><last>Wen</last></author>
      <author><first>Jie</first><last>Lou</last></author>
      <author><first>Xinyu</first><last>Lu</last></author>
      <author><first>Yuqiu</first><last>Ji</last></author>
      <author><first>Xinyan</first><last>Guan</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Debing</first><last>Zhang</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>5215-5231</pages>
      <abstract>Hallucination occurs when large language models exhibit behavior that deviates from the boundaries of their knowledge during response generation. To address this critical issue, previous learning-based methods attempt to finetune models but are limited by off-policy sampling and coarse-grained feedback. In this paper, we present <i>Reinforcement Learning for Hallucination</i> (RLFH), an on-policy self-alignment approach that enables LLMs to actively explore their knowledge boundaries and self-correct generation behavior through fine-grained feedback signals. RLFH introduces a self-assessment framework where the policy serves as its own judge. Through this framework, responses are automatically decomposed into atomic facts and their truthfulness and informativeness are assessed against external knowledge sources. The resulting fine-grained feedback at the statement level are then converted into token-level dense reward signals. This enables online reinforcement learning to achieve precise and timely optimization without human intervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography benchmarks validate RLFH’s effectiveness in hallucination mitigation.</abstract>
      <url hash="f717c415">2025.findings-acl.271</url>
      <bibkey>wen-etal-2025-policy</bibkey>
    </paper>
    <paper id="272">
      <title>From Phrases to Subgraphs: Fine-Grained Semantic Parsing for Knowledge Graph Question Answering</title>
      <author><first>Yurun</first><last>Song</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Xiangqing</first><last>Shen</last></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>5232-5246</pages>
      <abstract>The recent emergence of large language models (LLMs) has brought new opportunities to knowledge graph question answering (KGQA), but also introduces challenges such as semantic misalignment and reasoning noise. Semantic parsing (SP), previously a mainstream approach for KGQA, enables precise graph pattern matching by mapping natural language queries to executable logical forms. However, it faces limitations in scalability and generalization, especially when dealing with complex, multi-hop reasoning tasks.In this work, we propose a Fine-Grained Semantic Parsing (FGSP) framework for KGQA. Our framework constructs a fine-grained mapping library via phrase-level segmentation of historical question-logical form pairs, and performs online retrieval and fusion of relevant subgraph fragments to answer complex queries. This fine-grained, compositional approach ensures tighter semantic alignment between questions and knowledge graph structures, enhancing both interpretability and adaptability to diverse query types. Experimental results on two KGQA benchmarks demonstrate the effectiveness of FGSP, with a notable 18.5% relative F1 performance improvement over the SOTA on the complex multi-hop CWQ dataset. Our code is available at https://github.com/NUSTM/From-Phrases-to-Subgraphs.</abstract>
      <url hash="0b5a2b08">2025.findings-acl.272</url>
      <bibkey>song-etal-2025-phrases</bibkey>
    </paper>
    <paper id="273">
      <title><fixed-case>S</fixed-case>table<fixed-case>T</fixed-case>ool<fixed-case>B</fixed-case>ench-<fixed-case>M</fixed-case>irror<fixed-case>API</fixed-case>: Modeling Tool Environments as Mirrors of 7,000+ Real-World <fixed-case>API</fixed-case>s</title>
      <author id="zhicheng-guo-tsinghua"><first>Zhicheng</first><last>Guo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Sijie</first><last>Cheng</last></author>
      <author><first>Yuchen</first><last>Niu</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Sicheng</first><last>Zhou</last></author>
      <author><first>Wenbing</first><last>Huang</last><affiliation>Renmin University of China</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>5247-5270</pages>
      <abstract>The rapid advancement of large language models (LLMs) has spurred significant interest in tool learning, where LLMs are augmented with external tools to tackle complex tasks. However, existing tool environments face challenges in balancing stability, scale, and realism, particularly for benchmarking purposes. To address this, we propose MirrorAPI, a novel framework that trains specialized LLMs to accurately simulate real API responses, effectively acting as “mirrors” to tool environments. Using a comprehensive dataset of request-response pairs from 7,000+ APIs, we employ supervised fine-tuning and chain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves superior accuracy and stability compared to state-of-the-art methods, as demonstrated by its performance on the newly constructed MirrorAPI-Bench and its integration into StableToolBench.</abstract>
      <url hash="1e7bdf46">2025.findings-acl.273</url>
      <bibkey>guo-etal-2025-stabletoolbench</bibkey>
    </paper>
    <paper id="274">
      <title><fixed-case>C</fixed-case>laim<fixed-case>PKG</fixed-case>: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized <fixed-case>LLM</fixed-case></title>
      <author><first>Hoang</first><last>Pham</last><affiliation>Viettel AI &amp; Data Service Center</affiliation></author>
      <author><first>Thanh-Do</first><last>Nguyen</last></author>
      <author><first>Khac-Hoai Nam</first><last>Bui</last><affiliation>Viettel Group</affiliation></author>
      <pages>5271-5290</pages>
      <abstract>Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones.</abstract>
      <url hash="318e6eb0">2025.findings-acl.274</url>
      <bibkey>pham-etal-2025-claimpkg</bibkey>
    </paper>
    <paper id="275">
      <title><fixed-case>T</fixed-case>ri<fixed-case>E</fixed-case>mbed: Bridge the Gap between Text and Token Indices with Embedding Reparameterization</title>
      <author><first>Baizhou</first><last>Huang</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>5291-5297</pages>
      <abstract>The current paradigm of language modeling is a two-stage pipeline that first transforms raw text to token indices, where the distribution is then estimated. It inherently discards linguistic relations between tokens during tokenization, creating a fundamental gap. To address this, we propose <b>TriEmbed</b>, a reparameterization method for embeddings that incorporates the morphological relationships inherent in subword tokenizer algorithms. Specifically, by organizing the vocabulary into a Trie structure, we can encode these relations and reparametrize the embeddings, facilitating the recovery of other linguistic relationships during training. Empirical results across various settings demonstrate that TriEmbed outperforms conventional embeddings from the perspective of scaling, while offering more linguistically informative token embeddings.</abstract>
      <url hash="a2c428a4">2025.findings-acl.275</url>
      <bibkey>huang-wan-2025-triembed</bibkey>
    </paper>
    <paper id="276">
      <title>Chain of Methodologies: Scaling Test Time Computation without Training</title>
      <author id="cong-liu"><first>Cong</first><last>Liu</last></author>
      <author><first>Jie</first><last>Wu</last><affiliation>Temple University</affiliation></author>
      <author><first>Weigang</first><last>Wu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Wei-Shi</first><last>Zheng</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>5298-5312</pages>
      <abstract>Large Language Models (LLMs) often struggle with complex reasoning tasks due to insufficient in-depth insights in their training data, which are frequently absent in publicly available documents. This paper introduces the Chain of Methodologies (CoM), a simple and innovative iterative prompting framework designed to build structured reasoning processes by injecting human methodological insights, thereby enabling LLMs to perform long and effective reasoning for complex tasks. Assuming that LLMs possess certain metacognitive abilities, CoM leverages user-defined methodologies to stimulate the cognitive insights that LLMs have learned implicitly from training data. Experimental results indicate that CoM outperforms competitive baselines, highlighting the potential of training-free prompting methods as general solutions for complex reasoning tasks and the possibility of incorporating human-like methodological insights to bridge the gap to human-level reasoning.</abstract>
      <url hash="a71717e4">2025.findings-acl.276</url>
      <bibkey>liu-etal-2025-chain-methodologies</bibkey>
    </paper>
    <paper id="277">
      <title>A Survey on Personalized <fixed-case>A</fixed-case>lignment—<fixed-case>T</fixed-case>he Missing Piece for Large Language Models in Real-World Applications</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Junfei</first><last>Wu</last></author>
      <author><first>Jia-Nan</first><last>Li</last></author>
      <author><first>Chuanqi</first><last>Cheng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <pages>5313-5333</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users’ diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment—a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.</abstract>
      <url hash="2e03e142">2025.findings-acl.277</url>
      <bibkey>guan-etal-2025-survey</bibkey>
    </paper>
    <paper id="278">
      <title><fixed-case>S</fixed-case>u<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Subspace Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</title>
      <author><first>Chenhao</first><last>Ding</last></author>
      <author><first>Jiangyang</first><last>Li</last></author>
      <author><first>SongLin</first><last>Dong</last></author>
      <author><first>Xinyuan</first><last>Gao</last></author>
      <author><first>Yuhang</first><last>He</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Yihong</first><last>Gong</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>5334-5349</pages>
      <abstract>As the scale of large language models (LLMs) grows and natural language tasks become increasingly diverse, Parameter-Efficient Fine-Tuning (PEFT) has become the standard paradigm for fine-tuning LLMs. Among PEFT methods, LoRA is widely adopted for not introducing additional inference overhead. However, existing LoRA’s shared parameter space paradigm introduces parameter interference, leading to a gap in generalization performance for specific tasks compared to full fine-tuning. To address this issue, we propose a parameter-separated low-rank adapter, called Subspace Low-Rank Adaptation (SuLoRA). The core idea of SuLoRA is to account for task differences by decomposing LoRA’s parameter matrix into multiple independent subspaces and assigning them differentially to distinct tasks. This prevents interference across tasks and enhances the effectiveness of low-rank adaptation. Additionally, SuLoRA achieves higher rank expansion by freezing the A matrix, further improving generalization capability. We conduct extensive experiments on various NLP tasks, demonstrating that SuLoRA significantly outperforms LoRA in trainable parameter efficiency and overall model performance. Furthermore, we validate SuLoRA’s effectiveness in domain generalization and multi-modal tasks, showcasing its strong generalization ability.</abstract>
      <url hash="9873d1b9">2025.findings-acl.278</url>
      <bibkey>ding-etal-2025-sulora</bibkey>
    </paper>
    <paper id="279">
      <title><fixed-case>MIR</fixed-case>e: Enhancing Multimodal Queries Representation via Fusion-Free Modality Interaction for Multimodal Retrieval</title>
      <author><first>Yeong-Joon</first><last>Ju</last></author>
      <author><first>Ho-Joong</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Seong-Whan</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>5350-5363</pages>
      <abstract>Recent multimodal retrieval methods have endowed text-based retrievers with multimodal capabilities by utilizing pre-training strategies for visual-text alignment. They often directly fuse the two modalities for cross-reference during the alignment to understand multimodal queries. However, existing methods often overlook crucial visual information due to a text-dominant issue, which overly depends on text-driven signals. In this paper, we introduce MIRe, a retrieval framework that achieves modality interaction without fusing textual features during the alignment. Our method allows the textual query to attend to visual embeddings while not feeding text-driven signals back into the visual representations. Additionally, we construct a pre-training dataset for multimodal query retrieval by transforming concise question-answer pairs into extended passages. Our experiments demonstrate that our pre-training strategy significantly enhances the understanding of multimodal queries, resulting in strong performance across four multimodal retrieval benchmarks under zero-shot settings. Moreover, our ablation studies and analyses explicitly verify the effectiveness of our framework in mitigating the text-dominant issue. Our code is publicly available: https://github.com/yeongjoonJu/MIRe</abstract>
      <url hash="601ea374">2025.findings-acl.279</url>
      <bibkey>ju-etal-2025-mire</bibkey>
    </paper>
    <paper id="280">
      <title>Correcting on Graph: Faithful Semantic Parsing over Knowledge Graphs with Large Language Models</title>
      <author><first>Ruilin</first><last>Zhao</last></author>
      <author><first>Feng</first><last>Zhao</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hong</first><last>Zhang</last></author>
      <pages>5364-5376</pages>
      <abstract>Complex multi-hop questions often require comprehensive retrieval and reasoning. As a result, effectively parsing such questions and establishing an efficient interaction channel between large language models (LLMs) and knowledge graphs (KGs) is essential for ensuring reliable reasoning. In this paper, we present a novel semantic parsing framework Correcting on Graph (CoG), aiming to establish faithful logical queries that connect LLMs and KGs. We first propose a structured knowledge decoding that enables the LLM to generate fact-aware logical queries during inference, while leveraging its parametric knowledge to fill in the blank intermediate entities. Then, we introduce a knowledge path correction that combines the logical query with KGs to correct hallucination entities and path deficiencies in the generated content, ensuring the reliability and comprehensiveness of the retrieved knowledge. Extensive experiments demonstrate that CoG outperforms the state-of-the-art KGQA methods on two knowledge-intensive question answering benchmarks. CoG achieves a high answer hit rate and exhibits competitive F1 performance for complex multi-hop questions.</abstract>
      <url hash="cc43540e">2025.findings-acl.280</url>
      <bibkey>zhao-etal-2025-correcting</bibkey>
    </paper>
    <paper id="281">
      <title><fixed-case>COPR</fixed-case>: Continual Human Preference Learning via Optimal Policy Regularization</title>
      <author><first>Han</first><last>Zhang</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yu</first><last>Lei</last><affiliation>Peng Cheng Laboratory, Shenzhen, China</affiliation></author>
      <author><first>Yuanzhao</first><last>Zhai</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yehong</first><last>Zhang</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Bin</first><last>Liang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>5377-5398</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) is effective for aligning Large Language Models (LLMs) with human preferences. However, RLHF’s complex process limits its ability to continually learn human feedback, making it impractical for real-world applications where the deployed model continuously receives feedback from users. The non-RL-based method, such as Direct Preference Optimization (DPO), is not primitively favorable for Continual Learning (CL). We observe that when combined with Experiment Relay (ER) for CL, DPO tends to significantly widen the gap in the probability of human-preferred and dispreferred responses. Consequently, this diminishes the diversity in model generation, potentially leading to model collapse. To overcome the above challenges, we propose the Continual Optimal Policy Regularization (COPR), a novel non-RL offline method to convert the historical optimal policies into optimization constraints when continually learning new preferences. We first derive a moderate reward function from the pairwise ranking loss and then use the moderate reward to calculate a new sampling distribution to construct novel learning objectives and constraints. We also provide formal proof of the learnability of COPR. The experimental results show that COPR outperforms strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4 evaluations and human assessment.</abstract>
      <url hash="7a923101">2025.findings-acl.281</url>
      <bibkey>zhang-etal-2025-copr</bibkey>
    </paper>
    <paper id="282">
      <title>Robust Preference Optimization via Dynamic Target Margins</title>
      <author><first>Jie</first><last>Sun</last></author>
      <author><first>Junkang</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiancan</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhibo</first><last>Zhu</last><affiliation>Ant Group, China</affiliation></author>
      <author><first>Xingyu</first><last>Lu</last></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Lintao</first><last>Ma</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5399-5416</pages>
      <abstract>The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose <tex-math>\gamma</tex-math>-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, <tex-math>\gamma</tex-math>-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, <tex-math>\gamma</tex-math>-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, <tex-math>\gamma</tex-math>-PO achieves an average 4.4% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, <tex-math>\gamma</tex-math>-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO.</abstract>
      <url hash="a55c1b79">2025.findings-acl.282</url>
      <bibkey>sun-etal-2025-robust</bibkey>
    </paper>
    <paper id="283">
      <title><fixed-case>A</fixed-case>da<fixed-case>R</fixed-case>e<fixed-case>T</fixed-case>a<fixed-case>K</fixed-case>e: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding</title>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Qingyi</first><last>Si</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shiyu</first><last>Zhu</last></author>
      <author><first>Jianlong</first><last>Wu</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Li</first><last>Cao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen) and Shandong University</affiliation></author>
      <pages>5417-5432</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have revolutionized video understanding, yet are still limited by context length when processing long videos. Recent methods compress videos by leveraging visual redundancy uniformly, yielding promising results. Nevertheless, our quantitative analysis shows that redundancy varies significantly across time and model layers, necessitating a more flexible compression strategy. We propose **AdaReTaKe**, a training-free method that flexibly reduces visual redundancy by allocating compression ratios among time and layers with theoretical guarantees. Integrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity from 256 to 2048 frames while preserving critical information. Experiments on VideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe outperforms existing methods by 2.3% and 2.8% for 7B and 72B models, respectively, with even greater improvements of 5.9% and 6.0% on the longest LVBench.</abstract>
      <url hash="3c0e340c">2025.findings-acl.283</url>
      <bibkey>wang-etal-2025-adaretake</bibkey>
    </paper>
    <paper id="284">
      <title>Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges</title>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenyu</first><last>Huang</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yuanhao</first><last>Xi</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Jianqiao</first><last>Lu</last></author>
      <author><first>Huan</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Hu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5433-5453</pages>
      <abstract>Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose DialogTool, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) <i>tool creation</i>; 2) <i>tool utilization</i>: tool awareness, tool selection, tool execution; and 3) <i>role-consistent response</i>: response generation and role play. Furthermore, we build VirtualMobile – an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons .</abstract>
      <url hash="4c926f70">2025.findings-acl.284</url>
      <bibkey>wang-etal-2025-rethinking-stateful</bibkey>
    </paper>
    <paper id="285">
      <title>Open-Set Living Need Prediction with Large Language Models</title>
      <author><first>Xiaochong</first><last>Lan</last></author>
      <author><first>Jie</first><last>Feng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last></author>
      <author><first>Chen</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiahuan</first><last>Lei</last></author>
      <author><first>Xinleishi</first><last>Xinleishi</last></author>
      <author><first>Hengliang</first><last>Luo</last></author>
      <author><first>Yong</first><last>Li</last></author>
      <pages>5454-5472</pages>
      <abstract>Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslow’s hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment.</abstract>
      <url hash="75131f83">2025.findings-acl.285</url>
      <bibkey>lan-etal-2025-open</bibkey>
    </paper>
    <paper id="286">
      <title>Improve Rule Retrieval and Reasoning with Self-Induction and Relevance <fixed-case>R</fixed-case>e<fixed-case>E</fixed-case>stimate</title>
      <author><first>Ziyang</first><last>Huang</last></author>
      <author><first>Wangtao</first><last>Sun</last></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>5473-5488</pages>
      <abstract>This paper systematically addresses the challenge of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R<tex-math>^3</tex-math>), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.</abstract>
      <url hash="6ee9fe0a">2025.findings-acl.286</url>
      <bibkey>huang-etal-2025-improve</bibkey>
    </paper>
    <paper id="287">
      <title>Beyond Words: Integrating Theory of Mind into Conversational Agents for Human-Like Belief, Desire, and Intention Alignment</title>
      <author><first>Mehdi</first><last>Jafari</last></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Hao</first><last>Xue</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Flora D.</first><last>Salim</last><affiliation>University of New South Wales</affiliation></author>
      <pages>5489-5508</pages>
      <abstract>Natural language interaction has long served as the primary medium through which humans exchange ideas. A key enabler of this communication is the human capacity for Theory of Mind (ToM)—the ability to infer and align with the mental states of others. ToM is usually modeled as components of desires, beliefs, and intentions. Research in linguistics and psychology has shown that people oftentimes reveal their ToM through pragmatic aspects of language. Considering the advancements in natural language generation and perception that Large Language Models (LLMs) have made in recent years, a critical question arises in relation to ToM: can LLM-powered agents develop similar abilities for inferring mental states during natural language communication? This study investigates the extent to which open-source LLaMA models can represent and retain ToM-related constructs, and whether these internal representations contribute to a coherent mental state modeling in a given conversation. Additionally, we explore the potential for manipulating ToM-related information to generate more aligned responses. Empirical evaluations of LLaMA-3 models (3B and 8B) demonstrate that ToM-informed alignment improves response quality, achieving win rates of 63% and 67%, respectively. These findings suggest that integrating ToM principles can enhance alignment in LLM-based conversational agents. For further details, refer to the [code repository](https://github.com/cruiseresearchgroup/ToM_and_Alignment).</abstract>
      <url hash="9b643bb0">2025.findings-acl.287</url>
      <bibkey>jafari-etal-2025-beyond</bibkey>
    </paper>
    <paper id="288">
      <title>Multimodal Causal Reasoning Benchmark: Challenging Multimodal Large Language Models to Discern Causal Links Across Modalities</title>
      <author><first>Zhiyuan</first><last>Li</last></author>
      <author><first>Heng</first><last>Wang</last><affiliation>Sony R&amp;D and University of Sydney, University of Sydney</affiliation></author>
      <author><first>Dongnan</first><last>Liu</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Chaoyi</first><last>Zhang</last><affiliation>The University of Sydney</affiliation></author>
      <author><first>Ao</first><last>Ma</last></author>
      <author><first>Jieting</first><last>Long</last></author>
      <author><first>Weidong</first><last>Cai</last><affiliation>The University of Sydney</affiliation></author>
      <pages>5509-5533</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have showcased exceptional Chain-of-Thought (CoT) reasoning ability in complex textual inference tasks including causal reasoning. However, will these causalities remain straightforward when crucial hints hide in visual details? If not, what factors might influence cross-modal generalization? Whether we can effectively enhance their capacity for robust causal inference across both text and vision? Motivated by these, we introduce **MuCR** - a novel **Mu**ltimodal **C**ausal **R**easoning benchmark that leverages synthetic siamese images and text pairs to challenge MLLMs. Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess MLLMs’ comprehension abilities. Our experiments reveal that current MLLMs fall short in multimodal causal reasoning compared to their performance in purely textual settings. Additionally, we find that identifying visual cues across images is key to effective cross-modal generalization. Finally, we propose the **VcCoT** strategy that better highlights visual cues, and our results confirm its efficacy in enhancing multimodal causal reasoning.</abstract>
      <url hash="27fb9dd2">2025.findings-acl.288</url>
      <bibkey>li-etal-2025-multimodal-causal</bibkey>
    </paper>
    <paper id="289">
      <title>Context-Aware Hierarchical Merging for Long Document Summarization</title>
      <author><first>Litu</first><last>Ou</last></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>5534-5561</pages>
      <abstract>Hierarchical Merging is a technique commonly used to summarize very long texts (&gt;100K tokens) by breaking down the input into smaller sections, summarizing those sections individually, and then merging or combining those summaries into a final coherent summary. Although it helps address the limitations of large language models (LLMs) with fixed input length constraints, the recursive merging process can amplify LLM hallucinations, increasing the risk of factual inaccuracies. In this paper, we seek to mitigate hallucinations by enriching hierarchical merging with context from the source document. Specifically, we propose different approaches to contextual augmentation ranging from *replacing* intermediate summaries with relevant input context, to *refining* them while using the context as supporting evidence, and *aligning* them implicitly (via citations) to the input. Experimental results on datasets representing legal and narrative domains show that contextual augmentation consistently outperforms zero-shot and hierarchical merging baselines for the Llama 3.1 model family. Our analysis further reveals that refinement methods tend to perform best when paired with extractive summarization for identifying relevant input.</abstract>
      <url hash="bbf1e422">2025.findings-acl.289</url>
      <bibkey>ou-lapata-2025-context</bibkey>
    </paper>
    <paper id="290">
      <title><fixed-case>VCD</fixed-case>: A Dataset for Visual Commonsense Discovery in Images</title>
      <author><first>Xiangqing</first><last>Shen</last></author>
      <author><first>Fanfan</first><last>Wang</last></author>
      <author><first>Siwei</first><last>Wu</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>5562-5577</pages>
      <abstract>Visual commonsense plays a vital role in understanding and reasoning about the visual world. While commonsense knowledge bases like ConceptNet provide structured collections of general facts, they lack visually grounded representations. Scene graph datasets like Visual Genome, though rich in object-level descriptions, primarily focus on directly observable information and lack systematic categorization of commonsense knowledge. We present Visual Commonsense Dataset (VCD), a large-scale dataset containing over 100,000 images and 14 million object-commonsense pairs that bridges this gap. VCD introduces a novel three-level taxonomy for visual commonsense, integrating both Seen (directly observable) and Unseen (inferrable) commonsense across Property, Action, and Space aspects. Each commonsense is represented as a triple where the head entity is grounded to object bounding boxes in images, enabling scene-dependent and object-specific visual commonsense representation. To demonstrate VCD’s utility, we develop VCM, a generative model that combines a vision-language model with instruction tuning to discover diverse visual commonsense from images. Extensive evaluations demonstrate both the high quality of VCD and its value as a resource for advancing visually grounded commonsense understanding and reasoning. Our dataset and code will be released on https://github.com/NUSTM/VCD.</abstract>
      <url hash="daa450a6">2025.findings-acl.290</url>
      <bibkey>shen-etal-2025-vcd</bibkey>
    </paper>
    <paper id="291">
      <title>Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst</title>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shijue</first><last>Huang</last></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5578-5596</pages>
      <abstract>Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce <i>Self-Reasoning Language Model</i> (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model’s initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than +2.5 points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute +7.89 average improvement with 64 sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline .</abstract>
      <url hash="e297854c">2025.findings-acl.291</url>
      <bibkey>wang-etal-2025-self-reasoning</bibkey>
    </paper>
    <paper id="292">
      <title><fixed-case>H</fixed-case>yper<fixed-case>CRS</fixed-case>: Hypergraph-Aware Multi-Grained Preference Learning to Burst Filter Bubbles in Conversational Recommendation System</title>
      <author><first>Yongsen</first><last>Zheng</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Mingjie</first><last>Qian</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Guohua</first><last>Wang</last><affiliation>South China Agricultural University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Ziliang</first><last>Chen</last></author>
      <author><first>Mingzhi</first><last>Mao</last></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Kwok-Yan</first><last>Lam</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>5597-5608</pages>
      <abstract>The filter bubble is a notorious issue in Recommender Systems (RSs), characterized by users being confined to a limited corpus of information or content that strengthens and amplifies their pre-established preferences and beliefs. Most existing methods primarily aim to analyze filter bubbles in the relatively static recommendation environment. Nevertheless, the filter bubble phenomenon continues to exacerbate as users interact with the system over time. To address these issues, we propose a novel paradigm, Hypergraph-Aware Multi-Grained Preference Learning to Burst Filter Bubbles in Conversational Recommendation System (HyperCRS), aiming to burst filter bubbles by learning multi-grained user preferences during the dynamic user-system interactions via natural language conversations. HyperCRS develops Multi-Grained Hypergraph (user-, item-, and attribute-grained) to explore diverse relations and capture high-order connectivity. It employs Hypergraph-Empowered Policy Learning, which includes Multi-Grained Preference Modeling to model user preferences and Preference-based Decision Making to disrupt filter bubbles during user interactions. Extensive results on four publicly CRS-based datasets show that HyperCRS achieves new state-of-the-art performance, and the superior of bursting filter bubbles in the CRS.</abstract>
      <url hash="b5498c54">2025.findings-acl.292</url>
      <bibkey>zheng-etal-2025-hypercrs</bibkey>
    </paper>
    <paper id="293">
      <title>Is <fixed-case>LLM</fixed-case> an Overconfident Judge? Unveiling the Capabilities of <fixed-case>LLM</fixed-case>s in Detecting Offensive Language with Annotation Disagreement</title>
      <author><first>Junyu</first><last>Lu</last></author>
      <author><first>Kai</first><last>Ma</last></author>
      <author><first>Kaichun</first><last>Wang</last></author>
      <author><first>Kelaiti</first><last>Xiao</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Liang</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>5609-5626</pages>
      <abstract>Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.</abstract>
      <url hash="b00586f2">2025.findings-acl.293</url>
      <bibkey>lu-etal-2025-llm</bibkey>
    </paper>
    <paper id="294">
      <title>Language Repository for Long Video Understanding</title>
      <author><first>Kumara</first><last>Kahatapitiya</last><affiliation>Meta</affiliation></author>
      <author><first>Kanchana</first><last>Ranasinghe</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Jongwoo</first><last>Park</last></author>
      <author><first>Michael S</first><last>Ryoo</last><affiliation>Salesforce AI Research and Stony Brook University</affiliation></author>
      <pages>5627-5646</pages>
      <abstract>Language has become a prominent modality in computer vision with the rise of LLMs. Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. This becomes critical, especially in applications such as long-form video understanding. In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation. Our repository is updated iteratively based on multi-scale video chunks. We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales. The proposed framework is evaluated on zero-shot visual question-answering benchmarks, showing state-of-the-art performance at its scale. Our code is available at https://github.com/kkahatapitiya/LangRepo.</abstract>
      <url hash="e50441da">2025.findings-acl.294</url>
      <bibkey>kahatapitiya-etal-2025-language</bibkey>
    </paper>
    <paper id="295">
      <title>Investigating Language Preference of Multilingual <fixed-case>RAG</fixed-case> Systems</title>
      <author><first>Jeonghyun</first><last>Park</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>5647-5675</pages>
      <abstract>Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings. Code is available at <url>https://github.com/jeonghyunpark2002/LanguagePreference.git</url></abstract>
      <url hash="51042a84">2025.findings-acl.295</url>
      <bibkey>park-lee-2025-investigating</bibkey>
    </paper>
    <paper id="296">
      <title><fixed-case>FGDGNN</fixed-case>: Fine-Grained Dynamic Graph Neural Network for Rumor Detection on Social Media</title>
      <author><first>Mei</first><last>Guo</last></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Nankai University</affiliation></author>
      <author><first>Chunyan</first><last>Hou</last><affiliation>Tianjin University of Technology</affiliation></author>
      <author><first>Yike</first><last>Wu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaojie</first><last>Yuan</last><affiliation>Nankai University</affiliation></author>
      <pages>5676-5687</pages>
      <abstract>Detecting rumors on social media has become a crucial issue.Propagation structure-based methods have recently attracted increasing attention.When the propagation structure is represented by the dynamic graph, temporal information is considered.However, existing rumor detection models using dynamic graph typically focus only on coarse-grained temporal information and ignore the fine-grained temporal dynamics within individual snapshots and across snapshots.In this paper, we propose a novel Fine-Grained Dynamic Graph Neural Network (FGDGNN) model, which can incorporate the fine-grained temporal information of dynamic propagation graph in the intra-snapshot and dynamic embedding update mechanism in the inter-snapshots into a unified framework for rumor detection.Specifically, we first construct the edge-weighted propagation graph and the edge-aware graph isomorphism network is proposed.To obtain fine-grained temporal representations across snapshots, we propose an embedding transformation layer to update node embeddings.Finally, we integrate the temporal information in the inter-snapshots at the graph level to enhance the effectiveness of the proposed model.Extensive experiments conducted on three public real-world datasets demonstrate that our FGDGNN model achieves significant improvements compared with the state-of-the-art baselines.</abstract>
      <url hash="e7e9729b">2025.findings-acl.296</url>
      <bibkey>guo-etal-2025-fgdgnn</bibkey>
    </paper>
    <paper id="297">
      <title>Self-Tuning: Instructing <fixed-case>LLM</fixed-case>s to Effectively Acquire New Knowledge through Self-Teaching</title>
      <author><first>Xiaoying</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Yipeng</first><last>Zhang</last></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Helen M.</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5688-5724</pages>
      <abstract>Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM’s ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM’s knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.</abstract>
      <url hash="a860eeba">2025.findings-acl.297</url>
      <bibkey>zhang-etal-2025-self-tuning</bibkey>
    </paper>
    <paper id="298">
      <title><fixed-case>Q</fixed-case>uery<fixed-case>A</fixed-case>ttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language</title>
      <author><first>Qingsong</first><last>Zou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jingyu</first><last>Xiao</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qing</first><last>Li</last><affiliation>Pengcheng Laboratory and Pengcheng Laboratory</affiliation></author>
      <author><first>Zhi</first><last>Yan</last><affiliation>Jilin University</affiliation></author>
      <author><first>Yuhang</first><last>Wang</last></author>
      <author><first>Li</first><last>Xu</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kuofeng</first><last>Gao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ruoyu</first><last>Li</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>5725-5741</pages>
      <abstract>Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is available at https://anonymous.4open.science/r/QueryAttack-334B.</abstract>
      <url hash="97291c7a">2025.findings-acl.298</url>
      <bibkey>zou-etal-2025-queryattack</bibkey>
    </paper>
    <paper id="299">
      <title>Memory or Reasoning? Explore How <fixed-case>LLM</fixed-case>s Compute Mixed Arithmetic Expressions</title>
      <author><first>Chengzhi</first><last>Li</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Ping</first><last>Jian</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Chenxu</first><last>Wang</last></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <pages>5742-5763</pages>
      <abstract>Large language models (LLMs) can solve complex multi-step math reasoning problems, but little is known about how these computations are implemented internally. Many recent studies have investigated the mechanisms of LLMs on simple arithmetic tasks (e.g., <tex-math>a+b</tex-math>, <tex-math>a\times b</tex-math>), but how LLMs solve mixed arithmetic tasks still remains unexplored. This gap highlights the limitation of these findings in reflecting real-world scenarios. In this work, we take a step further to explore how LLMs compute mixed arithmetic expressions. We find that LLMs follow a similar workflow to mixed arithmetic calculations: first parsing the complete expression, then using attention heads to aggregate information to the last token position for result generation, without step-by-step reasoning at the token dimension. However, **for some specific expressions, the model generates the final result depends on the generation of intermediate results at the last token position, which is similar to human thinking.** Furthermore, we propose a **C**ausal **E**ffect **D**riven **F**ine-tuning method (CEDF) to adaptively enhance the identified key components used to execute mixed arithmetic calculations to improve LLMs reasoning ability.</abstract>
      <url hash="0bb9f886">2025.findings-acl.299</url>
      <bibkey>li-etal-2025-memory</bibkey>
    </paper>
    <paper id="300">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>X</fixed-case>: A Recommendation Agent-Oriented User Modeling Framework for Long Behavior Sequence</title>
      <author><first>Yunxiao</first><last>Shi</last></author>
      <author><first>Wujiang</first><last>Xu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Zhang</first><last>Zeqi</last></author>
      <author><first>Xing</first><last>Zi</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Qiang</first><last>Wu</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Min</first><last>Xu</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>5764-5787</pages>
      <abstract>User profile embedded in the prompt template of personalized recommendation agents play a crucial role in shaping their decision-making process. High-quality user profiles are essential for aligning agent behavior with real user interests. Typically, these profiles are constructed by leveraging LLMs for user profile modeling (LLM-UM). However, this process faces several challenges: (1) LLMs struggle with long user behaviors due to context length limitations and performance degradation. (2) Existing methods often extract only partial segments from full historical behavior sequence, inevitably discarding diverse user interests embedded in the omitted content, leading to incomplete modeling and suboptimal profiling. (3) User profiling is often tightly coupled with the inference context, requiring online processing, which introduces significant latency overhead. <b>In this paper, we propose PersonaX, an agent-agnostic LLM-UM framework to address these challenges. It augments downstream recommendation agents to achieve better recommendation performance and inference efficiency.</b> PersonaX (a) segments complete historical behaviors into clustered groups, (b) selects multiple sub-behavior sequences (SBS) with a balance of prototypicality and diversity to form a high-quality core set, (c) performs offline multi-persona profiling to capture diverse user interests and generate fine-grained, cached textual personas, and (d) decouples user profiling from online inference, enabling profile retrieval instead of real-time generation. <b>Extensive experiments demonstrate its effectiveness: using only 30–50% of behavioral data (sequence length 480), PersonaX enhances AgentCF by 3–11% and Agent4Rec by 10–50%.</b> As a scalable and model-agnostic LLM-UM solution, PersonaX sets a new benchmark in scalable user modeling. The code is available at URL .</abstract>
      <url hash="9f26c3ad">2025.findings-acl.300</url>
      <bibkey>shi-etal-2025-personax</bibkey>
    </paper>
    <paper id="301">
      <title>Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models</title>
      <author><first>Shuliang</first><last>Liu</last></author>
      <author><first>Xinze</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zheni</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>5788-5807</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilizes the judge-consistency to evaluate these judgments, and selects the chosen and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at https://github.com/OpenBMB/ConsJudge.</abstract>
      <url hash="579c71ee">2025.findings-acl.301</url>
      <bibkey>liu-etal-2025-judge</bibkey>
    </paper>
    <paper id="302">
      <title>Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability</title>
      <author><first>Chiwei</first><last>Zhu</last></author>
      <author><first>Benfeng</first><last>Xu</last></author>
      <author><first>An</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5808-5835</pages>
      <abstract>Training language models with rationales augmentation has been shown to be beneficial in many existing works. In this paper, we identify that such a prevailing view does not hold consistently. We conduct comprehensive investigations to thoroughly inspect the impact of rationales on model performance as well as a novel perspective of model reliability. The results lead to several key findings that add new insights upon existing understandings: 1) Rationales can, at times, deteriorate model performance; 2) Rationales can, at times, improve model reliability, even outperforming their untrained counterparts; 3) A linear correspondence exists in between the performance and reliability improvements, while both are driven by the intrinsic difficulty of the task. These findings provide informative regulations on the broad utilization of rationales and raise critical implications on the procedure of explicitly aligning language models with implicit human thoughts. Codes can be found in this anonymous link: https://anonymous.4open.science/r/rationales-CEE8.</abstract>
      <url hash="66fa14f0">2025.findings-acl.302</url>
      <bibkey>zhu-etal-2025-rationales</bibkey>
    </paper>
    <paper id="303">
      <title><fixed-case>CA</fixed-case>-<fixed-case>GAR</fixed-case>: Context-Aware Alignment of <fixed-case>LLM</fixed-case> Generation for Document Retrieval</title>
      <author><first>Heng</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Junfeng</first><last>Kang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Liyang</first><last>He</last></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shuanghong</first><last>Shen</last><affiliation>Institute of Artificial Intelligence, Hefei Comprehensive National Science Center</affiliation></author>
      <author><first>Junyu</first><last>Lu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>5836-5849</pages>
      <abstract>Information retrieval has evolved from traditional sparse and dense retrieval methods to approaches driven by large language models (LLMs). Recent techniques, such as Generation-Augmented Retrieval (GAR) and Generative Document Retrieval (GDR), leverage LLMs to enhance retrieval but face key challenges: GAR’s generated content may not always align with the target document corpus, while GDR limits the generative capacity of LLMs by constraining outputs to predefined document identifiers. To address these issues, we propose Context-Aware Generation-Augmented Retrieval (CA-GAR), which enhances LLMs by integrating corpus information into their generation process. CA-GAR optimizes token selection by incorporating relevant document information and leverages a Distribution Alignment Strategy to extract corpus information using a lexicon-based approach. Experimental evaluations on seven tasks from the BEIR benchmark and four non-English languages from Mr.TyDi demonstrate that CA-GAR outperforms existing methods.</abstract>
      <url hash="f2922131">2025.findings-acl.303</url>
      <bibkey>yu-etal-2025-ca</bibkey>
    </paper>
    <paper id="304">
      <title><fixed-case>A</fixed-case>gent<fixed-case>C</fixed-case>ourt: Simulating Court with Adversarial Evolvable Lawyer Agents</title>
      <author><first>Guhong</first><last>Chen</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Liyang</first><last>Fan</last></author>
      <author><first>Zihan</first><last>Gong</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Nan</first><last>Xie</last></author>
      <author><first>Zixuan</first><last>Li</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Chengming</first><last>Li</last><affiliation>Shenzhen MSU-BIT University</affiliation></author>
      <author><first>Qiang</first><last>Qu</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>5850-5865</pages>
      <abstract>Current research in LLM-based simulation systems lacks comprehensive solutions for modeling real-world court proceedings, while existing legal language models struggle with dynamic courtroom interactions. We present **AgentCourt**, a comprehensive legal simulation framework that addresses these challenges through adversarial evolution of LLM-based agents. Our AgentCourt introduces a new adversarial evolutionary approach for agents called **AdvEvol**, which performs dynamic knowledge learning and evolution through structured adversarial interactions in a simulated courtroom program, breaking the limitations of the traditional reliance on static knowledge bases or manual annotations. By simulating 1,000 civil cases, we construct an evolving knowledge base that enhances the agents’ legal reasoning abilities. The evolved lawyer agents demonstrated outstanding performance on our newly introduced **CourtBench** benchmark, achieving a 12.1% improvement in performance compared to the original lawyer agents. Evaluations by professional lawyers confirm the effectiveness of our approach across three critical dimensions: cognitive agility, professional knowledge, and logical rigor. Beyond outperforming specialized legal models in interactive reasoning tasks, our findings emphasize the importance of adversarial learning in legal AI and suggest promising directions for extending simulation-based legal reasoning to broader judicial and regulatory contexts.</abstract>
      <url hash="fd015fbe">2025.findings-acl.304</url>
      <bibkey>chen-etal-2025-agentcourt</bibkey>
    </paper>
    <paper id="305">
      <title><fixed-case>MLD</fixed-case>ebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios</title>
      <author><first>JinYang</first><last>Huang</last></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Hanjie</first><last>Zhao</last></author>
      <author><first>Zihui</first><last>Cheng</last><affiliation>Central South University</affiliation></author>
      <author><first>Jiesong</first><last>Bai</last></author>
      <author><first>Jingxuan</first><last>Zhou</last><affiliation>Central South University</affiliation></author>
      <author><first>Min</first><last>Li</last><affiliation>Central South University</affiliation></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <pages>5866-5879</pages>
      <abstract>Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.</abstract>
      <url hash="4add197d">2025.findings-acl.305</url>
      <bibkey>huang-etal-2025-mldebugging</bibkey>
    </paper>
    <paper id="306">
      <title>An Empirical Study of <fixed-case>LLM</fixed-case>-as-a-Judge for <fixed-case>LLM</fixed-case> Evaluation: Fine-tuned Judge Model is not a General Substitute for <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongli</first><last>Zhou</last></author>
      <author><first>Yingqi</first><last>Qu</last></author>
      <author><first>Jing</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>5880-5895</pages>
      <abstract>Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge models based on open-source LLMs for evaluation. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of LLM-as-a-Judge. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness and adaptability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations.</abstract>
      <url hash="dc053da8">2025.findings-acl.306</url>
      <bibkey>huang-etal-2025-empirical</bibkey>
    </paper>
    <paper id="307">
      <title>Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent</title>
      <author><first>Xueyang</first><last>Feng</last></author>
      <author><first>Jingsen</first><last>Zhang</last></author>
      <author><first>Jiakai</first><last>Tang</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Guohao</first><last>Cai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Quanyu</first><last>Dai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yue</first><last>Zhu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <pages>5896-5914</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm **ECPO**, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we also introduce an LLM-based user simulator, **AILO**, to simulate user feedback and expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA’s interaction capabilities, offering notable improvements in both efficiency and effectiveness over existing MTPO methods.</abstract>
      <url hash="479070ed">2025.findings-acl.307</url>
      <bibkey>feng-etal-2025-expectation</bibkey>
    </paper>
    <paper id="308">
      <title><fixed-case>P</fixed-case>ro<fixed-case>M</fixed-case>ed<fixed-case>TS</fixed-case>: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series</title>
      <author><first>Shuai</first><last>Niu</last><affiliation>University of Hong Kong and Hong Kong Baptist University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Liang</first><last>Bai</last></author>
      <author><first>Zhihua</first><last>Wang</last><affiliation>Shanghai Institute for Advanced Study of Zhejiang University</affiliation></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Richard Yi Da</first><last>Xu</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Guo</first><last>Li</last><affiliation>The Manchester Metropolitan University</affiliation></author>
      <author><first>Xian</first><last>Yang</last><affiliation>University of Manchester</affiliation></author>
      <pages>5915-5928</pages>
      <abstract>Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data, such as lab test results, capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative prompt embeddings. These prompt embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.</abstract>
      <url hash="744d8f84">2025.findings-acl.308</url>
      <bibkey>niu-etal-2025-promedts</bibkey>
    </paper>
    <paper id="309">
      <title><fixed-case>C</fixed-case>ipher<fixed-case>B</fixed-case>ank: Exploring the Boundary of <fixed-case>LLM</fixed-case> Reasoning Capabilities through Cryptography Challenge</title>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Qizhi</first><last>Pei</last></author>
      <author><first>Mengyuan</first><last>Sun</last></author>
      <author><first>Honglin</first><last>Lin</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Chenlin</first><last>Ming</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Jiang</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>5929-5965</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning <b>9 distinct algorithms</b>, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning.These findings underscore the need for continuous advancements in LLM reasoning capabilities.</abstract>
      <url hash="11da0d70">2025.findings-acl.309</url>
      <bibkey>li-etal-2025-cipherbank</bibkey>
    </paper>
    <paper id="310">
      <title>Which Retain Set Matters for <fixed-case>LLM</fixed-case> Unlearning? A Case Study on Entity Unlearning</title>
      <author><first>Hwan</first><last>Chang</last></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>5966-5982</pages>
      <abstract>Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focuses on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the <tex-math>\textit{Syntactically Similar Neighbor Set}</tex-math>, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.</abstract>
      <url hash="6ed66bee">2025.findings-acl.310</url>
      <bibkey>chang-lee-2025-retain</bibkey>
    </paper>
    <paper id="311">
      <title>Tell Me What You Don’t Know: Enhancing Refusal Capabilities of Role-Playing Agents via Representation Space Analysis and Editing</title>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Siyu</first><last>An</last></author>
      <author><first>Junru</first><last>Lu</last><affiliation>Tencent Youtu Lab</affiliation></author>
      <author><first>Muling</first><last>Wu</last></author>
      <author><first>Tianlong</first><last>Li</last></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Di</first><last>Yin</last></author>
      <author><first>Xing</first><last>Sun</last><affiliation>Tencent YouTu Lab</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>5983-6005</pages>
      <abstract>Role-Playing Agents (RPAs) have shown remarkable performance in various applications, yet they often struggle to recognize and appropriately respond to hard queries that conflict with their role-play knowledge. To investigate RPAs’ performance when faced with different types of conflicting requests, we develop an evaluation benchmark that includes contextual knowledge conflicting requests, parametric knowledge conflicting requests, and non-conflicting requests to assess RPAs’ ability to identify conflicts and refuse to answer appropriately without over-refusing. Through extensive evaluation, we find that most RPAs behave significant performance gaps toward different conflict requests. To elucidate the reasons, we conduct an in-depth representation-level analysis of RPAs under various conflict scenarios. Our findings reveal the existence of rejection regions and direct response regions within the model’s forwarding representation, and thus influence the RPA’s final response behavior. Therefore, we introduce a lightweight representation editing approach that conveniently shifts conflicting requests to the rejection region, thereby enhancing the model’s refusal accuracy. The extensive experiments validate the effectiveness of our editing method, improving RPAs’ refusal ability of conflicting requests while maintaining their general role-playing capabilities.</abstract>
      <url hash="287e69bb">2025.findings-acl.311</url>
      <bibkey>liu-etal-2025-tell</bibkey>
    </paper>
    <paper id="312">
      <title><fixed-case>LR</fixed-case>²<fixed-case>B</fixed-case>ench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems</title>
      <author><first>Jianghao</first><last>Chen</last></author>
      <author><first>Zhenlin</first><last>Wei</last></author>
      <author><first>Zhenjiang</first><last>Ren</last></author>
      <author><first>Ziyong</first><last>Li</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>6006-6032</pages>
      <abstract>Recent progress in o1-like models has significantly enhanced the reasoning abilities of Large Language Models (LLMs), empowering them to tackle increasingly complex tasks through reflection capabilities, such as making assumptions, backtracking, and self-refinement. However, effectively evaluating such reflection capabilities remains challenging due to the lack of appropriate benchmarks. To bridge this gap, we introduce LR<tex-math>^2</tex-math>Bench, a novel benchmark designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs. LR<tex-math>^2</tex-math>Bench comprises 850 samples across six Constraint Satisfaction Problems (CSPs) where reflective reasoning is crucial for deriving solutions that meet all given constraints. Each type of task focuses on distinct constraint patterns, such as knowledge-based, logical, and spatial constraints, providing a comprehensive evaluation of diverse problem-solving scenarios. We conduct extensive evaluation on both conventional models and o1-like models. Our experimental results reveal that even the most advanced reasoning-specific models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in LR<tex-math>^2</tex-math>Bench, achieving an average Exact Match score of only 20.0% and 23.6%, respectively. These findings underscore the significant room for improvement in the reflective reasoning capabilities of current LLMs.</abstract>
      <url hash="46cc57ea">2025.findings-acl.312</url>
      <bibkey>chen-etal-2025-lr2bench</bibkey>
    </paper>
    <paper id="313">
      <title><fixed-case>M</fixed-case>c<fixed-case>BE</fixed-case>: A Multi-task <fixed-case>C</fixed-case>hinese Bias Evaluation Benchmark for Large Language Models</title>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Xiangdong</first><last>Su</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Xu</first><last>Liu</last></author>
      <author><first>Ruirui</first><last>Wang</last></author>
      <author><first>Ke</first><last>Chang</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Guanglai</first><last>Gao</last><affiliation>Inner Mongolia University</affiliation></author>
      <pages>6033-6056</pages>
      <abstract>As large language models (LLMs) are increasingly applied to various NLP tasks, their inherent biases are gradually disclosed. Therefore, measuring biases in LLMs is crucial to mitigate its ethical risks. However, most existing bias evaluation datasets are focus on English andNorth American culture, and their bias categories are not fully applicable to other cultures. The datasets grounded in the Chinese language and culture are scarce. More importantly, these datasets usually only support single evaluation task and cannot evaluate the bias from multiple aspects in LLMs. To address these issues, we present a Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias evaluation instances, covering 12 single bias categories, 82 subcategories and introducing 5 evaluation tasks, providing extensive category coverage, content diversity, and measuring comprehensiveness. Additionally, we evaluate several popular LLMs from different series and with parameter sizes. In general, all these LLMs demonstrated varying degrees of bias. We conduct an in-depth analysis of results, offering novel insights into bias in LLMs.</abstract>
      <url hash="f9ed6b2e">2025.findings-acl.313</url>
      <bibkey>lan-etal-2025-mcbe</bibkey>
    </paper>
    <paper id="314">
      <title><fixed-case>MARK</fixed-case>: Multi-agent Collaboration with Ranking Guidance for Text-attributed Graph Clustering</title>
      <author><first>Yiwei</first><last>Fu</last></author>
      <author><first>Yuxing</first><last>Zhang</last></author>
      <author><first>Chunchun</first><last>Chen</last></author>
      <author><first>Jianwen</first><last>Ma</last></author>
      <author><first>Quan</first><last>Yuan</last></author>
      <author><first>Rong-Cheng</first><last>Tu</last></author>
      <author><first>Xinli</first><last>Huang</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Xiao</first><last>Luo</last></author>
      <author><first>Minghua</first><last>Deng</last></author>
      <pages>6057-6072</pages>
      <abstract>This paper studies the problem of text-attributed graph clustering, which aims to cluster each node into different groups using both textual attributes and structural information. Although graph neural networks (GNNs) have been proposed to solve this problem, their performance is usually limited when uncertain nodes are near the cluster boundaries due to label scarcity. In this paper, we introduce a new perspective of leveraging large language models (LLMs) to enhance text-attributed graph clustering and develop a novel approach named Multi-agent Collaboration with Ranking Guidance (MARK). The core of our MARK is to generate reliable guidance using the collaboration of three LLM-based agents as ranking-based supervision signals. In particular, we first conduct the coarse graph clustering, and utilize a concept agent to induce the semantics of each cluster. Then, we infer the robustness under perturbations to identify uncertain nodes and use a generation agent to produce synthetic text that closely aligns with their topology. An inference agent is adopted to provide the ranking semantics for each uncertain node in comparison to its synthetic counterpart. The consistent feedback between uncertain and synthetic texts is identified as reliable guidance for fine-tuning the clustering model within a ranking-based supervision objective. Experimental results on various benchmark datasets validate the effectiveness of the proposed MARK compared with competing baselines.</abstract>
      <url hash="e729da67">2025.findings-acl.314</url>
      <bibkey>fu-etal-2025-mark</bibkey>
    </paper>
    <paper id="315">
      <title>Can Language Models Capture Human Writing Preferences for Domain-Specific Text Summarization?</title>
      <author><first>Jingbao</first><last>Luo</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <author><first>Ran</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences and University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongpan</first><last>Sheng</last><affiliation>Southwest University</affiliation></author>
      <author><first>Xin</first><last>Hu</last><affiliation>Deakin University</affiliation></author>
      <author><first>Gang</first><last>Li</last><affiliation>Deakin University</affiliation></author>
      <author><first>WupengNjust</first><last>WupengNjust</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>6073-6091</pages>
      <abstract>With the popularity of large language models and their high-quality text generation capabilities, researchers are using them as auxiliary tools for text summary writing. Although summaries generated by these large language models are smooth and capture key information sufficiently, the quality of their output depends on the prompt, and the generated text is somewhat procedural to a certain extent. We construct LecSumm to verify whether language models truly capture human writing preferences, in which we recruit 200 college students to write summaries for lecture notes on ten different machine-learning topics and analyze writing preferences in real-world human summaries through the dimensions of length, content depth, tone &amp; style, and summary format. We define the method of capturing human writing preferences by language models as finetuning pre-trained models with data and designing prompts to optimize the output of large language models. The results of translating the analyzed human writing preferences into prompts and conducting experiments show that both models still fail to capture human writing preferences effectively. Our LecSumm dataset brings new challenges to finetuned and prompt-based large language models on the task of human-centered text summarization.</abstract>
      <url hash="fce2f185">2025.findings-acl.315</url>
      <bibkey>luo-etal-2025-language</bibkey>
    </paper>
    <paper id="316">
      <title>Mitigate Position Bias in <fixed-case>LLM</fixed-case>s via Scaling a Single Hidden States Channel</title>
      <author><first>Yijiong</first><last>Yu</last></author>
      <author><first>Huiqiang</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xufang</first><last>Luo</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Qianhui</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chin-Yew</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Yongfeng</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lili</first><last>Qiu</last><affiliation>Microsoft and University of Texas at Austin</affiliation></author>
      <pages>6092-6111</pages>
      <abstract>Long-context language models (LCLMs) can process long context, but still exhibit position bias, also known as “lost in the middle”, which indicates placing key information in the middle of the context will significantly affect performance. To mitigating this, we first explore the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. Then we identify that, in addition to position embeddings, positional information in hidden states also contributes to position bias, and it manifests itself in specific channels of hidden states, called positional hidden states. Based on these, we propose a method to mitigate position bias by scaling positional hidden states. Experiments on NaturalQuestions Multi-document QA, KV retrieval and LongBench, using various models including RoPE models, context window-extended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% in “lost in the middle” benchmark by modifying just one channel of hidden states. Our code is available at https://aka.ms/PositionalHidden.</abstract>
      <url hash="55acb03a">2025.findings-acl.316</url>
      <bibkey>yu-etal-2025-mitigate</bibkey>
    </paper>
    <paper id="317">
      <title>Self-attention-based Graph-of-Thought for Math Problem Solving</title>
      <author><first>Ruiqiao</first><last>Bai</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Xue</first><last>Han</last></author>
      <author><first>Shuo</first><last>Lei</last></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Yanyan</first><last>Luo</last><affiliation>China Mobile Communications Company Limited Research Institute</affiliation></author>
      <author><first>Chao</first><last>Deng</last><affiliation>China Mobile Research Institute</affiliation></author>
      <pages>6112-6125</pages>
      <abstract>Applying Large Language Models (LLM) to solve math problems is one of the hottest research topics at present. Traditional Chain-of-Thought-based methods typically generate the reasoning path in a chain structure, leading to unnecessary interference caused by non-zero self-attention among weakly related reasoning steps. Such a setting also differs from humans’ typical graph-structured reasoning habit (with an inter-step relationship graph in mind). To solve the problem, this paper proposes a novel decoding method for Transformer-based LLM, named Self-attention-based Graph-of-Thought (SaGoT). SaGoT constructs a thought graph simultaneously as an LLM inference (based on a newly defined inter-step self-attention indicator), and generates reasoning steps with a novel graph-structured self-attention mechanism. It is a significant contribution for SaGoT to enable an LLM’s graph-like reasoning ability by modifying its inner working operations, compared to SOTA prompting methods that are ex-post, rely on huge LLMs and redundant reasoning step generation to form a graph (inefficient &amp; non-human-like). In addition, SaGoT is a training-free technique that can be seamlessly incorporated into pre-trained Transformer-based LLMs. Our experimental results have shown that SaGoT could significantly enhance mathematical reasoning accuracy without the reliance on huge computationally over-expensive LLMs. It also avoids SOTA methods’ performance degradation issues when the LLM is too small to comprehend complex prompts. Moreover, SaGoT integrates intrinsic interpretability into the LLM’s reasoning procedure, intuitively assisting humans in understanding how an LLM views the relationships among its reasoning steps, and why the LLM succeeds or fails.</abstract>
      <url hash="616edd2a">2025.findings-acl.317</url>
      <bibkey>bai-etal-2025-self</bibkey>
    </paper>
    <paper id="318">
      <title><fixed-case>BAR</fixed-case>: A Backward Reasoning based Agent for Complex <fixed-case>M</fixed-case>inecraft Tasks</title>
      <author><first>Weihong</first><last>Du</last></author>
      <author><first>Wenrui</first><last>Liao</last></author>
      <author><first>Binyu</first><last>Yan</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Hongru</first><last>Liang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Anthony G</first><last>Cohn</last><affiliation>Alan Turing Institute and University of Leeds</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>6126-6149</pages>
      <abstract>Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent’s initial state. However, this forward reasoning paradigm doesn’t work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent’s initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a backward reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.</abstract>
      <url hash="6d91a945">2025.findings-acl.318</url>
      <bibkey>du-etal-2025-bar</bibkey>
    </paper>
    <paper id="319">
      <title><fixed-case>KAPA</fixed-case>: A Deliberative Agent Framework with Tree-Structured Knowledge Base for Multi-Domain User Intent Understanding</title>
      <author><first>Jiakai</first><last>Tang</last></author>
      <author><first>Shiqi</first><last>Shen</last><affiliation>Wechat, Tencent</affiliation></author>
      <author><first>ZhipengWang</first><last>ZhipengWang</last></author>
      <author><first>Gong</first><last>Zhi</last><affiliation>WeChat</affiliation></author>
      <author><first>Xueyang</first><last>Feng</last></author>
      <author><first>Zexu</first><last>Sun</last></author>
      <author><first>Haoran</first><last>Tan</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>6150-6166</pages>
      <abstract>Dialogue assistants have become ubiquitous in modern applications, fundamentally reshaping human daily communication patterns and information access behaviors. In real-world conversational interactions, however, user queries are often volatile, ambiguous, and diverse, making it difficult accurately and efficiently grasp the user’s underlying intentions. To address this challenge, we propose a simple yet effective deliberative agent framework that leverages human thought process to build high-level domain knowledge. To further achieve efficient knowledge accumulation and retrieval, we design a tree-structured knowledge base to store refined experience and data. Moreover, we construct a new benchmark, User-Intent-Understanding (UIU), which covers multi-domain, multi-tone, and sequential multi-turn personalized user queries. Extensive experiments demonstrate the effectiveness of our proposed method across multi-step evaluations.</abstract>
      <url hash="83971289">2025.findings-acl.319</url>
      <bibkey>tang-etal-2025-kapa</bibkey>
    </paper>
    <paper id="320">
      <title><fixed-case>RASD</fixed-case>: Retrieval-Augmented Speculative Decoding</title>
      <author><first>Guofeng</first><last>Quan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenfeng</first><last>Feng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chuzhan</first><last>Hao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Guochao</first><last>Jiang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuewei</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hao Henry</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>6167-6177</pages>
      <abstract>Speculative decoding accelerates inference in large language models (LLMs) by generating draft tokens for target model verification. Current approaches for obtaining draft tokens rely on lightweight draft models or additional model structures to generate draft tokens and retrieve context from databases. Due to the draft model’s small size and limited training data, model-based speculative decoding frequently becomes less effective in out-of-domain scenarios. Additionally, the time cost of the drafting phase results in a low upper limit on acceptance length during the verification step, limiting overall efficiency. This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which adopts retrieval methods to enhance model-based speculative decoding. We introduce tree pruning and tree fusion to achieve this. Specifically, we develop a pruning method based on the draft model’s probability distribution to construct the optimal retrieval tree. Second, we employ the longest prefix matching algorithm to merge the tree generated by the draft model with the retrieval tree, resulting in a unified tree for verification. Experimental results demonstrate that RASD achieves state-of-the-art inference acceleration across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD exhibits strong scalability, seamlessly integrating with various speculative decoding approaches, including both generation-based and retrieval-based methods.</abstract>
      <url hash="cd51cf3c">2025.findings-acl.320</url>
      <bibkey>quan-etal-2025-rasd</bibkey>
    </paper>
    <paper id="321">
      <title><fixed-case>FRAG</fixed-case>: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs</title>
      <author><first>Zengyi</first><last>Gao</last></author>
      <author><first>Yukun</first><last>Cao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hairu</first><last>Wang</last></author>
      <author><first>Ao</first><last>Ke</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yuan</first><last>Feng</last></author>
      <author><first>S Kevin</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xike</first><last>Xie</last></author>
      <pages>6178-6192</pages>
      <abstract>To mitigate the hallucination and knowledge deficiency in large language models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) has shown promising potential by utilizing KGs as an external resource to enhance LLM reasoning.However, existing KG-RAG approaches struggle with a trade-off between flexibility and retrieval quality. Modular methods prioritize flexibility by avoiding the use of KG-fine-tuned models during retrieval, leading to fixed retrieval strategies and suboptimal retrieval quality. Conversely, coupled methods embed KG information within models to improve retrieval quality but at the expense of flexibility.In this paper, we propose a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the advantages of both approaches. FRAG estimates the hop range of reasoning paths based solely on the query and classifies it as either simple or complex.To match the complexity of the query, tailored pipelines are applied to ensure efficient and accurate reasoning path retrieval, thus fostering the final reasoning process. By using the query text instead of the KG to infer the structural information of reasoning paths and employing adaptable retrieval strategies, FRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG does not require extra LLM fine-tuning or calls, significantly boosting efficiency and conserving resources. Extensive experiments show that FRAG achieves state-of-the-art performance with high efficiency and low resource consumption. The code for our method is publicly available at https://github.com/gzy02/FRAG.</abstract>
      <url hash="68dd1ce4">2025.findings-acl.321</url>
      <bibkey>gao-etal-2025-frag</bibkey>
    </paper>
    <paper id="322">
      <title>Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models</title>
      <author><first>Kening</first><last>Zheng</last></author>
      <author><first>Junkai</first><last>Chen</last></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Zou</last></author>
      <author><first>Huiyu</first><last>Zhou</last><affiliation>Guangxi Zhuang Autonomous Region Big Data Research Institute</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>6193-6212</pages>
      <abstract>Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs’ ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence. The dataset and code are released at https://github.com/JackChen-seu/Reefknot.</abstract>
      <url hash="77b75470">2025.findings-acl.322</url>
      <bibkey>zheng-etal-2025-reefknot</bibkey>
    </paper>
    <paper id="323">
      <title>Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning</title>
      <author><first>Yilei</first><last>Tu</last></author>
      <author><first>Andrew</first><last>Xue</last></author>
      <author><first>Freda</first><last>Shi</last><affiliation>University of Waterloo and Vector Institute</affiliation></author>
      <pages>6213-6248</pages>
      <abstract>While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding when and why it works well.In this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study show that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.</abstract>
      <url hash="8d4ba8c5">2025.findings-acl.323</url>
      <bibkey>tu-etal-2025-blessing</bibkey>
    </paper>
    <paper id="324">
      <title><fixed-case>SEK</fixed-case>: Self-Explained Keywords Empower Large Language Models for Code Generation</title>
      <author><first>Lishui</first><last>Fan</last></author>
      <author><first>Mouxiang</first><last>Chen</last></author>
      <author><first>Zhongxin</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>6249-6278</pages>
      <abstract>Large language models (LLMs) have achieved impressive performance in code generation. Despite the remarkable success, we observed that LLMs often misunderstand or overlook some problem-specific undertrained keywords during code generation, compromising the accuracy of the generated code. After explicitly explaining these undertrained keywords using well-trained terms in the prompt, LLMs are more likely to generate correct code implementation. Inspired by this observation, we propose a novel technique named SEK(Self-Explained Keywords), which empowers an LLM for better code generation by extracting and explaining the key terms in the problem description with the LLM itself. Comprehensive experiments across four benchmarks, i.e., HumanEval(+), MBPP(+), APPS and BigCodeBench, with five representative LLMs, show that SEK can significantly improve LLMs in code generation, yielding substantial and consistent gains. For instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4% to 93.3% on the Humaneval benchmark. Further analysis confirms that SEK enables the LLMs to shift their attention from low-frequency keywords to their corresponding explanations.</abstract>
      <url hash="ef72d9da">2025.findings-acl.324</url>
      <bibkey>fan-etal-2025-sek</bibkey>
    </paper>
    <paper id="325">
      <title>Why Not Act on What You Know? Unleashing Safety Potential of <fixed-case>LLM</fixed-case>s via Self-Aware Guard Enhancement</title>
      <author><first>Peng</first><last>Ding</last><affiliation>nanjing university</affiliation></author>
      <author><first>Jun</first><last>Kuang</last><affiliation>Meituan</affiliation></author>
      <author><first>ZongYu</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xuezhi</first><last>Cao</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>6279-6299</pages>
      <abstract>Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE(Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs’ strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE’s effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at <url>https://github.com/NJUNLP/SAGE</url>.</abstract>
      <url hash="3ce35cdf">2025.findings-acl.325</url>
      <bibkey>ding-etal-2025-act</bibkey>
    </paper>
    <paper id="326">
      <title>Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents</title>
      <author><first>Vardaan</first><last>Pahuja</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <author><first>Yadong</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Corby</first><last>Rosset</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Boyu</first><last>Gou</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Arindam</first><last>Mitra</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Spencer</first><last>Whitehead</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yu</first><last>Su</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <pages>6300-6323</pages>
      <abstract>Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human-level capabilities in more realistic online settings. A key bottleneck is the lack of diverse and large-scale trajectory-level datasets across various domains, which are expensive to collect. In this paper, we address this challenge by developing a scalable recipe to synthesize the largest and most diverse trajectory-level dataset to date, containing over 94K successful multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and 33M web elements. In particular, we leverage extensive web exploration and refinement to obtain diverse task intents. The average cost is 28 cents per successful trajectory, making it affordable to a wide range of users in the community. Leveraging this dataset, we train Explorer, a multimodal web agent, and demonstrate strong performance on both offline and online web agent benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++. Additionally, our experiments highlight data scaling as a key driver for improving web agent capabilities. We hope this study makes state-of-the-art LMM-based agent research at a larger scale more accessible.</abstract>
      <url hash="03baa01c">2025.findings-acl.326</url>
      <bibkey>pahuja-etal-2025-explorer</bibkey>
    </paper>
    <paper id="327">
      <title>Advancing General Multimodal Capability of Vision-language Models with Pyramid-descent Visual Position Encoding</title>
      <author><first>Zhanpeng</first><last>Chen</last></author>
      <author><first>Mingxiao</first><last>Li</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Ziyang</first><last>Chen</last></author>
      <author><first>Nan</first><last>Du</last><affiliation>Tencent INC</affiliation></author>
      <author><first>Xiaolong</first><last>Li</last><affiliation>Tencent America LLC</affiliation></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>6324-6341</pages>
      <abstract>Vision-language Models (VLMs) have shown remarkable capabilities in advancing general artificial intelligence, yet the irrational encoding of visual positions persists in inhibiting the models’ comprehensive perception performance across different levels of granularity. In this work, we propose Pyramid-descent Visual Position Encoding (PyPE), a novel approach designed to enhance the perception of visual tokens within VLMs. By assigning visual position indexes from the periphery to the center and expanding the central receptive field incrementally, PyPE addresses the limitations of traditional raster-scan methods and mitigates the long-term decay effects induced by Rotary Position Embedding (RoPE). Our method reduces the relative distance between interrelated visual elements and instruction tokens, promoting a more rational allocation of attention weights and allowing for a multi-granularity perception of visual elements and countering the over-reliance on anchor tokens. Extensive experimental evaluations demonstrate that PyPE consistently improves the general capabilities of VLMs across various sizes. Code is available at https://anonymous.4open.science/r/PyPE-34EE.</abstract>
      <url hash="0b8c557a">2025.findings-acl.327</url>
      <bibkey>chen-etal-2025-advancing</bibkey>
    </paper>
    <paper id="328">
      <title><fixed-case>P</fixed-case>-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Experts</title>
      <author><first>Yuhao</first><last>Dan</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Qin</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Junfeng</first><last>Tian</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Liang</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <pages>6342-6362</pages>
      <abstract>Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality.</abstract>
      <url hash="7d2a428b">2025.findings-acl.328</url>
      <bibkey>dan-etal-2025-p</bibkey>
    </paper>
    <paper id="329">
      <title><fixed-case>E</fixed-case>ssay<fixed-case>J</fixed-case>udge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models</title>
      <author><first>Jiamin</first><last>Su</last></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Fangteng</first><last>Fu</last></author>
      <author><first>Zhang</first><last>Han</last></author>
      <author><first>Jingheng</first><last>Ye</last></author>
      <author><first>Xiang</first><last>Liu</last></author>
      <author><first>Jiahao</first><last>Huo</last><affiliation>The Hong Kong University of Science and Technology and Tongji University</affiliation></author>
      <author><first>Huiyu</first><last>Zhou</last><affiliation>Guangxi Zhuang Autonomous Region Big Data Research Institute</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>6363-6389</pages>
      <abstract>Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (i) reliance on handcrafted features that limit generalizability, (ii) difficulty in capturing fine-grained traits like coherence and argumentation, and (iii) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose **EssayJudge**, the **first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits**. By leveraging MLLMs’ strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.</abstract>
      <url hash="bf7818ce">2025.findings-acl.329</url>
      <bibkey>su-etal-2025-essayjudge</bibkey>
    </paper>
    <paper id="330">
      <title>Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks</title>
      <author><first>Yuanjie</first><last>Lyu</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <author><first>Yuhao</first><last>Chen</last></author>
      <author><first>Yong</first><last>Chen</last></author>
      <author><first>Tong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6390-6404</pages>
      <abstract>In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the “Chain of Models” approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.</abstract>
      <url hash="25336fa3">2025.findings-acl.330</url>
      <bibkey>lyu-etal-2025-streamlining</bibkey>
    </paper>
    <paper id="331">
      <title>Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks</title>
      <author><first>Jiayi</first><last>He</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hehai</first><last>Lin</last></author>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>6405-6421</pages>
      <abstract>While Vision-Language Models (VLMs) have shown remarkable abilities, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance models’ reasoning ability through additional training, enabling them to generate high-quality responses directly without further refinement.</abstract>
      <url hash="498726e8">2025.findings-acl.331</url>
      <bibkey>he-etal-2025-self</bibkey>
    </paper>
    <paper id="332">
      <title>Beyond Reactive Safety: Risk-Aware <fixed-case>LLM</fixed-case> Alignment via Long-Horizon Simulation</title>
      <author><first>Chenkai</first><last>Sun</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Denghui</first><last>Zhang</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>ChengXiang</first><last>Zhai</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>6422-6434</pages>
      <abstract>Given the growing influence of language model-based agents on high-stakes societal decisions, from public policy to healthcare, ensuring their beneficial impact requires understanding the far-reaching implications of their suggestions. We propose a proof-of-concept framework that projects how model-generated advice could propagate through societal systems on a macroscopic scale over time, enabling more robust alignment. To assess the long-term safety awareness of language models, we also introduce a dataset of 100 indirect harm scenarios, testing models’ ability to foresee adverse, non-obvious outcomes from seemingly harmless user prompts. Our approach achieves not only over 20% improvement on the new dataset but also an average win rate exceeding 70% against strong baselines on existing safety benchmarks (AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer agents.</abstract>
      <url hash="53d7e607">2025.findings-acl.332</url>
      <bibkey>sun-etal-2025-beyond</bibkey>
    </paper>
    <paper id="333">
      <title>Probability-Consistent Preference Optimization for Enhanced <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Yunqiao</first><last>Yang</last></author>
      <author><first>Houxing</first><last>Ren</last><affiliation>Sensetime</affiliation></author>
      <author><first>Zimu</first><last>Lu</last></author>
      <author><first>Ke</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Weikang</first><last>Shi</last></author>
      <author><first>Aojun</first><last>Zhou</last></author>
      <author><first>Junting</first><last>Pan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Mingjie</first><last>Zhan</last></author>
      <author><first>Hongsheng</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6435-6448</pages>
      <abstract>Recent advances in preference optimization have demonstrated significant potential for improving mathematical reasoning capabilities in large language models (LLMs). While current approaches leverage high-quality pairwise preference data through outcome-based criteria like answer correctness or consistency, they fundamentally neglect the internal logical coherence of responses. To overcome this, we propose Probability-Consistent Preference Optimization (PCPO), a novel framework that establishes dual quantitative metrics for preference selection: (1) surface-level answer correctness and (2) intrinsic token-level probability consistency across responses. Extensive experiments show that our PCPO consistently outperforms existing outcome-only criterion approaches across a diverse range of LLMs and benchmarks. Our code is publicly available at https://github.com/YunqiaoYang/PCPO.</abstract>
      <url hash="3113baee">2025.findings-acl.333</url>
      <bibkey>yang-etal-2025-probability</bibkey>
    </paper>
    <paper id="334">
      <title><fixed-case>IW</fixed-case>-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web</title>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Junhao</first><last>Chen</last></author>
      <author><first>Yaonan</first><last>Gu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junjia</first><last>Du</last></author>
      <author><first>Shaosheng</first><last>Cao</last></author>
      <author><first>Binyuan</first><last>Hui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Jianxin</first><last>Ma</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>6449-6466</pages>
      <abstract>Recently, advancements in large multimodal models have led to significant strides in image comprehension capabilities. Despite these advancements, there is a lack of a robust benchmark specifically for assessing the image‐to‐web conversion proficiency of these large models. It is essential to ensure the integrity of the web elements generated, which comprise both visible and invisible categories. Previous evaluation methods (e.g., BLEU) are notably susceptible to significant alterations due to the presence of invisible elements. Furthermore, it is crucial to measure the layout information of web pages—i.e., the positional relationships between elements—which has been overlooked by prior work. To address these challenges, we have curated and aligned a benchmark of images and corresponding web codes (IW-bench). Specifically, we propose Element Accuracy, which tests the completeness of elements by parsing the Document Object Model (DOM) tree. We also introduce Layout Accuracy to analyze positional relationships by converting the DOM tree into a common subsequence. In addition, we design a five‐hop multimodal Chain‐of‐Thought prompting strategy for improved performance, consisting of: 1) SoM prompt injection, 2) inferring elements, 3) inferring layout, 4) inferring web code, and 5) reflection. Our benchmark comprises 1,200 image–code pairs with varying levels of difficulty. We have conducted extensive experiments on existing large multimodal models, providing insights into their performance and identifying areas for improvement in the image‐to‐web domain.</abstract>
      <url hash="29a1103c">2025.findings-acl.334</url>
      <bibkey>guo-etal-2025-iw</bibkey>
    </paper>
    <paper id="335">
      <title><fixed-case>TDCSA</fixed-case>: <fixed-case>LLM</fixed-case>-Guided Top-Down Approach for Robust Citation Sentiment Analysis</title>
      <author><first>Fan</first><last>Gao</last></author>
      <author><first>Jieyang</first><last>Peng</last></author>
      <author><first>Xiaoming</first><last>Tao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Wang</first><last>Youzheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6467-6484</pages>
      <abstract>Citation Sentiment Analysis (CSA) plays a crucial role in understanding academic influence and knowledge diffusion. While pre-trained language models (PLMs) and large language models (LLMs) showed remarkable success in general sentiment analysis, they encounter specialized challenges in CSA due to the less significant and implicit sentiment expressions in academic writing, as well as complex sentiment transitions. % importance &amp; limitations In order to address the challenges, We propose TDCSA, a Top-Down framework that leverages LLMs’ semantic understanding capabilities to enhance PLM-based CSA, which transforms the traditional bottom-up feature engineering paradigm into a top-down architecture. % what we do Our framework consists of three key components: (1) a Dual LLM Feature Generation module for robust quadruple extraction, (2) a Multi-view Feature Representation mechanism for neutral citation processing, and (3) a Quad Feature Enhanced PLM. % how we do Experiments demonstrate that TDCSA significantly outperforms existing methods, achieving state-of-the-art performance while maintaining robustness to quadruple quality variations.</abstract>
      <url hash="26b1d3f3">2025.findings-acl.335</url>
      <bibkey>gao-etal-2025-tdcsa</bibkey>
    </paper>
    <paper id="336">
      <title><fixed-case>D</fixed-case>eep<fixed-case>RTL</fixed-case>2: A Versatile Model for <fixed-case>RTL</fixed-case>-Related Tasks</title>
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Hongji</first><last>Zhang</last></author>
      <author><first>Yunhao</first><last>Zhou</last></author>
      <author><first>Zhengyuan</first><last>Shi</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Changran</first><last>Xu</last></author>
      <author><first>Qiang</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6485-6500</pages>
      <abstract>The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code generation and understanding. While previous studies have demonstrated the efficacy of fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally critical to EDA workflows, have been largely overlooked. These tasks, including natural language code search, RTL code functionality equivalence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present <tex-math>\textbf{DeepRTL2}</tex-math>, a family of versatile LLMs that unifies both generation- and embedding-based tasks related to RTL. By simultaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse challenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the-art performance across all evaluated tasks.</abstract>
      <url hash="1cabc773">2025.findings-acl.336</url>
      <bibkey>liu-etal-2025-deeprtl2</bibkey>
    </paper>
    <paper id="337">
      <title>The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?</title>
      <author><first>Yutao</first><last>Sun</last></author>
      <author><first>Mingshuai</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tiancheng</first><last>Zhao</last><affiliation>Binjiang Institute of Zhejiang University</affiliation></author>
      <author><first>Ruochen</first><last>Xu</last><affiliation>Om AI</affiliation></author>
      <author><first>Zilun</first><last>Zhang</last><affiliation>Binjiang Insititute</affiliation></author>
      <author><first>Jianwei</first><last>Yin</last><affiliation>Zhejiang University</affiliation></author>
      <pages>6501-6512</pages>
      <abstract>Self-improving large language models (LLMs) – i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself – is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent – a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distill LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.</abstract>
      <url hash="4008e8e9">2025.findings-acl.337</url>
      <bibkey>sun-etal-2025-self</bibkey>
    </paper>
    <paper id="338">
      <title>Cross-lingual Multimodal Sentiment Analysis for Low-Resource Languages via Language Family Disentanglement and Rethinking Transfer</title>
      <author><first>Long</first><last>Chen</last><affiliation>Xi’an University of Posts and Telecommunications</affiliation></author>
      <author><first>Shuoyu</first><last>Guan</last><affiliation>XUPT</affiliation></author>
      <author><first>Xiaohua</first><last>Huang</last><affiliation>Northwest University Xi’an</affiliation></author>
      <author><first>Wen-Jing</first><last>Wang</last><affiliation>Xi’an University of Posts and Telecommunications</affiliation></author>
      <author><first>Cai</first><last>Xu</last><affiliation>Xidian University</affiliation></author>
      <author><first>Ziyu</first><last>Guan</last><affiliation>Xidian University</affiliation></author>
      <author><first>Wei</first><last>Zhao</last><affiliation>Xidian University</affiliation></author>
      <pages>6513-6522</pages>
      <abstract>Existing multimodal sentiment analysis (MSA) methods have achieved significant success, leveraging cross-modal large-scale models (LLMs) and extensive pre-training data. However, these methods struggle to handle MSA tasks in low-resource languages. While multilingual LLMs enable cross-lingual transfer, they are limited to textual data and cannot address multimodal scenarios. To achieve MSA in low-resource languages, we propose a novel transfer learning framework named Language Family Disentanglement and Rethinking Transfer (LFD-RT). During pre-training, we establish cross-lingual and cross-modal alignments, followed by a language family disentanglement module that enhances the sharing of language universals within families while reducing noise from cross-family alignments. We propose a rethinking strategy for unsupervised fine-tuning that adapts the pre-trained model to MSA tasks in low-resource languages. Experimental results demonstrate the superiority of our method and its strong language-transfer capability on target low-resource languages. We commit to making our code and data publicly available, and the access link will be provided here.</abstract>
      <url hash="c28d2140">2025.findings-acl.338</url>
      <bibkey>chen-etal-2025-cross</bibkey>
    </paper>
    <paper id="339">
      <title>Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?</title>
      <author><first>Chengda</first><last>Lu</last></author>
      <author><first>Xiaoyu</first><last>Fan</last></author>
      <author><first>Yu</first><last>Huang</last><affiliation>The Wharton School, University of Pennsylvania</affiliation></author>
      <author><first>Rongwu</first><last>Xu</last></author>
      <author><first>Jijie</first><last>Li</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6523-6546</pages>
      <abstract>Jailbreak attacks have been observed to largely fail against recent reasoning models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying mechanism remains underexplored, and relying solely on reasoning capacity may raise security concerns. In this paper, we try to answer the question: Does CoT reasoning really reduce harmfulness from jailbreaking? Through rigorous theoretical analysis, we demonstrate that CoT reasoning has dual effects on jailbreaking harmfulness. Based on the theoretical insights, we propose a novel jailbreak method, FicDetail, whose practical performance validates our theoretical findings.</abstract>
      <url hash="99512824">2025.findings-acl.339</url>
      <bibkey>lu-etal-2025-chain</bibkey>
    </paper>
    <paper id="340">
      <title><fixed-case>I</fixed-case>ntern<fixed-case>LM</fixed-case>-<fixed-case>XC</fixed-case>omposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title>
      <author><first>Yuhang</first><last>Zang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaoyi</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Pan</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yuhang</first><last>Cao</last></author>
      <author><first>Ziyu</first><last>Liu</last></author>
      <author><first>Shengyuan</first><last>Ding</last></author>
      <author><first>Shenxi</first><last>Wu</last></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Haodong</first><last>Duan</last></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>6547-6563</pages>
      <abstract>Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data.</abstract>
      <url hash="b3d3da08">2025.findings-acl.340</url>
      <bibkey>zang-etal-2025-internlm</bibkey>
    </paper>
    <paper id="341">
      <title><fixed-case>RATE</fixed-case>-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models</title>
      <author><first>Junjie</first><last>Li</last></author>
      <author><first>Nan</first><last>Zhang</last></author>
      <author><first>Xiaoyang</first><last>Qu</last></author>
      <author><first>Kai</first><last>Lu</last></author>
      <author><first>Guokuan</first><last>Li</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jiguang</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jianzong</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <pages>6564-6574</pages>
      <abstract>Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods.</abstract>
      <url hash="e48e8a8f">2025.findings-acl.341</url>
      <bibkey>li-etal-2025-rate</bibkey>
    </paper>
    <paper id="342">
      <title><fixed-case>RM</fixed-case>o<fixed-case>A</fixed-case>: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation</title>
      <author><first>Zhentao</first><last>Xie</last></author>
      <author><first>Chengcheng</first><last>Han</last></author>
      <author><first>Jinxin</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Wenjun</first><last>Cui</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xingjiao</first><last>Wu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Jiabao</first><last>Zhao</last><affiliation>Donghua University, Shanghai</affiliation></author>
      <pages>6575-6602</pages>
      <abstract>Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet’s residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at https://github.com/mindhunter01/RMoA.</abstract>
      <url hash="e07b4512">2025.findings-acl.342</url>
      <bibkey>xie-etal-2025-rmoa</bibkey>
    </paper>
    <paper id="343">
      <title>Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</title>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Chuhan</first><last>Wu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Xinyi</first><last>Dai</last></author>
      <author><first>Yan</first><last>Xu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Weinan</first><last>Gan</last><affiliation>Noah’s Ark Lab, Huawei</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>6603-6618</pages>
      <abstract>The improvement of LLMs’ instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm—Web as Instruction and Web as Response—where each web document is designated as either the input or output role to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort.</abstract>
      <url hash="b8e45345">2025.findings-acl.343</url>
      <bibkey>jiang-etal-2025-instruction</bibkey>
    </paper>
    <paper id="344">
      <title><fixed-case>RLKGF</fixed-case>: Reinforcement Learning from Knowledge Graph Feedback Without Human Annotations</title>
      <author><first>Lian</first><last>Yan</last></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Yi</first><last>Guan</last><affiliation>Harbin institute of technology</affiliation></author>
      <author><first>Haotian</first><last>Wang</last></author>
      <author><first>Songyuan</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Haifeng</first><last>Liu</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Jingchi</first><last>Jiang</last></author>
      <pages>6619-6633</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) has been shown to effectively align large language models (LLMs) with human knowledge. However, the lack of human preference labels remains a significant bottleneck when applying RLHF to a downstream domain. Humans in RLHF play a critical role in injecting reasoning preferences into LLM, and we assume the reasoning process underlying human assessments may potentially be replaced by reasoning pathways derived from Knowledge Graphs (KGs). Inspired by this assumption, we propose Reinforcement Learning from Knowledge Graph Feedback (RLKGF), a novel method that leverages KG semantics and structure to derive RL rewards in the absence of manual annotations. Unlike Reinforcement Learning from AI Feedback (RLAIF), RLKGF directly integrates human priors encoded in KGs as the reward model, aligning LLM responses with expert knowledge without additional preference labeling or reward model training. RLKGF structures context-relevant facts into knowledge subgraphs and defines rewards by simulating information flow across semantic and logical connections between question and candidate response entities. Experiments on three public and one private medical dialogue dataset demonstrate that RLKGF significantly outperforms the competitive RLAIF in improving LLM diagnostic accuracy. The code is available at <url>https://github.com/YanPioneer/RLKGF</url>.</abstract>
      <url hash="aa4a3669">2025.findings-acl.344</url>
      <bibkey>yan-etal-2025-rlkgf</bibkey>
    </paper>
    <paper id="345">
      <title>Learning Task Representations from In-Context Learning</title>
      <author><first>Baturay</first><last>Saglam</last><affiliation>Yale University</affiliation></author>
      <author><first>Xinyang</first><last>Hu</last></author>
      <author><first>Zhuoran</first><last>Yang</last></author>
      <author><first>Dionysis</first><last>Kalogerias</last><affiliation>Yale University</affiliation></author>
      <author><first>Amin</first><last>Karbasi</last></author>
      <pages>6634-6663</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable proficiency in in-context learning (ICL), where models adapt to new tasks through example-based prompts without requiring parameter updates. However, understanding how tasks are internally encoded and generalized remains a challenge. To address some of the empirical and technical gaps in the literature, we introduce an automated formulation for encoding task information in ICL prompts as a function of attention heads within the transformer architecture. This approach computes a single task vector as a weighted sum of attention heads, with the weights optimized causally via gradient descent. Our findings show that existing methods fail to generalize effectively to modalities beyond text. In response, we also design a benchmark to evaluate whether a task vector can preserve task fidelity in functional regression tasks. The proposed method successfully extracts task-specific information from in-context demonstrations and excels in both text and regression tasks, demonstrating its generalizability across modalities.</abstract>
      <url hash="fd21b8d9">2025.findings-acl.345</url>
      <bibkey>saglam-etal-2025-learning</bibkey>
    </paper>
    <paper id="346">
      <title><fixed-case>CAVGAN</fixed-case>: Unifying Jailbreak and Defense of <fixed-case>LLM</fixed-case>s via Generative Adversarial Attacks on their Internal Representations</title>
      <author><first>Xiaohu</first><last>Li</last></author>
      <author><first>Yunfeng</first><last>Ning</last></author>
      <author><first>Zepeng</first><last>Bao</last></author>
      <author><first>Mayi</first><last>Xu</last></author>
      <author><first>Jianhao</first><last>Chen</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <pages>6664-6678</pages>
      <abstract>Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security.Warning: This paper contains some harmful text examples.</abstract>
      <url hash="880db29c">2025.findings-acl.346</url>
      <bibkey>li-etal-2025-cavgan</bibkey>
    </paper>
    <paper id="347">
      <title>Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions</title>
      <author><first>Yubo</first><last>Li</last></author>
      <author><first>Yidi</first><last>Miao</last></author>
      <author><first>Xueying</first><last>Ding</last></author>
      <author><first>Ramayya</first><last>Krishnan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Rema</first><last>Padman</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>6679-6700</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent and coherent behavior across multiple rounds of user interaction. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions . First, we introduce Position-Weighted Consistency (PWC), a metric designed to capture both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present MT-Consistency, a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by explicitly integrating internal model confidence scores during the generation process. Experimental results demonstrate that CARG significantly improves response stability without sacrificing accuracy, offering a practical path toward more dependable LLM behavior in critical, real-world deployments.</abstract>
      <url hash="661e29dd">2025.findings-acl.347</url>
      <bibkey>li-etal-2025-firm</bibkey>
    </paper>
    <paper id="348">
      <title><fixed-case>OS</fixed-case>-Kairos: Adaptive Interaction for <fixed-case>MLLM</fixed-case>-Powered <fixed-case>GUI</fixed-case> Agents</title>
      <author><first>Pengzhou</first><last>Cheng</last></author>
      <author><first>Zheng</first><last>Wu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zongru</first><last>Wu</last></author>
      <author><first>Tianjie</first><last>Ju</last></author>
      <author><first>Aston</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>6701-6725</pages>
      <abstract>Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: <b>over-execution</b>, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce <i>OS-Kairos</i>, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. <i>OS-Kairos</i> is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that <i>OS-Kairos</i> substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59%~87.29% improvements in task success rate. <i>OS-Kairos</i> facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at Anonymous.</abstract>
      <url hash="5987c134">2025.findings-acl.348</url>
      <bibkey>cheng-etal-2025-os</bibkey>
    </paper>
    <paper id="349">
      <title>Red-Teaming <fixed-case>LLM</fixed-case> Multi-Agent Systems via Communication Attacks</title>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yuping</first><last>Lin</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Shen</first><last>Dong</last></author>
      <author><first>Han</first><last>Xu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <pages>6726-6747</pages>
      <abstract>Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.</abstract>
      <url hash="6e898f00">2025.findings-acl.349</url>
      <bibkey>he-etal-2025-red</bibkey>
    </paper>
    <paper id="350">
      <title>Can We Trust <fixed-case>AI</fixed-case> Doctors? A Survey of Medical Hallucination in Large Language and Large Vision-Language Models</title>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Yunyan</first><last>Zhang</last><affiliation>Jarvis Research Center, Tencent YouTu Lab</affiliation></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Fan</first><last>Zhang</last></author>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>QingqingLong</first><last>QingqingLong</last></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <pages>6748-6769</pages>
      <abstract>Hallucination has emerged as a critical challenge for large language models (LLMs) and large vision-language models (LVLMs), particularly in high-stakes medical applications. Despite its significance, dedicated research on medical hallucination remains unexplored. In this survey, we first provide a unified perspective on medical hallucination for both LLMs and LVLMs, and delve into its causes. Subsequently, we review recent advancements in detecting, evaluating, and mitigating medical hallucinations, offering a comprehensive overview of evaluation benchmarks, metrics, and strategies developed to tackle this issue. Moreover, we delineate the current challenges and delve into new frontiers, thereby shedding light on future research. We hope this work coupled with open-source resources can provide the community with quick access and spur breakthrough research in medical hallucination.</abstract>
      <url hash="9da162d3">2025.findings-acl.350</url>
      <bibkey>zhu-etal-2025-trust</bibkey>
    </paper>
    <paper id="351">
      <title><fixed-case>DRT</fixed-case>: Deep Reasoning Translation via Long Chain-of-Thought</title>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Tencent</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>6770-6782</pages>
      <abstract>Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs’ long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to quantify the translation quality in each round. In this way, we collect tens of thousands of long-thought MT data, which is used to train our DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the thought process during machine translation, and outperform vanilla LLMs as well as LLMs which are simply fine-tuning on the paired sentences without long thought, showing its effectiveness.</abstract>
      <url hash="753758b1">2025.findings-acl.351</url>
      <bibkey>wang-etal-2025-drt</bibkey>
    </paper>
    <paper id="352">
      <title><fixed-case>CTPD</fixed-case>: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis</title>
      <author><first>Fuying</first><last>Wang</last></author>
      <author><first>Feng</first><last>Wu</last></author>
      <author><first>Yihan</first><last>Tang</last></author>
      <author><first>Lequan</first><last>Yu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>6783-6799</pages>
      <abstract>Integrating multimodal clinical records—such as Electronic Health Records (EHR) and free-text clinical reports—has shown great potential in predicting clinical outcomes. However, prior work has primarily focused on capturing temporal interactions within individual samples and fusing multimodal information, overlooking critical temporal patterns across different patients. These patterns, such as trends in vital signs like abnormal heart rate or blood pressure, can indicate deteriorating health or an impending critical event of any individual in a given population. Similarly, clinical notes often contain textual descriptions that reflect these changes. Identifying corresponding temporal patterns across different modalities is crucial for improving the accuracy of clinical outcome predictions, yet it remains a challenging task. To address this gap, we introduce a Cross-modal Temporal Pattern Discovery (CTPD) framework, designed to efficiently extract meaningful cross-modal temporal patterns from multimodal EHR data. Our approach introduces shared initial temporal pattern representations and refines them using slot attention to generate temporal semantic embeddings. To ensure rich cross-modal temporal semantics in the learned patterns, we introduce a Temporal Pattern Noise Contrastive Estimation (TP-NCE) loss for cross-modal alignment, along with two reconstruction losses to retain core information of each modality. Evaluations on two clinically critical tasks—48 hour in-hospital mortality and 24-hour phenotype classification—using the MIMIC-III database demonstrate the superiority of our method over existing approaches. The code is anonymously available at https://github.com/HKU-MedAI/CTPD.</abstract>
      <url hash="85ee18b1">2025.findings-acl.352</url>
      <bibkey>wang-etal-2025-ctpd</bibkey>
    </paper>
    <paper id="353">
      <title>Vision-aided Unsupervised Constituency Parsing with Multi-<fixed-case>MLLM</fixed-case> Debating</title>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Haiyan</first><last>Tian</last><affiliation>Soochow University</affiliation></author>
      <author><first>Qingying</first><last>Sun</last></author>
      <author><first>Shoushan</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <pages>6800-6810</pages>
      <abstract>This paper presents a novel framework for vision-aided unsupervised constituency parsing (VUCP), leveraging multimodal large language models (MLLMs) pre-trained on diverse image-text or video-text data. Unlike previous methods requiring explicit cross-modal alignment, our approach eliminates this need by using pre-trained models like Qwen-VL and VideoLLaVA, which seamlessly handle multimodal inputs. We introduce two multi-agent debating mechanisms—consensus-driven (CD) and round-driven (RD)—to enable cooperation between models with complementary strengths. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on both image-text and video-text datasets for VUCP, improving robustness and accuracy.</abstract>
      <url hash="07a8722f">2025.findings-acl.353</url>
      <bibkey>zhang-etal-2025-vision</bibkey>
    </paper>
    <paper id="354">
      <title>Inter-Passage Verification for Multi-evidence Multi-answer <fixed-case>QA</fixed-case></title>
      <author><first>Bingsen</first><last>Chen</last><affiliation>New York University</affiliation></author>
      <author><first>Shenji</first><last>Wan</last><affiliation>University of Washington, University of Illinois, Urbana Champaign and New York University, Shanghai</affiliation></author>
      <author><first>Xi</first><last>Ye</last></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>NYU Shanghai</affiliation></author>
      <pages>6811-6829</pages>
      <abstract>Multi-answer question answering (QA), where questions can have many valid answers, presents a significant challenge for existing retrieval-augmented generation-based QA systems, as these systems struggle to retrieve and then synthesize a large number of evidence passages. To tackle these challenges, we propose a new multi-answer QA framework – Retrieval-augmented Independent Reading with Inter-passage Verification (RI²VER). Our framework retrieves a large set of passages and processes each passage individually to generate an initial high-recall but noisy answer set. Then we propose a new inter-passage verification pipeline that validates every candidate answer through (1) Verification Question Generation, (2) Gathering Additional Evidence, and (3) Verification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA datasets demonstrate that our framework significantly outperforms existing baselines across various model sizes, achieving an average F1 score improvement of 11.17%. Further analysis validates that our inter-passage verification pipeline enables our framework to be particularly beneficial for questions requiring multi-evidence synthesis.</abstract>
      <url hash="39238f24">2025.findings-acl.354</url>
      <bibkey>chen-etal-2025-inter</bibkey>
    </paper>
    <paper id="355">
      <title><fixed-case>PROMTEC</fixed-case>: Fast <fixed-case>LLM</fixed-case> Inference Decoding using Prompt Multi-Lookup with Template Database and Common Sequences</title>
      <author><first>Alan Chi-Man</first><last>Lee</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wing-Sun</first><last>Cheng</last><affiliation>Risksis Technology Limited</affiliation></author>
      <author><first>Calvin Chun-Kit</first><last>Chan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6830-6842</pages>
      <abstract>We propose PROMTEC, a novel multi-faceted approach to accelerate the inference of large language models (LLMs) by leveraging three key techniques: Prompt Multi-Lookup, Template Datastore, and Common Sequences methods. Prompt Multi-Lookup enhances the autoregressive decoding efficiency by generating multiple candidate sequences from context. Template Datastore exploits structured patterns, particularly in mathematical and code generation tasks, to enable fast and accurate candidate generation. Common Sequences optimize inference by precomputing frequent short sequences in specialized domains. For mathematical generation, PROMTEC achieves a 3.91 <tex-math>\times</tex-math> speedup on the miniF2F benchmark. For code generation, it achieves up to a 4.23 <tex-math>\times</tex-math> speedup on the HumanEval benchmark. This work highlights the potential of integrated candidate generation to accelerate LLM inference while maintaining high-quality outputs.</abstract>
      <url hash="81f26877">2025.findings-acl.355</url>
      <bibkey>lee-etal-2025-promtec</bibkey>
    </paper>
    <paper id="356">
      <title>Logical <fixed-case>DA</fixed-case>: Enhancing Data Augmentation for Logical Reasoning via a Multi-Agent System</title>
      <author><first>Haoqi</first><last>Zheng</last></author>
      <author><first>Dong</first><last>Wang</last></author>
      <author><first>Silin</first><last>Yang</last></author>
      <author><first>Yunpeng</first><last>Qi</last></author>
      <author><first>Ruochun</first><last>Jin</last></author>
      <author><first>Liyang</first><last>Xu</last></author>
      <pages>6843-6855</pages>
      <abstract>Recent advancements in large language models (LLMs) have highlighted the importance of improving their reasoning capabilities. A critical challenge lies in the scarcity of high-quality reasoning data—characterized by diversity and rich supervisory signals—necessary for robust model training. While data augmentation (DA) methods have been leveraged to mitigate this scarcity, prevailing approaches often introduce noise and exhibit logical inconsistencies, thereby diminishing their utility for complex reasoning tasks. Moreover, existing DA paradigms predominantly isolate data synthesis from label validation, failing to unify these complementary processes within a cohesive architecture.To address these limitations, we introduce Logical DA, a multi-agent framework for enhancing reasoning-focused data augmentation in few-shot learning scenarios. Our system includes four agents operating through two synergistic phases: (1) diverse data generation, and (2) label verification.The system incorporates a reflection mechanism to continuously improve data quality by leveraging feedback from logical validation. We demonstrate the effectiveness of Logical DA through experiments on various tasks and datasets, achieving the highest average improvement in task accuracy in both fine-tuning and in-context learning paradigms, with an average improvement of 7.61% when applied to fine-tuning.</abstract>
      <url hash="d4f54ba6">2025.findings-acl.356</url>
      <bibkey>zheng-etal-2025-logical</bibkey>
    </paper>
    <paper id="357">
      <title>Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval</title>
      <author><first>Yubai</first><last>Wei</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiale</first><last>Han</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>6856-6870</pages>
      <abstract>Text embedding models play a cornerstone role in AI applications, such as retrieval-augmented generation (RAG). While general-purpose text embedding models demonstrate strong performance on generic retrieval benchmarks, their effectiveness diminishes when applied to private datasets (e.g., company-specific proprietary data), which often contain specialized terminology and lingo. In this work, we introduce BMEmbed, a novel method for adapting general-purpose text embedding models to private datasets. By leveraging the well-established keyword-based retrieval technique (BM25), we construct supervisory signals from the ranking of keyword-based retrieval results to facilitate model adaptation. We evaluate BMEmbed across a range of domains, datasets, and models, showing consistent improvements in retrieval performance. Moreover, we provide empirical insights into how BM25-based signals contribute to improving embeddings by fostering alignment and uniformity, highlighting the value of this approach in adapting models to domain-specific data. We release the source code for the research community.</abstract>
      <url hash="836edb95">2025.findings-acl.357</url>
      <bibkey>wei-etal-2025-adapting</bibkey>
    </paper>
    <paper id="358">
      <title><fixed-case>SQL</fixed-case> Injection Jailbreak: A Structural Disaster of Large Language Models</title>
      <author><first>Jiawei</first><last>Zhao</last></author>
      <author><first>Kejiang</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6871-6891</pages>
      <abstract>Large Language Models (LLMs) are susceptible to jailbreak attacks that can induce them to generate harmful content.Previous jailbreak methods primarily exploited the internal properties or capabilities of LLMs, such as optimization-based jailbreak methods and methods that leveraged the model’s context-learning abilities. In this paper, we introduce a novel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the external properties of LLMs, specifically, the way LLMs construct input prompts. By injecting jailbreak information into user prompts, SIJ successfully induces the model to output harmful content. For open-source models, SIJ achieves near 100% attack success rates on five well-known LLMs on the AdvBench and HEx-PHI, while incurring lower time costs compared to previous methods. For closed-source models, SIJ achieves an average attack success rate over 85% across five models in the GPT and Doubao series. Additionally, SIJ exposes a new vulnerability in LLMs that urgently requires mitigation. To address this, we propose a simple adaptive defense method called Self-Reminder-Key to counter SIJ and demonstrate its effectiveness through experimental results. Our code is available at https://github.com/weiyezhimeng/SQL-Injection-Jailbreak.</abstract>
      <url hash="f21872f5">2025.findings-acl.358</url>
      <bibkey>zhao-etal-2025-sql</bibkey>
    </paper>
    <paper id="359">
      <title><fixed-case>TAMP</fixed-case>: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models</title>
      <author><first>Jaewoo</first><last>Lee</last></author>
      <author><first>Keyang</first><last>Xuan</last></author>
      <author><first>Chanakya</first><last>Ekbote</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Sandeep</first><last>Polisetty</last></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Paul Pu</first><last>Liang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>6892-6908</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have shown remarkable versatility in understanding diverse multimodal data and tasks. However, these capabilities come with an increased model scale. While post-training pruning reduces model size in unimodal models, its application to MLLMs often yields limited success. Our analysis discovers that conventional methods fail to account for the unique token attributes across layers and modalities inherent to MLLMs. Inspired by this observation, we propose TAMP, a simple yet effective pruning framework tailored for MLLMs, featuring two key components: (1) Diversity-Aware Sparsity, which adjusts sparsity ratio per layer based on diversities among multimodal output tokens, preserving more parameters in high-diversity layers; and (2) Adaptive Multimodal Input Activation, which identifies representative multimodal input tokens using attention scores to guide unstructured weight pruning. We validate our method on two state-of-the-art MLLMs: LLaVA-NeXT, designed for vision-language tasks, and VideoLLaMA2, capable of processing audio, visual, and language modalities. Empirical experiments across various multimodal evaluation benchmarks demonstrate that each component of our approach substantially outperforms existing pruning techniques. Our code is available at https://github.com/G-JWLee/TAMP</abstract>
      <url hash="8acc4e7a">2025.findings-acl.359</url>
      <bibkey>lee-etal-2025-tamp</bibkey>
    </paper>
    <paper id="360">
      <title>Generative Music Models’ Alignment with Professional and Amateur Users’ Expectations</title>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Jiaxing</first><last>Yu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haoxuan</first><last>Liu</last></author>
      <author><first>Zehui</first><last>Zheng</last></author>
      <author><first>Yuhang</first><last>Jin</last></author>
      <author><first>Shuyu</first><last>Li</last></author>
      <author><first>Shulei</first><last>Ji</last></author>
      <author><first>Kejun</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>6909-6920</pages>
      <abstract>Recent years have witnessed rapid advancements in text-to-music generation using large language models, yielding notable outputs. A critical challenge is understanding users with diverse musical expertise and generating music that meets their expectations, an area that remains underexplored.To address this gap, we introduce the novel task of Professional and Amateur Description-to-Song Generation. This task focuses on aligning generated content with human expressions from varying musical proficiency levels, aiming to produce songs that accurately meet auditory expectations and adhere to musical structural conventions. We utilized the MuChin dataset, which contains annotations from both professionals and amateurs for identical songs, as the source for these distinct description types. We also collected a pre-train dataset of over 1.5 million songs; lyrics were included for some, while for others, lyrics were generated using Automatic Speech Recognition (ASR) models.Furthermore, we propose MuDiT/MuSiT, a single-stage framework designed to enhance human-machine alignment in song generation. This framework employs Chinese MuLan (ChinMu) for cross-modal comprehension between natural language descriptions and auditory musical attributes, thereby aligning generated songs with user-defined outcomes. Concurrently, a DiT/SiT model facilitates end-to-end generation of complete songs audio, encompassing both vocals and instrumentation. We proposed metrics to evaluate semantic and auditory discrepancies between generated content and target music. Experimental results demonstrate that MuDiT/MuSiT outperforms baseline models and exhibits superior alignment with both professional and amateur song descriptions.</abstract>
      <url hash="53d945a9">2025.findings-acl.360</url>
      <bibkey>wang-etal-2025-generative</bibkey>
    </paper>
    <paper id="361">
      <title><fixed-case>LLM</fixed-case>-Forest: Ensemble Learning of <fixed-case>LLM</fixed-case>s with Graph-Augmented Prompts for Data Imputation</title>
      <author><first>Xinrui</first><last>He</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Yikun</first><last>Ban</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Jiaru</first><last>Zou</last></author>
      <author><first>Tianxin</first><last>Wei</last></author>
      <author><first>Curtiss</first><last>Cook</last></author>
      <author><first>Jingrui</first><last>He</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>6921-6936</pages>
      <abstract>Missing data imputation is a critical challenge in various domains, such as healthcare and finance, where data completeness is vital for accurate analysis. Large language models (LLMs), trained on vast corpora, have shown strong potential in data generation, making them a promising tool for data imputation. However, challenges persist in designing effective prompts for a finetuning-free process and in mitigating biases and uncertainty in LLM outputs. To address these issues, we propose a novel framework, LLM-Forest, which introduces a “forest” of few-shot learning LLM “trees” with their outputs aggregated via confidence-based weighted voting based on LLM self-assessment, inspired by the ensemble learning (Random Forest). This framework is established on a new concept of bipartite information graphs to identify high-quality relevant neighboring entries with both feature and value granularity. Extensive experiments on 9 real-world datasets demonstrate the effectiveness and efficiency of LLM-Forest. The implementation is available at https://github.com/Xinrui17/LLM-Forest</abstract>
      <url hash="67def0f1">2025.findings-acl.361</url>
      <bibkey>he-etal-2025-llm</bibkey>
    </paper>
    <paper id="362">
      <title>Task Calibration: Calibrating Large Language Models on Inference Tasks</title>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yun</first><last>Luo</last><affiliation>westlake university</affiliation></author>
      <author><first>Xiaotian</first><last>Xie</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>6937-6951</pages>
      <abstract>Large language models (LLMs) have exhibited impressive zero-shot performance on inference tasks. However, LLMs may suffer from spurious correlations between input texts and output labels, which limits LLMs’ ability to reason based purely on general language understanding. For example, in the natural language inference (NLI) task, LLMs may make predictions primarily based on premise or hypothesis, rather than both components. To address this problem that may lead to unexpected performance degradation, we propose task calibration (TC), a zero-shot and inference-only calibration method inspired by mutual information which recovers LLM performance through task reformulation. In NLI, TC encourages LLMs to reason based on both premise and hypothesis, while mitigating the models’ over-reliance on individual premise or hypothesis for inference. Experimental results show that TC achieves a substantial improvement on 13 different benchmarks in the zero-shot setup. We further validate the effectiveness of TC in few-shot setups and various natural language understanding tasks. Further analysis indicates that TC is also robust to prompt templates and has the potential to be integrated with other calibration methods. We publicly release our code to facilitate future research.</abstract>
      <url hash="af511a01">2025.findings-acl.362</url>
      <bibkey>li-etal-2025-task</bibkey>
    </paper>
    <paper id="363">
      <title><fixed-case>M</fixed-case>ini<fixed-case>ELM</fixed-case>: A Lightweight and Adaptive Query Rewriting Framework for <fixed-case>E</fixed-case>-Commerce Search Optimization</title>
      <author><first>Duy A.</first><last>Nguyen</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Rishi Kesav</first><last>Mohan</last></author>
      <author><first>Shimeng</first><last>Yang</last></author>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>6952-6964</pages>
      <abstract>Query rewriting (QR) is a critical technique in e-commerce search, addressing the lexical gap between user queries and product descriptions to enhance search performance. Existing QR approaches typically fall into two categories: discriminative models and generative methods leveraging large language models (LLMs). Discriminative models often struggle with natural language understanding and offer limited flexibility in rewriting, while generative LLMs, despite producing high-quality rewrites, face high inference latency and cost in online settings. These limitations force offline deployment, making them vulnerable to issues like information staleness and semantic drift. To overcome these challenges, we propose a novel hybrid pipeline for QR that balances efficiency and effectiveness. Our approach combines **offline knowledge distillation** to create a lightweight but efficient student model with **online reinforcement learning (RL)** to refine query rewriting dynamically using real-time feedback. A key innovation is the use of LLMs as **simulated human feedback**, enabling scalable reward signals and cost-effective evaluation without manual annotations. Experimental results on Amazon ESCI dataset demonstrate significant improvements in query relevance, diversity, and adaptability, as well as positive feedback from the LLM simulation. This work contributes to advancing LLM capabilities for domain-specific applications, offering a robust solution for dynamic and complex e-commerce search environments.</abstract>
      <url hash="4ed50b42">2025.findings-acl.363</url>
      <bibkey>nguyen-etal-2025-minielm</bibkey>
    </paper>
    <paper id="364">
      <title>Visibility as Survival: Generalizing <fixed-case>NLP</fixed-case> for Native Alaskan Language Identification</title>
      <author><first>Ivory</first><last>Yang</last></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Yuxin</first><last>Wang</last></author>
      <author><first>Zhongyu</first><last>Ouyang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>6965-6979</pages>
      <abstract>Indigenous languages remain largely invisible in commercial language identification (LID) systems, a stark reality exemplified by Google Translate’s LangID tool, which supports over 100 languages but excludes all 150 Indigenous languages of North America. This technological marginalization is particularly acute for Alaska’s 20 Native languages, all of which face endangerment despite their rich linguistic heritage. We present GenAlaskan, a framework demonstrating how both large language models and specialized classifiers can effectively identify these languages with minimal data. Working closely with Native Alaskan community members, we create Akutaq-2k, a carefully curated dataset of 2000 sentences spanning all 20 languages, named after the traditional Yup’ik dessert, symbolizing the blending of diverse elements. We design few-shot prompting on proprietary and open-source LLMs, achieving nearly perfect accuracy with just 40 examples per language. While initial zero-shot attempts show limited success, our systematic attention head pruning revealed critical architectural components for accurate language differentiation, providing insights into model decision-making for low-resource languages. Our results challenge the notion that effective Indigenous language identification requires massive resources or corporate infrastructure, demonstrating that targeted technological interventions can drive meaningful progress in preserving endangered languages in the digital age.</abstract>
      <url hash="198cf990">2025.findings-acl.364</url>
      <bibkey>yang-etal-2025-visibility</bibkey>
    </paper>
    <paper id="365">
      <title><fixed-case>K</fixed-case>od<fixed-case>C</fixed-case>ode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding</title>
      <author><first>Zhangchen</first><last>Xu</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Yueqin</first><last>Yin</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Mingyuan</first><last>Zhou</last><affiliation>Google and The University of Texas at Austin</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>6980-7008</pages>
      <abstract>We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question–solution–test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. It is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.</abstract>
      <url hash="100754bc">2025.findings-acl.365</url>
      <bibkey>xu-etal-2025-kodcode</bibkey>
    </paper>
    <paper id="366">
      <title>Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation</title>
      <author><first>Xiaochuan</first><last>Liu</last></author>
      <author><first>Ruihua</first><last>Song</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>7009-7028</pages>
      <abstract>Automatic related work generation (RWG) can save people’s time and effort when writing a draft of related work section (RWS) for further revision. However, existing methods for RWG always suffer from shallow comprehension due to taking the limited portions of references papers as input and isolated explanation for each reference due to ineffective capturing the relationships among them. To address these issues, we focus on full-text-based RWG task and propose a novel multi-agent framework. Our framework consists of three agents: a selector that decides which section of the papers is going to read next, a reader that digests the selected section and updates a shared working memory, and a writer that generates RWS based on the final curated memory. To better capture the relationships among references, we also propose two graph-aware strategies for selector, enabling to optimize the reading order with constrains of the graph structure. Extensive experiments demonstrate that our framework consistently improves performance across three base models and various input configurations. The graph-aware selectors outperform alternative selectors, achieving state-of-the-art results. The code and data are available at https://github.com/1190200817/Full_Text_RWG.</abstract>
      <url hash="19e3eb33">2025.findings-acl.366</url>
      <bibkey>liu-etal-2025-select</bibkey>
    </paper>
    <paper id="367">
      <title>Graph-Assisted Culturally Adaptable Idiomatic Translation for <fixed-case>I</fixed-case>ndic languages</title>
      <author><first>Pratik Rakesh</first><last>Singh</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Kritarth</first><last>Prasad</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Mohammadi</first><last>Zaki</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Pankaj</first><last>Wasnik</last><affiliation>Sony Research India</affiliation></author>
      <pages>7029-7044</pages>
      <abstract>Translating multi-word expressions (MWEs) and idioms requires a deep understanding of the cultural nuances of both the source and target languages. This challenge is further amplified by the one-to-many nature of idiomatic translations, where a single source idiom can have multiple target-language equivalents depending on cultural references and contextual variations. Traditional static knowledge graphs (KGs) and prompt-based approaches struggle to capture these complex relationships, often leading to suboptimal translations. To address this, we propose an IdiomCE, an adaptive graph neural network (GNN) based methodology that learns intricate mappings between idiomatic expressions, effectively generalizing to both seen and unseen nodes during training. Our proposed method enhances translation quality even in resource-constrained settings, facilitating improved idiomatic translation in smaller models. We evaluate our approach on multiple idiomatic translation datasets using reference-less metrics, demonstrating significant improvements in translating idioms from English to various Indian languages</abstract>
      <url hash="f2a3626a">2025.findings-acl.367</url>
      <bibkey>singh-etal-2025-graph</bibkey>
    </paper>
    <paper id="368">
      <title>Question Answering in Climate Adaptation for Agriculture: Model Development and Evaluation with Expert Feedback</title>
      <author><first>Vincent</first><last>Nguyen</last></author>
      <author><first>Sarvnaz</first><last>Karimi</last><affiliation>CSIRO</affiliation></author>
      <author><first>Willow</first><last>Hallgren</last><affiliation>CSIRO</affiliation></author>
      <author><first>Mahesh</first><last>Prakash</last></author>
      <pages>7045-7075</pages>
      <abstract>The generative capabilities of the large language models (LLMs) are deployed for domain-specific question answering systems. However, their ability to answer climate adaptation questions remains unclear. In particular, can they be used by agronomists and climate scientists to answer questions on the best climate adaptation strategies? Answering questions in this domain requires knowledge of climate data and its uncertainties, and the ability to link them to the broader climate literature while accommodating the unique constraints of users and experts. We investigate the generative and evaluative capabilities of several state-of-the-art LLMs, open-source and proprietary, on climate adaptation for agriculture questions posed by domain experts using evaluation criteria designed by the experts.We propose an iterative exploration framework that enables LLMs to dynamically aggregate information from heterogeneous sources, such as text from climate literature and structured tabular climate data from climate model projections and historical observations. Our experiments demonstrate that LLMs can aggregate heterogeneous data to (1) answer questions, but at a trade-off between presentation quality and epistemological accuracy; and, (2) evaluate answers, but are not as competent at identifying high-quality answers and erroneous information compared to domain experts.</abstract>
      <url hash="b708ae56">2025.findings-acl.368</url>
      <bibkey>nguyen-etal-2025-question</bibkey>
    </paper>
    <paper id="369">
      <title><fixed-case>AGR</fixed-case>ec: Adapting Autoregressive Decoders with Graph Reasoning for <fixed-case>LLM</fixed-case>-based Sequential Recommendation</title>
      <author><first>Xinfeng</first><last>Wang</last></author>
      <author><first>Jin</first><last>Cui</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last><affiliation>Yamanashi University</affiliation></author>
      <author><first>Yoshimi</first><last>Suzuki</last><affiliation>Yamanashi University</affiliation></author>
      <pages>7076-7090</pages>
      <abstract>Autoregressive decoders in large language models (LLMs) excel at capturing users’ sequential behaviors for generative recommendations. However, they inherently struggle to leverage graph-structured user-item interactions, which are widely recognized as beneficial. This paper presents AGRec, adapting LLMs’ decoders with graph reasoning for recommendation. We reveal that LLMs and graph neural networks (GNNs) manifest complementary strengths in distinct user domains. Building on this, we augment the decoding logits of LLMs with an auxiliary GNN model to optimize token generation. Moreover, we introduce a rankable finite state machine to tackle two challenges: (1) adjusting autoregressive generation with discriminative decoders that directly predict user-item similarity, and (2) token homogeneity, where LLMs often generate items with similar prefix tokens, narrowing the scope of beam search. This approach offers a novel perspective to enhance LLMs with graph knowledge. Our AGRec outperforms state-of-the-art models in sequential recommendations. Our code is available online.</abstract>
      <url hash="a657f785">2025.findings-acl.369</url>
      <bibkey>wang-etal-2025-agrec</bibkey>
    </paper>
    <paper id="370">
      <title>Causal Denoising Prototypical Network for Few-Shot Multi-label Aspect Category Detection</title>
      <author><first>Jin</first><last>Cui</last></author>
      <author><first>Xinfeng</first><last>Wang</last></author>
      <author><first>Yoshimi</first><last>Suzuki</last><affiliation>Yamanashi University</affiliation></author>
      <author><first>Fumiyo</first><last>Fukumoto</last><affiliation>Yamanashi University</affiliation></author>
      <pages>7091-7104</pages>
      <abstract>The multi-label aspect category detection (MACD) task has attracted great attention in sentiment analysis. Many recent methods have formulated the MACD task by learning robust prototypes to represent categories with limited support samples. However, few of them address the noise categories in the support set that hinder their models from effective prototype generations. To this end, we propose a causal denoising prototypical network (CDPN) for few-shot MACD. We reveal the underlying relation between causal inference and contrastive learning, and present causal contrastive learning (CCL) using discrete and continuous noise as negative samples. We empirically found that CCL can (1) prevent models from overly predicting more categories and (2) mitigate semantic ambiguity issues among categories. Experimental results show that CDPN outperforms competitive baselines. Our code is available online.</abstract>
      <url hash="a4429656">2025.findings-acl.370</url>
      <bibkey>cui-etal-2025-causal</bibkey>
    </paper>
    <paper id="371">
      <title><fixed-case>R</fixed-case>eal<fixed-case>H</fixed-case>i<fixed-case>TB</fixed-case>ench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating <fixed-case>LLM</fixed-case>-Based Table Analysis</title>
      <author><first>Pengzuo</first><last>Wu</last><affiliation>Zhejiang Lab</affiliation></author>
      <author><first>Yuhang</first><last>Yang</last></author>
      <author><first>Guangcheng</first><last>Zhu</last></author>
      <author><first>Chao</first><last>Ye</last></author>
      <author><first>Hong</first><last>Gu</last><affiliation>Hangzhou VIVO Information Technology Co., Ltd</affiliation></author>
      <author><first>Xu</first><last>Lu</last></author>
      <author><first>Ruixuan</first><last>Xiao</last></author>
      <author><first>Bowen</first><last>Bao</last></author>
      <author><first>Yijing</first><last>He</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Liangyu</first><last>Zha</last></author>
      <author><first>Wentao</first><last>Ye</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>7105-7137</pages>
      <abstract>With the rapid advancement of Large Language Models (LLMs), there is an increasing need for challenging benchmarks to evaluate their capabilities in handling complex tabular data. However, existing benchmarks are either based on outdated data setups or focus solely on simple, flat table structures. In this paper, we introduce **RealHiTBench**, a comprehensive benchmark designed to evaluate the performance of both LLMs and Multimodal LLMs (MLLMs) across a variety of input formats for complex tabular data, including LaTeX, HTML, and PNG. RealHiTBench also includes a diverse collection of tables with intricate structures, spanning a wide range of task types. Our experimental results, using **25** state-of-the-art LLMs, demonstrate that RealHiTBench is indeed a challenging benchmark. Moreover, we also develop TreeThinker, a tree-based agent that organizes hierarchical headers into a tree structure for enhanced tabular reasoning, validating the importance of improving LLMs’ perception of table hierarchies. We hope that our work will inspire further research on tabular data reasoning and the development of more robust models. The code and data are available at https://github.com/cspzyy/RealHiTBench.</abstract>
      <url hash="4b43e8ac">2025.findings-acl.371</url>
      <bibkey>wu-etal-2025-realhitbench</bibkey>
    </paper>
    <paper id="372">
      <title>A Query-Response Framework for Whole-Page Complex-Layout Document Image Translation with Relevant Regional Concentration</title>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Yaping</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yupu</first><last>Liang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiyuan</first><last>Chen</last></author>
      <author><first>Lu</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7138-7149</pages>
      <abstract>Document Image Translation (DIT), which aims at translating documents in images from source language to the target, plays an important role in Document Intelligence. It requires a comprehensive understanding of document multi-modalities and a focused concentration on relevant textual regions during translation. However, most existing methods usually rely on the vanilla encoder-decoder paradigm, severely losing concentration on key regions that are especially crucial for complex-layout document translation. To tackle this issue, in this paper, we propose a new Query-Response DIT framework (QRDIT). QRDIT reformulates the DIT task into a parallel response/translation process of the multiple queries (i.e., relevant source texts), explicitly centralizing its focus toward the most relevant textual regions to ensure translation accuracy. A novel dynamic aggregation mechanism is also designed to enhance the text semantics in query features toward translation. Extensive experiments in four translation directions on three benchmarks demonstrate its state-of-the-art performance, showing significant translation quality improvements toward whole-page complex-layout document images.</abstract>
      <url hash="7a71df9d">2025.findings-acl.372</url>
      <bibkey>zhang-etal-2025-query</bibkey>
    </paper>
    <paper id="373">
      <title><fixed-case>D</fixed-case>epend<fixed-case>E</fixed-case>val: Benchmarking <fixed-case>LLM</fixed-case>s for Repository Dependency Understanding</title>
      <author><first>Junjia</first><last>Du</last></author>
      <author><first>Yadi</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Jiawei</first><last>Wang</last></author>
      <author><first>Haojian</first><last>Huang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yunyi</first><last>Ni</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>7150-7179</pages>
      <abstract>While large language models (LLMs) have shown considerable promise in code generation, real-world software development demands advanced repository-level reasoning. This includes understanding dependencies, project structures, and managing multi-file changes. However, the ability of LLMs to effectively comprehend and handle complex code repositories has yet to be fully explored. To address these challenges, we introduce a hierarchical benchmark designed to evaluate repository dependency understanding(DependEval) for LLMs. The benchmark is based on 2683 repositories collected from real-world websites. It evaluates models on three core tasks: Dependency Recognition, Repository Construction, and Multi-file Editing, across 8 programming languages from actual code repositories. Our evaluation of over 25 LLMs reveals substantial performance gaps and provides valuable insights into repository-level code understanding.</abstract>
      <url hash="0142c8b3">2025.findings-acl.373</url>
      <bibkey>du-etal-2025-dependeval</bibkey>
    </paper>
    <paper id="374">
      <title>A General Knowledge Injection Framework for <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Xu</first><last>Zhang</last></author>
      <author id="kun-zhang"><first>Kun</first><last>Zhang</last></author>
      <author><first>Wenxin</first><last>Ma</last></author>
      <author><first>Rongsheng</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Chenxu</first><last>Wu</last></author>
      <author><first>Yingtai</first><last>Li</last></author>
      <author><first>S Kevin</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>7180-7189</pages>
      <abstract>ICD Coding aims to assign a wide range of medical codes to a medical text document, which is a popular and challenging task in the healthcare domain. To alleviate the problems of long-tail distribution and the lack of annotations of code-specific evidence, many previous works have proposed incorporating code knowledge to improve coding performance. However, existing methods often focus on a single type of knowledge and design specialized modules that are complex and incompatible with each other, thereby limiting their scalability and effectiveness. To address this issue, we propose GKI-ICD, a novel, general knowledge injection framework that integrates three key types of knowledge, namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized design of additional modules. The comprehensive utilization of the above knowledge, which exhibits both differences and complementarity, can effectively enhance the ICD coding performance. Extensive experiments on existing popular ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves the state-of-the-art performance on most evaluation metrics. Code is available at https://github.com/xuzhang0112/GKI-ICD.</abstract>
      <url hash="7b6eca07">2025.findings-acl.374</url>
      <bibkey>zhang-etal-2025-general</bibkey>
    </paper>
    <paper id="375">
      <title><fixed-case>MMU</fixed-case>nlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models</title>
      <author><first>Jiahao</first><last>Huo</last><affiliation>The Hong Kong University of Science and Technology and Tongji University</affiliation></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xu</first><last>Zheng</last></author>
      <author><first>Yuanhuiyi</first><last>Lyu</last></author>
      <author><first>Xin</first><last>Zou</last></author>
      <author><first>Zhihua</first><last>Wei</last><affiliation>Tongji University</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>7190-7206</pages>
      <abstract>Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to **reformulate the task of multimodal MU in the era of MLLMs**, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we **develop a novel geometry-constrained gradient ascent method MMUnlearner**. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.</abstract>
      <url hash="79f4456f">2025.findings-acl.375</url>
      <bibkey>huo-etal-2025-mmunlearner</bibkey>
    </paper>
    <paper id="376">
      <title>Generating Questions, Answers, and Distractors for Videos: Exploring Semantic Uncertainty of Object Motions</title>
      <author><first>Wenjian</first><last>Ding</last></author>
      <author><first>Yao</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Zhenglu</first><last>Yang</last><affiliation>Nankai University</affiliation></author>
      <pages>7207-7220</pages>
      <abstract>Video Question-Answer-Distractors (QADs) show promising values for assessing the performance of systems in perceiving and comprehending multimedia content. Given the significant cost and labor demands of manual annotation, existing large-scale Video QADs benchmarks are typically generated automatically using video captions. Since video captions are incomplete representations of visual content and susceptible to error propagation, direct generation of QADs from video is crucial. This work first leverages a large vision-language model for video QADs generation. To enhance the consistency and diversity of the generated QADs, we propose utilizing temporal motion to describe the video objects. In addition, We design a selection mechanism that chooses diverse temporal object motions to generate diverse QADs focusing on different objects and interactions, maximizing overall semantic uncertainty for a given video. Evaluation on the NExT-QA and Perception Test benchmarks demonstrates that the proposed approach significantly improves both the consistency and diversity of QADs generated by a range of large vision-language models, thus highlighting its effectiveness and generalizability.</abstract>
      <url hash="86bf10c9">2025.findings-acl.376</url>
      <bibkey>ding-etal-2025-generating</bibkey>
    </paper>
    <paper id="377">
      <title><fixed-case>D</fixed-case>iff<fixed-case>S</fixed-case>kip: Differential Layer Skipping in Large Language Models</title>
      <author><first>Xuan</first><last>Luo</last></author>
      <author><first>Weizhi</first><last>Wang</last></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>7221-7231</pages>
      <abstract>Existing Large Language Models (LLMs) enforce uniform computation across all tokens. We analyze the correlation between the input-output difference of self-attention block and Feed-Forward Network (FFN) within the same transformer layer, and find that these two differential vectors are highly correlated. Thus, we propose to dynamically skip the FFN blocks based on the self-attention difference and introduce Diffential Layer Skipping (DiffSkip) to show that LLMs are inherently dynamic-depth models, capable of adjusting computational depth when generating different tokens. DiffSkip employs a lightweight router module to dynamically skip a set of FFN blocks in LLMs and only requires efficient fine-tuning while keeping the whole LLM frozen. Experimental results demonstrate that DiffSkip effectively enables dynamic FFN skipping in decoder-only language models, even in continuous token generation tasks where many layer-skipping methods struggle.</abstract>
      <url hash="949063ec">2025.findings-acl.377</url>
      <bibkey>luo-etal-2025-diffskip</bibkey>
    </paper>
    <paper id="378">
      <title>Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework</title>
      <author><first>Zihao</first><last>Jiang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ben</first><last>Liu</last></author>
      <author><first>Miao</first><last>Peng</last></author>
      <author><first>Wenjie</first><last>Xu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Yao</first><last>Xiao</last><affiliation>Wuhan University and Central China Normal University</affiliation></author>
      <author><first>Zhenyan</first><last>Shan</last></author>
      <author><first>Min</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <pages>7232-7251</pages>
      <abstract>While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs’ capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating robust generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.</abstract>
      <url hash="cb68300c">2025.findings-acl.378</url>
      <bibkey>jiang-etal-2025-towards-explainable</bibkey>
    </paper>
    <paper id="379">
      <title>A Bounding Box is Worth One Token - Interleaving Layout and Text in a Large Language Model for Document Understanding</title>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Yanjie</first><last>Wang</last><affiliation>ByteDance Inc</affiliation></author>
      <author><first>Yongjie</first><last>Ye</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jingqun</first><last>Tang</last></author>
      <author><first>Ziwei</first><last>Yang</last></author>
      <author><first>Binghong</first><last>Wu</last><affiliation>Tencent Hunyuan Team</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>Bytedance Inc.</affiliation></author>
      <author><first>Hao</first><last>Feng</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Hao</first><last>Liu</last><affiliation>Bytedance</affiliation></author>
      <author><first>Can</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <pages>7252-7273</pages>
      <abstract>Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout andText in a Large Language Model (LayTextLLM) for document understanding. LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in KIE and VQA. Comprehensive benchmark evaluations reveal significant improvements of LayTextLLM, with a 15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA OCR-based LLMs. All resources are available at URL masked for anonymous review.</abstract>
      <url hash="19964a46">2025.findings-acl.379</url>
      <bibkey>lu-etal-2025-bounding</bibkey>
    </paper>
    <paper id="380">
      <title>Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation</title>
      <author><first>Mingzhe</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xin</first><last>Lu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>7274-7289</pages>
      <abstract>Large language models (LLMs) with instruction following capabilities have demonstrated impressive problem-solving abilities. While synthesizing instructional data from unsupervised text has become a common approach for training such models, conventional methods rely heavily on human effort for data annotation. Although existing automated synthesis paradigms have alleviated this constraint, they still exhibit significant limitations in ensuring adequate diversity and difficulty of synthesized instructions. To address these challenges, we propose Self-Foveate, an innovative LLM-driven method for instruction synthesis. This approach introduces a “Micro-Scatter-Macro” multi-level foveation methodology that effectively guides the LLM to deeply excavate fine-grained information embedded in unsupervised text, thereby enhancing both the diversity and difficulty of synthesized instructions. Comprehensive experiments across multiple unsupervised corpora and diverse model architectures validate the effectiveness and superiority of our proposed method. We publicly release our data and codes: https://github.com/Mubuky/Self-Foveate</abstract>
      <url hash="2025c21b">2025.findings-acl.380</url>
      <bibkey>li-etal-2025-self-foveate</bibkey>
    </paper>
    <paper id="381">
      <title><fixed-case>T</fixed-case>able<fixed-case>D</fixed-case>reamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning</title>
      <author><first>Mingyu</first><last>Zheng</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhifan</first><last>Feng</last><affiliation>Baidu</affiliation></author>
      <author><first>Jia</first><last>Wang</last></author>
      <author><first>Lanrui</first><last>Wang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Weiping</first><last>Wang</last><affiliation>IIE</affiliation></author>
      <pages>7290-7315</pages>
      <abstract>Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (<tex-math>49.07\rightarrow60.69</tex-math>) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data.</abstract>
      <url hash="e92dad4f">2025.findings-acl.381</url>
      <bibkey>zheng-etal-2025-tabledreamer</bibkey>
    </paper>
    <paper id="382">
      <title>Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition</title>
      <author><first>Nagham</first><last>Hamad</last><affiliation>Birzeit University and Palestine Technical University - Kadoorie</affiliation></author>
      <author><first>Mohammed</first><last>Khalilia</last><affiliation>Birzeit University and Qualtrics XM</affiliation></author>
      <author><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <pages>7316-7331</pages>
      <abstract>We introduce , a novel multi-dimensional corpus covering 16 Arabic dialects across 10 domains, resulting in 160 distinct corpora. The corpus comprises about 777k tokens, carefully collected and manually annotated with 21 entity types using both nested and flat annotation schemes - using the Wojood guidelines. While is useful for various NLP tasks like domain adaptation and transfer learning, this paper primarily focuses on benchmarking existing Arabic Named Entity Recognition (NER) models, especially cross-domain and cross-dialect model performance. Our benchmarking of four Arabic NER models using reveals a significant drop in performance of up to 38% when compared to the in-distribution data. Furthermore, we present an in-depth analysis of domain and dialect divergence and the impact of resource scarcity. We also measured the overlap between domains and dialects using the Maximum Mean Discrepancy (MMD) metric, and illustrated why certain NER models perform better on specific dialects and domains. is open-source and publicly available at <url>https://sina.birzeit.edu/wojood/#download</url></abstract>
      <url hash="a0226f88">2025.findings-acl.382</url>
      <bibkey>hamad-etal-2025-konooz</bibkey>
    </paper>
    <paper id="383">
      <title>Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation</title>
      <author><first>Hongji</first><last>Yang</last><affiliation>University of Macau</affiliation></author>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Macau</affiliation></author>
      <author><first>Wencheng</first><last>Han</last></author>
      <author><first>Jianbing</first><last>Shen</last><affiliation>University of Macau</affiliation></author>
      <pages>7332-7349</pages>
      <abstract>Text-to-image models are powerful for producing high-quality images based on given text prompts, but crafting these prompts often requires specialized vocabulary. To address this, existing methods train rewriting models with supervision from large amounts of manually annotated data and trained aesthetic assessment models. To alleviate the dependence on data scale for model training and the biases introduced by trained models, we propose a novel prompt optimization framework, designed to rephrase a simple user prompt into a sophisticated prompt to a text-to-image model. Specifically, we employ the large vision language models (LVLMs) as the solver to rewrite the user prompt, and concurrently, employ LVLMs as a reward model to score the aesthetics and alignment of the images generated by the optimized prompt. Instead of laborious human feedback, we exploit the prior knowledge of the LVLM to provide rewards, i.e., AI feedback. Simultaneously, the solver and the reward model are unified into one model and iterated in reinforcement learning to achieve self-improvement by giving a solution and judging itself. Results on two popular datasets demonstrate that our method outperforms other strong competitors.</abstract>
      <url hash="7a15812b">2025.findings-acl.383</url>
      <bibkey>yang-etal-2025-self-rewarding</bibkey>
    </paper>
    <paper id="384">
      <title><fixed-case>C</fixed-case>ode<fixed-case>V</fixed-case>: Issue Resolving with Visual Data</title>
      <author><first>Linhao</first><last>Zhang</last></author>
      <author><first>Daoguang</first><last>Zan</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Quanshun</first><last>Yang</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhirong</first><last>Huang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Dong</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Bo</first><last>Shen</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Yongshun</first><last>Gong</last><affiliation>Shandong University</affiliation></author>
      <author><first>Huang</first><last>Pengjie</last></author>
      <author><first>Xudong</first><last>Lu</last></author>
      <author><first>Guangtai</first><last>Liang</last></author>
      <author><first>Lizhen</first><last>Cui</last><affiliation>Shandong University</affiliation></author>
      <author><first>Qianxiang</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <pages>7350-7361</pages>
      <abstract>Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CodeV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CodeV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CodeV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues.</abstract>
      <url hash="11538caa">2025.findings-acl.384</url>
      <bibkey>zhang-etal-2025-codev</bibkey>
    </paper>
    <paper id="385">
      <title>A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions</title>
      <author><first>Hongbin</first><last>Na</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Yining</first><last>Hua</last></author>
      <author><first>Zimu</first><last>Wang</last><affiliation>Monash University and University of Liverpool</affiliation></author>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Beibei</first><last>Yu</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Lilin</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Xi’an Jiaotong-Liverpool University</affiliation></author>
      <author><first>John</first><last>Torous</last><affiliation>Harvard University</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>7362-7376</pages>
      <abstract>Mental health is increasingly critical in contemporary healthcare, with psychotherapy demanding dynamic, context-sensitive interactions that traditional NLP methods struggle to capture. Large Language Models (LLMs) offer significant potential for addressing this gap due to their ability to handle extensive context and multi-turn reasoning. This review introduces a conceptual taxonomy dividing psychotherapy into interconnected stages–assessment, diagnosis, and treatment–to systematically examine LLM advancements and challenges. Our comprehensive analysis reveals imbalances in current research, such as a focus on common disorders, linguistic biases, fragmented methods, and limited theoretical integration. We identify critical challenges including capturing dynamic symptom fluctuations, overcoming linguistic and cultural biases, and ensuring diagnostic reliability. Highlighting future directions, we advocate for continuous multi-stage modeling, real-time adaptive systems grounded in psychological theory, and diversified research covering broader mental disorders and therapeutic approaches, aiming toward more holistic and clinically integrated psychotherapy LLMs systems.</abstract>
      <url hash="3604e742">2025.findings-acl.385</url>
      <bibkey>na-etal-2025-survey</bibkey>
    </paper>
    <paper id="386">
      <title>Breaking the Reasoning Barrier A Survey on <fixed-case>LLM</fixed-case> Complex Reasoning through the Lens of Self-Evolution</title>
      <author><first>Tao</first><last>He</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Hao</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jingchang</first><last>Chen</last></author>
      <author><first>Runxuan</first><last>Liu</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Zihao</first><last>Zheng</last></author>
      <author><first>Zheng</first><last>Chu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jiafeng</first><last>Liang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>7377-7417</pages>
      <abstract>The release of OpenAI’s O1 and subsequent projects like DeepSeek R1 has significantly advanced research on complex reasoning in LLMs. This paper systematically analyzes existing reasoning studies from the perspective of self-evolution, structured into three components: data evolution, model evolution, and self-evolution. Data evolution explores methods to generate higher-quality reasoning training data. Model evolution focuses on training strategies to boost reasoning capabilities. Self-evolution research autonomous system evolution via iterating cycles of data and model evolution. We further discuss the scaling law of self-evolution and analyze representative O1-like works through this lens. By summarizing advanced methods and outlining future directions, this paper aims to drive advancements in LLMs’ reasoning abilities.</abstract>
      <url hash="2d179cda">2025.findings-acl.386</url>
      <bibkey>he-etal-2025-breaking</bibkey>
    </paper>
    <paper id="387">
      <title><fixed-case>SEE</fixed-case>: Continual Fine-tuning with Sequential Ensemble of Experts</title>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Yafu</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>7418-7432</pages>
      <abstract>Continual fine-tuning of large language models (LLMs) suffers from catastrophic forgetting. Rehearsal-based methods mitigate this problem by retaining a small set of old data. Nevertheless, they still suffer inevitable performance loss. Although training separate experts for each task can help prevent forgetting, effectively assembling them remains a challenge. Some approaches use routers to assign tasks to experts, but in continual learning, they often require retraining for optimal performance. To address these challenges, we introduce the Sequential Ensemble of Experts (SEE) framework. SEE removes the need for an additional router, allowing each expert to independently decide whether a query should be handled. The framework employs distributed routing, and during continual fine-tuning, SEE only requires the training of new experts for incoming tasks, rather than retraining the entire system. Experiments reveal that the SEE outperforms prior approaches, including multi-task learning, in continual fine-tuning. It also demonstrates remarkable generalization ability, as the expert can effectively identify out-of-distribution queries, which can then be directed to a more generalized model for resolution. This work highlights the promising potential of integrating routing and response mechanisms within each expert, paving the way for the future of distributed model ensembling.</abstract>
      <url hash="b2a667a8">2025.findings-acl.387</url>
      <bibkey>wang-etal-2025-see</bibkey>
    </paper>
    <paper id="388">
      <title>Boosting Policy and Process Reward Models with <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search in Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Chi-Min</first><last>Chan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Junqi</first><last>Zhu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Donghai</first><last>Hong</last></author>
      <author><first>Pengcheng</first><last>Wen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chunyang</first><last>Jiang</last></author>
      <author><first>Zhen</first><last>Ye</last></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Xue</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>7433-7451</pages>
      <abstract>The recent introduction of OpenAI’s O1/O3 model represents a significant milestone in developing strong reasoning capabilities in Large Language Models (LLMs). By introducing more computational budget during test-time, LLMs have the potential to explore more accurate and higher-quality solutions. However, such paradigms are primarily verified in domains that have well-defined criteria for responses, such as coding and mathematics. Inspired by the success of this paradigm, we aim to bridge it to more subtle open-domain question answering. Specifically, we utilize search mechanisms such as Monte Carlo Tree Search (MCTS) for both policy model improvement and reward model improvement that achieve better performance in test-time scaling strategies. Our contributions are summarized in two folds: For the training phase, we demonstrate that our approach surpasses previous SOTA automatic data annotation methods and various public instruction-tuning datasets, with fewer data points. This offers a more data-efficient solution for training robust models. For the inference phase, we utilize the intermediate values collected during training data construction to train a process reward model called PRM+. This model employs a novel two-stage training method to provide finer-grained guidance across the generation trajectory. This introduces no additional overhead during training data collection and further enhances performance by scaling test-time computation. Experimental results show that our method can effectively improve the performance of both the policy model and the reward model.</abstract>
      <url hash="b3056545">2025.findings-acl.388</url>
      <bibkey>chan-etal-2025-boosting</bibkey>
    </paper>
    <paper id="389">
      <title>Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models</title>
      <author><first>Rui</first><last>Hu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Delai</first><last>Qiu</last></author>
      <author><first>Shuyu</first><last>Wei</last></author>
      <author><first>Jiaming</first><last>Zhang</last><affiliation>Nanyang Technological University and A*STAR</affiliation></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Jitao</first><last>Sang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>7452-7463</pages>
      <abstract>Omnimodal Large Language Models (OLLMs) have shown significant progress in integrating vision and text, but still struggle with integrating vision and audio, often exhibiting suboptimal performance when processing audio queries compared to text queries. This disparity is primarily due to insufficient alignment between vision and audio modalities during training, leading to inadequate attention to visual information when using audio queries. To mitigate this issue, we propose a Self-Knowledge Distillation (Self-KD) training method where the vision-text component of the OLLM serves as the teacher and the vision-audio component as the student. This enables the model to process audio in a manner analogous to its text processing. Our experimental results demonstrate that Self-KD is an effective method for enhancing the vision-audio capabilities of OLLMs by learning from the vision-text components, which subsequently improves the interaction between audio and images and results in improved performance on multimodal tasks.</abstract>
      <url hash="6c31b19c">2025.findings-acl.389</url>
      <bibkey>hu-etal-2025-investigating</bibkey>
    </paper>
    <paper id="390">
      <title><fixed-case>O</fixed-case>pen<fixed-case>H</fixed-case>u<fixed-case>E</fixed-case>val: Evaluating Large Language Model on <fixed-case>H</fixed-case>ungarian Specifics</title>
      <author><first>Haote</first><last>Yang</last><affiliation>PJLab</affiliation></author>
      <author><first>Xingjian</first><last>Wei</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jiang</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Noémi</first><last>Ligeti-Nagy</last><affiliation>Hungarian Research Centre for Linguistics</affiliation></author>
      <author><first>Jiaxing</first><last>Sun</last></author>
      <author><first>Yinfan</first><last>Wang</last></author>
      <author><first>Győző Zijian</first><last>Yang</last><affiliation>Hungarian Research Centre for Linguistics</affiliation></author>
      <author><first>Junyuan</first><last>Gao</last></author>
      <author><first>Jingchao</first><last>Wang</last></author>
      <author><first>Bowen</first><last>Jiang</last></author>
      <author><first>Shasha</first><last>Wang</last></author>
      <author><first>Nanjun</first><last>Yu</last></author>
      <author><first>Zihao</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shixin</first><last>Hong</last></author>
      <author><first>Hongwei</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Gábor</first><last>Prószéky</last><affiliation>Hungarian Research Centre for Linguistics, Pazmany Peter Catholic University and MorphoLogic</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>7464-7520</pages>
      <abstract>We introduce OpenHuEval, the first benchmark for LLMs focusing on the Hungarian language and specifics. OpenHuEval is constructed from a vast collection of Hungarian-specific materials sourced from multiple origins. In the construction, we incorporated the latest design principles for evaluating LLMs, such as using real user queries from the internet, emphasizing the assessment of LLMs’ generative capabilities, and employing LLM-as-judge to enhance the multidimensionality and accuracy of evaluations. Ultimately, OpenHuEval encompasses eight Hungarian-specific dimensions, featuring five tasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive, in-depth, and scientifically accurate assessment of LLM performance in the context of the Hungarian language and its specifics. We evaluated current mainstream LLMs, including both traditional LLMs and recently developed Large Reasoning Models. The results demonstrate the significant necessity for evaluation and model optimization tailored to the Hungarian language and specifics. We also established the framework for analyzing the thinking processes of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms of these models in non-English languages, with Hungarian serving as a representative example. We will release OpenHuEval at https://github.com/opendatalab/OpenHuEval .</abstract>
      <url hash="59adfd02">2025.findings-acl.390</url>
      <bibkey>yang-etal-2025-openhueval</bibkey>
    </paper>
    <paper id="391">
      <title><fixed-case>S</fixed-case>truct<fixed-case>F</fixed-case>act: Reasoning Factual Knowledge from Structured Data with Large Language Models</title>
      <author><first>Sirui</first><last>Huang</last><affiliation>University of Technology Sydney and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yanggan</first><last>Gu</last></author>
      <author><first>Zhonghao</first><last>Li</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Guandong</first><last>Xu</last><affiliation>The Education University of Hong Kong</affiliation></author>
      <pages>7521-7552</pages>
      <abstract>Large language models (LLMs) have made significant strides in natural language processing by leveraging their ability to comprehend and reason with factual knowledge. However, a significant amount of factual knowledge is stored in structured data, which has unique characteristics not typically encountered in the unstructured texts used for pretraining LLMs. To evaluate the capability of LLMs in handling facts structurally stored, we introduce a benchmark called StructFact, which includes meticulously annotated factual questions, spanning five tasks that reflect the intrinsic properties of structured data. This benchmark aims to delineate the strengths and limitations of LLMs in reasoning with structured data for knowledge-intensive tasks in practical applications. Extensive experiments conducted on 10 common LLMs have yielded several insights, one notable finding being that these models struggle significantly with the heterogeneity of structured data during reasoning.</abstract>
      <url hash="0b6608df">2025.findings-acl.391</url>
      <bibkey>huang-etal-2025-structfact</bibkey>
    </paper>
    <paper id="392">
      <title>From Imitation to Introspection: Probing Self-Consciousness in Language Models</title>
      <author><first>Sirui</first><last>Chen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Shu</first><last>Yu</last></author>
      <author><first>Shengjie</first><last>Zhao</last><affiliation>Tongji University</affiliation></author>
      <author><first>Chaochao</first><last>Lu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>7553-7583</pages>
      <abstract>Self-consciousness, the introspection of one’s existence and thoughts, represents a high-level cognitive process. As language models advance at an unprecedented pace, a critical question arises: Are these models becoming self-conscious? Drawing upon insights from psychological and neural science, this work presents a practical definition of self-consciousness for language models and refines ten core concepts. Our work pioneers an investigation into self-consciousness in language models by, for the first time, leveraging structural causal games to establish the functional definitions of the ten core concepts. Based on our definitions, we conduct a comprehensive four-stage experiment: quantification (evaluation of ten leading models), representation (visualization of self-consciousness within the models), manipulation (modification of the models’ representation), and acquisition (fine-tuning the models on core concepts). Our findings indicate that although models are in the early stages of developing self-consciousness, there is a discernible representation of certain concepts within their internal mechanisms. However, these representations of self-consciousness are hard to manipulate positively at the current stage, yet they can be acquired through targeted fine-tuning.</abstract>
      <url hash="28f41ae3">2025.findings-acl.392</url>
      <bibkey>chen-etal-2025-imitation</bibkey>
    </paper>
    <paper id="393">
      <title><fixed-case>D</fixed-case>oc<fixed-case>F</fixed-case>usion: A Unified Framework for Document Parsing Tasks</title>
      <author><first>Mingxu</first><last>Chai</last></author>
      <author><first>Ziyu</first><last>Shen</last></author>
      <author><first>Chong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Jihua</first><last>Kang</last></author>
      <author><first>Jiazheng</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <pages>7584-7599</pages>
      <abstract>Document parsing involves layout element detection and recognition, essential for extracting information. However, existing methods often employ multiple models for these tasks, leading to increased system complexity and maintenance overhead. While some models attempt to unify detection and recognition, they often fail to address the intrinsic differences in data representations, thereby limiting performance in document processing. Our research reveals that recognition relies on discrete tokens, whereas detection relies on continuous coordinates, leading to challenges in gradient updates and optimization. To bridge this gap, we propose the Gaussian-Kernel Cross-Entropy Loss (GK-CEL), enabling generative frameworks to handle both tasks simultaneously. Building upon GK-CEL, we propose DocFusion, a unified document parsing model with only 0.28B parameters. Additionally, we construct the DocLatex-1.6M dataset to provide high-quality training support. Experimental results show that DocFusion, equipped with GK-CEL, performs competitively across four core document parsing tasks, validating the effectiveness of our unified approach.</abstract>
      <url hash="d0bb0d73">2025.findings-acl.393</url>
      <bibkey>chai-etal-2025-docfusion</bibkey>
    </paper>
    <paper id="394">
      <title>Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models</title>
      <author><first>Yue</first><last>Li</last></author>
      <author><first>Xin</first><last>Yi</last></author>
      <author><first>Dongsheng</first><last>Shi</last></author>
      <author><first>Gerard</first><last>De Melo</last><affiliation>Hasso Plattner Institute and University of Potsdam</affiliation></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <pages>7600-7612</pages>
      <abstract>With the increasing size of Large Vision-Language Models (LVLMs), network pruning techniques aimed at compressing models for deployment in resource-constrained environments have garnered significant attention. However, we observe that pruning often leads to a degradation in safety performance. To address this issue, we present a novel and lightweight approach, termed Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the contribution of each attention head to safety, identifying the most critical ones, and then selectively restoring neurons directly within these attention heads that play a pivotal role in maintaining safety. This process hierarchically realigns the safety of pruned LVLMs, progressing from the attention head level to the neuron level. We validate HSR across various models and pruning strategies, consistently achieving notable improvements in safety performance. To our knowledge, this is the first work explicitly focused on restoring safety in LVLMs post-pruning.</abstract>
      <url hash="1b59aa28">2025.findings-acl.394</url>
      <bibkey>li-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="395">
      <title><fixed-case>L</fixed-case>ong<fixed-case>DPO</fixed-case>: Unlock Better Long-form Generation Abilities for <fixed-case>LLM</fixed-case>s via Critique-augmented Stepwise Information</title>
      <author><first>Bowen</first><last>Ping</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Shanghang</first><last>Zhang</last></author>
      <pages>7613-7632</pages>
      <abstract>Recent advancements in large language models (LLMs) have markedly improved their capacity to handle long text inputs; however, current models, including GPT-4o, still exhibit unsatisfactory performance in long-form generation. Generating high-quality long-form content still remains a significant challenge. In this paper, we present LongDPO, a novel approach designed to enhance long-form text generation through step-level supervision. By leveraging Monte Carlo Tree Search (MCTS) to collect stepwise preference pairs and employing a global memory pool to maintain factual accuracy, LongDPO effectively mitigates issues such as inconsistencies that are prevalent in long-context LLMs. Furthermore, we integrate critique-augmented generation to refine the selected preference pairs. Following the collection of stepwise preference pairs, we apply stepwise preference learning for fine-grained optimization. Experimental results demonstrate that our method enhances performance on long-form generation benchmarks (e.g. LongBench-Write) while maintaining nearly lossless performance on several general benchmarks.</abstract>
      <url hash="48c775f9">2025.findings-acl.395</url>
      <bibkey>ping-etal-2025-longdpo</bibkey>
    </paper>
    <paper id="396">
      <title>Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts</title>
      <author><first>Quanyu</first><last>Long</last></author>
      <author><first>Jianda</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>I2R</affiliation></author>
      <author><first>Nancy F.</first><last>Chen</last></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Sinno Jialin</first><last>Pan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>7633-7651</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet they often rely on external context to handle complex tasks. While retrieval-augmented frameworks traditionally focus on selecting top-ranked documents in a single pass, many real-world scenarios demand compositional retrieval, where multiple sources must be combined in a coordinated manner. In this work, we propose a tri-encoder sequential retriever that models this process as a Markov Decision Process (MDP), decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples. We train the retriever in two stages: first, we efficiently construct supervised sequential data for initial policy training; we then refine the policy to align with the LLM’s preferences using a reward grounded in the structural correspondence of generated programs. Experimental results show that our method consistently and significantly outperforms baselines, underscoring the importance of explicitly modeling inter-example dependencies. These findings highlight the potential of compositional retrieval for tasks requiring multiple pieces of evidence or examples.</abstract>
      <url hash="4b0f1222">2025.findings-acl.396</url>
      <bibkey>long-etal-2025-reinforcing</bibkey>
    </paper>
    <paper id="397">
      <title>Towards A Better Initial Policy Model For Scalable Long-<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Reinforcement Learning</title>
      <author><first>Bofei</first><last>Gao</last></author>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Yibo</first><last>Miao</last></author>
      <author><first>Ruoyu</first><last>Wu</last></author>
      <author><first>Feifan</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <author><first>Longhui</first><last>Yu</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>7652-7665</pages>
      <abstract>Long-CoT reasoning combined with reinforcement learning for large language models demonstrates remarkable performance and scalability. However, we observe that the initial policy model could significantly influence the final performance as well as the token efficiency. Additionally, there is a lack of systematic guidelines for obtaining a better initial policy model. To bridge this gap, we initiate a comprehensive investigation by activating the initial model using a variety of datasets with different data volumes and reasoning patterns. Then, we conduct a thorough analysis and comparison of the RL process for different initial models from the perspectives of upper bounds, diversity, and token efficiency, providing a deeper understanding and insight into the long-CoT RL. Based on our empirical results, we propose a systematic guideline and a novel Re-RFT method for constructing a better RL start point. Our experiment results based on the 14B model surpass the DeepSeek-R1-Distill-Qwen-14B by an average of 4.6%, demonstrating our approach’s effectiveness and superiority.</abstract>
      <url hash="e9956ddf">2025.findings-acl.397</url>
      <bibkey>gao-etal-2025-towards</bibkey>
    </paper>
    <paper id="398">
      <title>Topic Modeling for Short Texts via Optimal Transport-Based Clustering</title>
      <author><first>Tu</first><last>Vu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Manh</first><last>Do</last></author>
      <author><first>Tung</first><last>Nguyen</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>7666-7680</pages>
      <abstract>Discovering topics and learning document representations in topic space are two crucial aspects of topic modeling, particularly in the short-text setting, where inferring topic proportions for individual documents is highly challenging. Despite significant progress in neural topic modeling, effectively distinguishing document representations as well as topic embeddings remains an open problem. In this paper, we propose a novel method called **En**hancing Global **C**lustering with **O**ptimal **T**ransport in Topic Modeling (EnCOT). Our approach utilizes an abstract global clusters concept to capture global information and then employs the Optimal Transport framework to align document representations in the topic space with global clusters, while also aligning global clusters with topics. This dual alignment not only enhances the separation of documents in the topic space but also facilitates learning of latent topics. Through extensive experiments, we demonstrate that our method outperforms state-of-the-art techniques in short-text topic modeling across commonly used metrics.</abstract>
      <url hash="9de87bab">2025.findings-acl.398</url>
      <bibkey>vu-etal-2025-topic</bibkey>
    </paper>
    <paper id="399">
      <title>Lemmatisation &amp; Morphological Analysis of Unedited <fixed-case>G</fixed-case>reek: Do Simple Tasks Need Complex Solutions?</title>
      <author><first>Colin</first><last>Swaelens</last></author>
      <author><first>Ilse</first><last>De Vos</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>Ghent University</affiliation></author>
      <pages>7681-7689</pages>
      <abstract>Fine-tuning transformer-based models for part-of-speech tagging of unedited Greek text has outperformed traditional systems. However, when applied to lemmatisation or morphological analysis, fine-tuning has not yet achieved competitive results. This paper explores various approaches to combine morphological features to both reduce label complexity and enhance multi-task training. Specifically, we group three nominal features into a single label, and combine the three most distinctive features of verbs into another unified label. These combined labels are used to fine-tune DBBERT, a BERT model pre-trained on both ancient and modern Greek. Additionally, we experiment with joint training – both among these labels and in combination with POS tagging – within a multi-task framework to improve performance by transferring parameters. To evaluate our models, we use a manually annotated gold standard from the Database of Byzantine Book Epigrams. Our results show a nearly 9 pp. improvement, demonstrating that multi-task learning is a promising approach for linguistic annotation in less standardised corpora.</abstract>
      <url hash="aa5e8214">2025.findings-acl.399</url>
      <bibkey>swaelens-etal-2025-lemmatisation</bibkey>
    </paper>
    <paper id="400">
      <title><fixed-case>FRAME</fixed-case>: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
      <author><first>Chengzhang</first><last>Yu</last></author>
      <author><first>Yiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhixin</first><last>Liu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Zenghui</first><last>Ding</last><affiliation>Hefei Institute of Physical Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yining</first><last>Sun</last><affiliation>Institute of Intelligent Machines,Chinese Academy Of Sciences</affiliation></author>
      <author><first>Zhanpeng</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <pages>7690-7704</pages>
      <abstract>The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME’s effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.</abstract>
      <url hash="8d880559">2025.findings-acl.400</url>
      <bibkey>yu-etal-2025-frame</bibkey>
    </paper>
    <paper id="401">
      <title>Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models</title>
      <author><first>Xi</first><last>Li</last><affiliation>University of Alabama at Birmingham</affiliation></author>
      <author><first>Ruofan</first><last>Mao</last></author>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Renze</first><last>Lou</last></author>
      <author><first>Chen</first><last>Wu</last><affiliation>Facebook</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <pages>7705-7727</pages>
      <abstract>Large Language Models (LLMs), especially those accessed via APIs, have demonstrated impressive capabilities across various domains. However, users without technical expertise often turn to (untrustworthy) third-party services, such as prompt engineering, to enhance their LLM experience, creating vulnerabilities to adversarial threats like backdoor attacks. Backdoor-compromised LLMs generate malicious outputs to users when inputs contain specific “triggers” set by attackers. Traditional defense strategies, originally designed for small-scale models, are impractical for API-accessible LLMs due to limited model access, high computational costs, and data requirements. To address these limitations, we propose Chain-of-Scrutiny (CoS) which leverages LLMs’ unique reasoning abilities to mitigate backdoor attacks. It guides the LLM to generate reasoning steps for a given input and scrutinizes for consistency with the final output – any inconsistencies indicating a potential attack. It is well-suited for the popular API-only LLM deployments, enabling detection at minimal cost and with little data. User-friendly and driven by natural language, it allows non-experts to perform the defense independently while maintaining transparency. We validate the effectiveness of CoS through extensive experiments on various tasks and LLMs, with results showing greater benefits for more powerful LLMs.</abstract>
      <url hash="e868a23c">2025.findings-acl.401</url>
      <bibkey>li-etal-2025-chain</bibkey>
    </paper>
    <paper id="402">
      <title>Relevance Scores Calibration for Ranked List Truncation via <fixed-case>TMP</fixed-case> Adapter</title>
      <author><first>Pavel</first><last>Posokhov</last><affiliation>ITMO University</affiliation></author>
      <author><first>Sergei</first><last>Masliukhin</last><affiliation>STC-Innovation</affiliation></author>
      <author><first>Skrylnikov</first><last>Stepan</last><affiliation>STC</affiliation></author>
      <author><first>Danil</first><last>Tirskikh</last></author>
      <author><first>Olesia</first><last>Makhnytkina</last></author>
      <pages>7728-7734</pages>
      <abstract>The ranked list truncation task involves determining a truncation point to retrieve the relevant items from a ranked list. Despite current advancements, truncation methods struggle with limited capacity, unstable training and inconsistency of selected threshold. To address these problems we introduce TMP Adapter, a novel approach that builds upon the improved adapter model and incorporates the Threshold Margin Penalty (TMP) as an additive loss function to calibrate ranking model relevance scores for ranked list truncation. We evaluate TMP Adapter’s performance on various retrieval datasets and observe that TMP Adapter is a promising advancement in the calibration methods, which offers both theoretical and practical benefits for ranked list truncation.</abstract>
      <url hash="94bb39c9">2025.findings-acl.402</url>
      <bibkey>posokhov-etal-2025-relevance</bibkey>
    </paper>
    <paper id="403">
      <title>Neuron Activation Modulation for Text Style Transfer: Guiding Large Language Models</title>
      <author><first>Chaona</first><last>Kong</last></author>
      <author><first>Jianyi</first><last>Liu</last></author>
      <author><first>Yifan</first><last>Tang</last></author>
      <author><first>Ru</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>7735-7747</pages>
      <abstract>Text style transfer (TST) aims to flexibly adjust the style of text while preserving its core content. Although large language models (LLMs) excel in TST tasks, they often face unidirectional issues due to imbalanced training data and their tendency to generate safer responses. These challenges present a significant obstacle in achieving effective style transfer. To address this issue, we propose a novel method for text style transfer based on neuron activation modulation (NAM-TST). This approach identifies neurons related to style through gradient-based activation difference analysis and calculates the activation differences between the source and target styles. During text generation, we use the activation difference to align the activation values of style-related neurons with those of the target style to guide the model in performing the transfer. This strategy enables the model to generate text that satisfies specific style requirements, effectively mitigating the unidirectional issue inherent in LLMs during style transfer. Experiments on benchmark datasets demonstrate that NAM-TST significantly enhances style transfer quality while preserving content consistency.</abstract>
      <url hash="3069f85b">2025.findings-acl.403</url>
      <bibkey>kong-etal-2025-neuron</bibkey>
    </paper>
    <paper id="404">
      <title><fixed-case>MTVQA</fixed-case>: Benchmarking Multilingual Text-Centric Visual Question Answering</title>
      <author><first>Jingqun</first><last>Tang</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>Bytedance Inc.</affiliation></author>
      <author><first>Yongjie</first><last>Ye</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shu</first><last>Wei</last></author>
      <author><first>An-Lan</first><last>Wang</last></author>
      <author><first>Chunhui</first><last>Lin</last><affiliation>Bytedance</affiliation></author>
      <author><first>Hao</first><last>Feng</last></author>
      <author><first>Zhen</first><last>Zhao</last></author>
      <author><first>Yanjie</first><last>Wang</last><affiliation>ByteDance Inc</affiliation></author>
      <author><first>Yuliang</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hao</first><last>Liu</last><affiliation>Bytedance</affiliation></author>
      <author><first>Xiang</first><last>Bai</last></author>
      <author><first>Can</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <pages>7748-7763</pages>
      <abstract>Text-Centric Visual Question Answering (TEC-VQA) in its proper format not only facilitates human-machine interaction in text-centric visual environments but also serves as a de facto gold proxy to evaluate AI models in the domain of text-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks focus on high-resource languages like English and Chinese. Despite pioneering works expanding multilingual QA pairs in non-text-centric VQA datasets through translation engines, the translation-based protocol encounters a substantial “visual-textual misalignment” problem when applied to TEC-VQA. Specifically, it prioritizes the text in question-answer pairs while disregarding the visual text present in images. Moreover, it fails to address complexities related to nuanced meaning, contextual distortion, language bias, and question-type diversity. In this work, we tackle multilingual TEC-VQA by introducing MTVQA, the first benchmark featuring high-quality human expert annotations across 9 diverse languages, consisting of 6,778 question-answer pairs across 2,116 images. Further, by comprehensively evaluating numerous state-of-the-art Multimodal Large Language Models (MLLMs), including Qwen2.5-VL, InternVL-2.5, GPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that there is still a large room for performance improvement (InternVL-2.5 scoring 32.2 versus 79.7 for human performance), underscoring the value of MTVQA. By providing a dataset with nuanced multilingual annotations, MTVQA aims to set a new standard for benchmarks, fostering advancements in multilingual visual text comprehension.</abstract>
      <url hash="5b4eb38e">2025.findings-acl.404</url>
      <bibkey>tang-etal-2025-mtvqa</bibkey>
    </paper>
    <paper id="405">
      <title><fixed-case>HICD</fixed-case>: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models</title>
      <author><first>Xinyan</first><last>Jiang</last></author>
      <author><first>Hang</first><last>Ye</last></author>
      <author><first>Yongxin</first><last>Zhu</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaoying</first><last>Zheng</last><affiliation>Shanghai Advanced Research Institute, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zikang</first><last>Chen</last></author>
      <author><first>Jun</first><last>Gong</last></author>
      <pages>7764-7786</pages>
      <abstract>Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model’s prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more “contrast-effective” hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.</abstract>
      <url hash="443cdc76">2025.findings-acl.405</url>
      <bibkey>jiang-etal-2025-hicd</bibkey>
    </paper>
    <paper id="406">
      <title>Understanding the Repeat Curse in Large Language Models from a Feature Perspective</title>
      <author><first>Junchi</first><last>Yao</last></author>
      <author id="shu-yang"><first>Shu</first><last>Yang</last></author>
      <author><first>Jianhua</first><last>Xu</last></author>
      <author><first>Lijie</first><last>Hu</last></author>
      <author><first>Mengdi</first><last>Li</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>7787-7815</pages>
      <abstract>Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the ”Repeat Curse”. While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach—”Duplicatus Charm”—to induce and analyze the Repeat Curse. Our method systematically identifies “Repetition Features” -the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse.</abstract>
      <url hash="a7949b72">2025.findings-acl.406</url>
      <bibkey>yao-etal-2025-understanding</bibkey>
    </paper>
    <paper id="407">
      <title>Code-Switching Curriculum Learning for Multilingual Transfer in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Cheonbok</first><last>Park</last><affiliation>NAVER</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>Sogang University</affiliation></author>
      <pages>7816-7836</pages>
      <abstract>Large language models (LLMs) now exhibit near human-level performance in various tasks, but their performance drops drastically after a handful of high-resource languages due to the imbalance in pre-training data. Inspired by the human process of second language acquisition, particularly code-switching—the practice of language alternation in a conversation—we propose code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of human language learning by progressively training models with a curriculum consisting of 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as our underlying model, we demonstrate the efficacy of the CSCL in improving language transfer to Korean, achieving significant performance gains compared to monolingual continual pre-training methods. Ablation studies reveal that both token- and sentence-level code-switching significantly enhance cross-lingual transfer and that curriculum learning amplifies these effects. We also extend our findings into various languages, including Japanese (high-resource) and Indonesian (low-resource), and using two additional models (Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious correlations between language resources and safety alignment, presenting a robust, efficient framework for more equitable language transfer in LLMs. We observe that CSCL is effective for low-resource settings where high-quality, monolingual corpora for language transfer are hardly available.</abstract>
      <url hash="4e40a5d9">2025.findings-acl.407</url>
      <bibkey>yoo-etal-2025-code-switching</bibkey>
    </paper>
    <paper id="408">
      <title>A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos</title>
      <author><first>Yang</first><last>Yao</last><affiliation>Shanghai Artificial Intelligence Laboratory and University of Hong Kong</affiliation></author>
      <author><first>Xuan</first><last>Tong</last></author>
      <author><first>Ruofan</first><last>Wang</last></author>
      <author><first>Yixu</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Lujundong</first><last>Li</last></author>
      <author><first>Liang</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yan</first><last>Teng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yingchun</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>7837-7855</pages>
      <abstract>Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.</abstract>
      <url hash="1722763b">2025.findings-acl.408</url>
      <bibkey>yao-etal-2025-mousetrap</bibkey>
    </paper>
    <paper id="409">
      <title>Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection</title>
      <author><first>Yixuan</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shiqi</first><last>Zhou</last></author>
      <author><first>Chuanzhe</first><last>Guo</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>7856-7869</pages>
      <abstract>Evol-Instruct has made significant improvements as a data synthesis method in several areas. Existing methods typically rely on a fixed set of strategies to evolve, which require manual design and are monolithic in form. In addition, iterative evolution also makes the acquisition of hard samples expensive. In view of this, we propose the Tag-Evol framework, a more diverse and efficient instruction evolving method. Specifically, Tag-Evol uses diverse and specific knowledge tags as strategies to achieve controlled evolution by injecting different combinations of tags into the original instructions. Experiments with multiple backbones in mathematical and code domain benchmarks show that the proposed method generates significantly better evolved data than other methods. Furthermore, we conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is not only efficient but also generates more diverse and challenging data.</abstract>
      <url hash="13e5c526">2025.findings-acl.409</url>
      <bibkey>wang-etal-2025-tag</bibkey>
    </paper>
    <paper id="410">
      <title>Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space</title>
      <author><first>Yao</first><last>Huang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yitong</first><last>Sun</last><affiliation>Beihang University</affiliation></author>
      <author><first>Shouwei</first><last>Ruan</last></author>
      <author><first>Yichi</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yinpeng</first><last>Dong</last></author>
      <author><first>Xingxing</first><last>Wei</last><affiliation>Beihang University</affiliation></author>
      <pages>7870-7888</pages>
      <abstract>Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: https://github.com/Aries-iai/CL-GSO.</abstract>
      <url hash="52711a40">2025.findings-acl.410</url>
      <bibkey>huang-etal-2025-breaking</bibkey>
    </paper>
    <paper id="411">
      <title><fixed-case>G</fixed-case>e<fixed-case>NR</fixed-case>e: A <fixed-case>F</fixed-case>rench Gender-Neutral Rewriting System Using Collective Nouns</title>
      <author><first>Enzo</first><last>Doyen</last></author>
      <author><first>Amalia</first><last>Todirascu</last><affiliation>Université de Strasbourg</affiliation></author>
      <pages>7889-7909</pages>
      <abstract>A significant portion of the textual data used in the field of Natural Language Processing (NLP) exhibits gender biases, particularly due to the use of masculine generics (masculine words that are supposed to refer to mixed groups of men and women), which can perpetuate and amplify stereotypes. Gender rewriting, an NLP task that involves automatically detecting and replacing gendered forms with neutral or opposite forms (e.g., from masculine to feminine), can be employed to mitigate these biases. While such systems have been developed in a number of languages (English, Arabic, Portuguese, German, French), automatic use of gender neutralization techniques (as opposed to inclusive or gender-switching techniques) has only been studied for English. This paper presents GeNRe, the very first French gender-neutral rewriting system using collective nouns, which are gender-fixed in French. We introduce a rule-based system (RBS) tailored for the French language alongside two fine-tuned language models trained on data generated by our RBS. We also explore the use of instruct-based models to enhance the performance of our other systems and find that Claude 3 Opus combined with our dictionary achieves results close to our RBS. Through this contribution, we hope to promote the advancement of gender bias mitigation techniques in NLP for French.</abstract>
      <url hash="d90e2023">2025.findings-acl.411</url>
      <bibkey>doyen-todirascu-2025-genre</bibkey>
    </paper>
    <paper id="412">
      <title><fixed-case>LGAR</fixed-case>: Zero-Shot <fixed-case>LLM</fixed-case>-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews</title>
      <author><first>Christian</first><last>Jaumann</last><affiliation>XITASO GmbH and Universität Augsburg</affiliation></author>
      <author><first>Andreas</first><last>Wiedholz</last><affiliation>XITASO GmbH</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>7910-7927</pages>
      <abstract>The scientific literature is growing rapidly, making it hard to keep track of the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and evaluate all relevant papers on a topic. After retrieving a set of candidate papers, the abstract screening phase determines initial relevance. To date, abstract screening methods using large language models (LLMs) focus on binary classification settings; existing question answering (QA) based ranking approaches suffer from error propagation. LLMs offer a unique opportunity to evaluate the SLR’s inclusion and exclusion criteria, yet, existing benchmarks do not provide them exhaustively. We manually extract these criteria as well as research questions for 57 SLRs, mostly in the medical domain, enabling principled comparisons between approaches. Moreover, we propose LGAR, a zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance scorer and a dense re-ranker. Our extensive experiments show that LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision. Our code and data is publicly available.</abstract>
      <url hash="664813f3">2025.findings-acl.412</url>
      <bibkey>jaumann-etal-2025-lgar</bibkey>
    </paper>
    <paper id="413">
      <title><fixed-case>LCHAIM</fixed-case> - Investigating Long Context Reasoning in <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Ehud</first><last>Malul</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Oriel</first><last>Perets</last></author>
      <author><first>Ziv</first><last>Mor</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Yigal</first><last>Kassel</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Elior</first><last>Sulem</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>7928-7939</pages>
      <abstract>Natural Language Inference (NLI) has gained significant attention recently due to its importance in understanding how machines comprehend and reason about language. While English has received tremendous interest, Morphologically Rich Languages (MRLs) like Hebrew, require more research. In this paper, we address the evaluation of Hebrew NLI models by introducing LCHAIM, a dataset designed to evaluate these models on tasks involving long premises and complex reasoning. The dataset, created by translating and validating the English ConTRoL dataset, consists of 8,325 context-hypothesis pairs that require coreferential, temporal, logical and analytical reasoning. Our experiments show the difficulty of contextual reasoning in Hebrew, as evidenced by the performance of different models. Fine-tuning the LongHero model on both the shorter premise Hebrew NLI and the LCHAIM datasets yielded a mean accuracy of 52%, that is 35% less than human performance. Similarly, Large language Models (LLMs) like Gemma-9B, Dicta-LM-2.0-7B, and GPT-4o achieved a top mean accuracy of 60.12% in few-shot setting.</abstract>
      <url hash="2ea16c21">2025.findings-acl.413</url>
      <bibkey>malul-etal-2025-lchaim</bibkey>
    </paper>
    <paper id="414">
      <title><fixed-case>CL</fixed-case>e<fixed-case>V</fixed-case>e<fixed-case>R</fixed-case>: Multi-modal Contrastive Learning for Vulnerability Code Representation</title>
      <author><first>Jiayuan</first><last>Li</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Sen</first><last>Zhao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yun</first><last>Yang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lun</first><last>Li</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongsong</first><last>Zhu</last><affiliation>institute of information engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>7940-7951</pages>
      <abstract>Automated vulnerability detection has become increasingly important. Many existing methods utilize deep learning models to obtain code representations for vulnerability detection. However, these approaches predominantly capture the overall semantics of the code rather than its intrinsic vulnerability-specific semantics. To address this issue, we propose CLeVeR, the first approach that leverages contrastive learning to generate precise vulnerability code representations under the supervision of vulnerability descriptions. Specifically, we introduce an Adapter, a Representation Refinement module, and a Description Simulator to mitigate the challenges of semantic misalignment and imbalance between code and descriptions, and input data inconsistency between pre-training and fine-tuning stages, respectively. For vulnerability detection and classification tasks, CLeVeR achieves F1 scores of 72.82% (real-world dataset) and 80.34%, outperforming state-of-the-art methods (SOTAs) by 11.85% and 13.61%. Additionally, CLeVeR also outperforms SOTAs in zero-shot inference, demonstrating the transferability of its generated vulnerability code representations.</abstract>
      <url hash="65af6bfb">2025.findings-acl.414</url>
      <bibkey>li-etal-2025-clever</bibkey>
    </paper>
    <paper id="415">
      <title><fixed-case>MEMIT</fixed-case>-Merge: Addressing <fixed-case>MEMIT</fixed-case>’s Key-Value Conflicts in Same-Subject Batch Editing for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zilu</first><last>Dong</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Xiangqing</first><last>Shen</last></author>
      <author><first>Rui</first><last>Xia</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <pages>7952-7960</pages>
      <abstract>As large language models (LLMs) continue to scale up, knowledge editing techniques that modify models’ internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncovers a critical limitation that MEMIT’s editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals the root cause lies in MEMIT’s key-value modeling framework: when multiple facts with the same subject in a batch are modeled through MEMIT’s key-value mechanism, identical keys (derived from the shared subject) are forced to represent different values (corresponding to distinct knowledge), resulting in update conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in same-subject batch editing scenarios. Experimental results demonstrate that at a batch size of 5, while the original MEMIT’s success rate drops to 46%, MEMIT-Merge maintains a 98% editing success rate, showcasing remarkable robustness to subject entity collisions.</abstract>
      <url hash="d631cf69">2025.findings-acl.415</url>
      <bibkey>dong-etal-2025-memit</bibkey>
    </paper>
    <paper id="416">
      <title>Large Language Models for Predictive Analysis: How Far Are They?</title>
      <author><first>Qin</first><last>Chen</last></author>
      <author><first>Yuanyi</first><last>Ren</last></author>
      <author><first>Xiaojun</first><last>Ma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuyang</first><last>Shi</last></author>
      <pages>7961-7978</pages>
      <abstract>Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there are no relevant evaluations in existing studies. To bridge this gap, we introduce the PredictiQ benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis.</abstract>
      <url hash="e1cee51a">2025.findings-acl.416</url>
      <bibkey>chen-etal-2025-large-language</bibkey>
    </paper>
    <paper id="417">
      <title>Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking</title>
      <author><first>Xiaoxue</first><last>Cheng</last></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>7979-7990</pages>
      <abstract>Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, often leading to untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g., MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway. To balance efficiency and quality, we introduce a hierarchical system switch mechanism, which dynamically switches between fast and slow thinking modes at both instance and step levels. We conduct extensive experiments on both English and Chinese datasets, and the results show that our approach significantly outperforms baseline approaches.</abstract>
      <url hash="e98d4873">2025.findings-acl.417</url>
      <bibkey>cheng-etal-2025-think</bibkey>
    </paper>
    <paper id="418">
      <title>Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation</title>
      <author><first>Qitao</first><last>Qin</last></author>
      <author><first>Yucong</first><last>Luo</last></author>
      <author><first>Yihang</first><last>Lu</last></author>
      <author><first>Zhibo</first><last>Chu</last></author>
      <author><first>Xiaoman</first><last>Liu</last></author>
      <author><first>Xianwei</first><last>Meng</last></author>
      <pages>7991-8004</pages>
      <abstract>Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration.To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model’s memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available .</abstract>
      <url hash="1ddd6356">2025.findings-acl.418</url>
      <bibkey>qin-etal-2025-towards</bibkey>
    </paper>
    <paper id="419">
      <title>Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping</title>
      <author><first>Yijie</first><last>Chen</last></author>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>8005-8018</pages>
      <abstract>Knowledge Distillation (KD) has emerged as a prominent technique for model compression. However, conventional KD approaches primarily focus on homogeneous architectures with identical tokenizers, constraining their applicability in cross-architecture scenarios. As for the cross-tokenizer KD, the differences in the tokenizers give rise to two fundamental challenges: (1) sequence misalignment caused by divergent tokenization strategies, and (2) mismatched vocabulary size and composition. While existing probability-matching methods attempt to address these issues, their efficacy remains limited due to suboptimal alignment in both the sequence and vocabulary aspects. To overcome these limitations, we propose Contextual Dynamic Mapping (CDM), a novel cross-tokenizer distillation framework that employs contextual information to enhance sequence alignment precision and dynamically improves vocabulary mapping. We evaluated the effectiveness of our approach across five advanced and widely-used model families (<i>i.e,</i>LLama3, Phi3, Gemma2, OPT and Qwen2), which were configured into three distinct teacher-student pairs. Our method shows significant advantages over existing cross-tokenizer distillation baselines across diverse benchmarks, including instruction-following, code generation and math. Notably, our analysis reveals that combining conventional same-tokenizer distillation and cross-tokenizer distillation through CDM yields further performance improvements.</abstract>
      <url hash="b5dc1094">2025.findings-acl.419</url>
      <bibkey>chen-etal-2025-enhancing-cross</bibkey>
    </paper>
    <paper id="420">
      <title>A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models</title>
      <author><first>Jian</first><last>Gu</last></author>
      <author><first>Aldeida</first><last>Aleti</last><affiliation>Monash University</affiliation></author>
      <author><first>Chunyang</first><last>Chen</last><affiliation>Monash University</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>Chongqing University</affiliation></author>
      <pages>8019-8033</pages>
      <abstract>Finetuning language models (LMs) is crucial for adapting the models to downstream data and tasks. However, full finetuning is usually costly. Existing work, such as parameter-efficient finetuning (PEFT), often focuses on <i>how to finetune</i> but neglects the issue of <i>where to finetune</i>. As a pioneering work on reducing the cost of backpropagation (at the layer level) by answering where to finetune, we conduct a semantic analysis of the LM inference process. We first propose using transition traces of the latent representation to compute deviations (or loss). Then, using a derived formula of scaling law, we estimate the gain of each layer in reducing deviation (or loss). Further, we narrow down the scope for finetuning, and also, study the cost-benefit balance of LM finetuning. We perform extensive experiments across well-known LMs and datasets. The results show that our approach is effective and efficient, and outperforms the existing baselines. Our approach is orthogonal to other techniques for improving finetuning efficiency, such as PEFT methods, offering practical values on LM finetuning.</abstract>
      <url hash="d5b7ce84">2025.findings-acl.420</url>
      <bibkey>gu-etal-2025-semantic</bibkey>
    </paper>
    <paper id="421">
      <title><fixed-case>CNNS</fixed-case>um: Exploring Long-Context Summarization with Large Language Models in <fixed-case>C</fixed-case>hinese Novels</title>
      <author><first>Lingxiao</first><last>Wei</last></author>
      <author><first>He</first><last>Yan</last><affiliation>iQIYI</affiliation></author>
      <author><first>Lu</first><last>Xiangju</last></author>
      <author><first>Junmin</first><last>Zhu</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <pages>8034-8062</pages>
      <abstract>Large language models (LLMs) have been well-researched in various long-context tasks. However, the scarcity of long-context summarization datasets hinders progress in this area. To address this, we introduce CNNSum, a multi-scale long-context summarization benchmark based on Chinese novels, featuring human-driven annotations across four subsets totaling 695 samples, with lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct detailed human assessments to summarize abnormal output types. Furthermore, we extensively explore how to improve long-context summarization. In our study: (1) Advanced LLMs may generate much subjective commentary, leading to vague summaries. (2) Currently, long-context summarization mainly relies on memory ability. The advantages of Large LLMs are hard to utilize, thus small LLMs are more cost-effective. (3) Different prompt types paired with various version models may cause large performance gaps. In further fine-tuning, these can be mitigated, and the Base version models perform better. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential; using short-context data can significantly improve long-context summarization performance. However, further applying other interpolation methods requires careful selection. (5) CNNSum provides more reliable evaluation results than other benchmarks. We release CNNSum to advance future research.</abstract>
      <url hash="9068dad7">2025.findings-acl.421</url>
      <bibkey>wei-etal-2025-cnnsum</bibkey>
    </paper>
    <paper id="422">
      <title>Document Segmentation Matters for Retrieval-Augmented Generation</title>
      <author><first>Zhitong</first><last>Wang</last></author>
      <author><first>Cheng</first><last>Gao</last></author>
      <author><first>Chaojun</first><last>Xiao</last></author>
      <author><first>Yufei</first><last>Huang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Kangyang</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuzhuo</first><last>Bai</last></author>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Tangjian</first><last>Duan</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Chuancheng</first><last>Lv</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Guoshan</first><last>Lu</last></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>8063-8075</pages>
      <abstract>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge. A critical yet underexplored challenge in RAG is document segmentation, also known as document chunking. Existing widely-used rule-based chunking methods usually lead to suboptimal splits, where overly large chunks introduce irrelevant information and small chunks lack semantic coherence. Existing semantic-based approaches either require costly LLM calls or fail to adaptively group contextually related sentences. To address these limitations, we propose PIC, Pseudo-Instruction for document Chunking), a simple yet effective method that leverages document summaries as pseudo-instructions to guide chunking. By computing semantic similarity between sentences and the summary, PIC dynamically groups sentences into chunks that align with the document’s key themes, ensuring semantic completeness and relevance to potential user instructions. Experiments on multiple open-domain question-answering benchmarks demonstrate that PIC can significantly improve retrieval accuracy (Hits@k) and end-to-end QA performance (Exact Match) without any additional training.</abstract>
      <url hash="1be40342">2025.findings-acl.422</url>
      <bibkey>wang-etal-2025-document</bibkey>
    </paper>
    <paper id="423">
      <title><fixed-case>UB</fixed-case>ench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions</title>
      <author><first>Xunzhi</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zhuowei</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Gaonan</first><last>Chen</last></author>
      <author><first>Qiongyu</first><last>Li</last></author>
      <author><first>Bitong</first><last>Luo</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zhixin</first><last>Han</last><affiliation>Nankai University</affiliation></author>
      <author><first>Haotian</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Hang</first><last>Gao</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <pages>8076-8107</pages>
      <abstract>Despite recent progress in systematic evaluation frameworks, benchmarking the uncertainty of large language models (LLMs) remains a highly challenging task. Existing methods for benchmarking the uncertainty of LLMs face three key challenges: the need for internal model access, additional training, or high computational costs. This is particularly unfavorable for closed-source models. To this end, we introduce UBench, a new benchmark for evaluating the uncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence intervals. It encompasses 11,978 multiple-choice questions spanning knowledge, language, understanding, and reasoning capabilities. Based on this, we conduct extensive experiments. This includes comparisons with other advanced uncertainty estimation methods, the assessment of the uncertainty of 20 LLMs, and an exploration of the effects of Chain-of-Thought (CoT) prompts, role-playing (RP) prompts, and temperature on model uncertainty. Our analysis reveals several crucial insights: 1) Our confidence interval-based methods are highly effective for uncertainty quantification; 2) Regarding uncertainty, outstanding open-source models show competitive performance versus closed-source models; 3) CoT and RP prompts present potential ways to improve model reliability, while the influence of temperature changes follows no universal rule. Our implementation is available at https://github.com/Cyno2232/UBENCH.</abstract>
      <url hash="064fe410">2025.findings-acl.423</url>
      <bibkey>wang-etal-2025-ubench</bibkey>
    </paper>
    <paper id="424">
      <title>Embracing Large Language Models in Traffic Flow Forecasting</title>
      <author><first>Yusheng</first><last>Zhao</last></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Haomin</first><last>Wen</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Wei</first><last>Ju</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>8108-8123</pages>
      <abstract>Traffic flow forecasting aims to predict future traffic flows based on historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods being proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes in traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures, respectively. The two branches are first pre-trained individually, and during test time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of LEAF. Our code is available at https://github.com/YushengZhao/LEAF.</abstract>
      <url hash="f03a4fe5">2025.findings-acl.424</url>
      <bibkey>zhao-etal-2025-embracing</bibkey>
    </paper>
    <paper id="425">
      <title><fixed-case>F</fixed-case>low2<fixed-case>C</fixed-case>ode: Evaluating Large Language Models for Flowchart-based Code Generation Capability</title>
      <author><first>Mengliang</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Jiayi</first><last>Zeng</last></author>
      <author><first>Yankai</first><last>Jiang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Aimin</first><last>Zhou</last><affiliation>East China Normal University</affiliation></author>
      <pages>8124-8146</pages>
      <abstract>While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models’ performance. The dataset will be publicly available.</abstract>
      <url hash="b01ba329">2025.findings-acl.425</url>
      <bibkey>he-etal-2025-flow2code</bibkey>
    </paper>
    <paper id="426">
      <title>Smarter, Not Harder: Training-Free Adaptive Computation for Transformers</title>
      <author><first>Romain</first><last>Storaï</last><affiliation>Seoul National University and Ecole Nationale Supérieure des Mines d’Alès</affiliation></author>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>8147-8155</pages>
      <abstract>Adaptive Computation in Transformers (ACT) has been pursued in two directions: efficiency- and performance-focused. We study performance-focused ACT, or PACT, which invests more computation on hard steps to improve performance, such as by adding forward passes. We first discuss beam search and hesitation-based methods as PACT and their limitations. While the hesitation-based approach outperforms beam search by perturbing input embeddings, it suffers from inefficiency due to invalidating KVCache and exhibits instability due to its reliance on randomness. To address this, we propose IMPACT, a novel PACT method that perturbs network weights rather than input embeddings. This approach enables the reuse of KVCache, offers deterministic predictions, and significantly improves memory and computational efficiency. By achieving a better balance between performance and efficiency, IMPACT makes PACT accessible to communities with consumer-grade hardware.</abstract>
      <url hash="b04720e9">2025.findings-acl.426</url>
      <bibkey>storai-etal-2025-smarter</bibkey>
    </paper>
    <paper id="427">
      <title><fixed-case>UCS</fixed-case>-<fixed-case>SQL</fixed-case>: Uniting Content and Structure for Enhanced Semantic Bridging In Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Zhenhe</first><last>Wu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Zhongqiu</first><last>Li</last></author>
      <author><first>JieZhangChinaTele</first><last>JieZhangChinaTele</last><affiliation>The Principia</affiliation></author>
      <author><first>Zhongjiang</first><last>He</last><affiliation>ChinaTelecom</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Ruiyu</first><last>Fang</last></author>
      <author><first>Bing</first><last>Wang</last></author>
      <author><first>Hongyan</first><last>Xie</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Shuangyong</first><last>Song</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>8156-8168</pages>
      <abstract>With the rapid advancement of large language models (LLMs), recent researchers have increasingly focused on the superior capabilities of LLMs in text/code understanding and generation to tackle text-to-SQL tasks. Traditional approaches adopt schema linking to first eliminate redundant tables and columns and prompt LLMs for SQL generation. However, they often struggle with accurately identifying corresponding tables and columns, due to discrepancies in naming conventions between natural language questions (NL) and database schemas. Besides, existing methods overlook the challenge of effectively transforming structure information from NL into SQL. To address these limitations, we introduce UCS-SQL, a novel text-to-SQL framework, uniting both content and structure pipes to bridge the gap between NL and SQL. Specifically, the content pipe focuses on identifying key content within the original content, while the structure pipe is dedicated to transforming the linguistic structure from NL to SQL. Additionally, we strategically selects few-shot examples by considering both the SQL Skeleton and Question Expression (SS-QE selection method), thus providing targeted examples for SQL generation. Experimental results on BIRD and Spider demonstrate the effectiveness of our UCS-SQL framework.</abstract>
      <url hash="086083e7">2025.findings-acl.427</url>
      <bibkey>wu-etal-2025-ucs</bibkey>
    </paper>
    <paper id="428">
      <title><fixed-case>C</fixed-case>ode<fixed-case>PRM</fixed-case>: Execution Feedback-enhanced Process Reward Model for Code Generation</title>
      <author><first>Qingyao</first><last>Li</last></author>
      <author><first>Xinyi</first><last>Dai</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Yong</first><last>Yu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>8169-8182</pages>
      <abstract>Code generation is a critical reasoning task for large language models (LLMs). Recent advancements have focused on optimizing the thought process of code generation, achieving significant improvements. However, such thought process lacks effective process supervision, making it hard to optimize the thoughts. Although Process Reward Models (PRMs) have been widely established in mathematical reasoning, building a code PRM is still not trivial for the gap between thoughts to code. In this paper, we propose CodePRM, a novel approach that leverages the code execution feedback to build a code PRM. Specifically, we first collect a large dataset of thought traces, where each thought step is labeled with their derived code’ pass rates, accompanied by the corresponding code snippets, and execution feedback. During training, we train a PRM to take both the reasoning process and code execution feedback as input to score individual thought steps, enabling it to leverage code execution results to distinguish between high-quality and low-quality thought steps. Finally, to use the PRM during inference, we develop a Generate-Verify-Refine (GVR) pipeline where the CodePRM serves as a process verifier to dynamically identify and correct errors in the thought process during code search. Experimental results demonstrate that CodePRM with the inference algorithm outperforms strong baselines, significantly enhancing code generation performance. Further analysis reveals the key factors for building a code PRM.</abstract>
      <url hash="3d9bb449">2025.findings-acl.428</url>
      <bibkey>li-etal-2025-codeprm</bibkey>
    </paper>
    <paper id="429">
      <title><fixed-case>STEM</fixed-case>-<fixed-case>POM</fixed-case>: Evaluating Language Models Math-Symbol Reasoning in Document Parsing</title>
      <author><first>Jiaru</first><last>Zou</last></author>
      <author><first>Qing</first><last>Wang</last></author>
      <author><first>Pratyush</first><last>Thakur</last></author>
      <author><first>Nickvash</first><last>Kani</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>8183-8199</pages>
      <abstract>Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM (Science, Technology, Engineering, and Mathematics) documents.While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs’ reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments demonstrate that state-of-the-art LLMs achieve an average accuracy of 20-60% under in-context learning and 50-60% with fine-tuning, highlighting a substantial gap in their ability to classify mathematical symbols. By improving LLMs’ mathematical symbol classification, STEM-PoM further enhances models’ downstream mathematical reasoning capabilities. The code and data are available at https://github.com/jiaruzouu/STEM-PoM.</abstract>
      <url hash="d66c8ab4">2025.findings-acl.429</url>
      <bibkey>zou-etal-2025-stem</bibkey>
    </paper>
    <paper id="430">
      <title>Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models</title>
      <author><first>Jihoon</first><last>Lee</last></author>
      <author><first>Min</first><last>Song</last><affiliation>Yonsei University and Yonsei University</affiliation></author>
      <pages>8200-8219</pages>
      <abstract>Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.</abstract>
      <url hash="3dde626c">2025.findings-acl.430</url>
      <bibkey>lee-song-2025-retrieval</bibkey>
    </paper>
    <paper id="431">
      <title>Leveraging <fixed-case>LLM</fixed-case>s for <fixed-case>B</fixed-case>angla Grammar Error Correction: Error Categorization, Synthetic Data, and Model Evaluation</title>
      <author><first>Pramit</first><last>Bhattacharyya</last></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>8220-8239</pages>
      <abstract>Large Language Models (LLMs) perform exceedingly well in Natural Language Understanding (NLU) tasks for many languages including English. However, despite being the fifth most-spoken language globally, Grammatical Error Correction (GEC) in Bangla remains underdeveloped. In this work, we investigate how LLMs can be leveraged for improving Bangla GEC. For that, we first do an extensive categorization of 12 error classes in Bangla, and take a survey of native Bangla speakers to collect real-world errors. We next devise a rule-based noise injection method to create grammatically incorrect sentences corresponding to correct ones. The Vaiyākaraṇa dataset, thus created, consists of 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then used to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show that instruction-tuning with Vaiyākaraṇa improves GEC performance of LLMs by 3-7 percentage points as compared to the zero-shot setting, and makes them achieve human-like performance in grammatical error identification. Humans, though, remain superior in error correction. The data and code are available from https://github.com/Bangla-iitk/Vaiyakarana.</abstract>
      <url hash="2adb76d2">2025.findings-acl.431</url>
      <bibkey>bhattacharyya-bhattacharya-2025-leveraging</bibkey>
    </paper>
    <paper id="432">
      <title>Think Both Ways: Teacher-Student Bidirectional Reasoning Enhances <fixed-case>MCQ</fixed-case> Generation and Distractor Quality</title>
      <author><first>Yimiao</first><last>Qiu</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Quanming</first><last>Yao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhimeng</first><last>Zhang</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Zhiang</first><last>Dong</last></author>
      <author><first>Chang</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8240-8253</pages>
      <abstract>Generating high-quality Multiple Choice Questions (MCQs) remains challenging for educational tools due to the need for contextual relevance and plausible distractors. Existing methods still struggle with these dual requirements, leading to questions that lack depth and distractors that are either too obvious or irrelevant. In this paper, we propose BiFlow, a novel framework that integrates bidirectional reasoning perspectives: teacher reasoning generates contextually relevant questions and plausible distractors, while student reasoning evaluates question clarity and the misleading nature of the distractors. To further enhance reasoning, we introduce PathFinder, a mechanism that employs breadth-first search and Chain-of-Thought (CoT) strategies to explore diverse reasoning paths, improving both the quality and diversity of generated questions and distractors. Additionally, we enrich the FairytaleQA dataset to FairytaleMCQ with high-quality distractors, providing a robust benchmark for MCQ generation. Experimental results demonstrate that BiFlow outperforms existing methods, particularly in generating text-grounded questions and high-quality distractors for narrative contexts, highlighting its value in educational applications.</abstract>
      <url hash="fed3ff34">2025.findings-acl.432</url>
      <bibkey>qiu-etal-2025-think</bibkey>
    </paper>
    <paper id="433">
      <title>mm<fixed-case>E</fixed-case>5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data</title>
      <author><first>Haonan</first><last>Chen</last></author>
      <author><first>Liang</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nan</first><last>Yang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>8254-8275</pages>
      <abstract>Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets, and models are released in https://github.com/haon-chen/mmE5.</abstract>
      <url hash="5b78f4c7">2025.findings-acl.433</url>
      <bibkey>chen-etal-2025-mme5</bibkey>
    </paper>
    <paper id="434">
      <title><fixed-case>W</fixed-case>ord2<fixed-case>P</fixed-case>assage: Word-level Importance Re-weighting for Query Expansion</title>
      <author><first>Jeonghwan</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minjeong</first><last>Ban</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minseok</first><last>Kim</last><affiliation>Facebook</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>8276-8296</pages>
      <abstract>Retrieval-augmented generation (RAG) enhances the quality of LLM generation by providing relevant chunks, but retrieving accurately from external knowledge remains challenging due to missing contextually important words in query. We present Word2Passage, a novel approach that improves retrieval accuracy by optimizing word importance in query expansion. Our method generates references at word, sentence, and passage levels for query expansion, then determines word importance by considering both their reference level origin and characteristics derived from query types and corpus analysis. Specifically, our method assigns distinct importance scores to words based on whether they originate from word, sentence, or passage-level references. Extensive experiments demonstrate that Word2Passage outperforms existing methods across various datasets and LLM configurations, effectively enhancing both retrieval accuracy and generation quality. The code is publicly available at https://github.com/DISL-Lab/Word2Passage</abstract>
      <url hash="df8704c2">2025.findings-acl.434</url>
      <bibkey>choi-etal-2025-word2passage</bibkey>
    </paper>
    <paper id="435">
      <title><fixed-case>MEC</fixed-case>o<fixed-case>T</fixed-case>: <fixed-case>M</fixed-case>arkov Emotional Chain-of-Thought for Personality-Consistent Role-Playing</title>
      <author><first>Yangbo</first><last>Wei</last></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <author><first>Fangzhou</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Feng</last></author>
      <author><first>Wei W.</first><last>Xing</last><affiliation>University of Sheffield</affiliation></author>
      <pages>8297-8314</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable capabilities in role-playing dialogues, yet they often struggle to maintain emotionally consistent and psychologically plausible character personalities. We present MECoT (Markov Emotional Chain-of-Thought), a framework that enhances LLMs’ ability to generate authentic personality-driven dialogues through stochastic emotional transitions. Inspired by dual-process theory, MECoT combines a Markov-chain-driven emotional processor for intuitive responses with an LLM-based reasoning mechanism for rational regulation, mapped onto a 12-dimensional Emotion Circumplex Model. The framework dynamically adjusts emotional transitions using personality-weighted matrices and historical context, ensuring both emotional coherence and character consistency. We introduce the Role-playing And Personality Dialogue (RAPD) dataset, featuring diverse character interactions with fine-grained emotional annotations, along with novel metrics for evaluating emotional authenticity and personality alignment. Experimental results demonstrate MECoT’s effectiveness, achieving 93.3% emotional accuracy on RAPD and substantially outperforming existing approaches. Our analysis reveals optimal emotional granularity (12-16 categories) and validates our data-driven personality optimization approach. Code and data are available at <url>https://anonymous.4open.science/r/MECoT</url></abstract>
      <url hash="e2057f61">2025.findings-acl.435</url>
      <bibkey>wei-etal-2025-mecot</bibkey>
    </paper>
    <paper id="436">
      <title><fixed-case>F</fixed-case>i<fixed-case>D</fixed-case>e<fixed-case>L</fixed-case>i<fixed-case>S</fixed-case>: Faithful Reasoning in Large Language Models for Knowledge Graph Question Answering</title>
      <author><first>Yuan</first><last>Sui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yufei</first><last>He</last></author>
      <author><first>Nian</first><last>Liu</last></author>
      <author><first>Xiaoxin</first><last>He</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>8315-8330</pages>
      <abstract>Large Language Models (LLMs) are often challenged by generating erroneous or hallucinated responses, especially in complex reasoning tasks. Leveraging Knowledge Graphs (KGs) as external knowledge sources has emerged as a viable solution. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this paper, we propose a unified framework, FiDeLiS, designed to improve the factuality of LLM responses by anchoring answers to verifiable reasoning steps retrieved from KGs. To achieve this, we leverage step-wise beam search with a deductive scoring function, allowing the LLM to validate reasoning process step by step, and halt the search once the question is deducible. In addition, we propose a Path-RAG module to pre-select a smaller candidate set for each beam search step, reducing computational costs by narrowing the search space. Extensive experiments show that our method, as a training-free framework, not only improve the performance but also enhance the factuality and interpretability across different benchmarks.</abstract>
      <url hash="b955bca8">2025.findings-acl.436</url>
      <bibkey>sui-etal-2025-fidelis</bibkey>
    </paper>
    <paper id="437">
      <title><fixed-case>REALM</fixed-case>: A Dataset of Real-World <fixed-case>LLM</fixed-case> Use Cases</title>
      <author><first>Jingwen</first><last>Cheng</last></author>
      <author><first>Kshitish</first><last>Ghate</last></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Hong</first><last>Shen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Fei</first><last>Fang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>8331-8341</pages>
      <abstract>Large Language Models (LLMs), such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited.To address this, we introduce **REALM**, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. **REALM** captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users’ occupations relate to the types of applications they use.By integrating real-world data, **REALM** offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles. An interactive dashboard ([https://realm-e7682.web.app/](https://realm-e7682.web.app/)) is provided for easy exploration of the dataset.</abstract>
      <url hash="bdc860b1">2025.findings-acl.437</url>
      <bibkey>cheng-etal-2025-realm</bibkey>
    </paper>
    <paper id="438">
      <title><fixed-case>BabelEdits</fixed-case>: A Benchmark and a Modular Approach for Robust Cross-lingual Knowledge Editing of Large Language Models</title>
      <author><first>Tommaso</first><last>Green</last></author>
      <author><first>Félix</first><last>Gaschi</last><affiliation>Posos</affiliation></author>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>8342-8369</pages>
      <abstract>With Large Language Models (LLMs) becoming increasingly multilingual, effective knowledge editing (KE) needs to propagate edits across languages. Evaluation of the existing methods for cross-lingual knowledge editing (CKE) is limited both w.r.t. edit effectiveness: benchmarks do not account for entity aliases and use faulty entity translations; as well as robustness: existing work fails to report on downstream generation and task-solving abilities of LLMs after editing. In this work, we aim to (i) maximize the effectiveness of CKE while at the same time (ii) minimizing the extent of downstream model collapse due to the edits. To accurately measure the effectiveness of CKE methods, we introduce BabelEdits, a new CKE benchmark covering 60 languages that combines high-quality multilingual synsets from BabelNet with marker-based translation to ensure entity translation quality. Unlike existing CKE benchmarks, BabelEdits accounts for the rich variety of entity aliases within and across languages. We then propose BabelReFT, a modular CKE approach based on representation fine-tuning (ReFT) which learns entity-scope ReFT modules, applying them to all multilingual aliases at inference. Our experimental results show that not only is BabelReFT more effective in CKE than state-of-the-art methods, but, owing to its modular design, much more robust against downstream model collapse when subjected to many sequential edits.</abstract>
      <url hash="6d1657b7">2025.findings-acl.438</url>
      <bibkey>green-etal-2025-babeledits</bibkey>
    </paper>
    <paper id="439">
      <title><fixed-case>CDS</fixed-case>: Data Synthesis Method Guided by Cognitive Diagnosis Theory</title>
      <author><first>Haokun</first><last>Zhao</last></author>
      <author><first>Jinyi</first><last>Han</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaojun</first><last>Meng</last><affiliation>Noah’s Ark Lab, Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jiansheng</first><last>Wei</last></author>
      <pages>8370-8393</pages>
      <abstract>Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the **Cognitive Diagnostic Synthesis** (CDS) method, which incorporates a diagnostic process inspired by **Cognitive Diagnosis Theory** (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub https://anonymous.4open.science/r/cds-04D1.</abstract>
      <url hash="a64e423d">2025.findings-acl.439</url>
      <bibkey>zhao-etal-2025-cds</bibkey>
    </paper>
    <paper id="440">
      <title>Problem-Solving Logic Guided Curriculum In-Context Learning for <fixed-case>LLM</fixed-case>s Complex Reasoning</title>
      <author><first>Xuetao</first><last>Ma</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Wenbin</first><last>Jiang</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Hua</first><last>Huang</last><affiliation>Beijing Normal University</affiliation></author>
      <pages>8394-8412</pages>
      <abstract>In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently.</abstract>
      <url hash="286022e6">2025.findings-acl.440</url>
      <bibkey>ma-etal-2025-problem</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>BESSTIE</fixed-case>: A Benchmark for Sentiment and Sarcasm Classification for Varieties of <fixed-case>E</fixed-case>nglish</title>
      <author><first>Dipankar</first><last>Srirag</last></author>
      <author><first>Aditya</first><last>Joshi</last><affiliation>UNSW</affiliation></author>
      <author><first>Jordan</first><last>Painter</last></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <pages>8413-8429</pages>
      <abstract>Despite large language models (LLMs) being known to exhibit bias against non-mainstream varieties, there are no known labeled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two domains, namely, Google Place reviews and Reddit comments, we collect datasets for these language varieties using two methods: location-based and topic-based filtering. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine large language models (LLMs) (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results reveal that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), with significant performance drops for en-IN, particularly in sarcasm detection. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE datasets, code, and models will be publicly available upon acceptance.</abstract>
      <url hash="2bcf39d2">2025.findings-acl.441</url>
      <bibkey>srirag-etal-2025-besstie</bibkey>
    </paper>
    <paper id="442">
      <title><fixed-case>N</fixed-case>av<fixed-case>RAG</fixed-case>: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented <fixed-case>LLM</fixed-case></title>
      <author><first>Zihan</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yaohui</first><last>Zhu</last><affiliation>Beijing University of Chemical Technology</affiliation></author>
      <author><first>Gim Hee</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yachun</first><last>Fan</last></author>
      <pages>8430-8440</pages>
      <abstract>Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users’ communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models. The model trained on our NavRAG dataset achieves SOTA performance on the REVERIE benchmark.</abstract>
      <url hash="c1f434ed">2025.findings-acl.442</url>
      <bibkey>wang-etal-2025-navrag</bibkey>
    </paper>
    <paper id="443">
      <title><fixed-case>SQLF</fixed-case>orge: Synthesizing Reliable and Diverse Data to Enhance Text-to-<fixed-case>SQL</fixed-case> Reasoning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yu</first><last>Guo</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Dong</first><last>Jin</last></author>
      <author><first>Shenghao</first><last>Ye</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shuangwu</first><last>Chen</last></author>
      <author><first>Jianyang</first><last>Jianyang</last><affiliation>University of Science and Technology of China and University of Science and Technology of China</affiliation></author>
      <author><first>Xiaobin</first><last>Tan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>8441-8452</pages>
      <abstract>Large Language models (LLMs) have demonstrated significant potential in text-to-SQL reasoning tasks, yet a substantial performance gap persists between existing open-source models and their closed-source counterparts. In this paper, we introduce SQLForge, a novel approach for synthesizing reliable and diverse data to enhance text-to-SQL reasoning in LLMs. We improve data reliability through SQL syntax constraints and SQL-to-question reverse translation, ensuring data logic at both structural and semantic levels. We also propose an SQL template enrichment and iterative data domain exploration mechanism to boost data diversity. Building on the augmented data, we fine-tune a variety of open-source models with different architectures and parameter sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves the state-of-the-art performance on the widely recognized Spider and BIRD benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing the performance gap with closed-source methods.</abstract>
      <url hash="08184d62">2025.findings-acl.443</url>
      <bibkey>guo-etal-2025-sqlforge</bibkey>
    </paper>
    <paper id="444">
      <title>Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning</title>
      <author><first>Jiachen</first><last>Zhu</last></author>
      <author><first>Congmin</first><last>Zheng</last></author>
      <author><first>Jianghao</first><last>Lin</last></author>
      <author><first>Kounianhua</first><last>Du</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Ying</first><last>Wen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yong</first><last>Yu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>University College London</affiliation></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <pages>8453-8468</pages>
      <abstract>While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies the OOD issues including step OOD, arising from differences in reasoning patterns across model types and sizes, and question OOD, due to dataset shifts between training and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps for PRM as a warmup to stimulate its potential to judge target steps, improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetreivalPRM model, establishing a new standard for PRM performance.</abstract>
      <url hash="92052639">2025.findings-acl.444</url>
      <bibkey>zhu-etal-2025-retrieval</bibkey>
    </paper>
    <paper id="445">
      <title>Contrastive Learning for Task-Independent <fixed-case>S</fixed-case>peech<fixed-case>LLM</fixed-case>-Pretraining</title>
      <author><first>Maike</first><last>Züfle</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>8469-8490</pages>
      <abstract>Large language models (LLMs) excel in natural language processing but adapting these LLMs to speech processing tasks efficiently is not straightforward. Direct task-specific fine-tuning is limited by overfitting risks, data requirements, and computational costs. To address these challenges, we propose a scalable, two-stage training approach: (1) A task-independent speech pretraining stage using contrastive learning to align text and speech representations over all layers, followed by (2) a task-specific fine-tuning stage requiring minimal data. This approach outperforms traditional ASR pretraining and enables the model to surpass models specialized on speech translation and question answering while being trained on only 10% of the task-specific data.</abstract>
      <url hash="6019fdf4">2025.findings-acl.445</url>
      <bibkey>zufle-niehues-2025-contrastive</bibkey>
    </paper>
    <paper id="446">
      <title><fixed-case>Q</fixed-case>i<fixed-case>M</fixed-case>eng-Attention: <fixed-case>SOTA</fixed-case> Attention Operator is generated by <fixed-case>SOTA</fixed-case> Attention Algorithm</title>
      <author><first>Qirui</first><last>Zhou</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaohui</first><last>Peng</last></author>
      <author><first>Weiqiang</first><last>Xiong</last></author>
      <author><first>Haixin</first><last>Chen</last></author>
      <author><first>Yuanbo</first><last>Wen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Haochen</first><last>Li</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ling</first><last>Li</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Qi</first><last>Guo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongwei</first><last>Zhao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ke</first><last>Gao</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruizhi</first><last>Chen</last></author>
      <author><first>Yanjun</first><last>Wu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhao</first><last>Chen</last><affiliation>Institute of Software CAS&gt;</affiliation></author>
      <author><first>Yunji</first><last>Chen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8491-8505</pages>
      <abstract>The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance.To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs’ understanding of attention operator.Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms.Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to <tex-math>35.16\times</tex-math>.Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.</abstract>
      <url hash="636f1966">2025.findings-acl.446</url>
      <bibkey>zhou-etal-2025-qimeng</bibkey>
    </paper>
    <paper id="447">
      <title><fixed-case>ALW</fixed-case>: Adaptive Layer-Wise contrastive decoding enhancing reasoning ability in Large Language Models</title>
      <author><first>Yuechi</first><last>Zhou</last></author>
      <author><first>Chuyue</first><last>Zhou</last></author>
      <author><first>Jianxin</first><last>Zhang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>8506-8524</pages>
      <abstract>Large language models (LLMs) have achieved remarkable performance across various reasoning tasks. However, many LLMs still encounter challenges in reasoning, especially for LLMs with fewer parameters or insufficient pre-training data. Through our experiments, we identify that noise accumulation across layers often leads to unstable token predictions during reasoning. We find that contrasting the probability distributions across layers effectively mitigates this interference. Building on this insight, we propose Adaptive Layer-Wise contrastive decoding (ALW), a novel framework that enhances reasoning ability by dynamically disentangling noise in shallow layers from critical signals in deep layers. Extensive experiments on several reasoning benchmarks demonstrate that ALW consistently improves answer accuracy across multiple LLMs while maintaining inference efficiency. For example, we achieve a 48% improvement on the Gsm8k using the LLaMA-7B model and an absolute accuracy increase of 5.2 points on the BBH evaluation benchmark with the LLaMA-65B model.</abstract>
      <url hash="4cc1f057">2025.findings-acl.447</url>
      <bibkey>zhou-etal-2025-alw</bibkey>
    </paper>
    <paper id="448">
      <title>Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models</title>
      <author><first>Xinlong</first><last>Chen</last></author>
      <author><first>Yuanxing</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Junfei</first><last>Wu</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Tieniu</first><last>Tan</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>8525-8542</pages>
      <abstract>Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model’s attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model’s attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. Code is available at https://github.com/xlchen0205/MoD.</abstract>
      <url hash="30a05e8e">2025.findings-acl.448</url>
      <bibkey>chen-etal-2025-mixture-decoding</bibkey>
    </paper>
    <paper id="449">
      <title><fixed-case>V</fixed-case>id<fixed-case>C</fixed-case>ap<fixed-case>B</fixed-case>ench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation</title>
      <author><first>Xinlong</first><last>Chen</last></author>
      <author><first>Yuanxing</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Chongling</first><last>Rao</last></author>
      <author><first>Yushuo</first><last>Guan</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Chengru</first><last>Song</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Tieniu</first><last>Tan</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>8543-8563</pages>
      <abstract>The training of controllable text-to-video (T2V) models relies heavily on the alignment between videos and captions, yet little existing research connects video caption evaluation with T2V generation assessment. This paper introduces VidCapBench, a video caption evaluation scheme specifically designed for T2V generation, agnostic to any particular caption format. VidCapBench employs a data annotation pipeline, combining expert model labeling and human refinement, to associate each collected video with key information spanning video aesthetics, content, motion, and physical laws. VidCapBench then partitions these key information attributes into automatically assessable and manually assessable subsets, catering to both the rapid evaluation needs of agile development and the accuracy requirements of thorough validation. By evaluating numerous state-of-the-art captioning models, we demonstrate the superior stability and comprehensiveness of VidCapBench compared to existing video captioning evaluation approaches. Verification with off-the-shelf T2V models reveals a significant positive correlation between scores on VidCapBench and the T2V quality evaluation metrics, indicating that VidCapBench can provide valuable guidance for training T2V models. The project is available at https://github.com/VidCapBench/VidCapBench.</abstract>
      <url hash="445a8eaf">2025.findings-acl.449</url>
      <bibkey>chen-etal-2025-vidcapbench</bibkey>
    </paper>
    <paper id="450">
      <title>Mitigating Demonstration Bias through Global Coevolutionary Reasoning</title>
      <author><first>Chuan</first><last>Gou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bangwei</first><last>Li</last></author>
      <author><first>Jianhua</first><last>Dai</last></author>
      <author><first>Xiaoyang</first><last>Han</last></author>
      <author><first>Ming</first><last>Cai</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8564-8578</pages>
      <abstract>Recent advances in large language models (LLMs) have demonstrated the effectiveness of chain-of-thought (CoT) prompting. Few-Shot-CoT relies on task-specific, manually labeled demonstrations, limiting its generalization to unseen tasks. While Zero-Shot-CoT eliminates this reliance, it often underperforms. To address this, existing methods aim to automatically generate demonstrations in zero-shot settings. However, these generated demonstrations face challenges due to demonstration bias: 1) selected demonstrations may contain errors, and 2) they may not be suitable or representative enough for all questions. To mitigate these biases, we propose Global Coevolutionary Reasoning (GCR). The method first applies Zero-Shot-CoT to answer all questions, then clusters the results. For each cluster, a random sample is selected, and these selected samples serve as demonstrations for each other. The model then iteratively re-answers the questions and updates their rationales based on these demonstrations, enabling coevolutionary reasoning to progressively improve the quality of the answers. This process of random sampling and coevolutionary reasoning is repeated until all questions have been re-answered. Experimental results on ten datasets using GPT-3.5-turbo and GPT-4o-mini show that GCR outperforms baseline methods without any performance degradation caused by demonstration bias. Additionally, GCR is orthogonal to existing methods and can be seamlessly integrated with them. The code is available at: https://github.com/GouChuan/GCR.</abstract>
      <url hash="69297632">2025.findings-acl.450</url>
      <bibkey>gou-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="451">
      <title>A Representation Level Analysis of <fixed-case>NMT</fixed-case> Model Robustness to Grammatical Errors</title>
      <author><first>Abderrahmane</first><last>Issam</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Yusuf Can</first><last>Semerci</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Jan</first><last>Scholtes</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>8579-8601</pages>
      <abstract>Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term *Robustness Heads*. We find that *Robustness Heads* attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on *Robustness Heads* for updating the ungrammatical word representation.</abstract>
      <url hash="e2c95581">2025.findings-acl.451</url>
      <bibkey>issam-etal-2025-representation</bibkey>
    </paper>
    <paper id="452">
      <title><tex-math>T^2DR</tex-math>: A Two-Tier Deficiency-Resistant Framework for Incomplete Multimodal Learning</title>
      <author><first>Han</first><last>Lin</last></author>
      <author><first>Xiu</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huan</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wenxue</first><last>Cao</last></author>
      <author><first>Sai</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chang</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lidan</first><last>Shou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Gang</first><last>Chen</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <pages>8602-8616</pages>
      <abstract>Multimodal learning is garnering significant attention for its capacity to represent diverse human perceptions (e.g., linguistic, acoustic, and visual signals), achieving more natural and intuitive interactions with technology.However, the frequent occurrence of incomplete data, either within a single modality (intra-modality) or across different modalities (inter-modality), presents substantial challenges in reliable semantic interpretation and model reasoning.Furthermore, there is currently no robust representation learning mechanism capable of managing both intra-modality and inter-modality real-data deficiencies.To address this challenge, we present <tex-math>\text{T}^2</tex-math>DR, a two-tier deficiency-resistant framework for incomplete multimodal learning, which comprises two main modules:(1) Intra-Modal Deficiency-Resistant module (IADR): To address fine-grained deficiencies, we introduce Intra-Attn to focus on the available data while avoiding excessive suppression of the missing regions.(2) Inter-Modal Deficiency-Resistant module (IEDR): To handle coarse-grained deficiencies, we propose the shared feature prediction (SFP) to leverage cross-modal shared features for preliminary data imputation. Subsequently, we apply Inter-Attn to allocate appropriate attention to each modality based on the results from the capability-aware scorer (CAS).Extensive experiments are performed on two well-known multimodal benchmarks, CMU-MOSI and CMU-MOSEI, across various missing scenarios for sentiment analysis. Experimental results show that <tex-math>\text{T}^2</tex-math>DR significantly outperforms the SOTA models. Code is available at <url>https://github.com/LH019/T2DR</url>.</abstract>
      <url hash="79172a88">2025.findings-acl.452</url>
      <bibkey>lin-etal-2025-t2dr</bibkey>
    </paper>
    <paper id="453">
      <title>From Specific-<fixed-case>MLLM</fixed-case>s to Omni-<fixed-case>MLLM</fixed-case>s: A Survey on <fixed-case>MLLM</fixed-case>s Aligned with Multi-modalities</title>
      <author><first>Shixin</first><last>Jiang</last></author>
      <author><first>Jiafeng</first><last>Liang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jiyuan</first><last>Wang</last></author>
      <author><first>Xuan</first><last>Dong</last></author>
      <author><first>Heng</first><last>Chang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Jinhua</first><last>Du</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>8617-8652</pages>
      <abstract>To tackle complex tasks in real-world scenarios, more researchers are focusing on Omni-MLLMs, which aim to achieve omni-modal understanding and generation. Beyond the constraints of any specific non-linguistic modality, Omni-MLLMs map various non-linguistic modalities into the embedding space of LLMs and enable the interaction and understanding of arbitrary combinations of modalities within a single model. In this paper, we systematically investigate relevant research and provide a comprehensive survey of Omni-MLLMs. Specifically, we first explain the four core components of Omni-MLLMs for unified multi-modal modeling with a meticulous taxonomy that offers novel perspectives. Then, we introduce the effective integration achieved through two-stage training and discuss the corresponding datasets as well as evaluation. Furthermore, we summarize the main challenges of current Omni-MLLMs and outline future directions. We hope this paper serves as an introduction for beginners and promotes the advancement of related research. Resources have been made publicly availableat https://github.com/threegold116/Awesome-Omni-MLLMs.</abstract>
      <url hash="8c46d449">2025.findings-acl.453</url>
      <bibkey>jiang-etal-2025-specific</bibkey>
    </paper>
    <paper id="454">
      <title>Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter</title>
      <author><first>Verena</first><last>Blaschke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Masha</first><last>Fedzechkina</last><affiliation>Apple</affiliation></author>
      <author><first>Maartje</first><last>Ter Hoeve</last><affiliation>Apple</affiliation></author>
      <pages>8653-8684</pages>
      <abstract>Cross-lingual transfer is a popular approach to increase the amount of training data for NLP tasks in a low-resource context. However, the best strategy to decide which cross-lingual data to include is unclear. Prior research often focuses on a small set of languages from a few language families and/or a single task. It is still an open question how these findings extend to a wider variety of languages and tasks. In this work, we analyze cross-lingual transfer for 263 languages from a wide variety of language families. Moreover, we include three popular NLP tasks: POS tagging, dependency parsing, and topic classification. Our findings indicate that the effect of linguistic similarity on transfer performance depends on a range of factors: the NLP task, the (mono- or multilingual) input representations, and the definition of linguistic similarity.</abstract>
      <url hash="31f94938">2025.findings-acl.454</url>
      <bibkey>blaschke-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="455">
      <title>Agents generalize to novel levels of abstraction by using adaptive linguistic strategies</title>
      <author><first>Kristina</first><last>Kobrock</last><affiliation>Universität Osnabrück</affiliation></author>
      <author><first>Xenia</first><last>Ohmer</last></author>
      <author><first>Elia</first><last>Bruni</last><affiliation>Universität Osnabrück</affiliation></author>
      <author><first>Nicole</first><last>Gotzner</last><affiliation>Institute of Cognitive Science, Osnabrück University, Universität Osnabrück</affiliation></author>
      <pages>8685-8699</pages>
      <abstract>We study abstraction in an emergent communication paradigm. In emergent communication, two artificial neural network agents develop a language while solving a communicative task. In this study, the agents play a concept-level reference game. This means that the speaker agent has to describe a concept to a listener agent, who has to pick the correct target objects that satisfy the concept. Concepts consist of multiple objects and can be either more specific, i.e. the target objects share many attributes, or more generic, i.e. the target objects share fewer attributes. We tested two directions of zero-shot generalization to novel levels of abstraction: When generalizing from more generic to very specific concepts, agents utilized a compositional strategy. When generalizing from more specific to very generic concepts, agents utilized a more flexible linguistic strategy that involves reusing many messages from training. Our results provide evidence that neural network agents can learn robust concepts based on which they can generalize using adaptive linguistic strategies. We discuss how this research provides new hypotheses on abstraction and informs linguistic theories on efficient communication.</abstract>
      <url hash="854c0d31">2025.findings-acl.455</url>
      <bibkey>kobrock-etal-2025-agents</bibkey>
    </paper>
    <paper id="456">
      <title>The Linguistic Connectivities Within Large Language Models</title>
      <author><first>Dan</first><last>Wang</last></author>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Ning</first><last>Bian</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jia</first><last>Zheng</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last><affiliation>Ricoh Software Research Center Beijing Co., Ltd.</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <pages>8700-8714</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable multilingual abilities in various applications. Unfortunately, recent studies have discovered that there exist notable disparities in their performance across different languages. Understanding the underlying mechanisms behind such disparities is crucial ensuring equitable access to LLMs for a global user base. Therefore, this paper conducts a systematic investigation into the behaviors of LLMs across 27 different languages on 3 different scenarios, and reveals a Linguistic Map correlates with the richness of available resources and linguistic family relations. Specifically, high-resource languages within specific language family exhibit greater knowledge consistency and mutual information dissemination, while isolated or low-resource languages tend to remain marginalized. Our research sheds light on a deep understanding of LLM’s cross-language behavior, highlights the inherent biases in LLMs within multilingual environments and underscores the need to address these inequities.</abstract>
      <url hash="a2a8b5a0">2025.findings-acl.456</url>
      <bibkey>wang-etal-2025-linguistic</bibkey>
    </paper>
    <paper id="457">
      <title><fixed-case>XF</fixed-case>in<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>LLM</fixed-case>s in Complex Financial Problem Solving and Reasoning</title>
      <author id="zhihan-zhang-smu"><first>Zhihan</first><last>Zhang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <pages>8715-8758</pages>
      <abstract>Solving financial problems demands complex reasoning, multimodal data processing, and a broad technical understanding, presenting unique challenges for current large language models (LLMs). We introduce **XFinBench**, a novel benchmark with 4,235 examples designed to evaluate LLM’s ability in solving comple**X**, knowledge-intensive **Fin**ancial problems across diverse graduate-level finance topics with multi-modal context. We identify five core capabilities of LLMs using XFinBench, i.e., _terminology understanding_, _temporal reasoning_, _future forecasting_, _scenario planning_, and _numerical modelling_. Upon XFinBench, we conduct extensive experiments on 18 leading models. The result shows that o1 is the best-performing text-only model with an overall accuracy of 67.3%, but still lags significantly behind human experts with 12.5%, especially in temporal reasoning and scenario planning capabilities. We further construct a knowledge bank with 3,032 finance terms for knowledge augmentation analysis, and find that relevant knowledge to the question only brings consistent accuracy improvements to small open-source model. Additionally, our error analysis reveals that rounding errors during calculation and blindness to position and intersection of curves in the image are two primary issues leading to model’s poor performance in calculating and visual-context questions, respectively.</abstract>
      <url hash="e232e714">2025.findings-acl.457</url>
      <bibkey>zhang-etal-2025-xfinbench</bibkey>
    </paper>
    <paper id="458">
      <title><fixed-case>A</fixed-case>lign<tex-math>^2</tex-math><fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>: Cascaded Human and Large Language Model Preference Alignment for Multi-modal Instruction Curation</title>
      <author><first>Hongzhe</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiang</first><last>Liu</last></author>
      <author><first>Zhewen</first><last>Yu</last></author>
      <author><first>Li</first><last>Cai</last></author>
      <author><first>Dian</first><last>Jiao</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Wenqiao</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Siliang</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Juncheng</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Hao</first><last>Jiang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Yueting</first><last>Zhuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8759-8781</pages>
      <abstract>Recent advances in Multi-modal Large Language Models (MLLMs), such as LLaVA-series models, are driven by massive machine-generated instruction-following data tuning. Such automatic instruction collection pipelines, however, inadvertently introduce significant variability in data quality. This paper introduces a novel instruction curation algorithm, derived from two unique perspectives, human and LLM preference alignment, to compress this vast corpus of machine-generated multimodal instructions to a compact and high-quality form: (i) For human preference alignment, we have collected a machine-generated multimodal instruction dataset and established a comprehensive set of both subjective and objective criteria to guide the data quality assessment critically from human experts. By doing so, a reward model was trained on the annotated dataset to internalize the nuanced human understanding of instruction alignment. (ii) For LLM preference alignment, given the instruction selected by the reward model, we propose leveraging the inner LLM used in MLLM to align the writing style of visual instructions with that of the inner LLM itself, resulting in LLM-aligned instruction improvement. Extensive experiments demonstrate that we can maintain or even improve model performance by compressing synthetic multimodal instructions by up to 90%. Impressively, by aggressively reducing the training instructions from 158k to 14k (9× smaller), our model consistently outperforms its full-size dataset counterpart across various MLLM benchmarks. Our project is available at https://github.com/DCDmllm/Align2LLaVA.</abstract>
      <url hash="e6e46bde">2025.findings-acl.458</url>
      <bibkey>huang-etal-2025-align2llava</bibkey>
    </paper>
    <paper id="459">
      <title>Achieving binary weight and activation for <fixed-case>LLM</fixed-case>s using Post-Training Quantization</title>
      <author><first>Siqing</first><last>Song</last></author>
      <author><first>Chuang</first><last>Wang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Rui-Qi</first><last>Wang</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xu-Yao</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>8782-8795</pages>
      <abstract>Quantizing large language models (LLMs) to 1-bit precision significantly reduces computational costs, but existing quantization techniques suffer from noticeable performance degradation when using weight and activation precisions below 4 bits (W4A4). In this paper, we propose a post-training quantization framework with W(1+1)A(1×4) configuration, where weights are quantized to 1 bit with an additional 1 bit for fine-grain grouping and activations are quantized to 1 bit with a 4-fold increase in the number of channels. For weight quantization, we propose utilizing Hessian-aware fine-grained grouping along with an EM-based quantization scheme. For activation quantization, we decompose INT4-quantized activations into a 4 × INT1 format equivalently and simultaneously smooth the scaling factors based on quantization errors, which further reduces the quantization errors in activations. Our method surpasses state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple tasks, pushing the boundaries of existing LLM quantization methods toward fully binarized models. Code is available at https://github.com/JimmyCrave/LLM-PTQ-binarization.</abstract>
      <url hash="340ac566">2025.findings-acl.459</url>
      <bibkey>song-etal-2025-achieving</bibkey>
    </paper>
    <paper id="460">
      <title>Mitigating Negative Interference in Multilingual Knowledge Editing through Null-Space Constraints</title>
      <author><first>Wei</first><last>Sun</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Tingyu</first><last>Qu</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Mingxiao</first><last>Li</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Jesse</first><last>Davis</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Marie-Francine</first><last>Moens</last><affiliation>KU Leuven, KU Leuven</affiliation></author>
      <pages>8796-8810</pages>
      <abstract>Efficiently updating multilingual knowledge in large language models (LLMs) without disrupting coherent factual representations across languages remains a significant challenge. While deploying separate editing systems for each language might seem viable, this approach incurs substantial costs due to the need to manage multiple models. A more efficient solution involves integrating knowledge updates across all languages into a unified model. However, sequential edits across languages often lead to destructive parameter interference, significantly degrading multilingual generalization and the accuracy of injected knowledge. To address this issue, we propose LangEdit, a novel null-space constrained framework designed to precisely isolate language-specific knowledge updates. The core innovation of LangEdit lies in its ability to project parameter updates for each language onto the orthogonal complement of other languages’ subspaces. This approach mathematically guarantees update independence while preserving multilingual generalization capabilities. We conduct a comprehensive evaluation across three model architectures, six languages, and four downstream tasks, demonstrating that LangEdit effectively mitigates parameter interference and outperforms existing state-of-the-art editing methods. Our results highlight its potential for enabling efficient and accurate multilingual knowledge updates in LLMs.</abstract>
      <url hash="f9fab66c">2025.findings-acl.460</url>
      <bibkey>sun-etal-2025-mitigating-negative</bibkey>
    </paper>
    <paper id="461">
      <title>From Awareness to Adaptability: Enhancing Tool Utilization for Scientific Reasoning</title>
      <author><first>Wenjing</first><last>Xie</last></author>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Wanfu</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Qiaoming</first><last>Zhu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>8811-8831</pages>
      <abstract>As large language models (LLMs) are increasingly applied to complex scientific problem-solving, their effectiveness is often limited by unconscious or failed tool usage. To address this issue, we introduce the Tool-Awareness Training (TAT) method, designed to enhance scientific reasoning. This approach leverages both forward and backward data generation strategies to strengthen the model’s conscious and selective tool utilization in multi-step reasoning tasks. Our method unfolds in three stages: (1) developing tool-knowledge through backward tooluse data generation (2) enhancing tool-awareness in multi-step reasoning by utilizing forward reasoning data, and (3) improving domain adaptability through large-scale domain-specific data for multi-task learning. These three stages progressively establish the foundation for tool learning and scientific reasoning, effectively integrating both, enabling the model to tackle multi-domain scientific tasks while optimizing tool usage. Our experimental results demonstrate that TAT significantly enhances LLM performance in mathematical and scientific reasoning tasks, particularly by improving the model’s tool utilization capabilities, including proactivity and execution success rates.</abstract>
      <url hash="8239b252">2025.findings-acl.461</url>
      <bibkey>xie-etal-2025-awareness</bibkey>
    </paper>
    <paper id="462">
      <title><fixed-case>AM</fixed-case>o<fixed-case>PO</fixed-case>: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Jingqing</first><last>Ruan</last></author>
      <author><first>Hao</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Haodong</first><last>Zhao</last></author>
      <author><first>Desheng</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Jiansong</first><last>Chen</last></author>
      <author><first>Wan</first><last>Guanglu</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <author><first>Zhi</first><last>Zheng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>8832-8866</pages>
      <abstract>Existing multi-objective preference alignment methods for large language models (LLMs) face limitations: (1) the inability to effectively balance various preference dimensions, and (2) reliance on auxiliary reward/reference models introduces computational complexity. To address these challenges, we propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel framework that achieves dynamic balance across preference dimensions. By introducing the multi-objective optimization paradigm to use the dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with diverse preferences without additional reward models or reference models. We introduce an adaptive weight assignment mechanism that models the generation space as a Gaussian distribution, allowing dynamic prioritization of preference dimensions. Empirical results demonstrate that AMoPO outperforms state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B models reveal the scaling ability of AMoPO. Moreover, additional analysis of multiple dimensions verifies its adaptability and effectiveness. These findings validate AMoPO’s capability to achieve dimension-aware preference alignment, highlighting its superiority. Our codes and datasets are available at https://github.com/Javkonline/AMoPO.</abstract>
      <url hash="ede82427">2025.findings-acl.462</url>
      <bibkey>liu-etal-2025-amopo</bibkey>
    </paper>
    <paper id="463">
      <title>Supervised Optimism Correction: Be Confident When <fixed-case>LLM</fixed-case>s Are Sure</title>
      <author><first>Junjie</first><last>Zhang</last></author>
      <author><first>Rushuai</first><last>Yang</last></author>
      <author><first>Shunyu</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Yi</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>8867-8880</pages>
      <abstract>In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit <tex-math>Q</tex-math>-function for inference.Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated <tex-math>Q</tex-math>-value estimations of suboptimal steps. To address this limitation, we propose **S**upervised **O**ptimism **C**orrection (SOC), which introduces a simple yet effective auxiliary loss for token-level <tex-math>Q</tex-math>-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularizationto boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses.Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models.</abstract>
      <url hash="b76f8f0e">2025.findings-acl.463</url>
      <bibkey>zhang-etal-2025-supervised</bibkey>
    </paper>
    <paper id="464">
      <title>Offline Reinforcement Learning for <fixed-case>LLM</fixed-case> Multi-step Reasoning</title>
      <author><first>Huaijie</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shibo</first><last>Hao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hanze</first><last>Dong</last></author>
      <author><first>Shenao</first><last>Zhang</last><affiliation>Northwestern University and Georgia Institute of Technology</affiliation></author>
      <author><first>Yilin</first><last>Bao</last></author>
      <author><first>Ziran</first><last>Yang</last><affiliation>University of California, San Diego and Peking University</affiliation></author>
      <author><first>Yi</first><last>Wu</last></author>
      <pages>8881-8893</pages>
      <abstract>Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline REasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH), and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost the performance during test time.</abstract>
      <url hash="0ad85e29">2025.findings-acl.464</url>
      <bibkey>wang-etal-2025-offline</bibkey>
    </paper>
    <paper id="465">
      <title>Sampling-based Pseudo-Likelihood for Membership Inference Attacks</title>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Youmi</first><last>Ma</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Yuki</first><last>Wata</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <pages>8894-8907</pages>
      <abstract>Large Language Models (LLMs) are trained on large-scale web data, which makes it difficult to grasp the contribution of each text. This poses the risk of leaking inappropriate data such as benchmarks, personal information, and copyrighted texts in the training data. Membership Inference Attacks (MIA), which determine whether a given text is included in the model’s training data, have been attracting attention. Previous studies of MIAs revealed that likelihood-based classification is effective for detecting leaks in LLMs. However, the existing likelihood-based methods cannot be applied to some proprietary models like ChatGPT or Claude 3 because the likelihood for input text is unavailable to the user. In this study, we propose a Sampling-based Pseudo-Likelihood (SPL) method for MIA (SaMIA) that calculates SPL using only the text generated by an LLM to detect leaks. The SaMIA treats the target text as the reference text and multiple outputs from the LLM as text samples, calculates the degree of <tex-math>n</tex-math>-gram match as SPL, and determines the membership of the text in the training data. Even without likelihoods, SaMIA performed on par with existing likelihood-based methods.</abstract>
      <url hash="6cd27a39">2025.findings-acl.465</url>
      <bibkey>kaneko-etal-2025-sampling</bibkey>
    </paper>
    <paper id="466">
      <title><fixed-case>A</fixed-case>gent<fixed-case>S</fixed-case>tore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant</title>
      <author><first>Chengyou</first><last>Jia</last></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Zhuohang</first><last>Dang</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Junlin</first><last>Hu</last></author>
      <author><first>Tianbao</first><last>Xie</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>8908-8934</pages>
      <abstract>Digital agents capable of automating complex computer tasks have attracted considerable attention. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore allows the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core MetaAgent with the AgentToken strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three interactive real-world benchmarks demonstrate that AgentStore significantly expands the capability boundaries of agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist computer assistant.</abstract>
      <url hash="37d91aea">2025.findings-acl.466</url>
      <bibkey>jia-etal-2025-agentstore</bibkey>
    </paper>
    <paper id="467">
      <title>Boosting Vulnerability Detection of <fixed-case>LLM</fixed-case>s via Curriculum Preference Optimization with Synthetic Reasoning Data</title>
      <author><first>Xin-Cheng</first><last>Wen</last></author>
      <author><first>Yijun</first><last>Yang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Cuiyun</first><last>Gao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Xiao</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Deheng</first><last>Ye</last><affiliation>Tencent and Tencent</affiliation></author>
      <pages>8935-8949</pages>
      <abstract>Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models’ ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24%-22.77% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.</abstract>
      <url hash="f690ca69">2025.findings-acl.467</url>
      <bibkey>wen-etal-2025-boosting</bibkey>
    </paper>
    <paper id="468">
      <title><tex-math>GA-S^3</tex-math>: Comprehensive Social Network Simulation with Group Agents</title>
      <author><first>Yunyao</first><last>Zhang</last></author>
      <author><first>Zikai</first><last>Song</last></author>
      <author><first>Hang</first><last>Zhou</last></author>
      <author><first>Wenfeng</first><last>Ren</last><affiliation>Silicon Universe Technology Co., Ltd.</affiliation></author>
      <author><first>Yi-Ping Phoebe</first><last>Chen</last><affiliation>La Trobe University</affiliation></author>
      <author><first>Junqing</first><last>Yu</last></author>
      <author><first>Wei</first><last>Yang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>8950-8970</pages>
      <abstract>Social network simulation is developed to provide a comprehensive understanding of social networks in the real world, which can be leveraged for a wide range of applications such as group behavior emergence, policy optimization, and business strategy development. However, billions of individuals and their evolving interactions involved in social networks pose challenges in accurately reflecting real-world complexities. In this study, we propose a comprehensive <tex-math>S</tex-math>ocial network <tex-math>S</tex-math>imulation <tex-math>S</tex-math>ystem (<tex-math>GA\text{-}S^3</tex-math>) that leverages newly designed <tex-math>G</tex-math>roup <tex-math>A</tex-math>gents to make intelligent decisions regarding various online events. Unlike other intelligent agents that represent an individual entity, our group agents model a collection of individuals exhibiting similar behaviors, facilitating the simulation of large-scale network phenomena with complex interactions at a manageable computational cost. Additionally, we have constructed a social network benchmark from 2024 popular online events that contains fine-grained information on Internet traffic variations. The experiment demonstrates that our approach is capable of achieving accurate and highly realistic prediction results.</abstract>
      <url hash="e439df67">2025.findings-acl.468</url>
      <bibkey>zhang-etal-2025-ga</bibkey>
    </paper>
    <paper id="469">
      <title><fixed-case>M</fixed-case>-<fixed-case>R</fixed-case>ange<fixed-case>D</fixed-case>etector: Enhancing Generalization in Machine-Generated Text Detection through Multi-Range Attention Masks</title>
      <author><first>Kaijie</first><last>Jiao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Licheng</first><last>Zhang</last></author>
      <author><first>Zikang</first><last>Guo</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>8971-8983</pages>
      <abstract>The increasing capability and widespread usage of large language models (LLMs) highlight the desirability of automatic detection of machine-generated text. Existing supervised detectors often overfit within their training domains, as they have primarily learned domain-specific textual features, such as word frequency, syntax, and semantics. In this paper, we introduce a domain-independent feature, namely the difference of writing strategy between LLMs and human, to improve the out-of-domain generalization capability of detectors. LLMs focus on the preceding range tokens when generating a token, while human consider multiple ranges, including bidirectional, global, and local contexts. The attention mask influences the range of tokens to which the model can attend. Therefore, we propose a method called M-RangeDetector, which integrates four distinct attention masking strategies into a Multi-Range Attention module, enabling the model to capture diverse writing strategies. Specifically, with the global mask, band mask, dilated mask, and random mask, our method learns various writing strategies for machine-generated text detection. The experimental results on three datasets demonstrate the superior generalization capability of our method.</abstract>
      <url hash="8a8f2741">2025.findings-acl.469</url>
      <bibkey>jiao-etal-2025-rangedetector</bibkey>
    </paper>
    <paper id="470">
      <title>Does Your Voice Assistant Remember? Analyzing Conversational Context Recall and Utilization in Voice Interaction Models</title>
      <author><first>Heeseung</first><last>Kim</last></author>
      <author><first>Che Hyun</first><last>Lee</last></author>
      <author><first>Sangkwon</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jiheum</first><last>Yeom</last></author>
      <author><first>Nohil</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>8984-9014</pages>
      <abstract>Recent advancements in multi-turn voice interaction models have improved user-model communication. However, while closed-source models effectively retain and recall past utterances, whether open-source models share this ability remains unexplored. To fill this gap, we systematically evaluate how well open-source interaction models utilize past utterances using ContextDialog, a benchmark we proposed for this purpose. Our findings show that speech-based models have more difficulty than text-based ones, especially when recalling information conveyed in speech, and even with retrieval-augmented generation, models still struggle with questions about past utterances. These insights highlight key limitations in open-source models and suggest ways to improve memory retention and retrieval robustness.</abstract>
      <url hash="549cd820">2025.findings-acl.470</url>
      <bibkey>kim-etal-2025-voice-assistant</bibkey>
    </paper>
    <paper id="471">
      <title><fixed-case>N</fixed-case>euron<fixed-case>M</fixed-case>erge: Merging Models via Functional Neuron Groups</title>
      <author><first>Wangyun</first><last>Gu</last></author>
      <author><first>Qianghua</first><last>Gao</last></author>
      <author><first>Zhang</first><last>Li-Xin</last></author>
      <author><first>Xu</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>9015-9037</pages>
      <abstract>Model merging techniques like task arithmetic, which combines model parameters through weighted averaging, have proven effective. However, the success of task arithmetic relies on the linearity between model weight differences and output feature changes, which is often lacking in conventional fine-tuned models. In this work, we employ neuron description methods to analyze and classify neurons based on their functionalities. We theoretically demonstrate that grouping Multi-Layer Perceptron (MLP) neurons by functionality enhances model linearity. Building on this, we propose a neuron-based task arithmetic merging method that consistently improves performance across various tasks and model scales. Our approach is complementary to existing merging techniques, achieving superior results in merging models fine-tuned on fundamental tasks like Math, Code and Translation.</abstract>
      <url hash="99bbf2e8">2025.findings-acl.471</url>
      <bibkey>gu-etal-2025-neuronmerge</bibkey>
    </paper>
    <paper id="472">
      <title><fixed-case>H</fixed-case>ella<fixed-case>S</fixed-case>wag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of <fixed-case>LLM</fixed-case>s in Commonsense Reasoning</title>
      <author><first>Xiaoyuan</first><last>Li</last></author>
      <author><first>Moxin</first><last>Li</last></author>
      <author><first>Rui</first><last>Men</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yichang</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Keqin</first><last>Bao</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>9038-9072</pages>
      <abstract>Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.</abstract>
      <url hash="b606daa4">2025.findings-acl.472</url>
      <bibkey>li-etal-2025-hellaswag</bibkey>
    </paper>
    <paper id="473">
      <title>Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models</title>
      <author><first>Hao</first><last>Xiang</last></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>9073-9085</pages>
      <abstract>The key to effective alignment lies in high-quality preference data. Recent research has focused on automated alignment, which involves developing alignment systems with minimal human intervention. However, prior research has predominantly focused on developing data generation methods, while insufficient attention has been paid to quality control mechanisms and often produces inaccurate and unhelpful data, leading to unpredictable benefits during iterative optimization. In this paper, we present Self-Steering Optimization (<tex-math>SSO</tex-math>), an algorithm that autonomously generates high-quality preference data, eliminating manual annotation requirements. <tex-math>SSO</tex-math> employs a specialized optimization objective to build a data generator from the policy model itself, which is used to produce accurate and on-policy data. We demonstrate <tex-math>SSO</tex-math>‘s effectiveness through comprehensive experiments on two series of models: Llama 3 and Qwen 2. Our evaluation across diverse benchmarks shows that <tex-math>SSO</tex-math> consistently outperforms baselines in human preference alignment and reward optimization. Further analysis validates <tex-math>SSO</tex-math> as a scalable framework for preference optimization, benefiting the advancement in automated alignment techniques.</abstract>
      <url hash="1639c281">2025.findings-acl.473</url>
      <bibkey>xiang-etal-2025-self</bibkey>
    </paper>
    <paper id="474">
      <title><fixed-case>LIME</fixed-case>: Less Is More for <fixed-case>MLLM</fixed-case> Evaluation</title>
      <author><first>King</first><last>Zhu</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Qianbo</first><last>Zang</last></author>
      <author><first>Shian</first><last>Jia</last></author>
      <author><first>Siwei</first><last>Wu</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Shuyue</first><last>Guo</last></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Jiawei</first><last>Guo</last><affiliation>01.AI</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Haoning</first><last>Wu</last><affiliation>Rhymes AI</affiliation></author>
      <author><first>Xingwei</first><last>Qu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>9086-9121</pages>
      <abstract>Multimodal Large Language Models (MLLMs) are measured on numerous benchmarks like image captioning, visual question answer, and reasoning. However, these benchmarks often include overly simple or uninformative samples, making it difficult to effectively distinguish the performance of different MLLMs. Additionally, evaluating models across many benchmarks creates a significant computational burden. To address these issues, we propose LIME (Less Is More for MLLM Evaluation), a refined and efficient benchmark curated using a semi-automated pipeline. This pipeline filters out uninformative samples and eliminates answer leakage by focusing on tasks that require image-based understanding. Our experiments show that LIME reduces the number of samples by 76% and evaluation time by 77%, while it can more effectively distinguish different models’ abilities. Notably, we find that traditional automatic metrics like CIDEr are insufficient for evaluating MLLMs’ captioning performance, and excluding the caption task score yields a more accurate reflection of overall model performance. All code and data are available at https://anonymous.4open.science/r/LIME-49CD</abstract>
      <url hash="1db5f132">2025.findings-acl.474</url>
      <bibkey>zhu-etal-2025-lime</bibkey>
    </paper>
    <paper id="475">
      <title>Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement</title>
      <author><first>Xiaofeng</first><last>Zhou</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <pages>9122-9137</pages>
      <abstract>Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques—such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection—struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&amp;R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.</abstract>
      <url hash="7e32b135">2025.findings-acl.475</url>
      <bibkey>zhou-etal-2025-debate</bibkey>
    </paper>
    <paper id="476">
      <title><fixed-case>C</fixed-case>ode<fixed-case>R</fixed-case>eview<fixed-case>QA</fixed-case>: The Code Review Comprehension Assessment for Large Language Models</title>
      <author><first>Hong Yi</first><last>Lin</last></author>
      <author><first>Chunhua</first><last>Liu</last></author>
      <author><first>Haoyu</first><last>Gao</last></author>
      <author><first>Patanamon</first><last>Thongtanunam</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Christoph</first><last>Treude</last><affiliation>Singapore Management University</affiliation></author>
      <pages>9138-9166</pages>
      <abstract>State-of-the-art large language models (LLMs) have demonstrated impressive code generation capabilities but struggle with real-world software engineering tasks, such as revising source code to address code reviews, hindering their practical use. Code review comments are often implicit, ambiguous, and colloquial, requiring models to grasp both code and human intent. This challenge calls for evaluating large language models’ ability to bridge both technical and conversational contexts. While existing work has employed the automated code refinement (ACR) task to resolve these comments, current evaluation methods fall short, relying on text matching metrics that provide limited insight into model failures and remain susceptible to training data contamination.To address these limitations, we introduce a novel evaluation benchmark, <tex-math>\textbf{CodeReviewQA}</tex-math> that enables us to conduct fine-grained assessment of model capabilities and mitigate data contamination risks.In CodeReviewQA, we decompose the generation task of code refinement into <tex-math>\textbf{three essential reasoning steps}</tex-math>: <tex-math>\textit{change type recognition}</tex-math> (CTR), <tex-math>\textit{change localisation}</tex-math> (CL), and <tex-math>\textit{solution identification}</tex-math> (SI). Each step is reformulated as multiple-choice questions with varied difficulty levels, enabling precise assessment of model capabilities, while mitigating data contamination risks. Our comprehensive evaluation spans 72 recently released large language models on <tex-math>\textbf{900 manually curated, high-quality examples}</tex-math> across nine programming languages. Our results show that CodeReviewQA is able to expose specific model weaknesses in code review comprehension, disentangled from their generative automated code refinement results.</abstract>
      <url hash="1e9cf4cb">2025.findings-acl.476</url>
      <bibkey>lin-etal-2025-codereviewqa</bibkey>
    </paper>
    <paper id="477">
      <title>Narrative Media Framing in Political Discourse</title>
      <author><first>Yulia</first><last>Otmakhova</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <pages>9167-9196</pages>
      <abstract>Narrative frames are a powerful way of conceptualizing and communicating complex, controversial ideas, however automated frame analysis to date has mostly overlooked this framing device. In this paper, we connect elements of narrativity with fundamental aspects of framing, and present a framework which formalizes and operationalizes such aspects. We annotate and release a data set of news articles in the climate change domain, analyze the dominance of narrative frame components across political leanings, and test LLMs in their ability to predict narrative frames and their components. Finally, we apply our framework in an unsupervised way to elicit components of narrative framing in a second domain, the COVID-19 crisis, where our predictions are congruent with prior theoretical work showing the generalizability of our approach.</abstract>
      <url hash="b9467b28">2025.findings-acl.477</url>
      <bibkey>otmakhova-frermann-2025-narrative</bibkey>
    </paper>
    <paper id="478">
      <title><fixed-case>MHALO</fixed-case>: Evaluating <fixed-case>MLLM</fixed-case>s as Fine-grained Hallucination Detectors</title>
      <author><first>Yishuo</first><last>Cai</last></author>
      <author><first>Renjie</first><last>Gu</last></author>
      <author><first>Jiaxu</first><last>Li</last></author>
      <author><first>Xuancheng</first><last>Huang</last><affiliation>Zhipu.AI</affiliation></author>
      <author><first>Junzhe</first><last>Chen</last></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>9197-9222</pages>
      <abstract>Hallucination remains a critical challenge for multimodal large language models (MLLMs), undermining their reliability in real-world applications. While fine-grained hallucination detection (FHD) holds promise for enhancing high-quality vision-language data construction and model alignment through enriched feedback signals, automated solutions for this task have yet to be systematically explored. Inspired by the concept of “MLLM as a Judge”, we introduce MHALO, the first comprehensive benchmark specifically designed for evaluating MLLMs’ capability in performing token-level FHD. Our benchmark encompasses 12 distinct hallucination types spanning both multimodal perception and reasoning domains. Through extensive evaluations of 9 selected MLLMs, we reveal substantial performance limitations, with the leading model achieving an average <tex-math>F1_{IoU}</tex-math> of only 40.59%. To address this limitation, we develop HaloDet-4B, a specialized model trained on our curated training data, which significantly outperforms existing models. We hope the benchmark can provide valuable insights for future research on hallucination mitigation in MLLMs. The code and dataset will be publicly available.</abstract>
      <url hash="57e9b31b">2025.findings-acl.478</url>
      <bibkey>cai-etal-2025-mhalo</bibkey>
    </paper>
    <paper id="479">
      <title>Semantic Topology: a New Perspective for Communication Style Characterization</title>
      <author><first>Barbara</first><last>Scalvini</last><affiliation>University of the Faroe Islands</affiliation></author>
      <author><first>Alireza</first><last>Mashaghi</last><affiliation>Leiden University</affiliation></author>
      <pages>9223-9233</pages>
      <abstract>We introduce semantic topology, a novel framework for discourse analysis that leverages Circuit Topology to quantify the semantic arrangement of sentences in a text. By mapping recurring themes as series, parallel, or cross relationships, we identify statistical differences in communication patterns in long-form true and fake news. Our analysis of large-scale news datasets reveals that true news are more likely to exhibit more complex topological structures, with greater thematic interleaving and long-range coherence, whereas fake news favor simpler, more linear narratives. These findings suggest that topological features capture stylistic distinctions beyond traditional linguistic cues, offering new insights for discourse modeling.</abstract>
      <url hash="4f1caa4e">2025.findings-acl.479</url>
      <bibkey>scalvini-mashaghi-2025-semantic</bibkey>
    </paper>
    <paper id="480">
      <title>Decoding <fixed-case>LLM</fixed-case> Personality Measurement: Forced-Choice vs. <fixed-case>L</fixed-case>ikert</title>
      <author><first>Xiaoyu</first><last>Li</last></author>
      <author><first>Haoran</first><last>Shi</last></author>
      <author><first>Zengyi</first><last>Yu</last><affiliation>Zhejiang University of Technology and East China Normal University</affiliation></author>
      <author><first>Yukun</first><last>Tu</last></author>
      <author><first>Chanjin</first><last>Zheng</last><affiliation>East China Normal University</affiliation></author>
      <pages>9234-9247</pages>
      <abstract>Recent research has focused on investigating the psychological characteristics of Large Language Models (LLMs), emphasizing the importance of comprehending their behavioral traits. Likert scale personality questionnaires have become the primary tool for assessing these characteristics in LLMs. However, such scales can be skewed by factors such as social desirability, distorting the assessment of true personality traits. To address this issue, we firstly incorporate the forced-choice test, a method known for reducing response bias in human personality assessments, into the evaluation of LLM. Specifically, we evaluated six LLMs: Llama-3.1-8B, GLM-4-9B, GPT-3.5-turbo, GPT-4o, Claude-3.5-sonnet, and Deepseek-V3. We compared the Likert scale and forced-choice test results for LLMs’ Big Five personality scores, as well as their reliability. In addition, we looked at how temperature parameter and language affected LLM personality scores. The results show that the forced-choice test better captures differences between LLMs across various personality dimensions and is less influenced by temperature parameters. Furthermore, we found both broad trends and specific variations in personality scores across models and languages.</abstract>
      <url hash="5ba82f27">2025.findings-acl.480</url>
      <bibkey>li-etal-2025-decoding-llm</bibkey>
    </paper>
    <paper id="481">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>MSD</fixed-case>: A Corpus for Multilingual Medical Text Simplification from Online Medical References</title>
      <author><first>Koki</first><last>Horiguchi</last><affiliation>Ehime University</affiliation></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last><affiliation>Ehime University</affiliation></author>
      <author><first>Takashi</first><last>Ninomiya</last><affiliation>Ehime University</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>9248-9258</pages>
      <abstract>We release a parallel corpus for medical text simplification, which paraphrases medical terms into expressions easily understood by patients. Medical texts written by medical practitioners contain a lot of technical terms, and patients who are non-experts are often unable to use the information effectively. Therefore, there is a strong social demand for medical text simplification that paraphrases input sentences without using medical terms. However, this task has not been sufficiently studied in non-English languages. We therefore developed parallel corpora for medical text simplification in nine languages: German, English, Spanish, French, Italian, Japanese, Portuguese, Russian, and Chinese, each with 10,000 sentence pairs, by automatic sentence alignment to online medical references for professionals and consumers. We also propose a method for training text simplification models to actively paraphrase complex expressions, including medical terms. Experimental results show that the proposed method improves the performance of medical text simplification. In addition, we confirmed that training with a multilingual dataset is more effective than training with a monolingual dataset.</abstract>
      <url hash="a24fb8b5">2025.findings-acl.481</url>
      <bibkey>horiguchi-etal-2025-multimsd</bibkey>
    </paper>
    <paper id="482">
      <title><fixed-case>B</fixed-case>ad<fixed-case>W</fixed-case>indtunnel: Defending Backdoor in High-noise Simulated Training with Confidence Variance</title>
      <author><first>Ruyi</first><last>Zhang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Songlei</first><last>Jian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yusong</first><last>Tan</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Heng</first><last>Gao</last></author>
      <author><first>Haifang</first><last>Zhou</last></author>
      <author><first>Kai</first><last>Lu</last></author>
      <pages>9259-9273</pages>
      <abstract>Current backdoor attack defenders in Natural Language Processing (NLP) typically involve data reduction or model pruning, risking losing crucial information. To address this challenge, we introduce a novel backdoor defender, i.e., BadWindtunnel, in which we build a high-noise simulated training environment, similar to the wind tunnel, which allows precise control over training conditions to model the backdoor learning behavior without affecting the final model. We also use the confidence variance as a learning behavior quantification metric in the simulated training, which is based on the characteristics of backdoor-poisoned data (shorted in poisoned data): higher learnability and robustness. In addition, we propose a two-step strategy to further model poisoned data, including target label identification and poisoned data revealing. Extensive experiments demonstrate BadWindtunnel’s superiority, with a 21% higher average reduction in attack success rate than the second-best defender.</abstract>
      <url hash="226c1ce8">2025.findings-acl.482</url>
      <bibkey>zhang-etal-2025-badwindtunnel</bibkey>
    </paper>
    <paper id="483">
      <title>Multimodal Machine Translation with Text-Image In-depth Questioning</title>
      <author><first>Yue</first><last>Gao</last></author>
      <author><first>Jing</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Shiliang</first><last>Sun</last></author>
      <author><first>Xiaosong</first><last>Qiao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Tengfei</first><last>Song</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>9274-9287</pages>
      <abstract>Multimodal machine translation (MMT) integrates visual information to address ambiguity and contextual limitations in neural machine translation (NMT). Some empirical studies have revealed that many MMT models underutilize visual data during translation. They attempt to enhance cross-modal interactions to enable better exploitation of visual data. However, they only focus on simple interactions between nouns in text and corresponding entities in image, overlooking global semantic alignment, particularly for prepositional phrases and verbs in text which are more likely to be translated incorrectly. To address this, we design a Text-Image In-depth Questioning method to deepen interactions and optimize translations. Furthermore, to mitigate errors arising from contextually irrelevant image noise, we propose a Consistency Constraint strategy to improve our approach’s robustness. Our approach achieves state-of-the-art results on five translation directions of Multi30K and AmbigCaps, with +2.35 BLEU on the challenging MSCOCO benchmark, validating our method’s effectiveness in utilizing visual data and capturing comprehensive textual semantics.</abstract>
      <url hash="b2603ed4">2025.findings-acl.483</url>
      <bibkey>gao-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="484">
      <title><fixed-case>R</fixed-case>e<fixed-case>KG</fixed-case>-<fixed-case>MCTS</fixed-case>: Reinforcing <fixed-case>LLM</fixed-case> Reasoning on Knowledge Graphs via Training-Free <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search</title>
      <author><first>Xiaozhuang</first><last>Song</last><affiliation>Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Shufei</first><last>Zhang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Tianshu</first><last>Yu</last><affiliation>Chinese University of Hong Kong (Shenzhen)</affiliation></author>
      <pages>9288-9306</pages>
      <abstract>Recent advancements in combining knowledge graphs (KGs) with large language models (LLMs) have demonstrated promising potential in complex KG reasoning tasks, yet existing approaches face limitations in path exploration strategies or excessive computational overhead. We propose ReKG-MCTS, a novel training-free framework that synergizes Monte Carlo Tree Search (MCTS) with LLM capabilities to enable dynamic reasoning over KGs. The framework conceptualizes KG reasoning as a decision-making process, where MCTS strategically explores paths over KG while LLMs provide semantic guidance for reasoning paths. The framework consists of four phases: (1) UCB-based node selection that balances exploration-exploitation on KG, (2) path expansion with KG structural constraints, (3) LLM-guided MC rollouts for simulation, and (4) value backpropagation. Experimental results on WebQSP and CWQ demonstrate that ReKG-MCTS outperforms existing training-free methods and achieves competitive performance compared to fine-tuned baselines. These findings suggest a new paradigm for leveraging language models in KG reasoning tasks. The code is available at https://github.com/ShawnKS/rekgmcts.</abstract>
      <url hash="3a76a6c7">2025.findings-acl.484</url>
      <bibkey>song-etal-2025-rekg</bibkey>
    </paper>
    <paper id="485">
      <title><fixed-case>HTML</fixed-case>: Hierarchical Topology Multi-task Learning for Semantic Parsing in Knowledge Base Question Answering</title>
      <author><first>Aziguli</first><last>Wulamu</last></author>
      <author><first>Lyu</first><last>Zhengyu</last></author>
      <author><first>Kaiyuan</first><last>Gong</last></author>
      <author><first>Yu</first><last>Han</last></author>
      <author><first>Zewen</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Bowen</first><last>Xing</last><affiliation>University of Science and Technology Beijing</affiliation></author>
      <pages>9307-9321</pages>
      <abstract>Knowledge base question answering (KBQA) aims to answer natural language questions by reasoning over structured knowledge bases. Existing approaches often struggle with the complexity of mapping questions to precise logical forms, particularly when dealing with diverse entities and relations. In this paper, we propose Hierarchical Topology Multi-task Learning (HTML), a novel framework that leverages a hierarchical multi-task learning paradigm to enhance the performance of logical form generation. Our framework consists of a main task: generating logical forms from questions, and three auxiliary tasks: entity prediction from the input question, relation prediction for the given entities, and logical form generation based on the given entities and relations. Through joint instruction-tuning, HTML allows mutual guidance and knowledge transfer among the hierarchical tasks, capturing the subtle dependencies between entities, relations, and logical forms. Extensive experiments on public benchmarks show that HTML markedly outperforms both supervised fine-tuning methods and training-free ones based on powerful large language models (e.g., GPT-4), demonstrating its superiority in question understanding and structural knowledge reasoning.</abstract>
      <url hash="de2e43e5">2025.findings-acl.485</url>
      <bibkey>wulamu-etal-2025-html</bibkey>
    </paper>
    <paper id="486">
      <title><fixed-case>S</fixed-case>truct<fixed-case>F</fixed-case>low<fixed-case>B</fixed-case>ench: A Structured Flow Benchmark for Multi-turn Instruction Following</title>
      <author><first>Jinnan</first><last>Li</last></author>
      <author><first>Jinzhe</first><last>Li</last></author>
      <author><first>Yue</first><last>Wang</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Yi</first><last>Chang</last><affiliation>Jilin University, China</affiliation></author>
      <author><first>Yuan</first><last>Wu</last><affiliation>Jilin University</affiliation></author>
      <pages>9322-9341</pages>
      <abstract>Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependencies between dialogue turns that distinguish multi-turn from single-turn interactions. These structural dependencies not only reflect user intent but also establish an essential second dimension for the instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark defines an innovative structural flow framework with six fundamental inter-turn relationships. These relationships introduce novel structural constraints for model evaluation and also serve as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models’ comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.</abstract>
      <url hash="c5d8a4aa">2025.findings-acl.486</url>
      <bibkey>li-etal-2025-structflowbench</bibkey>
    </paper>
    <paper id="487">
      <title><fixed-case>CMIE</fixed-case>: Combining <fixed-case>MLLM</fixed-case> Insights with External Evidence for Explainable Out-of-Context Misinformation Detection</title>
      <author><first>Fanxiao</first><last>Li</last></author>
      <author><first>Jiaying</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Canyuan</first><last>He</last></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Yunnan University</affiliation></author>
      <pages>9342-9354</pages>
      <abstract>Multimodal large language models (MLLMs) have demonstrated impressive capabilities in visual reasoning and text generation. While previous studies have explored the application of MLLM for detecting out-of-context (OOC) misinformation, our empirical analysis reveals two persisting challenges of this paradigm. Evaluating the representative GPT-4o model on direct reasoning and evidence augmented reasoning, results indicate that MLLM struggle to capture the deeper relationships—specifically, cases in which the image and text are not directly connected but are associated through underlying semantic links. Moreover, noise in the evidence further impairs detection accuracy.To address these challenges, we propose CMIE, a novel OOC misinformation detection framework that incorporates a Coexistence Relationship Generation (CRG) strategy and an Association Scoring (AS) mechanism. CMIE identifies the underlying coexistence relationships between images and text, and selectively utilizes relevant evidence to enhance misinformation detection. Experimental results demonstrate that our approach outperforms existing methods.</abstract>
      <url hash="6dd7bf56">2025.findings-acl.487</url>
      <bibkey>li-etal-2025-cmie</bibkey>
    </paper>
    <paper id="488">
      <title><fixed-case>E</fixed-case>ti<fixed-case>C</fixed-case>or++: Towards Understanding Etiquettical Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ashutosh</first><last>Dwivedi</last></author>
      <author><first>Siddhant Shivdutt</first><last>Singh</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>9355-9376</pages>
      <abstract>In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.</abstract>
      <url hash="69a3bea6">2025.findings-acl.488</url>
      <bibkey>dwivedi-etal-2025-eticor</bibkey>
    </paper>
    <paper id="489">
      <title><fixed-case>F</fixed-case>in<fixed-case>R</fixed-case>ipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness</title>
      <author><first>Yuanjian</first><last>Xu</last></author>
      <author><first>Jianing</first><last>Hao</last></author>
      <author><first>Kunsheng</first><last>Tang</last></author>
      <author><first>Jingnan</first><last>Chen</last></author>
      <author><first>Anxian</first><last>Liu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Peng</first><last>Liu</last></author>
      <author><first>Guang</first><last>Zhang</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>9377-9398</pages>
      <abstract>Financial markets exhibit complex dynamics where localized events trigger ripple effects across entities. Previous event studies, constrained by static single-companies analyses and simplistic assumptions, fail to capture these ripple effects. While large language models (LLMs) offer emergent reasoning capabilities, their direct application falters due to structural market unawareness and limited capacity to analyze ripple effects. We propose FinRipple, an elegant framework that empowers LLMs with the ability to analyze ripple effects through financial theory-guided large-scale reinforcement learning. We begin by relaxing the assumptions of previous methods, incorporating a time-varying knowledge graph to accurately represent market structure. By seamlessly integrating classical asset pricing theory, we align the LLM with the market, enabling it to predict ripple effects. To the best of our knowledge, we are the first to provide a standardized definition of ripple effect prediction, a task that is extremely important yet unexplored in the financial domain. Extensive experiments demonstrate that FinRipple provides a promising solution to this task.</abstract>
      <url hash="0eaec18c">2025.findings-acl.489</url>
      <bibkey>xu-etal-2025-finripple</bibkey>
    </paper>
    <paper id="490">
      <title>Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation</title>
      <author><first>Yingfeng</first><last>Luo</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Zheng</last></author>
      <author><first>Yongyu</first><last>Mu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Bei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Qinghong</first><last>Zhang</last></author>
      <author><first>Yongqi</first><last>Gao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Ziqiang</first><last>Xu</last></author>
      <author><first>Peinan</first><last>Feng</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaoqian</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>9399-9431</pages>
      <abstract>The field of neural machine translation (NMT) has changed with the advent of large language models (LLMs). Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained Transformer decoder, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve <tex-math>2.4 \sim 6.5 \times</tex-math> inference speedups and a 75% reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.</abstract>
      <url hash="cd2d2e40">2025.findings-acl.490</url>
      <bibkey>luo-etal-2025-beyond</bibkey>
    </paper>
    <paper id="491">
      <title><fixed-case>EC</fixed-case>-<fixed-case>RAFT</fixed-case>: Automated Generation of Clinical Trial Eligibility Criteria through Retrieval-Augmented Fine-Tuning</title>
      <author><first>Nopporn</first><last>Lekuthai</last><affiliation>Mahidol University and Mahidol University</affiliation></author>
      <author><first>Nattawit</first><last>Pewngam</last><affiliation>Ravis Technology</affiliation></author>
      <author><first>Supitcha</first><last>Sokrai</last><affiliation>Ravis Technology</affiliation></author>
      <author><first>Titipat</first><last>Achakulvisut</last><affiliation>Mahidol University</affiliation></author>
      <pages>9432-9444</pages>
      <abstract>Eligibility criteria (EC) are critical components of clinical trial design, defining the parameters for participant inclusion and exclusion. However, designing EC remains a complex, expertise-intensive process. Traditional approaches to EC generation may fail to produce comprehensive, contextually appropriate criteria. To address these challenges, we introduce EC-RAFT, a method that utilizes Retrieval-Augmented Fine-Tuning (RAFT) to generate structured and cohesive EC directly from clinical trial titles and descriptions. EC-RAFT integrates contextual retrieval, synthesized intermediate reasoning, and fine-tuned language models to produce comprehensive EC sets. To enhance clinical alignment evaluation with referenced criteria, we also propose an LLM-guided evaluation pipeline. Our results demonstrate that our solution, which uses Llama-3.1-8B-Instruct as a base model, achieves a BERTScore of 86.23 and an EC-matched LLM-as-a-Judge score of 1.66 out of 3, outperforming zero-shot Llama-3.1 and Gemini-1.5 by 0.41 and 0.11 points, respectively. On top of that, EC-RAFT also outperforms other fine-tuned versions of Llama-3.1. EC-RAFT was trained in a low-cost setup and, therefore, can be used as a practical solution for EC generation while ensuring quality and relevance in clinical trial design. We release our code on GitHub at https://github.com/biodatlab/ec-raft/</abstract>
      <url hash="5f5ffafe">2025.findings-acl.491</url>
      <bibkey>lekuthai-etal-2025-ec</bibkey>
    </paper>
    <paper id="492">
      <title>Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models</title>
      <author><first>Elena</first><last>Stringli</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Maria</first><last>Lymperaiou</last></author>
      <author><first>Giorgos</first><last>Filandrianos</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Athanasios</first><last>Voulodimos</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Giorgos</first><last>Stamou</last><affiliation>National Technical University of Athens</affiliation></author>
      <pages>9445-9469</pages>
      <abstract>Inverse tasks can uncover potential reasoning gaps as Large Language Models (LLMs) scale up. In this work, we explore the redefinition task, in which we assign alternative values to well-known physical constants and units of measure, prompting LLMs to respond accordingly. Our findings show that not only does model performance degrade with scale, but its false confidence also rises. Moreover, while factors such as prompting strategies or response formatting are influential, they do not preclude LLMs from anchoring to memorized values.</abstract>
      <url hash="f92ea4ad">2025.findings-acl.492</url>
      <bibkey>stringli-etal-2025-pitfalls</bibkey>
    </paper>
    <paper id="493">
      <title>Implicit Reasoning in Transformers is Reasoning through Shortcuts</title>
      <author><first>Tianhe</first><last>Lin</last></author>
      <author><first>Jian</first><last>Xie</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>9470-9487</pages>
      <abstract>Test-time compute is emerging as a new paradigm for enhancing language models’ complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI’s o1 and o3, as well as DeepSeek’s R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization. Resources are available at https://github.com/TianheL/LM-Implicit-Reasoning.</abstract>
      <url hash="6ae1f23b">2025.findings-acl.493</url>
      <bibkey>lin-etal-2025-implicit</bibkey>
    </paper>
    <paper id="494">
      <title>Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework</title>
      <author><first>Kaishuai</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tiezheng</first><last>Yu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Wenjun</first><last>Hou</last></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>9488-9502</pages>
      <abstract>Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.</abstract>
      <url hash="d2b1d887">2025.findings-acl.494</url>
      <bibkey>xu-etal-2025-learning</bibkey>
    </paper>
    <paper id="495">
      <title><fixed-case>C</fixed-case>ortex<fixed-case>D</fixed-case>ebate: Debating Sparsely and Equally for Multi-Agent Debate</title>
      <author><first>Yiliu</first><last>Sun</last></author>
      <author><first>Zicheng</first><last>Zhao</last></author>
      <author><first>Sheng</first><last>Wan</last></author>
      <author><first>Chen</first><last>Gong</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>9503-9523</pages>
      <abstract>Nowadays, single Large Language Model (LLM) struggles with critical issues such as hallucination and inadequate reasoning abilities. To mitigate these issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where LLM agents engage in in-depth debates with others on tasks. However, existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness. To address these limitations, we propose a novel MAD method called ”CortexDebate”. Inspired by the human brain’s tendency to establish a sparse and dynamically optimized network among cortical areas governed by white matter, CortexDebate constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization. The effectiveness of our CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.</abstract>
      <url hash="5d230389">2025.findings-acl.495</url>
      <bibkey>sun-etal-2025-cortexdebate</bibkey>
    </paper>
    <paper id="496">
      <title><fixed-case>PAP</fixed-case>2<fixed-case>PAT</fixed-case>: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs</title>
      <author><first>Valentin</first><last>Knappich</last></author>
      <author><first>Anna</first><last>Hätty</last><affiliation>Bosch</affiliation></author>
      <author><first>Simon</first><last>Razniewski</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>9524-9554</pages>
      <abstract>Dealing with long and highly complex technical text is a challenge for Large Language Models (LLMs), which still have to unfold their potential in supporting expensive and time intensive processes like patent drafting. Within patents, the description constitutes more than 90% of the document on average. Yet, its automatic generation remains understudied. When drafting patent applications, patent attorneys typically receive invention reports (IRs), which are usually confidential, hindering research on LLM-supported patent drafting.Often, pre-publication research papers serve as IRs. We leverage this duality to build PAP2PAT, an open and realistic benchmark for patent drafting consisting of 1.8k patent-paper pairs describing the same inventions. To address the complex long-document patent generation task, we propose chunk-based outline-guided generation using the research paper as invention specification. Our extensive evaluation using PAP2PAT and a human case study show that LLMs can effectively leverage information from the paper, but still struggle to provide the necessary level of detail. Fine-tuning leads to more patent-style language, but also to more hallucination. We release our data and code.</abstract>
      <url hash="00923409">2025.findings-acl.496</url>
      <bibkey>knappich-etal-2025-pap2pat</bibkey>
    </paper>
    <paper id="497">
      <title>Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent</title>
      <author><first>Xiaofeng</first><last>Wang</last></author>
      <author><first>Zhixin</first><last>Zhang</last></author>
      <author><first>Jin Guang</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yiming</first><last>Ai</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>9555-9577</pages>
      <abstract>Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.</abstract>
      <url hash="2d299b5d">2025.findings-acl.497</url>
      <bibkey>wang-etal-2025-debt</bibkey>
    </paper>
    <paper id="498">
      <title>Focused-<fixed-case>DPO</fixed-case>: Enhancing Code Generation Through Focused Preference Optimization on Error-Prone Points</title>
      <author><first>Kechi</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yihong</first><last>Dong</last></author>
      <author><first>Jia</first><last>Li</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <pages>9578-9591</pages>
      <abstract>Code generation models have shown significant potential for automating programming tasks. However, the challenge of generating accurate and reliable code persists due to the highly complex and long-reasoning nature of the task. Even state-of-the-art models often fail in code generation due to small errors, which can drastically affect the overall functionality of code. Our study identifies that current models tend to produce errors concentrated at specific error-prone points, which significantly impacts the accuracy of the generated code. To address this issue, we introduce Focused-DPO, a framework that enhances code generation by directing preference optimization towards these critical error-prone areas. This approach builds on Direct Preference Optimization, emphasizing accuracy in parts prone to errors. Additionally, we develop a method called Error-Point Identification, which constructs a dataset that targets these problematic points without requiring costly human annotations. Our experiments on benchmarks such as HumanEval(+), MBPP(+), and LiveCodeBench demonstrate that Focused-DPO significantly improves the precision and reliability of code generation, reducing common errors and enhancing overall code quality. By focusing on error-prone points, Focused-DPO advances the accuracy and functionality of model-generated code.</abstract>
      <url hash="519f19e5">2025.findings-acl.498</url>
      <bibkey>zhang-etal-2025-focused</bibkey>
    </paper>
    <paper id="499">
      <title>Supervised and Unsupervised Probing of Shortcut Learning: Case Study on the Emergence and Evolution of Syntactic Heuristics in <fixed-case>BERT</fixed-case></title>
      <author><first>Elke</first><last>Vandermeerschen</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <pages>9592-9604</pages>
      <abstract>Contemporary language models (LMs) such as BERT (Devlin et al., 2019, T5 (Raffel et al., 2023), GPT-4 (OpenAI, 2023), have exhibited remarkable capabilities, effectively addressing long-standing challenges in the field. However, these models rely on shortcut learning, using a decision rule that relies on superficial cues that are spuriously correlated with the labels (Geirhos et al., 2020). In this research, we focus on the reliance on a specific type of shortcuts, namely syntactic heuristics, in BERT when performing Natural Language Inference (NLI), a representative task in Natural Language Understanding (Jeretic et al., 2020). By making use of two probing methods, one supervised, one unsupervised, we investigate where these shortcuts emerge, how they evolve and how they impact the latent knowledge of the LM. Our findings reveal that syntactic heuristics are absent in pretrained models but emerge and evolve as the model is finetuned with datasets of increasing size. The adoption of these shortcuts varies across different hidden layers, with specific layers closer to the output contributing more to this phenomenon. Despite the model’s reliance on shortcuts during inference, it retains information relevant to the task, and our supervised and unsupervised probes process this information differently.</abstract>
      <url hash="c5379681">2025.findings-acl.499</url>
      <bibkey>vandermeerschen-de-lhoneux-2025-supervised</bibkey>
    </paper>
    <paper id="500">
      <title><fixed-case>GIMMICK</fixed-case>: Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking</title>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>9605-9668</pages>
      <abstract>Large Vision-Language Models (LVLMs) have recently gained attention due to their distinctive performance and broad applicability. While it has been previously shown that their efficacy in usage scenarios involving non-Western contexts falls short, existing studies are limited in scope, covering just a narrow range of cultures, focusing exclusively on a small number of cultural aspects, or evaluating a limited selection of models on a single task only. Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive multimodal benchmark designed to assess a broad spectrum of cultural knowledge across 144 countries representing six global macro-regions. GIMMICK comprises six tasks built upon three new datasets that span 728 unique cultural events or facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary and 26 open-weight models of all sizes. We systematically examine (1) regional cultural biases, (2) the influence of model size, (3) input modalities, and (4) external cues. Our analyses reveal strong biases toward Western cultures across models and tasks and highlight strong correlations between model size and performance, as well as the effectiveness of multimodal input and external geographic cues. We further find that models have more knowledge of tangible than intangible aspects (e.g., food vs. rituals) and that they excel in recognizing broad cultural origins but struggle with a more nuanced understanding.</abstract>
      <url hash="813ca861">2025.findings-acl.500</url>
      <bibkey>schneider-etal-2025-gimmick</bibkey>
    </paper>
    <paper id="501">
      <title><fixed-case>R</fixed-case>-<fixed-case>VLM</fixed-case>: Region-Aware Vision Language Model for Precise <fixed-case>GUI</fixed-case> Grounding</title>
      <author><first>Joonhyung</first><last>Park</last></author>
      <author><first>Peng</first><last>Tang</last><affiliation>Meta</affiliation></author>
      <author><first>Sagnik</first><last>Das</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Srikar</first><last>Appalaraju</last><affiliation>Amazon</affiliation></author>
      <author><first>Kunwar Yashraj</first><last>Singh</last><affiliation>Amazon</affiliation></author>
      <author><first>R.</first><last>Manmatha</last></author>
      <author><first>Shabnam</first><last>Ghadar</last><affiliation>Amazon</affiliation></author>
      <pages>9669-9685</pages>
      <abstract>Visual agent models for automating human activities on Graphical User Interfaces (GUIs) have emerged as a promising research direction, driven by advances in large Vision Language Models (VLMs). A critical challenge in GUI automation is the precise grounding of interface elements across diverse platforms. Existing vision-only GUI agents directly ground elements from large and cluttered screenshots, requiring them to process substantial irrelevant information that compromises their accuracy. In addition, these approaches typically employ basic cross-entropy loss for learning grounding objectives, which fails to effectively capture grounding quality compared to established object detection metrics like Intersection-over-Union (IoU). To address these issues, we introduce R-VLM, a novel GUI grounding approach that leverages zoomed-in region proposals for precise element localization. We also propose an IoU-aware objective function that facilitates model convergence toward high IoU predictions. Our approach bridges the gap between VLMs and conventional object detection techniques, improving the state-of-the-art grounding accuracy by 13% across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.</abstract>
      <url hash="09d920ab">2025.findings-acl.501</url>
      <bibkey>park-etal-2025-r</bibkey>
    </paper>
    <paper id="502">
      <title>Perspective Transition of Large Language Models for Solving Subjective Tasks</title>
      <author><first>Xiaolong</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuanchi</first><last>Zhang</last></author>
      <author><first>Ziyue</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuzhuang</first><last>Xu</last></author>
      <author><first>Fuwen</first><last>Luo</last></author>
      <author><first>Yile</first><last>Wang</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>9686-9704</pages>
      <abstract>Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.</abstract>
      <url hash="0cd297d9">2025.findings-acl.502</url>
      <bibkey>wang-etal-2025-perspective</bibkey>
    </paper>
    <paper id="503">
      <title><fixed-case>T</fixed-case>rip<fixed-case>T</fixed-case>ailor: A Real-World Benchmark for Personalized Travel Planning</title>
      <author><first>Kaimin</first><last>Wang</last></author>
      <author><first>Yuanzhe</first><last>Shen</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>9705-9723</pages>
      <abstract>The continuous evolution and enhanced reasoning capabilities of large language models (LLMs) have elevated their role in complex tasks, notably in travel planning, where demand for personalized, high-quality itineraries is rising. However, current benchmarks often rely on unrealistic simulated data, failing to reflect the differences between LLM-generated and real-world itineraries. Existing evaluation metrics, which primarily emphasize constraints, fall short of providing a comprehensive assessment of the overall quality of travel plans. To address these limitations, we introduce TripTailor, a benchmark designed specifically for personalized travel planning in real-world scenarios. This dataset features an extensive collection of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries, complete with detailed information, providing a more authentic evaluation framework. Experiments show that fewer than 10% of the itineraries generated by the latest state-of-the-art LLMs achieve human-level performance. Moreover, we identify several critical challenges in travel planning, including the feasibility, rationality, and personalized customization of the proposed solutions. We hope that TripTailor will drive the development of travel planning agents capable of understanding and meeting user needs while generating practical itineraries.</abstract>
      <url hash="88641d11">2025.findings-acl.503</url>
      <bibkey>wang-etal-2025-triptailor</bibkey>
    </paper>
    <paper id="504">
      <title>Random Splitting Negatively Impacts <fixed-case>NER</fixed-case> Evaluation: Quantifying and Eliminating the Overestimation of <fixed-case>NER</fixed-case> Performance</title>
      <author><first>Florian</first><last>Babl</last></author>
      <author><first>Moritz</first><last>Hennen</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Jakob</first><last>Murauer</last></author>
      <author><first>Michaela</first><last>Geierhos</last><affiliation>Universität der Bundeswehr München</affiliation></author>
      <pages>9724-9738</pages>
      <abstract>In named entity recognition (NER), models are evaluated on their ability to identify entity mentions in text. However, standard evaluation methods often rely on test sets that contain named entities already present in the training data, raising concerns about overestimation of model performance.This work investigates the impact of varying degrees of entity contamination on a dataset level on the generalization ability and reported F1 scores of three state-of-the-art NER models.Experiments on five standard benchmarks show that F1 scores for contaminated entities statistically significantly inflate reported F1 scores as contamination rates increase, with F1 performance gaps ranging from 2-10% compared to entities not seen during training.To address these inflated F1 scores, we additionally propose a novel NER dataset splitting method using a minimum cut algorithm to minimize train-test entity leakage.While our splitting method ensures near-zero entity contamination, we also compare new and existing dataset splits on named entity sample counts.</abstract>
      <url hash="47babbe9">2025.findings-acl.504</url>
      <bibkey>babl-etal-2025-random</bibkey>
    </paper>
    <paper id="505">
      <title>Structure-adaptive Adversarial Contrastive Learning for Multi-Domain Fake News Detection</title>
      <author><first>Lingwei</first><last>Wei</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Dou</first><last>Hu</last></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Institute of Information Engeering</affiliation></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>9739-9752</pages>
      <abstract>The rapid proliferation of fake news across multiple domains poses significant threats to society. Existing multi-domain detection models typically capture domain-shared semantic features to achieve generalized detection. However, they often fail to generalize well due to poor adaptability, which limits their ability to provide complementary features for detection, especially in data-constrained conditions. To address these challenges, we investigate the propagation-adaptive multi-domain fake news detection paradigm. We propose a novel framework, Structure-adaptive Adversarial Contrastive Learning (StruACL), to adaptively enable structure knowledge transfer between multiple domains. Specifically, we first contrast representations between content-only and propagation-rich data to preserve structural patterns in the shared representation space. Additionally, we design a propagation-guided adversarial training strategy to enhance the diversity of representations. Under the StruACL objective, we leverage a unified Transformer-based and graph-based model to jointly learn transferable semantic and structural features for detection across multiple domains. Experiments on seven fake news datasets demonstrate that StruACL-TGN achieves better multi-domain detection performance on general and data-constrained scenarios, showing the effectiveness and better generalization of StruACL.</abstract>
      <url hash="539a8657">2025.findings-acl.505</url>
      <bibkey>wei-etal-2025-structure</bibkey>
    </paper>
    <paper id="506">
      <title><fixed-case>B</fixed-case>ias<fixed-case>G</fixed-case>uard: A Reasoning-Enhanced Bias Detection Tool for Large Language Models</title>
      <author><first>Zhiting</first><last>Fan</last></author>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>9753-9764</pages>
      <abstract>Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.</abstract>
      <url hash="6cddaf36">2025.findings-acl.506</url>
      <bibkey>fan-etal-2025-biasguard</bibkey>
    </paper>
    <paper id="507">
      <title>Qorǵau: Evaluating Safety in <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian Bilingual Contexts</title>
      <author><first>Maiya</first><last>Goloburda</last></author>
      <author><first>Nurkhan</first><last>Laiyk</last></author>
      <author><first>Diana</first><last>Turmakhan</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Mukhammed</first><last>Togmanov</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Askhat</first><last>Sametov</last></author>
      <author><first>Nurdaulet</first><last>Mukhituly</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Monash University</affiliation></author>
      <author><first>Daniil</first><last>Orel</last></author>
      <author><first>Zain Muhammad</first><last>Mujahid</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>9765-9784</pages>
      <abstract>Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorǵau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased.</abstract>
      <url hash="482e70c1">2025.findings-acl.507</url>
      <bibkey>goloburda-etal-2025-qorgau</bibkey>
    </paper>
    <paper id="508">
      <title><fixed-case>MMXU</fixed-case>: A Multi-Modal and Multi-<fixed-case>X</fixed-case>-ray Understanding Dataset for Disease Progression</title>
      <author><first>Linjie</first><last>Mu</last></author>
      <author><first>Zhongzhen</first><last>Huang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shengqian</first><last>Qin</last></author>
      <author><first>Yakun</first><last>Zhu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>9785-9803</pages>
      <abstract>Large vision-language models (LVLMs) have shown great promise in medical applications, particularly in visual question answering (MedVQA) and diagnosis from medical images. However, existing datasets and models often fail to consider critical aspects of medical diagnostics, such as the integration of historical records and the analysis of disease progression over time. In this paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel dataset for MedVQA that focuses on identifying changes in specific regions between two patient visits. Unlike previous datasets that primarily address single-image questions, MMXU enables multi-image questions, incorporating both current and historical patient data. We demonstrate the limitations of current LVLMs in identifying disease progression on MMXU-<i>test</i>, even those that perform well on traditional benchmarks. To address this, we propose a MedRecord-Augmented Generation (MAG) approach, incorporating both global and regional historical records.Our experiments show that integrating historical records significantly enhances diagnostic accuracy by at least 20%, bridging the gap between current LVLMs and human expert performance. Additionally, we fine-tune models with MAG on MMXU-<i>dev</i>, which demonstrates notable improvements. We hope this work could illuminate the avenue of advancing the use of LVLMs in medical diagnostics by emphasizing the importance of historical context in interpreting medical images.Our dataset is released at github.</abstract>
      <url hash="f7cfe094">2025.findings-acl.508</url>
      <bibkey>mu-etal-2025-mmxu</bibkey>
    </paper>
    <paper id="509">
      <title>Tree-of-Code: A Self-Growing Tree Framework for End-to-End Code Generation and Execution in Complex Tasks</title>
      <author><first>Ziyi</first><last>Ni</last></author>
      <author><first>Yifan</first><last>Li</last></author>
      <author><first>Ning</first><last>Yang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Dou</first><last>Shen</last><affiliation>Baidu</affiliation></author>
      <author><first>Pin</first><last>Lyu</last></author>
      <author><first>Daxiang</first><last>Dong</last></author>
      <pages>9804-9819</pages>
      <abstract>Solving complex reasoning tasks is a key real-world application of agents. Thanks to the pretraining of Large Language Models (LLMs) on code data, recent approaches like CodeAct successfully use code as LLM agents’ action, achieving good results. However, CodeAct greedily generates the next action’s code block by relying on fragmented thoughts, resulting in inconsistency and accumulative hallucination. Moreover, CodeAct lacks action-related ground-truth (GT), making its supervision signals and termination conditions questionable in multi-turn interactions. To address these issues, we propose Tree-of-Code (ToC), a self-growing framework that generates nodes through self-supervision, incorporating prompt and model exploration in a GT-free setting. Each node employs CodeProgram, an end-to-end code generation paradigm that aligns executable code logic with global reasoning. This approach uses task-level execution success as both node validity and stop-growing flags, bypassing process supervision to enable online applications. Experiments on two datasets with ten popular zero-shot LLMs show that ToC boosts accuracy by nearly 20% over CodeAct with fewer than 1/4 turns. To further investigate the trade-off between efficacy and efficiency, ablation studies on different ToC tree sizes and exploration mechanisms validate ToC’s superiority.</abstract>
      <url hash="ee1e9586">2025.findings-acl.509</url>
      <bibkey>ni-etal-2025-tree</bibkey>
    </paper>
    <paper id="510">
      <title><fixed-case>A</fixed-case>kan Cinematic Emotions (<fixed-case>ACE</fixed-case>): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues</title>
      <author><first>David</first><last>Sasu</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Zehui</first><last>Wu</last></author>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Run</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Pengyuan</first><last>Shi</last><affiliation>Columbia University</affiliation></author>
      <author><first>Lin</first><last>Ai</last></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <author><first>Natalie</first><last>Schluter</last><affiliation>Technical University of Denmark, Apple and IT University</affiliation></author>
      <pages>9820-9831</pages>
      <abstract>In this paper, we introduce the Akan Cinematic Emotions (AkaCE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. AkaCE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of AkaCE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope AkaCE inspires further work on inclusive, linguistically and culturally diverse NLP resources.</abstract>
      <url hash="09f54e78">2025.findings-acl.510</url>
      <bibkey>sasu-etal-2025-akan</bibkey>
    </paper>
    <paper id="511">
      <title>A Cognitive Writing Perspective for Constrained Long-Form Text Generation</title>
      <author><first>Kaiyang</first><last>Wan</last></author>
      <author><first>Honglin</first><last>Mu</last><affiliation>Harbin Institute Of Technology</affiliation></author>
      <author><first>Rui</first><last>Hao</last></author>
      <author><first>Haoran</first><last>Luo</last></author>
      <author><first>Tianle</first><last>Gu</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>9832-9844</pages>
      <abstract>Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: https://anonymous.4open.science/r/CogWriter-8DFE.</abstract>
      <url hash="b85d62f1">2025.findings-acl.511</url>
      <bibkey>wan-etal-2025-cognitive</bibkey>
    </paper>
    <paper id="512">
      <title>Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models</title>
      <author><first>You</first><last>Li</last></author>
      <author><first>Heyu</first><last>Huang</last></author>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Kaiyu</first><last>Huang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Chao</first><last>Huang</last></author>
      <author><first>Zonghao</first><last>Guo</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yuhua</first><last>Li</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ruixuan</first><last>Li</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>9845-9867</pages>
      <abstract>The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 24.94% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced at https://migician-vg.github.io/.</abstract>
      <url hash="eb282006">2025.findings-acl.512</url>
      <bibkey>li-etal-2025-migician</bibkey>
    </paper>
    <paper id="513">
      <title><fixed-case>SIK</fixed-case>e<fixed-case>D</fixed-case>: Self-guided Iterative Knowledge Distillation for Mathematical Reasoning</title>
      <author><first>Shivam</first><last>Adarsh</last></author>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Caglar</first><last>Gulcehre</last><affiliation>Microsoft and EPFL - EPF Lausanne</affiliation></author>
      <author><first>Nicholas</first><last>Monath</last><affiliation>Meta</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>9868-9880</pages>
      <abstract>Large Language Models (LLMs) can transfer their reasoning skills to smaller models by teaching them to generate the intermediate reasoning process required to solve multistep reasoning tasks. While LLMs can accurately solve reasoning tasks through a variety of strategies, even without fine-tuning, smaller models are not expressive enough to fit the LLMs distribution on all strategies when distilled and tend to prioritize one strategy over the others. This reliance on one strategy poses a challenge for smaller models when attempting to solve reasoning tasks that may be difficult with their preferred strategy. To address this, we propose a distillation method <tex-math>SIKeD</tex-math>: **S**elf-guided **I**terative **K**nowledg**e** **D**istillation, where the LLM teaches the smaller model to approach a task using different strategies and the smaller model uses its self-generated on-policy outputs to choose the most suitable strategy for the given task. The training continues in a self-guided iterative manner, where for each training iteration, a decision is made on how to combine the LLM data with the self-generated outputs. Unlike traditional distillation methods, SIKeD allows the smaller model to learn which strategy is suitable for a given task while continuously learning to solve a task using different strategies. Our experiments on various mathematical reasoning datasets show that SIKeD significantly outperforms traditional distillation techniques across smaller models of different sizes.</abstract>
      <url hash="8ad839a7">2025.findings-acl.513</url>
      <bibkey>adarsh-etal-2025-siked</bibkey>
    </paper>
    <paper id="514">
      <title>Chain of Attack: Hide Your Intention through Multi-Turn Interrogation</title>
      <author><first>Xikang</first><last>Yang</last><affiliation>Institute of Information Engineering</affiliation></author>
      <author><first>Biyu</first><last>Zhou</last></author>
      <author><first>Xuehai</first><last>Tang</last></author>
      <author><first>Jizhong</first><last>Han</last><affiliation>Institute of Information Engineering</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>9881-9901</pages>
      <abstract>The latent knowledge of large language models (LLMs) contains harmful or unethical content, which introduces significant security risks upon their widespread deployment. Conducting jailbreak attacks on LLMs can proactively identify vulnerabilities to enhance their security measures. However, previous jailbreak attacks primarily focus on single-turn dialogue scenarios, leaving vulnerabilities in multi-turn dialogue contexts inadequately explored. This paper investigates the resilience of black-box LLMs in multi-turn jailbreak attack scenarios from a novel interrogation perspective. We propose an optimal interrogation principle to conceal the jailbreak intent and introduce a multi-turn attack chain generation strategy called CoA. By employing two effective interrogation strategies tailored for LLMs, coupled with an interrogation history record management mechanis, it achieves a significant optimization of the attack process. Our approach enables the iterative generation of attack chains, offering a powerful tool for LLM red team testing. Experimental results demonstrate that LLMs exhibit insufficient resistance under multi-turn interrogation, with our method shows more advantages(ASR, 83% vs 64%). This work offers new insights into improving the safety of LLMs.</abstract>
      <url hash="dbb2c8e8">2025.findings-acl.514</url>
      <bibkey>yang-etal-2025-chain</bibkey>
    </paper>
    <paper id="515">
      <title><fixed-case>MIG</fixed-case>: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space</title>
      <author><first>Yicheng</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yining</first><last>Li</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Kai</first><last>Hu</last></author>
      <author><first>Ma</first><last>Zerun</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>HaochenYe</first><last>HaochenYe</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>9902-9915</pages>
      <abstract>Data quality and diversity are key to the construction of effective instruction-tuning datasets. With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. Notably, the model fine-tuned with 5% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73% on AlpacaEval and +6.89% on Wildbench.</abstract>
      <url hash="31ec55f8">2025.findings-acl.515</url>
      <bibkey>chen-etal-2025-mig</bibkey>
    </paper>
    <paper id="516">
      <title>Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval</title>
      <author><first>Yongchan</first><last>Chun</last></author>
      <author><first>Minhyuk</first><last>Kim</last></author>
      <author><first>Dongjun</first><last>Kim</last></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>9916-9926</pages>
      <abstract>Automatic Term Extraction (ATE) identifies domain-specific expressions that are crucial for downstream tasks such as machine translation and information retrieval. Although large language models (LLMs) have significantly advanced various NLP tasks, their potential for ATE has scarcely been examined. We propose a retrieval-based prompting strategy that, in the few-shot setting, selects demonstrations according to syntactic rather than semantic similarity. This syntactic retrieval method is domain-agnostic and provides more reliable guidance for capturing term boundaries. We evaluate the approach in both in-domain and cross-domain settings, analyzing how lexical overlap between the query sentence and its retrieved examples affects performance. Experiments on three specialized ATE benchmarks show that syntactic retrieval improves F1-score. These findings highlight the importance of syntactic cues when adapting LLMs to terminology-extraction tasks.</abstract>
      <url hash="956d1701">2025.findings-acl.516</url>
      <bibkey>chun-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="517">
      <title>Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation</title>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Ziyang</first><last>Gao</last></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>9927-9944</pages>
      <abstract>Depression is a widespread mental health disorder, and clinical interviews are the gold standard for assessment. However, their reliance on scarce professionals highlights the need for automated detection. Current systems mainly employ black-box neural networks, which lack interpretability, which is crucial in mental health contexts. Some attempts to improve interpretability use post-hoc LLM generation but suffer from hallucination. To address these limitations, we propose RED, a Retrieval-augmented generation framework for Explainable depression Detection. RED retrieves evidence from clinical interview transcripts, providing explanations for predictions. Traditional query-based retrieval systems use a one-size-fits-all approach, which may not be optimal for depression detection, as user backgrounds and situations vary. We introduce a personalized query generation module that combines standard queries with user-specific background inferred by LLMs, tailoring retrieval to individual contexts. Additionally, to enhance LLM performance in social intelligence, we augment LLMs by retrieving relevant knowledge from a social intelligence datastore using an event-centric retriever. Experimental results on the real-world benchmark demonstrate RED’s effectiveness compared to neural networks and LLM-based baselines.</abstract>
      <url hash="33d1a596">2025.findings-acl.517</url>
      <bibkey>zhang-etal-2025-explainable</bibkey>
    </paper>
    <paper id="518">
      <title><fixed-case>EMPEC</fixed-case>: A Comprehensive Benchmark for Evaluating Large Language Models Across Diverse Healthcare Professions</title>
      <author><first>Zheheng</first><last>Luo</last></author>
      <author><first>Chenhan</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <pages>9945-9958</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) show their potential in accurately answering biomedical questions, yet current healthcare benchmarks primarily assess knowledge mastered by medical doctors, neglecting other essential professions. To address this gap, we introduce the Examinations for Medical PErsonnel in Chinese (EMPEC), a comprehensive healthcare knowledge benchmark featuring 157,803 exam questions across 124 subjects and 20 healthcare professions, including underrepresented roles like Optometrists and Audiologists. Each question is tagged for release time and source authenticity. We evaluated 17 LLMs, including proprietary and open-source models, finding that while models like GPT-4 achieved over 75% accuracy, they struggled with specialized fields and alternative medicine. Notably, we find that most medical-specific LLMs underperform their general-purpose counterparts in EMPEC, and incorporating EMPEC’s data in fine-tuning improves performance. In addition, we tested LLMs on questions released after the completion of their training to examine their ability in unseen queries. We also translated the test set into English and simplified Chinese and analyse the impact on different models. Our findings emphasize the need for broader benchmarks to assess LLM applicability in real-world healthcare, and we will provide the dataset and evaluation toolkit for future research.</abstract>
      <url hash="36698373">2025.findings-acl.518</url>
      <bibkey>luo-etal-2025-empec</bibkey>
    </paper>
    <paper id="519">
      <title>Beyond Numeric Rewards: In-Context Dueling Bandits with <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Fanzeng</first><last>Xia</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hao</first><last>Liu</last><affiliation>California Institute of Technology</affiliation></author>
      <author><first>Yisong</first><last>Yue</last><affiliation>California Institute of Technology</affiliation></author>
      <author><first>Tongxin</first><last>Li</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>9959-9988</pages>
      <abstract>In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation-model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of large language models (LLMs) out of the box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL on the Dueling Bandits (DB) problem, a stateless preference-based RL setting. We find that top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environments by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and they are sensitive to prompt variations. To bridge this gap, we propose an agentic-flow framework—LLM with Enhanced Algorithmic Dueling (LEAD)—which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD inherits theoretical guarantees from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.</abstract>
      <url hash="4bb87208">2025.findings-acl.519</url>
      <bibkey>xia-etal-2025-beyond-numeric</bibkey>
    </paper>
    <paper id="520">
      <title>“Well, Keep Thinking”: Enhancing <fixed-case>LLM</fixed-case> Reasoning with Adaptive Injection Decoding</title>
      <author><first>Hyunbin</first><last>Jin</last></author>
      <author><first>Je Won</first><last>Yeom</last></author>
      <author><first>Seunghyun</first><last>Bae</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taesup</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>9989-10018</pages>
      <abstract>Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot Chain-of-Thought (CoT) prompting. While effective, these methods require labor-intensive prompt engineering, raising the question of whether reasoning can be induced without reliance on explicit prompts. In this work, we unlock the reasoning capabilities of LLMs without explicit prompting.Inspired by zero-shot CoT and CoT-decoding, we propose a novel decoding strategy that systematically nudges LLMs to continue reasoning, thereby preventing immature reasoning processes. Specifically, we monitor the model’s generation and inject a designated phrase, whenever the model is likely to halt or drift away from logical reasoning process. Our experimental evaluations on diverse reasoning benchmarks demonstrate that our proposed strategy substantially improves LLM reasoning capabilities, highlighting the potential of decoding-based interventions as an alternative to traditional prompting techniques.</abstract>
      <url hash="58c3fb5b">2025.findings-acl.520</url>
      <bibkey>jin-etal-2025-well</bibkey>
    </paper>
    <paper id="521">
      <title><fixed-case>S</fixed-case>peech<fixed-case>T</fixed-case>-<fixed-case>RAG</fixed-case>: Reliable Depression Detection in <fixed-case>LLM</fixed-case>s with Retrieval-Augmented Generation Using Speech Timing Information</title>
      <author><first>Xiangyu</first><last>Zhang</last></author>
      <author><first>Hexin</first><last>Liu</last></author>
      <author><first>Qiquan</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Beena</first><last>Ahmed</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Julien</first><last>Epps</last><affiliation>University of New South Wales</affiliation></author>
      <pages>10019-10030</pages>
      <abstract>Large Language Models (LLMs) have been increasingly adopted for health-related tasks, yet their performance in depression detection remains limited when relying solely on text input. While Retrieval-Augmented Generation (RAG) typically enhances LLM capabilities, our experiments indicate that traditional text-based RAG systems struggle to significantly improve depression detection accuracy. This challenge stems partly from the rich depression-relevant information encoded in acoustic speech patterns — information that current text-only approaches fail to capture effectively. To address this limitation, we conduct a systematic analysis of temporal speech patterns, comparing healthy individuals with those experiencing depression. Based on our findings, we introduce Speech Timing-based Retrieval-Augmented Generation, SpeechT-RAG, a novel system that leverages speech timing features for both accurate depression detection and reliable confidence estimation. This integrated approach not only outperforms traditional text-based RAG systems in detection accuracy but also enhances uncertainty quantification through a confidence scoring mechanism that naturally extends from the same temporal features. Our unified framework achieves comparable results to fine-tuned LLMs without additional training while simultaneously addressing the fundamental requirements for both accuracy and trustworthiness in mental health assessment</abstract>
      <url hash="aa530282">2025.findings-acl.521</url>
      <bibkey>zhang-etal-2025-speecht</bibkey>
    </paper>
    <paper id="522">
      <title>Fine-grained Knowledge Enhancement for Retrieval-Augmented Generation</title>
      <author><first>Jingxuan</first><last>Han</last></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>People’s Daily Online</affiliation></author>
      <author><first>Yexuan</first><last>Che</last></author>
      <author><first>Zheren</first><last>Fu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>10031-10044</pages>
      <abstract>Retrieval-augmented generation (RAG) effectively mitigates hallucinations in large language models (LLMs) by filling knowledge gaps with retrieved external information. Most existing studies primarily retrieve knowledge documents based on semantic similarity to assist in answering questions but ignore the fine-grained necessary information within documents. In this paper, we propose a novel fine-grained knowledge enhancement method (FKE) for RAG, where fine-grained knowledge primarily includes sentence-level information easily overlooked in the document-based retrieval process. Concretely, we create a disentangled Chain-of-Thought prompting procedure to retrieve fine-grained knowledge from the external knowledge corpus. Then we develop a decoding enhancement strategy to constrain the document-based decoding process using fine-grained knowledge, thereby facilitating more accurate generated answers. Given an existing RAG pipeline, our method could be applied in a plug-and-play manner to enhance its performance with no additional modules or training process. Extensive experiments verify the effectiveness and generality of our method.</abstract>
      <url hash="e8fbc296">2025.findings-acl.522</url>
      <bibkey>han-etal-2025-fine</bibkey>
    </paper>
    <paper id="523">
      <title><fixed-case>B</fixed-case>ayesian Optimization for Controlled Image Editing via <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chengkun</first><last>Cai</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Haoliang</first><last>Liu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Xu</first><last>Zhao</last><affiliation>Southeast University - Monash University Joint Graduate School</affiliation></author>
      <author><first>Zhongyu</first><last>Jiang</last></author>
      <author><first>Tianfang</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zongkai</first><last>Wu</last></author>
      <author><first>John</first><last>Lee</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Jenq-Neng</first><last>Hwang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>10045-10056</pages>
      <abstract>In the rapidly evolving field of image generation, achieving precise control over generated content and maintaining semantic consistency remain significant limitations, particularly concerning grounding techniques and the necessity for model fine-tuning. To address these challenges, we propose BayesGenie, an off-the-shelf approach that integrates Large Language Models (LLMs) with Bayesian Optimization to facilitate precise and user-friendly image editing. Our method enables users to modify images through natural language descriptions without manual area marking, while preserving the original image’s semantic integrity. Unlike existing techniques that require extensive pre-training or fine-tuning, our approach demonstrates remarkable adaptability across various LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian optimization strategy to automatically refine the inference process parameters, achieving high-precision image editing with minimal user intervention. Through extensive experiments across diverse scenarios, we demonstrate that our framework outperforms existing methods in both editing accuracy and semantic preservation, as validated using different LLMs including Claude3 and GPT-4.</abstract>
      <url hash="e2f4f1c6">2025.findings-acl.523</url>
      <bibkey>cai-etal-2025-bayesian</bibkey>
    </paper>
    <paper id="524">
      <title><fixed-case>SPOT</fixed-case>: Zero-Shot Semantic Parsing Over Property Graphs</title>
      <author><first>Francesco</first><last>Cazzaro</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>Justin</first><last>Kleindienst</last><affiliation>dMetrics</affiliation></author>
      <author><first>Sofia Márquez</first><last>Gomez</last><affiliation>dMetrics</affiliation></author>
      <author><first>Ariadna</first><last>Quattoni</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <pages>10057-10073</pages>
      <abstract>Knowledge Graphs (KGs) have gained popularity as a means of storing structured data, with property graphs, in particular, gaining traction in recent years. Consequently, the task of semantic parsing remains crucial in enabling access to the information in these graphs via natural language queries. However, annotated data is scarce, requires significant effort to create, and is not easily transferable between different graphs. To address these challenges we introduce SPOT, a method to generate training data for semantic parsing over Property Graphs without human annotations. We generate tree patterns, match them to the KG to obtain a query program, and use a finite-state transducer to produce a proto-natural language realization of the query. Finally, we paraphrase the proto-NL with an LLM to generate samples for training a semantic parser. We demonstrate the effectiveness of SPOT on two property graph benchmarks utilizing the Cypher query language. In addition, we show that our approach can also be applied effectively to RDF graphs.</abstract>
      <url hash="1c52edce">2025.findings-acl.524</url>
      <bibkey>cazzaro-etal-2025-spot</bibkey>
    </paper>
    <paper id="525">
      <title>Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference</title>
      <author><first>Geonhee</first><last>Kim</last></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>10074-10095</pages>
      <abstract>Recent studies on reasoning in language models (LMs) have sparked a debate on whether they can learn systematic inferential principles or merely exploit superficial patterns in the training data. To understand and uncover the mechanisms adopted for formal reasoning in LMs, this paper presents a mechanistic interpretation of syllogistic inference. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent and formal reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic inference, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures. The identified circuit is sufficient and necessary for syllogistic schemes on which the models achieve high accuracy (<tex-math>\geq</tex-math> 60%), with compatible activation patterns across models of different families. Overall, our findings suggest that LMs learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.</abstract>
      <url hash="c0e4dc36">2025.findings-acl.525</url>
      <bibkey>kim-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="526">
      <title>Multi-Hop Question Generation via Dual-Perspective Keyword Guidance</title>
      <author><first>Maodong</first><last>Li</last></author>
      <author><first>Longyin</first><last>Zhang</last></author>
      <author><first>Fang</first><last>Kong</last><affiliation>Soochow University</affiliation></author>
      <pages>10096-10112</pages>
      <abstract>Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords—question and document keywords—and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner’s intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments on HotpotQA demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task.</abstract>
      <url hash="68e75a89">2025.findings-acl.526</url>
      <bibkey>li-etal-2025-multi-hop</bibkey>
    </paper>
    <paper id="527">
      <title><fixed-case>L</fixed-case>o<fixed-case>RMA</fixed-case>: Low-Rank Multiplicative Adaptation for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Harsh</first><last>Bihany</last></author>
      <author><first>Shubham</first><last>Patel</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>10113-10133</pages>
      <abstract>Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.</abstract>
      <url hash="e2d9f74a">2025.findings-acl.527</url>
      <bibkey>bihany-etal-2025-lorma</bibkey>
    </paper>
    <paper id="528">
      <title><fixed-case>DI</fixed-case>-<fixed-case>BENCH</fixed-case>: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale</title>
      <author><first>Linghao</first><last>Zhang</last></author>
      <author><first>Junhao</first><last>Wang</last></author>
      <author><first>Shilin</first><last>He</last></author>
      <author><first>Chaoyun</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Kang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Jiaheng</first><last>Wen</last></author>
      <author><first>Chengxing</first><last>Xie</last></author>
      <author><first>Maoquan</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yufan</first><last>Huang</last></author>
      <author><first>Elsie</first><last>Nallipogu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yingnong</first><last>Dang</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <pages>10134-10153</pages>
      <abstract>Large Language Models have advanced automated software development, however, it remains a challenge to correctly infer dependencies, namely, identifying the internal components and external packages required for a repository to successfully run. Existing studies highlight that dependency-related issues cause over 40% of observed runtime errors on the generated repository. To address this, we introduce DI-BENCH, a large-scale benchmark and evaluation framework specifically designed to assess LLMs’ capability on dependency inference. The benchmark features 581 repositories with testing environments across Python, C#, Rust, and JavaScript. Extensive experiments with textual and execution-based metrics reveal that the current best-performing model achieves only a 48% execution pass rate on Python, indicating significant room for improvement. DI-BENCH establishes a new viewpoint for evaluating LLM performance on repositories, paving the way for more robust end-to-end software synthesis.</abstract>
      <url hash="53105ba9">2025.findings-acl.528</url>
      <bibkey>zhang-etal-2025-di</bibkey>
    </paper>
    <paper id="529">
      <title>Weak-to-Strong Honesty Alignment via Learning-to-Rank Supervision</title>
      <author><first>YunfanXie</first><last>YunfanXie</last></author>
      <author><first>Lixin</first><last>Zou</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dan</first><last>Luo</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Min</first><last>Tang</last></author>
      <author><first>Chenliang</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <pages>10154-10168</pages>
      <abstract>Honest alignment refers to the ability of a language model to truthfully convey its knowledge limitations by appropriately refusing to answer questions when it lacks sufficient information. Existing solutions, such as prompt engineering and fine-tuning, face limitations: the former provides only marginal improvements, while the latter struggles to enhance honesty when annotated data is scarce.To overcome the above limitations, we propose , a novel framework that enhances honesty through weak-to-strong generalization. Specifically, we train the strong LLMs under weak model supervision to improve their honesty. For the weak model, we employ a learning-to-rank strategy to train a “honest head”, which learns to select the most honest response among model’s outputs generated through beam search. For the strong LLM, we leverage the self-labeled dataset to update its parameters. Our proposal requires only minimal training data to train the weak honest model, yet achieve decent performance for labeling data. In addition, it enables the strong LLMs to have the capabilities to generalize even facing with the flawed label data. Extensive experiments show significantly boosts honest alignment in large models even with limited labeled data. Our code is available at <url>https://github.com/zewanfaan/WHAT_Honesty</url>.</abstract>
      <url hash="5d59e36c">2025.findings-acl.529</url>
      <bibkey>yunfanxie-etal-2025-weak</bibkey>
    </paper>
    <paper id="530">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>H</fixed-case>oax: A Dataset of Multi-hop False-premise questions</title>
      <author><first>Mohammadamin</first><last>Shafiei</last><affiliation>University of Milan</affiliation></author>
      <author><first>Hamidreza</first><last>Saffari</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <pages>10169-10187</pages>
      <abstract>As Large Language Models are increasingly deployed in high-stakes domains, their ability to detect false assumptions and reason critically is crucial for ensuring reliable outputs. False-premise questions (FPQs) serve as an important evaluation method by exposing cases where flawed assumptions lead to incorrect responses. While existing benchmarks focus on single-hop FPQs, real-world reasoning often requires multi-hop inference, where models must verify consistency across multiple reasoning steps rather than relying on surface-level cues. To address this gap, we introduce MultiHoax, a benchmark for evaluating LLMs’ ability to handle false premises in complex, multi-step reasoning tasks. Our dataset spans seven countries and ten diverse knowledge categories, using Wikipedia as the primary knowledge source to enable cross-regional factual reasoning. Experiments reveal that state-of-the-art LLMs struggle to detect false premises across different countries, knowledge categories, and multi-hop reasoning types, highlighting the need for improved false premise detection and more robust multi-hop reasoning capabilities in LLMs.</abstract>
      <url hash="f3532e37">2025.findings-acl.530</url>
      <bibkey>shafiei-etal-2025-multihoax</bibkey>
    </paper>
    <paper id="531">
      <title>Learning to Play Like Humans: A Framework for <fixed-case>LLM</fixed-case> Adaptation in Interactive Fiction Games</title>
      <author><first>Jinming</first><last>Zhang</last><affiliation>University of Essex</affiliation></author>
      <author><first>Yunfei</first><last>Long</last><affiliation>Queen Mary, University of London</affiliation></author>
      <pages>10188-10205</pages>
      <abstract>Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents’ behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.</abstract>
      <url hash="168acdd3">2025.findings-acl.531</url>
      <bibkey>zhang-long-2025-learning</bibkey>
    </paper>
    <paper id="532">
      <title><fixed-case>STATE</fixed-case> <fixed-case>T</fixed-case>oxi<fixed-case>CN</fixed-case>: A Benchmark for Span-level Target-Aware Toxicity Extraction in <fixed-case>C</fixed-case>hinese Hate Speech Detection</title>
      <author><first>Zewen</first><last>Bai</last></author>
      <author><first>Liang</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Shengdi</first><last>Yin</last></author>
      <author><first>Junyu</first><last>Lu</last></author>
      <author><first>Jingjie</first><last>Zeng</last></author>
      <author><first>Haohao</first><last>Zhu</last></author>
      <author><first>Yuanyuan</first><last>Sun</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>10206-10219</pages>
      <abstract>The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide two valuable fine-grained Chinese hate speech detection research resources. First, we construct a Span-level Target-Aware Toxicity Extraction dataset (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to understand hate semantics. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.</abstract>
      <url hash="b0434e1d">2025.findings-acl.532</url>
      <bibkey>bai-etal-2025-state</bibkey>
    </paper>
    <paper id="533">
      <title><fixed-case>R</fixed-case>el<fixed-case>E</fixed-case>dit: Evaluating Conceptual Knowledge Editing in Language Models via Relational Reasoning</title>
      <author><first>Yifan</first><last>Niu</last></author>
      <author><first>Miao</first><last>Peng</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Yatao</first><last>Bian</last><affiliation>National University of Singapore and Tencent AI Lab</affiliation></author>
      <author><first>Tingyang</first><last>Xu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>10220-10238</pages>
      <abstract>The conceptual knowledge in Large Language Models (LLMs) can become outdated over time, and concept editing is often an option. Current evaluations on conceptual knowledge editing primarily focus on whether the definitions of concepts are successfully edited, neglecting the impact on the model’s related beliefs. To address this gap, we introduce a benchmark called RelEdit, which includes criteria and questions to assess both concept-level and instance-level relational reasoning abilities of edited models. Our findings reveal that existing knowledge editing methods struggle to reason about related conceptual knowledge effectively. Additionally, we introduce a simple memory-based in-context editing baseline, MICE, which prompts the language model to generate answers that align with the stored edited concepts in external memory. In addition, we find that MICE obtains the best scores on our benchmark, suggesting a promising research direction for model editing.</abstract>
      <url hash="b083b248">2025.findings-acl.533</url>
      <bibkey>niu-etal-2025-reledit</bibkey>
    </paper>
    <paper id="534">
      <title>Unlocking Speech Instruction Data Potential with Query Rewriting</title>
      <author><first>Yonghua</first><last>Hei</last></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuliang</first><last>Liu</last></author>
      <author><first>Huiyu</first><last>Zhou</last><affiliation>Guangxi Zhuang Autonomous Region Big Data Research Institute</affiliation></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>10239-10260</pages>
      <abstract>End-to-end Large Speech Language Models (**LSLMs**) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models (**LLMs**) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating speech instruction datasets by humans, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech (**TTS**) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models. To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 72% to 93%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.</abstract>
      <url hash="19f745d8">2025.findings-acl.534</url>
      <bibkey>hei-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="535">
      <title>From Evasion to Concealment: Stealthy Knowledge Unlearning for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tianle</first><last>Gu</last></author>
      <author><first>Kexin</first><last>Huang</last></author>
      <author><first>Ruilin</first><last>Luo</last></author>
      <author><first>Yuanqi</first><last>Yao</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yan</first><last>Teng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yingchun</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>10261-10279</pages>
      <abstract>LLM Unlearning plays a crucial role in removing sensitive information from language models to mitigate potential misuse. However, previous approaches often treat nonsensical responses or template-based refusals (e.g., “Sorry, I cannot answer.”) as the unlearning target, which can give the impression of deliberate information suppression, making the process even more vulnerable to attacks and jailbreaks. Moreover, most methods rely on auxiliary models or retaining datasets, which adds complexity to the unlearning process. To address these challenges, we propose MEOW, a streamlined and stealthy unlearning method that eliminates the need for auxiliary models or retaining data while avoiding leakage through its innovative use of inverted facts. These inverted facts are generated by an offline LLM and serve as fine-tuning labels. Meanwhile, we introduce MEMO, a novel metric that measures the model’s memorization, to select optimal fine-tuning targets. The use of inverted facts not only maintains the covert nature of the model but also ensures that sensitive information is effectively forgotten without revealing the target data. Evaluated on the ToFU Knowledge Unlearning dataset using Llama2-7B-Chat and Phi-1.5, MEOW outperforms baselines in forgetting quality while preserving model utility. MEOW also maintains strong performance across NLU and NLG tasks and demonstrates superior resilience to attacks, validated via the Min-K% membership inference method.</abstract>
      <url hash="10b0fbcc">2025.findings-acl.535</url>
      <bibkey>gu-etal-2025-evasion</bibkey>
    </paper>
    <paper id="536">
      <title>Context-<fixed-case>DPO</fixed-case>: Aligning Language Models for Context-Faithfulness</title>
      <author><first>Baolong</first><last>Bi</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Tianchi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Lingrui</first><last>Mei</last><affiliation>Skywork AI</affiliation></author>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Zehao</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Weiwei</first><last>Deng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Feng</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Shenghua</first><last>Liu</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>10280-10300</pages>
      <abstract>Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment remains underexplored. To address this, we propose Context-DPO, the first alignment method specifically designed to enhance LLMs’ context-faithfulness. We introduce ConFiQA, a benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts to evaluate context-faithfulness. By leveraging faithful and stubborn responses to questions with provided context from ConFiQA, our Context-DPO aligns LLMs through direct preference optimization. Extensive experiments demonstrate that our Context-DPO significantly improves context-faithfulness, achieving 35% to 280% improvements on popular open-source models. Further analysis demonstrates that Context-DPO preserves LLMs’ generative capabilities while providing interpretable insights into context utilization.</abstract>
      <url hash="0525e2b5">2025.findings-acl.536</url>
      <bibkey>bi-etal-2025-context</bibkey>
    </paper>
    <paper id="537">
      <title>Reasoning Does Not Necessarily Improve Role-Playing Ability</title>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Longxu</first><last>Dou</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <pages>10301-10314</pages>
      <abstract>The application of role-playing large language models (LLMs) is rapidly expanding in both academic and commercial domains, driving an increasing demand for high-precision role-playing models. Simultaneously, the rapid advancement of reasoning techniques has continuously pushed the performance boundaries of LLMs. This intersection of practical role-playing demands and evolving reasoning capabilities raises an important research question: Can reasoning techniques enhance the role-playing capabilities of LLMs?” To address this, we conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3 distinct role-playing strategies, comparing the effectiveness of direct zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may reduce role-playing performance, reasoning-optimized LLMs are unsuitable for role-playing, reasoning ability disrupts the role-playing scaling law, and large models still lack proficiency in advanced role-playing. Furthermore, based on extensive experimental results, we propose two promising future research directions: Role-aware Chain-of-Thought for improving role-playing LLMs and Reinforcement Learning for role-playing LLMs, aiming to enhance the adaptability, consistency, and effectiveness of role-playing LLMs for both research and real-world applications.</abstract>
      <url hash="47ab4882">2025.findings-acl.537</url>
      <bibkey>feng-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="538">
      <title><fixed-case>T</fixed-case>able<fixed-case>LLM</fixed-case>: Enabling Tabular Data Manipulation by <fixed-case>LLM</fixed-case>s in Real Office Usage Scenarios</title>
      <author><first>Xiaokang</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Sijia</first><last>Luo</last></author>
      <author><first>Bohan</first><last>Zhang</last></author>
      <author><first>Zeyao</first><last>Ma</last></author>
      <author><first>Jing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Guanlin</first><last>Li</last></author>
      <author><first>Zijun</first><last>Yao</last></author>
      <author><first>Kangli</first><last>Xu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jinchang</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Daniel</first><last>Zhang-Li</last></author>
      <author><first>Jifan</first><last>Yu</last></author>
      <author><first>Shu</first><last>Zhao</last><affiliation>Anhui University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>10315-10344</pages>
      <abstract>We introduce TableLLM, a robust large language model (LLM) with 8 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted benchmarks tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction on this anonymized repository.</abstract>
      <url hash="2a5cc081">2025.findings-acl.538</url>
      <bibkey>zhang-etal-2025-tablellm</bibkey>
    </paper>
    <paper id="539">
      <title>A Survey of <fixed-case>LLM</fixed-case>-based Agents in Medicine: How far are we from Baymax?</title>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zizhan</first><last>Ma</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zheng</first><last>Wang</last></author>
      <author><first>Chenghan</first><last>Wu</last></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Wenting</first><last>Chen</last></author>
      <author><first>Xiang</first><last>Li</last><affiliation>Massachusetts General Hospital, Harvard University</affiliation></author>
      <author><first>Yixuan</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>10345-10359</pages>
      <abstract>Large Language Models (LLMs) are transforming healthcare through LLM-based agents that can understand and assist with medical tasks. This survey examines the architectures, applications, and challenges of LLM-based agents in medicine. We analyze key components including system profiles, clinical planning, medical reasoning frameworks, and external capacity enhancement. The survey covers major applications in clinical decision support, medical documentation, training simulations, and healthcare service optimization, along with evaluation frameworks and metrics. While these agents show promise in enhancing healthcare delivery, challenges remain in hallucination management, multimodal integration, implementation, and ethics. We conclude by highlighting future directions in medical reasoning, physical system integration, and training simulations, providing researchers and practitioners with a structured overview of the field’s current state and prospects.</abstract>
      <url hash="19a07e3c">2025.findings-acl.539</url>
      <bibkey>wang-etal-2025-survey</bibkey>
    </paper>
    <paper id="540">
      <title>Context-Robust Knowledge Editing for Language Models</title>
      <author><first>Haewon</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Gyubin</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minjun</first><last>Kim</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <pages>10360-10385</pages>
      <abstract>Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we have developed CHED—a benchmark designed to evaluate the context robustness of KE methods. Evaluations on CHED show that they often fail when preceding contexts are present. To mitigate this shortcoming, we introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge. This method not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model. We also provide an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and we dissect attention-score patterns to assess how specific tokens influence editing success. We release our dataset and code at [https://github.com/holi-lab/CoRE](https://github.com/holi-lab/CoRE).</abstract>
      <url hash="ceb5a6db">2025.findings-acl.540</url>
      <bibkey>park-etal-2025-context</bibkey>
    </paper>
    <paper id="541">
      <title>Multi-Agent Collaboration via Cross-Team Orchestration</title>
      <author><first>Zhuoyun</first><last>Du</last></author>
      <author><first>Chen</first><last>Qian</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Zihao</first><last>Xie</last></author>
      <author><first>YiFei</first><last>Wang</last></author>
      <author><first>Rennai</first><last>Qiu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yufan</first><last>Dang</last></author>
      <author><first>Weize</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Xuantang</first><last>Xiong</last></author>
      <author><first>Lei</first><last>Han</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>10386-10406</pages>
      <abstract>Large Language Models (LLMs) have significantly impacted various domains, especially through organized LLM-driven autonomous agents. A representative scenario is in software development, where agents can collaborate in a team like humans, following predefined phases to complete sub-tasks sequentially. However, for an agent team, each phase yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. Consequently leading to suboptimal results or extensive trial and error. To address this, we introduce Cross-Team Orchestration (Croto), a scalable multi-team framework that enables orchestrated teams to jointly propose various task-oriented solutions and interact with their insights in a self-independence while cross-team collaboration environment for superior solutions generation. Experiments reveal a notable increase in software quality compared to state-of-the-art baselines. We further tested our framework on story generation tasks, which demonstrated a promising generalization ability of our framework in other domains. The code and data is available at https://github.com/OpenBMB/ChatDev/tree/macnet</abstract>
      <url hash="5d00f824">2025.findings-acl.541</url>
      <bibkey>du-etal-2025-multi</bibkey>
    </paper>
    <paper id="542">
      <title>Semantic Evaluation of Multilingual Data-to-Text Generation via <fixed-case>NLI</fixed-case> Fine-Tuning: Precision, Recall and F1 scores</title>
      <author><first>William Soto</first><last>Martinez</last></author>
      <author><first>Yannick</first><last>Parmentier</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>10407-10427</pages>
      <abstract>Performance in the KG-to-Text task has improved over the years, particularly in English. However, models are still prone to mistakes like Additions and Omissions. Furthermore, few languages are taken into account since both train and test data are not readily available. In this paper, we hope to facilitate the development and improvement of multilingual KG-to-Text models by providing a multilingual evaluation framework that is reference-less (no need for test data) and permits estimating how much a KG-to-Text Model under- (omission) or over- (addition) generates. We focus on two high (English, Russian) and five low (Breton, Irish, Maltese, Welsh, Xhosa) resource languages and show that our metric has fair to moderate correlation with reference-based metrics, positioning it as a consistent alternative when no references are available. We also show that our metric outperforms prior reference-less metrics in correlation with existing human judgments. Additional human evaluation shows moderate to strong correlation with human annotators in assessing precision and recall at a higher granularity level than shown in previous studies. Since our metric provides scores for precision and recall, it helps better assess the level of over- or under-generation of multilingual KG-to-Text models.</abstract>
      <url hash="8d0cc046">2025.findings-acl.542</url>
      <bibkey>martinez-etal-2025-semantic</bibkey>
    </paper>
    <paper id="543">
      <title>Optimized Text Embedding Models and Benchmarks for <fixed-case>A</fixed-case>mharic Passage Retrieval</title>
      <author><first>Kidist Amde</first><last>Mekonnen</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Yosef Worku</first><last>Alemneh</last><affiliation>NeoMatrix Ltd</affiliation></author>
      <author><first>Maarten</first><last>de Rijke</last></author>
      <pages>10428-10445</pages>
      <abstract>Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13<tex-math>\times</tex-math> smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.</abstract>
      <url hash="22d50ec6">2025.findings-acl.543</url>
      <bibkey>mekonnen-etal-2025-optimized</bibkey>
    </paper>
    <paper id="544">
      <title>Enhancing Transformation from Natural Language to Signal Temporal Logic Using <fixed-case>LLM</fixed-case>s with Diverse External Knowledge</title>
      <author><first>Yue</first><last>Fang</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University</affiliation></author>
      <author><first>Jie</first><last>An</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongshen</first><last>Chen</last><affiliation>JD.com</affiliation></author>
      <author><first>Xiaohong</first><last>Chen</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Naijun</first><last>Zhan</last><affiliation>Peking University</affiliation></author>
      <pages>10446-10458</pages>
      <abstract>Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose a NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), comprising 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.</abstract>
      <url hash="4880c810">2025.findings-acl.544</url>
      <bibkey>fang-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="545">
      <title><fixed-case>DAGS</fixed-case>: A Dependency-Based Dual-Attention and Global Semantic Improvement Framework for Metaphor Recognition</title>
      <author><first>Puli</first><last>Chen</last></author>
      <author><first>Cheng</first><last>Yang</last></author>
      <author><first>Xingmao</first><last>Zhang</last><affiliation>Guangxi Arts University</affiliation></author>
      <author><first>Qingbao</first><last>Huang</last><affiliation>Guangxi University</affiliation></author>
      <pages>10459-10476</pages>
      <abstract>Current metaphor recognition mainly rely on Metaphor Detection Theory (MDT), such as the Metaphor Identification Procedure, which recognizes metaphors by comparing the basic meaning of target word with context meaning. Existing studies have gradually adopted literal annotations to model basic meanings, rejecting the aggregated meanings of target words. However, these methods ignore the problem of interference caused by literal annotations, and do not make full use of semantic expression relations of MDT, making the models difficult to detect and generalize. To address these challenges, we propose a dependency-based <b>D</b>ual-<b>A</b>ttention and <b>G</b>lobal <b>S</b>emantic Improvement (DAGS) framework. DAGS first extracts literal annotations of target words as basic meaning from several mainstream corpora. Then, we apply dependency tree and dual-attention while filtering on input sentences and basic meanings. Finally, we improve the MDT to further consider the global semantic relationship on contexts. The DAGS can not only extract features from multiple information sources but alsoeffectively removes redundancy, while focusing on mission-critical information. We achieve state-of-the-art on several mainstream metaphor datasets (e.g., VUA ALL, VUAverb, TroFi and PSUCMC), which suggests that filtering and global semantic improvement of contexts is crucial for enhancing metaphor recognition performance.</abstract>
      <url hash="5803cc6f">2025.findings-acl.545</url>
      <bibkey>chen-etal-2025-dags</bibkey>
    </paper>
    <paper id="546">
      <title><fixed-case>ESF</fixed-case>: Efficient Sensitive Fingerprinting for Black-Box Tamper Detection of Large Language Models</title>
      <author><first>Xiaofan</first><last>Bai</last></author>
      <author><first>Pingyi</first><last>Hu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xiaojing</first><last>Ma</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Linchen</first><last>Yu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Bin Benjamin</first><last>Zhu</last><affiliation>Microsoft AI Asia</affiliation></author>
      <pages>10477-10494</pages>
      <abstract>The rapid adoption of large language models (LLMs) in diverse applications has intensified concerns over their security and integrity, especially in cloud environments where internal model parameters are inaccessible to users. Traditional tamper detection methods, designed for deterministic classification models, fail to address the output randomness and massive parameter spaces characteristic of LLMs. In this paper, we introduce <i>Efficient Sensitive Fingerprinting (ESF)</i>, the first fingerprinting method tailored for black-box tamper detection of LLMs. ESF generates fingerprint samples by optimizing output sensitivity at selected detection token positions and leverages <i>Randomness-Set Consistency Checking (RSCC)</i> to accommodate inherent output randomness. Furthermore, a novel <i>Max Coverage Strategy (MCS)</i> is proposed to select an optimal set of fingerprint samples that maximizes joint sensitivity to tampering. Grounded in a rigorous theoretical framework, ESF is both computationally efficient and scalable to large models. Extensive experiments across state-of-the-art LLMs demonstrate that ESF reliably detects tampering, such as fine-tuning, model compression, and backdoor injection, with a detection rate exceeding 99.2% using 5 fingerprint samples, thereby offering a robust solution for securing cloud-based AI systems.</abstract>
      <url hash="f3a6590c">2025.findings-acl.546</url>
      <bibkey>bai-etal-2025-esf</bibkey>
    </paper>
    <paper id="547">
      <title>The Lessons of Developing Process Reward Models in Mathematical Reasoning</title>
      <author><first>Zhenru</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chujie</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yangzhen</first><last>Wu</last></author>
      <author><first>Beichen</first><last>Zhang</last></author>
      <author><first>Runji</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <pages>10495-10516</pages>
      <abstract>Process Reward Models (PRMs) aim to identify and mitigate intermediate errors in the reasoning processes in mathematical reasoning of Large Language Models (LLMs).However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies.In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods.Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs.To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task.Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research.</abstract>
      <url hash="af82cae9">2025.findings-acl.547</url>
      <bibkey>zhang-etal-2025-lessons</bibkey>
    </paper>
    <paper id="548">
      <title><fixed-case>M</fixed-case>inos<fixed-case>E</fixed-case>val: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended <fixed-case>QA</fixed-case> Evaluation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Yating</first><last>Wang</last></author>
      <author><first>Guandong</first><last>Wang</last></author>
      <author><first>Zhai</first><last>Jie</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Qi</first><last>Ye</last></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>10517-10548</pages>
      <abstract>Open-ended question answering (QA) is a key task for evaluating the capabilities of large language models (LLMs). Compared to closed-ended QA, it demands longer answer statements, more nuanced reasoning processes, and diverse expressions, making refined and interpretable automatic evaluation both crucial and challenging. Traditional metrics like ROUGE and BERTScore struggle to capture semantic similarities due to different patterns between model responses and reference answers. Current LLM-based evaluation approaches, such as pairwise or listwise comparisons of candidate answers, lack intuitive interpretability. While pointwise scoring of each response provides some descriptions, it fails to adapt across different question contents. Most notably, existing methods overlook the distinction between factoid and non-factoid questions. To address these challenges, we propose <b>MinosEval</b>, a novel evaluation method that first distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy. Experiments on multiple open-ended QA datasets, including self-built ones with more candidate responses to complement community resources, show that MinosEval better aligns with human annotations and offers more interpretable results.</abstract>
      <url hash="bfce0c6f">2025.findings-acl.548</url>
      <bibkey>fan-etal-2025-minoseval</bibkey>
    </paper>
    <paper id="549">
      <title>Towards Conditioning Clinical Text Generation for User Control</title>
      <author><first>Osman Alperen</first><last>Koraş</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Rabi</first><last>Bahnan</last></author>
      <author><first>Jens</first><last>Kleesiek</last><affiliation>Institute for AI in Medicine (IKIM), University Medicine Essen</affiliation></author>
      <author><first>Amin</first><last>Dada</last></author>
      <pages>10549-10569</pages>
      <abstract>Deploying natural language generation systems in clinical settings remains challenging despite advances in Large Language Models (LLMs), which continue to exhibit hallucinations and factual inconsistencies, necessitating human oversight. This paper explores automated dataset augmentation using LLMs as human proxies to condition LLMs for clinician control without increasing cognitive workload. On the BioNLP ACL’24 Discharge Me! Shared Task, we achieve new state-of-the-art results with simpler methods than prior submissions through more efficient training, yielding a 9% relative improvement without augmented training and up to 34% with dataset augmentation. Preliminary human evaluation further supports the effectiveness of our approach, highlighting the potential of augmenting clinical text generation for control to enhance relevance, accuracy, and factual consistency.</abstract>
      <url hash="b7985751">2025.findings-acl.549</url>
      <bibkey>koras-etal-2025-towards</bibkey>
    </paper>
    <paper id="550">
      <title><fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>et-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings</title>
      <author><first>Daniil</first><last>Orel</last></author>
      <author><first>Dilshod</first><last>Azizov</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>10570-10593</pages>
      <abstract>Large Language Models (LLMs) have revolutionized code generation, automating programming with remarkable efficiency. However, this has had important consequences for programming skills, ethics, and assessment integrity, thus making the detection of LLM-generated code essential for maintaining accountability and standards. While, there has been some previous research on this problem, it generally lacks domain coverage and robustness, and only covers a small number of programming languages. Here, we aim to bridge this gap. In particular, we propose a framework capable of distinguishing between human-written and LLM-generated program code across multiple programming languages, code generators, and domains. We use a large-scale dataset from renowned platforms and LLM-based code generators, alongside applying rigorous data quality checks, feature engineering, and comparative analysis of traditional machine learning models, pre-trained language models (PLMs), and LLMs for code detection. We perform an evaluation on out-of-domain scenarios, such as detecting authorship and hybrid authorship of generated code and generalizing to unseen models, domains, and programming languages. Our extensive experiments show that our framework effectively distinguishes human-written from LLM-generated program code, setting a new benchmark for the task.</abstract>
      <url hash="149ec2df">2025.findings-acl.550</url>
      <bibkey>orel-etal-2025-codet</bibkey>
    </paper>
    <paper id="551">
      <title><fixed-case>Q</fixed-case>-Mamba: Towards more efficient Mamba models via post-training quantization</title>
      <author><first>Chen</first><last>Tianqi</last></author>
      <author><first>Yuanteng</first><last>Chen</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Peisong</first><last>Wang</last><affiliation>Institute of Automation of,Chinese Academy of Sciences</affiliation></author>
      <author><first>Weixiang</first><last>Xu</last></author>
      <author><first>Zeyu</first><last>Zhu</last></author>
      <author><first>Jian</first><last>Cheng</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>10594-10610</pages>
      <abstract>State Space Models (SSMs), such as Mamba, have recently demonstrated potential in language understanding tasks, positioning them as competitors to transformer architectures. However, our investigations reveal that the Mamba architecture still has room for further optimization—not only in linear projections but also in state caches, which contribute significantly to memory consumption, particularly after quantizing the former into low bits. After a theoretical analysis of the causes of outliers in states, we propose Decoupled Scale Quantization (DSQ), which mitigates outliers in both the state and channel dimensions by applying separate quantization scales. To preserve the selective ability of quantized Mamba, we introduce Efficient Selectivity Reconstruction (ESR), a novel quantization simulation scheme in block-wise reconstruction that enables fast parallel scan algorithms with the non-linear quantization function. We demonstrate the effectiveness of Q-Mamba across various quantization settings, model sizes, and both generation and zero-shot tasks. In particular, for Mamba2-2.7B with W8A8H4 (8-bit weights and activations, 4-bit state caches) quantization, Q-Mamba achieves a 50% reduction in memory consumption with only a 2.13% average accuracy degradation on zero-shot tasks.</abstract>
      <url hash="75e2e3c5">2025.findings-acl.551</url>
      <bibkey>tianqi-etal-2025-q</bibkey>
    </paper>
    <paper id="552">
      <title><fixed-case>P</fixed-case>²<fixed-case>N</fixed-case>et: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts</title>
      <author><first>Kaiwen</first><last>Wei</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Jie</first><last>Yao</last></author>
      <author><first>Jiang</first><last>Zhong</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Jingyuan</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Changlong</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Fengmao</first><last>Lv</last><affiliation>Southwest Jiaotong University</affiliation></author>
      <author><first>Li</first><last>Jin</last></author>
      <pages>10611-10626</pages>
      <abstract>Key Information Extraction (KIE) is a challenging multimodal task aimed at extracting structured value entities from visually rich documents. Despite recent advancements, two major challenges remain. First, existing datasets typically feature fixed layouts and a limited set of entity categories, while current methods are based on a full-shot setting that is difficult to apply in real-world scenarios, where new entity categories frequently emerge. Secondly, current methods often treat key entities simply as parts of the OCR-parsed context, neglecting the positive impact of the relationships between key-value entities. To address the first challenge, we introduce a new large-scale, human-annotated dataset, Complex Layout document for Key Information Extraction (CLEX). Comprising 5,860 images with 1,162 entity categories, CLEX is larger and more complex than existing datasets. It also primarily focuses on the zero-shot and few-shot KIE tasks, which are more aligned with real-world applications. To tackle the second challenge, we propose the Parallel Pointer-based Network (P²Net). This model frames KIE as a pointer-based classification task and effectively leverages implicit relationships between key-value entities to enhance extraction. Its parallel extraction mechanism enables simultaneous and efficient extraction of multiple results. Experiments on widely-used datasets, including SROIE, CORD, and the newly introduced CLEX, demonstrate that P²Net outperforms existing state-of-the-art methods (including GPT-4V) while maintaining fast inference speeds.</abstract>
      <url hash="61a22618">2025.findings-acl.552</url>
      <bibkey>wei-etal-2025-p2net</bibkey>
    </paper>
    <paper id="553">
      <title>Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models</title>
      <author><first>Liyang</first><last>He</last></author>
      <author><first>Chenglong</first><last>Liu</last></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shulan</first><last>Ruan</last></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>10627-10643</pages>
      <abstract>Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.</abstract>
      <url hash="c212eeb0">2025.findings-acl.553</url>
      <bibkey>he-etal-2025-refining</bibkey>
    </paper>
    <paper id="554">
      <title><fixed-case>RQT</fixed-case>: Hierarchical Residual Quantization for Multi-Model Compression</title>
      <author><first>Chen</first><last>Tianqi</last></author>
      <author><first>Peisong</first><last>Wang</last><affiliation>Institute of Automation of,Chinese Academy of Sciences</affiliation></author>
      <author><first>Weixiang</first><last>Xu</last></author>
      <author><first>Zeyu</first><last>Zhu</last></author>
      <author><first>Jian</first><last>Cheng</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>10644-10660</pages>
      <abstract>Delta compression methods focus on efficiently serving multiple uniquely fine-tuned models, each tailored to specific tasks and user requirements. These approaches decompose a fine-tuned LLM into a base model and corresponding delta weights, which are compressed using low-rank or low-bit representations to reduce storage costs. However, their effectiveness is highly sensitive to the magnitude of the model deltas—a factor directly influenced by the scale of the training data. We propose the Residual Quantization Tree (RQT), a hierarchical quantization framework that automatically shares low-bit integer weights across similar fine-tuned models. The RQT construction employs a two-phase greedy algorithm: a bottom-up aggregation of models based on weight matrix similarity, and top-down residual quantization, in which each node optimizes the quantization parameters and then delegates residual errors to child nodes. We evaluate RQT on fine-tuned models across mathematics, coding, chatbot, and Chinese LLMs. The results show that RQT achieves an average accuracy degradation of approximately 3% (comparable to previous 4-bit post-training quantization) while maintaining an effective bitwidth of around 2 bits.</abstract>
      <url hash="eed11d25">2025.findings-acl.554</url>
      <bibkey>tianqi-etal-2025-rqt</bibkey>
    </paper>
    <paper id="555">
      <title>taz2024full: Analysing <fixed-case>G</fixed-case>erman Newspapers for Gender Bias and Discrimination across Decades</title>
      <author><first>Stefanie</first><last>Urchs</last><affiliation>Ludwig-Maximilians-Universität München and Hochschule München</affiliation></author>
      <author><first>Veronika</first><last>Thurner</last><affiliation>Hochschule München</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Christian</first><last>Heumann</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Stephanie</first><last>Thiemichen</last><affiliation>Hochschule München</affiliation></author>
      <pages>10661-10671</pages>
      <abstract>Open-access corpora are essential for advancing natural language processing (NLP) and computational social science (CSS). However,large-scale resources for German remain limited, restricting research on linguistic trends and societal issues such as gender bias. Wepresent taz2024full, the largest publicly available corpus of German newspaper articles to date, comprising over 1.8 million texts fromtaz, spanning 1980 to 2024.As a demonstration of the corpus’s utility for bias and discrimination research, we analyse gender representation across four decades ofreporting. We find a consistent overrepresentation of men, but also a gradual shift toward more balanced coverage in recent years. Usinga scalable, structured analysis pipeline, we provide a foundation for studying actor mentions, sentiment, and linguistic framing in Germanjournalistic texts.The corpus supports a wide range of applications, from diachronic language analysis to critical media studies, and is freely available tofoster inclusive and reproducible research in German-language NLP.</abstract>
      <url hash="2e9ee903">2025.findings-acl.555</url>
      <bibkey>urchs-etal-2025-taz2024full</bibkey>
    </paper>
    <paper id="556">
      <title><fixed-case>LCFO</fixed-case>: Long Context and Long Form Output Dataset and Benchmarking</title>
      <author><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <author><first>Pierre</first><last>Andrews</last></author>
      <author><first>Mariano Coria</first><last>Meglioli</last><affiliation>Meta</affiliation></author>
      <author><first>Joy</first><last>Chen</last><affiliation>Georgia Institute of Technology and Facebook</affiliation></author>
      <author><first>Joe</first><last>Chuang</last><affiliation>FAIR</affiliation></author>
      <author><first>David</first><last>Dale</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta</affiliation></author>
      <author><first>Alexandre</first><last>Mourachko</last><affiliation>Research, Facebook</affiliation></author>
      <author><first>Eduardo</first><last>Sánchez</last><affiliation>University College London, University of London and Meta</affiliation></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Tuan A.</first><last>Tran</last><affiliation>Meta Platform</affiliation></author>
      <author><first>Arina</first><last>Turkatenko</last><affiliation>Facebook</affiliation></author>
      <author><first>Carleigh</first><last>Wood</last></author>
      <pages>10672-10700</pages>
      <abstract>This paper presents the Long Context and Form Output (LCFO) benchmark, a novel evaluation framework for assessing gradual summarization and summary expansion capabilities across diverse domains. LCFO consists of long input documents (5k words average length), each of which comes with three summaries of different lengths (20%, 10%, and 5% of the input text), as well as approximately 15 questions and answers (QA) related to the input content. Notably, LCFO also provides alignments between specific QA pairs and corresponding summaries in 7 domains. The primary motivation behind providing summaries of different lengths is to establish a controllable framework for generating long texts from shorter inputs, i.e. summary expansion. To establish an evaluation metric framework for summarization and summary expansion, we provide human evaluation scores for human-generated outputs, as well as results from various state-of-the-art large language models (LLMs). GPT-4o-mini achieves best human scores among automatic systems in both summarization and summary expansion tasks (≈ +10% and +20%, respectively). It even surpasses human output quality in the case of short summaries (≈ +7%). Overall automatic metrics achieve low correlations with human evaluation scores (≈ 0.4) but moderate correlation on specific evaluation aspects such as fluency and attribution (≈ 0.6).</abstract>
      <url hash="df96caf0">2025.findings-acl.556</url>
      <bibkey>costa-jussa-etal-2025-lcfo</bibkey>
    </paper>
    <paper id="557">
      <title>Span-based Semantic Role Labeling as Lexicalized Constituency Tree Parsing</title>
      <author><first>Yang</first><last>Hou</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <pages>10701-10713</pages>
      <abstract>Semantic Role Labeling (SRL) is a critical task that focuses on identifying predicate-argument structures in sentences. Span-based SRL, a prominent paradigm, is often tackled using BIO-based or graph-based methods. However, these approaches often fail to capture the inherent relationship between syntax and semantics. While syntax-aware models have been proposed to address this limitation, they heavily rely on pre-existing syntactic resources, limiting their general applicability. In this work, we propose a lexicalized tree representation for span-based SRL, which integrates constituency and dependency parsing to explicitly model predicate-argument structures. By structurally representing predicates as roots and arguments as subtrees directly linked to the predicate, our approach bridges the gap between syntactic and semantic representations. Experiments on standard English benchmarks (CoNLL05 and CoNLL12) demonstrate that our model achieves competitive performance, with particular improvement in predicate-given settings.</abstract>
      <url hash="018c17ce">2025.findings-acl.557</url>
      <bibkey>hou-li-2025-span</bibkey>
    </paper>
    <paper id="558">
      <title>Learning from Negative Samples in Biomedical Generative Entity Linking</title>
      <author><first>Chanhwi</first><last>Kim</last></author>
      <author><first>Hyunjae</first><last>Kim</last><affiliation>Yale University</affiliation></author>
      <author><first>Sihyeon</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Jiwoo</first><last>Lee</last></author>
      <author><first>Mujeen</first><last>Sung</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>10714-10730</pages>
      <abstract>Generative models have become widely used in biomedical entity linking (BioEL) due to their excellent performance and efficient memory usage. However, these models are usually trained only with positive samples—entities that match the input mention’s identifier—and do not explicitly learn from hard negative samples, which are entities that look similar but have different meanings. To address this limitation, we introduce ANGEL (Learning from Negative Samples in Biomedical Generative Entity Linking), the first framework that trains generative BioEL models using negative samples. Specifically, a generative model is initially trained to generate positive entity names from the knowledge base for given input entities. Subsequently, both correct and incorrect outputs are gathered from the model’s top-k predictions. Finally, the model is updated to prioritize the correct predictions through preference optimization. Our models fine-tuned with ANGEL outperform the previous best baseline models by up to an average top-1 accuracy of 1.4% on five benchmarks. When incorporating our framework into pre-training, the performance improvement increases further to 1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning stages. The code and model weights are available at https://github.com/dmis-lab/ANGEL.</abstract>
      <url hash="e5a5d348">2025.findings-acl.558</url>
      <bibkey>kim-etal-2025-learning-negative</bibkey>
    </paper>
    <paper id="559">
      <title>Self-play through Computational Runtimes improves Chart Reasoning</title>
      <author><first>Tautvydas</first><last>Misiūnas</last><affiliation>Research, Google</affiliation></author>
      <author><first>Hassan</first><last>Mansoor</last><affiliation>Google</affiliation></author>
      <author><first>Jasper</first><last>Uijlings</last><affiliation>Google</affiliation></author>
      <author><first>Oriana</first><last>Riva</last><affiliation>Google and Microsoft</affiliation></author>
      <author><first>Victor</first><last>Carbune</last><affiliation>Google</affiliation></author>
      <pages>10731-10746</pages>
      <abstract>Vision-language models (VLMs) achieve impressive zero-shot performance on multimodal reasoning tasks. Typically, best reported performance is achieved with a zero- or a few-shot prompt. We observe that asking the model to take other routes of solving the same task, such as through code generation, hurts performance. Furthermore, training sets are typically no longer useful for improving model performance through few-shot learning, due to their use in training. Indeed, we observe that auto-prompting techniques such as DSPy (CITATION), when applied on training sets, do not produce few-shot examples that further improve validation performance. Further, when used in conjunction with program-of-thought, performance becomes even worse.Our work overcomes these limitations by introducing a novel self-play programming interface which leverages the ability of VLMs to first generate code to decompose a complex visual reasoning task in sub-tasks, then use itself, or other models, as a tool to solve decomposed tasks. Our approach enables DSPy to not suffer from performance drops, when applied iteratively on training sets. Furthermore, it outperforms zero-shot baselines on difficult chart reasoning benchmarks. We report the performance of our approach on ChartQA, PlotQA and ChartFC. This enables large models, such as Gemini or GPT to autonomously learn how to use themselves as tools and iteratively improve without the need for additional data.</abstract>
      <url hash="069918dc">2025.findings-acl.559</url>
      <bibkey>misiunas-etal-2025-self</bibkey>
    </paper>
    <paper id="560">
      <title>Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness</title>
      <author><first>Jiachun</first><last>Li</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Jiexin</first><last>Xu</last></author>
      <author><first>Huaijun</first><last>Li</last></author>
      <author><first>Xiaojian</first><last>Jiang</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>10747-10765</pages>
      <abstract>Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks.Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.</abstract>
      <url hash="7c95a7d8">2025.findings-acl.560</url>
      <bibkey>li-etal-2025-towards-better</bibkey>
    </paper>
    <paper id="561">
      <title>A Couch Potato is not a Potato on a Couch: Prompting Strategies, Image Generation, and Compositionality Prediction for Noun Compounds</title>
      <author><first>Sinan</first><last>Kurtyigit</last></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Carina</first><last>Silberer</last></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>10766-10776</pages>
      <abstract>We explore the role of the visual modality and of vision transformers in predicting the compositionality of English noun compounds. Crucially, we contribute a framework to address the challenge of obtaining adequate images that represent non-compositional compounds (such as “couch potato”), making it relevant for any image-based approach targeting figurative language. Our method uses prompting strategies and diffusion models to generate images. Comparing and combining our approach with a state-of-the-art text-based approach reveals complementary contributions regarding features as well as degrees of abstractness in compounds.</abstract>
      <url hash="eadf5912">2025.findings-acl.561</url>
      <bibkey>kurtyigit-etal-2025-couch</bibkey>
    </paper>
    <paper id="562">
      <title>A Rose by Any Other Name: <fixed-case>LLM</fixed-case>-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on <fixed-case>NLI</fixed-case></title>
      <author><first>Beiduo</first><last>Chen</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Siyao</first><last>Peng</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>10777-10802</pages>
      <abstract>Disagreement in human labeling is ubiquitous, and can be captured in human judgment distributions (HJDs). Recent research has shown that explanations provide valuable information for understanding human label variation (HLV) and large language models (LLMs) can approximate HJD from a few human-provided label-explanation pairs. However, collecting explanations for every label is still time-consuming. This paper examines whether LLMs can be used to replace humans in generating explanations for approximating HJD. Specifically, we use LLMs as annotators to generate model explanations for a few given human labels. We test ways to obtain and combine these label-explanations with the goal to approximate human judgment distributions. We further compare the resulting human with model-generated explanations, and test automatic and human explanation selection. Our experiments show that LLM explanations are promising for NLI: to estimate HJDs, generated explanations yield comparable results to human’s when provided with human labels. Importantly, our results generalize from datasets with human explanations to i) datasets where they are not available and ii) challenging out-of-distribution test sets.</abstract>
      <url hash="bf68f742">2025.findings-acl.562</url>
      <bibkey>chen-etal-2025-rose</bibkey>
    </paper>
    <paper id="563">
      <title>Measuring What Matters: Evaluating Ensemble <fixed-case>LLM</fixed-case>s with Label Refinement in Inductive Coding</title>
      <author><first>Angelina</first><last>Parfenova</last></author>
      <author><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <pages>10803-10816</pages>
      <abstract>Inductive coding traditionally relies on labor-intensive human efforts, who are prone to inconsistencies and individual biases. Although large language models (LLMs) offer promising automation capabilities, their standalone use often results in inconsistent outputs, limiting their reliability. In this work, we propose a framework that combines ensemble methods with code refinement methodology to address these challenges. Our approach integrates multiple smaller LLMs, fine-tuned via Low-Rank Adaptation (LoRA), and employs a moderator-based mechanism to simulate human consensus. To address the limitations of metrics like ROUGE and BERTScore, we introduce a composite evaluation metric that combines code conciseness and contextual similarity. The validity of this metric is confirmed through correlation analysis with human expert ratings. Results demonstrate that smaller ensemble models with refined outputs consistently outperform other ensembles, individual models, and even large-scale LLMs like GPT-4. Our evidence suggests that smaller ensemble models significantly outperform larger standalone language models, pointing out the risk of relying solely on a single large model for qualitative analysis.</abstract>
      <url hash="dce11ac1">2025.findings-acl.563</url>
      <bibkey>parfenova-pfeffer-2025-measuring</bibkey>
    </paper>
    <paper id="564">
      <title>Dynamic Evil Score-Guided Decoding: An Efficient Decoding Framework For Red-Team Model</title>
      <author><first>Cong</first><last>Gao</last></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Xi’an Research Institute of High Technology</affiliation></author>
      <author><first>Linkang</first><last>Yang</last></author>
      <author><first>Minghao</first><last>Hu</last><affiliation>Center of Information Research, AMS</affiliation></author>
      <author><first>Zhunchen</first><last>Luo</last></author>
      <author><first>Xiaoying</first><last>Bai</last><affiliation>AIBD</affiliation></author>
      <author><first>Guotong</first><last>Geng</last><affiliation>AMS</affiliation></author>
      <author><first>Jun</first><last>Zhang</last></author>
      <author><first>Yunhua</first><last>Xue</last><affiliation>Nankai University</affiliation></author>
      <pages>10817-10833</pages>
      <abstract>Large language models (LLMs) have achieved significant advances but can potentially generate harmful content such as social biases, extremism, and misinformation. Red teaming is a promising approach to enhance model safety by creating adversarial prompts to test and improve model robustness. However, existing red-teaming methods often require expensive fine-tuning, especially for large LLMs. We propose the Dynamic Evil Score-Guided Decoding framework (DESGD), an efficient red-teaming method that does not increase computational cost with the target model size. DESGD introduces the concept of an ‘evil score’ to dynamically evaluate the potential of tokens to contribute to harmful outputs during decoding. This framework constructs a small unsafe model using an adversarial dataset and adjusts the logits vector of the target model based on the evil score. Experiments show that DESGD achieves an ASR of 92.83% on the Llama-3.2-3B-Instruct model, compared to 83.48% with adversarial fine-tuning while using less computational resources. Similarly, on the Qwen2.5-3B-Instruct model, DESGD reaches an ASR of 88.62%, outperforming adversarial fine-tuning (77.56%).</abstract>
      <url hash="c5efd357">2025.findings-acl.564</url>
      <bibkey>gao-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="565">
      <title><fixed-case>C</fixed-case>o<fixed-case>M</fixed-case>u<fixed-case>MDR</fixed-case>: Code-mixed Multi-modal Multi-domain corpus for Discourse pa<fixed-case>R</fixed-case>sing in conversations</title>
      <author><first>Divyaksh</first><last>Shukla</last></author>
      <author><first>Ritesh</first><last>Baviskar</last></author>
      <author><first>Dwijesh</first><last>Gohil</last></author>
      <author><first>Aniket</first><last>Tiwari</last><affiliation>convin</affiliation></author>
      <author><first>Atul</first><last>Shree</last><affiliation>Convin</affiliation></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>10834-10849</pages>
      <abstract>Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.</abstract>
      <url hash="71b55787">2025.findings-acl.565</url>
      <bibkey>shukla-etal-2025-comumdr</bibkey>
    </paper>
    <paper id="566">
      <title>Multi-word Measures: Modeling Semantic Change in Compound Nouns</title>
      <author><first>Chris</first><last>Jenkins</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Filip</first><last>Miletić</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Sabine</first><last>Schulte Im Walde</last><affiliation>University of Stuttgart</affiliation></author>
      <pages>10850-10864</pages>
      <abstract>Compound words (e.g. shower thought) provide a multifaceted challenge for diachronic models of semantic change. Datasets describing noun compound semantics tend to describe only the predominant sense of a compound, which is limiting, especially in diachronic settings where senses may shift over time. We create a novel dataset of relatedness judgements of noun compounds in English and German, the first to capture diachronic meaning changes for multi-word expressions without prematurely condensing individual senses into an aggregate value. Furthermore, we introduce a novel, sense-targeting approach for noun compounds that evaluates two contrasting vector representations in their ability to cluster example sentence pairs. Our clustering approach targets both noun compounds and their constituent parts, to model the interdependence of these terms over time. We calculate time-delineated distributions of these clusters and compare them against measures of semantic change aggregated from the human relatedness annotations.</abstract>
      <url hash="64bad9d0">2025.findings-acl.566</url>
      <bibkey>jenkins-etal-2025-multi</bibkey>
    </paper>
    <paper id="567">
      <title>Bridge-Coder: Transferring Model Capabilities from High-Resource to Low-Resource Programming Language</title>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Jianshu</first><last>Zhang</last></author>
      <author><first>Yuanzhe</first><last>Li</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Rui</first><last>Pan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Runtao</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Ziqiang</last><affiliation>King Abdullah University of Science and Technology and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>10865-10882</pages>
      <abstract>Most LLMs universally excel at generating code for high-resource programming languages (HRPLs) like Python, a capability that has become standard due to the abundance of training data. However, they struggle significantly with low-resource programming languages (LRPLs) such as D, exacerbating the digital divide. This gap limits developers using LRPLs from equally benefiting and hinders innovation within underrepresented programming communities. To make matters worse, manually generating data for LRPLs is highly labor intensive and requires expensive expert effort. In this work, we begin by analyzing the NL-PL Gap, where LLMs’ direct-generated LRPL data often suffers from subpar quality due to the misalignment between natural language (NL) instructions and programming language (PL) outputs. To address this issue, we introduce Bridge-Assist Generation, a method to generate LRPL data utilizing LLM’s general knowledge, HRPL proficiency, and in-context learning capabilities. To further maximize the utility of the generated data, we propose Bridged Alignment to obtain Bridge-Coder. To thoroughly evaluate our approach, we select four relatively LRPLs: R, D, Racket, and Bash. Experimental results reveal that Bridge-Coder achieves significant improvements over the original model, with average gains of 18.71 and 10.81 on two comprehensive benchmarks, M-HumanEval and M-MBPP.</abstract>
      <url hash="ee68697a">2025.findings-acl.567</url>
      <bibkey>zhang-etal-2025-bridge</bibkey>
    </paper>
    <paper id="568">
      <title><fixed-case>P</fixed-case>ro<fixed-case>B</fixed-case>ench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks</title>
      <author><first>Yan</first><last>Yang</last><affiliation>Australian National University and CSIRO</affiliation></author>
      <author><first>Dongxu</first><last>Li</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Haoning</first><last>Wu</last><affiliation>Rhymes AI</affiliation></author>
      <author><first>Bei</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Liu</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liyuan</first><last>Pan</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <pages>10883-10892</pages>
      <abstract>Solving expert-level multimodal tasks is a key milestone in general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to evolve, evaluation of frontier multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries encapsulating professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently collected from professionals based on their productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, they all face significant challenges in visual perception, textual understanding, domain knowledge, and advanced reasoning. Our benchmark is publicly accessible at <url>TBC</url>.</abstract>
      <url hash="8debbdea">2025.findings-acl.568</url>
      <bibkey>yang-etal-2025-probench</bibkey>
    </paper>
    <paper id="569">
      <title>2<fixed-case>M</fixed-case>-<fixed-case>BELEBELE</fixed-case>: Highly Multilingual Speech and <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Comprehension Dataset Download <fixed-case>PDF</fixed-case></title>
      <author><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <author><first>Bokai</first><last>Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Pierre</first><last>Andrews</last></author>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Necati Cihan</first><last>Camgoz</last><affiliation>Meta</affiliation></author>
      <author><first>Joe</first><last>Chuang</last><affiliation>FAIR</affiliation></author>
      <author><first>Jean</first><last>Maillard</last><affiliation>Meta Fundamental Artificial Intelligence Research (FAIR)</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta</affiliation></author>
      <author><first>Arina</first><last>Turkatenko</last><affiliation>Facebook</affiliation></author>
      <author><first>Carleigh</first><last>Wood</last></author>
      <pages>10893-10904</pages>
      <abstract>We introduce the first highly multilingual speech and American Sign Language (ASL) comprehension dataset by extending BELEBELE. Our dataset covers 91 spoken languages at the intersection of BELEBELE and FLEURS, and one sign language (ASL). As a by-product we also extend the Automatic Speech Recognition Benchmark, FLEURS, by 20%. We evaluate 2M-BELEBELE dataset for both 5-shot and zero-shot settings and across languages, the speech comprehension accuracy is ≈ 10% average lower compared to reading comprehension.</abstract>
      <url hash="0ff9b429">2025.findings-acl.569</url>
      <bibkey>costa-jussa-etal-2025-2m</bibkey>
    </paper>
    <paper id="570">
      <title><fixed-case>LSC</fixed-case>-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using <fixed-case>LLM</fixed-case>-Generated Synthetic Data</title>
      <author><first>Naomi</first><last>Baes</last></author>
      <author><first>Raphael</first><last>Merx</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Nick</first><last>Haslam</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Ekaterina</first><last>Vylomova</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>10905-10939</pages>
      <abstract>Lexical Semantic Change (LSC) provides insight into cultural and social dynamics. Yet, the validity of methods for measuring different kinds of LSC remains unestablished due to the absence of historical benchmark datasets. To address this gap, we propose LSC-Eval, a novel three-stage general-purpose evaluation framework to: (1) develop a scalable methodology for generating synthetic datasets that simulate theory-driven LSC using In-Context Learning and a lexical database; (2) use these datasets to evaluate the sensitivity of computational methods to synthetic change; and (3) assess their suitability for detecting change in specific dimensions and domains. We apply LSC-Eval to simulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions, as defined in the SIBling framework, using examples from psychology. We then evaluate the ability of selected methods to detect these controlled interventions. Our findings validate the use of synthetic benchmarks, demonstrate that tailored methods effectively detect changes along SIB dimensions, and reveal that a state-of-the-art LSC model faces challenges in detecting affective dimensions of LSC. LSC-Eval offers a valuable tool for dimension- and domain-specific benchmarking of LSC methods, with particular relevance to the social sciences.</abstract>
      <url hash="8640ad51">2025.findings-acl.570</url>
      <bibkey>baes-etal-2025-lsc</bibkey>
    </paper>
    <paper id="571">
      <title>Chain-of-Jailbreak Attack for Image Generation Models via Step by Step Editing</title>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kuiyi</first><last>Gao</last></author>
      <author><first>Youliang</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong-Shenzhen</affiliation></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Qiuzhi</first><last>Liu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>10940-10957</pages>
      <abstract>Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, including nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models’ safety against our CoJ attack method, we also propose an effective prompting-based method, Think-Twice Prompting, that can successfully defend over 95% of CoJ attack. Our dataset and code are included in the supplementary materials and will be made publicly available upon publication.</abstract>
      <url hash="e667d447">2025.findings-acl.571</url>
      <bibkey>wang-etal-2025-chain-jailbreak</bibkey>
    </paper>
    <paper id="572">
      <title>Tokenization is Sensitive to Language Variation</title>
      <author><first>Anna</first><last>Wegmann</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Dong</first><last>Nguyen</last><affiliation>Utrecht University</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>10958-10983</pages>
      <abstract>Variation in language is ubiquitous and often systematically linked to regional, social, and contextual factors. Tokenizers split texts into smaller units and might behave differently for less common linguistic forms. This might affect downstream LLM performance differently on two types of tasks: Tasks where the model should be robust to language variation (e.g., for semantic tasks like NLI, labels do not depend on whether a text uses British or American spelling) and tasks where the model should be sensitive to language variation (e.g., for form-based tasks like authorship verification, labels depend on whether a text uses British or American spelling). We pre-train BERT base models with the popular Byte-Pair Encoding algorithm to investigate how key tokenization design choices impact the performance of downstream models: the corpus used to train the tokenizer, the pre-tokenizer and the vocabulary size. We find that the best tokenizer varies on the two task types and that the pre-tokenizer has the biggest overall impact on performance. Further, we introduce a new approach to estimate tokenizer impact on downstream LLM performance, showing substantial improvement over metrics like Rényi efficiency. We encourage more work on language variation and its relation to tokenizers and thus LLM performance.</abstract>
      <url hash="a2f63b5e">2025.findings-acl.572</url>
      <bibkey>wegmann-etal-2025-tokenization</bibkey>
    </paper>
    <paper id="573">
      <title><fixed-case>W</fixed-case>ireless<fixed-case>M</fixed-case>ath<fixed-case>B</fixed-case>ench: A Mathematical Modeling Benchmark for <fixed-case>LLM</fixed-case>s in Wireless Communications</title>
      <author><first>Xin</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Mengbing</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Li</first><last>Wei</last></author>
      <author><first>Jiancheng</first><last>An</last></author>
      <author><first>Merouane Abdelkader</first><last>Debbah</last><affiliation>Khalifa University of Science, Technology and Research</affiliation></author>
      <author><first>Chau</first><last>Yuen</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>10984-11009</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning—particularly in wireless communications—remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.</abstract>
      <url hash="17336b78">2025.findings-acl.573</url>
      <bibkey>li-etal-2025-wirelessmathbench</bibkey>
    </paper>
    <paper id="574">
      <title>Self-Improvement Towards <fixed-case>P</fixed-case>areto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title>
      <author><first>Moxin</first><last>Li</last></author>
      <author><first>Yuantao</first><last>Zhang</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Wentao</first><last>Shi</last></author>
      <author><first>Zhuo</first><last>Liu</last></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>11010-11031</pages>
      <abstract>Multi-Objective Alignment (MOA) aims to align LLMs’ responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines</abstract>
      <url hash="af7e616b">2025.findings-acl.574</url>
      <bibkey>li-etal-2025-self-improvement</bibkey>
    </paper>
    <paper id="575">
      <title>Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training</title>
      <author><first>Zhijun</first><last>Wang</last></author>
      <author><first>Jiahuan</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Rongxiang</first><last>Weng</last><affiliation>Meituan</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xin</first><last>Huang</last><affiliation>China Mobile Communications Company Limited Research Institute</affiliation></author>
      <author><first>Xue</first><last>Han</last></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Chao</first><last>Deng</last><affiliation>China Mobile Research Institute</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>11032-11046</pages>
      <abstract>Large language models (LLMs) exhibit remarkable multilingual capabilities despite the extreme language imbalance in the pre-training data. In this paper, we closely examine the reasons behind this phenomenon, focusing on the pre-training corpus. We find that the existence of code-switching, alternating between different languages within a context, is key to multilingual capabilities. We conduct an analysis to investigate code-switching in the pre-training corpus, examining its presence and categorizing it into four types within two quadrants. We then assess its impact on multilingual performance. These types of code-switching data are unbalanced in proportions and demonstrate different effects on facilitating language transfer. To better explore the power of code-switching for language alignment during pre-training, we investigate the strategy of synthetic code-switching. We continuously scale up the synthetic code-switching data and observe remarkable improvements in both benchmarks and representation space. Extensive experiments indicate that incorporating synthetic code-switching data enables better language alignment and generalizes well to high, medium, and low-resource languages with pre-training corpora of varying qualities.</abstract>
      <url hash="7e9fb2c8">2025.findings-acl.575</url>
      <bibkey>wang-etal-2025-investigating-scaling</bibkey>
    </paper>
    <paper id="576">
      <title>User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sougata</first><last>Saha</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>11047-11065</pages>
      <abstract>Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework’s predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama.</abstract>
      <url hash="5f02c574">2025.findings-acl.576</url>
      <bibkey>saha-choudhury-2025-user</bibkey>
    </paper>
    <paper id="577">
      <title>Beyond Browsing: <fixed-case>API</fixed-case>-Based Web Agents</title>
      <author><first>Yueqi</first><last>Song</last></author>
      <author><first>Frank F.</first><last>Xu</last></author>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>11066-11085</pages>
      <abstract>Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing.However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask – *what if we were to take tasks traditionally tackled by Browsing Agents, and give AI agents access to APIs*?To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs.In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-Based Agents outperform web Browsing Agents.Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 24.0% absolute improvement over web browsing alone, achieving a success rate of 38.9%, the SOTA performance among task-agnostic agents.These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.</abstract>
      <url hash="041ec78b">2025.findings-acl.577</url>
      <bibkey>song-etal-2025-beyond</bibkey>
    </paper>
    <paper id="578">
      <title><fixed-case>M</fixed-case>i<fixed-case>L</fixed-case>i<fixed-case>C</fixed-case>-Eval: Benchmarking Multilingual <fixed-case>LLM</fixed-case>s for <fixed-case>C</fixed-case>hina’s Minority Languages</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Mingxu</first><last>Tao</last></author>
      <author><first>Zhiyuan</first><last>Liao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>11086-11102</pages>
      <abstract>Large language models (LLMs) excel in high-resource languages but struggle with low-resource languages (LRLs), particularly those spoken by minority communities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To systematically track the progress in these languages, we introduce MiLiC-Eval, a benchmark designed for minority languages in China, featuring 24K instances across 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its parallelism between tasks and languages can provide a faithful and fine-grained assessment of linguistic and problem-solving skills. Our evaluation reveals that open-source LLMs perform poorly on syntax-intensive tasks and multi-script languages. We further demonstrate how MiLiC-Eval can help advance LRL research in handling diverse writing systems and understanding the process of language adaptation.</abstract>
      <url hash="e178923d">2025.findings-acl.578</url>
      <bibkey>zhang-etal-2025-milic</bibkey>
    </paper>
    <paper id="579">
      <title><fixed-case>A</fixed-case>rg<fixed-case>I</fixed-case>nstruct: Specialized Instruction Fine-Tuning for Computational Argumentation</title>
      <author><first>Maja</first><last>Stahl</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <author><first>Timon</first><last>Ziegenbein</last><affiliation>Universität Hannover</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>11103-11127</pages>
      <abstract>Training large language models (LLMs) to follow instructions has significantly enhanced their ability to tackle unseen tasks. However, despite their strong generalization capabilities, instruction-following LLMs encounter difficulties when dealing with tasks that require domain knowledge. This work introduces a specialized instruction fine-tuning for the domain of computational argumentation (CA). The goal is to enable an LLM to effectively tackle any unseen CA tasks while preserving its generalization capabilities. Reviewing existing CA research, we crafted natural language instructions for 105 CA tasks to this end. On this basis, we developed a CA-specific benchmark for LLMs that allows for a comprehensive evaluation of LLMs’ capabilities in solving various CA tasks. We synthesized 52k CA-related instructions, adapting the self-instruct process to train a CA-specialized instruction-following LLM. Our experiments suggest that CA-specialized instruction fine-tuning significantly enhances the LLM on both seen and unseen CA tasks. At the same time, performance on the general NLP tasks of the SuperNI benchmark remains stable.</abstract>
      <url hash="527a3a32">2025.findings-acl.579</url>
      <bibkey>stahl-etal-2025-arginstruct</bibkey>
    </paper>
    <paper id="580">
      <title>Crabs: Consuming Resource via Auto-generation for <fixed-case>LLM</fixed-case>-<fixed-case>D</fixed-case>o<fixed-case>S</fixed-case> Attack under Black-box Settings</title>
      <author><first>Yuanhe</first><last>Zhang</last></author>
      <author><first>Zhenhong</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Xinyue</first><last>Wang</last></author>
      <author><first>Xiaojun</first><last>Jia</last><affiliation>Nanyang Technological University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Sen</first><last>Su</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>11128-11150</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks yet still are vulnerable to external threats, particularly LLM Denial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust computational resources and block services. However, existing studies predominantly focus on white-box attacks, leaving black-box scenarios underexplored. In this paper, we introduce Auto-Generation for LLM-DoS (<tex-math>\textbf{AutoDoS}</tex-math>) attack, an automated algorithm designed for black-box LLMs. AutoDoS constructs the DoS Attack Tree and expands the node coverage to achieve effectiveness under black-box conditions. By transferability-driven iterative optimization, AutoDoS could work across different models in one prompt.Furthermore, we reveal that embedding the Length Trojan allows AutoDoS to bypass existing defenses more effectively.Experimental results show that AutoDoS significantly amplifies service response latency by over <tex-math>\textbf{250}\times\uparrow</tex-math>, leading to severe resource consumption in terms of GPU utilization and memory usage. Our work provides a new perspective on LLM-DoS attacks and security defenses.</abstract>
      <url hash="0400ad2c">2025.findings-acl.580</url>
      <bibkey>zhang-etal-2025-crabs</bibkey>
    </paper>
    <paper id="581">
      <title>Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models</title>
      <author><first>Chenchen</first><last>Yuan</last></author>
      <author><first>Zheyu</first><last>Zhang</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Shuo</first><last>Yang</last></author>
      <author><first>Bardh</first><last>Prenkaj</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Gjergji</first><last>Kasneci</last><affiliation>Technische Universität München and University of Tuebingen</affiliation></author>
      <pages>11151-11168</pages>
      <abstract>Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs’ moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.</abstract>
      <url hash="e50a93fb">2025.findings-acl.581</url>
      <bibkey>yuan-etal-2025-probabilistic</bibkey>
    </paper>
    <paper id="582">
      <title>Unlocking Recursive Thinking of <fixed-case>LLM</fixed-case>s: Alignment via Refinement</title>
      <author><first>Haoke</first><last>Zhang</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>11169-11182</pages>
      <abstract>The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose <b>AvR</b>: <b>Alignment via Refinement</b>, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize <b>refinement-aware rewards</b>. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20% in win rate on AlpacaEval 2.0. Our code is available at Github .</abstract>
      <url hash="e97688ad">2025.findings-acl.582</url>
      <bibkey>zhang-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="583">
      <title><fixed-case>C</fixed-case>ita<fixed-case>L</fixed-case>aw: Enhancing <fixed-case>LLM</fixed-case> with Citations in Legal Domain</title>
      <author><first>Kepu</first><last>Zhang</last></author>
      <author><first>Weijie</first><last>Yu</last><affiliation>University of International Business and Economics</affiliation></author>
      <author><first>Sunhao</first><last>Dai</last></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11183-11196</pages>
      <abstract>In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs’ ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.</abstract>
      <url hash="ac76edbb">2025.findings-acl.583</url>
      <bibkey>zhang-etal-2025-citalaw</bibkey>
    </paper>
    <paper id="584">
      <title><fixed-case>MEG</fixed-case>en: Generative Backdoor into Large Language Models via Model Editing</title>
      <author><first>Jiyang</first><last>Qiu</last></author>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yun</first><last>Li</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Qianren</first><last>Wang</last></author>
      <pages>11197-11214</pages>
      <abstract>Large language models (LLMs) have exhibited remarkable versatility and adaptability, while their widespread adoption across various applications also raises critical safety concerns.This paper focuses on the impact of backdoored LLMs. Traditional backdoor injection methods are primarily limited to yes-or-no discriminative tasks, leading users to underestimate the potential risks of backdoored LLMs.Given the inherently generative nature of LLMs, this paper reveals that a generative backdoor injected into LLMs can expose the true safety risks in their applications. We propose an editing-based generative backdoor, named MEGen, aiming to expand the backdoor to generative tasks in a unified format of any text-to any text, leading to natural generations with a specific intention. Experiments show that MEGen achieves a high attack success rate by adjusting only a small set of local parameters with few-shot samples. Notably, we show that the backdoored model, when triggered, can freely output pre-set dangerous information while completing downstream tasks.Our work highlights that MEGen enables backdoors in LLMs to exhibit generative capabilities, causing potential safety risks by altering the generative style. The code is available at [https://github.com/MonoQ-hub/MEGen](https://github.com/MonoQ-hub/MEGen).</abstract>
      <url hash="246ece89">2025.findings-acl.584</url>
      <bibkey>qiu-etal-2025-megen</bibkey>
    </paper>
    <paper id="585">
      <title>Social Bias Benchmark for Generation: A Comparison of Generation and <fixed-case>QA</fixed-case>-Based Evaluations</title>
      <author><first>Jiho</first><last>Jin</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Woosung</first><last>Kang</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Junho</first><last>Myung</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>11215-11228</pages>
      <abstract>Measuring social bias in large language models (LLMs) is crucial, but existing bias evaluation methods struggle to assess bias in long-form generation. We propose a Bias Benchmark for Generation (BBG), an adaptation of the Bias Benchmark for QA (BBQ), designed to evaluate social bias in long-form generation by having LLMs generate continuations of story prompts. Building our benchmark in English and Korean, we measure the probability of neutral and biased generations across ten LLMs. We also compare our long-form story generation evaluation results with multiple-choice BBQ evaluation, showing that the two approaches produce inconsistent results.</abstract>
      <url hash="21f5c33f">2025.findings-acl.585</url>
      <bibkey>jin-etal-2025-social</bibkey>
    </paper>
    <paper id="586">
      <title>Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models</title>
      <author><first>Junling</first><last>Wang</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Anna</first><last>Rutkiewicz</last></author>
      <author><first>April</first><last>Wang</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>11229-11257</pages>
      <abstract>Visuals are valuable tools for teaching math word problems (MWPs), helping young learners interpret textual descriptions into mathematical expressions before solving them.However, creating such visuals is labor-intensive and there is a lack of automated methods to support this process. In this paper, we present Math2Visual, an automatic framework for generating pedagogically meaningful visuals from MWP text descriptions. Math2Visual leverages a pre-defined visual language and a design space grounded in interviews with math teachers, to illustrate the core mathematical relationships in MWPs.Using Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate Text-to-Image (TTI) models for their ability to generate visuals that align with our design. We further fine-tune several TTI models with our dataset, demonstrating improvements in educational visual generation. Our work establishes a new benchmark for automated generation of pedagogically meaningful visuals and offers insights into key challenges in producing multimodal educational content, such as the misrepresentation of mathematical relationships and the omission of essential visual elements.</abstract>
      <url hash="64051c64">2025.findings-acl.586</url>
      <bibkey>wang-etal-2025-generating-pedagogically</bibkey>
    </paper>
    <paper id="587">
      <title><fixed-case>RASP</fixed-case>berry: Retrieval-Augmented <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Self-Play with Reasoning Consistency for Multi-Hop Question Answering</title>
      <author><first>Baixuan</first><last>Li</last></author>
      <author><first>Yunlong</first><last>Fan</last><affiliation>Southeast University</affiliation></author>
      <author><first>Tianyi</first><last>Ma</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Miao</first><last>Gao</last><affiliation>Southeast university</affiliation></author>
      <author><first>Chuanqi</first><last>Shi</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zhiqiang</first><last>Gao</last><affiliation>Southeast University</affiliation></author>
      <pages>11258-11276</pages>
      <abstract>Complex multi-hop question answering requires large language models (LLMs) not only to retrieve external knowledge but also to reason over the retrieved information in order to arrive at the final solution. This involves two key challenges: (i) how to effectively explore the solution space and generate more potentially correct solution candidates, and (ii) how to select the optimal solution from multiple solution candidates, both of which require a training-free approach without introducing a more powerful teacher model. To address these challenges, we propose Retrieval-Augmented Monte Carlo Tree Self-Play with Reasoning Consistency (RASPberry), which introduces a more flexible action-level sampling granularity compared to existing methods, leverages Monte Carlo Tree Search for efficient solution space exploration, and utilizes an enhanced version of reasoning consistency to guide the selection of the optimal solution. Experimental results demonstrate that our proposed RASPberry effectively tackles the two challenges outlined above, achieving more efficient RAG inference-time scaling. Our code is available at https://github.com/BaixuanLi/RASPberry.</abstract>
      <url hash="6cf693d7">2025.findings-acl.587</url>
      <bibkey>li-etal-2025-raspberry</bibkey>
    </paper>
    <paper id="588">
      <title>All That Glitters is Not Gold: Improving Robust Retrieval-Augmented Language Models with Fact-Centric Preference Alignment</title>
      <author><first>Jia</first><last>Hao</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chunhong</first><last>Zhang</last></author>
      <author><first>Jiarun</first><last>Liu</last></author>
      <author><first>Haiyu</first><last>Zhao</last></author>
      <author><first>Zhiqiang</first><last>Zhan</last></author>
      <author><first>Zheng</first><last>Hu</last></author>
      <pages>11277-11292</pages>
      <abstract>Retrieval-augmented language model (RALM) relies on retrieved external knowledge to generate responses, resulting in vulnerability in the face of retrieval results with noisy documents. Previous works integrate additional filters or finetune Large Language Models (LLMs) to learn adaptive retrieval to reduce the performance damage of noisy documents. However, prior noise filtering may lead to the loss of crucial information, and these methods do not focus on distracting documents with high semantic relevance, which is the most challenging problem. In this study, we propose a training method for fact-centric preference alignment (FPA) to improve the ability of LLMs to directly extract useful information from noisy retrieval results without prior filtering. Our method performs positive document mining based on factual consistency and uses LLMs self-generated synthetic data as training data without manual annotation. We evaluate our FPA on four question answering benchmarks, and the experimental results demonstrate that our method achieves significant improvement with a small scale of training data.</abstract>
      <url hash="caddbdb1">2025.findings-acl.588</url>
      <bibkey>hao-etal-2025-glitters</bibkey>
    </paper>
    <paper id="589">
      <title><fixed-case>F</fixed-case>air<fixed-case>S</fixed-case>teer: Inference Time Debiasing for <fixed-case>LLM</fixed-case>s with Dynamic Activation Steering</title>
      <author><first>Yichen</first><last>Li</last></author>
      <author><first>Zhiting</first><last>Fan</last></author>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Xiaotang</first><last>Gai</last></author>
      <author><first>Luqi</first><last>Gong</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11293-11312</pages>
      <abstract>Large language models (LLMs) are prone to capturing biases from training corpus, leading to potential negative social impacts. Existing prompt-based debiasing methods exhibit instability due to their sensitivity to prompt changes, while fine-tuning-based techniques incur substantial computational overhead and catastrophic forgetting. In this paper, we propose FairSteer, a novel inference-time debiasing framework without requiring customized prompt design or model retraining. Motivated by the linear representation hypothesis, our preliminary investigation demonstrates that fairness-related features can be encoded into separable directions in the hidden activation space. FairSteer operates in three steps: biased activation detection, debiasing steering vector (DSV) computation, and dynamic activation steering. Specifically, it first trains a lightweight linear classifier to detect bias signatures in activations, and then computes DSVs as intervention directions derived from small contrastive prompt pairs. Subsequently, it performs debiasing by adjusting activations with DSVs in the inference stage. Comprehensive evaluation with six LLMs demonstrates the superiority of FairSteer across question-answering, counterfactual input evaluation and open-ended text generation tasks. Code will be released.</abstract>
      <url hash="077d0d93">2025.findings-acl.589</url>
      <bibkey>li-etal-2025-fairsteer</bibkey>
    </paper>
    <paper id="590">
      <title>Listen, Watch, and Learn to Feel: Retrieval-Augmented Emotion Reasoning for Compound Emotion Generation</title>
      <author><first>Zhuofan</first><last>Wen</last></author>
      <author><first>Zheng</first><last>Lian</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shun</first><last>Chen</last></author>
      <author><first>Hailiang</first><last>Yao</last></author>
      <author><first>Longjiang</first><last>Yang</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Bin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Jianhua</first><last>Tao</last><affiliation>Tsinghua University</affiliation></author>
      <pages>11313-11327</pages>
      <abstract>The ability to comprehend human emotion using multimodal large language models (MLLMs) is essential for advancing human-AI interaction and multimodal sentiment analysis. While psychology theory-based human annotations have contributed to multimodal emotion tasks, the subjective nature of emotional perception often leads to inconsistent annotations, limiting the robustness of current models. Addressing these challenges requires more fine-grained methods and evaluation frameworks. In this paper, we propose the Retrieval-Augmented Emotion Reasoning (RAER) framework, a plug-and-play module that enhances MLLMs’ ability to tackle compound and context-rich emotion tasks. To systematically evaluate model performance, we introduce the Stimulus-Armed Bandit (SAB) framework, designed to benchmark emotional reasoning capabilities. Additionally, we construct the Compound Emotion QA dataset, an AI-generated multimodal dataset aimed at strengthening emotion understanding in MLLMs. Experimental results demonstrate the effectiveness of RAER across both traditional benchmarks and SAB evaluations, highlighting its potential to enhance emotional intelligence in multimodal AI systems.</abstract>
      <url hash="1f71a529">2025.findings-acl.590</url>
      <bibkey>wen-etal-2025-listen</bibkey>
    </paper>
    <paper id="591">
      <title><fixed-case>GLTW</fixed-case>: Joint Improved Graph Transformer and <fixed-case>LLM</fixed-case> via Three-Word Language for Knowledge Graph Completion</title>
      <author><first>Kangyang</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuzhuo</first><last>Bai</last></author>
      <author><first>Cheng</first><last>Gao</last></author>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhu</first><last>Liu</last></author>
      <author><first>Yingli</first><last>Shen</last></author>
      <author><first>Zhitong</first><last>Wang</last></author>
      <author><first>Cunliang</first><last>Kong</last></author>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Yufei</first><last>Huang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Xuantang</first><last>Xiong</last></author>
      <author><first>Lei</first><last>Han</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>11328-11344</pages>
      <abstract>Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called <b>GLTW</b>, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (<b>iGT</b>) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency. Importantly, we combine iGT with an LLM that takes KG language prompts as input. Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.</abstract>
      <url hash="b3b2a5dc">2025.findings-acl.591</url>
      <bibkey>luo-etal-2025-gltw</bibkey>
    </paper>
    <paper id="592">
      <title>Learning to Select In-Context Demonstration Preferred by Large Language Model</title>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Shaocheng</first><last>Lan</last></author>
      <author><first>Lei</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yexin</first><last>Li</last><affiliation>State Key Laboratory of General Artificial Intelligence, BIGAI</affiliation></author>
      <author><first>Kan</first><last>Ren</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>11345-11360</pages>
      <abstract>In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks during inference using only a few demonstrations. However, ICL performance is highly dependent on the selection of these demonstrations. Recent work explores retrieval-based methods for selecting query-specific demonstrations, but these approaches often rely on surrogate objectives such as metric learning, failing to directly optimize ICL performance. Consequently, they struggle to identify truly beneficial demonstrations. Moreover, their discriminative retrieval paradigm is ineffective when the candidate pool lacks sufficient high-quality demonstrations. To address these challenges, we propose GenICL, a novel generative preference learning framework that leverages LLM feedback to directly optimize demonstration selection for ICL. Experiments on 19 datasets across 11 task categories demonstrate that GenICL achieves superior performance than existing methods in selecting the most effective demonstrations, leading to better ICL performance.</abstract>
      <url hash="64453164">2025.findings-acl.592</url>
      <bibkey>zhang-etal-2025-learning-select</bibkey>
    </paper>
    <paper id="593">
      <title>Beyond the Spelling Miracle: Investigating Substring Awareness in Character-Blind Language Models</title>
      <author><first>Cristiano</first><last>Ciaccio</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Marta</first><last>Sartor</last><affiliation>CNR</affiliation></author>
      <author><first>Alessio</first><last>Miaschi</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <pages>11361-11372</pages>
      <abstract>Correctly identifying characters and substrings of words should be a basic but essential ability of any Language Model that aims to proficiently understand and produce language. Despite so, the majority of Pre-trained Language Models (PLMs) are “character-blind” and struggle in spelling tasks, although they still seem to acquire some character knowledge during pre-training, a phenomenon dubbed Spelling Miracle. To shed light on this phenomenon, we systematically evaluate a range of PLMs with different parameter sizes using a controlled binary substring identification task. Through a series of experiments, we propose the first comprehensive investigation on where, when, and how a PLMs develop awareness of characters and substrings, with a particular linguistic focus on morphemic units such as prefixes, suffixes, and roots.</abstract>
      <url hash="5419e722">2025.findings-acl.593</url>
      <bibkey>ciaccio-etal-2025-beyond</bibkey>
    </paper>
    <paper id="594">
      <title><fixed-case>DEMO</fixed-case>: Reframing Dialogue Interaction with Fine-grained Element Modeling</title>
      <author><first>Minzheng</first><last>Wang</last></author>
      <author><first>Xinghua</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>11373-11401</pages>
      <abstract>Large language models (LLMs) enabled dialogue systems have become one of the central modes in human-machine interaction, which bring about vast amounts of conversation logs and increasing demand for dialogue generation. The dialogue’s life-cycle spans from <tex-math>\textit{Prelude}</tex-math> through <tex-math>\textit{Interlocution}</tex-math> to <tex-math>\textit{Epilogue}</tex-math>, encompassing rich dialogue elements. Despite large volumes of dialogue-related studies, there is a lack of systematic investigation into the dialogue stages to frame benchmark construction that covers comprehensive dialogue elements. This hinders the precise modeling, generation and assessment of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce a new research task—<tex-math>\textbf{D}</tex-math>ialogue <tex-math>\textbf{E}</tex-math>lement <tex-math>\textbf{MO}</tex-math>deling, including <tex-math>\textit{Element Awareness}</tex-math> and <tex-math>\textit{Dialogue Agent Interaction}</tex-math>, and propose a novel benchmark, <tex-math>\textbf{DEMO}</tex-math>, designed for a comprehensive dialogue modeling and assessment. On this basis, we further build the DEMO agent with the adept ability to model dialogue elements via imitation learning. Extensive experiments on DEMO indicate that current representative LLMs still have considerable potential for enhancement, and our DEMO agent performs well in both dialogue element modeling and out-of-domain tasks.</abstract>
      <url hash="8f0ca828">2025.findings-acl.594</url>
      <bibkey>wang-etal-2025-demo</bibkey>
    </paper>
    <paper id="595">
      <title><fixed-case>I</fixed-case>nfinite<fixed-case>ICL</fixed-case>: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation</title>
      <author><first>Bowen</first><last>Cao</last></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>11402-11415</pages>
      <abstract>In-context learning (ICL) is critical for large language models (LLMs), but its effectiveness is constrained by finite context windows, particularly in ultra-long contexts. To overcome this, we introduce **InfiniteICL**, a framework that parallels context and parameters in LLMs with short- and long-term memory in human cognitive systems, focusing on transforming temporary context knowledge into permanent parameter updates. This approach significantly reduces memory usage, maintains robust performance across varying input lengths, and theoretically enables infinite context integration through the principles of context knowledge elicitation, selection, and consolidation. Evaluations demonstrate that our method reduces context length by 90% while achieving 103% average performance of full-context prompting across fact recall, grounded reasoning, and skill acquisition tasks. When conducting sequential multi-turn transformations on complex, real-world contexts (with length up to 2M tokens), our approach surpasses full-context prompting while using only 0.4% of the original contexts. These findings highlight InfiniteICL’s potential to enhance the scalability and efficiency of LLMs by breaking the limitations of conventional context window sizes.</abstract>
      <url hash="f1e618ce">2025.findings-acl.595</url>
      <bibkey>cao-etal-2025-infiniteicl</bibkey>
    </paper>
    <paper id="596">
      <title><fixed-case>M</fixed-case>3<fixed-case>HG</fixed-case>: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations</title>
      <author><first>Qiao</first><last>Liang</last></author>
      <author><first>Ying</first><last>Shen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Tiantian</first><last>Chen</last></author>
      <author><first>Lin</first><last>Zhang</last><affiliation>Tongji University</affiliation></author>
      <pages>11416-11431</pages>
      <abstract>Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. Codes are available at https://anonymous.4open.science/r/M3HG-6B34.</abstract>
      <url hash="0260ca0f">2025.findings-acl.596</url>
      <bibkey>liang-etal-2025-m3hg</bibkey>
    </paper>
    <paper id="597">
      <title>Large Language Models Are Natural Video Popularity Predictors</title>
      <author><first>Pratik</first><last>Kayal</last></author>
      <author><first>Pascal</first><last>Mettes</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Nima</first><last>Dehmamy</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Minsu</first><last>Park</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <pages>11432-11464</pages>
      <abstract>Predicting video popularity is often framed as a supervised learning task, relying heavily on meta-information and aggregated engagement data. However, video popularity is shaped by complex cultural and social factors that such approaches often overlook. We argue that Large Language Models (LLMs), with their deep contextual awareness, can better capture these nuances. To bridge the gap between pixel-based video data and token-based LLMs, we convert frame-level visuals into sequential text representations using Vision-Language Models. This enables LLMs to process multimodal content—titles, frame-based descriptions, and captions—capturing both engagement intensity (view count) and geographic spread (number of countries where a video trends). On 13,639 popular videos, a supervised neural network using content embeddings achieves 80% accuracy, while our LLM-based approach reaches 82% without fine-tuning. Combining the neural network’s predictions with the LLM further improves accuracy to 85.5%. Moreover, the LLM generates interpretable, attribute-based explanations for its predictions. Manual validations confirm the quality of these hypotheses and address concerns about hallucinations in the video-to-text conversion process. Overall, our findings suggest that LLMs, equipped with text-based multimodal representations, offer a powerful, interpretable, and data-efficient solution for tasks requiring rich contextual insight, such as video popularity prediction.</abstract>
      <url hash="c44e7a3d">2025.findings-acl.597</url>
      <bibkey>kayal-etal-2025-large</bibkey>
    </paper>
    <paper id="598">
      <title><fixed-case>DELMAN</fixed-case>: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing</title>
      <author><first>Yi</first><last>Wang</last></author>
      <author><first>Fenghua</first><last>Weng</last></author>
      <author><first>Sibei</first><last>Yang</last></author>
      <author><first>Zhan</first><last>Qin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Wenjie</first><last>Wang</last></author>
      <pages>11465-11481</pages>
      <abstract>Large Language Models (LLMs) are widely applied in decision making, but their deployment is threatened by jailbreak attacks, where adversarial users manipulate model behavior to bypass safety measures. Existing defense mechanisms, such as safety fine-tuning and model editing, either require extensive parameter modifications or lack precision, leading to performance degradation on general tasks, which is unsuitable to post-deployment safety alignment. To address these challenges, we propose DELMAN (**D**ynamic **E**diting for **L**L**M**s J**A**ilbreak Defe**N**se), a novel approach leveraging direct model editing for precise, dynamic protection against jailbreak attacks. DELMAN directly updates a minimal set of relevant parameters to neutralize harmful behaviors while preserving the model’s utility. To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries. Experimental results demonstrate that DELMAN outperforms baseline methods in mitigating jailbreak attacks while preserving the model’s utility, and adapts seamlessly to new attack instances, providing a practical and efficient solution for post-deployment model protection.</abstract>
      <url hash="b6ddb104">2025.findings-acl.598</url>
      <bibkey>wang-etal-2025-delman</bibkey>
    </paper>
    <paper id="599">
      <title>You need to <fixed-case>MIMIC</fixed-case> to get <fixed-case>FAME</fixed-case>: Solving Meeting Transcript Scarcity with Multi-Agent Conversations</title>
      <author><first>Frederic</first><last>Kirstein</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Muneeb</first><last>Khan</last></author>
      <author><first>Jan Philip</first><last>Wahle</last><affiliation>University of Göttingen, Germany</affiliation></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <pages>11482-11525</pages>
      <abstract>Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 points in difficulty). These findings show FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.</abstract>
      <url hash="3466588c">2025.findings-acl.599</url>
      <bibkey>kirstein-etal-2025-need</bibkey>
    </paper>
    <paper id="600">
      <title>Code-Switching and Syntax: A Large-Scale Experiment</title>
      <author><first>Igor</first><last>Sterner</last></author>
      <author><first>Simone</first><last>Teufel</last><affiliation>Department of Computer Science and Technology (Formerly Computer Laboratory)</affiliation></author>
      <pages>11526-11533</pages>
      <abstract>The theoretical code-switching (CS) literature provides numerous pointwise investigations that aim to explain patterns in CS, i.e. why bilinguals switch language in certain positions in a sentence more often than in others. A resulting consensus is that CS can be explained by the syntax of the contributing languages. There is however no large-scale, multi-language, cross-phenomena experiment that tests this claim. When designing such an experiment, we need to make sure that the system that is predicting where bilinguals tend to switch has access only to syntactic information. We provide such an experiment here. Results show that syntax alone is sufficient for an automatic system to distinguish between sentences in minimal pairs of CS, to the same degree as bilingual humans. Furthermore, the learnt syntactic patterns generalise well to unseen language pairs.</abstract>
      <url hash="30c864f1">2025.findings-acl.600</url>
      <bibkey>sterner-teufel-2025-code</bibkey>
    </paper>
    <paper id="601">
      <title>Optima: Optimizing Effectiveness and Efficiency for <fixed-case>LLM</fixed-case>-Based Multi-Agent System</title>
      <author><first>Weize</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiarui</first><last>Yuan</last></author>
      <author><first>Chen</first><last>Qian</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>11534-11557</pages>
      <abstract>Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optimashows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B / 3.2 3B, achieving up to 2.8x performance gain with less than 10% tokens on tasks requiring heavy information exchange. Moreover, Optima’s efficiency gains enable more effective compute utilization during inference, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS.</abstract>
      <url hash="bb4bc9f1">2025.findings-acl.601</url>
      <bibkey>chen-etal-2025-optima</bibkey>
    </paper>
    <paper id="602">
      <title>Generating Domain-Specific Knowledge Graphs from Large Language Models</title>
      <author><first>Marinela</first><last>Parović</last></author>
      <author><first>Ze</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jinhua</first><last>Du</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>11558-11574</pages>
      <abstract>Knowledge graphs (KGs) have been a cornerstone of search and recommendation due to their ability to store factual knowledge about any domain in a structured form enabling easy search and retrieval. Large language models (LLMs) have shown impressive world knowledge across different benchmarks and domains but their knowledge is inconveniently scattered across their billions of parameters. In this paper, we propose a prompt-based method to construct domain-specific KGs by extracting knowledge solely from LLMs’ parameters. First, we use an LLM to create a schema for a specific domain, which contains a set of domain-representative entities and relations. After that, we use the schema to guide the LLM through an iterative data generation process equipped with Chain-of-Verification (CoVe) for increased data quality. Using this method, we construct KGs for two domains: books and landmarks, which we then evaluate against Wikidata, an open-source human-created KG. Our results show that LLMs can generate large domain-specific KGs containing tens of thousands of entities and relations. However, due to the increased hallucination rates as the procedure evolves, the utility of large-scale LLM-generated KGs in practical applications could remain limited.</abstract>
      <url hash="1cb744e3">2025.findings-acl.602</url>
      <bibkey>parovic-etal-2025-generating</bibkey>
    </paper>
    <paper id="603">
      <title>Large Language Models are Miscalibrated In-Context Learners</title>
      <author><first>Chengzu</first><last>Li</last></author>
      <author><first>Han</first><last>Zhou</last></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>Google DeepMind and University of Cambridge</affiliation></author>
      <pages>11575-11596</pages>
      <abstract>When adapting ICL with or without fine-tuning, we are curious about whether the instruction-tuned language model is able to achieve well-calibrated results without suffering from the problem of overconfidence (i.e., miscalibration) considering its strong instruction following ability, especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration. Through extensive controlled experiments, we observe that the miscalibration problem exists across all learning methods in low-resource setups. To achieve simultaneous gain for both in-task performance and calibration, we then study the potential of self-ensembling applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies) to make the predictions more calibrated and have comparable or even better performance. We find that self-ensembling with max probability produces robust and calibrated predictions. Our work reveals the potential calibration problem of using ICL despite the improvements in task performance and sheds light on which learning paradigm to choose. We also provide practical guidelines for choosing learning paradigms depending on whether the data has been seen by the model before and a worthwhile solution via self-ensembling on how to enhance both task performance and calibration of LMs, which we hope could encourage further study.</abstract>
      <url hash="014fc0b9">2025.findings-acl.603</url>
      <bibkey>li-etal-2025-large-language-models</bibkey>
    </paper>
    <paper id="604">
      <title><fixed-case>ST</fixed-case>e<fixed-case>C</fixed-case>a: Step-level Trajectory Calibration for <fixed-case>LLM</fixed-case> Agent Learning</title>
      <author><first>Hanlin</first><last>Wang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jian</first><last>Wang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Chak Tou</first><last>Leong</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>11597-11614</pages>
      <abstract>Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations or preference learning through exploratory trajectory sampling. However, these methods often struggle to address long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories.To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. We finally leverage these calibrated trajectories with successful trajectories for reinforced training.Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that timely calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.</abstract>
      <url hash="84411a91">2025.findings-acl.604</url>
      <bibkey>wang-etal-2025-steca</bibkey>
    </paper>
    <paper id="605">
      <title><fixed-case>LEMMA</fixed-case>: Learning from Errors for <fixed-case>M</fixed-case>athe<fixed-case>M</fixed-case>atical Advancement in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhuoshi</first><last>Pan</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Honglin</first><last>Lin</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Qizhi</first><last>Pei</last></author>
      <author><first>Zinan</first><last>Tang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Chenlin</first><last>Ming</last></author>
      <author><first>H. Vicky</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>11615-11639</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model’s reflective ability. Though some studies attempted to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes.In this work, we propose to enhance LLM’s reasoning ability by Learning from Errors for MatheMatical Advancement (LEMMA). LEMMA constructs data consists of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an _error-type grounded mistake augmentation_ method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. By fine-tuning on the constructed dataset, the model is able to _self-correct errors autonomously_ within the generation process _without relying on external critique models_. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong models with less than <tex-math>90k</tex-math> data.</abstract>
      <url hash="b964081f">2025.findings-acl.605</url>
      <bibkey>pan-etal-2025-lemma</bibkey>
    </paper>
    <paper id="606">
      <title>Voting or Consensus? Decision-Making in Multi-Agent Debate</title>
      <author><first>Lars Benedikt</first><last>Kaesberg</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Jonas</first><last>Becker</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Jan Philip</first><last>Wahle</last><affiliation>University of Göttingen, Germany</affiliation></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <pages>11640-11671</pages>
      <abstract>Much of the success of multi-agent debates depends on carefully choosing the right parameters. The decision-making protocol stands out as it can highly impact final model answers, depending on how decisions are reached. Systematic comparison of decision protocols is difficult because many studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making influences different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time - the decision protocol - to analyze how different methods affect the collaboration between agents and measure differences in knowledge and reasoning tasks. Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks compared to other decision protocols. Increasing the number of agents improves performance, while more discussion rounds before voting reduce it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.</abstract>
      <url hash="3eb86b63">2025.findings-acl.606</url>
      <bibkey>kaesberg-etal-2025-voting</bibkey>
    </paper>
    <paper id="607">
      <title>Rhetorical Device-Aware Sarcasm Detection with Counterfactual Data Augmentation</title>
      <author><first>Qingqing</first><last>Hong</last></author>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author><first>Jiayi</first><last>Lin</last></author>
      <author><first>Dapeng</first><last>Yin</last><affiliation>Tongji University</affiliation></author>
      <author><first>Shuyue</first><last>Zhu</last><affiliation>Tongji University</affiliation></author>
      <author><first>Junli</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>11672-11685</pages>
      <abstract>Sarcasm is a complex form of sentiment expression widely used in human daily life. Previous work primarily defines sarcasm as a form of verbal irony, which covers only a subset of real-world sarcastic expressions. However, sarcasm serves multifaceted functions and manifests itself through various rhetorical devices, such as echoic mention, rhetorical question and hyperbole. To fully capture its complexity, this paper investigates fine-grained sarcasm classification through the lens of rhetorical devices, and introduces <tex-math>\textbf{RedSD}</tex-math>, a <tex-math>\textbf{R}</tex-math>h<tex-math>\textbf{E}</tex-math>torical <tex-math>\textbf{D}</tex-math>evice-Aware <tex-math>\textbf{S}</tex-math>arcasm <tex-math>\textbf{D}</tex-math>ataset with counterfactually augmented data.To construct the dataset, we extract sarcastic dialogues from situation comedies (i.e., sitcoms), and summarize nine rhetorical devices commonly employed in sarcasm. We then propose a rhetorical device-aware counterfactual data generation pipeline facilitated by both Large Language Models (LLMs) and human revision. Additionally, we propose duplex counterfactual augmentation that generates counterfactuals for both sarcastic and non-sarcastic dialogues, to further enhance the scale and diversity of the dataset.Experimental results on the dataset demonstrate that fine-tuned models exhibit a more balanced performance compared to zero-shot models, including GPT-3.5 and LLaMA 3.1, underscoring the importance of integrating various rhetorical devices in sarcasm detection. Our dataset is avaliable at https://github.com/qqHong73/RedSD.</abstract>
      <url hash="ed35c9c4">2025.findings-acl.607</url>
      <bibkey>hong-etal-2025-rhetorical</bibkey>
    </paper>
    <paper id="608">
      <title>Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching</title>
      <author><first>Jianfei</first><last>Zhang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Bei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Jun</first><last>Bai</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Rumei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Yanmeng</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Wenge</first><last>Rong</last><affiliation>Beihang University</affiliation></author>
      <pages>11686-11704</pages>
      <abstract>In-Context Learning (ICL) empowers Large Language Models (LLMs) for rapid task adaptation without Fine-Tuning (FT), but its reliance on demonstration selection remains a critical challenge. While many-shot ICL shows promising performance through scaled demonstrations, the selection method for many-shot demonstrations remains limited to random selection in existing work. Since the conventional instance-level retrieval is not suitable for many-shot scenarios, we hypothesize that the data requirements for in-context learning and fine-tuning are analogous. To this end, we introduce a novel gradient matching approach that selects demonstrations by aligning fine-tuning gradients between the entire training set of the target task and the selected examples, so as to approach the learning effect on the entire training set within the selected examples. Through gradient matching on relatively small models, e.g., Qwen2.5-3B or Llama3-8B, our method consistently outperforms random selection on larger LLMs from 4-shot to 128-shot scenarios across 9 diverse datasets. For instance, it surpasses random selection by 4% on Qwen2.5-72B and Llama3-70B, and by around 2% on 5 closed-source LLMs. This work unlocks more reliable and effective many-shot ICL, paving the way for its broader application.</abstract>
      <url hash="fe0270b4">2025.findings-acl.608</url>
      <bibkey>zhang-etal-2025-selecting</bibkey>
    </paper>
    <paper id="609">
      <title>Cheap Character Noise for <fixed-case>OCR</fixed-case>-Robust Multilingual Embeddings</title>
      <author><first>Andrianos</first><last>Michail</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Juri</first><last>Opitz</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Yining</first><last>Wang</last><affiliation>GESIS – Leibniz Institute for the Social Sciences</affiliation></author>
      <author><first>Robin</first><last>Meister</last></author>
      <author><first>Rico</first><last>Sennrich</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Simon</first><last>Clematide</last><affiliation>University of Zurich</affiliation></author>
      <pages>11705-11716</pages>
      <abstract>The large amount of text collections digitized by imperfect OCR systems requires semantic search models that perform robustly on noisy input. Such collections are highly heterogeneous, with varying degrees of OCR quality, spelling conventions and other inconsistencies —all phenomena that are underrepresented in the training data of standard embedding models, with ramifications for their generalization. In our paper, we show that this problem can be alleviated with a simple and inexpensive method that does not require supervision or in-domain training. Specifically, we fine-tune existing multilingual models using noisy texts and a contrastive loss. We show that these models show considerable improvements across different noise conditions. Control experiments indicate minimal, and occasionally positive, impact on standard similarity tasks. These findings suggest that embedding models can be inexpensively adapted for cross-lingual semantic search in heterogeneous, digitized corpora. We publicly release our code, datasets, and models at https://github.com/impresso/ocr-robust-multilingual-embeddings.</abstract>
      <url hash="cf9ad34a">2025.findings-acl.609</url>
      <bibkey>michail-etal-2025-cheap</bibkey>
    </paper>
    <paper id="610">
      <title>Physics: Benchmarking Foundation Models on University-Level Physics Problem Solving</title>
      <author><first>Kaiyue</first><last>Feng</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Tianyu</first><last>Yang</last></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>NYU Shanghai</affiliation></author>
      <author><first>John</first><last>Sous</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>11717-11743</pages>
      <abstract>We introduce Physics, a comprehensive benchmark for university-level physics problem solving. It contains 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics.Each problem requires advanced physics knowledge and mathematical reasoning.We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems.Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.</abstract>
      <url hash="b81d98a4">2025.findings-acl.610</url>
      <bibkey>feng-etal-2025-physics</bibkey>
    </paper>
    <paper id="611">
      <title><fixed-case>DOVE</fixed-case>: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Eliya</first><last>Habba</last></author>
      <author><first>Ofir</first><last>Arviv</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Itay</first><last>Itzhak</last></author>
      <author><first>Yotam</first><last>Perlitz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elron</first><last>Bandel</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>Massachusetts Institute of Technology and International Business Machines</affiliation></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>11744-11763</pages>
      <abstract>Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation. Browse the data, contribute, and more at: https://slab-nlp.github.io/DOVE</abstract>
      <url hash="d8b04bc2">2025.findings-acl.611</url>
      <bibkey>habba-etal-2025-dove</bibkey>
    </paper>
    <paper id="612">
      <title><fixed-case>ALPS</fixed-case>: Attention Localization and Pruning Strategy for Efficient Adaptation of Large Language Models</title>
      <author><first>Hao</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haoze</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Xiao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lirong</first><last>Gao</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xiaomeng</first><last>Hu</last></author>
      <author><first>Ningtao</first><last>Wang</last></author>
      <author><first>Xing</first><last>Fu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11764-11780</pages>
      <abstract>Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant training adjustment costs. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the Attention Localization and Pruning Strategy ALPS, an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only 10% of attention parameters during fine-tuning while achieving a 2% performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment.</abstract>
      <url hash="c7d3b546">2025.findings-acl.612</url>
      <bibkey>chen-etal-2025-alps</bibkey>
    </paper>
    <paper id="613">
      <title><fixed-case>D</fixed-case>e<fixed-case>TAM</fixed-case>: Defending <fixed-case>LLM</fixed-case>s Against Jailbreak Attacks via Targeted Attention Modification</title>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Han</first><last>Jiang</last></author>
      <author><first>Zhihua</first><last>Wei</last><affiliation>Tongji University</affiliation></author>
      <pages>11781-11797</pages>
      <abstract>With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DeTAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize users’ core intentions, minimizing interference from attack tokens. Our experimental results demonstrate that DeTAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, we compare DeTAM with the baselines on over-defense datasets, further validating its superior balance between helpfulness and harmlessness.</abstract>
      <url hash="16151eaa">2025.findings-acl.613</url>
      <bibkey>li-etal-2025-detam</bibkey>
    </paper>
    <paper id="614">
      <title>A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges</title>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiamin</first><last>Su</last></author>
      <author><first>Jianxiang</first><last>He</last></author>
      <author><first>Fangteng</first><last>Fu</last></author>
      <author><first>Xu</first><last>Zheng</last></author>
      <author><first>Yuanhuiyi</first><last>Lyu</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Shen</first><last>Wang</last></author>
      <author><first>Qingsong</first><last>Wen</last><affiliation>Squirrel Ai Learning</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>11798-11827</pages>
      <abstract>Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides **the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs)**. We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.</abstract>
      <url hash="5cc65b20">2025.findings-acl.614</url>
      <bibkey>yan-etal-2025-survey</bibkey>
    </paper>
    <paper id="615">
      <title>Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors</title>
      <author><first>Andrei Catalin</first><last>Coman</last></author>
      <author><first>Christos</first><last>Theodoropoulos</last></author>
      <author><first>Marie-Francine</first><last>Moens</last><affiliation>KU Leuven, KU Leuven</affiliation></author>
      <author><first>James</first><last>Henderson</last><affiliation>Idiap Research Institute</affiliation></author>
      <pages>11828-11841</pages>
      <abstract>We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a Transformer-based framework that unifies textual and structural information for inductive link prediction in text-attributed knowledge graphs. We demonstrate that, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce the reliance on resource-intensive textual encoders. This makes the model both fast at training and inference time, as well as frugal in terms of cost. We perform a comprehensive evaluation on three popular datasets and show that FnF-TG can achieve superior performance compared to previous state-of-the-art methods. We also extend inductive learning to a fully inductive setting, where relations don’t rely on transductive (fixed) representations, as in previous work, but are a function of their textual description. Additionally, we introduce new variants of existing datasets, specifically designed to test the performance of models on unseen relations at inference time, thus offering a new test-bench for fully inductive link prediction.</abstract>
      <url hash="739e3d7d">2025.findings-acl.615</url>
      <bibkey>coman-etal-2025-fast</bibkey>
    </paper>
    <paper id="616">
      <title><fixed-case>N</fixed-case>eo<fixed-case>QA</fixed-case>: Evidence-based Question Answering with Generated News Events</title>
      <author><first>Max</first><last>Glockner</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Xiang</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon</affiliation></author>
      <pages>11842-11926</pages>
      <abstract>Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q&amp;A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.</abstract>
      <url hash="ef5e22d6">2025.findings-acl.616</url>
      <bibkey>glockner-etal-2025-neoqa</bibkey>
    </paper>
    <paper id="617">
      <title><fixed-case>C</fixed-case>hat<fixed-case>M</fixed-case>ap: Mining Human Thought Processes for Customer Service Chatbots via Multi-Agent Collaboration</title>
      <author><first>Xinyi</first><last>Jiang</last></author>
      <author><first>Tianyi</first><last>Hu</last></author>
      <author><first>Yuheng</first><last>Qin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Guoming</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Huan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kehan</first><last>Chen</last></author>
      <author><first>Gang</first><last>Huang</last></author>
      <author><first>Rongxing</first><last>Lu</last><affiliation>Queen’s University and University of New Brunswick</affiliation></author>
      <author><first>Siliang</first><last>Tang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11927-11947</pages>
      <abstract>Leveraging Large Language Models (LLMs) to build domain-specific conversational agents, especially for e-commerce customer service chatbots, is a growing focus. While existing methods enhance dialogue performance by extracting core patterns from dialogue data and integrating them into models, two key challenges persist: (1) heavy reliance on human experts for dialogue strategy induction, and (2) LLM-based automatic extraction often focuses on summarizing specific behaviors, neglecting the underlying thought processes behind strategy selection. In this paper, we present ChatMap, which focuses on enhancing customer service chatbots by mining thought processes using a Multi-Agent aPproach. Specifically, the process begins by extracting customer requests and solutions from a raw dialogue dataset, followed by clustering similar requests, analyzing the thought processes behind solutions, and refining service thoughts. Through a quality inspection and reflection mechanism, the final service thought dataset is generated, helping chatbots provide more appropriate responses. Offline experimental results show that ChatMap performs comparably to manually annotated thought processes and significantly outperforms other baselines, demonstrating its ability to automate human annotation and enhance dialogue capabilities through strategic understanding. Online A/B tests on Taobao, a popular e-commerce platform in China reveal that ChatMap can better improve customer satisfaction and address customer requests from a business perspective.</abstract>
      <url hash="48384bf6">2025.findings-acl.617</url>
      <bibkey>jiang-etal-2025-chatmap</bibkey>
    </paper>
    <paper id="618">
      <title>P3: Prompts Promote Prompting</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Yuanquan</first><last>Hu</last></author>
      <author><first>Fangchao</first><last>Liu</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11948-11965</pages>
      <abstract>Current large language model (LLM) applications often employ multi-component prompts, comprising both system and user prompts, to guide model behaviors. While recent advancements have demonstrated the efficacy of automatically optimizing either the system or user prompt to boost performance, such unilateral approaches often yield suboptimal outcomes due to the interdependent nature of these components. In this work, we introduce P3, a novel self-improvement framework that concurrently optimizes both system and user prompts through an iterative process. The offline optimized prompts are further leveraged to promote online prompting by performing query-dependent prompt optimization. Extensive experiments on general tasks (e.g., Arena-hard and Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3 achieves superior performance in the realm of automatic prompt optimization. Our results highlight the effectiveness of a holistic optimization strategy in enhancing LLM performance across diverse domains.</abstract>
      <url hash="195d45af">2025.findings-acl.618</url>
      <bibkey>zhang-etal-2025-p3</bibkey>
    </paper>
    <paper id="619">
      <title><fixed-case>VAQUUM</fixed-case>: Are Vague Quantifiers Grounded in Visual Data?</title>
      <author><first>Hugh Mee</first><last>Wong</last></author>
      <author><first>Rick</first><last>Nouwen</last></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <pages>11966-11982</pages>
      <abstract>Vague quantifiers such as “a few” and “many” are influenced by various contextual factors, including the number of objects present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20,300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes. We release our dataset and code at https://github.com/hughmee/vaquum.</abstract>
      <url hash="01653ccd">2025.findings-acl.619</url>
      <bibkey>wong-etal-2025-vaquum</bibkey>
    </paper>
    <paper id="620">
      <title>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</title>
      <author><first>William</first><last>Rudman</last></author>
      <author><first>Michal</first><last>Golovanevsky</last><affiliation>Brown University</affiliation></author>
      <author><first>Amir</first><last>Bar</last></author>
      <author><first>Vedant</first><last>Palit</last></author>
      <author><first>Yann</first><last>LeCun</last><affiliation>Facebook and New York University</affiliation></author>
      <author><first>Carsten</first><last>Eickhoff</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Ritambhara</first><last>Singh</last><affiliation>Brown University</affiliation></author>
      <pages>11983-11998</pages>
      <abstract>Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of “sides” nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o’s accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning.</abstract>
      <url hash="14556741">2025.findings-acl.620</url>
      <bibkey>rudman-etal-2025-forgotten</bibkey>
    </paper>
    <paper id="621">
      <title><fixed-case>M</fixed-case>ind<fixed-case>B</fixed-case>ridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality</title>
      <author><first>Shuaike</first><last>Li</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>11999-12013</pages>
      <abstract>Knowledge editing is a technique for efficiently and accurately updating the knowledge of large language models (LLMs) to alleviate obsolescence and correct errors. However, most existing methods overfit to specific models, causing edited knowledge to be discarded during each LLM update and requiring frequent re-editing, which is particularly burdensome in today’s rapidly evolving open-source community. To address this issue, we propose the problem of cross-model knowledge editing and introduce **MindBridge**, a scalable solution inspired by the low coupling between modality processing and LLMs in multi-modal models. MindBridge introduces the novel concept of **memory modality**, which encodes edited knowledge as an independent modality. It first performs LLM-agnostic pre-training of the memory modality and then integrates it with various LLMs. Extensive experiments on multiple LLMs and popular knowledge editing datasets demonstrate that MindBridge achieves superior performance even in editing tens of thousands of knowledge entries and can flexibly adapt to different LLMs. Our code is available at https://github.com/CrashBugger/MindBridge.</abstract>
      <url hash="701f3f81">2025.findings-acl.621</url>
      <bibkey>li-etal-2025-mindbridge</bibkey>
    </paper>
    <paper id="622">
      <title><fixed-case>FIHA</fixed-case>: Automated Fine-grained Hallucinations Evaluations in Large Vision Language Models with <fixed-case>D</fixed-case>avidson Scene Graphs</title>
      <author><first>Bowen</first><last>Yan</last></author>
      <author><first>Zhengsong</first><last>Zhang</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Liqiang</first><last>Jing</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Eftekhar</first><last>Hossain</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Xinya</first><last>Du</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>12014-12026</pages>
      <abstract>The rapid development of Large Vision-Language Models (LVLMs) often comes with widespread hallucination issues, making cost-effective and comprehensive assessments increasingly vital. Current approaches mainly rely on costly annotations and are not comprehensive – in terms of evaluating all aspects, such as relations, attributes, and dependencies between aspects. Therefore, we introduce the FIHA (automated Fine-graIned Hallucination evAluation in LVLMs), which could access LVLMs hallucination in an LLM-free and annotation-free way and model the dependency between different types of hallucinations. FIHA can generate Q&amp;A pairs on any image dataset at minimal cost, enabling hallucination assessment from both image and caption. Based on this approach, we introduce a benchmark called FIHA-v1, which consists of diverse questions on various images from three datasets. Furthermore, we use the Davidson Scene Graph (DSG) to organize the structure among Q&amp;A pairs, in which we can increase the reliability of the evaluation. We evaluate representative models using FIHA-v1, highlighting their limitations and challenges. We released our code and data at https://github.com/confidentzzzs/FIHA.</abstract>
      <url hash="d168c17b">2025.findings-acl.622</url>
      <bibkey>yan-etal-2025-fiha</bibkey>
    </paper>
    <paper id="623">
      <title>On the Role of Semantic Proto-roles in Semantic Analysis: What do <fixed-case>LLM</fixed-case>s know about agency?</title>
      <author><first>Elizabeth</first><last>Spaulding</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Shafiuddin Rehan</first><last>Ahmed</last></author>
      <author><first>James</first><last>Martin</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>12027-12048</pages>
      <abstract>Large language models (LLMs) are increasingly used in decision-making contexts, yet their ability to reason over event structure—an important component in the situational awareness needed to make complex decisions—is not well understood. By operationalizing proto-role theory, which characterizes agents via properties such as *instigation* and *volition* and patients via properties such as *change of state*, we examine the ability of LLMs to answer questions that require complex, multi-step event reasoning. Specifically, we investigate the extent to which LLMs capture semantic roles such as “agent” and “patient” through zero-shot prompts, and whether incorporating semantic proto-role labeling (SPRL) context improves semantic role labeling (SRL) performance in a zero-shot setting. We find that, while SPRL context sometimes degrades SRL accuracy in high-performing models (e.g., GPT-4o), it also uncovers an internal consistency between SPRL and SRL predictions that mirrors linguistic theory, and provides evidence that LLMs implicitly encode consistent multi-dimensional event role knowledge. Furthermore, our experiments support prior work showing that LLMs underperform human annotators in complex semantic analysis.</abstract>
      <url hash="b1a02e57">2025.findings-acl.623</url>
      <bibkey>spaulding-etal-2025-role</bibkey>
    </paper>
    <paper id="624">
      <title><fixed-case>G</fixed-case>e<fixed-case>AR</fixed-case>: Graph-enhanced Agent for Retrieval-augmented Generation</title>
      <author><first>Zhili</first><last>Shen</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Chenxin</first><last>Diao</last></author>
      <author><first>Pavlos</first><last>Vougiouklis</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Pascual</first><last>Merita</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shriram</first><last>Piramanayagam</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Enting</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Damien</first><last>Graux</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Andre</first><last>Melo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ruofei</first><last>Lai</last></author>
      <author><first>Zeren</first><last>Jiang</last></author>
      <author><first>Zhongyang</first><last>Li</last></author>
      <author><first>Ye</first><last>Qi</last></author>
      <author><first>Yang</first><last>Ren</last></author>
      <author><first>Dandan</first><last>Tu</last></author>
      <author><first>Jeff Z.</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>12049-12072</pages>
      <abstract>Retrieval-augmented Generation (RAG) relies on effective retrieval capabilities, yet traditional sparse and dense retrievers inherently struggle with multi-hop retrieval scenarios. In this paper, we introduce <tex-math>\text{G\small{E}\normalsize{AR}}</tex-math>, a system that advances RAG performance through two key innovations: (i) an efficient graph expansion mechanism that augments any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates the resulting graph-based retrieval into a multi-step retrieval framework. Our evaluation demonstrates <tex-math>\text{G\small{E}\normalsize{AR}}</tex-math>‘s superior retrieval capabilities across three multi-hop question answering datasets. Notably, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while consuming fewer tokens and requiring fewer iterations than existing multi-step retrieval systems. The project page is available at https://gear-rag.github.io.</abstract>
      <url hash="c8cc6918">2025.findings-acl.624</url>
      <bibkey>shen-etal-2025-gear</bibkey>
    </paper>
    <paper id="625">
      <title><fixed-case>W</fixed-case>eb<fixed-case>NLG</fixed-case>-<fixed-case>IT</fixed-case>: Construction of an aligned <fixed-case>RDF</fixed-case>-<fixed-case>I</fixed-case>talian corpus through Machine Translation techniques</title>
      <author><first>Michael</first><last>Oliverio</last></author>
      <author><first>Pier Felice</first><last>Balestrucci</last></author>
      <author><first>Alessandro</first><last>Mazzei</last><affiliation>University of Turin</affiliation></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <pages>12073-12083</pages>
      <abstract>The main goal of this work is the creation of the Italian version of the WebNLG corpus through the application of Neural Machine Translation (NMT) and post-editing with hand-written rules. To achieve this goal, in a first step, several existing NMT models were analysed and compared in order to identify the system with the highest performance on the original corpus. In a second step, after using the best NMT system, we semi-automatically designed and applied a number of rules to refine and improve the quality of the produced resource, creating a new corpus named WebNLG-IT. We used this resource for fine-tuning several LLMs for RDF-to-text tasks. In this way, comparing the performance of LLM-based generators on both Italian and English, we have (1) evaluated the quality of WebNLG-IT with respect to the original English version, (2) released the first fine-tuned LLM-based system for generating Italian from semantic web triples and (3) introduced an Italian version of a modular generation pipeline for RDF-to-text.</abstract>
      <url hash="182d8bc0">2025.findings-acl.625</url>
      <bibkey>oliverio-etal-2025-webnlg</bibkey>
    </paper>
    <paper id="626">
      <title>Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation</title>
      <author><first>Hanyin</first><last>Wang</last><affiliation>Mayo Clinic and UIUC</affiliation></author>
      <author><first>Chufan</first><last>Gao</last></author>
      <author><first>Bolun</first><last>Liu</last><affiliation>Mayo Clinic</affiliation></author>
      <author><first>Qiping</first><last>Xu</last><affiliation>Mayo Clinic</affiliation></author>
      <author><first>Guleid</first><last>Hussein</last><affiliation>Mayo Clinic</affiliation></author>
      <author><first>Mohamad El</first><last>Labban</last><affiliation>Mayo Clinic</affiliation></author>
      <author><first>Kingsley</first><last>Iheasirim</last></author>
      <author><first>Hariprasad Reddy</first><last>Korsapati</last></author>
      <author><first>Chuck</first><last>Outcalt</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <pages>12084-12117</pages>
      <abstract>Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (92.8%) of individual evaluations rated the notes generated by LLaMA-Clinic as “acceptable” or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging “Assessment and Plan” section, LLaMA-Clinic received the same score as the notes authored by physicians. We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice.</abstract>
      <url hash="e39f909a">2025.findings-acl.626</url>
      <bibkey>wang-etal-2025-towards-adapting</bibkey>
    </paper>
    <paper id="627">
      <title>Bridging Robustness and Generalization Against Word Substitution Attacks in <fixed-case>NLP</fixed-case> via the Growth Bound Matrix Approach</title>
      <author><first>Mohammed</first><last>Bouri</last><affiliation>University Mohammed VI Polytechnic</affiliation></author>
      <author><first>Adnane</first><last>Saoud</last><affiliation>University Mohammed VI Polytechnic</affiliation></author>
      <pages>12118-12137</pages>
      <abstract>Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to (8.8%) over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM</abstract>
      <url hash="beadc39c">2025.findings-acl.627</url>
      <bibkey>bouri-saoud-2025-bridging</bibkey>
    </paper>
    <paper id="628">
      <title>Neuro-Symbolic Query Compiler</title>
      <author><first>Yuyao</first><last>Zhang</last></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xiaoxi</first><last>Li</last></author>
      <author><first>Jiajie</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yongkang</first><last>Wu</last></author>
      <author><first>Zhonghua</first><last>Li</last></author>
      <author><first>Ye</first><last>Qi</last></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>12138-12155</pages>
      <abstract>Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents **QCompiler**, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically presents a minimal yet sufficient Backus-Naur Form (BNF) grammar <tex-math>G[q]</tex-math> to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a query expression translator, a Lexical syntax parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system’s ability to address complex queries.</abstract>
      <url hash="8551a17e">2025.findings-acl.628</url>
      <bibkey>zhang-etal-2025-neuro</bibkey>
    </paper>
    <paper id="629">
      <title>Revealing and Mitigating the Local Pattern Shortcuts of Mamba</title>
      <author><first>WangJie</first><last>You</last></author>
      <author><first>Zecheng</first><last>Tang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Lili</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>12156-12178</pages>
      <abstract>Large language models (LLMs) have advanced significantly due to the attention mechanism, but their quadratic complexity and linear memory demands limit their performance on long-context tasks. Recently, researchers introduced Mamba, an advanced model built upon State Space Models (SSMs) that offers linear complexity and constant memory. Although Mamba is reported to match or surpass the performance of attention-based models, our analysis reveals a performance gap: Mamba excels in tasks that involve localized key information but faces challenges with tasks that require handling distributed key information. Our controlled experiments suggest that the inconsistency arises from Mamba’s reliance on **local pattern shortcuts** across model scales (10M to 1.4B), which enable Mamba to remember local key information within its limited memory but hinder its ability to retain more dispersed information. Therefore, we introduce a global gate module into the Mamba model to address this issue. Experiments on extensive synthetic tasks, as well as real-world tasks, demonstrate the effectiveness of our method. Notably, with the introduction of only 4M extra parameters, our approach enables the Mamba model (130M) to achieve a significant improvement on tasks with distributed information, increasing its performance from **below 5% to 80%**.</abstract>
      <url hash="a6c040a0">2025.findings-acl.629</url>
      <bibkey>you-etal-2025-revealing</bibkey>
    </paper>
    <paper id="630">
      <title>Forget the Token and Pixel: Rethinking Gradient Ascent for Concept Unlearning in Multimodal Generative Models</title>
      <author><first>Jiaqi</first><last>Li</last></author>
      <author><first>Chuanyi</first><last>Zhang</last><affiliation>Hohai University</affiliation></author>
      <author><first>Miaozeng</first><last>Du</last></author>
      <author><first>Hui</first><last>Zhang</last><affiliation>VIPSHOP</affiliation></author>
      <author><first>Yongrui</first><last>Chen</last><affiliation>Southeast University</affiliation></author>
      <author><first>Qianshan</first><last>Wei</last></author>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Ruipeng</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Sheng</first><last>Bi</last><affiliation>Southeast University</affiliation></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <pages>12179-12200</pages>
      <abstract>Gradient Ascent (GA) has emerged as a promising approach for concept unlearning in Multimodal Generative Models (MGMs), such as Multimodal Large Language Models (MLLMs) and Stable Diffusion Models (SDMs). Despite its effectiveness in removing undesired knowledge, GA leads to severe utility degradation in MGMs. In this paper, we explore the mechanism behind this degradation by quantifying two distinct forms of knowledge in MGMs: (i) Conceptual Knowledge, which represents specific information about concepts; (ii) Natural Knowledge, which refers to the ability to produce coherent and logically structured outputs. Our analysis reveals that applying GA globally not only removes the targeted Conceptual Knowledge but also inadvertently diminishes Natural Knowledge, resulting in utility collapse. To address this issue, we propose Forget the Token and Pixel (FTTP), a novel approach that selectively applies GA to targeted Conceptual Knowledge while preserving Natural Knowledge through Gradient Descent (GD). FTTP eliminates the need for additional retain sets and a large number of training steps, thereby reducing computational resource costs. Extensive experiments demonstrate FTTP’s efficiency and superior utility-unlearning tradeoff for both text and image generation tasks. Our source code will be released in the near future.</abstract>
      <url hash="e67aded2">2025.findings-acl.630</url>
      <bibkey>li-etal-2025-forget</bibkey>
    </paper>
    <paper id="631">
      <title>Slamming: Training a Speech Language Model on One <fixed-case>GPU</fixed-case> in a Day</title>
      <author><first>Gallil</first><last>Maimon</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Avishai</first><last>Elmakies</last></author>
      <author><first>Yossi</first><last>Adi</last><affiliation>Hebrew University of Jerusalem and Meta</affiliation></author>
      <pages>12201-12216</pages>
      <abstract>We introduce *Slam*, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .</abstract>
      <url hash="903c3856">2025.findings-acl.631</url>
      <bibkey>maimon-etal-2025-slamming</bibkey>
    </paper>
    <paper id="632">
      <title>Boosting <fixed-case>LLM</fixed-case> Translation Skills without General Ability Loss via Rationale Distillation</title>
      <author><first>Junhong</first><last>Wu</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yangyifan</first><last>Xu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Bing</first><last>Liu</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>12217-12236</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results across numerous NLP tasks, and fine-tuning them for Machine Translation (MT) has improved their performance. However, vanilla fine-tuning often leads to catastrophic forgetting, compromising the broad general abilities of LLMs and introducing potential security risks. These abilities, which are developed using proprietary and unavailable training data, make simple data replay methods ineffective. To overcome this issue, we propose a novel approach called **Ra**tionale **Dis**tillation. RaDis harnesses the strong generative capabilities of LLMs to create rationales for training data, which are then “replayed” to prevent forgetting. These rationales connect prior knowledge with new tasks, acting as self-distillation targets to regulate the training process. By jointly training on reference translations and self-generated rationales, the model can learn new translation skills while preserving its general abilities across other tasks. Additionally, RaDis provides a fresh perspective on using rationales in the CL field and has the potential to serve as a general continual learning method for a variety of tasks.</abstract>
      <url hash="2d5d26d4">2025.findings-acl.632</url>
      <bibkey>wu-etal-2025-boosting</bibkey>
    </paper>
    <paper id="633">
      <title>Clarifying Underspecified Discourse Relations in Instructional Texts</title>
      <author><first>Berfin</first><last>Aktas</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>12237-12256</pages>
      <abstract>Discourse relations contribute to the structure of a text and can optionally be realized through explicit connectives such as “but” and “while”. But when are these connectives necessary to avoid possible misunderstandings? We investigate this question by first building a corpus of 4,274 text revisions in each of which a connective was explicitly inserted. For a subset of 250 cases, we collect plausibility annotations on other connectives to check whether they would represent suitable alternative relations. The results of this annotation show that several relations are often perceived as plausible in our data. Furthermore, we analyze the extent to which large language models can identify instances with multiple plausible relations as a possible source of misunderstandings. We find that the models predict plausibility of individual connectives with up to 66% accuracy, but they are not reliable in estimating when multiple relations are plausible.</abstract>
      <url hash="1ef60cae">2025.findings-acl.633</url>
      <bibkey>aktas-roth-2025-clarifying</bibkey>
    </paper>
    <paper id="634">
      <title><fixed-case>WMT</fixed-case>24++: Expanding the Language Coverage of <fixed-case>WMT</fixed-case>24 to 55 Languages &amp; Dialects</title>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>Eleftheria</first><last>Briakou</last><affiliation>Google</affiliation></author>
      <author><first>Isaac Rayburn</first><last>Caswell</last><affiliation>Google</affiliation></author>
      <author><first>Mara</first><last>Finkelstein</last><affiliation>Google</affiliation></author>
      <author><first>Rebecca</first><last>Galor</last><affiliation>Reichman University</affiliation></author>
      <author><first>Juraj</first><last>Juraska</last><affiliation>Google</affiliation></author>
      <author><first>Geza</first><last>Kovacs</last><affiliation>Google</affiliation></author>
      <author><first>Alison</first><last>Lui</last><affiliation>Google</affiliation></author>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Unbabel</affiliation></author>
      <author><first>Jason</first><last>Riesa</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Shruti</first><last>Rijhwani</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Parker</first><last>Riley</last><affiliation>Google</affiliation></author>
      <author><first>Elizabeth</first><last>Salesky</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Firas</first><last>Trabelsi</last><affiliation>Google</affiliation></author>
      <author><first>Stephanie</first><last>Winkler</last><affiliation>DeepMind</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <pages>12257-12284</pages>
      <abstract>As large language models (LLM) become more and more capable in languages other than English, it is important to collect benchmark datasets in order to evaluate their multilingual performance, including on tasks like machine translation (MT). In this work, we extend the WMT24 dataset to cover 55 languages by collecting new human-written references and post-edits for 46 new languages/dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. We benchmark a variety of MT providers and LLMs on the collected dataset using automatic metrics and find that LLMs are the best-performing MT systems in all 55 languages. However, we caution against using our results to reach strong conclusions about MT quality without a human-based evaluation due to limitations of automatic evaluation metrics, which we leave for future work.</abstract>
      <url hash="d883b854">2025.findings-acl.634</url>
      <bibkey>deutsch-etal-2025-wmt24</bibkey>
    </paper>
    <paper id="635">
      <title>Exploring Graph Representations of Logical Forms for Language Modeling</title>
      <author><first>Michael</first><last>Sullivan</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>12285-12307</pages>
      <abstract>We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the <tex-math>\underline{G}\textit{raph-based }\underline{Fo}\textit{rmal-}\underline{L}\textit{ogical }\underline{D}\textit{istributional }\underline{S}\textit{emantics}</tex-math> (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs (BERT) pretrained on the same data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.</abstract>
      <url hash="73337e95">2025.findings-acl.635</url>
      <bibkey>sullivan-2025-exploring</bibkey>
    </paper>
    <paper id="636">
      <title><fixed-case>SEA</fixed-case>-<fixed-case>HELM</fixed-case>: <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sian Holistic Evaluation of Language Models</title>
      <author><first>Yosephine</first><last>Susanto</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Adithya Venkatadri</first><last>Hulagadri</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jann Railey</first><last>Montalan</last><affiliation>AI Singapore and Ateneo de Manila University</affiliation></author>
      <author><first>Jian Gang</first><last>Ngui</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Xianbin</first><last>Yong</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Wei Qi</first><last>Leong</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Hamsawardhini</first><last>Rengarajan</last></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Yifan</first><last>Mai</last><affiliation>Stanford University</affiliation></author>
      <author><first>William Chandra</first><last>Tjhi</last><affiliation>AI Singapore</affiliation></author>
      <pages>12308-12336</pages>
      <abstract>With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multiculturalbenchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specificcapabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA)region, a comprehensive and culturally representative evaluation suite for the SEA languages has not been developed thus far.Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasises SEA languages, comprisingfive core pillars: (1) NLP CLASSICS, (2) LLM-SPECIFICS, (3) SEA LINGUISTICS, (4) SEA CULTURE, (5) SAFETY. SEA-HELMcurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models’ multilingual and multicultural performance in a systematic and user-friendly manner. We make the SEA-HELM evaluation code publicly available.</abstract>
      <url hash="b08510fa">2025.findings-acl.636</url>
      <bibkey>susanto-etal-2025-sea</bibkey>
    </paper>
    <paper id="637">
      <title><fixed-case>TRANS</fixed-case>-<fixed-case>ZERO</fixed-case>: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data</title>
      <author><first>Wei</first><last>Zou</last></author>
      <author><first>Sen</first><last>Yang</last></author>
      <author><first>Yu</first><last>Bao</last><affiliation>ByteDance Research</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shanbo</first><last>Cheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>12337-12347</pages>
      <abstract>The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework’s success.</abstract>
      <url hash="6bf326a0">2025.findings-acl.637</url>
      <bibkey>zou-etal-2025-trans</bibkey>
    </paper>
    <paper id="638">
      <title>A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of <fixed-case>CLIPS</fixed-case>core Quality Estimates</title>
      <author><first>Goncalo Emanuel Cavaco</first><last>Gomes</last></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <pages>12348-12365</pages>
      <abstract>This study explores current limitations of learned image captioning evaluation metrics, specifically the lack of granular assessments for errors within captions, and the reliance on single-point quality estimates without considering uncertainty. To address the limitations, we propose a simple yet effective strategy for generating and calibrating distributions of CLIPScore values. Leveraging a model-agnostic conformal risk control framework, we calibrate CLIPScore values for task-specific control variables, tackling the aforementioned limitations. Experimental results demonstrate that using conformal risk control, over score distributions produced with simple methods such as input masking, can achieve competitive performance compared to more complex approaches. Our method effectively detects erroneous words, while providing formal guarantees aligned with desired risk levels. It also improves the correlation between uncertainty estimations and prediction errors, thus enhancing the overall reliability of caption evaluation metrics.</abstract>
      <url hash="9b65aff8">2025.findings-acl.638</url>
      <bibkey>gomes-etal-2025-conformal</bibkey>
    </paper>
    <paper id="639">
      <title><fixed-case>SGDPO</fixed-case>: Self-Guided Direct Preference Optimization for Language Model Alignment</title>
      <author><first>Wenqiao</first><last>Zhu</last></author>
      <author><first>Ji</first><last>Liu</last><affiliation>Hithink RoyalFlush Information Network Co., Ltd.</affiliation></author>
      <author><first>Lulu</first><last>Wang</last></author>
      <author><first>Jun</first><last>Wu</last><affiliation>Zhejiang RoyalFlush Network Technology Co., Ltd.</affiliation></author>
      <author><first>Yulun</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>12366-12383</pages>
      <abstract>Direct Preference Optimization (DPO) is broadly utilized for aligning Large Language Models (LLMs) with human values because of its flexibility. Despite its effectiveness, it has been observed that the capability of DPO to generate human-preferred response is limited and the results of DPO are far from resilient. To address these limitations, in this paper we propose a novel Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which incorporates a pilot term to steer the gradient flow during the optimization process, allowing for fine-grained control over the updates of chosen and rejected rewards. We provide a detailed theoretical analysis of our proposed method and elucidate its operational mechanism. Furthermore, we conduct comprehensive experiments on various models and benchmarks. The extensive experimental results demonstrate the consistency between the empirical results and our theoretical analysis and confirm the effectiveness of our proposed approach (up to 9.19% higher score).</abstract>
      <url hash="397289c9">2025.findings-acl.639</url>
      <bibkey>zhu-etal-2025-sgdpo</bibkey>
    </paper>
    <paper id="640">
      <title>Socratic Style Chain-of-Thoughts Help <fixed-case>LLM</fixed-case>s to be a Better Reasoner</title>
      <author><first>Jiangbo</first><last>Pei</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Peiyu</first><last>Liu</last><affiliation>University of International Business and Economics</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Aidong</first><last>Men</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>12384-12395</pages>
      <abstract>Synthetic data generation has emerged as a promising approach to enhance the reasoning capabilities of large language models. However, existing methods remain hindered by high costs—either through expensive API access or additional intermediate training—and are limited in their ability to generalize across different domains. To address these challenges, we propose a multi-agent debate framework based on the Socratic questioning strategy, abbreviated as SoDa. Distinguished from previous methods that prioritize data quantity, we highlight the wisdom of Socratic questioning in augmenting reasoning quality by deepening the thinking process to encourage exploration and broadening it to motivate self-reflection on each question. Combined with our efficient production pipeline, SoDa enables scaling while maintaining affordable costs. We use SoDa to generate diverse datasets for mathematics and code generation tasks with the Qwen2.5-7B-Instruct model, successfully fine-tuning a range of foundation models, from general-purpose ones to OpenAI o1-like ones. For mathematics, the experimental results show that SoDa outperforms the performance of existing datasets at the same scale, achieving improvements ranging from 1.3% to 13.5%. Remarkably, SoDa with 30K examples even surpasses the ScaleQuest dataset with 1000K samples, demonstrating significant efficiency. Our findings highlight the potential of SoDa as a universal, scalable, and cost-effective method for enhancing reasoning capabilities in large models across domains.</abstract>
      <url hash="f7163a0d">2025.findings-acl.640</url>
      <bibkey>pei-etal-2025-socratic</bibkey>
    </paper>
    <paper id="641">
      <title>Quantile Regression with Large Language Models for Price Prediction</title>
      <author><first>Nikhita</first><last>Vedula</last><affiliation>Amazon</affiliation></author>
      <author><first>Dushyanta</first><last>Dhyani</last><affiliation>Amazon</affiliation></author>
      <author><first>Laleh</first><last>Jalali</last><affiliation>Amazon</affiliation></author>
      <author><first>Boris N.</first><last>Oreshkin</last></author>
      <author><first>Mohsen</first><last>Bayati</last><affiliation>Stanford University, Stanford University and Stanford University</affiliation></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>12396-12415</pages>
      <abstract>Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods.We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical.We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration.Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods.Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at https://github.com/vnik18/llm-price-quantile-reg/ to support future research.</abstract>
      <url hash="f56205b3">2025.findings-acl.641</url>
      <bibkey>vedula-etal-2025-quantile</bibkey>
    </paper>
    <paper id="642">
      <title>Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of <fixed-case>LLM</fixed-case>s as Your Coding Tutors</title>
      <author><first>Jian</first><last>Wang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yinpei</first><last>Dai</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yichi</first><last>Zhang</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Ziqiao</first><last>Ma</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Joyce</first><last>Chai</last><affiliation>University of Michigan</affiliation></author>
      <pages>12416-12436</pages>
      <abstract>Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized knowledge in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students towards completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student’s knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our approach can be extended beyond coding, providing valuable insights into advancing tutoring agents for human task learning.</abstract>
      <url hash="b2f082b2">2025.findings-acl.642</url>
      <bibkey>wang-etal-2025-training</bibkey>
    </paper>
    <paper id="643">
      <title><fixed-case>AIG</fixed-case>uard: A Benchmark and Lightweight Detection for <fixed-case>E</fixed-case>-commerce <fixed-case>AIGC</fixed-case> Risks</title>
      <author><first>Wenhua</first><last>Zhang</last></author>
      <author><first>Weicheng</first><last>Li</last></author>
      <author><first>Xuanrong</first><last>Rao</last></author>
      <author><first>Lixin</first><last>Zou</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Xiangyang</first><last>Luo</last><affiliation>State Key Lab of Mathematical Engineering and Advanced Computing</affiliation></author>
      <author><first>Chubin</first><last>Zhuang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongjie</first><last>Hong</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhen</first><last>Qin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hengyu</first><last>Chang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chenliang</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>12437-12450</pages>
      <abstract>Recent advancements in AI-generated content (AIGC) have heightened concerns about harmful outputs, such as misinformation and malicious misuse.Existing detection methods face two key limitations:(1) lacking real-world AIGC scenarios and corresponding risk datasets, and(2) both traditional and multimodal large language models (MLLMs) struggle to detect risks in AIGC.Towards this end, we introduce **AIGuard**, the first benchmark for AIGC risk detection in real-world e-commerce. It includes 253,420 image-text pairs (i.e., the risk content and risk description) across four critical categories: *abnormal body*, *violating physical laws*, *misleading or illogical context*, and *harmful or problematic message*.To effectively detect these risks, we propose distilling text annotations into dense soft prompts and identifying risk content through image soft prompt matching during inference.Experiments on the benchmark show that this method achieves a 9.68% higher recall than leading multimodal models while using only 25% of the training resources and improving inference speed by 37.8 times.For further research, our benchmark and code are available at [https://github.com/wenh-zhang/aiguard-dataset](https://github.com/wenh-zhang/aiguard-dataset).</abstract>
      <url hash="54c084cc">2025.findings-acl.643</url>
      <bibkey>zhang-etal-2025-aiguard</bibkey>
    </paper>
    <paper id="644">
      <title><fixed-case>A</fixed-case><tex-math>^2</tex-math><fixed-case>ATS</fixed-case>: Retrieval-Based <fixed-case>KV</fixed-case> Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization</title>
      <author><first>Junhui</first><last>He</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Junna</first><last>Xing</last></author>
      <author><first>Nan</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Rui</first><last>Xu</last></author>
      <author><first>Shangyu</first><last>Wu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Peng</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chun Jason</first><last>Xue</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Qingan</first><last>Li</last></author>
      <pages>12451-12463</pages>
      <abstract>Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache.Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference.However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead.To address these limitations, this paper proposes A<tex-math>^2</tex-math>ATS, a novel retrieval-based KV cache reduction method.A<tex-math>^2</tex-math>ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens.First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding.Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly.Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes.Experimental results demonstrate that A<tex-math>^2</tex-math>ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to <tex-math>2.7 \times</tex-math>.</abstract>
      <url hash="01ad43ef">2025.findings-acl.644</url>
      <bibkey>he-etal-2025-a2ats</bibkey>
    </paper>
    <paper id="645">
      <title><fixed-case>T</fixed-case>rans<fixed-case>B</fixed-case>ench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments</title>
      <author><first>Yuheng</first><last>Lu</last></author>
      <author><first>Qian</first><last>Yu</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Wei</first><last>Su</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yanping</first><last>Liu</last><affiliation>BIT</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <author><first>Maocheng</first><last>Liang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yunhong</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>12464-12478</pages>
      <abstract>Graphical User Interface (GUI) agents, which autonomously operate on digital interfaces through natural language instructions, hold transformative potential for accessibility, automation, and user experience. A critical aspect of their functionality is grounding — the ability to map linguistic intents to visual and structural interface elements. However, existing GUI agents often struggle to adapt to the dynamic and interconnected nature of real-world digital environments, where tasks frequently span multiple platforms and applications while also being impacted by version updates. To address this, we introduce TransBench, the first benchmark designed to systematically evaluate and enhance the transferability of GUI agents across three key dimensions: cross-version transferability (adapting to version updates), cross-platform transferability (generalizing across platforms like iOS, Android, and Web), and cross-application transferability (handling tasks spanning functionally distinct apps). TransBench includes 15 app categories with diverse functionalities, capturing essential pages across versions and platforms to enable robust evaluation. Our experiments demonstrate significant improvements in grounding accuracy, showcasing the practical utility of GUI agents in dynamic, real-world environments. Our code and data will be publicly available at GitHub.</abstract>
      <url hash="3bd6128f">2025.findings-acl.645</url>
      <bibkey>lu-etal-2025-transbench</bibkey>
    </paper>
    <paper id="646">
      <title>Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following</title>
      <author><first>Jie</first><last>Zeng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qianyu</first><last>He</last></author>
      <author><first>Qingyu</first><last>Ren</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Weikang</first><last>Zhou</last></author>
      <author><first>Zeye</first><last>Sun</last></author>
      <author><first>Fei</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>12479-12492</pages>
      <abstract>Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a “hard-to-easy” order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM’s attention and constraint orders. Our code and dataset are publicly available at https://github.com/meowpass/PBIF.</abstract>
      <url hash="8d671df0">2025.findings-acl.646</url>
      <bibkey>zeng-etal-2025-order</bibkey>
    </paper>
    <paper id="647">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>-<fixed-case>VTM</fixed-case>: Visual-to-Music Generation with Chain-of-Thought Reasoning</title>
      <author><first>Xikang</first><last>Guan</last></author>
      <author><first>Zheng</first><last>Gu</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Jing</first><last>Huo</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Tianyu</first><last>Ding</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yang</first><last>Gao</last><affiliation>Nanjing University</affiliation></author>
      <pages>12493-12510</pages>
      <abstract>The application of visual-to-music generation (VTM) is rapidly growing. However, current VTM methods struggle with capturing the relationship between visuals and music in open-domain settings, mainly due to two challenges: the lack of large-scale, high-quality visual-music paired datasets and the absence of direct semantic correspondence between visuals and music. In this work, we propose CoT-VTM, a framework that distills Chain-of-Thought (CoT) reasoning to enable visual-to-music generation without paired data, while efficiently producing music aligned with visual content in open-domain settings. We first bridge the gap between visual, music, and text data using appropriate foundation models. Next, we identify key elements of the visual-music relationship and design a CoT prompt for visual-to-music mapping. To fully distill the reasoning of CoT, we incorporate latent information from intermediate reasoning steps as supervisory signals alongside visual and music supervision. Finally, we design a two-stage mapping distillation training process: the first stage uses discriminative MLP modules, while the second uses a generative embedding diffusion model (EDM). Our model achieves optimal performance on both image-to-music and video-to-music tasks. Project page: https://xxkkxxx.github.io/cot-vtm/</abstract>
      <url hash="fad9e96d">2025.findings-acl.647</url>
      <bibkey>guan-etal-2025-cot</bibkey>
    </paper>
    <paper id="648">
      <title>A Tale of Evaluating Factual Consistency: Case Study on Long Document Summarization Evaluation</title>
      <author><first>Yang</first><last>Zhong</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>12511-12532</pages>
      <abstract>Ensuring factual consistency in summarization remains a challenge, especially for long-document evaluation. While automated, reference-free evaluation models are essential given the impracticality of large-scale human assessment for lengthy texts, challenges persist in evaluating different systems on how to handle different summary granularities and evolving model generations. In this work, we conduct a systematic study on diverse factual-consistency evaluation systems across four long-document datasets, encompassing summaries generated by models from non-LLMs to proprietary LLMs. Our analysis reveals that fine-grained continuous scores can provide more reliable assessments of different evaluation systems’ capabilities than binary classification. We also examine the relationship between sentence-level and summary-level model performance, highlighting its dependency on dataset characteristics. Moreover, our study reveals that advanced systems can achieve higher recall in error detection for older summaries, yet struggle with false positives and fine-grained error detection. Our analysis and case studies provide further insights into designing robust factuality evaluation systems, which are becoming increasingly in demand as generative models advance rapidly.</abstract>
      <url hash="5b130b1b">2025.findings-acl.648</url>
      <bibkey>zhong-litman-2025-tale</bibkey>
    </paper>
    <paper id="649">
      <title>Evaluating Pretrained Causal Language Models for Synonymy</title>
      <author><first>Ioana</first><last>Ivan</last><affiliation>Université d’Aix-Marseille</affiliation></author>
      <author><first>Carlos</first><last>Ramisch</last><affiliation>LIS - Laboratoire d’Informatique et Systèmes and AMU - Aix Marseille University</affiliation></author>
      <author><first>Alexis</first><last>Nasr</last><affiliation>Aix Marseille University</affiliation></author>
      <pages>12533-12551</pages>
      <abstract>The scaling of causal language models in size and training data enabled them to tackle increasingly complex tasks. Despite the development of sophisticated tests to reveal their new capabilities, the underlying basis of these complex skills remains unclear. We argue that complex skills might be explained using simpler ones, represented by linguistic concepts. As an initial step in exploring this hypothesis, we focus on the lexical-semantic concept of synonymy, laying the groundwork for research into its relationship with more complex skills. We develop a comprehensive test suite to assess various aspects of synonymy under different conditions, and evaluate causal open-source models ranging up to 10 billion parameters. We find that these models effectively recognize synonymy but struggle to generate synonyms when prompted with relevant context.</abstract>
      <url hash="8433d084">2025.findings-acl.649</url>
      <bibkey>ivan-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="650">
      <title><fixed-case>MDIT</fixed-case>-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models</title>
      <author><first>Bohan</first><last>Jin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shuhan</first><last>Qi</last><affiliation>Harbin Insitute of Technology, Shenzhen</affiliation></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xinyi</first><last>Guo</last><affiliation>University of Barcelona</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Harbin Institute of Technology,Shenzhen</affiliation></author>
      <pages>12552-12574</pages>
      <abstract>The widespread use of Large Multimodal Models (LMMs) has raised concerns about model toxicity. However, current research mainly focuses on explicit toxicity, with less attention to some more implicit toxicity regarding prejudice and discrimination. To address this limitation, we introduce a subtler type of toxicity named dual-implicit toxicity and a novel toxicity benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark. Specifically, we first create the MDIT-Dataset with dual-implicit toxicity using the proposed Multi-stage Human-in-loop In-context Generation method. Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating the sensitivity of models to dual-implicit toxicity, with 317,638 questions covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes three difficulty levels, and we propose a metric to measure the toxicity gap exhibited by the model across them. In the experiment, we conducted MDIT-Bench on 13 prominent LMMs, and the results show that these LMMs cannot handle dual-implicit toxicity effectively. The model’s performance drops significantly in hard level, revealing that these LMMs still contain a significant amount of hidden but activatable toxicity. The data will be released upon the paper’s acceptance.</abstract>
      <url hash="6713c44f">2025.findings-acl.650</url>
      <bibkey>jin-etal-2025-mdit</bibkey>
    </paper>
    <paper id="651">
      <title><fixed-case>C</fixed-case>o<fixed-case>VE</fixed-case>: Compressed Vocabulary Expansion Makes Better <fixed-case>LLM</fixed-case>-based Recommender Systems</title>
      <author><first>Haochen</first><last>Zhang</last></author>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Junze</first><last>Yin</last></author>
      <author><first>Oren</first><last>Gal</last></author>
      <author><first>Anshumali</first><last>Shrivastava</last><affiliation>Rice University and ThirdAI Corp.</affiliation></author>
      <author><first>Vladimir</first><last>Braverman</last><affiliation>UT Health, Rice University, Google and Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>12575-12591</pages>
      <abstract>Recommender systems play a pivotal role in providing relevant content to users. With the rapid development of large language models (LLMs), researchers have begun utilizing LLMs to build more powerful recommender systems. However, existing approaches that focus on aligning LLMs with recommendation tasks do not fully leverage their sequential information processing capabilities, leading to suboptimal performance. In this paper, we propose a novel system called compressed vocabulary expansion (CoVE). In CoVE, each item is assigned a unique ID within the expanded vocabulary. Our framework effectively capitalizes on sequence understanding abilities of LLMs, significantly enhancing their performance on recommendation tasks. Additionally, we compress the embedding layer, making CoVE practical for large-scale industrial applications. The effectiveness and performance of CoVE are demonstrated through comprehensive experiments on multiple recommendation datasets and comparisons with prior works. Our code can be found at <url>https://github.com/HaochenZhang717/CoVE-official-Repo</url>.</abstract>
      <url hash="10a8da5e">2025.findings-acl.651</url>
      <bibkey>zhang-etal-2025-cove</bibkey>
    </paper>
    <paper id="652">
      <title><fixed-case>C</fixed-case>trl<fixed-case>A</fixed-case>: Adaptive Retrieval-Augmented Generation via Inherent Control</title>
      <author><first>Liu</first><last>Huanshuo</last></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhijiang</first><last>Guo</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Jing</first><last>Wang</last></author>
      <author><first>Kuicai</first><last>Dong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <author><first>Yi Quan</first><last>Lee</last></author>
      <author><first>Cong</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>12592-12618</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved external knowledge. Adaptive RAG enhances this approach by enabling dynamic retrieval during generation, activating retrieval only when the query exceeds LLM’s internal knowledge. Existing methods primarily focus on detecting LLM’s confidence via statistical uncertainty. Instead, we present the first attempts to solve adaptive RAG from a representation perspective and develop an inherent control-based framework, termed CtrlA. Specifically, we extract the features that represent the honesty and confidence directions of LLM and adopt them to control LLM behavior and guide retrieval timing decisions. We also design a simple yet effective query formulation strategy to support adaptive retrieval. Experiments show that CtrlA is superior to existing adaptive RAG methods on a diverse set of tasks. Honesty steering can effectively make LLMs more honest and confidence monitoring is a promising indicator of retrieval trigger.</abstract>
      <url hash="d9bd2386">2025.findings-acl.652</url>
      <bibkey>huanshuo-etal-2025-ctrla</bibkey>
    </paper>
    <paper id="653">
      <title>Maximum Score Routing For Mixture-of-Experts</title>
      <author><first>Bowen</first><last>Dong</last><affiliation>Tsinghua University, Tsinghua University and Tencent AI Lab</affiliation></author>
      <author><first>Yilong</first><last>Fan</last></author>
      <author><first>Yutao</first><last>Sun</last></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Tengyu</first><last>Pan</last></author>
      <author><first>Zhou</first><last>Xun</last></author>
      <author><first>Jianyong</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>12619-12632</pages>
      <abstract>Routing networks in sparsely activated mixture-of-experts (MoE) dynamically allocate input tokens to top-k experts through differentiable sparse transformations, enabling scalable model capacity while preserving computational efficiency. Traditional MoE networks impose an expert capacity constraint to ensure GPU-friendly computation. However, this leads to token dropping when capacity is saturated and results in low hardware efficiency due to padding in underutilized experts. Removing the capacity constraint, in turn, compromises load balancing and computational efficiency.To address these issues, we propose Maximum Score Routing (**MaxScore**), a novel MoE routing paradigm that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator. MaxScore resolves the fundamental limitations of iterative rerouting and optimal transport formulations, achieving lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines.</abstract>
      <url hash="95fb715b">2025.findings-acl.653</url>
      <bibkey>dong-etal-2025-maximum</bibkey>
    </paper>
    <paper id="654">
      <title>Time Course <fixed-case>M</fixed-case>ech<fixed-case>I</fixed-case>nterp: Analyzing the Evolution of Components and Knowledge in Large Language Models</title>
      <author><first>Ahmad Dawar</first><last>Hakimi</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Ali</first><last>Modarressi</last><affiliation>Center for Information and Language Processing, LMU Munich</affiliation></author>
      <author><first>Philipp</first><last>Wicke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>12633-12653</pages>
      <abstract>Understanding how large language models (LLMs) acquire and store factual knowledge is crucial for enhancing their interpretability, reliability, and efficiency. In this work, we analyze the evolution of factual knowledge representation in the OLMo-7B model by tracking the roles of its Attention Heads and Feed Forward Networks (FFNs) over training. We classify these components into four roles—general, entity, relation-answer, and fact-answer specific—and examine their stability and transitions. Our results show that LLMs initially depend on broad, general-purpose components, which later specialize as training progresses. Once the model reliably predicts answers, some components are repurposed, suggesting an adaptive learning process. Notably, answer-specific attention heads display the highest turnover, whereas FFNs remain stable, continually refining stored knowledge. These insights offer a mechanistic view of knowledge formation in LLMs and have implications for model pruning, optimization, and transparency.</abstract>
      <url hash="cd99280d">2025.findings-acl.654</url>
      <bibkey>hakimi-etal-2025-time</bibkey>
    </paper>
    <paper id="655">
      <title>Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding</title>
      <author><first>Feifan</first><last>Song</last><affiliation>Peking University</affiliation></author>
      <author><first>Shaohang</first><last>Wei</last><affiliation>Peking University</affiliation></author>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxuan</first><last>Fan</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>12654-12670</pages>
      <abstract>Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.</abstract>
      <url hash="a9994412">2025.findings-acl.655</url>
      <bibkey>song-etal-2025-well</bibkey>
    </paper>
    <paper id="656">
      <title>Disentangling Text and Math in Word Problems: Evidence for the Bidimensional Structure of Large Language Models’ Reasoning</title>
      <author><first>Pedro</first><last>Calais</last><affiliation>IBMEC</affiliation></author>
      <author><first>Gabriel</first><last>Franco</last><affiliation>Boston University</affiliation></author>
      <author><first>Zilu</first><last>Tang</last></author>
      <author><first>Themistoklis</first><last>Nikas</last></author>
      <author><first>Wagner Meira</first><last>Jr.</last><affiliation>Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais</affiliation></author>
      <author><first>Evimaria</first><last>Terzi</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Mark</first><last>Crovella</last><affiliation>Boston University</affiliation></author>
      <pages>12671-12688</pages>
      <abstract>Do LLMs process text and mathematics as a unified skill, or do these components rely on distinct underlying mechanisms? We investigate this question by disentangling the textual interpretation and mathematical solving steps in word problems drawn from Brazil’s largest college entrance exam (ENEM) and GSM8K, a popular grade school-level benchmark. Using the symbolic solver SymPy, we transform word problems into equivalent purely mathematical representations, isolating equation formulation from textual comprehension. Our extended benchmarks enable a structured analysis of LLM performance across these two dimensions. Through empirical evaluations, we find that small-scale LLMs struggle significantly more with text interpretation than with equation solving, with accuracy dropping by a factor of 2 to 7 when solving full word problems compared to their math-only counterparts. Exploratory factor analysis confirms a bidimensional structure in LLM reasoning, where models exhibit distinct proficiencies in textual and mathematical components, underscoring the need for targeted improvements in language comprehension. By analyzing the latent factors associated with each model, our findings provide a framework for researchers and practitioners to make informed choices when selecting models based on computational costs and the nature of their tasks.</abstract>
      <url hash="4f4c74d4">2025.findings-acl.656</url>
      <bibkey>calais-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="657">
      <title>Human-<fixed-case>LLM</fixed-case> Coevolution: Evidence from Academic Writing</title>
      <author><first>Mingmeng</first><last>Geng</last><affiliation>Ecole Normale Supérieure – PSL</affiliation></author>
      <author><first>Roberto</first><last>Trotta</last></author>
      <pages>12689-12696</pages>
      <abstract>With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as “delve”, starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as “significant”, has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs’ disfavor. The coevolution between humans and LLMs also merits further study.</abstract>
      <url hash="4cd7ded5">2025.findings-acl.657</url>
      <bibkey>geng-trotta-2025-human</bibkey>
    </paper>
    <paper id="658">
      <title>Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning</title>
      <author><first>Hao</first><last>Dong</last></author>
      <author><first>Ziyue</first><last>Qiao</last><affiliation>Great Bay University</affiliation></author>
      <author><first>Zhiyuan</first><last>Ning</last></author>
      <author><first>Qi</first><last>Hao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Yi</first><last>Du</last><affiliation>computer network information center, cas</affiliation></author>
      <author><first>Pengyang</first><last>Wang</last><affiliation>University of Macau</affiliation></author>
      <author><first>Yuanchun</first><last>Zhou</last><affiliation>Computer Network Information Center, Chinese Academy of Sciences,</affiliation></author>
      <pages>12697-12707</pages>
      <abstract>Temporal Knowledge Graphs (TKGs) incorporate the temporal feature to express the transience of knowledge by describing when facts occur. TKG extrapolation aims to infer possible future facts based on known history, which has garnered significant attention in recent years. Some existing methods treat TKG as a sequence of independent subgraphs to model temporal evolution patterns, demonstrating impressive reasoning performance. However, they still have limitations: 1) In modeling subgraph semantic evolution, they usually neglect the internal structural interactions between subgraphs, which are actually crucial for encoding TKGs. 2) They overlook the potential smooth features that do not lead to semantic changes, which should be distinguished from the semantic evolution process. Therefore, we propose Disentangled Multi-span Evolutionary Network (DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution strategy that captures local neighbor features while perceiving historical neighbor semantic information, thus enabling internal interactions between subgraphs during the evolution process. To maximize the capture of semantic change patterns, we design a disentangle component that adaptively separates nodes’ active and stable features, used to dynamically control the influence of historical semantics on future evolution. Extensive experiments demonstrate that DiMNet achieves substantial performance in TKG reasoning, outperforming the state-of-the-art up to 22.7% in MRR.</abstract>
      <url hash="a7870dc1">2025.findings-acl.658</url>
      <bibkey>dong-etal-2025-disentangled</bibkey>
    </paper>
    <paper id="659">
      <title><fixed-case>GRAF</fixed-case>: Graph Retrieval Augmented by Facts for <fixed-case>R</fixed-case>omanian Legal Multi-Choice Question Answering</title>
      <author><first>Cristian-George</first><last>Craciun</last></author>
      <author><first>Răzvan-Alexandru</first><last>Smădu</last><affiliation>, University Politehnica of Bucharest</affiliation></author>
      <author><first>Dumitru-Clementin</first><last>Cercel</last><affiliation>National University of Science and Technology POLITEHNICA Bucharest</affiliation></author>
      <author><first>Mihaela-Claudia</first><last>Cercel</last></author>
      <pages>12708-12742</pages>
      <abstract>Pre-trained language models have shown remarkable performance in recent years, setting a new paradigm for natural language processing (NLP) research. The legal domain has received some attention from the NLP community, in part due to its textual nature. Question answering (QA) systems represent some of the tasks in this domain. This work explores the legal multiple-choice QA (MCQA) for Romanian. The contribution of this work is multi-fold. We introduce JuRO, the first openly available Romanian legal MCQA dataset, comprising 10,836 questions from three examinations. Along with this dataset, we introduce CROL, an organized corpus of laws comprising a total of 93 distinct documents with their modifications over 763 time spans, which we used for information retrieval techniques in this work. Additionally, we construct Law-RoG, the first graph of legal knowledge for the Romanian language, derived from the aforementioned corpus. Lastly, we propose a novel approach for MCQA, namely Graph Retrieval Augmented by Facts (GRAF), which achieves competitive results with generally accepted state-of-the-art methods and even exceeds them in most settings.</abstract>
      <url hash="178cd70d">2025.findings-acl.659</url>
      <bibkey>craciun-etal-2025-graf</bibkey>
    </paper>
    <paper id="660">
      <title>Express What You See: Can Multimodal <fixed-case>LLM</fixed-case>s Decode Visual Ciphers with Intuitive Semiosis Comprehension?</title>
      <author><first>Jiayi</first><last>Kuang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Chen</first><last>Wang</last></author>
      <author><first>Haohao</first><last>Luo</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Wenhao</first><last>Jiang</last><affiliation>Guangming Laboratory</affiliation></author>
      <pages>12743-12774</pages>
      <abstract>Bridging the gap between visual and language remains a pivotal challenge for the multimodal community. Traditional VQA benchmarks encounter a modality gap and over-reliance on language priors, whereas human cognition excels at intuitive semiosis, associating abstract visual symbols to linguistic semantics. Inspired by this neurocognitive mechanism, we focus on emojis, the visual cipher conveying abstract textual semantics. Specifically, we propose a novel task of generating abstract linguistics from emoji sequence images, where such reasoning underpins critical applications in cryptography, thus challenging MLLMs’ reasoning of decoding complex semantics of visual ciphers. We introduce eWe-bench (Express What you SeE), assessing MLLMs’ capability of intuitive semiosis like humans. Our data construction framework ensures high visual sensitivity and data quality, which can be extended to future data enhancement. Evaluation results on advanced MLLMs highlight critical deficiencies in visual intuitive symbolic reasoning. We believe our interesting insights for advancing visual semiosis in MLLMs will pave the way for cryptographic analysis and high-level intuitive cognition intelligence of MLLMs.</abstract>
      <url hash="f5bd87ca">2025.findings-acl.660</url>
      <bibkey>kuang-etal-2025-express</bibkey>
    </paper>
    <paper id="661">
      <title><fixed-case>C</fixed-case>on<fixed-case>F</fixed-case>it v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining</title>
      <author><first>Xiao</first><last>Yu</last></author>
      <author><first>Ruize</first><last>Xu</last></author>
      <author><first>Chengyuan</first><last>Xue</last></author>
      <author><first>Jinzhong</first><last>Zhang</last><affiliation>IntelliproGroup Inc.</affiliation></author>
      <author><first>Xu</first><last>Ma</last><affiliation>haier</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>12775-12790</pages>
      <abstract>A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder’s contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. This method also simplifies the representation space of the encoder. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.</abstract>
      <url hash="712e6306">2025.findings-acl.661</url>
      <bibkey>yu-etal-2025-confit</bibkey>
    </paper>
    <paper id="662">
      <title>Knowing Before Saying: <fixed-case>LLM</fixed-case> Representations Encode Information About Chain-of-Thought Success Before Completion</title>
      <author><first>Anum</first><last>Afzal</last></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Gal</first><last>Chechik</last><affiliation>Bar Ilan University and NVIDIA</affiliation></author>
      <author><first>Yftah</first><last>Ziser</last><affiliation>NVIDIA</affiliation></author>
      <pages>12791-12806</pages>
      <abstract>We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. Our classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse—likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier’s guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT’s efficiency while preserving its benefits.</abstract>
      <url hash="8d8888e4">2025.findings-acl.662</url>
      <bibkey>afzal-etal-2025-knowing</bibkey>
    </paper>
    <paper id="663">
      <title>Grounding Task Assistance with Multimodal Cues from a Single Demonstration</title>
      <author><first>Gabriel Herbert</first><last>Sarch</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Balasaravanan Thoravi</first><last>Kumaravel</last></author>
      <author><first>Sahithya</first><last>Ravi</last></author>
      <author><first>Vibhav</first><last>Vineet</last><affiliation>Microsoft</affiliation></author>
      <author><first>Andrew D</first><last>Wilson</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>12807-12833</pages>
      <abstract>A person’s demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.</abstract>
      <url hash="e51cc0bb">2025.findings-acl.663</url>
      <bibkey>sarch-etal-2025-grounding</bibkey>
    </paper>
    <paper id="664">
      <title>Awes, Laws, and Flaws From Today’s <fixed-case>LLM</fixed-case> Research</title>
      <author><first>Adrian</first><last>de Wynter</last><affiliation>Microsoft</affiliation></author>
      <pages>12834-12854</pages>
      <abstract>We perform a critical examination of the scientific methodology behind contemporary large language model (LLM) research. For this we assess over 2,000 research works released between 2020 and 2024 based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility), and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour). We find multiple trends, such as declines in ethics disclaimers, a rise of LLMs as evaluators, and an increase on claims of LLM reasoning abilities without leveraging human evaluation. We note that conference checklists are effective at curtailing some of these issues, but balancing velocity and rigour in research cannot solely rely on these. We tie all these findings to findings from recent meta-reviews and extend recommendations on how to address what does, does not, and should work in LLM research.</abstract>
      <url hash="99f22350">2025.findings-acl.664</url>
      <bibkey>de-wynter-2025-awes</bibkey>
    </paper>
    <paper id="665">
      <title>Dual Debiasing for Noisy In-Context Learning for Text Generation</title>
      <author><first>Siqi</first><last>Liang</last></author>
      <author><first>Sumyeong</first><last>Ahn</last><affiliation>KENTECH</affiliation></author>
      <author><first>Paramveer</first><last>Dhillon</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jiayu</first><last>Zhou</last><affiliation>University of Michigan - Ann Arbor and Michigan State University</affiliation></author>
      <pages>12855-12868</pages>
      <abstract>In-context learning (ICL) relies heavily on high-quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed.We re-examine the perplexity-based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain-specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual-debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust <i>Sample Cleanliness Score</i>. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level.Extensive experiments demonstrate our method’s superior noise-detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.</abstract>
      <url hash="a287efdc">2025.findings-acl.665</url>
      <bibkey>liang-etal-2025-dual</bibkey>
    </paper>
    <paper id="666">
      <title><fixed-case>DRS</fixed-case>: Deep Question Reformulation With Structured Output</title>
      <author><first>Zhecheng</first><last>Li</last></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yujun</first><last>Cai</last><affiliation>The University of Queensland</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>12869-12882</pages>
      <abstract>Question answering represents a core capability of large language models (LLMs). However, when individuals encounter unfamiliar knowledge in texts, they often formulate questions that the text itself cannot answer due to insufficient understanding of the underlying information. Recent studies reveal that while LLMs can detect unanswerable questions, they struggle to assist users in reformulating these questions. Even advanced models like GPT-3.5 demonstrate limited effectiveness in this regard. To address this limitation, we propose DRS: Deep Question Reformulation with Structured Output, a novel zero-shot method aimed at enhancing LLMs’ ability to assist users in reformulating questions to extract relevant information from new documents. DRS combines the strengths of LLMs with a DFS-based algorithm to iteratively explore potential entity combinations and constrain outputs using predefined entities. This structured approach significantly enhances the reformulation capabilities of LLMs. Comprehensive experimental evaluations demonstrate that DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while also enhancing the performance of open-source models, such as Gemma2-9B, from 26.35% to 56.75%.</abstract>
      <url hash="ee3f94ec">2025.findings-acl.666</url>
      <bibkey>li-etal-2025-drs</bibkey>
    </paper>
    <paper id="667">
      <title>Towards Explainable Hate Speech Detection</title>
      <author><first>Happy Khairunnisa</first><last>Sariyanto</last><affiliation>Ernst-Moritz-Arndt Universität Greifswald</affiliation></author>
      <author><first>Diclehan</first><last>Ulucan</last><affiliation>Ernst-Moritz-Arndt Universität Greifswald</affiliation></author>
      <author><first>Oguzhan</first><last>Ulucan</last><affiliation>University of Greifswald and University of Greifswald</affiliation></author>
      <author><first>Marc</first><last>Ebner</last><affiliation>Universität Greifswald</affiliation></author>
      <pages>12883-12893</pages>
      <abstract>Recent advancements in deep learning have significantly enhanced the efficiency and accuracy of natural language processing (NLP) tasks. However, these models often require substantial computational resources, which remains a major drawback. Reducing the complexity of deep learning architectures, and exploring simpler yet effective approaches can lead to cost-efficient NLP solutions. This is also a step towards explainable AI, i.e., uncovering how a particular task is carried out. For this analysis, we chose the task of hate speech detection. We address hate speech detection by introducing a model that employs a weighted sum of valence, arousal, and dominance (VAD) scores for classification. To determine the optimal weights and classification strategies, we analyze hate speech and non-hate speech words based on both their individual and summed VAD-values. Our experimental results demonstrate that this straightforward approach can compete with state-of-the-art neural network methods, including GPT-based models, in detecting hate speech.</abstract>
      <url hash="e3fa873c">2025.findings-acl.667</url>
      <bibkey>sariyanto-etal-2025-towards</bibkey>
    </paper>
    <paper id="668">
      <title><fixed-case>B</fixed-case>io<fixed-case>H</fixed-case>op<fixed-case>R</fixed-case>: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain</title>
      <author><first>Yunsoo</first><last>Kim</last></author>
      <author><first>Yusuf</first><last>Abdulle</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Honghan</first><last>Wu</last><affiliation>University of Glasgow</affiliation></author>
      <pages>12894-12908</pages>
      <abstract>Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities.Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs. BioHopR is available at https://huggingface.co/datasets/knowlab-research/BioHopR.</abstract>
      <url hash="7867c4ce">2025.findings-acl.668</url>
      <bibkey>kim-etal-2025-biohopr</bibkey>
    </paper>
    <paper id="669">
      <title><fixed-case>P</fixed-case>ipe<fixed-case>S</fixed-case>pec: Breaking Stage Dependencies in Hierarchical <fixed-case>LLM</fixed-case> Decoding</title>
      <author><first>Bradley</first><last>McDanel</last><affiliation>Franklin and Marshall College</affiliation></author>
      <author><first>Sai Qian</first><last>Zhang</last><affiliation>New York University</affiliation></author>
      <author><first>Yunhai</first><last>Hu</last></author>
      <author><first>Zining</first><last>Liu</last></author>
      <pages>12909-12920</pages>
      <abstract>Speculative decoding accelerates large language model inference by using smaller draft models to generate candidate tokens for parallel verification. However, current approaches are limited by sequential stage dependencies that prevent full hardware utilization. We present PipeSpec, a framework that generalizes speculative decoding to use multiple models arranged in a hierarchical pipeline, enabling asynchronous execution with lightweight coordination for prediction verification and rollback. Our analytical model characterizes token generation rates across pipeline stages and proves guaranteed throughput improvements over traditional decoding for any non-zero acceptance rate. We further derive closed-form expressions for steady-state verification probabilities that explain the empirical benefits of pipeline depth. We validate PipeSpec across text summarization, mathematical reasoning, and code generation tasks using LLaMA 2 and 3 models, demonstrating that pipeline efficiency increases with model depth, providing a scalable approach to accelerating LLM inference on multi-device systems. Our code is available at https://github.com/BradMcDanel/PipeSpec.</abstract>
      <url hash="2ddba655">2025.findings-acl.669</url>
      <bibkey>mcdanel-etal-2025-pipespec</bibkey>
    </paper>
    <paper id="670">
      <title><fixed-case>LAM</fixed-case> <fixed-case>SIMULATOR</fixed-case>: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback</title>
      <author><first>Thai Quoc</first><last>Hoang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shirley</first><last>Kokane</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Zuxin</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jake</first><last>Grigsby</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Tian</first><last>Lan</last><affiliation>SalesForce</affiliation></author>
      <author><first>Michael S</first><last>Ryoo</last><affiliation>Salesforce AI Research and Stony Brook University</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Huan</first><last>Wang</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Juan Carlos</first><last>Niebles</last><affiliation>Salesforce Research and Stanford University</affiliation></author>
      <pages>12921-12934</pages>
      <abstract>Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR’s efficiency and effectiveness in speeding up development of AI agents.</abstract>
      <url hash="fde5adf8">2025.findings-acl.670</url>
      <bibkey>hoang-etal-2025-lam</bibkey>
    </paper>
    <paper id="671">
      <title>Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion</title>
      <author><first>Sahil</first><last>Mishra</last></author>
      <author><first>Kumar</first><last>Arjun</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>12935-12953</pages>
      <abstract>Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex (<tex-math>\textbf{L}</tex-math>ineage-<tex-math>\textbf{O}</tex-math>riented <tex-math>\textbf{Re}</tex-math>asoning for Taxonomy E<tex-math>\textbf{x}</tex-math>pansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates’ hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu &amp; Palmer similarity by 5% over state-of-the-art methods.</abstract>
      <url hash="ab6a3e4c">2025.findings-acl.671</url>
      <bibkey>mishra-etal-2025-rank</bibkey>
    </paper>
    <paper id="672">
      <title>Probing Subphonemes in Morphology Models</title>
      <author><first>Gal</first><last>Astrach</last></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>12954-12961</pages>
      <abstract>Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer’s encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.</abstract>
      <url hash="a1a9fe49">2025.findings-acl.672</url>
      <bibkey>astrach-pinter-2025-probing</bibkey>
    </paper>
    <paper id="673">
      <title>Exploiting Instruction-Following Retrievers for Malicious Information Retrieval</title>
      <author><first>Parishad</first><last>BehnamGhader</last></author>
      <author><first>Nicholas</first><last>Meade</last><affiliation>McGill University</affiliation></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>ServiceNow Inc, Mila, McGill University and Mila, McGill University</affiliation></author>
      <pages>12962-12980</pages>
      <abstract>Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for &gt;50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.</abstract>
      <url hash="96f36f6e">2025.findings-acl.673</url>
      <bibkey>behnamghader-etal-2025-exploiting</bibkey>
    </paper>
    <paper id="674">
      <title>Improving Causal Interventions in Amnesic Probing with Mean Projection or <fixed-case>LEACE</fixed-case></title>
      <author><first>Alicja</first><last>Dobrzeniecka</last><affiliation>NASK - National Research Institute</affiliation></author>
      <author><first>Antske</first><last>Fokkens</last><affiliation>VU University Amsterdam</affiliation></author>
      <author><first>Pia</first><last>Sommerauer</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <pages>12981-12993</pages>
      <abstract>Amnesic probing is a technique used to examine the influence of specific linguistic information on the behaviour of a model. This involves identifying and removing the relevant information and then assessing whether the model’s performance on the main task changes. If the removed information is relevant, the model’s performance should decline. The difficulty with this approach lies in removing <i>only</i> the target information while leaving other information unchanged. It has been shown that Iterative Nullspace Projection (INLP), a widely used removal technique, introduces random modifications to representations when eliminating target information. We demonstrate that Mean Projection (MP) and LEACE, two proposed alternatives, remove information in a more targeted manner, thereby enhancing the potential for obtaining behavioural explanations through Amnesic Probing.</abstract>
      <url hash="ae490524">2025.findings-acl.674</url>
      <bibkey>dobrzeniecka-etal-2025-improving</bibkey>
    </paper>
    <paper id="675">
      <title>The Threat of <fixed-case>PROMPTS</fixed-case> in Large Language Models: A System and User Prompt Perspective</title>
      <author><first>Zixuan</first><last>Xia</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Huazheng</first><last>Wang</last></author>
      <author><first>Xiaoyuan</first><last>Fu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jianxin</first><last>Liao</last></author>
      <pages>12994-13035</pages>
      <abstract>Prompts, especially high-quality ones, play an invaluable role in assisting large language models (LLMs) to accomplish various natural language processing tasks. However, carefully crafted prompts can also manipulate model behavior. Therefore, the security risks that “prompts themselves face” and those “arising from harmful prompts” cannot be overlooked and we define the Prompt Threat (PT) issues. In this paper, we review the latest attack methods related to prompt threats, focusing on prompt leakage attacks and prompt jailbreak attacks. Additionally, we summarize the experimental setups of these methods and explore the relationship between prompt threats and prompt injection attacks.</abstract>
      <url hash="32e22475">2025.findings-acl.675</url>
      <bibkey>xia-etal-2025-threat</bibkey>
    </paper>
    <paper id="676">
      <title><fixed-case>R</fixed-case>ose<fixed-case>RAG</fixed-case>: Robust Retrieval-augmented Generation with Small-scale <fixed-case>LLM</fixed-case>s via Margin-aware Preference Optimization</title>
      <author><first>Tianci</first><last>Liu</last></author>
      <author><first>Haoxiang</first><last>Jiang</last></author>
      <author><first>Tianze</first><last>Wang</last></author>
      <author><first>Ran</first><last>Xu</last><affiliation>Emory University</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Meta</affiliation></author>
      <author><first>Linjun</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Tuo</first><last>Zhao</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Haoyu</first><last>Wang</last><affiliation>State University of New York at Albany</affiliation></author>
      <pages>13036-13054</pages>
      <abstract>Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.</abstract>
      <url hash="401febe7">2025.findings-acl.676</url>
      <bibkey>liu-etal-2025-roserag</bibkey>
    </paper>
    <paper id="677">
      <title>Instruction-Tuning <fixed-case>LLM</fixed-case>s for Event Extraction with Annotation Guidelines</title>
      <author><first>Saurabh</first><last>Srivastava</last></author>
      <author><first>Sweta</first><last>Pati</last></author>
      <author><first>Ziyu</first><last>Yao</last><affiliation>George Mason University</affiliation></author>
      <pages>13055-13071</pages>
      <abstract>In this work, we study the effect of annotation guidelines–textual descriptions of event types and arguments, when instruction-tuning large language models for event extraction. We conducted a series of experiments with both human-provided and machine-generated guidelines in both full- and low-data settings. Our results demonstrate the promise of annotation guidelines when there is a decent amount of training data and highlight its effectiveness in improving cross-schema generalization and low-frequency event-type performance.</abstract>
      <url hash="b6266670">2025.findings-acl.677</url>
      <bibkey>srivastava-etal-2025-instruction</bibkey>
    </paper>
    <paper id="678">
      <title>m<fixed-case>RAKL</fixed-case>: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages</title>
      <author><first>Hellina Hailu</first><last>Nigatu</last></author>
      <author><first>Min</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Maartje</first><last>Ter Hoeve</last><affiliation>Apple</affiliation></author>
      <author><first>Saloni</first><last>Potdar</last><affiliation>Apple</affiliation></author>
      <author><first>Sarah</first><last>Chasins</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>13072-13089</pages>
      <abstract>Knowledge Graphs represent real-world entities and the relationships between them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of automatically constructing or predicting missing entities and links for knowledge graphs in a multilingual setting. In this work, we reformulate the mKGC task as a Question Answering (QA) task and introduce mRAKL: a Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve this by using the head entity and linking relation in a question, and having our model predict the tail entity as an answer. Our experiments focus primarily on two low-resourced languages: Tigrinya and Amharic. We experiment with using higher-resourced languages, Arabic and English, to utilize cross-lingual transfer for mKGC. With a BM25 retriever, we find that the RAG-based approach improves performance over a no-context setting. Further, our ablation studies show that with an idealized retrieval system, mRAKL improves accuracy by up to 4.92 and 8.79 percentage points for Tigrinya and Amharic, respectively.</abstract>
      <url hash="5306dac8">2025.findings-acl.678</url>
      <bibkey>nigatu-etal-2025-mrakl</bibkey>
    </paper>
    <paper id="679">
      <title>Mechanistic Interpretability of Emotion Inference in Large Language Models</title>
      <author><first>Ala N.</first><last>Tak</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Amin</first><last>Banayeeanzade</last><affiliation>Department of Computer Science, Viterbi School of Engineering</affiliation></author>
      <author><first>Anahita</first><last>Bolourani</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Mina</first><last>Kian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jonathan</first><last>Gratch</last><affiliation>University of Southern California</affiliation></author>
      <pages>13090-13120</pages>
      <abstract>Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes, and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory—a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and control emotion inference, potentially benefiting safety and alignment in sensitive affective domains.</abstract>
      <url hash="75094d71">2025.findings-acl.679</url>
      <bibkey>tak-etal-2025-mechanistic</bibkey>
    </paper>
    <paper id="680">
      <title><fixed-case>RL</fixed-case>-Guider: Leveraging Historical Decisions and Feedback for Drug Editing with Large Language Models</title>
      <author><first>Xufeng</first><last>Liu</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Yixuan</first><last>Ding</last></author>
      <author><first>Jingxiang</first><last>Qu</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Wenhan</first><last>Gao</last></author>
      <author><first>Yi</first><last>Liu</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>13121-13138</pages>
      <abstract>Recent success of large language models (LLMs) in diverse domains showcases their potential to revolutionize scientific fields, including drug editing. Traditional drug editing relies on iterative conversations with domain experts, refining the drug until the desired property is achieved. This interactive and iterative process mirrors the strengths of LLMs, making them well-suited for drug editing. *In existing works, LLMs edit each molecule independently without leveraging knowledge from past edits.* However, human experts develop intuition about effective modifications over time through historical experience; accumulating past knowledge is pivotal for human experts, and so it is for LLMs. *In this work, we propose RL-Guider — a reinforcement-learning agent to provide suggestions to LLMs; it uses the rich information provided from evaluating editing results made by the LLM based on the recommendations to improve itself over time.* RL-Guider is the first work that leverages both the comprehensive “world-level” knowledge of LLMs and the knowledge accumulated from historical feedback. As a result, RL-Guider mitigates several shortcomings of existing approaches and demonstrates superior performance. The code is available at [https://github.com/xufliu/RL-Guider](https://github.com/xufliu/RL-Guider).</abstract>
      <url hash="91417370">2025.findings-acl.680</url>
      <bibkey>liu-etal-2025-rl</bibkey>
    </paper>
    <paper id="681">
      <title><fixed-case>B</fixed-case>rief<fixed-case>M</fixed-case>e: A Legal <fixed-case>NLP</fixed-case> Benchmark for Assisting with Legal Briefs</title>
      <author><first>Jesse</first><last>Woo</last><affiliation>New York University</affiliation></author>
      <author><first>Fateme</first><last>Hashemi Chaleshtori</last><affiliation>University of Utah</affiliation></author>
      <author><first>Ana</first><last>Marasovic</last><affiliation>University of Utah</affiliation></author>
      <author><first>Kenneth</first><last>Marino</last><affiliation>University of Utah and Google</affiliation></author>
      <pages>13139-13190</pages>
      <abstract>A core part of legal work that has been underexplored in Legal NLP is the writing and editing of legal briefs. This requires not only a thorough understanding of the law of a jurisdiction, from judgments to statutes, but also the ability to make new arguments to try to expand the law in a new direction and make novel and creative arguments that are persuasive to judges. To capture and evaluate these legal skills in language models, we introduce BRIEFME, a new dataset focused on legal briefs. It contains three tasks for language models to assist legal professionals in writing briefs: argument summarization, argument completion, and case retrieval. In this work, we describe the creation of these tasks, analyze them, and show how current models perform. We see that today’s large language models (LLMs) are already quite good at the summarization and guided completion tasks, even beating human-generated headings. Yet, they perform poorly on other tasks in our benchmark: realistic argument completion and retrieving relevant legal cases. We hope this dataset encourages more development in Legal NLP in ways that will specifically aid people in performing legal work.</abstract>
      <url hash="4b3a52d4">2025.findings-acl.681</url>
      <bibkey>woo-etal-2025-briefme</bibkey>
    </paper>
    <paper id="682">
      <title><fixed-case>I</fixed-case> see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue</title>
      <author><first>Esam</first><last>Ghaleb</last></author>
      <author><first>Bulat</first><last>Khaertdinov</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Asli</first><last>Ozyurek</last><affiliation>mpi for psycholinguistics</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <pages>13191-13206</pages>
      <abstract>In face-to-face interaction, we use multiple modalities, including speech and gestures, to communicate information and resolve references to objects. However, how representational co-speech gestures refer to objects remains understudied from a computational perspective. In this work, we address this gap by introducing a multimodal reference resolution task centred on representational gestures, while simultaneously tackling the challenge of learning robust gesture embeddings. We propose a self-supervised pre-training approach to gesture representation learning that grounds body movements in spoken language. Our experiments show that the learned embeddings align with expert annotations and have significant predictive power. Moreover, reference resolution accuracy further improves when (1) using multimodal gesture representations, even when speech is unavailable at inference time, and (2) leveraging dialogue history. Overall, our findings highlight the complementary roles of gesture and speech in reference resolution, offering a step towards more naturalistic models of human-machine interaction.</abstract>
      <url hash="2993f50f">2025.findings-acl.682</url>
      <bibkey>ghaleb-etal-2025-see</bibkey>
    </paper>
    <paper id="683">
      <title>World Knowledge Resolves Some Aspectual Ambiguity</title>
      <author><first>Katarzyna</first><last>Pruś</last></author>
      <author><first>Mark</first><last>Steedman</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Adam</first><last>Lopez</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>13207-13220</pages>
      <abstract>Annotating event descriptions with their aspectual features is often seen as a pre-requisite to temporal reasoning. However, a recent study by Pruś et al. (2024) has shown that non-experts’ annotations of the aspectual class of English verb phrases can disagree with both expert linguistic annotations and each another. They hypothesised that people use their world knowledge to tacitly conjure their own contexts, leading to disagreement between them. In this paper, we test that hypothesis by adding context to Pruś et al.’s examples and mirroring their experiment. Our results show that whilst their hypothesis explains some of the disagreement, some examples continue to yield divided responses even with the additional context. Finally, we show that outputs from GPT-4, despite to some degree capturing the aspectual class division, are not an accurate predictor of human answers.</abstract>
      <url hash="9d4d7ede">2025.findings-acl.683</url>
      <bibkey>prus-etal-2025-world</bibkey>
    </paper>
    <paper id="684">
      <title><fixed-case>ACCESS</fixed-case> <fixed-case>DENIED</fixed-case> <fixed-case>INC</fixed-case>: The First Benchmark Environment for Sensitivity Awareness</title>
      <author><first>Dren</first><last>Fazlija</last><affiliation>L3S Research Center Hannover</affiliation></author>
      <author><first>Arkadij</first><last>Orlov</last><affiliation>E.ON Grid Solutions</affiliation></author>
      <author><first>Sandipan</first><last>Sikdar</last><affiliation>Universität Hannover</affiliation></author>
      <pages>13221-13240</pages>
      <abstract>Large language models (LLMs) are increasingly becoming valuable to corporate data management due to their ability to process text from various document formats and facilitate user interactions through natural language queries. However, LLMs must consider the sensitivity of information when communicating with employees, especially given access restrictions. Simple filtering based on user clearance levels can pose both performance and privacy challenges. To address this, we propose the concept of sensitivity awareness (SA), which enables LLMs to adhere to predefined access rights rules. In addition, we developed a benchmarking environment called ACCESS DENIED INC to evaluate SA. Our experimental findings reveal significant variations in model behavior, particularly in managing unauthorized data requests while effectively addressing legitimate queries. This work establishes a foundation for benchmarking sensitivity-aware language models and provides insights to enhance privacy-centric AI systems in corporate environments.</abstract>
      <url hash="b7ca39ee">2025.findings-acl.684</url>
      <bibkey>fazlija-etal-2025-access</bibkey>
    </paper>
    <paper id="685">
      <title>Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis</title>
      <author><first>Chi-Jane</first><last>Chen</last></author>
      <author><first>Yuhang</first><last>Chen</last></author>
      <author><first>Sukwon</first><last>Yun</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Natalie</first><last>Stanley</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>13241-13252</pages>
      <abstract>Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry’s analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information—they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently—they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates both single-cell expression and spatial information into natural language using a multi-sentence approach. Given an expression matrix and spatial coordinates, Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations are processed by LLMs, enabling them to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets for diabetes and brain tumors, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: <url>https://github.com/UNITES-Lab/Spatial2Sentence</url>.</abstract>
      <url hash="bac1d431">2025.findings-acl.685</url>
      <bibkey>chen-etal-2025-spatial</bibkey>
    </paper>
    <paper id="686">
      <title><fixed-case>H</fixed-case>uman<fixed-case>E</fixed-case>val Pro and <fixed-case>MBPP</fixed-case> Pro: Evaluating Large Language Models on Self-invoking Code Generation Task</title>
      <author><first>Zhaojian</first><last>Yu</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Xiao-Ping</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>13253-13279</pages>
      <abstract>In this paper, we present HumanEval Pro and MBPP Pro, a series of benchmarks to evaluate LLMs on self-invoking code generation task. This task involves providing LLMs with a base problem alongside a related, more complex problem. The models must solve the base problem and leverage its solution to address the more complex one, thereby showcasing their capacity for progressive reasoning and problem-solving. This work features three key contributions. First, we propose a general recipe for generating more challenging versions of existing benchmarks. Second, from the analysis of experimental results over twenty large language models (LLM) on our benchmarks, we have two important observations: (i) Most LLMs excel in traditional code generation benchmarks like HumanEval and MBPP, but their performance declines on self-invoking tasks. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. (ii) On self-invoking code generation task, the instruction-tuned models demonstrate only marginal improvements compared to the base models. Third, we disclose the types of failure modes that exist in our evaluation results. All these results underscore the need for further advancements in this area and provide a new prospective to future research.</abstract>
      <url hash="cb65718f">2025.findings-acl.686</url>
      <bibkey>yu-etal-2025-humaneval</bibkey>
    </paper>
    <paper id="687">
      <title><fixed-case>TCS</fixed-case>inger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis</title>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Wenxiang</first><last>Guo</last></author>
      <author><first>Changhao</first><last>Pan</last></author>
      <author><first>Dongyu</first><last>Yao</last></author>
      <author><first>Zhiyuan</first><last>Zhu</last></author>
      <author><first>Ziyue</first><last>Jiang</last></author>
      <author><first>Yuhan</first><last>Wang</last></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>13280-13294</pages>
      <abstract>Customizable multilingual zero-shot singing voice synthesis (SVS) has various potential applications in music composition and short video dubbing. However, existing SVS models overly depend on phoneme and note boundary annotations, limiting their robustness in zero-shot scenarios and producing poor transitions between phonemes and notes. Moreover, they also lack effective multi-level style control via diverse prompts. To overcome these challenges, we introduce TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer and style control based on various prompts. TCSinger 2 mainly includes three key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration, extends content embedding, and applies masking to the boundaries to enable smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to extract aligned representations from singing, speech, and textual prompts. 3) Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision, enhancing both the synthesis quality and style modeling of the generated singing voice. Experimental results show that TCSinger 2 outperforms baseline models in both subjective and objective metrics across multiple related tasks.</abstract>
      <url hash="ded55dd2">2025.findings-acl.687</url>
      <bibkey>zhang-etal-2025-tcsinger</bibkey>
    </paper>
    <paper id="688">
      <title>Compute Optimal Scaling of Skills: Knowledge vs Reasoning</title>
      <author><first>Nicholas</first><last>Roberts</last><affiliation>University of Wisconsin-Madison</affiliation></author>
      <author><first>Niladri S.</first><last>Chatterji</last></author>
      <author><first>Sharan</first><last>Narang</last><affiliation>Meta</affiliation></author>
      <author><first>Mike</first><last>Lewis</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Dieuwke</first><last>Hupkes</last><affiliation>Facebook</affiliation></author>
      <pages>13295-13316</pages>
      <abstract>Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as ‘compute-optimally’ trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.</abstract>
      <url hash="b3c67b2b">2025.findings-acl.688</url>
      <bibkey>roberts-etal-2025-compute</bibkey>
    </paper>
    <paper id="689">
      <title><fixed-case>PECAN</fixed-case>: <fixed-case>LLM</fixed-case>-Guided Dynamic Progress Control with Attention-Guided Hierarchical Weighted Graph for Long-Document <fixed-case>QA</fixed-case></title>
      <author><first>Xinyu</first><last>Wang</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yanzheng</first><last>Xiang</last></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>13317-13335</pages>
      <abstract>Long-document QA presents challenges with large-scale text and long-distance dependencies. Recent advances in Large Language Models (LLMs) enable entire documents to be processed in a single pass. However, their computational cost is significantly high. Retrieval-Augmented Generation (RAG) methods split text into smaller chunks, but they often yield inferior results and may lose global context. Recent approaches that integrate LLMs into RAG via iterative summarization either underutilize LLM capabilities or still incur high computational costs. In this paper, we combine the high accuracy of LLMs with the efficiency of RAG and propose LLM-Guided Dynamic Progress Control with Attention-Based Hierarchical Weighted Graph (PECAN). Our method introduces two key improvements: (1) LLM-Guided Dynamic Progress Control: We leverage LLMs to dynamically control the retrieval process, adjusting the amount of retrieved information based on different queries to achieve a better balance of effectiveness and efficiency. (2) Attention-Guided Retrieval: We propose a novel retrieval method that constructs a hierarchical graph where edges are derived by LLM attention weights. Experimental results demonstrate that PECAN achieves LLM-level performance while maintaining computational complexity comparable to that of RAG methods on two single-document and two multi-document QA datasets.</abstract>
      <url hash="45b77680">2025.findings-acl.689</url>
      <bibkey>wang-etal-2025-pecan</bibkey>
    </paper>
    <paper id="690">
      <title>Lifelong Model Editing with Graph-Based External Memory</title>
      <author><first>Yash Kumar</first><last>Atri</last></author>
      <author><first>Ahmed</first><last>Alaa</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Thomas</first><last>Hartvigsen</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>13336-13352</pages>
      <abstract>Large language models (LLMs) have revolutionized natural language processing, yet their practical utility is often limited by persistent issues of hallucinations and outdated parametric knowledge. Although post-training model editing offers a pathway for dynamic updates, existing methods frequently suffer from overfitting and catastrophic forgetting. To tackle these challenges, we propose a novel framework that leverages hyperbolic geometry and graph neural networks for precise and stable model edits. We introduce HYPE, (HYperbolic Parameter Editing), which comprises three key components: (i) Hyperbolic Graph Construction, which uses Poincaré embeddings to represent knowledge triples in hyperbolic space, preserving hierarchical relationships and preventing unintended side effects by ensuring that edits to parent concepts do not inadvertently affect child concepts; (ii) Möbius-Transformed Updates, which apply hyperbolic addition to propagate edits while maintaining structural consistency within the hyperbolic manifold, unlike conventional Euclidean updates that distort relational distances; and (iii) Dual Stabilization, which combines gradient masking and periodic GNN parameter resetting to prevent catastrophic forgetting by focusing updates on critical parameters and preserving long-term knowledge. Experiments on CounterFact, CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE significantly enhances edit stability, factual accuracy, and multi-hop reasoning.</abstract>
      <url hash="9485305f">2025.findings-acl.690</url>
      <bibkey>atri-etal-2025-lifelong</bibkey>
    </paper>
    <paper id="691">
      <title>Multi-Sense Embeddings for Language Models and Knowledge Distillation</title>
      <author><first>Qitong</first><last>Wang</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Mohammed J</first><last>Zaki</last></author>
      <author><first>Georgios</first><last>Kollias</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Vasileios</first><last>Kalantzis</last><affiliation>International Business Machines</affiliation></author>
      <pages>13353-13369</pages>
      <abstract>Transformer-based large language models (LLMs) rely on contextual embeddings which generate different (continuous) representations for the same token depending on its surrounding context. Nonetheless, words and tokens typically have a limited number of senses (or meanings). We propose multi-sense embeddings as a drop-in replacement for each token in order to capture the range of their uses in a language. To construct a sense embedding dictionary, we apply a clustering algorithm to embeddings generated by an LLM and consider the cluster centers as representative sense embeddings. In addition, we propose a novel knowledge distillation method that leverages the sense dictionary to learn a smaller student model that mimics the senses from the much larger base LLM model, offering significant space and inference time savings, while maintaining competitive performance. Via thorough experiments on various benchmarks, we showcase the effectiveness of our sense embeddings and knowledge distillation approach.</abstract>
      <url hash="3d13a348">2025.findings-acl.691</url>
      <bibkey>wang-etal-2025-multi-sense</bibkey>
    </paper>
    <paper id="692">
      <title><fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>cientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation</title>
      <author><first>Peter</first><last>Jansen</last><affiliation>University of Arizona and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Marissa</first><last>Radensky</last></author>
      <author><first>Pao</first><last>Siangliulue</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Hebrew University, Hebrew University of Jerusalem and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bhavana</first><last>Dalvi Mishra</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Daniel S</first><last>Weld</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>13370-13467</pages>
      <abstract>Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.</abstract>
      <url hash="239cce09">2025.findings-acl.692</url>
      <bibkey>jansen-etal-2025-codescientist</bibkey>
    </paper>
    <paper id="693">
      <title>Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation</title>
      <author><first>Chris</first><last>Samarinas</last></author>
      <author><first>Alexander</first><last>Krubner</last><affiliation>Salzburg University of Applied Sciences</affiliation></author>
      <author><first>Alireza</first><last>Salemi</last></author>
      <author><first>Youngwoo</first><last>Kim</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <pages>13468-13482</pages>
      <abstract>This paper presents ICAT, an evaluation framework for measuring coverage of diverse factual information in long-form text generation. ICAT breaks down a long output text into a list of atomic claims and not only verifies each claim through retrieval from a (reliable) knowledge source, but also computes the alignment between the atomic factual claims and various aspects expected to be presented in the output. We study three implementations of the ICAT framework, each with a different assumption on the availability of aspects and alignment method. By adopting data from the diversification task in the TREC Web Track and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong correlation with human judgments and provide comprehensive evaluation across multiple state-of-the-art LLMs. Our framework further offers interpretable and fine-grained analysis of diversity and coverage. Its modular design allows for easy adaptation to different domains and datasets, making it a valuable tool for evaluating the qualitative aspects of long-form responses produced by LLMs.</abstract>
      <url hash="6410f6c1">2025.findings-acl.693</url>
      <bibkey>samarinas-etal-2025-beyond</bibkey>
    </paper>
    <paper id="694">
      <title>Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for <fixed-case>B</fixed-case>it<fixed-case>N</fixed-case>et language models?</title>
      <author><first>Jacob</first><last>Nielsen</last></author>
      <author><first>Peter</first><last>Schneider-Kamp</last><affiliation>University of Southern Denmark - SDU</affiliation></author>
      <author><first>Lukas</first><last>Galke</last><affiliation>University of Southern Denmark - SDU</affiliation></author>
      <pages>13483-13493</pages>
      <abstract>Large language models (LLMs) require immense resources for training and inference. Quantization, a technique that reduces the precision of model parameters, offers a promising solution for improving LLM efficiency and sustainability. While post-training quantization methods typically achieve 4-8 bits per parameter, recent research suggests that training LLMs with 1.58 bits per weight parameter from scratch can maintain model accuracy while greatly reducing memory requirements and energy consumption at inference time. Here, we investigate a training strategy for quantization-aware pre-training, where the models are first trained with 16-bit precision and then transition into 1.58-bit quantization-aware training. Our results on 11 downstream tasks, show that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit training and leaves models closer to those which have undergone 16-bit training. We further investigate the effects of retaining the optimizer state at the transition point and gradually phasing in quantization strength - finding that both techniques alleviate the magnitude of loss spikes, but also that these effects can be compensated through further training.</abstract>
      <url hash="1f18eeec">2025.findings-acl.694</url>
      <bibkey>nielsen-etal-2025-continual</bibkey>
    </paper>
    <paper id="695">
      <title>When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text</title>
      <author><first>Hillary</first><last>Dawkins</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>13494-13527</pages>
      <abstract>Detecting AI-generated text is a difficult problem to begin with; detecting AI-generated text on social media is made even more difficult due to the short text length and informal, idiosyncratic language of the internet. It is nonetheless important to tackle this problem, as social media represents a significant attack vector in online influence campaigns, which may be bolstered through the use of mass-produced AI-generated posts supporting (or opposing) particular policies, decisions, or events. We approach this problem with the mindset and resources of a reasonably sophisticated threat actor, and create a dataset of 505,159 AI-generated social media posts from a combination of open-source, closed-source, and fine-tuned LLMs, covering 11 different controversial topics. We show that while the posts can be detected under typical research assumptions about knowledge of and access to the generating models, under the more realistic assumption that an attacker will not release their fine-tuned model to the public, detectability drops dramatically. This result is confirmed with a human study. Ablation experiments highlight the vulnerability of various detection algorithms to fine-tuned LLMs. This result has implications across all detection domains, since fine-tuning is a generally applicable and realistic LLM use case.</abstract>
      <url hash="1b62e060">2025.findings-acl.695</url>
      <bibkey>dawkins-etal-2025-detection</bibkey>
    </paper>
    <paper id="696">
      <title>Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events</title>
      <author><first>James A.</first><last>Michaelov</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Reeka</first><last>Estacio</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zhien</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ben</first><last>Bergen</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>13528-13551</pages>
      <abstract>Can language models reliably predict that possible events are more likely than merely improbable ones? By teasing apart possibility, typicality, and contextual relatedness, we show that despite the results of previous work, language models’ ability to do this is far from robust. In fact, under certain conditions, all models tested—including Llama 3, Gemma 2, and Mistral NeMo—perform at worse-than-chance level, assigning higher probabilities to impossible sentences such as ‘the car was given a parking ticket by the brake’ than to merely unlikely sentences such as ‘the car was given a parking ticket by the explorer’.</abstract>
      <url hash="2b2e3a61">2025.findings-acl.696</url>
      <bibkey>michaelov-etal-2025-quite</bibkey>
    </paper>
    <paper id="697">
      <title>The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval</title>
      <author><first>Ting-Rui</first><last>Chiang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Dani</first><last>Yogatama</last><affiliation>Google DeepMind and DeepMind</affiliation></author>
      <pages>13552-13562</pages>
      <abstract>The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.</abstract>
      <url hash="a69d0dd4">2025.findings-acl.697</url>
      <bibkey>chiang-yogatama-2025-rotary</bibkey>
    </paper>
    <paper id="698">
      <title><fixed-case>IDEA</fixed-case>: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction</title>
      <author><first>Kaiyu</first><last>He</last></author>
      <author><first>Mian</first><last>Zhang</last></author>
      <author><first>Shuo</first><last>Yan</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Peilin</first><last>Wu</last></author>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <pages>13563-13597</pages>
      <abstract>While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of **I**nduction, **De**duction, and **A**bduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data have been released at: https://github.com/KaiyuHe998/RULEARN_IDEA.</abstract>
      <url hash="62c0760e">2025.findings-acl.698</url>
      <bibkey>he-etal-2025-idea</bibkey>
    </paper>
    <paper id="699">
      <title><fixed-case>E</fixed-case>nigma<fixed-case>T</fixed-case>o<fixed-case>M</fixed-case>: Improve <fixed-case>LLM</fixed-case>s’ Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States</title>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Siya</first><last>Qi</last></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yuxiang</first><last>Zhou</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Jinhua</first><last>Du</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Caroline</first><last>Catmur</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>13598-13622</pages>
      <abstract>Theory-of-Mind (ToM), the ability to infer others’ perceptions and mental states, is fundamental to human interaction but remains challenging for Large Language Models (LLMs). While existing ToM reasoning methods show promise with reasoning via perceptual perspective-taking, they often rely excessively on off-the-shelf LLMs, reducing their efficiency and limiting their applicability to high-order ToM reasoning. To address these issues, we present EnigmaToM, a novel neuro-symbolic framework that enhances ToM reasoning by integrating a Neural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired iterative masking mechanism that facilitates accurate perspective-taking and (2) knowledge injection that elicits key entity information. Enigma generates structured knowledge of entity states to build spatial scene graphs for belief tracking across various ToM orders and enrich events with fine-grained entity state details. Experimental results on ToMi, HiToM, and FANToM benchmarks show that EnigmaToM significantly improves ToM reasoning across LLMs of varying sizes, particularly excelling in high-order reasoning scenarios.</abstract>
      <url hash="25853233">2025.findings-acl.699</url>
      <bibkey>xu-etal-2025-enigmatom</bibkey>
    </paper>
    <paper id="700">
      <title><fixed-case>R</fixed-case>easoner<fixed-case>R</fixed-case>ank: Redefining Language Model Evaluation with Ground-Truth-Free Ranking Frameworks</title>
      <author><first>Jiamu</first><last>Zhang</last><affiliation>Rice University</affiliation></author>
      <author><first>Jiayi</first><last>Yuan</last></author>
      <author><first>Andrew</first><last>Wen</last><affiliation>Rice University and University of Texas Health Science Center at Houston</affiliation></author>
      <author><first>Hoang Anh Duy</first><last>Le</last><affiliation>Rice University</affiliation></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Soo-Hyun</first><last>Choi</last><affiliation>Samsung Electronics America</affiliation></author>
      <author><first>Rui</first><last>Chen</last><affiliation>Samsung Electronics America</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>13623-13639</pages>
      <abstract>Large Language Models (LLMs) are increasingly adopted across real-world applications, yet traditional evaluations rely on expensive, domain-specific ground-truth labels that are often unavailable or infeasible. We introduce a ground-truth-free evaluation framework focused on reasoning consistency and instruction following, shifting the emphasis from correctness—which is elusive without labels—to transparent, coherent, evidence-based reasoning. Each model response must include a direct answer, a structured multi-step explanation, and supporting evidence, all assessed via semantic similarity and output adherence checks. We further propose TopK-ReRank, which refines rankings by constructing a consensus answer from the most reliable models, reducing ambiguity across diverse reasoning styles. Experiments show that our framework outperforms existing label-free methods, including majority voting, triplet ranking, and peer-review approaches, providing a more interpretable and efficient alternative for evaluating LLMs in the absence of ground-truth labels.</abstract>
      <url hash="3866fbe8">2025.findings-acl.700</url>
      <bibkey>zhang-etal-2025-reasonerrank</bibkey>
    </paper>
    <paper id="701">
      <title><fixed-case>H</fixed-case>y<fixed-case>G</fixed-case>enar: An <fixed-case>LLM</fixed-case>-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation</title>
      <author><first>Weizhi</first><last>Tang</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yixuan</first><last>Li</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Chris</first><last>Sypherd</last></author>
      <author><first>Elizabeth</first><last>Polgreen</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Vaishak</first><last>Belle</last><affiliation>University of Toronto, University of Edinburgh and KU Leuven</affiliation></author>
      <pages>13640-13665</pages>
      <abstract>Grammar plays a critical role in natural language processing and text/code generation by enabling the definition of syntax, the creation of parsers, and guiding structured outputs. Although large language models (LLMs) demonstrate impressive capabilities across domains, their ability to infer and generate grammars has not yet been thoroughly explored. In this paper, we aim to study and improve the ability of LLMs for few-shot grammar generation, where grammars are inferred from sets of a small number of positive and negative examples and generated in Backus-Naur Form. To explore this, we introduced a novel dataset comprising 540 structured grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs against it. Our findings reveal that existing LLMs perform sub-optimally in grammar generation. To address this, we propose an LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements in both the syntactic and semantic correctness of generated grammars across LLMs.</abstract>
      <url hash="51ca2ac4">2025.findings-acl.701</url>
      <bibkey>tang-etal-2025-hygenar</bibkey>
    </paper>
    <paper id="702">
      <title>Can Large Language Models Understand Argument Schemes?</title>
      <author><first>Elfia</first><last>Bezou-Vrakatseli</last></author>
      <author><first>Oana</first><last>Cocarascu</last><affiliation>King’s College London</affiliation></author>
      <author><first>Sanjay</first><last>Modgil</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>13666-13681</pages>
      <abstract>Argument schemes represent stereotypical patterns of reasoning that occur in everyday arguments. However, despite their usefulness, argument scheme classification, that is classifying natural language arguments according to the schemes they are instances of, is an under-explored task in NLP. In this paper we present a systematic evaluation of large language models (LLMs) for classifying argument schemes based on Walton’s taxonomy. We experiment with seven LLMs in zero-shot, few-shot, and chain-of-thought prompting, and explore two strategies to enhance task instructions: employing formal definitions and LLM-generated descriptions. Our analysis on both manually annotated and automatically generated arguments, including enthymemes, indicates that while larger models exhibit satisfactory performance in identifying argument schemes, challenges remain for smaller models. Our work offers the first comprehensive assessment of LLMs in identifying argument schemes, and provides insights for advancing reasoning capabilities in computational argumentation.</abstract>
      <url hash="18f58e50">2025.findings-acl.702</url>
      <bibkey>bezou-vrakatseli-etal-2025-large</bibkey>
    </paper>
    <paper id="703">
      <title><fixed-case>MMI</fixed-case>n<fixed-case>A</fixed-case>: Benchmarking Multihop Multimodal <fixed-case>I</fixed-case>nternet Agents</title>
      <author><first>Shulin</first><last>Tian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ziniu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Liangyu</first><last>Chen</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>13682-13697</pages>
      <abstract>Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: ***1) Evolving real-world multimodal websites.*** Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to extract multimodal information from web pages as observations autonomously. ***2) Multihop web browsing.*** Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks. ***3) Holistic evaluation.*** We propose a novel protocol for evaluating an agent’s progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks of more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach replaying past action trajectories to reflect. Our method significantly improves the performance of both the single-hop and multihop web browsing abilities.</abstract>
      <url hash="e99989d0">2025.findings-acl.703</url>
      <bibkey>tian-etal-2025-mmina</bibkey>
    </paper>
    <paper id="704">
      <title><fixed-case>T</fixed-case>hink<fixed-case>G</fixed-case>uard: Deliberative Slow Thinking Leads to Cautious Guardrails</title>
      <author><first>Xiaofei</first><last>Wen</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Wenjie Jacky</first><last>Mo</last></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>13698-13713</pages>
      <abstract>Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail’s cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.</abstract>
      <url hash="eb9b5c25">2025.findings-acl.704</url>
      <bibkey>wen-etal-2025-thinkguard</bibkey>
    </paper>
    <paper id="705">
      <title>Neutralizing Bias in <fixed-case>LLM</fixed-case> Reasoning using Entailment Graphs</title>
      <author><first>Liang</first><last>Cheng</last></author>
      <author><first>Tianyi</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Edinburgh University, University of Edinburgh and Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianyang</first><last>Liu</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Mark</first><last>Steedman</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>13714-13730</pages>
      <abstract>LLMs are often claimed to be capable of Natural Language Inference (NLI), which is widely regarded as a cornerstone of more complex forms of reasoning. However, recent works show that LLMs still suffer from hallucinations in NLI due to <i>attestation bias</i>, where LLMs overly rely on propositional memory to build shortcuts. To solve the issue, we design an unsupervised framework to construct counterfactual reasoning data and fine-tune LLMs to reduce attestation bias. To measure bias reduction, we build <i>bias-adversarial</i> variants of NLI datasets with randomly replaced predicates in premises while keeping hypotheses unchanged. Extensive evaluations show that our framework can significantly reduce hallucinations from attestation bias. Then, we further evaluate LLMs fine-tuned with our framework on original NLI datasets and their bias-neutralized versions, where original entities are replaced with randomly sampled ones. Extensive results show that our framework consistently improves inferential performance on both original and bias-neutralized NLI datasets.</abstract>
      <url hash="6ef37906">2025.findings-acl.705</url>
      <bibkey>cheng-etal-2025-neutralizing</bibkey>
    </paper>
    <paper id="706">
      <title>Dynamic Steering With Episodic Memory For Large Language Models</title>
      <author><first>Van Dai</first><last>Do</last></author>
      <author><first>Quan Hung</first><last>Tran</last><affiliation>Facebook</affiliation></author>
      <author><first>Svetha</first><last>Venkatesh</last><affiliation>Deakin University</affiliation></author>
      <author><first>Hung</first><last>Le</last><affiliation>Deakin University</affiliation></author>
      <pages>13731-13749</pages>
      <abstract>Large Language Models (LLMs) exhibit emergent in-context learning (ICL) capabilities, allowing them to adapt to unseen tasks based on example demonstrations. Traditional ICL embeds examples within the prompt, while activation steering, uses a vector derived from examples to guide the latent states of LLMs toward desired behaviors. However, traditional ICL is difficult to control quantitatively and consumes valuable context space. Existing activation steering methods apply a single sentence-level steering vector uniformly across all tokens, ignoring LLMs’ token-wise, auto-regressive nature. This coarse control can lead to inconsistencies and suboptimal adjustments during generation. To address this problem, we introduce Dynamic Steering with Episodic Memory (DSEM), a novel training-free framework that aligns LLMs to given demonstrations by steering at the token level conditioned on the input query. DSEM employs a key-value memory to store associations between generated tokens and steering vectors. During inference, it uses a nearest-neighbor mechanism to dynamically compute steering vectors for each token chunk, enabling more precise and adaptive guidance. Our method surpasses strong baselines across diverse alignment tasks - including safety, style transfer, and role-playing - demonstrating improved alignment as demonstration size scales.</abstract>
      <url hash="522c08a5">2025.findings-acl.706</url>
      <bibkey>do-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="707">
      <title>Eeyore: Realistic Depression Simulation via Expert-in-the-Loop Supervised and Preference Optimization</title>
      <author><first>Siyang</first><last>Liu</last></author>
      <author><first>Bianca</first><last>Brie</last></author>
      <author><first>Wenda</first><last>Li</last><affiliation/></author>
      <author><first>Laura</first><last>Biester</last><affiliation>Middlebury College</affiliation></author>
      <author><first>Andrew</first><last>Lee</last><affiliation>School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>James</first><last>Pennebaker</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>13750-13770</pages>
      <abstract>Large Language Models (LLMs) have been previously explored for mental healthcare training and therapy client simulation, but they still fall short in authentically capturing diverse client traits and psychological conditions. We introduce <b>Eeyore</b> , an 8B model optimized for realistic depression simulation through a structured alignment framework, incorporating expert input at every stage.First, we systematically curate real-world depression-related conversations, extracting depressive traits to guide data filtering and psychological profile construction, and use this dataset to instruction-tune Eeyore for profile adherence. Next, to further enhance realism, Eeyore undergoes iterative preference optimization—first leveraging model-generated preferences and then calibrating with a small set of expert-annotated preferences.Throughout the entire pipeline, we actively collaborate with domain experts, developing interactive interfaces to validate trait extraction and iteratively refine structured psychological profiles for clinically meaningful role-play customization.Despite its smaller model size, the Eeyore depression simulation outperforms GPT-4o with SOTA prompting strategies, both in linguistic authenticity and profile adherence.</abstract>
      <url hash="17e212a3">2025.findings-acl.707</url>
      <bibkey>liu-etal-2025-eeyore</bibkey>
    </paper>
    <paper id="708">
      <title>Lost in Translation: Benchmarking Commercial Machine Translation Models for Dyslexic-Style Text</title>
      <author><first>Gregory</first><last>Price</last><affiliation>AImpower.org</affiliation></author>
      <author><first>Shaomei</first><last>Wu</last><affiliation>AImpower.org</affiliation></author>
      <pages>13771-13782</pages>
      <abstract>Dyslexia can affect writing, leading to unique patterns such as letter and homophone swapping. As a result, text produced by people with dyslexia often differs from the text typically used to train natural language processing (NLP) models, raising concerns about their effectiveness for dyslexic users. This paper examines the fairness of four commercial machine translation (MT) systems towards dyslexic text through a systematic audit using both synthetically generated dyslexic text and real writing from individuals with dyslexia. By programmatically introducing various dyslexic-style errors into the WMT dataset, we present insights on how dyslexic biases manifest in MT systems as the text becomes more dyslexic, especially with real-word errors. Our results shed light on the NLP biases affecting people with dyslexia – a population that often relies on NLP tools as assistive technologies, highlighting the need for more diverse data and user representation in the development of foundational NLP models.</abstract>
      <url hash="6b2aa463">2025.findings-acl.708</url>
      <bibkey>price-wu-2025-lost</bibkey>
    </paper>
    <paper id="709">
      <title>Divide-Verify-Refine: Can <fixed-case>LLM</fixed-case>s Self-align with Complex Instructions?</title>
      <author><first>Xianren</first><last>Zhang</last></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zongyu</first><last>Wu</last><affiliation>The Pennsylvania State University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>13783-13800</pages>
      <abstract>Recent studies show LLMs struggle with complex instructions involving multiple constraints (e.g., length, format, sentiment). Existing research enhances open-source LLMs using closed-source guidance (e.g., GPT-4), but this heavily relies on generated data quality. An alternative is leveraging LLMs’ self-correction to refine responses for better constraint adherence. However, this is limited by the feedback quality, as we found LLMs cannot generate reliable feedback or detect errors. Moreover, the self-correction effectiveness relies on few-shot examples illustrating response modifications. As constraints in complex instructions are diverse, manually crafting such examples for each constraint type can be labor-intensive and sub-optimal. To address these two challenges, we propose the Divide-Verify-Refine (DVR) framework with three steps: (1) Divide complex instructions into single constraints and prepare appropriate tools; (2) Verify responses using tools that provide rigorous check and textual guidance (e.g., Python scripts for format checks or pre-trained classifiers for content analysis); (3) Refine: To maximize refinement effectiveness, we propose dynamic few-shot prompting, where a refinement repository collects successful refinements, and these examples are selectively retrieved for future refinements. Recognizing the lack of complexity in existing datasets, we create a new dataset of complex instructions. DVR doubles Llama3.1-8B’s constraint adherence and triples Mistral-7B’s performance.</abstract>
      <url hash="10f2476e">2025.findings-acl.709</url>
      <bibkey>zhang-etal-2025-divide</bibkey>
    </paper>
    <paper id="710">
      <title><fixed-case>L</fixed-case>lama<fixed-case>PIE</fixed-case>: Proactive In-Ear Conversation Assistants</title>
      <author><first>Tuochao</first><last>Chen</last></author>
      <author><first>Nicholas Scott</first><last>Batchelder</last><affiliation>University of Washington</affiliation></author>
      <author><first>Alisa</first><last>Liu</last><affiliation>NVIDIA and University of Washington</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Shyamnath</first><last>Gollakota</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>13801-13824</pages>
      <abstract>We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive AI assistant, highlighting the potential of LlamaPIE to enhance live conversations.</abstract>
      <url hash="6bad1e05">2025.findings-acl.710</url>
      <bibkey>chen-etal-2025-llamapie</bibkey>
    </paper>
    <paper id="711">
      <title>Task-Oriented Automatic Fact-Checking with Frame-Semantics</title>
      <author><first>Jacob</first><last>Devasier</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Akshith Reddy</first><last>Putta</last></author>
      <author><first>Rishabh</first><last>Mediratta</last></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>13825-13842</pages>
      <abstract>We propose a novel paradigm for automatic fact-checking that leverages frame semantics to enhance the structured understanding of claims and guide the process of fact-checking them. To support this, we introduce a pilot dataset of real-world claims extracted from PolitiFact, specifically annotated for large-scale structured data. This dataset underpins two case studies: the first investigates voting-related claims using the Vote semantic frame, while the second explores various semantic frames based on data sources from the Organisation for Economic Co-operation and Development (OECD). Our findings demonstrate the effectiveness of frame semantics in improving evidence retrieval and explainability for fact-checking. Finally, we conducted a survey of frames evoked in fact-checked claims, identifying high-impact frames to guide future work in this direction.</abstract>
      <url hash="9010d471">2025.findings-acl.711</url>
      <bibkey>devasier-etal-2025-task</bibkey>
    </paper>
    <paper id="712">
      <title><fixed-case>C</fixed-case>raw4<fixed-case>LLM</fixed-case>: Efficient Web Crawling for <fixed-case>LLM</fixed-case> Pretraining</title>
      <author><first>Shi</first><last>Yu</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>13843-13851</pages>
      <abstract>Web crawl is a main source of large language models’ (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler’s scheduler, replacing the standard graph-connectivity-based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine’s index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.</abstract>
      <url hash="e88e2949">2025.findings-acl.712</url>
      <bibkey>yu-etal-2025-craw4llm</bibkey>
    </paper>
    <paper id="713">
      <title>Be Cautious When Merging Unfamiliar <fixed-case>LLM</fixed-case>s: A Phishing Model Capable of Stealing Privacy</title>
      <author><first>Guo</first><last>Zhenyuan</last></author>
      <author><first>Yi</first><last>Shi</last></author>
      <author><first>Wenlong</first><last>Meng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Chengkun</first><last>Wei</last></author>
      <author><first>Wenzhi</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>13852-13871</pages>
      <abstract>Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: *an unsafe model could compromise the privacy of other LLMs involved in the model merging*. Specifically, we propose *PhiMM*, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9% and MI leakage increased by 17.4% on average. We release the code of *PhiMM* through an anonymous link.</abstract>
      <url hash="9f7eff04">2025.findings-acl.713</url>
      <bibkey>zhenyuan-etal-2025-cautious</bibkey>
    </paper>
    <paper id="714">
      <title>Understand User Opinions of Large Language Models via <fixed-case>LLM</fixed-case>-Powered In-the-Moment User Experience Interviews</title>
      <author><first>Mengqiao</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Tevin</first><last>Wang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Cassandra A.</first><last>Cohen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sarah</first><last>Li</last></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>13872-13893</pages>
      <abstract>Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer.</abstract>
      <url hash="0164dc0b">2025.findings-acl.714</url>
      <bibkey>liu-etal-2025-understand</bibkey>
    </paper>
    <paper id="715">
      <title><fixed-case>H</fixed-case>i<fixed-case>COT</fixed-case>: Improving Neural Topic Models via Optimal Transport and Contrastive Learning</title>
      <author><first>Hoang Tran</first><last>Vuong</last></author>
      <author><first>Tue</first><last>Le</last></author>
      <author><first>Tu</first><last>Vu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Tung</first><last>Nguyen</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Sang</first><last>Dinh</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>13894-13920</pages>
      <abstract>Recent advances in neural topic models (NTMs) have improved topic quality but still face challenges: weak document-topic alignment, high inference costs due to large pretrained language models (PLMs), and limited modeling of hierarchical topic structures. To address these issues, we introduce HiCOT (Hierarchical Clustering and Contrastive Learning with Optimal Transport for Neural Topic Modeling), a novel framework that enhances topic coherence and efficiency. HiCOT integrates Optimal Transport to refine document-topic relationships using compact PLM-based embeddings, captures semantic structure of the documents. Additionally, it employs hierarchical clustering combine with contrastive learning to disentangle topic-word and topic-topic relationships, ensuring clearer structure and better coherence. Experimental results on multiple benchmark datasets demonstrate HiCOT’s superior effectiveness over existing NTMs in topic coherence, topic performance, representation quality, and computational efficiency.</abstract>
      <url hash="b8e5e161">2025.findings-acl.715</url>
      <bibkey>vuong-etal-2025-hicot</bibkey>
    </paper>
    <paper id="716">
      <title><fixed-case>FLAG</fixed-case>-<fixed-case>TRADER</fixed-case>: Fusion <fixed-case>LLM</fixed-case>-Agent with Gradient-based Reinforcement Learning for Financial Trading</title>
      <author><first>Guojun</first><last>Xiong</last><affiliation>Harvard University</affiliation></author>
      <author><first>Zhiyang</first><last>Deng</last></author>
      <author><first>Keyi</first><last>Wang</last></author>
      <author><first>Yupeng</first><last>Cao</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Haohang</first><last>Li</last></author>
      <author><first>Yangyang</first><last>Yu</last><affiliation>Accenture</affiliation></author>
      <author><first>Xueqing</first><last>Peng</last><affiliation>Yale University</affiliation></author>
      <author><first>Mingquan</first><last>Lin</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Kaleb E</first><last>Smith</last></author>
      <author><first>Xiao-Yang</first><last>Liu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Qianqian</first><last>Xie</last></author>
      <pages>13921-13934</pages>
      <abstract>Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.</abstract>
      <url hash="83b7d454">2025.findings-acl.716</url>
      <bibkey>xiong-etal-2025-flag</bibkey>
    </paper>
    <paper id="717">
      <title>The Silent Saboteur: Imperceptible Adversarial Attacks against Black-Box Retrieval-Augmented Generation Systems</title>
      <author><first>Hongru</first><last>Song</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu-An</first><last>Liu</last></author>
      <author><first>Ruqing</first><last>Zhang</last></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jianming</first><last>Lv</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Maarten</first><last>de Rijke</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>13935-13952</pages>
      <abstract>We explore adversarial attacks against retrieval-augmented generation (RAG) systems to identify their vulnerabilities. We focus on generating human-imperceptible adversarial examples and introduce a novel imperceptible retrieve-to-generate attack against RAG. This task aims to find imperceptible perturbations that retrieve a target document, originally excluded from the initial top-k candidate set, in order to influence the final answer generation. To address this task, we propose ReGENT, a reinforcement learning-based framework that tracks interactions between the attacker and the target RAG and continuously refines attack strategies based on relevance-generation-naturalness rewards. Experiments on newly constructed factual and non-factual question-answering benchmarks demonstrate that ReGENT significantly outperforms existing attack methods in misleading RAG systems with small imperceptible text perturbations.</abstract>
      <url hash="69ed1906">2025.findings-acl.717</url>
      <bibkey>song-etal-2025-silent</bibkey>
    </paper>
    <paper id="718">
      <title><fixed-case>CROSSAGENTIE</fixed-case>: Cross-Type and Cross-Task Multi-Agent <fixed-case>LLM</fixed-case> Collaboration for Zero-Shot Information Extraction</title>
      <author><first>Meng</first><last>Lu</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Yuzhang</first><last>Xie</last></author>
      <author><first>Zhenyu</first><last>Bi</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Shuxiang</first><last>Cao</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>13953-13977</pages>
      <abstract>Large language models (LLMs) excel in generating unstructured text. However, they struggle with producing structured output while maintaining accuracy in zero-shot information extraction (IE), such as named entity recognition (NER) and relation extraction (RE). To address these challenges, we propose CROSSAGENTIE, a multi-agent framework that enhances zero-shot IE through multi-agent LLM collaboration. CROSSAGENTIE refines LLM predictions iteratively through two mechanisms: intra-group cross-type debate, which resolves entity-label conflicts through context-based evidence and confidence aggregation, and inter-group cross-task debate, where NER and RE mutually refine outputs via bidirectional feedback. Furthermore, we introduce template fine-tuning, distilling high-confidence multi-agent outputs into a single model, significantly reducing inference cost while preserving accuracy. Experiments across five NER and five RE datasets show that CROSSAGENTIE significantly outperforms state-of-the-art zero-shot baselines by a large margin. CROSSAGENTIE effectively addresses LLMs limitations in structured prediction with an effective and efficient approach for zero-shot information extraction.</abstract>
      <url hash="8821c7a1">2025.findings-acl.718</url>
      <bibkey>lu-etal-2025-crossagentie</bibkey>
    </paper>
    <paper id="719">
      <title>Decoupling Memories, Muting Neurons: Towards Practical Machine Unlearning for Large Language Models</title>
      <author><first>Lishuai</first><last>Hou</last></author>
      <author><first>Zixiong</first><last>Wang</last></author>
      <author><first>Gaoyang</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Chen</first><last>Wang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Kai</first><last>Peng</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>13978-13999</pages>
      <abstract>Machine Unlearning (MU) has emerged as a promising solution for removing the influence of data that an owner wishes to unlearn from Large Language Models (LLMs). However, existing MU methods, which require tuning the entire model parameters on the unlearned data with random labels or perturbed gradients, significantly degrade model utility, especially given the difficulty of accessing the original training data. This presents a key challenge: how can we achieve MU using only the unlearned data while preserving model utility?In this paper, we propose NeuMuter, a simple but effective MU method that eliminates the influence of unlearned data from LLMs by modulating the outputs of merely 1% of the neurons in the feed-forward network (FFN) modules within the Transformer blocks, minimizing disruption to the model’s performance. We design a trainable masking scheme that decouples the memorization of different training data within the neurons of LLMs, allowing us to precisely identify and modify neurons associated with the unlearned data. Through comprehensive evaluations on two benchmarks across four different LLMs, we demonstrate that modifying the outputs of a few fraction of the total neurons can effectively achieve MU while preserving the model’s utility across downstream tasks.</abstract>
      <url hash="279d7404">2025.findings-acl.719</url>
      <bibkey>hou-etal-2025-decoupling</bibkey>
    </paper>
    <paper id="720">
      <title>Assimilation and Accommodation: Task-Adaptive Hierarchical Abstraction for Solving Web Tasks</title>
      <author><first>Xinyu</first><last>Pang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Ruixin</first><last>Hong</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Changshui</first><last>Zhang</last><affiliation>Tsinghua University and Tsinghua University</affiliation></author>
      <pages>14000-14014</pages>
      <abstract>Web tasks, which involve processing data from online resources, challenge agents to generalize beyond fixed knowledge to unseen task contexts. Learning from experience, the ability to derive reusable patterns from past tasks, is crucial for improving generalization. However, existing methods focus on summarizing workflows, i.e., common sub-routines, which may introduce excessive low-level details that distract models. Additionally, the absence of task-specific objectives can lead to inconsistencies between workflows and future task queries, hindering reasoning performance. This paper seeks to mitigate these issues by proposing <tex-math>A^2</tex-math>, a framework that derives task-adaptive hierarchical abstraction to enhance web task reasoning. Our approach first extracts general-purpose semantic abstraction from past task-solution pairs. Combined with the next task query, this abstraction forms a task-adaptive episodic abstraction that guides subsequent reasoning. Experiments show that <tex-math>A^2</tex-math> achieves superior performance with competitive cost-efficiency, improving success rates by 0.7% on Mind2web and 4.6% on Webarena.</abstract>
      <url hash="e98a2102">2025.findings-acl.720</url>
      <bibkey>pang-etal-2025-assimilation</bibkey>
    </paper>
    <paper id="721">
      <title><fixed-case>S</fixed-case>afe<fixed-case>L</fixed-case>aw<fixed-case>B</fixed-case>ench: Towards Safe Alignment of Large Language Models</title>
      <author><first>Chuxue</first><last>Cao</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Han</first><last>Zhu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Qichao</first><last>Sun</last><affiliation>The Hong Kong University of Science and Technology and Guangdong University of Technology</affiliation></author>
      <author><first>Zhenghao</first><last>Zhu</last></author>
      <author><first>Wu</first><last>Yinyu</last></author>
      <author><first>Josef</first><last>Dai</last><affiliation>Peking University</affiliation></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>14015-14048</pages>
      <abstract>With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMs’ safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMs’ safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8%. We urge the community to prioritize research on the safety of LLMs.</abstract>
      <url hash="149af710">2025.findings-acl.721</url>
      <bibkey>cao-etal-2025-safelawbench</bibkey>
    </paper>
    <paper id="722">
      <title>3<fixed-case>DM</fixed-case>: Distill, Dynamic Drop, and Merge for Debiasing Multi-modal Large Language Models</title>
      <author><first>Zhaoxi</first><last>Zhang</last></author>
      <author><first>Sanwoo</first><last>Lee</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhixiang</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>14049-14059</pages>
      <abstract>The rapid advancement of Multi-modal Language Models (MLLMs) has significantly enhanced performance in multimodal tasks, yet these models often exhibit inherent biases that compromise their reliability and fairness. Traditional debiasing methods face a trade-off between the need for extensive labeled datasets and high computational costs. Model merging, which efficiently combines multiple models into a single one, offers a promising alternative but its usage is limited to MLLMs with the same architecture. We propose 3DM, a novel framework integrating Distill, Dynamic Drop, and Merge to address these challenges. 3DM employs knowledge distillation to harmonize models with divergent architectures and introduces a dynamic dropping strategy that assigns parameter-specific drop rates based on their contributions to bias and overall performance. This approach preserves critical weights while mitigating biases, as validated on the MMSD2.0 sarcasm detection dataset. Our key contributions include architecture-agnostic merging, dynamic dropping, and the introduction of the Bias Ratio (BR) metric for systematic bias assessment. Empirical results demonstrate that 3DM outperforms existing methods in balancing debiasing and enhancing the overall performance, offering a practical and scalable solution for deploying fair and efficient MLLMs in real-world applications.</abstract>
      <url hash="b683289b">2025.findings-acl.722</url>
      <bibkey>zhang-etal-2025-3dm</bibkey>
    </paper>
    <paper id="723">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>A</fixed-case>bstain: Enhancing Multilingual <fixed-case>LLM</fixed-case>s with Causal Reasoning for Trustworthy Abstention</title>
      <author><first>Yuxi</first><last>Sun</last></author>
      <author><first>Aoqi</first><last>Zuo</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Wei</first><last>Gao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>14060-14076</pages>
      <abstract>Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to <tex-math>\textit{abstain}</tex-math> when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce <tex-math>\textit{CausalAbstain}</tex-math>, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that <tex-math>\textit{CausalAbstain}</tex-math> effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (<tex-math>\textit{Casual-native}</tex-math>) and multilingual (<tex-math>\textit{Causal-multi}</tex-math>) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks.</abstract>
      <url hash="d38f64b6">2025.findings-acl.723</url>
      <bibkey>sun-etal-2025-causalabstain</bibkey>
    </paper>
    <paper id="724">
      <title><fixed-case>C</fixed-case>ap<fixed-case>A</fixed-case>rena: Benchmarking and Analyzing Detailed Image Captioning in the <fixed-case>LLM</fixed-case> Era</title>
      <author><first>Kanzhi</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Wenpo</first><last>Song</last></author>
      <author><first>Jiaxin</first><last>Fan</last></author>
      <author><first>Zheng</first><last>Ma</last></author>
      <author><first>Qiushi</first><last>Sun</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Fangzhi</first><last>Xu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chenyang</first><last>Yan</last></author>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Jianbing</first><last>Zhang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>14077-14094</pages>
      <abstract>Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our Arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show high caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 93.4% correlation with human rankings at just $4 per test. All data and evaluation resources have been open-sourced.</abstract>
      <url hash="9508bd14">2025.findings-acl.724</url>
      <bibkey>cheng-etal-2025-caparena</bibkey>
    </paper>
    <paper id="725">
      <title><fixed-case>LLM</fixed-case>-Empowered Class Imbalanced Graph Prompt Learning for Online Drug Trafficking Detection</title>
      <author><first>Tianyi</first><last>Ma</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yiyue</first><last>Qian</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Zehong</first><last>Wang</last></author>
      <author><first>Zheyuan</first><last>Zhang</last></author>
      <author><first>Chuxu</first><last>Zhang</last><affiliation>University of Connecticut</affiliation></author>
      <author><first>Yanfang</first><last>Ye</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>14095-14114</pages>
      <abstract>As the market for illicit drugs remains extremely profitable, major online platforms have become direct-to-consumer intermediaries for illicit drug trafficking participants. These online activities raise significant social concerns that require immediate actions. Existing approaches to combat this challenge are generally impractical due to the scarcity of labeled samples and imbalance of classes in real-world applications. To this end, we propose a novel <tex-math>\textbf{L}</tex-math>arge <tex-math>\textbf{L}</tex-math>anguage <b>M</b>odel-empowered <tex-math>\textbf{Het}</tex-math>erogeneous <tex-math>\textbf{G}</tex-math>raph Prompt Learning framework for illicit <tex-math>\textbf{D}</tex-math>rug <tex-math>\textbf{T}</tex-math>rafficking detection, called <tex-math>\textbf{LLM-HetGDT}</tex-math> that leverages LLM to facilitate heterogeneous graph neural networks (HGNNs) to effectively identify minority classes, i.e., drug trafficking participants, in the class-imbalanced scenarios. Specifically, we first pre-train HGNN over a contrastive pretext task to capture the inherent node and structure information over an unlabeled drug trafficking heterogeneous graph (HG). Afterward, to alleviate the class-imbalanced issue, we leverage LLMs to augment the HG by generating high-quality synthetic user nodes in the minority classes. Then, we fine-tune the soft prompts on the augmented HG to capture the important information in the minority classes for the downstream drug trafficking detection task. To comprehensively study online illicit drug trafficking activities, we collect a new HG dataset over Twitter, called Twitter-HetDrug. Extensive experiments on this dataset demonstrate the effectiveness, efficiency, and applicability of our proposed method by comparing it with state-of-the-art baseline methods. Our source code is available at https://github.com/GraphResearcher/LLM-HetGDT.</abstract>
      <url hash="41e844cf">2025.findings-acl.725</url>
      <bibkey>ma-etal-2025-llm</bibkey>
    </paper>
    <paper id="726">
      <title><fixed-case>C</fixed-case>o<fixed-case>LA</fixed-case>: Collaborative Low-Rank Adaptation</title>
      <author><first>Yiyun</first><last>Zhou</last><affiliation>Zhejiang University, Renmin University of China, Renmin University of China, Renmin University of China, Yale University, Department of Computer Science, Yale University, University of Michigan - Ann Arbor, Electrical Engineering and Computer Science, University of Michigan - Ann Arbor, National University of Singapore, National University of Singapore, Beijing Jiaotong University, University of California, Berkeley, Electrical Engineering &amp; Computer Science Department, University of California, Berkeley, UC Berkeley, University of California, Berkeley, Computer Science and Artificial Intelligence Laboratory, Electrical Engineering &amp; Computer Science, Massachusetts Institute of Technology and Montanuniversität Leoben</affiliation></author>
      <author><first>Chang</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>14115-14130</pages>
      <abstract>The scaling law of Large Language Models (LLMs) reveals a power-law relationship, showing diminishing return on performance as model scale increases. While training LLMs from scratch is resource-intensive, fine-tuning a pre-trained model for specific tasks has become a practical alternative. Full fine-tuning (FFT) achieves strong performance; however, it is computationally expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like LoRA, have been proposed to address these challenges by freezing the pre-trained model and adding lightweight task-specific modules. LoRA, in particular, has proven effective, but its application to multi-task scenarios is limited by interference between tasks. Recent approaches, such as Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these issues but still struggle with sample scarcity and noise interference due to their fixed structure. In response, we propose CoLA, a more flexible LoRA architecture with an efficient initialization scheme, which introduces three collaborative strategies to enhance performance by better utilizing the quantitative relationships between matrices <tex-math>A</tex-math> and <tex-math>B</tex-math>. Our experiments demonstrate the effectiveness and robustness of CoLA, outperforming existing PEFT methods, especially in low-sample scenarios. Our data and code are fully publicly available: https://github.com/zyy-2001/CoLA.</abstract>
      <url hash="9a158d07">2025.findings-acl.726</url>
      <bibkey>zhou-etal-2025-cola</bibkey>
    </paper>
    <paper id="727">
      <title><fixed-case>GL</fixed-case>i<fixed-case>M</fixed-case>: Integrating Graph Transformer and <fixed-case>LLM</fixed-case> for Document-Level Biomedical Relation Extraction with Incomplete Labeling</title>
      <author><first>Hao</first><last>Fang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yuejie</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Feng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yingwen</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qing</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wen</first><last>He</last></author>
      <author><first>Xiaobo</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Shang</first><last>Gao</last><affiliation>Deakin University</affiliation></author>
      <pages>14131-14146</pages>
      <abstract>Document-level relation extraction (DocRE) identifies relations between entities across an entire document. However, as the number and complexity of entities and entity-pair relations grow, the problem space expands quadratically, causing incomplete annotations and frequent false negatives, especially in biomedical datasets due to high construction costs. This leads to low recall in real-world scenarios. To address this, we propose GLiM, a novel framework that reduces the problem space using a graph-enhanced Transformer-based model and leverages large language models (LLMs) for reasoning. GLiM employs a cascaded approach: first, a graph-enhanced Transformer processes entity-pair relations with finer granularity by dynamically adjusting the graph size based on the number of entities; then, LLM inference handles challenging cases. Experiments show that GLiM boosts average recall and F1 scores by +6.34 and +4.41, respectively, outperforming state-of-the-art models on biomedical benchmarks. These results demonstrate the effectiveness of combining graph-enhanced Transformers with LLM inference for biomedical DocRE. Code will be released at https://github.com/HaoFang10/GLiM.</abstract>
      <url hash="655d3851">2025.findings-acl.727</url>
      <bibkey>fang-etal-2025-glim</bibkey>
    </paper>
    <paper id="728">
      <title><fixed-case>A</fixed-case>nalytic<fixed-case>KWS</fixed-case>: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting</title>
      <author><first>Yang</first><last>Xiao</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Peng</first><last>Tianyi</last><affiliation>Hubei Optics Valley Laboratory</affiliation></author>
      <author><first>Rohan Kumar</first><last>Das</last><affiliation>Fortemedia</affiliation></author>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Huiping</first><last>Zhuang</last><affiliation>South China University of Technology</affiliation></author>
      <pages>14147-14158</pages>
      <abstract>Keyword spotting (KWS) offers a vital mechanism to identify spoken commands in voice-enabled systems, where user demands often shift, requiring models to learn new keywords continually over time. However, a major problem is catastrophic forgetting, where models lose their ability to recognize earlier keywords. Although several continual learning methods have proven their usefulness for reducing forgetting, most existing approaches depend on storing and revisiting old data to combat catastrophic forgetting. Though effective, these methods face two practical challenges: 1) privacy risks from keeping user data and 2) large memory and time consumption that limit deployment on small devices. To address these issues, we propose an exemplar-free Analytic Continual Learning (AnalyticKWS) method that updates model parameters without revisiting earlier data. Inspired by efficient learning principles, AnalyticKWS computes a closed-form analytical solution for model updates and requires only a single epoch of adaptation for incoming keywords. AnalyticKWS demands fewer computational resources by avoiding gradient-based updates and does not store old data. By eliminating the need for back-propagation during incremental learning, the model remains lightweight and efficient. As a result, AnalyticKWS meets the challenges mentioned earlier and suits resource-limited settings well. Extensive experiments on various datasets and settings show that AnalyticKWS consistently outperforms existing continual learning methods.</abstract>
      <url hash="1e940401">2025.findings-acl.728</url>
      <bibkey>xiao-etal-2025-analytickws</bibkey>
    </paper>
    <paper id="729">
      <title>Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions</title>
      <author><first>Taedong</first><last>Yun</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Eric</first><last>Yang</last><affiliation>Verily</affiliation></author>
      <author><first>Mustafa</first><last>Safdari</last><affiliation>Research, Google</affiliation></author>
      <author><first>Jong Ha</first><last>Lee</last><affiliation>Verily Life Sciences</affiliation></author>
      <author><first>Vaishnavi Vinod</first><last>Kumar</last><affiliation>Google</affiliation></author>
      <author><first>S. Sara</first><last>Mahdavi</last><affiliation>Google</affiliation></author>
      <author><first>Jonathan</first><last>Amar</last></author>
      <author><first>Derek</first><last>Peyton</last></author>
      <author><first>Reut</first><last>Aharony</last></author>
      <author><first>Andreas Michaelides</first><last>PhD</last><affiliation>Google</affiliation></author>
      <author><first>Logan Douglas</first><last>Schneider</last><affiliation>Stanford University and Google</affiliation></author>
      <author><first>Isaac</first><last>Galatzer-Levy</last></author>
      <author><first>Yugang</first><last>Jia</last><affiliation>Massachusetts Institute of Technology and Verily Life Science</affiliation></author>
      <author><first>John</first><last>Canny</last><affiliation>University of California - Berkeley and University of California Berkeley</affiliation></author>
      <author><first>Arthur</first><last>Gretton</last></author>
      <author><first>Maja</first><last>Mataric</last><affiliation>University of Southern California</affiliation></author>
      <pages>14159-14181</pages>
      <abstract>We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent’s understanding of the synthetic users’ needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.</abstract>
      <url hash="bc6a6d6e">2025.findings-acl.729</url>
      <bibkey>yun-etal-2025-sleepless</bibkey>
    </paper>
    <paper id="730">
      <title>Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models</title>
      <author><first>Suho</first><last>Yoo</last></author>
      <author><first>Hyunjong</first><last>Ok</last></author>
      <author><first>Jaeho</first><last>Lee</last><affiliation>Google and Pohang University of Science and Technology</affiliation></author>
      <pages>14182-14193</pages>
      <abstract>Language models pretrained on text-only corpora often struggle with tasks that require auditory commonsense knowledge.Previous work addresses this problem by augmenting the language model to retrieve knowledge from external audio databases.This approach has several limitations, such as the potential lack of relevant audio in databases and the high costs associated with constructing the databases. To address these issues, we propose Imagine to Hear, a novel approach that dynamically generates auditory knowledge using generative models. Our framework detects multiple audio-related textual spans from the given prompt and generates corresponding auditory knowledge. We develop several mechanisms to efficiently process multiple auditory knowledge, including a CLAP-based rejection sampler and a language-audio fusion module. Our experiments show that our method achieves state-of-the-art performance on AuditoryBench without relying on external databases, highlighting the effectiveness of our generation-based approach.</abstract>
      <url hash="9a96a63e">2025.findings-acl.730</url>
      <bibkey>yoo-etal-2025-imagine</bibkey>
    </paper>
    <paper id="731">
      <title><fixed-case>S</fixed-case>afe<fixed-case>E</fixed-case>raser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning</title>
      <author><first>Junkai</first><last>Chen</last></author>
      <author><first>Zhijie</first><last>Deng</last></author>
      <author><first>Kening</first><last>Zheng</last></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuliang</first><last>Liu</last></author>
      <author><first>PeiJun</first><last>Wu</last></author>
      <author><first>Peijie</first><last>Jiang</last></author>
      <author><first>Jia</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>14194-14224</pages>
      <abstract>As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. **Machine Unlearning (MU)**, as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, *MU for safety in MLLM has yet to be fully explored*. To address this issue, we propose , a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: **_forget quality_** and **_model utility_**. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from **_over-forgetting_**. Hence, we introduce **Prompt Decouple (PD) Loss** to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called **Safe Answer Refusal Rate (SARR)**. Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. **Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.**</abstract>
      <url hash="b034a1c7">2025.findings-acl.731</url>
      <bibkey>chen-etal-2025-safeeraser</bibkey>
    </paper>
    <paper id="732">
      <title>Prediction-Augmented Generation for Automatic Diagnosis Tasks</title>
      <author><first>Chan-Yang</first><last>Ju</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Dong-Ho</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <pages>14225-14246</pages>
      <abstract>Most Large language models (LLMs) adopt an autoregressive architecture, predicting the next word token based on the preceding context. While this approach is robust for language generation tasks such as writing and summarization, it has limitations for high-level reasoning tasks, such as prediction and decision-making. To overcome these limitations, we introduce a new method called Prediction-Augmented Generation (PAG). PAG can improve the generation quality and predictive accuracy of large language models in inference-driven tasks by integrating task-specific predictive models as external tools, enabling more structured and precise reasoning. Moreover, our method does not simply copy the inferences of a predictive model, but improves the inference results with knowledge from the large language model to create better predictions. We comprehensively evaluate our proposed method on diverse datasets for automatic diagnosis tasks requiring extensive domain knowledge and advanced reasoning.</abstract>
      <url hash="5030a0e6">2025.findings-acl.732</url>
      <bibkey>ju-lee-2025-prediction</bibkey>
    </paper>
    <paper id="733">
      <title><fixed-case>F</fixed-case>ed<fixed-case>LEKE</fixed-case>: Federated Locate-then-Edit Knowledge Editing for Multi-Client Collaboration</title>
      <author><first>Zongkai</first><last>Zhao</last></author>
      <author><first>Guozeng</first><last>Xu</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Xiuhua</first><last>Li</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Kaiwen</first><last>Wei</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Jiang</first><last>Zhong</last><affiliation>Chongqing University</affiliation></author>
      <pages>14247-14258</pages>
      <abstract>Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating large language models (LLMs) without full retraining. However, existing methods assume a single-user setting and become inefficient in real-world multi-client scenarios, where decentralized organizations (e.g., hospitals, financial institutions) independently update overlapping knowledge, leading to redundant mediator knowledge vector (MKV) computations and privacy concerns.To address these challenges, we introduce <tex-math>\textbf{Fed}</tex-math>erated <tex-math>\textbf{L}</tex-math>ocate-then-<tex-math>\textbf{E}</tex-math>dit <tex-math>\textbf{K}</tex-math>nowledge <tex-math>\textbf{E}</tex-math>diting (FedLEKE), a novel task that enables multiple clients to collaboratively perform LEKE while preserving privacy and reducing computational overhead. To achieve this, we propose FedEdit, a two-stage framework that optimizes MKV selection and reuse.In the first stage, clients locally apply LEKE and upload the computed MKVs. In the second stage, rather than relying solely on server-based MKV sharing, FedLEKE allows clients retrieve relevant MKVs based on cosine similarity, enabling knowledge re-edit and minimizing redundant computations.Experimental results on two benchmark datasets demonstrate that FedEdit retains over 96% of the performance of non-federated LEKE while significantly outperforming a FedAvg-based baseline by approximately twofold. Besides, we find that MEMIT performs more consistently than PMET in the FedLEKE task with our FedEdit framework. Our code is available at https://github.com/zongkaiz/FedLEKE.</abstract>
      <url hash="a9c73243">2025.findings-acl.733</url>
      <bibkey>zhao-etal-2025-fedleke</bibkey>
    </paper>
    <paper id="734">
      <title><fixed-case>D</fixed-case>i<fixed-case>SC</fixed-case>o: Device-Server Collaborative <fixed-case>LLM</fixed-case>-based Text Streaming Services</title>
      <author><first>Ting</first><last>Sun</last><affiliation>Lion Rock AI Lab</affiliation></author>
      <author><first>Penghan</first><last>Wang</last><affiliation>Purdue University and Tsinghua University</affiliation></author>
      <author><first>Fan</first><last>Lai</last><affiliation>University of Illinois Urbana-Champaign and Google</affiliation></author>
      <pages>14259-14277</pages>
      <abstract>The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources. We introduce , a device-server cooperative scheduler designed to optimize users’ QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads—including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3—show that can improve users’ QoE by reducing tail TTFT (11-52%) and mean TTFT (6-78%) across different model-device configurations, while dramatically reducing serving costs by up to 84% through its migration mechanism while maintaining comparable QoE levels.</abstract>
      <url hash="436315e5">2025.findings-acl.734</url>
      <bibkey>sun-etal-2025-disco</bibkey>
    </paper>
    <paper id="735">
      <title>Customizing In-context Learning for Dynamic Interest Adaption in <fixed-case>LLM</fixed-case>-based Recommendation</title>
      <author><first>Keqin</first><last>Bao</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jizhi</first><last>Zhang</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xiangnan</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>14278-14291</pages>
      <abstract>Frequently updating Large Language Model (LLM)-based recommender systems to adapt to dynamic user interests—as done for traditional ones—is impractical due to high training costs, even with acceleration methods. This work explores the possibility of adapting the model to dynamic user interests without any model-level updates via In-context Learning (ICL), which enables adaptation through few-shot examples within input prompts. While using recent user interactions as ICL demonstrations offers a potential solution for dynamic interest adaptation, existing LLM-based recommenders face critical limitations: recommendation-specific tuning often diminishes the model’s in-context learning ability, and the original LLM’s ICL lacks task-specific optimization for recommendations. To bridge this gap, we introduce RecICL, a framework that establishes recommendation-oriented in-context learning by structuring recent user interactions and current inputs into ICL formats. RecICL achieves dual objectives: (1) preserving fundamental ICL capabilities during recommendation adaptation and (2) dynamically capturing user preference evolution through the most recent interactions. Extensive experiments across multiple benchmarks demonstrate RecICL’s superior performance, achieving better results without model updates. Our implementation is publicly available at <url>https://anonymous.4open.science/r/RecICL-8003</url>.</abstract>
      <url hash="dcbf1cfc">2025.findings-acl.735</url>
      <bibkey>bao-etal-2025-customizing</bibkey>
    </paper>
    <paper id="736">
      <title>Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</title>
      <author><first>Xinyue</first><last>Cui</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Johnny</first><last>Wei</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Swabha</first><last>Swayamdipta</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <pages>14292-14306</pages>
      <abstract>Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization after pretraining, while overlooking challenges that arise in other stages of the LLM pipeline, such as the risk of watermark filtering during data preprocessing, or potential forgetting through post-training, or verification difficulties due to API-only access. We propose a novel data watermarking approach that injects coherent and plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks’ density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain robust throughout LLM development, maintaining their effectiveness after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.</abstract>
      <url hash="9f0e3a37">2025.findings-acl.736</url>
      <bibkey>cui-etal-2025-robust</bibkey>
    </paper>
    <paper id="737">
      <title><fixed-case>LLM</fixed-case>-Enhanced Query Generation and Retrieval Preservation for Task-Oriented Dialogue</title>
      <author><first>Jiale</first><last>Chen</last></author>
      <author><first>Xuelian</first><last>Dong</last></author>
      <author><first>Wenxiu</first><last>Xie</last><affiliation>Guangdong Polytechnic Normal University</affiliation></author>
      <author><first>Ru</first><last>Peng</last></author>
      <author><first>Kun</first><last>Zeng</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Tianyong</first><last>Hao</last></author>
      <pages>14307-14321</pages>
      <abstract>Knowledge retrieval and response generation are fundamental to task-oriented dialogue systems. However, dialogue context frequently contains noisy or irrelevant information, leading to sub-optimal result in knowledge retrieval. One possible approach to retrieving knowledge is to manually annotate standard queries for each dialogue. Yet, this approach is hindered by the challenge of data scarcity, as human annotation is costly. To solve the challenge, we propose an LLM-enhanced model of query-guided knowledge retrieval for task-oriented dialogue. It generates high-quality queries for knowledge retrieval in task-oriented dialogue solely using low-resource annotated queries. To strengthen the performance correlation between response generation and knowledge retrieval, we propose a retrieval preservation mechanism by further selecting the most relevant knowledge from retrieved top-<tex-math>K</tex-math> records and explicitly incorporating these as prompts to guide a generator in response generation. Experiments on three standard benchmarks demonstrate that our model and mechanism outperform previous state-of-the-art by 3.26% on average with two widely used evaluation metrics.</abstract>
      <url hash="3a367363">2025.findings-acl.737</url>
      <bibkey>chen-etal-2025-llm</bibkey>
    </paper>
    <paper id="738">
      <title><fixed-case>C</fixed-case>loze<fixed-case>M</fixed-case>ath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations</title>
      <author><first>Quang Hieu</first><last>Pham</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Thuy Duong</first><last>Nguyen</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Tung</first><last>Pham</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Dat Quoc</first><last>Nguyen</last><affiliation>Qualcomm AI Research</affiliation></author>
      <pages>14322-14329</pages>
      <abstract>The capabilities of large language models (LLMs) have been enhanced by training on data that reflects human thought processes, such as the Chain-of-Thought format. However, evidence suggests that the conventional scheme of next-word prediction may not fully capture how humans learn to think. Inspired by how humans generalize mathematical reasoning, we propose a new approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our ClozeMath involves a text-infilling task that predicts masked equations from a given solution, analogous to cloze exercises used in human learning. Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the strong baseline Masked Thought in performance and robustness, with two test-time scaling decoding algorithms, Beam Search and Chain-of-Thought decoding. Additionally, we conduct an ablation study to analyze the effects of various architectural and implementation choices on our approach.</abstract>
      <url hash="a92b3ab8">2025.findings-acl.738</url>
      <bibkey>pham-etal-2025-clozemath</bibkey>
    </paper>
    <paper id="739">
      <title>Low-Entropy Watermark Detection via <fixed-case>B</fixed-case>ayes’ Rule Derived Detector</title>
      <author><first>Beining</first><last>Huang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Du</first><last>Su</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>14330-14344</pages>
      <abstract>Text watermarking, which modify tokens to embed watermark, has proven effective in detecting machine-generated texts. Yet its application to low-entropy texts like code and mathematics presents significant challenges. A fair number of tokens in these texts are hardly modifiable without changing the intended meaning, causing statistical measures to falsely indicate the absence of a watermark. Existing research addresses this issue by rely mainly on a limited number of high-entropy tokens, which are considered flexible for modification, and accurately reflecting watermarks. However, their detection accuracy remains suboptimal, as they neglect strong watermark evidences embedded in low entropy tokens modified through watermarking. To overcome this limitation, we introduce Bayes’ Rule derived Watermark Detector (BRWD), which exploit watermark information from every token, by leveraging the posterior probability of watermark’s presence. We theoretically prove the optimality of our method in terms of detection accuracy, and demonstrate its superiority across various datasets, models, and watermark injection strategies. Notably, our method achieves up to 50% and 70% relative improvements in detection accuracy over the best baselines in code generation and math problem-solving tasks, respectively. Our code is available at https://github.com/cczslp/BRWD.</abstract>
      <url hash="da6e7fc3">2025.findings-acl.739</url>
      <bibkey>huang-etal-2025-low</bibkey>
    </paper>
    <paper id="740">
      <title><fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>, Towards an Interpretable Medical Agent using Chain of Diagnosis</title>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Chi</first><last>Gui</last></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ke</first><last>Ji</last></author>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>14345-14368</pages>
      <abstract>The field of AI healthcare has undergone a significant transformation with the advent of large language models (LLMs), yet the challenges of interpretability within these models remain largely unaddressed. This study introduces **Chain-of-Diagnosis (CoD)** to enhance the interpretability of medical automatic diagnosis. CoD transforms the diagnostic process into a diagnostic chain that mirrors a physician’s thought process, providing a transparent reasoning pathway. Additionally, CoD outputs the disease confidence distribution to ensure transparency in decision-making. This interpretability makes model diagnostics controllable and aids in identifying critical symptoms for inquiry through the entropy reduction of confidences. With CoD, we developed **DiagnosisGPT**, capable of diagnosing 9,604 diseases for validating CoD. Experimental results demonstrate that DiagnosisGPT outperforms other LLMs on automatic diagnostic tasks across three real-world benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring controllability in diagnostic rigor.</abstract>
      <url hash="9c279c98">2025.findings-acl.740</url>
      <bibkey>chen-etal-2025-cod</bibkey>
    </paper>
    <paper id="741">
      <title><fixed-case>D</fixed-case>a<fixed-case>N</fixed-case>et: Dual-Aware Enhanced Alignment Network for Multimodal Aspect-Based Sentiment Analysis</title>
      <author><first>Aoqiang</first><last>Zhu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Min</first><last>Hu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xiaohua</first><last>Wang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Jiaoyun</first><last>Yang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Yiming</first><last>Tang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Ning</first><last>An</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>14369-14381</pages>
      <abstract>Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect-sentiment pairs from text and image data. While significant progress has been made in image-aspect alignment, due to the subtlety and complexity of language expressions, there are not always explicit aspect words in the language to align with images. Existing methods typically assume a direct alignment between images and aspects, matching the entire image with a corresponding aspect. This rough alignment of images and aspects introduces noise. To address the above issues, this paper proposes a Dual-Aware Enhanced Alignment Network (DaNet) designed for fine-grained multimodal aspect-image alignment and denoising. Specifically, we first introduce a Multimodal Denoising Encoder (MDE) that jointly image and text to guide the compression and denoising of visual sequences. And then, aspect-aware and sentiment-aware networks are constructed to jointly enhance fine-grained alignment and denoising of text-image information. To better align implicit aspects, an Implicit Aspect Opinion Generation (IAOG) pretraining is designed under the guidance of large language model. Extensive experiments across three MABSA subtasks demonstrate that DaNet outperforms existing methods. Code will be available at https://github.com/***/DaNet.</abstract>
      <url hash="2990e86b">2025.findings-acl.741</url>
      <bibkey>zhu-etal-2025-danet</bibkey>
    </paper>
    <paper id="742">
      <title>Exploring Multimodal Challenges in Toxic <fixed-case>C</fixed-case>hinese Detection: Taxonomy, Benchmark, and Findings</title>
      <author><first>Shujian</first><last>Yang</last></author>
      <author><first>Shiyao</first><last>Cui</last></author>
      <author><first>Chuanrui</first><last>Hu</last><affiliation>Qihoo 360</affiliation></author>
      <author><first>Haicheng</first><last>Wang</last><affiliation>Shanghai Jiaotong University and Télécom Paris</affiliation></author>
      <author><first>Tianwei</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Jialiang</first><last>Lu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Han</first><last>Qiu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>14382-14396</pages>
      <abstract>Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs “overcorrect”: misidentify many normal Chinese contents as toxic.</abstract>
      <url hash="866545d5">2025.findings-acl.742</url>
      <bibkey>yang-etal-2025-exploring-multimodal</bibkey>
    </paper>
    <paper id="743">
      <title><fixed-case>LDIR</fixed-case>: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations</title>
      <author><first>Yile</first><last>Wang</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Zhanyu</first><last>Shen</last></author>
      <author><first>Hui</first><last>Huang</last><affiliation>Shenzhen University</affiliation></author>
      <pages>14397-14409</pages>
      <abstract>Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms ”0/1” embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions.</abstract>
      <url hash="e966a74c">2025.findings-acl.743</url>
      <bibkey>wang-etal-2025-ldir</bibkey>
    </paper>
    <paper id="744">
      <title>Ranked Voting based Self-Consistency of Large Language Models</title>
      <author><first>Weiqin</first><last>Wang</last></author>
      <author><first>Yile</first><last>Wang</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Hui</first><last>Huang</last><affiliation>Shenzhen University</affiliation></author>
      <pages>14410-14426</pages>
      <abstract>Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest ”self-consistency” among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. Code and logs will be released.</abstract>
      <url hash="01cd6dda">2025.findings-acl.744</url>
      <bibkey>wang-etal-2025-ranked</bibkey>
    </paper>
    <paper id="745">
      <title><fixed-case>S</fixed-case>emantic<fixed-case>C</fixed-case>amo: Jailbreaking Large Language Models through Semantic Camouflage</title>
      <author><first>Jihui</first><last>Yan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaocui</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yinzhi</first><last>Zhao</last><affiliation>Northeastern University</affiliation></author>
      <pages>14427-14452</pages>
      <abstract>The rapid development and increasingly widespread applications of Large Language Models (LLMs) have made the safety issues of LLMs more prominent and critical. Although safety training is widely used in LLMs, the mismatch between pre-training and safety training still leads to safety vulnerabilities. To expose the safety vulnerabilities in LLMs and improve LLMs’ performance in safety, we propose a novel framework, SemanticCamo, which attacks LLMs through semantic camouflage.SemanticCamo bypasses safety guardrails by replacing the original unsafe content with semantic features, thereby concealing malicious intent while keeping the query’s objectives unchanged. We conduct comprehensive experiments on the state-of-the-art LLMs, including GPT-4o and Claude-3.5, finding that SemanticCamo successfully induces harmful responses from the target models in over 80% of cases on average, outperforming previous counterparts. Additionally, the performance of SemanticCamo against various defenses is evaluated, demonstrating that semantic transformations introduce critical challenges to LLM safety, necessitating targeted alignment strategies to address this vulnerability. Code and data are available at https://github.com/Jihui-Yan/SemanticCamo.</abstract>
      <url hash="3e3b34b6">2025.findings-acl.745</url>
      <bibkey>yan-etal-2025-semanticcamo</bibkey>
    </paper>
    <paper id="746">
      <title>Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition</title>
      <author><first>Yoonjun</first><last>Cho</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Soeun</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongjae</first><last>Jeon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Kyelim</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Beomsoo</first><last>Lee</last></author>
      <author><first>Albert</first><last>No</last><affiliation>Yonsei University</affiliation></author>
      <pages>14453-14470</pages>
      <abstract>Decomposing weight matrices into quantization and low-rank components (<tex-math>\bf W\approx Q+LR</tex-math>) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component’s unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers’ negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.</abstract>
      <url hash="b56c8941">2025.findings-acl.746</url>
      <bibkey>cho-etal-2025-assigning</bibkey>
    </paper>
    <paper id="747">
      <title>Better Process Supervision with Bi-directional Rewarding Signals</title>
      <author><first>Wenxiang</first><last>Chen</last></author>
      <author><first>Wei</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Honglin</first><last>Guo</last><affiliation>Fudan University</affiliation></author>
      <author><first>Boyang</first><last>Hong</last></author>
      <author><first>Jiazheng</first><last>Zhang</last></author>
      <author><first>Nijun</first><last>Li</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yun</first><last>Li</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>14471-14485</pages>
      <abstract>Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute. Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target. To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target. Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success. We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method. Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.</abstract>
      <url hash="b232b134">2025.findings-acl.747</url>
      <bibkey>chen-etal-2025-better-process</bibkey>
    </paper>
    <paper id="748">
      <title><fixed-case>K</fixed-case>now<fixed-case>C</fixed-case>oder-<fixed-case>X</fixed-case>: Boosting Multilingual Information Extraction via Code</title>
      <author><first>Yuxin</first><last>Zuo</last></author>
      <author><first>Wenxuan</first><last>Jiang</last></author>
      <author><first>Wenxuan</first><last>Liu</last></author>
      <author><first>Zixuan</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Long</first><last>Bai</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hanbin</first><last>Wang</last></author>
      <author><first>Yutao</first><last>Zeng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xiaolong</first><last>Jin</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <pages>14486-14509</pages>
      <abstract>Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model’s cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17% and SoTA by 20.03%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder.</abstract>
      <url hash="e8c2213a">2025.findings-acl.748</url>
      <bibkey>zuo-etal-2025-knowcoder</bibkey>
    </paper>
    <paper id="749">
      <title><fixed-case>MEIT</fixed-case>: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation</title>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Chaofan</first><last>Tao</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Hui</first><last>Shen</last></author>
      <author><first>Jing</first><last>Xiong</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Rossella</first><last>Arcucci</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Mi</first><last>Zhang</last><affiliation>The Ohio State University</affiliation></author>
      <pages>14510-14527</pages>
      <abstract>Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT’s results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, resilience to signal perturbation, and alignment with human expert evaluation. These findings emphasize the efficacy of our MEIT framework and its potential for real-world clinical application.</abstract>
      <url hash="b39da6eb">2025.findings-acl.749</url>
      <bibkey>wan-etal-2025-meit</bibkey>
    </paper>
    <paper id="750">
      <title>Harnessing Large Language Models for Disaster Management: A Survey</title>
      <author><first>Zhenyu</first><last>Lei</last></author>
      <author><first>Yushun</first><last>Dong</last><affiliation>Florida State University</affiliation></author>
      <author><first>Weiyu</first><last>Li</last></author>
      <author><first>Rong</first><last>Ding</last></author>
      <author><first>Qi R.</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jundong</first><last>Li</last><affiliation>University of Virginia</affiliation></author>
      <pages>14528-14551</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including their emerging role in mitigating threats to human life, infrastructure, and the environment during natural disasters. Despite increasing research on disaster-focused LLMs, there remains a lack of systematic reviews and in-depth analyses of their applications in natural disaster management. To address this gap, this paper presents a comprehensive survey of LLMs in disaster response, introducing a taxonomy that categorizes existing works based on disaster phases and application scenarios. By compiling public datasets and identifying key challenges and opportunities, this study aims to provide valuable insights for the research community and practitioners in developing advanced LLM-driven solutions to enhance resilience against natural disasters.</abstract>
      <url hash="82a6db12">2025.findings-acl.750</url>
      <bibkey>lei-etal-2025-harnessing</bibkey>
    </paper>
    <paper id="751">
      <title>Towards Medical Complex Reasoning with <fixed-case>LLM</fixed-case>s through Medical Verifiable Problems</title>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Zhenyang</first><last>Cai</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Ke</first><last>Ji</last></author>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Wanlong</first><last>Liu</last></author>
      <author><first>Rongsheng</first><last>Wang</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>14552-14573</pages>
      <abstract>The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose **Medical Verifiable Problems** with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through **a two-stage approach**: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains. Code, datasets, and models are publicly available at https://github.com/FreedomIntelligence/HuatuoGPT-o1.</abstract>
      <url hash="2181679d">2025.findings-acl.751</url>
      <bibkey>chen-etal-2025-towards-medical</bibkey>
    </paper>
    <paper id="752">
      <title>Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation</title>
      <author><first>Yurui</first><last>Chang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>14574-14587</pages>
      <abstract>While large language models have demonstrated exceptional performance across a wide range of tasks, they remain susceptible to hallucinations – generating plausible yet factually incorrect contents. Existing methods to mitigating such risk often rely on sampling multiple full-length generations, which introduces significant response latency and becomes ineffective when the model consistently produces hallucinated outputs with high confidence. To address these limitations, we introduce Monitoring Decoding (MD), a novel framework that dynamically monitors the generation process and selectively applies in-process interventions, focusing on revising crucial tokens responsible for hallucinations. Instead of waiting until completion of multiple full-length generations, we identify hallucination-prone tokens during generation using a monitor function, and further refine these tokens through a tree-based decoding strategy. This approach ensures an enhanced factual accuracy and coherence in the generated output while maintaining efficiency. Experimental results demonstrate that MD consistently outperforms self-consistency-based approaches in both effectiveness and efficiency, achieving higher factual accuracy while significantly reducing computational overhead.</abstract>
      <url hash="2f908c45">2025.findings-acl.752</url>
      <bibkey>chang-etal-2025-monitoring</bibkey>
    </paper>
    <paper id="753">
      <title><fixed-case>LLM</fixed-case> Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback</title>
      <author><first>Bofei</first><last>Gao</last></author>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Runxin</first><last>Xu</last><affiliation>DeepSeek</affiliation></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Ce</first><last>Zheng</last><affiliation>Peking University</affiliation></author>
      <author><first>Runji</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Wen</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>14588-14604</pages>
      <abstract>In recent progress, mathematical verifiers have achieved success in mathematical reasoning tasks by validating the correctness of solutions generated by policy models. However, existing verifiers are trained with binary classification labels, which are not informative enough for the model to accurately assess the solutions. To mitigate the aforementioned insufficiency of binary labels, we introduce step-wise natural language feedback as rationale labels, that is, the correctness of each step and the detailed explanations. In this paper, we propose Math-Minos, a natural language feedback-enhanced verifier by constructing automatically generated training data and a two-stage training paradigm for effective training and efficient inference. Our experiments reveal that a small set of natural language feedback can significantly boost the performance of the verifier in both verification and reinforcement learning and also significantly alleviates the data-demanding problems of the reward model with an over 700% data efficiency improvement.</abstract>
      <url hash="34f62bab">2025.findings-acl.753</url>
      <bibkey>gao-etal-2025-llm-critics</bibkey>
    </paper>
    <paper id="754">
      <title><fixed-case>E</fixed-case>vo<fixed-case>B</fixed-case>ench: Towards Real-world <fixed-case>LLM</fixed-case>-Generated Text Detection Benchmarking for Evolving Large Language Models</title>
      <author><first>Xiao</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yi</first><last>Yu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kejiang</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>14605-14620</pages>
      <abstract>With the widespread of Large Language Models (LLMs), there has been an increasing need to detect LLM-generated texts, prompting extensive research in this area. However, existing detection methods mainly evaluate on static benchmarks, which neglect the evolving nature of LLMs. Relying on existing static benchmarks could create a misleading sense of security, overestimating the real-world effectiveness of detection methods.To bridge this gap, we introduce EvoBench, a dynamic benchmark considering a new dimension of generalization across continuously evolving LLMs.EvoBench categorizes the evolving LLMs into (1) updates over time and (2) developments like finetuning and pruning, covering 7 LLM families and their 29 evolving versions. To measure the generalization across evolving LLMs, we introduce a new EMG (Evolving Model Generalization) metric. Our evaluation of 14 detection methods on EvoBench reveals that they all struggle to maintain generalization when confronted with evolving LLMs. To mitigate the generalization problems, we further propose improvement strategies. For zero-shot detectors, we propose pruning the scoring model to extract shared features. For supervised detectors, we also propose a practical training strategy.Our research sheds light on critical challenges in real-world LLM-generated text detection and represents a significant step toward practical applications.</abstract>
      <url hash="5f3f4693">2025.findings-acl.754</url>
      <bibkey>yu-etal-2025-evobench</bibkey>
    </paper>
    <paper id="755">
      <title><fixed-case>MMS</fixed-case>ci<fixed-case>B</fixed-case>ench: Benchmarking Language Models on <fixed-case>C</fixed-case>hinese Multimodal Scientific Problems</title>
      <author><first>Xinwu</first><last>Ye</last><affiliation>Fudan University</affiliation></author>
      <author><first>Chengfan</first><last>Li</last></author>
      <author><first>Siming</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Robert</first><last>Tang</last></author>
      <pages>14621-14663</pages>
      <abstract>Recent advances in large language models (LLMs) and vision-language models (LVLMs) have shown promise across many tasks, yet their scientific reasoning capabilities remain untested, particularly in multimodal settings. We present MMSciBench, a benchmark for evaluating mathematical and physical reasoning through text-only and text-image formats, with human-annotated difficulty levels, solutions with detailed explanations, and taxonomic mappings. Evaluation of state-of-the-art models reveals significant limitations, with even the best model achieving only 63.77% accuracy and particularly struggling with visual reasoning tasks. Our analysis exposes critical gaps in complex reasoning and visual-textual integration, establishing MMSciBench as a rigorous standard for measuring progress in multimodal scientific understanding. The code for MMSciBench is open-sourced at GitHub, and the dataset is available at Hugging Face.</abstract>
      <url hash="5aabddce">2025.findings-acl.755</url>
      <bibkey>ye-etal-2025-mmscibench</bibkey>
    </paper>
    <paper id="756">
      <title>Lightweight Query Checkpoint: Classifying Faulty User Queries to Mitigate Hallucinations in Large Language Model Question Answering</title>
      <author><first>Minjoo</first><last>Son</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Jonghak</first><last>Jang</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Misuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>14664-14677</pages>
      <abstract>Question Answering (QA) with large language models has shown impressive performance, yet hallucinations still persist, particularly when user queries carry incorrect premises, insufficient context, or linguistic ambiguity. To address this issue, we propose Lightweight Query Checkpoint (LQC), a small classification model that detects verification-required queries before the LLM generates a potentially faulty answer. LQC leverages hidden states extracted from intermediate layers of a smaller-scale, non-instruct-tuned LLM to effectively distinguish queries requiring verification from clear queries. We first systematically define categories of queries that need verification, construct a dataset comprising both defective and clear queries, and train a binary contrastive learning model. Through extensive experiments on various QA datasets, we demonstrate that incorporating LQC into QA pipelines reduces hallucinations while preserving strong answer quality.</abstract>
      <url hash="f2ae6df3">2025.findings-acl.756</url>
      <bibkey>son-etal-2025-lightweight</bibkey>
    </paper>
    <paper id="757">
      <title>Exploring <fixed-case>LLM</fixed-case> Annotation for Adaptation of Clinical Information Extraction Models under Data-sharing Restrictions</title>
      <author><first>Seiji</first><last>Shimizu</last></author>
      <author><first>Hisada</first><last>Shohei</last></author>
      <author><first>Yutaka</first><last>Uno</last><affiliation>NEC</affiliation></author>
      <author><first>Shuntaro</first><last>Yada</last><affiliation>Tsukuba University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>14678-14694</pages>
      <abstract>In-hospital text data contains valuable clinical information, yet deploying fine-tuned small language models (SLMs) for information extraction remains challenging due to differences in formatting and vocabulary across institutions. Since access to the original in-hospital data (source domain) is often restricted, annotated data from the target hospital (target domain) is crucial for domain adaptation. However, clinical annotation is notoriously expensive and time-consuming, as it demands clinical and linguistic expertise. To address this issue, we leverage large language models (LLMs) to annotate the target domain data for the adaptation. We conduct experiments on four clinical information extraction tasks, including eight target domain data. Experimental results show that LLM-annotated data consistently enhances SLM performance and, with a larger number of annotated data, outperforms manual annotation in three out of four tasks.</abstract>
      <url hash="2363215f">2025.findings-acl.757</url>
      <bibkey>shimizu-etal-2025-exploring</bibkey>
    </paper>
    <paper id="758">
      <title>Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery</title>
      <author><first>Yifan</first><last>Sun</last></author>
      <author><first>Danding</first><last>Wang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Qiang</first><last>Sheng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Juan</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jintao</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>14695-14713</pages>
      <abstract>Concept-based explainable approaches have emerged as a promising method in explainable AI because they can interpret models in a way that aligns with human reasoning. However, their adaption in the text domain remains limited. Most existing methods rely on predefined concept annotations and cannot discover unseen concepts, while other methods that extract concepts without supervision often produce explanations that are not intuitively comprehensible to humans, potentially diminishing user trust. These methods fall short of discovering comprehensible concepts automatically. To address this issue, we propose ECO-Concept, an intrinsically interpretable framework to discover comprehensible concepts with no concept annotations. ECO-Concept first utilizes an object-centric architecture to extract semantic concepts automatically. Then the comprehensibility of the extracted concepts is evaluated by large language models. Finally, the evaluation result guides the subsequent model fine-tuning to obtain more understandable explanations using relatively comprehensible concepts. Experiments show that our method achieves superior performance across diverse tasks. Further concept evaluations validate that the concepts learned by ECO-Concept surpassed current counterparts in comprehensibility.</abstract>
      <url hash="5a30821e">2025.findings-acl.758</url>
      <bibkey>sun-etal-2025-enhancing-comprehensibility</bibkey>
    </paper>
    <paper id="759">
      <title><fixed-case>R</fixed-case>ecord<fixed-case>T</fixed-case>win: Towards Creating Safe Synthetic Clinical Corpora</title>
      <author><first>Seiji</first><last>Shimizu</last></author>
      <author><first>Ibrahim</first><last>Baroud</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Lisa</first><last>Raithel</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Shuntaro</first><last>Yada</last><affiliation>Tsukuba University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>14714-14726</pages>
      <abstract>The scarcity of publicly available clinical corpora hinders developing and applying NLP tools in clinical research. While existing work tackles this issue by utilizing generative models to create high-quality synthetic corpora, their methods require learning from the original in-hospital clinical documents, turning them unfeasible in practice. To address this problem, we introduce RecordTwin, a novel synthetic corpus creation method designed to generate synthetic documents from anonymized clinical entities. In this method, we first extract and anonymize entities from in-hospital documents to ensure the information contained in the synthetic corpus is restricted. Then, we use a large language model to fill the context between anonymized entities. To do so, we use a small, privacy-preserving subset of the original documents to mimic their formatting and writing style. This approach only requires anonymized entities and a small subset of original documents in the generation process, making it more feasible in practice. To evaluate the synthetic corpus created with our method, we conduct a proof-of-concept study using a publicly available clinical database. Our results demonstrate that the synthetic corpus has a utility comparable to the original data and a safety advantage over baselines, highlighting the potential of RecordTwin for privacy-preserving synthetic corpus creation.</abstract>
      <url hash="01686bdc">2025.findings-acl.759</url>
      <bibkey>shimizu-etal-2025-recordtwin</bibkey>
    </paper>
    <paper id="760">
      <title>Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shiyu</first><last>Xiang</last></author>
      <author><first>Ansen</first><last>Zhang</last></author>
      <author><first>Yanfei</first><last>Cao</last></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ronghao</first><last>Chen</last><affiliation>Peking University</affiliation></author>
      <pages>14727-14742</pages>
      <abstract>Although Aligned Large Language Models (LLMs) are trained to reject harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying “attack essences” remain the same. To address this issue, we introduce EDDF, an Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the “attack essence” from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20%, underscoring its superior robustness against jailbreak attacks.</abstract>
      <url hash="281a98ba">2025.findings-acl.760</url>
      <bibkey>xiang-etal-2025-beyond</bibkey>
    </paper>
    <paper id="761">
      <title>Multimodal Invariant Sentiment Representation Learning</title>
      <author><first>Aoqiang</first><last>Zhu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Min</first><last>Hu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xiaohua</first><last>Wang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Jiaoyun</first><last>Yang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Yiming</first><last>Tang</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Ning</first><last>An</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>14743-14755</pages>
      <abstract>Multimodal Sentiment Analysis (MSA) integrates diverse modalities to overcome the limitations of unimodal data. However, existing MSA datasets commonly exhibit significant sentiment distribution imbalances and cross-modal sentiment conflicts, which hinder performance improvement. This paper shows that distributional discrepancies and sentiment conflicts can be incorporated into the model training to learn stable multimodal invariant sentiment representation. To this end, we propose a Multimodal Invariant Sentiment Representation Learning (MISR) method. Specifically, we first learn a stable and consistent multimodal joint representation in the latent space of Gaussian distribution based on distributional constraints Then, under invariance constraint, we further learn multimodal invariant sentiment representations from multiple distributional environments constructed by the joint representation and unimodal data, achieving robust and efficient MSA performance. Extensive experiments demonstrate that MISR significantly enhances MSA performance and achieves new state-of-the-art.</abstract>
      <url hash="b894796b">2025.findings-acl.761</url>
      <bibkey>zhu-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="762">
      <title><fixed-case>C</fixed-case>hu<fixed-case>L</fixed-case>o: Chunk-Level Key Information Representation for Long Document Understanding</title>
      <author><first>Yan</first><last>Li</last></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <author><first>Yue</first><last>Dai</last></author>
      <author><first>Feiqi</first><last>Cao</last></author>
      <pages>14756-14773</pages>
      <abstract>Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model’s ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document understanding that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunks to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analysis.</abstract>
      <url hash="2bccb6bc">2025.findings-acl.762</url>
      <bibkey>li-etal-2025-chulo</bibkey>
    </paper>
    <paper id="763">
      <title><fixed-case>REVS</fixed-case>: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space</title>
      <author><first>Tomer</first><last>Ashuach</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Martin</first><last>Tutek</last><affiliation>University of Zagreb</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>14774-14797</pages>
      <abstract>Language models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens which form sensitive information. To adequately evaluate our method on truly sensitive information, we curate three datasets: an email and URL datasets naturally memorized by the models, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.</abstract>
      <url hash="906fbfb7">2025.findings-acl.763</url>
      <bibkey>ashuach-etal-2025-revs</bibkey>
    </paper>
    <paper id="764">
      <title>Is External Information Useful for Stance Detection with <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Quang Minh</first><last>Nguyen</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Taegyoon</first><last>Kim</last><affiliation>KAIST</affiliation></author>
      <pages>14798-14807</pages>
      <abstract>In the stance detection task, a text is classified as either favorable, opposing, or neutral towards a target. Prior work suggests that the use of external information, e.g., excerpts from Wikipedia, improves stance detection performance. However, whether or not such information can benefit large language models (LLMs) remains an unanswered question, despite their wide adoption in many reasoning tasks. In this study, we conduct a systematic evaluation on how Wikipedia and web search external information can affect stance detection across eight LLMs and in three datasets with 12 targets. Surprisingly, we find that such information degrades performance in most cases, with macro F1 scores dropping by up to 27.9%. We explain this through experiments showing LLMs’ tendency to align their predictions with the stance and sentiment of the provided information rather than the ground truth stance of the given text. We also find that performance degradation persists with chain-of-thought prompting, while fine-tuning mitigates but does not fully eliminate it. Our findings, in contrast to previous literature on BERT-based systems which suggests that external information enhances performance, highlight the risks of information biases in LLM-based stance classifiers.</abstract>
      <url hash="9e0edf7f">2025.findings-acl.764</url>
      <bibkey>nguyen-kim-2025-external</bibkey>
    </paper>
    <paper id="765">
      <title>Benchmarking Query-Conditioned Natural Language Inference</title>
      <author><first>Marc E.</first><last>Canby</last></author>
      <author><first>Xinchi</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Xing</first><last>Niu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jifan</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Sergul</first><last>Aydore</last><affiliation>Amazon</affiliation></author>
      <author><first>Vittorio</first><last>Castelli</last><affiliation>Amazon</affiliation></author>
      <pages>14808-14835</pages>
      <abstract>The growing excitement around the ability of large language models (LLMs) to tackle various tasks has been tempered by their propensity for generating unsubstantiated information (hallucination) and by their inability to effectively handle inconsistent inputs. To detect such issues, we propose the novel task of Query-Conditioned Natural Language Inference (QC-NLI), where the goal is to determine the semantic relationship (e.g. entailment or not entailment) between two documents conditioned on a query; we demonstrate that many common tasks regarding inconsistency detection can be formulated as QC-NLI problems. We focus on three applications in particular: fact verification, intrinsic hallucination detection, and document inconsistency detection. We convert existing datasets for these tasks into the QC-NLI format, and manual annotation confirms their high quality. Finally, we employ zero- and few-shot prompting methods to solve the QC-NLI prediction problem for each task, showing the critical importance of conditioning on the query.</abstract>
      <url hash="3958b01f">2025.findings-acl.765</url>
      <bibkey>canby-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="766">
      <title>Flowchart-Based Decision Making with Large Language Models</title>
      <author><first>Yuuki</first><last>Yamanaka</last><affiliation>NTT</affiliation></author>
      <author><first>Hiroshi</first><last>Takahashi</last><affiliation>NTT</affiliation></author>
      <author><first>Tomoya</first><last>Yamashita</last><affiliation>NTT</affiliation></author>
      <pages>14836-14842</pages>
      <abstract>Large language models (LLMs) are widely used for conversational systems, but they face significant challenges in interpretability of dialogue flow and reproducibility of expert knowledge. To address this, we propose a novel method that extracts flowcharts from dialogue data and incorporates them into LLMs. This approach not only makes the decision-making process more interpretable through visual representation, but also ensures the reproducibility of expert knowledge by explicitly modeling structured reasoning flows. By evaluating on dialogue datasets, we demonstrate that our method effectively reconstructs expert decision-making paths with high precision and recall scores. These findings underscore the potential of flowchart-based decision making to bridge the gap between flexibility and structured reasoning, making chatbot systems more interpretable for developers and end-users.</abstract>
      <url hash="ac4078b2">2025.findings-acl.766</url>
      <bibkey>yamanaka-etal-2025-flowchart</bibkey>
    </paper>
    <paper id="767">
      <title><fixed-case>N</fixed-case>ar<fixed-case>GINA</fixed-case>: Towards Accurate and Interpretable Children’s Narrative Ability Assessment via Narrative Graphs</title>
      <author><first>Jun</first><last>Zhong</last></author>
      <author><first>Longwei</first><last>Xu</last></author>
      <author><first>Li</first><last>Kong</last><affiliation>Nanjing Normal University</affiliation></author>
      <author><first>Xianzhuo</first><last>Li</last></author>
      <author><first>Dandan</first><last>Liang</last><affiliation>Nanjing Normal University</affiliation></author>
      <author><first>Junsheng</first><last>Zhou</last><affiliation>Nanjing Normal University</affiliation></author>
      <pages>14843-14860</pages>
      <abstract>The assessment of children’s narrative ability is crucial for diagnosing language disorders and planning interventions. Distinct from the typical automated essay scoring, this task focuses primarily on evaluating the completeness of narrative content and the coherence of expression, as well as the interpretability of assessment results. To address these issues, we propose a novel computational assessing framework NarGINA, under which the narrative graph is introduced to provide a concise and structured summary representation of narrative text, allowing for explicit narrative measurement. To this end, we construct the first Chinese children’s narrative assessment corpus based on real children’s narrative samples, and we then design a narrative graph construction model and a narrative graph-assisted scoring model to yield accurate narrative ability assessment. Particularly, to enable the scoring model to understand narrative graphs, we propose a multi-view graph contrastive learning strategy to pre-train the graph encoder and apply instruction-tuned large language models to generate scores. The extensive experimental results show that NarGINA can achieve significant performance improvement over the baselines, simultaneously possessing good interpretability. Our findings reveal that the utilization of structured narrative graphs beyond flat text is well suited for narrative ability assessment. The model and data are publicly available at https://github.com/JlexZzz/NarGINA.</abstract>
      <url hash="1a37d629">2025.findings-acl.767</url>
      <bibkey>zhong-etal-2025-nargina</bibkey>
    </paper>
    <paper id="768">
      <title>Improving Efficiency in Large Language Models via Extendable Block Floating Point Representation</title>
      <author><first>Dongyang</first><last>Li</last></author>
      <author><first>Zeyang</first><last>Li</last></author>
      <author><first>Bosheng</first><last>Liu</last></author>
      <author><first>Jigang</first><last>Wu</last><affiliation>Guangdong University of Technology</affiliation></author>
      <pages>14861-14873</pages>
      <abstract>Large language models (LLMs) have revolutionized natural language processing (NLP) tasks, yet their increasing size poses substantial challenges in terms of computational and memory resources. Block floating-point (BFP) arithmetic offers an effective solution by leveraging the strengths of both floating-point and fixed-point representations, leading to reductions in both storage and computational overhead. However, current low-bit BFP quantization approaches often struggle to handle extreme outliers, leading to significant accuracy degradation. To overcome this limitation, we introduce Extendable Exponent Sharing (EES), a novel BFP representation that extends the exponent bit width to capture a wider dynamic range. EES achieves this by embedding extendable exponent bits into the least significant mantissa bits, thereby increasing the shared exponent’s bit width without incurring additional storage costs. To optimize the trade-off between accuracy and energy efficiency, EES employs a design space exploration strategy to optimize the configuration of extendable exponent bit widths. Experimental results show that EES outperforms representative baselines in both accuracy and computational efficiency.</abstract>
      <url hash="2a8cf7db">2025.findings-acl.768</url>
      <bibkey>li-etal-2025-improving-efficiency</bibkey>
    </paper>
    <paper id="769">
      <title><fixed-case>E</fixed-case>pi<fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>e: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding</title>
      <author><first>Mingxu</first><last>Tao</last></author>
      <author><first>Jie</first><last>Hu</last></author>
      <author><first>Mingchuan</first><last>Yang</last></author>
      <author><first>Yunhuai</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>14874-14885</pages>
      <abstract>The remarkable performance of Large language models (LLMs) relies heavily on the availability of abundant high-quality training data. However, the high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe that boosts model performance in data-scarcity scenarios without extra training. We first employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors, by comparing the logit scores given by the extrapolated and the vanilla finetuned model. Experiments across three domains over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement. We also propose a new theoretical framework to reveal the mechanism behind contrastive decoding in data-scarcity scenarios, which further helps better understand the effectiveness of our EpiCoDe.</abstract>
      <url hash="4c364e47">2025.findings-acl.769</url>
      <bibkey>tao-etal-2025-epicode</bibkey>
    </paper>
    <paper id="770">
      <title><fixed-case>N</fixed-case>ativ<fixed-case>QA</fixed-case>: Multilingual Culturally-Aligned Natural Query for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Md. Arid</first><last>Hasan</last></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Fatema</first><last>Ahmad</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Sahinur Rahman</first><last>Laskar</last><affiliation>UPES</affiliation></author>
      <author><first>Sunaya</first><last>Upadhyay</last></author>
      <author><first>Vrunda N</first><last>Sukhadia</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Mucahid</first><last>Kutlu</last><affiliation>University of Qatar and TOBB University of Economics and Technology</affiliation></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>14886-14909</pages>
      <abstract>Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed and some work done in parallel, there is a notable lack of a framework and large-scale region-specific datasets queried by native users in their own languages. This gap hinders effective benchmarking and the development of fine-tuned models for regional and cultural specificities. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, MultiNativQA, consisting of approximately ~64K manually annotated QA pairs in seven languages, ranging from high- to extremely low-resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark both open- and closed-source LLMs using the MultiNativQA dataset. The dataset and related experimental scripts are publicly available for the community at: https://huggingface.co/datasets/QCRI/MultiNativQAand https://gitlab.com/nativqa/multinativqa.</abstract>
      <url hash="1e24bf76">2025.findings-acl.770</url>
      <bibkey>hasan-etal-2025-nativqa</bibkey>
    </paper>
    <paper id="771">
      <title><fixed-case>D</fixed-case>o<fixed-case>CIA</fixed-case>: An Online Document-Level Context Incorporation Agent for Speech Translation</title>
      <author><first>Xinglin</first><last>Lyu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Yuang</first><last>Li</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xiaofeng</first><last>Zhao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ming</first><last>Zhu</last></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yunfei</first><last>Lu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14910-14924</pages>
      <abstract>Document-level context is crucial for handling discourse challenges in text-to-text document-level machine translation (MT). Despite the increased discourse challenges introduced by noise from automatic speech recognition (ASR), the integration of document-level context in speech translation (ST) remains insufficiently explored. In this paper, we develop DoCIA, an online framework that enhances ST performance by incorporating document-level context. DoCIA decomposes the ST pipeline into four stages. Document-level context is integrated into the ASR refinement, MT, and MT refinement stages through auxiliary LLM (large language model)-based modules. Furthermore, DoCIA leverages document-level information in a multi-level manner while minimizing computational overhead. Additionally, a simple yet effective determination mechanism is introduced to prevent hallucinations from excessive refinement, ensuring the reliability of the final results. Experimental results show that DoCIA significantly outperforms traditional ST baselines in both sentence and discourse metrics across four LLMs, demonstrating its effectiveness in improving ST performance.</abstract>
      <url hash="156a032b">2025.findings-acl.771</url>
      <bibkey>lyu-etal-2025-docia</bibkey>
    </paper>
    <paper id="772">
      <title><fixed-case>RISE</fixed-case>: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering</title>
      <author><first>Bolei</first><last>He</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xinran</first><last>He</last><affiliation>Baidu</affiliation></author>
      <author><first>Mengke</first><last>Chen</last></author>
      <author><first>Xianwei</first><last>Xue</last><affiliation>Baidu</affiliation></author>
      <author><first>Ying</first><last>Zhu</last></author>
      <author><first>Zhen-Hua</first><last>Ling</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>14925-14948</pages>
      <abstract>Large Language Models (LLMs) excel in many areas but continue to face challenges with complex reasoning tasks, such as Multi-Hop Question Answering (MHQA). MHQA requires integrating evidence from diverse sources while managing intricate logical dependencies, often leads to errors in reasoning. Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces challenges in effectively filtering noisy data and retrieving all necessary evidence, thereby limiting its effectiveness in addressing MHQA challenges. To address these challenges, we propose RISE:Reasoning Enhancement via Iterative Self-Exploration, a novel framework designed to enhance models’ reasoning capability through iterative self-exploration. Specifically, RISE involves three key steps in addressing MHQA tasks: question decomposition, retrieve-then-read, and self-critique. By leveraging continuous self-exploration, RISE identifies accurate reasoning paths, iteratively self-improving the model’s capability to integrate evidence, maintain logical consistency, and enhance performance in MHQA tasks. Extensive experiments on multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning accuracy and task performance.</abstract>
      <url hash="705b28fa">2025.findings-acl.772</url>
      <bibkey>he-etal-2025-rise</bibkey>
    </paper>
    <paper id="773">
      <title><fixed-case>VADE</fixed-case>: Visual Attention Guided Hallucination Detection and Elimination</title>
      <author><first>Vishnu</first><last>Prabhakaran</last><affiliation>Amazon</affiliation></author>
      <author><first>Purav</first><last>Aggarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Vinay Kumar</first><last>Verma</last><affiliation>Amazon</affiliation></author>
      <author><first>Gokul</first><last>Swamy</last><affiliation>Amazon</affiliation></author>
      <author><first>Anoop</first><last>Saladi</last><affiliation>Amazon</affiliation></author>
      <pages>14949-14965</pages>
      <abstract>Vision Language Models (VLMs) have achieved significant advancements in complex visual understanding tasks. However, VLMs are prone to hallucinations—generating outputs that lack alignment with visual content. This paper addresses hallucination detection in VLMs by leveraging the visual grounding information encoded in transformer attention maps. We identify three primary challenges in this approach: the elective nature of visual grounding for certain tokens, the high-dimensional and noisy nature of attention maps, and the dynamic sequence length of attention on previous tokens. To address these, we propose VADE, a novel sequence modelling approach to effectively learn complex sequential patterns from high-dimensional and noisy attention maps for fine-grained hallucination detection and mitigation. VADE achieves an average PR-AUC of 80% in hallucination detection on M-HalDetect across four different model architectures and an 5% improvement in hallucination mitigation on MSCOCO.</abstract>
      <url hash="8c2d5df6">2025.findings-acl.773</url>
      <bibkey>prabhakaran-etal-2025-vade</bibkey>
    </paper>
    <paper id="774">
      <title><fixed-case>PGPO</fixed-case>: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization</title>
      <author><first>Zouying</first><last>Cao</last></author>
      <author><first>Runze</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Yang</last></author>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author><first>Xiaoyong</first><last>Zhu</last><affiliation>Alibaba Group and Microsoft</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>14966-14985</pages>
      <abstract>Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents’ ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style <tex-math>\underline{P}</tex-math>lanning <tex-math>\underline{G}</tex-math>uided <tex-math>\underline{P}</tex-math>reference <tex-math>\underline{O}</tex-math>ptimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents’ ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.</abstract>
      <url hash="eaed796d">2025.findings-acl.774</url>
      <bibkey>cao-etal-2025-pgpo</bibkey>
    </paper>
    <paper id="775">
      <title>The Effectiveness of Uncased Tokeniziaion for Clinical Notes</title>
      <author><first>Cory</first><last>Paik</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Katharina Von Der</first><last>Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>14986-14992</pages>
      <abstract>The impact of case-sensitive tokenization on clinical notes is not well understood. While clinical notes share similarities with biomedical text in terminology, they often lack the proper casing found in polished publications. Language models, unlike humans, require a fixed vocabulary and case sensitivity is a trade-off that must be considered carefully. Improper casing can lead to sub-optimal tokenization and increased sequence length, degrading downstream performance and increasing computational costs. While most recent open-domain encoder language models use uncased tokenization for all tasks, there is no clear trend in biomedical and clinical models. In this work we (1) show that uncased models exceed the performance of cased models on clinical notes, even on traditionally case-sensitive tasks such as named entity recognition and (2) introduce independent case encoding to better balance model performance on case-sensitive and improperly-cased tasks.</abstract>
      <url hash="ad987d4b">2025.findings-acl.775</url>
      <bibkey>paik-wense-2025-effectiveness</bibkey>
    </paper>
    <paper id="776">
      <title><fixed-case>AMXFP</fixed-case>4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>Janghwan</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Jiwoong</first><last>Park</last></author>
      <author><first>Jinseok</first><last>Kim</last><affiliation>Rebellions, Inc.</affiliation></author>
      <author><first>Yongjik</first><last>Kim</last><affiliation>Rebellions</affiliation></author>
      <author><first>Jungju</first><last>Oh</last><affiliation>Rebellions, Inc.</affiliation></author>
      <author><first>Jinwook</first><last>Oh</last><affiliation>Rebellions Inc.</affiliation></author>
      <author><first>Jungwook</first><last>Choi</last><affiliation>Hanyang University</affiliation></author>
      <pages>14993-15013</pages>
      <abstract>As large language models (LLMs) grow in parameter size and context length, computation precision has been reduced from 16-bit to 4-bit to improve inference efficiency. However, this reduction causes accuracy degradation due to activation outliers. Rotation-based INT4 methods address this via matrix calibration, but they introduce multi-hour overheads and leave key computations in full precision. Microscaling (MX) floating-point (FP) formats offer fine-grained representation with a shared scale, enabling fully quantized matrix multiplications through direct casting without calibration. However, existing research shows unsatisfactory empirical results for MXFP4 inference, and the robustness of MX formats remains largely unexplored. In this work, we uncover the fundamental tradeoffs of the MX format: while it effectively suppresses activation outliers, it does so at the cost of increased group-wise asymmetry. To address this, we propose AMXFP4, a 4-bit asymmetric FP format that handles both issues using asymmetric shared scales, without requiring calibration. Our custom MAC engine adds negligible hardware cost while improving accuracy: AMXFP4 outperforms MXFP4 by 3% on VQA and exceeds rotation-based methods by 1.6% on CSQA. It also surpasses recently deployed commercial MXFP4 variants. Code: https://github.com/aiha-lab/MX-QLLM</abstract>
      <url hash="3efbe7a1">2025.findings-acl.776</url>
      <bibkey>lee-etal-2025-amxfp4</bibkey>
    </paper>
    <paper id="777">
      <title>Improving Continual Pre-training Through Seamless Data Packing</title>
      <author><first>Ruicheng</first><last>Yin</last></author>
      <author><first>Xuan</first><last>Gao</last></author>
      <author><first>Changze</first><last>Lv</last></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>15014-15032</pages>
      <abstract>Continual pre-training has demonstrated significant potential in enhancing model performance, particularly in domain-specific scenarios. The most common approach for packing data before continual pre-training involves concatenating input texts and splitting them into fixed-length sequences. While straightforward and efficient, this method often leads to excessive truncation and context discontinuity, which can hinder model performance. To address these issues, we explore the potential of data engineering to enhance continual pre-training, particularly its impact on model performance and efficiency. We propose Seamless Packing (SP), a novel data packing strategy aimed at preserving contextual information and enhancing model performance. Our approach employs a sliding window technique in the first stage that synchronizes overlapping tokens across consecutive sequences, ensuring better continuity and contextual coherence. In the second stage, we adopt a First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger than the target sequence length, thereby minimizing padding and truncation. Empirical evaluations across various model architectures and corpus domains demonstrate the effectiveness of our method, outperforming baselines in 99% of all settings. Code is available at https://github.com/Infernus-WIND/Seamless-Packing.</abstract>
      <url hash="fd19899b">2025.findings-acl.777</url>
      <bibkey>yin-etal-2025-improving</bibkey>
    </paper>
    <paper id="778">
      <title>The Impact of Name Age Perception on Job Recommendations in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Mahammed</first><last>Kamruzzaman</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Gene Louis</first><last>Kim</last><affiliation>University of South Florida</affiliation></author>
      <pages>15033-15058</pages>
      <abstract>Names often carry generational connotations, with certain names stereotypically associated with younger or older age groups. This study examines implicit age-related name bias in LLMs used for job recommendations. Analyzing six LLMs and 117 American names categorized by perceived age across 30 occupations, we find systematic bias: older-sounding names are favored for senior roles, while younger-sounding names are linked to youth-dominant jobs, reinforcing generational stereotypes. We also find that this bias is based on perceived rather than real ages associated with the names.</abstract>
      <url hash="e9ab6e0e">2025.findings-acl.778</url>
      <bibkey>kamruzzaman-kim-2025-impact</bibkey>
    </paper>
    <paper id="779">
      <title><fixed-case>DAPI</fixed-case>: Domain Adaptive Toxicity Probe Vector Intervention, for Fine-Grained Detoxification</title>
      <author><first>Cho</first><last>Hyeonsu</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Dooyoung</first><last>Kim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Youngjoong</first><last>Ko</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>15059-15069</pages>
      <abstract>There have been attempts to utilize linear probe for detoxification, with existing studies relying on a single toxicity probe vector to reduce toxicity. However, toxicity can be fine-grained into various subcategories, making it difficult to remove certain types of toxicity by using a single toxicity probe vector. To address this limitation, we propose a category-specific toxicity probe vector approach. First, we train multiple toxicity probe vectors for different toxicity categories. During generation, we dynamically select the most relevant toxicity probe vector based on the current context. Finally, the selected vector is dynamically scaled and subtracted from model. Our method successfully mitigated toxicity from categories that the single probe vector approach failed to detoxify. Experiments demonstrate that our approach achieves up to a 78.52% reduction in toxicity on the evaluation dataset, while fluency remains nearly unchanged, with only a 0.052% drop compared to the unsteered model.</abstract>
      <url hash="5caa1c9b">2025.findings-acl.779</url>
      <bibkey>hyeonsu-etal-2025-dapi</bibkey>
    </paper>
    <paper id="780">
      <title>Task Knowledge Injection via Interpolations and Reinstatement for Large Language Model Generalization</title>
      <author><first>Yukun</first><last>Zhao</last></author>
      <author><first>Lingyong</first><last>Yan</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Zhenyang</first><last>Li</last><affiliation>Baidu</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>15070-15080</pages>
      <abstract>Large language models have shown tremendous potential across various NLP tasks, and instruction tuning has been widely adopted to elicit their superior performance. However, instruction tuning may overly tailor the models to task-specific formats, potentially compromising their generalization on unseen tasks. We attribute the issue to the spurious correlations learned between inputs and targets. We propose explicit task knowledge injection to mitigate these shortcuts with latent task adaptation and knowledge reinstatement. Latent tasks serve as interpolations between new tasks and facilitate knowledge sharing with joint adaptation enabling the model to build task knowledge more smoothly. Knowledge reinstatement helps optimize building new knowledge with prior knowledge. Specifically, we retrieve input-relevant latent tasks and jointly learn the task and the relevant latent tasks. Moreover, we prompt the model to recall the forms of inputs corresponding to the target and build the task knowledge through the reinstatement of prior knowledge while learning the new task.We conduct extensive experiments on state-of-the-art large language models including Llama3.1-8B and Vicuna-13B across 1000+ instruction-following tasks to demonstrate the effectiveness of our method. The results demonstrate our method improves generalization on both in-domain and out-of-domain unseen tasks.</abstract>
      <url hash="d7fab1d1">2025.findings-acl.780</url>
      <bibkey>zhao-etal-2025-task</bibkey>
    </paper>
    <paper id="781">
      <title><fixed-case>STARS</fixed-case>: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation</title>
      <author><first>Wenxiang</first><last>Guo</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Changhao</first><last>Pan</last></author>
      <author><first>Zhiyuan</first><last>Zhu</last></author>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>ZheTao</first><last>Chen</last></author>
      <author><first>Wenhao</first><last>Xu</last></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>15081-15093</pages>
      <abstract>Recent breakthroughs in singing voice synthesis (SVS) have heightened the demand for high-quality annotated datasets, yet manual annotation remains prohibitively labor-intensive and resource-intensive. Existing automatic singing annotation (ASA) methods, however, primarily tackle isolated aspects of the annotation pipeline. To address this fundamental challenge, we present STARS, which is, to our knowledge, the first unified framework that simultaneously addresses singing transcription, alignment, and refined style annotation. Our framework delivers comprehensive multi-level annotations encompassing: (1) precise phoneme-audio alignment, (2) robust note transcription and temporal localization, (3) expressive vocal technique identification, and (4) global stylistic characterization including emotion and pace. The proposed architecture employs hierarchical acoustic feature processing across frame, word, phoneme, note, and sentence levels. The novel non-autoregressive local acoustic encoders enable structured hierarchical representation learning. Experimental validation confirms the framework’s superior performance across multiple evaluation dimensions compared to existing annotation approaches. Furthermore, applications in SVS training demonstrate that models utilizing STARS-annotated data achieve significantly enhanced perceptual naturalness and precise style control. This work not only overcomes critical scalability challenges in the creation of singing datasets but also pioneers new methodologies for controllable singing voice synthesis.</abstract>
      <url hash="75616e16">2025.findings-acl.781</url>
      <bibkey>guo-etal-2025-stars</bibkey>
    </paper>
    <paper id="782">
      <title>Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning</title>
      <author><first>Xinghao</first><last>Chen</last></author>
      <author><first>Zhijing</first><last>Sun</last></author>
      <author><first>Guo</first><last>Wenjin</last></author>
      <author><first>Miaoran</first><last>Zhang</last><affiliation>Saarland University</affiliation></author>
      <author><first>Yanjun</first><last>Chen</last></author>
      <author><first>Yirong</first><last>Sun</last></author>
      <author><first>Hui</first><last>Su</last><affiliation>Meituan</affiliation></author>
      <author><first>Yijie</first><last>Pan</last><affiliation>Eastern Institute of Technology, Ningbo</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <pages>15094-15119</pages>
      <abstract>Large Language Models (LLMs) excel in reasoning tasks through Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases computational demands, which has prompted growing interest in distilling CoT capabilities into Small Language Models (SLMs). This study systematically examines the factors influencing CoT distillation, including the choice of granularity, format and teacher model. Through experiments involving four teacher models and seven student models across seven mathematical and commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs, SLMs exhibit a *non-monotonic* relationship with granularity, with stronger models benefiting from finer-grained reasoning and weaker models performing better with simpler CoT supervision; (2) CoT format significantly impacts LLMs but has *minimal* effect on SLMs, likely due to their reliance on supervised fine-tuning rather than pretraining preferences; (3) Stronger teacher models do *NOT* always produce better student models, as diversity and complexity in CoT supervision can outweigh accuracy alone. These findings emphasize the need to tailor CoT strategies to specific student model, offering actionable insights for optimizing CoT distillation in SLMs.</abstract>
      <url hash="e7745565">2025.findings-acl.782</url>
      <bibkey>chen-etal-2025-unveiling-key</bibkey>
    </paper>
    <paper id="783">
      <title><fixed-case>INT</fixed-case>: Establishing Information Transfer for Multilingual Intent Detection and Slot Filling</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Liting</first><last>Jiang</last><affiliation>Xinjiang University</affiliation></author>
      <author><first>Bohui</first><last>Mao</last></author>
      <author><first>Hongyan</first><last>Xie</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Haoxiang</first><last>Su</last></author>
      <author><first>Zhongjiang</first><last>He</last><affiliation>ChinaTelecom</affiliation></author>
      <author><first>Ruiyu</first><last>Fang</last></author>
      <author><first>Shuangyong</first><last>Song</last></author>
      <author><first>Hao</first><last>Huang</last></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>China Telecom and Northwestern Polytechnical University</affiliation></author>
      <pages>15120-15142</pages>
      <abstract>Multilingual spoken language understanding (SLU) involves intent detection (ID) and slot filling (SF) across multiple languages. The inherent linguistic diversity presents significant challenges in achieving performance comparable to traditional SLU. Recent studies have attempted to improve multilingual SLU performance by sharing multilingual encoders. However, these approaches have not directly established information flow between languages. To address this, we first demonstrate the feasibility of such information transfer and pinpoint the key challenges: prediction error mitigation and multilingual slot alignment. We then propose the INformation Transfer network (INT) to tackle these challenges. The gate unit in INT controls the information flow between languages, reducing the adverse impact of prediction errors on both ID and SF. Additionally, we reformulate SF as a span prediction problem and introduce a slot-matching attention mechanism to achieve slot alignment across languages. Experimental results on the MASSIVE and MASSIVE-UG datasets show that our model outperforms all baselines in overall accuracy across all languages, and demonstrates robust performance when different languages are used as the source.</abstract>
      <url hash="30144d5f">2025.findings-acl.783</url>
      <bibkey>wu-etal-2025-int</bibkey>
    </paper>
    <paper id="784">
      <title>Enhancing <fixed-case>LLM</fixed-case> Agent Safety via Causal Influence Prompting</title>
      <author><first>Dongyoon</first><last>Hahm</last></author>
      <author><first>Woogyeol</first><last>Jin</last></author>
      <author><first>June Suk</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sungsoo</first><last>Ahn</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Kimin</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>15143-15168</pages>
      <abstract>As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.</abstract>
      <url hash="a196ec89">2025.findings-acl.784</url>
      <bibkey>hahm-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="785">
      <title>Position Paper: <fixed-case>M</fixed-case>e<fixed-case>M</fixed-case>o: Towards Language Models with Associative Memory Mechanisms</title>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last><affiliation>Università degli Studi di Roma Tor Vergata</affiliation></author>
      <author><first>Giancarlo A.</first><last>Xompero</last><affiliation>University of Rome Tor Vergata and Almawave SpA</affiliation></author>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Davide</first><last>Venditti</last></author>
      <author><first>Federico</first><last>Ranaldi</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Cristina</first><last>Giannone</last></author>
      <author><first>Andrea</first><last>Favalli</last><affiliation>Almawave</affiliation></author>
      <author><first>Raniero</first><last>Romagnoli</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <pages>15169-15180</pages>
      <abstract>Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this position/theory paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that memorization precedes learning. We introduce MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented with the MeMo architecture, showing the memorization power of the one-layer and the multi-layer configurations.</abstract>
      <url hash="79971f6c">2025.findings-acl.785</url>
      <bibkey>zanzotto-etal-2025-position</bibkey>
    </paper>
    <paper id="786">
      <title><fixed-case>D</fixed-case>e<fixed-case>RAGEC</fixed-case>: Denoising Named Entity Candidates with Synthetic Rationale for <fixed-case>ASR</fixed-case> Error Correction</title>
      <author><first>Solee</first><last>Im</last></author>
      <author><first>Wonjun</first><last>Lee</last></author>
      <author><first>JinMyeong</first><last>An</last></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Jungseul</first><last>Ok</last><affiliation>POSTECH</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>15181-15193</pages>
      <abstract>We present DeRAGEC, a method for improving Named Entity (NE) correction in Automatic Speech Recognition (ASR) systems. By extending the Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC employs synthetic denoising rationales to filter out noisy NE candidates before correction. By leveraging phonetic similarity and augmented definitions, it refines noisy retrieved NEs using in-context learning, requiring no additional training. Experimental results on CommonVoice and STOP datasets show significant improvements in Word Error Rate (WER) and NE hit ratio, outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28% relative reduction in WER compared to ASR without postprocessing.</abstract>
      <url hash="18c319b1">2025.findings-acl.786</url>
      <bibkey>im-etal-2025-deragec</bibkey>
    </paper>
    <paper id="787">
      <title>Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models</title>
      <author><first>Yanyue</first><last>Zhang</last><affiliation>Southeast University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <pages>15194-15211</pages>
      <abstract>Personalized opinion summarization is crucial as it considers individual user interests while generating product summaries.Recent studies show that although large language models demonstrate powerful text summarization and evaluation capabilities without the need for training data, they face difficulties in personalized tasks involving long texts. To address this, <b>Rehearsal</b>, a personalized opinion summarization framework via LLM-based role-playing is proposed. Having the model act as the user, the model can better understand the user’s personalized needs.Additionally, a role-playing supervisor and practice process are introduced to improve the role-playing ability of the LLMs, leading to a better expression of user needs.Furthermore, the summary generation process is guided by suggestions from virtual users, ensuring that the generated summary includes the user’s interest, thus achieving personalized summary generation. Experiment results demonstrate that our method can effectively improve the level of personalization in large model-generated summaries.</abstract>
      <url hash="d03a428d">2025.findings-acl.787</url>
      <bibkey>zhang-etal-2025-rehearse</bibkey>
    </paper>
    <paper id="788">
      <title><fixed-case>A</fixed-case>d<fixed-case>P</fixed-case>araphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset</title>
      <author><first>Soichiro</first><last>Murakami</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>15212-15230</pages>
      <abstract>Identifying factors that make ad text attractive is essential for advertising success. This study proposes AdParaphrase v2.0, a dataset for ad text paraphrasing, containing human preference data, to enable the analysis of the linguistic factors and to support the development of methods for generating attractive ad texts. Compared with v1.0, this dataset is 20 times larger, comprising 16,460 ad text paraphrase pairs, each annotated with preference data from ten evaluators, thereby enabling a more comprehensive and reliable analysis. Through the experiments, we identified multiple linguistic features of engaging ad texts that were not observed in v1.0 and explored various methods for generating attractive ad texts. Furthermore, our analysis demonstrated the relationships between human preference and ad performance, and highlighted the potential of reference-free metrics based on large language models for evaluating ad text attractiveness.The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.</abstract>
      <url hash="20f743db">2025.findings-acl.788</url>
      <bibkey>murakami-etal-2025-adparaphrase-v2</bibkey>
    </paper>
    <paper id="789">
      <title>Beyond the Average Reader: the Reader Embedding Approach</title>
      <author><first>Calogero Jerik</first><last>Scozzaro</last></author>
      <author><first>Matteo</first><last>Delsanto</last><affiliation>University of Turin</affiliation></author>
      <author><first>Daniele P.</first><last>Radicioni</last><affiliation>University of Turin</affiliation></author>
      <pages>15231-15244</pages>
      <abstract>Focus of this work is the prediction of reading times as the task is customarily dealt with in literature: that is, by collecting eye-tracking data that are averaged and employed to train learning models. We start by observing that systems trained on average values are ill-suited for the prediction of the reading times for specific subjects, as they fail to account for individual variability and accurately analyze the reading gestures of specific reader groups, or to target specific user needs. To overcome such limitation, that is to predict the reading times for a specific subject, we propose a novel approach based on creating an embedding to compactly describe her/his fixations. Embeddings are used to individuate readers that share same or similar reading behavior from a reference corpus. Models are then trained on values averaged over this subset of similar readers. Experimental results indicate that the proposed approach consistently outperforms its corresponding variants, in which predictions of reading times for specific readers are based on data from all subjects rather than from the most similar ones.</abstract>
      <url hash="d88d9aa2">2025.findings-acl.789</url>
      <bibkey>scozzaro-etal-2025-beyond</bibkey>
    </paper>
    <paper id="790">
      <title><fixed-case>P</fixed-case>redicta<fixed-case>B</fixed-case>oard: Benchmarking <fixed-case>LLM</fixed-case> Score Predictability</title>
      <author><first>Lorenzo</first><last>Pacchiardi</last></author>
      <author><first>Konstantinos</first><last>Voudouris</last></author>
      <author><first>Ben</first><last>Slater</last></author>
      <author><first>Fernando</first><last>Martínez-Plumed</last><affiliation>Universidad Politécnica de Valencia</affiliation></author>
      <author><first>Jose</first><last>Hernandez-Orallo</last><affiliation>Universitat Politecnica de Valencia and University of Cambridge</affiliation></author>
      <author><first>Lexin</first><last>Zhou</last></author>
      <author><first>Wout</first><last>Schellaert</last></author>
      <pages>15245-15266</pages>
      <abstract>Despite possessing impressive skills, Large Language Models (LLMs) often fail unpre-dictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable “safe zone” is essential for mitigating risks. To address this, we present PredictaBoard, a novel collabo-rative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our bench-mark can be found at https://github. com/Kinds-of-Intelligence-CFI/PredictaBoard</abstract>
      <url hash="b18d6563">2025.findings-acl.790</url>
      <bibkey>pacchiardi-etal-2025-predictaboard</bibkey>
    </paper>
    <paper id="791">
      <title><fixed-case>F</fixed-case>ed<fixed-case>DQC</fixed-case>: Data Quality Control in Federated Instruction-tuning of Large Language Models</title>
      <author><first>Yaxin</first><last>Du</last></author>
      <author><first>Rui</first><last>Ye</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Fengting</first><last>Yuchi</last><affiliation>Johns Hopkins University and Shanghai Jiao Tong University</affiliation></author>
      <author><first>Wanru</first><last>Zhao</last></author>
      <author><first>Jingjing</first><last>Qu</last></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Siheng</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>15267-15291</pages>
      <abstract>Federated Learning (FL) enables privacy-preserving collaborative instruction tuning of large language models (LLMs) by leveraging massively distributed data. However, the decentralized nature of FL exacerbates data quality challenges, as local clients lack global visibility to filter noisy or low-quality samples before training. To resolve this issue, we propose FedDQC, a novel federated instruction tuning framework with dynamic data quality control. Our approach introduces two key innovations. First, we propose instruction-response alignment (IRA)—an efficient client-side metric for quality evaluation requiring only low-cost inference. We validate that higher-IRA data corresponds to more relevant and easier-to-learn question-answer pairs. Second, mirroring the human easy-to-hard knowledge acquisition process, we design a quality-aware hierarchical FL training framework, where the LLM is progressively fine-tuned from high- to low-IRA data in a collaborative manner. The framework also supports adaptive data quality assessment at each hierarchy, enabling dynamic adjustments throughout the training process. Extensive experiments on synthetic and real-world datasets show that our method significantly improves LLM performance on mixed-quality data in FL.</abstract>
      <url hash="62e470c2">2025.findings-acl.791</url>
      <bibkey>du-etal-2025-feddqc</bibkey>
    </paper>
    <paper id="792">
      <title>Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning</title>
      <author><first>Bo</first><last>Yuan</last></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Yin</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>15292-15311</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have shown impressive performance in various downstream tasks. However, in many real-world scenarios, the collected training data inevitably contains noisy labels. To learn from noisy labels, most solutions select samples with small losses for model training. However, the selected samples, in turn, impact the loss computation in the next iteration. An inaccurate initial selection can create a vicious cycle, leading to suboptimal performance. To break this cycle, we propose Delora, a novel framework that decouples the sample selection from model training. For sample selection, Delora establishes a noisy label detector by introducing clean and noisy LoRA. Benefiting from the memory effect, the clean LoRA is encouraged to memorize clean data, while the noisy LoRA is constrained to memorize mislabeled data, which serves as a learnable threshold for selecting clean and noisy samples. For model training, Delora can use carefully selected samples to fine-tune language models seamlessly. Experimental results on synthetic and real-world noisy datasets demonstrate the effectiveness of Delora in noisy label detection and text classification.</abstract>
      <url hash="9755992b">2025.findings-acl.792</url>
      <bibkey>yuan-etal-2025-weed</bibkey>
    </paper>
    <paper id="793">
      <title>“<fixed-case>I</fixed-case> understand your perspective”: <fixed-case>LLM</fixed-case> Persuasion through the Lens of Communicative Action Theory</title>
      <author><first>Esra</first><last>Dönmez</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Agnieszka</first><last>Falenska</last><affiliation>Interchange Forum for Reflecting on Intelligent Systems, University of Stuttgart</affiliation></author>
      <pages>15312-15327</pages>
      <abstract>Large Language Models (LLMs) can generate high-quality arguments, yet their ability to engage in *nuanced and persuasive communicative actions* remains largely unexplored. This work explores the persuasive potential of LLMs through the framework of Jürgen Habermas’ Theory of Communicative Action. It examines whether LLMs express illocutionary intent (i.e., pragmatic functions of language such as conveying knowledge, building trust, or signaling similarity) in ways that are comparable to human communication.We simulate online discussions between opinion holders and LLMs using conversations from the persuasive subreddit *ChangeMyView*. We then compare the likelihood of illocutionary intents in human-written and LLM-generated counter-arguments, specifically those that successfully changed the original poster’s view. We find that all three LLMs effectively convey illocutionary intent — often more so than humans — potentially increasing their anthropomorphism. Further, LLMs craft responses that closely align with the opinion holder’s intent, a strategy strongly associated with opinion change. Finally, crowd-sourced workers find LLM-generated counter-arguments more *agreeable* and consistently prefer them over human-written ones. These findings suggest that LLMs’ persuasive power extends beyond merely generating high-quality arguments. On the contrary, training LLMs with human preferences effectively tunes them to mirror human communication patterns, particularly nuanced communicative actions, potentially increasing individuals’ susceptibility to their influence.</abstract>
      <url hash="c9734b5f">2025.findings-acl.793</url>
      <bibkey>donmez-falenska-2025-understand</bibkey>
    </paper>
    <paper id="794">
      <title>Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on <fixed-case>K</fixed-case>orean Superstition</title>
      <author><first>Kyuhee</first><last>Kim</last></author>
      <author><first>Sangah</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>15328-15342</pages>
      <abstract>As large language models (LLMs) become key advisors in various domains, their cultural sensitivity and reasoning skills are crucial in multicultural environments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs’ cultural understanding, with a focus on Korean superstitions. The benchmark consists of 247 questions spanning 31 topics, assessing factual knowledge, culturally appropriate advice, and situational interpretation. We evaluate multilingual LLMs in both Korean and English to analyze their ability to reason about Korean cultural contexts and how language variations affect performance. To systematically assess cultural reasoning, we propose a novel verification strategy with customized scoring metrics that capture the extent to which models recognize cultural nuances and respond appropriately. Our findings highlight significant challenges in LLMs’ cultural reasoning. While models generally recognize factual information, they struggle to apply it in practical scenarios. Furthermore, explicit cultural framing enhances performance more effectively than relying solely on the language of the prompt. To support further research, we publicly release Nunchi-Bench alongside a leaderboard.</abstract>
      <url hash="3f856f1c">2025.findings-acl.794</url>
      <bibkey>kim-lee-2025-nunchi</bibkey>
    </paper>
    <paper id="795">
      <title>Let’s Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models</title>
      <author><first>Kangyang</first><last>Luo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zichen</first><last>Ding</last></author>
      <author><first>Zhenmin</first><last>Weng</last></author>
      <author><first>Lingfeng</first><last>Qiao</last></author>
      <author><first>Meng</first><last>Zhao</last><affiliation>Tencent Youtu Lab</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Di</first><last>Yin</last></author>
      <author><first>Jinlong</first><last>Shu</last><affiliation>Shanghai Normal University</affiliation></author>
      <pages>15343-15420</pages>
      <abstract>While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named <b>LBS3</b>, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines.</abstract>
      <url hash="9d6d4d58">2025.findings-acl.795</url>
      <bibkey>luo-etal-2025-lets</bibkey>
    </paper>
    <paper id="796">
      <title>da<fixed-case>DPO</fixed-case>: Distribution-Aware <fixed-case>DPO</fixed-case> for Distilling Conversational Abilities</title>
      <author><first>Zhengze</first><last>Zhang</last></author>
      <author><first>Shiqi</first><last>Wang</last></author>
      <author><first>Yiqun</first><last>Shen</last></author>
      <author><first>Simin</first><last>Guo</last><affiliation>Sensetime</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xiaoliang</first><last>Wang</last><affiliation>Nanjing university</affiliation></author>
      <author><first>Nguyen</first><last>Cam-Tu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fei</first><last>Tan</last><affiliation>Sensetime Research</affiliation></author>
      <pages>15421-15437</pages>
      <abstract>Large language models (LLMs) have demonstrated exceptional performance across various applications, but their conversational abilities decline sharply as model size decreases, presenting a barrier to their deployment in resource-constrained environments. Knowledge distillation (KD) with Direct Preference Optimization (DPO) has emerged as a promising approach to enhance the conversational abilities of smaller models using a larger teacher model. However, current methods primarily focus on “black-box” KD, which only uses the teacher’s responses, overlooking the rich distributional information within the teacher’s probability distribution. This paper addresses this gap by introducing daDPO (Distillation-Aware DPO), a novel framework that integrates the teacher’s distributional information into DPO distillation while preserving theoretical guarantees. Our framework offers a unified objective that enhances both preference optimization and distribution-based distillation. We provide rigorous theoretical analysis and empirical validation, showing that daDPO outperforms existing methods in restoring performance for pruned models and enhancing smaller models within the same LLM family. Notably, in in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve near-teacher performance (-7.3% preference rate), and allows Qwen2.5-1.5B to occasionally outperform its 7b teacher model (14.0% win rate).</abstract>
      <url hash="436be699">2025.findings-acl.796</url>
      <bibkey>zhang-etal-2025-dadpo</bibkey>
    </paper>
    <paper id="797">
      <title>Consultant Decoding: Yet Another Synergistic Mechanism</title>
      <author><first>Chuanghao</first><last>Ding</last></author>
      <author><first>Jiaping</first><last>Wang</last></author>
      <author><first>Ziqing</first><last>Yang</last><affiliation>Sensetime</affiliation></author>
      <author><first>Xiaoliang</first><last>Wang</last><affiliation>Nanjing university</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Nguyen</first><last>Cam-Tu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fei</first><last>Tan</last><affiliation>Sensetime Research</affiliation></author>
      <pages>15438-15452</pages>
      <abstract>The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD.In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (~100% of the target model’s performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude.In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks.CD’s performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.</abstract>
      <url hash="5c94d2b3">2025.findings-acl.797</url>
      <bibkey>ding-etal-2025-consultant</bibkey>
    </paper>
    <paper id="798">
      <title><fixed-case>I</fixed-case>ntelli<fixed-case>C</fixed-case>ockpit<fixed-case>B</fixed-case>ench: A Comprehensive Benchmark to Evaluate <fixed-case>VLM</fixed-case>s for Intelligent Cockpit</title>
      <author><first>Liang</first><last>Lin</last></author>
      <author><first>Siyuan</first><last>Chai</last><affiliation>Anhui University</affiliation></author>
      <author><first>Jiahao</first><last>Wu</last></author>
      <author><first>Hongbing</first><last>Hu</last><affiliation>Anhui University</affiliation></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Hao</first><last>Hu</last></author>
      <author><first>Fan</first><last>Zhang</last><affiliation>Anhui University</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last></author>
      <pages>15453-15475</pages>
      <abstract>The integration of sophisticated Vision-Language Models (VLMs) in vehicular systems is revolutionizing vehicle interaction and safety, performing tasks such as Visual Question Answering (VQA). However, a critical gap persists due to the lack of a comprehensive benchmark for multimodal VQA models in vehicular scenarios. To address this, we propose IntelliCockpitBench, a benchmark that encompasses diverse automotive scenarios. It includes images from front, side, and rear cameras, various road types, weather conditions, and interior views, integrating data from both moving and stationary states. Notably, all images and queries in the benchmark are verified for high levels of authenticity, ensuring the data accurately reflects real-world conditions. A sophisticated scoring methodology combining human and model-generated assessments enhances reliability and consistency. Our contributions include a diverse and authentic dataset for automotive VQA and a robust evaluation metric aligning human and machine assessments. All code and data can be found at <url>https://github.com/Lane315/IntelliCockpitBench</url>.</abstract>
      <url hash="392227b5">2025.findings-acl.798</url>
      <bibkey>lin-etal-2025-intellicockpitbench</bibkey>
    </paper>
    <paper id="799">
      <title>Analyzing Political Bias in <fixed-case>LLM</fixed-case>s via Target-Oriented Sentiment Classification</title>
      <author><first>Akram</first><last>Elbouanani</last></author>
      <author><first>Evan</first><last>Dufraisse</last></author>
      <author><first>Adrian</first><last>Popescu</last><affiliation>CEA</affiliation></author>
      <pages>15476-15505</pages>
      <abstract>Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts. The complete code, the data, and all analyses will be made public to enable reproducibility.</abstract>
      <url hash="1dc9a370">2025.findings-acl.799</url>
      <bibkey>elbouanani-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="800">
      <title><fixed-case>PISCO</fixed-case>: Pretty Simple Compression for Retrieval-Augmented Generation</title>
      <author><first>Maxime</first><last>Louis</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Hervé</first><last>Déjean</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Stéphane</first><last>Clinchant</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>15506-15521</pages>
      <abstract>Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models (LLMs) by retrieving relevant documents, but they face scalability issues due to high inference costs and limited context size. Document compression is a practical solution, but current soft compression methods often suffer from accuracy losses and require extensive pretraining. In this paper, we introduce PISCO, a novel method that achieves a 16x compression rate with minimal accuracy loss (0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing approaches, PISCO requires no pretraining or annotated data, relying solely on sequence-level knowledge distillation from document-based questions. With the ability to fine-tune a 7-10B LLM in 24 hours on a single A100 GPU, PISCO offers a highly efficient and scalable solution. We present comprehensive experiments showing that PISCO outperforms existing compression models by 8% in accuracy.</abstract>
      <url hash="52ddb843">2025.findings-acl.800</url>
      <bibkey>louis-etal-2025-pisco</bibkey>
    </paper>
    <paper id="801">
      <title><fixed-case>A</fixed-case>nchor<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Anchors Pave the Way for Multi-hop Reasoning</title>
      <author><first>Tianshi</first><last>Ming</last></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Yingying</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Zichuan</first><last>Fu</last></author>
      <author><first>Dawei</first><last>Cheng</last><affiliation>Tongji University</affiliation></author>
      <pages>15522-15536</pages>
      <abstract>Large Language Models (LLMs) have made substantial strides in a broad array of natural language tasks. Recently, LLMs have demonstrated potential reasoning capabilities through prompt design, such as the Chain of Thought (CoT). Despite their superiority in question answering, LLMs still face challenges in answering questions that require multi-hop reasoning, often generating unreliable reasoning chains during answer generation. To improve LLMs’ performance in multi-hop reasoning, we introduce a novel reasoning approach, AnchorCoT, designed to assist LLMs in answering questions involving complex logical reasoning steps. AnchorCoT first predicts key entities which work as important “anchors” to guide the reasoning process and then employs a novel ranking algorithm to ensure the logical sequence of the predicted answers.We implement AnchorCoT on Qwen2.5-7B/14B and GPT-4o and evaluate our method on widely used multi-hop reasoning datasets, including HotpotQA, 2WikiMultiHopQA, and MuSiQue-Ans. The experimental results show that AnchorCoT outperforms existing methods in multi-hop question reasoning and provides more accurate reasoning results in multi-hop question answering tasks.</abstract>
      <url hash="3481b12b">2025.findings-acl.801</url>
      <bibkey>ming-etal-2025-anchorcot</bibkey>
    </paper>
    <paper id="802">
      <title>Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?</title>
      <author><first>Zichen</first><last>Wen</last></author>
      <author><first>Yifeng</first><last>Gao</last></author>
      <author><first>Weijia</first><last>Li</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>15537-15549</pages>
      <abstract>Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods. Codes are available in the supplementary materials.</abstract>
      <url hash="9fab59f0">2025.findings-acl.802</url>
      <bibkey>wen-etal-2025-token</bibkey>
    </paper>
    <paper id="803">
      <title>Federated Data-Efficient Instruction Tuning for Large Language Models</title>
      <author><first>Zhen</first><last>Qin</last></author>
      <author><first>Zhaomin</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bingsheng</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Shuiguang</first><last>Deng</last><affiliation>Zhejiang University</affiliation></author>
      <pages>15550-15568</pages>
      <abstract>Instruction tuning is a crucial step in improving the responsiveness of pretrained large language models (LLMs) to human instructions. Federated learning (FL) helps to exploit the use of vast private instruction data from clients, becoming popular for LLM tuning by improving data diversity. Existing federated tuning simply consumes all local data, causing excessive computational overhead and overfitting to local data, while centralized data-efficient solutions are not suitable for FL due to privacy concerns. This work presents FedHDS, a federated data-efficient instruction tuning approach, which tunes LLMs with a representative subset of edge-side data. It reduces the data redundancy at both intra- and inter-client levels without sharing raw data. Experiments with various LLMs, datasets and partitions show that FedHDS improves Rouge-L on unseen tasks by an average of 10.72% over the SOTA full-data federated instruction tuning methods, while using less than 1.5% of the data samples, improving training efficiency by up to tens of times.</abstract>
      <url hash="4a59e45b">2025.findings-acl.803</url>
      <bibkey>qin-etal-2025-federated</bibkey>
    </paper>
    <paper id="804">
      <title>They want to pretend not to understand: The Limits of Current <fixed-case>LLM</fixed-case>s in Interpreting Implicit Content of Political Discourse</title>
      <author><first>Walter</first><last>Paci</last><affiliation>University of Florence</affiliation></author>
      <author><first>Alessandro</first><last>Panunzi</last><affiliation>University of Florence</affiliation></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>15569-15593</pages>
      <abstract>Implicit content plays a crucial role in political discourse, where systematically employ pragmatic strategies such as implicatures and presuppositions to influence their audiences. Large Language Models (LLMs) have demonstrated strong performance in tasks requiring complex semantic and pragmatic understanding, highlighting their potential for detecting and explaining the meaning of implicit content. However, their ability to do this within political discourse remains largely underexplored. Leveraging, for the very first time, the large IMPAQTS corpus comprising transcribed Italian political speeches with expert annotations of various types of implicit content, we propose methods to test the effectiveness of LLMs in this challenging problem. Through a multiple-choice task and an open-ended generation task, we demonstrate that all tested models struggle to interpret presuppositions and implicatures. To illustrate, the best-performing model provides a fully correct explanation in only one-fourth of cases in the open-ended generation setup. We conclude that current LLMs lack the key pragmatic capabilities necessary for accurately interpreting highly implicit language, such as that found in political discourse. At the same time, we highlight promising trends and future directions for enhancing model performance. We release our data and code at: <tex-math>\url{https://github.com/WalterPaci/IMPAQTS-PID}</tex-math></abstract>
      <url hash="6e1d1751">2025.findings-acl.804</url>
      <bibkey>paci-etal-2025-want</bibkey>
    </paper>
    <paper id="805">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>NER</fixed-case>: Fueling Zero-Shot Named Entity Recognition via Entity Type Descriptions</title>
      <author><first>Alessio</first><last>Cocchieri</last></author>
      <author><first>Marcos</first><last>Martínez Galindo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Giacomo</first><last>Frisoni</last></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <author><first>Claudio</first><last>Sartori</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Giuseppe</first><last>Tagliavini</last><affiliation>University of Bologna</affiliation></author>
      <pages>15594-15616</pages>
      <abstract>What happens when a named entity recognition (NER) system encounters entities it has never seen before? In practical applications, models must generalize to unseen entity types where labeled training data is either unavailable or severely limited—a challenge that demands zero-shot learning capabilities. While large language models (LLMs) offer extensive parametric knowledge, they fall short in cost-effectiveness compared to specialized small encoders. Existing zero-shot methods predominantly adopt a relaxed definition of the term with potential leakage issues and rely on entity type names for generalization, overlooking the value of richer descriptions for disambiguation. In this work, we introduce ZeroNER, a description-driven framework that enhances hard zero-shot NER in low-resource settings. By leveraging general-domain annotations and entity type descriptions with LLM supervision, ZeroNER enables a BERT-based student model to successfully identify unseen entity types. Evaluated on three real-world benchmarks, ZeroNER consistently outperforms LLMs by up to 16% in F1 score, and surpasses lightweight baselines that use type names alone. Our analysis further reveals that LLMs derive significant benefits from incorporating type descriptions in the prompts.</abstract>
      <url hash="e1b55cde">2025.findings-acl.805</url>
      <bibkey>cocchieri-etal-2025-zeroner</bibkey>
    </paper>
    <paper id="806">
      <title>Do Large Language Models Have “Emotion Neurons”? Investigating the Existence and Role</title>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Woojin</first><last>Lee</last></author>
      <author><first>Oh-Woog</first><last>Kwon</last></author>
      <author><first>Harksoo</first><last>Kim</last><affiliation>Konkuk University</affiliation></author>
      <pages>15617-15639</pages>
      <abstract>This study comprehensively explores whether there actually exist “emotion neurons” within large language models (LLMs) that selectively process and express certain emotions, and what functional role they play. Drawing on the representative emotion theory of the six basic emotions, we focus on six core emotions. Using synthetic dialogue data labeled with emotions, we identified sets of neurons that exhibit consistent activation patterns for each emotion. As a result, we confirmed that principal neurons handling emotion information do indeed exist within the model, forming distinct groups for each emotion, and that their distribution varies with model size and architectural depth. We then validated the functional significance of these emotion neurons by analyzing whether the prediction accuracy for a specific emotion significantly decreases when those neurons are artificially removed. We observed that in some emotions, the accuracy drops sharply upon neuron removal, while in others, the model’s performance largely remains intact or even improves, presumably due to overlapping and complementary mechanisms among neurons. Furthermore, by examining how prediction accuracy changes depending on which layer range and at what proportion the emotion neurons are masked, we revealed that emotion information is processed in a multilayered and complex manner within the model.</abstract>
      <url hash="2105e4bc">2025.findings-acl.806</url>
      <bibkey>lee-etal-2025-large</bibkey>
    </paper>
    <paper id="807">
      <title>Grammar-Based Code Representation: Is It a Worthy Pursuit for <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Qingyuan</first><last>Liang</last></author>
      <author><first>Zhao</first><last>Zhang</last></author>
      <author><first>Zeyu</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Qi</first><last>Luo</last></author>
      <author><first>Xiao</first><last>Yueyi</last></author>
      <author><first>Yizhou</first><last>Chen</last></author>
      <author><first>Yuqun</first><last>Zhang</last></author>
      <author><first>Haotian</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Lu</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Chenbin</first><last>Chenbin</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yingfei</first><last>Xiong</last><affiliation>Peking University</affiliation></author>
      <pages>15640-15653</pages>
      <abstract>Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs’ ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation.</abstract>
      <url hash="2809c821">2025.findings-acl.807</url>
      <bibkey>liang-etal-2025-grammar</bibkey>
    </paper>
    <paper id="808">
      <title>Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study</title>
      <author><first>Yujie</first><last>Lin</last></author>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Moye</first><last>Chen</last></author>
      <author><first>Jingyao</first><last>Liu</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xinyan</first><last>Xiao</last><affiliation>Baidu</affiliation></author>
      <pages>15654-15667</pages>
      <abstract>Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks.While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored.In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap.To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains.Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms.Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking.Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications.We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field. The code will be released upon acceptance.</abstract>
      <url hash="250b4220">2025.findings-acl.808</url>
      <bibkey>lin-etal-2025-investigating-inference</bibkey>
    </paper>
    <paper id="809">
      <title><fixed-case>UI</fixed-case>-<fixed-case>E</fixed-case>2<fixed-case>I</fixed-case>-Synth: Advancing <fixed-case>GUI</fixed-case> Grounding with Large-Scale Instruction Synthesis</title>
      <author><first>Xinyi</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaoyi</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ziyun</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Lu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>15668-15684</pages>
      <abstract>Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability.In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation.In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline <tex-math>\textit{UI-E2I-Synth}</tex-math> for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark <tex-math>\textit{UI-I2E-Bench}</tex-math>, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects.Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline.The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in this domain. We will release our dataset and benchmark to facilitate further development of GUI instruction grounding community.</abstract>
      <url hash="707c6ceb">2025.findings-acl.809</url>
      <bibkey>liu-etal-2025-ui</bibkey>
    </paper>
    <paper id="810">
      <title>A Study into Investigating Temporal Robustness of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jonas</first><last>Wallat</last><affiliation>L3S Research Center</affiliation></author>
      <author><first>Abdelrahman</first><last>Abdallah</last></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Avishek</first><last>Anand</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>15685-15705</pages>
      <abstract>Large Language Models (LLMs) encapsulate a surprising amount of factual world knowledge. However, their performance on temporal questions and historical knowledge is limited because they often cannot understand temporal scope and orientation or neglect the temporal aspect altogether.In this study, we aim to measure precisely how robust LLMs are for question answering based on their ability to process temporal information and perform tasks requiring temporal reasoning and temporal factual knowledge. Specifically, we design eight time-sensitiverobustness tests for factual information to check the sensitivity of six popular LLMs in the zero-shot setting.Overall, we find LLMs lacking temporal robustness, especially to temporal reformulations and the use of different granularities of temporal references. We show how a selection of these eight tests can be used automatically to judge a model’s temporal robustness for user questions on the fly. Finally, we apply the findings of this study to improve the temporal QA performance by up to 55%.</abstract>
      <url hash="6de9a7ae">2025.findings-acl.810</url>
      <bibkey>wallat-etal-2025-study</bibkey>
    </paper>
    <paper id="811">
      <title><fixed-case>T</fixed-case>ool<fixed-case>E</fixed-case>xp<fixed-case>N</fixed-case>et: Optimizing Multi-Tool Selection in <fixed-case>LLM</fixed-case>s with Similarity and Dependency-Aware Experience Networks</title>
      <author><first>Zijing</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhanpeng</first><last>Chen</last></author>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Ziyang</first><last>Chen</last></author>
      <author><first>Nan</first><last>Du</last><affiliation>Tencent INC</affiliation></author>
      <author><first>Xiaolong</first><last>Li</last><affiliation>Tencent America LLC</affiliation></author>
      <pages>15706-15722</pages>
      <abstract>Tool learning enhances Large Language Models’ (LLMs) dynamic interaction with external tools, improving their ability to solve complex problems. However, current empirical methods, which primarily focus on isolated tools learning, still struggle with accurate multi-tool selection due to issues like confusing similar tools and neglecting dependencies. To address these challenges, we propose the Tool Experience Network (ToolExpNet), which integrates tools and trial-and-error experiences into a network characterized by semantic similarity and dependency relationships. ToolExpNet iteratively conducts simulated experiments using adaptive sampling to explore subtle differences and connections between tools, and summarizes these experiences to provide insightful guidance for LLM tool selection. Our experiments demonstrate that learning the relationships between tools helps achieve more comprehensive tool learning. Evaluations on multiple real-world API datasets show that ToolExpNet effectively addresses common challenges in multi-tool selection, significantly outperforming existing baselines across different foundation LLMs.</abstract>
      <url hash="c37110eb">2025.findings-acl.811</url>
      <bibkey>zhang-etal-2025-toolexpnet</bibkey>
    </paper>
    <paper id="812">
      <title><fixed-case>SPILL</fixed-case>: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models</title>
      <author><first>I-Fan</first><last>Lin</last><affiliation>Leiden University</affiliation></author>
      <author><first>Faegheh</first><last>Hasibi</last><affiliation>Radboud University</affiliation></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <pages>15723-15737</pages>
      <abstract>In this paper, we propose Selection and Pooling with Large Language Models (SPILL), an intuitive, domain-adaptive method for intent clustering without fine-tuning. Existing embeddings-based clustering methods rely on a few labeled examples or unsupervised fine-tuning to optimize results for each new dataset, which makes them less generalizable to multiple datasets. Our goal is to make these existing embedders more generalizable to new domain datasets without further fine-tuning. Inspired by our theoretical derivation and simulation results on the effectiveness of sampling and pooling techniques, we view the clustering task as a small-scale selection problem. A good solution to this problem is associated with better clustering performance. Accordingly, we propose a two-stage approach: First, for each utterance (referred to as the seed), we derive its embedding using an existing embedder. Then, we apply a distance metric to select a pool of candidates close to the seed. Because the embedder is not optimized for new datasets, in the second stage, we use an LLM to further select utterances from these candidates that share the same intent as the seed. Finally, we pool these selected candidates with the seed to derive a refined embedding for the seed. We found that our method generally outperforms directly using an embedder, and it achieves comparable results to other state-of-the-art studies, even those that use much larger models and require fine-tuning, showing its strength and efficiency. Our results indicate that our method enables existing embedders to be further improved without additional fine-tuning, making them more adaptable to new domain datasets. Additionally, viewing the clustering task as a small-scale selection problem gives the potential of using LLMs to customize clustering tasks according to the user’s goals.</abstract>
      <url hash="91a2ed06">2025.findings-acl.812</url>
      <bibkey>lin-etal-2025-spill</bibkey>
    </paper>
    <paper id="813">
      <title>How Far are <fixed-case>LLM</fixed-case>s from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation</title>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Heming</first><last>Xia</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xinfeng</first><last>Yuan</last></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Lei</first><last>Sha</last><affiliation>Beihang University</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>15738-15763</pages>
      <abstract>Recently, LLMs have garnered increasing attention across academic disciplines for their potential as human digital twins, virtual proxies designed to replicate individuals and autonomously perform tasks such as decision-making, problem-solving, and reasoning on their behalf.However, current evaluations of LLMs primarily emphasize dialogue simulation while overlooking human behavior simulation, which is crucial for digital twins.To address this gap, we introduce BehaviorChain, the first benchmark for evaluating LLMs’ ability to simulate continuous human behavior.BehaviorChain comprises diverse, high-quality, persona-based behavior chains, totaling 15,846 distinct behaviors across 1,001 unique personas, each with detailed history and profile metadata.For evaluation, we integrate persona metadata into LLMs and employ them to iteratively infer contextually appropriate behaviors within dynamic scenarios provided by BehaviorChain. Comprehensive evaluation results demonstrated that even state-of-the-art models struggle with accurately simulating continuous human behavior.</abstract>
      <url hash="54280c3b">2025.findings-acl.813</url>
      <bibkey>li-etal-2025-far</bibkey>
    </paper>
    <paper id="814">
      <title><fixed-case>GRI</fixed-case>-<fixed-case>QA</fixed-case>: a Comprehensive Benchmark for Table Question Answering over Environmental Data</title>
      <author><first>Michele Luca</first><last>Contalbo</last></author>
      <author><first>Sara</first><last>Pederzoli</last></author>
      <author><first>Francesco Del</first><last>Buono</last><affiliation>University of Modena and Reggio Emilia</affiliation></author>
      <author><first>Venturelli</first><last>Valeria</last></author>
      <author><first>Francesco</first><last>Guerra</last></author>
      <author><first>Matteo</first><last>Paganelli</last></author>
      <pages>15764-15779</pages>
      <abstract>Assessing corporate environmental sustainability with Table Question Answering systems is challenging due to complex tables, specialized terminology, and the variety of questions they must handle. In this paper, we introduce GRI-QA, a test benchmark designed to evaluate Table QA approaches in the environmental domain. Using GRI standards, we extract and annotate tables from non-financial corporate reports, generating question-answer pairs through a hybrid LLM-human approach. The benchmark includes eight datasets, categorized by the types of operations required, including operations on multiple tables from multiple documents. Our evaluation reveals a significant gap between human and model performance, particularly in multi-step reasoning, highlighting the relevance of the benchmark and the need for further research in domain-specific Table QA. Code and benchmark datasets are available at https://github.com/softlab-unimore/gri_qa.</abstract>
      <url hash="c23948ed">2025.findings-acl.814</url>
      <bibkey>contalbo-etal-2025-gri</bibkey>
    </paper>
    <paper id="815">
      <title><fixed-case>W</fixed-case>eb<fixed-case>UIB</fixed-case>ench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in <fixed-case>W</fixed-case>eb<fixed-case>UI</fixed-case>-to-Code</title>
      <author><first>Zhiyu</first><last>Lin</last></author>
      <author><first>Zhengda</first><last>Zhou</last></author>
      <author><first>Zhiyuan</first><last>Zhao</last><affiliation>China Telecom</affiliation></author>
      <author><first>Tianrui</first><last>Wan</last></author>
      <author><first>Yilun</first><last>Ma</last></author>
      <author><first>Junyu</first><last>Gao</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>China Telecom and Northwestern Polytechnical University</affiliation></author>
      <pages>15780-15797</pages>
      <abstract>With the rapid advancement of Generative AI technology, Multimodal Large Language Models(MLLMs) have the potential to act as AI software engineers capable of executing complex web application development. Considering that the model requires a confluence of multidimensional sub-capabilities to address the challenges of various development phases, constructing a multi-view evaluation framework is crucial for accurately guiding the enhancement of development efficiency. However, existing benchmarks usually fail to provide an assessment of sub-capabilities and focus solely on webpage generation outcomes. In this work, we draw inspiration from the principles of software engineering and further propose WebUIBench, a benchmark systematically designed to evaluate MLLMs in four key areas: WebUI Perception, HTML Programming, WebUI-HTML Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality question-answer pairs derived from over 0.7K real-world websites. The extensive evaluation of 29 mainstream MLLMs uncovers the skill characteristics and various weakness that models encountered during the development process.</abstract>
      <url hash="a06c532f">2025.findings-acl.815</url>
      <bibkey>lin-etal-2025-webuibench</bibkey>
    </paper>
    <paper id="816">
      <title>Optimizing Multi-Hop Document Retrieval Through Intermediate Representations</title>
      <author><first>Linjiaen</first><last>Linjiaen</last></author>
      <author><first>Jingyu</first><last>Liu</last></author>
      <author><first>Yingbo</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>15798-15809</pages>
      <abstract>Retrieval-augmented generation (RAG) encounters challenges when addressing complex queries, particularly multi-hop questions. While several methods tackle multi-hop queries by iteratively generating internal queries and retrieving external documents, these approaches are computationally expensive. In this paper, we identify a three-stage information processing pattern in LLMs during layer-by-layer reasoning, consisting of extraction, processing, and subsequent extraction steps. This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike prior methods that focus on generating new internal queries, L-RAG leverages intermediate representations from the middle layers, which capture next-hop information, to retrieve external knowledge. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG. Experimental results show that L-RAG outperforms existing RAG methods on open-domain multi-hop question-answering datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is available in https://anonymous.4open.science/r/L-RAG-ADD5/.</abstract>
      <url hash="a6fc57c0">2025.findings-acl.816</url>
      <bibkey>linjiaen-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="817">
      <title>Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments</title>
      <author><first>Patomporn</first><last>Payoungkhamdee</last></author>
      <author><first>Pume</first><last>Tuchinda</last><affiliation>Vidyasirimedhi Institute of Science and Technology</affiliation></author>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last><affiliation>Vidyasirimedhi Institute of Science and Technology</affiliation></author>
      <author><first>Potsawee</first><last>Manakul</last><affiliation>SCB 10X</affiliation></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>15810-15828</pages>
      <abstract>Multi-step reasoning is essential for large language models (LLMs), yet multilingual performance remains challenging. While Chain-of-Thought (CoT) prompting improves reasoning, it struggles with non-English languages due to the entanglement of reasoning and execution. Program-of-Thought (PoT) prompting separates reasoning from execution, offering a promising alternative but shifting the challenge to generating programs from non-English questions. We propose a framework to evaluate PoT by separating multilingual reasoning from code execution to examine (i) the impact of fine-tuning on question-reasoning alignment and (ii) how reasoning quality affects answer correctness. Our findings demonstrate that PoT fine-tuning substantially enhances multilingual reasoning, outperforming CoT fine-tuned models. We further demonstrate a strong correlation between reasoning quality (measured through code quality) and answer accuracy, highlighting its potential as a test-time performance improvement heuristic.</abstract>
      <url hash="0007c7ac">2025.findings-acl.817</url>
      <bibkey>payoungkhamdee-etal-2025-towards</bibkey>
    </paper>
    <paper id="818">
      <title>A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models</title>
      <author><first>Kseniia</first><last>Petukhova</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>15829-15852</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation.</abstract>
      <url hash="97e6d155">2025.findings-acl.818</url>
      <bibkey>petukhova-kochmar-2025-fully</bibkey>
    </paper>
    <paper id="819">
      <title>Can Language Models Serve as Analogy Annotators?</title>
      <author><first>Xiaojing</first><last>Zhang</last><affiliation>DataCanvas</affiliation></author>
      <author><first>Bochen</first><last>Lyu</last></author>
      <pages>15853-15883</pages>
      <abstract>Conceptual abstraction and analogy-making are crucial for human learning, reasoning, and adapting to unfamiliar domains. Recently, large language models (LLMs) have made the synthesis of analogical data possible, which, however, still heavily relies on extensive human efforts to be annotated. This paper empirically examines the LLMs’ capability to annotate story-level analogical data. Specifically, we propose a novel multi-stage progressive reasoning prompt framework <tex-math>\texttt{A3E}</tex-math> (Automated Analogy Annotation Expert), which is based on the structure mapping theory from cognitive psychology and efficiently annotates candidate story pairs across six fine-grained categories. We use <tex-math>\texttt{A3E}</tex-math> to evaluate how well the state-of-the-art LLMs can serve as analogy annotators. Experimental results demonstrate that our proposed <tex-math>\texttt{A3E}</tex-math> achieves an average performance gain of + 73% across a range of prompting baselines and base LLMs. The code and data is available at https://github.com/zhangxjohn/A3E.</abstract>
      <url hash="70bbee18">2025.findings-acl.819</url>
      <bibkey>zhang-lyu-2025-language</bibkey>
    </paper>
    <paper id="820">
      <title>Reward Generalization in <fixed-case>RLHF</fixed-case>: A Topological Perspective</title>
      <author><first>Tianyi Alex</first><last>Qiu</last></author>
      <author><first>Fanzhi</first><last>Zeng</last></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Dong</first><last>Yan</last></author>
      <author><first>Kaile</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiayi</first><last>Zhou</last><affiliation>Peking University</affiliation></author>
      <author><first>Yang</first><last>Han</last><affiliation>China Telecom</affiliation></author>
      <author><first>Josef</first><last>Dai</last><affiliation>Peking University</affiliation></author>
      <author><first>Xuehai</first><last>Pan</last><affiliation>Peking University</affiliation></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <pages>15884-15930</pages>
      <abstract>Existing alignment methods share a common topology of information flow, where reward information is collected from humans, modeled with preference learning, and used to tune language models. However, this shared topology has not been systematically characterized, nor have its alternatives been thoroughly explored, leaving the problems of low data efficiency and unreliable generalization unaddressed. As a solution, we introduce a theory of **reward generalization** in reinforcement learning from human feedback (RLHF), focusing on the **topology of information flow** at both macro and micro levels. At the macro level, we portray the RLHF information flow as an autoencoding process over behavior distributions, formalizing the RLHF objective of distributional consistency between human preference and model behavior. At the micro level, we present *induced Bayesian networks* to model the impact of dataset topologies on reward generalization. Combining analysis on both levels, we propose **reward modeling from tree-structured preference information**. It is shown to reduce reward uncertainty by up to <tex-math>\Theta(\log n/\log\log n)</tex-math> times compared to baselines, where <tex-math>n</tex-math> is the dataset size. Validation on three NLP tasks shows that it achieves an average win rate of 65% against baselines, thus improving reward generalization *for free* via topology design, while *reducing* the amount of data requiring annotation.</abstract>
      <url hash="40e5c386">2025.findings-acl.820</url>
      <bibkey>qiu-etal-2025-reward</bibkey>
    </paper>
    <paper id="821">
      <title>Enhanced Data Synthesis for <fixed-case>LLM</fixed-case> through Reasoning Structures Generated by Hierarchical <fixed-case>GF</fixed-case>low<fixed-case>N</fixed-case>et</title>
      <author><first>Tianpeng</first><last>Bu</last></author>
      <author><first>Minying</first><last>Zhang</last></author>
      <author><first>Hongtao</first><last>Duan</last></author>
      <author><first>Shurui</first><last>Li</last></author>
      <author><first>Lulu</first><last>Hu</last></author>
      <author><first>Yu</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>15931-15958</pages>
      <abstract>Large language models (LLMs) excel in problem-solving but require training data with diverse reasoning processes. Existing methods mainly optimize instruction-response pairs but lack a systematic design for the underlying reasoning structure. This paper proposes RSS: a Reasoning Structure driven data Synthesis method. We first proactively develop a hierarchical GFlowNet to construct reasoning structures efficiently through a coarse-to-fine directed acyclic graph (DAG) growth process. Then reasoning DAGs are leveraged to actively guide the instruction generation via an iterative suggester-editor workflow and enhance response quality using a structure-aware strategy. Experiments show that LLMs trained on our synthetic datasets achieve 48.50%, 84.00%, 79.90% for AlpacaEval2, GSM8K and HumanEval, outperforming existing data synthesis methods.</abstract>
      <url hash="326f1a31">2025.findings-acl.821</url>
      <bibkey>bu-etal-2025-enhanced</bibkey>
    </paper>
    <paper id="822">
      <title>Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models</title>
      <author><first>Yanggan</first><last>Gu</last></author>
      <author><first>Junzhuo</first><last>Li</last></author>
      <author><first>Sirui</first><last>Huang</last><affiliation>University of Technology Sydney and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xin</first><last>Zou</last></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>15959-15973</pages>
      <abstract>Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher’s preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student’s intrinsic preference distribution to align with the teacher’s. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the Gemma model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.</abstract>
      <url hash="bca86973">2025.findings-acl.822</url>
      <bibkey>gu-etal-2025-capturing</bibkey>
    </paper>
    <paper id="823">
      <title>Token-level Preference Self-Alignment Optimization for Multi-style Outline Controllable Generation</title>
      <author><first>Zihao</first><last>Li</last></author>
      <author><first>Xuekong</first><last>Xu</last></author>
      <author><first>Ziyao</first><last>Chen</last></author>
      <author><first>Lixin</first><last>Zou</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ethanhjwu</first><last>Ethanhjwu</last><affiliation>Tencent</affiliation></author>
      <author><first>Qiang</first><last>Chen</last><affiliation>Tencent Inc.</affiliation></author>
      <author><first>Chenliang</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <pages>15974-16007</pages>
      <abstract>Multi-style outline controllable generation is crucial for multiple applications, including document semantic structuring and retrieval-augmented generation.The great success of preference alignment approaches encourages their application in controllable generation tasks.However, these attempts encounter several limitations: (1) response pair requirements, (2) substantial computation costs, and (3) insufficient exploitation of fine-grained preference signals.To address these problems, we propose a token-level preference self-alignment optimization, named TKPO, for outline controllable generation. TKPO extends the Bradley-Terry model from pair-wise to list-wise comparison, which is further applied at the token level for fine-grained preference signal utilization. In comparison to the representative methods, e.g., DPO, TKPO does not require response pairs; instead, we propose a controllable attributes-driven method to construct reject samples for self-alignment. Additionally, TKPO optimizes only the base model, thereby avoiding additional memory usage and substantial computational costs.We curate two outline controllable generation datasets with regard to language style and level-of-detail.Extensive experiments demonstrate that TKPO outperforms DPO by up to 19.28% in performance while requiring only 56.25% in training time.We release the code and datasets resources at https://github.com/WHUIR/TKPO.</abstract>
      <url hash="69d37705">2025.findings-acl.823</url>
      <bibkey>li-etal-2025-token</bibkey>
    </paper>
    <paper id="824">
      <title><fixed-case>H</fixed-case>ate<fixed-case>PRISM</fixed-case>: Policies, Platforms, and Research Integration. Advancing <fixed-case>NLP</fixed-case> for Hate Speech Proactive Mitigation</title>
      <author><first>Naquee</first><last>Rizwan</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Seid Muhie</first><last>Yimam</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Dr. Florian</first><last>Skupin</last><affiliation>Bucerius Law School gGmbH</affiliation></author>
      <author><first>Tim</first><last>Fischer</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Aarushi Ajay</first><last>Borkar</last></author>
      <author><first>Robert</first><last>Geislinger</last></author>
      <author><first>Punyajoy</first><last>Saha</last><affiliation>Samsung</affiliation></author>
      <author><first>Sarthak</first><last>Roy</last><affiliation>Indian Institute of Technology, Kharagpur</affiliation></author>
      <author><first>Martin</first><last>Semmann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>16008-16022</pages>
      <abstract>Despite regulations imposed by nations and social media platforms, e.g. (Government of India, 2021; European Parliament and Council of the European Union, 2022), inter alia, hateful content persists as a significant challenge. Existing approaches primarily rely on reactive measures such as blocking or suspending offensive messages, with emerging strategies focusing on proactive measurements like detoxification and counterspeech. In our work, which we call HATEPRISM, we conduct a comprehensive examination of hate speech regulations and strategies from three perspectives: country regulations, social platform policies, and NLP research datasets. Our findings reveal significant inconsistencies in hate speech definitions and moderation practices across jurisdictions and platforms, alongside a lack of alignment with research efforts. Based on these insights, we suggest ideas and research direction for further exploration of a unified framework for automated hate speech moderation incorporating diverse strategies.</abstract>
      <url hash="3d4172a9">2025.findings-acl.824</url>
      <bibkey>rizwan-etal-2025-hateprism</bibkey>
    </paper>
    <paper id="825">
      <title>Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving</title>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Kumar</first><last>Pratik</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Gabriele</first><last>Cesa</last><affiliation>University of Amsterdam and Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Arash</first><last>Behboodi</last><affiliation>QualComm</affiliation></author>
      <pages>16023-16040</pages>
      <abstract>The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the LLMs, even for the step-wise rewards, or large quantities of human-annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which, unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model’s reasoning accuracy and efficiency.</abstract>
      <url hash="1e1f772b">2025.findings-acl.825</url>
      <bibkey>rajaee-etal-2025-local</bibkey>
    </paper>
    <paper id="826">
      <title>Generalizable Cross-Lingual Cognitive Distortion Detection with Standardized Annotations and Multi-Task Learning</title>
      <author><first>Hongzhi</first><last>Qi</last></author>
      <author><first>Nan</first><last>Bai</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jianqiang</first><last>Li</last><affiliation>Beijing University of Technology</affiliation></author>
      <author><first>Wei</first><last>Zhai</last></author>
      <author><first>Qing</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Gao</last></author>
      <author><first>Bing Xiang</first><last>Yang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Guanghui</first><last>Fu</last></author>
      <pages>16041-16051</pages>
      <abstract>Cognitive distortion is a critical issue in psychology, with most existing studies based on Burns’ cognitive distortion theory. However, differences in annotation standards lead to variations in building analysis tools, resulting in inconsistent analyses and limiting the generalizability of findings, especially in large-scale and cross-linguistic contexts. To address this issue, we collected all publicly available datasets (four in total) and conducted a series of experiments to evaluate the generalizability of various cross-linguistic models. The results indicate that models exhibit significant performance differences across datasets, highlighting the generalization problem. To mitigate this issue, we propose two solutions. First, we propose a multi-task learning model based on teacher student architecture solution, which demonstrates improved generalization performance in our experiments. Second, we introduce a new dataset (~5,000 samples) derived from reannotating existing open datasets to ensure standardized alignment. The annotation process we provided is interpretable and grounded in psychological principles. Based on this, we constructed large language models with cognitive reasoning chains, enhancing both generalizability and interpretability. This study identifies the generalization challenge in cognitive distortion research, and our experiments show that the proposed solutions significantly improve model performance. The dataset and code are publicly available at: https://github.com/HongzhiQ/CrossLinCD.</abstract>
      <url hash="ddb24915">2025.findings-acl.826</url>
      <bibkey>qi-etal-2025-generalizable</bibkey>
    </paper>
    <paper id="827">
      <title>How Do Multilingual Language Models Remember Facts?</title>
      <author><first>Constanza</first><last>Fierro</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Negar</first><last>Foroutan</last><affiliation>School of Computer and Communication Sciences, EPFL - EPF Lausanne</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>Copenhagen University and University of Copenhagen</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>16052-16106</pages>
      <abstract>Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English monolingual models. The question of how these mechanisms generalize to non-English languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of three multilingual LLMs. First, we show that previously identified recall mechanisms in English largely apply to multilingual contexts, with nuances based on language and architecture. Next, through patching intermediate representations, we localize the role of language during recall, finding that subject enrichment is language-independent, while object extraction is language-dependent. Additionally, we discover that the last token representation acts as a Function Vector (FV), encoding both the language of the query and the content to be extracted from the subject. Furthermore, in decoder-only LLMs, FVs compose these two pieces of information in two separate stages. These insights reveal unique mechanisms in multilingual LLMs for recalling information, highlighting the need for new methodologies—such as knowledge evaluation, fact editing, and knowledge acquisition—that are specifically tailored for multilingual LLMs.</abstract>
      <url hash="1ae1a29a">2025.findings-acl.827</url>
      <bibkey>fierro-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="828">
      <title><fixed-case>S</fixed-case>eq<fixed-case>PO</fixed-case>-<fixed-case>S</fixed-case>i<fixed-case>MT</fixed-case>: Sequential Policy Optimization for Simultaneous Machine Translation</title>
      <author><first>Ting</first><last>Xu</last></author>
      <author><first>Zhichao</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <author><first>Jiankai</first><last>Sun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Shanbo</first><last>Cheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>16107-16123</pages>
      <abstract>We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En → Zh and Zh → En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En → Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.</abstract>
      <url hash="3cb3bae8">2025.findings-acl.828</url>
      <bibkey>xu-etal-2025-seqpo</bibkey>
    </paper>
    <paper id="829">
      <title>Do Large Language Models Know Folktales? A Case Study of Yokai in <fixed-case>J</fixed-case>apanese Folktales</title>
      <author><first>Ayuto</first><last>Tsutsumi</last></author>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <pages>16124-16146</pages>
      <abstract>Although Large Language Models (LLMs) have demonstrated strong language understanding and generation abilities across various languages, their cultural knowledge is often limited to English-speaking communities, which can marginalize the cultures of non-English communities. To address the problem, the evaluation of the cultural awareness of the LLMs and the methods to develop culturally aware LLMs have been investigated. In this study, we focus on evaluating knowledge of folktales, a key medium for conveying and circulating culture. In particular, we focus on Japanese folktales, specifically on knowledge of Yokai. Yokai are supernatural creatures originating from Japanese folktales that continue to be popular motifs in art and entertainment today. Yokai have long served as a medium for cultural expression, making them an ideal subject for assessing the cultural awareness of LLMs. We introduce YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions (each with four options) designed to probe knowledge about yokai. We evaluate the performance of 31 Japanese and multilingual LLMs on this dataset. The results show that models trained with Japanese language resources achieve higher accuracy than English-centric models, with those that underwent continued pretraining in Japanese, particularly those based on Llama-3, performing especially well.</abstract>
      <url hash="268e1a86">2025.findings-acl.829</url>
      <bibkey>tsutsumi-jinnai-2025-large</bibkey>
    </paper>
    <paper id="830">
      <title><fixed-case>BOSE</fixed-case>: A Systematic Evaluation Method Optimized for Base Models</title>
      <author><first>Hongzhi</first><last>Luan</last><affiliation>Ant Group</affiliation></author>
      <author><first>Changxin</first><last>Tian</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhaoxin</first><last>Huan</last></author>
      <author><first>Xiaolu</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Kunlong</first><last>Chen</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <pages>16147-16158</pages>
      <abstract>This paper poses two critical issues in evaluating base models (without post-training): (1) Unstable evaluation during training: in the early stages of pre-training, the models lack the capability to answer questions as required, leading to unstable evaluation results. This instability makes it difficult to provide solid conclusions to guide the training, especially for key experiments such as data ablation and scaling law. (2) Inconsistency between base and instruct models: base models generally exhibit poorer evaluation performance compared to corresponding instruct models. This gap poses a challenge for assessing whether a base model with better evaluation can truly lead to a better instruct model. To address these issues, we propose **B**ase model **O**riented **S**ystematic **E**valuation (**BOSE**), a method specifically designed to optimize the evaluation of base models. Specifically, BOSE introduces two key innovations: In-Context Light-instruction Prompt (**ICLiP**) for open-ended tasks and **Blank-ppl** for multi-choice tasks with candidate options, which transforms the standard perplexity (ppl) metric into a fill-in-the-blank format to mitigate early-stage evaluation fluctuations. Furthermore, we are the first to propose Kendall’s rank correlation to quantitatively measure the evaluation stability and consistency. Experimental results demonstrate that BOSE significantly enhances both the stability of evaluations during pre-training and the consistency between base and instruct models, thereby providing more reliable guidance for the LLMs’ training.</abstract>
      <url hash="189dee30">2025.findings-acl.830</url>
      <bibkey>luan-etal-2025-bose</bibkey>
    </paper>
    <paper id="831">
      <title><fixed-case>DPGA</fixed-case>-<fixed-case>T</fixed-case>ext<fixed-case>S</fixed-case>yn: Differentially Private Genetic Algorithm for Synthetic Text Generation</title>
      <author><first>Zhonghao</first><last>Sun</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Yuyi</first><last>Si</last></author>
      <author><first>Juhua</first><last>Zhang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Kai</first><last>Lu</last></author>
      <author><first>Zeyu</first><last>Xiong</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xinwang</first><last>Liu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>16159-16179</pages>
      <abstract>Using large language models (LLMs) has a potential risk of privacy leakage since the data with sensitive information may be used for fine-tuning the LLMs. Differential privacy (DP) provides theoretical guarantees of privacy protection, but its practical application in LLMs still has the problem of privacy-utility trade-off. Researchers synthesized data with strong generation capabilities closed-source LLMs (i.e., GPT-4) under DP to alleviate this problem, but this method is not so flexible in fitting the given privacy distributions without fine-tuning. Besides, such methods can hardly balance the diversity of synthetic data and its relevance to target privacy data without accessing so much private data. To this end, this paper proposes DPGA-TextSyn, combining general LLMs with genetic algorithm (GA) to produce relevant and diverse synthetic text under DP constraints. First, we integrate the privacy gene (i.e., metadata) to generate better initial samples. Then, to achieve survival of the fittest and avoid homogeneity, we use privacy nearest neighbor voting and similarity suppression to select elite samples. In addition, we expand elite samples via genetic strategies such as mutation, crossover, and generation to expand the search scope of GA. Experiments show that this method significantly improves the performance of the model in downstream tasks while ensuring privacy.</abstract>
      <url hash="2abfcebf">2025.findings-acl.831</url>
      <bibkey>sun-etal-2025-dpga</bibkey>
    </paper>
    <paper id="832">
      <title>Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer</title>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Seongtae</first><last>Hong</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>16180-16193</pages>
      <abstract>Large Language Models (LLMs) are increasingly incorporating multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model’s embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies to handle each non-overlapping token’s embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods, achieving lower loss and faster convergence during language adaptation. Notably, SALT achieves remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.</abstract>
      <url hash="54a044b8">2025.findings-acl.832</url>
      <bibkey>lee-etal-2025-semantic-aware</bibkey>
    </paper>
    <paper id="833">
      <title>Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation</title>
      <author><first>Kounianhua</first><last>Du</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hanjing</first><last>Wang</last></author>
      <author><first>Jianxing</first><last>Liu</last></author>
      <author><first>Jizheng</first><last>Chen</last></author>
      <author><first>Xinyi</first><last>Dai</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Ruiming</first><last>Tang</last></author>
      <author><first>Yong</first><last>Yu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jun</first><last>Wang</last><affiliation>University College London</affiliation></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <pages>16194-16204</pages>
      <abstract>To address these limitations, we propose BDC, a novel framework that Boosts reasoning exploration via multi-agent collaboration, Disentangles heterogeneous data into specialized experts, and Customizes solutions through dynamic model composition. BDC integrates a Monte Carlo Tree-of-Agents algorithm, where multiple LLMs mutually verify and refine reasoning paths through reflection-guided pruning, enabling efficient exploration of high-quality solutions. To handle data diversity, we cluster problems by latent semantics, train composable LoRA experts on each cluster, and deploy an input-aware hypernetwork to dynamically merge these experts into tailored solvers. Experiments on APPS and CodeContest benchmarks demonstrate BDC’s superiority: it achieves up to 73.8% accuracy on hard problems, outperforming state-of-the-art methods like LATS and RethinkMCTS by 9–15%. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks, offering a novel System2-to-System1 solution.</abstract>
      <url hash="5d0761bf">2025.findings-acl.833</url>
      <bibkey>du-etal-2025-boost</bibkey>
    </paper>
    <paper id="834">
      <title>On the Consistency of Commonsense in Large Language Models</title>
      <author><first>Guozheng</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Wenjun</first><last>Ke</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zijie</first><last>Xu</last></author>
      <author><first>Jiajun</first><last>Liu</last></author>
      <author><first>Ziyu</first><last>Shang</last><affiliation>Southeast University</affiliation></author>
      <pages>16205-16225</pages>
      <abstract>Commonsense, humans’ implicit understanding of everyday situations, is crucial for large language models (LLMs). Existing commonsense evaluations for LLMs primarily focus on downstream knowledge tasks, failing to probe whether LLMs truly understand and utilize knowledge or merely memorize it. They also rely heavily on human annotation and lack automated large-scale data generation. To address this, we propose to automatically construct a large benchmark named CoCo (Consistency of Commonsense) comprising 39K samples derived from commonsense knowledge graphs (CSKGs), paired with symbolic questions and ground-truth answers, which systematically assesses LLMs’ knowledge memorization, comprehension, and application and examines the consistency between these tasks. To enhance our evaluation, we also propose novel metrics and prompting strategies. Experimental results on multiple LLMs reveal that CoCo presents significant challenges, and our detailed analysis provides deeper insights into the strengths and limitations of LLMs’ commonsense abilities.</abstract>
      <url hash="3a69d129">2025.findings-acl.834</url>
      <bibkey>li-etal-2025-consistency</bibkey>
    </paper>
    <paper id="835">
      <title>Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models</title>
      <author><first>Ahmed</first><last>Elshabrawy</last></author>
      <author><first>Thanh-Nhi</first><last>Nguyen</last></author>
      <author><first>Yeeun</first><last>Kang</last></author>
      <author><first>Lihan</first><last>Feng</last><affiliation>New York University Shanghai</affiliation></author>
      <author><first>Annant</first><last>Jain</last></author>
      <author><first>Faadil Abdullah</first><last>Shaikh</last></author>
      <author><first>Jonibek</first><last>Mansurov</last></author>
      <author><first>Mohamed Fazli Mohamed</first><last>Imam</last></author>
      <author><first>Jesus-German</first><last>Ortiz-Barajas</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology</affiliation></author>
      <author><first>Rendi</first><last>Chevi</last></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>16226-16248</pages>
      <abstract>Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but achieving similar performance with encoder-only models like BERT and RoBERTa has been challenging due to their architecture. However, encoders offer advantages such as lower computational and memory costs. Recent work adapts them for zero-shot generalization using Statement Tuning, which reformulates tasks into finite templates. We extend this approach to multilingual NLP, exploring whether encoders can achieve zero-shot cross-lingual generalization and serve as efficient alternatives to memory-intensive LLMs for low-resource languages. Our results show that state-of-the-art encoder models generalize well across languages, rivaling multilingual LLMs while being more efficient. We also analyze multilingual Statement Tuning dataset design, efficiency gains, and language-specific generalization, contributing to more inclusive and resource-efficient NLP models. We release our code and models.</abstract>
      <url hash="9338f349">2025.findings-acl.835</url>
      <bibkey>elshabrawy-etal-2025-statement</bibkey>
    </paper>
    <paper id="836">
      <title>Evaluating Large Language Models for Confidence-based Check Set Selection</title>
      <author><first>Jane Arleth</first><last>dela Cruz</last></author>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <author><first>Martha</first><last>Larson</last></author>
      <pages>16249-16265</pages>
      <abstract>Large Language Models (LLMs) have shown promise in automating high-labor data tasks, but the adoption of LLMs in high-stake scenarios faces two key challenges: their tendency to answer despite uncertainty and their difficulty handling long input contexts robustly.We investigate commonly used off-the-shelf LLMs’ ability to identify low-confidence outputs for human review through “check set selection”–a process where LLMs prioritize information needing human judgment.Using a case study on social media monitoring for disaster risk management,we define the “check set” as a list of tweets escalated to the disaster manager when the LLM has the least confidence, enabling human oversight within budgeted effort.We test two strategies for LLM check set selection: *individual confidence elicitation* – LLMs assesses confidence for each tweet classification individually, requiring more prompts with shorter contexts, and *direct set confidence elicitation* – LLM evaluates confidence for a list of tweet classifications at once, using less prompts but longer contexts.Our results reveal that set selection via individual probabilities is more reliable but that direct set confidence merits further investigation.Direct set selection challenges include inconsistent outputs, incorrect check set size, and low inter-annotator agreement. Despite these challenges, our approach improves collaborative disaster tweet classification by outperforming random-sample check set selection, demonstrating the potential of human-LLM collaboration.</abstract>
      <url hash="c1235c59">2025.findings-acl.836</url>
      <bibkey>cruz-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="837">
      <title>Training Multi-Modal <fixed-case>LLM</fixed-case>s through Dialogue Planning for <fixed-case>HRI</fixed-case></title>
      <author><first>Claudiu Daniel</first><last>Hromei</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Federico</first><last>Borazio</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Andrea</first><last>Sensi</last></author>
      <author><first>Elisa</first><last>Passone</last></author>
      <author><first>Danilo</first><last>Croce</last></author>
      <author><first>Roberto</first><last>Basili</last><affiliation>University of Roma, Tor Vergata</affiliation></author>
      <pages>16266-16284</pages>
      <abstract>Grounded natural language understanding in Human-Robot Interaction (HRI) requires integrating linguistic, visual, and world knowledge to ensure effective task execution. We propose an approach that enhances Multi-Modal Large Language Models (MLLMs) with a novel explicit dialogue planning phase, allowing robotic agents to systematically refine their understanding of ambiguous commands through structured clarification steps. This reduces hallucinations and improves task feasibility.To evaluate this approach, we introduce a novel dataset of over 1,100 annotated dialogues in English and Italian, designed for fine-tuning and assessing Multi-Modal models in HRI scenarios. Experimental results show that dialogue planning improves response accuracy and quality, and contributes to cross-lingual generalisation, enabling models trained in one language to transfer effectively to another. To the best of our knowledge, this is the first application of structured, goal-driven, and explicit dialogue planning in Multi-Modal LLMs for grounded interaction.</abstract>
      <url hash="6466b54e">2025.findings-acl.837</url>
      <bibkey>hromei-etal-2025-training</bibkey>
    </paper>
    <paper id="838">
      <title><fixed-case>MVL</fixed-case>-<fixed-case>SIB</fixed-case>: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching</title>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>16285-16312</pages>
      <abstract>Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages – over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N’Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.</abstract>
      <url hash="993e58ba">2025.findings-acl.838</url>
      <bibkey>schmidt-etal-2025-mvl</bibkey>
    </paper>
    <paper id="839">
      <title>The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents</title>
      <author><first>Yihong</first><last>Tang</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Jie</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>16313-16337</pages>
      <abstract>Large Language Models (LLMs) have made remarkable advances in role-playing dialogue agents, demonstrating their utility in character simulations. However, it remains challenging for these agents to balance character portrayal utility with content safety because this essential character simulation often comes with the risk of generating unsafe content. To address this issue, we first conduct a systematic exploration of the safety-utility trade-off across multiple LLMs. Our analysis reveals that risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off. Building on this, we propose a novel Adaptive Dynamic Multi-Preference (ADMP) method, which dynamically adjusts safety-utility preferences based on the degree of risk coupling and guides the model to generate responses biased toward utility or safety. We further introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model’s ability to handle high-risk scenarios. Experimental results demonstrate that our approach improves safety metrics while maintaining utility.</abstract>
      <url hash="8b4d54a2">2025.findings-acl.839</url>
      <bibkey>tang-etal-2025-rise</bibkey>
    </paper>
    <paper id="840">
      <title><fixed-case>S</fixed-case>yn<fixed-case>G</fixed-case>raph: A Dynamic Graph-<fixed-case>LLM</fixed-case> Synthesis Framework for Sparse Streaming User Sentiment Modeling</title>
      <author><first>Xin</first><last>Zhang</last></author>
      <author><first>Qiyu</first><last>Wei</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Yingjie</first><last>Zhu</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <pages>16338-16356</pages>
      <abstract>User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.</abstract>
      <url hash="c07228ba">2025.findings-acl.840</url>
      <bibkey>zhang-etal-2025-syngraph</bibkey>
    </paper>
    <paper id="841">
      <title>Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists</title>
      <author><first>Yue</first><last>Cui</last></author>
      <author><first>Liuyi</first><last>Yao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shuchang</first><last>Tao</last></author>
      <author><first>Weijie</first><last>Shi</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yaliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bolin</first><last>Ding</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaofang</first><last>Zhou</last><affiliation>The Hong Kong University of Science and Technology and Hong Kong University of Science and Technology</affiliation></author>
      <pages>16357-16375</pages>
      <abstract>Large language models (LLMs) have significantly advanced natural language processing, particularly through the integration of external tools and APIs. However, their effectiveness is frequently hampered by parameter mis-filling during tool calling. In this paper, we propose the Hierarchical Tool Error Checklist (HiTEC) framework to systematically diagnose and mitigate tool-calling errors without relying on extensive real-world interactions. HiTEC introduces a two-tiered approach: a global error checklist that identifies common, cross-tool issues, and a local error checklist that targets tool-specific and contextual failures. Building on this structure, we propose two deployments: HiTEC-In Context Learning (HiTEC-ICL) and HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global checklist in the initial prompts and leverages a two-round conversational interaction to dynamically refine parameter handling, while HiTEC-KTO generates high-quality negative examples to drive fine-tuning via preference-based optimization. Extensive experiments across five public datasets demonstrate that our framework significantly improves parameter-filling accuracy and tool-calling success rates compared to baseline methods.</abstract>
      <url hash="796a6772">2025.findings-acl.841</url>
      <bibkey>cui-etal-2025-enhancing-tool</bibkey>
    </paper>
    <paper id="842">
      <title>A Large and Balanced Corpus for Fine-grained <fixed-case>A</fixed-case>rabic Readability Assessment</title>
      <author><first>Khalid</first><last>Elmadani</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Hanada</first><last>Taha</last></author>
      <pages>16376-16400</pages>
      <abstract>This paper introduces the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale, fine-grained dataset for Arabic readability assessment. BAREC consists of 69,441 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.8%, reflecting a high level of substantial agreement.Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods.To support research and education, we make BAREC openly available, along with detailed annotation guidelines and benchmark results: http://barec.camel-lab.com.</abstract>
      <url hash="d6b579f0">2025.findings-acl.842</url>
      <bibkey>elmadani-etal-2025-large</bibkey>
    </paper>
    <paper id="843">
      <title>Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</title>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Haozhe</first><last>Wang</last><affiliation>INF</affiliation></author>
      <author><first>Yinda</first><last>Chen</last></author>
      <author><first>Talha</first><last>Qaiser</last></author>
      <author><first>Chen</first><last>Jin</last><affiliation>Astrazeneca</affiliation></author>
      <author><first>Nikolay</first><last>Burlutskiy</last><affiliation>AstraZeneca</affiliation></author>
      <author><first>Fariba</first><last>Yousefi</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Rossella</first><last>Arcucci</last><affiliation>Imperial College London</affiliation></author>
      <pages>16401-16421</pages>
      <abstract>Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: Can MedVLP succeed using purely synthetic data? To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective.Our results show that MedVLP models trained exclusively on synthetic data outperform those trained on real data by 3.8% in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of 9.07%. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks.Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions[^1].[^1]: All data and code will be released upon acceptance.</abstract>
      <url hash="56cef6a4">2025.findings-acl.843</url>
      <bibkey>liu-etal-2025-medical</bibkey>
    </paper>
    <paper id="844">
      <title>See the World, Discover Knowledge: A <fixed-case>C</fixed-case>hinese Factuality Evaluation for Large Vision Language Models</title>
      <author><first>Jihao</first><last>Gu</last></author>
      <author><first>Yingyao</first><last>Wang</last></author>
      <author><first>Pi</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Wang</last></author>
      <author><first>Ziming</first><last>Wang</last></author>
      <author><first>Tengtao</first><last>Song</last></author>
      <author><first>Donglai</first><last>Wei</last></author>
      <author><first>Jiale</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Meng</first><last>Cao</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jun</first><last>Song</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yingshui</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>taobao</affiliation></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaoyong</first><last>Zhu</last><affiliation>Alibaba Group and Microsoft</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>16422-16447</pages>
      <abstract>The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models’ knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named <b>ChineseSimpleVQA</b>, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the <b>Chinese</b> language, <b>diverse</b> knowledge types, a <b>multi-hop</b> question construction, <b>high-quality</b> data, <b>static</b> consistency, and <b>easy-to-evaluate</b> through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field.</abstract>
      <url hash="0ac63d9e">2025.findings-acl.844</url>
      <bibkey>gu-etal-2025-see</bibkey>
    </paper>
    <paper id="845">
      <title>Argus: Benchmarking and Enhancing Vision-Language Models for 3<fixed-case>D</fixed-case> Radiology Report Generation</title>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Yuqi</first><last>Wang</last></author>
      <author><first>Hui</first><last>Shen</last></author>
      <author><first>Haozhe</first><last>Wang</last><affiliation>INF</affiliation></author>
      <author><first>Kangyu</first><last>Zheng</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Mi</first><last>Zhang</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Rossella</first><last>Arcucci</last><affiliation>Imperial College London</affiliation></author>
      <pages>16448-16460</pages>
      <abstract>Automatic radiology report generation holds significant potential to streamline the labor-intensive process of report writing by radiologists, particularly for 3D radiographs such as CT scans. While CT scans are critical for clinical diagnostics, they remain less explored compared to 2D radiographs. To date, there has been no comprehensive benchmark for 3D radiograph report generation (3DRRG), nor sufficient investigation into the optimal training strategies for Vision Language Models (VLMs) in this context, particularly with respect to vision encoder choices, visual token compression, and model scaling.In this work, we make two three contributions. We curate CT-3DRRG, the largest publicly available 3D CT-report dataset, establishing a robust and diverse benchmark for evaluating VLM performance on 3DRRG. Furthermore, we propose a comprehensive training recipe for building high-performing VLMs for 3DRRG, exploring key factors such as vision encoder pretraining strategies, visual token compression, and the impact of data &amp; model scale. Guided by these findings, we introduce Argus, a state-of-the-art family of VLMs that achieve superior performance across different model sizes and input 3D medical image resolutions, efficiently processing high-resolution 3D images up to 512 × 512 × 256.</abstract>
      <url hash="125a79ef">2025.findings-acl.845</url>
      <bibkey>liu-etal-2025-argus</bibkey>
    </paper>
    <paper id="846">
      <title>Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering</title>
      <author><first>Binquan</first><last>Ji</last></author>
      <author><first>Haibo</first><last>Luo</last></author>
      <author><first>YifeiLu</first><last>YifeiLu</last></author>
      <author><first>Lei</first><last>Hei</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jiaqi</first><last>Wang</last></author>
      <author><first>Tingjing</first><last>Liao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Wang</first><last>Lingyu</last></author>
      <author><first>Shichao</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Feiliang</first><last>Ren</last></author>
      <pages>16461-16479</pages>
      <abstract>Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges—such as hallucinations and semantic drift—for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.</abstract>
      <url hash="a2fc4e30">2025.findings-acl.846</url>
      <bibkey>ji-etal-2025-resource</bibkey>
    </paper>
    <paper id="847">
      <title>Evaluating <fixed-case>LLM</fixed-case>s’ Assessment of Mixed-Context Hallucination Through the Lens of Summarization</title>
      <author><first>Siya</first><last>Qi</last></author>
      <author><first>Rui</first><last>Cao</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>University of Sheffield</affiliation></author>
      <pages>16480-16503</pages>
      <abstract>With the rapid development of large language models (LLMs), LLM-as-a-judge has emerged as a widely adopted approach for text quality evaluation, including hallucination evaluation. While previous studies have focused exclusively on single-context evaluation (e.g., discourse faithfulness or world factuality), real-world hallucinations typically involve mixed contexts, which remains inadequately evaluated. In this study, we use summarization as a representative task to comprehensively evaluate LLMs’ capability in detecting mixed-context hallucinations, specifically distinguishing between factual and non-factual hallucinations. Through extensive experiments across direct generation and retrieval-based models of varying scales, our main observations are: (1) LLMs’ intrinsic knowledge introduces inherent biases in hallucination evaluation; (2) These biases particularly impact the detection of factual hallucinations, yielding a significant performance bottleneck; and (3) the fundamental challenge lies in effective knowledge utilization, balancing between LLMs’ intrinsic knowledge and external context for accurate mixed-context hallucination evaluation.</abstract>
      <url hash="2ff4de8a">2025.findings-acl.847</url>
      <bibkey>qi-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="848">
      <title><fixed-case>TUBA</fixed-case>: Cross-Lingual Transferability of Backdoor Attacks in <fixed-case>LLM</fixed-case>s with Instruction Tuning</title>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <author><first>Benjamin I. P.</first><last>Rubinstein</last><affiliation>The University of Melbourne and The University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>Google and The University of Melbourne</affiliation></author>
      <pages>16504-16544</pages>
      <abstract>The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined — such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instructiontuning data for one or two languages can affect the outputs for languages whose instructiontuning data were not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like BLOOM and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English/Chinese data, such as Llama2, Llama3, Qwen2.5, and Gemma. Moreover, our experiments demonstrate 1) High Transferability: the backdoor mechanism operates successfully in cross lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) Robustness: the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer.</abstract>
      <url hash="4d6d2916">2025.findings-acl.848</url>
      <bibkey>he-etal-2025-tuba</bibkey>
    </paper>
    <paper id="849">
      <title>Eliciting Textual Descriptions from Representations of Continuous Prompts</title>
      <author><first>Daniela</first><last>Gottesman</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Tel Aviv University and Google Research</affiliation></author>
      <author><first>Dana</first><last>Ramati</last></author>
      <pages>16545-16562</pages>
      <abstract>Continuous prompts, or “soft prompts”, are a widely-adopted parameter-efficient tuning strategy for large language models, but are often less favorable due to their opaque nature. Prior attempts to interpret continuous prompts relied on projecting individual prompt tokens onto the vocabulary space. However, this approach is problematic as performant prompts can yield arbitrary or contradictory text, and it individually interprets each prompt token. In this work, we propose a new approach to interpret continuous prompts that elicits textual descriptions from their representations during model inference. Using a Patchscopes variant (Ghandeharioun et al., 2024) called InSPEcT over various tasks, we show our method often yields accurate task descriptions which become more faithful as task performance increases. Moreover, an elaborated version of InSPEcT reveals biased features in continuous prompts, whose presence correlates with biased model predictions. Providing an effective interpretability solution, InSPEcT can be leveraged to debug unwanted properties in continuous prompts and inform developers on ways to mitigate them.</abstract>
      <url hash="70e29074">2025.findings-acl.849</url>
      <bibkey>gottesman-etal-2025-eliciting</bibkey>
    </paper>
    <paper id="850">
      <title>Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization</title>
      <author><first>Yuhan</first><last>Fu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent Hunyuan</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <author><first>Xirong</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <pages>16563-16577</pages>
      <abstract>Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.</abstract>
      <url hash="e66276b3">2025.findings-acl.850</url>
      <bibkey>fu-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="851">
      <title>Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models</title>
      <author><first>Jiangxu</first><last>Wu</last></author>
      <author><first>Cong</first><last>Wang</last></author>
      <author><first>Tianhuang</first><last>Su</last></author>
      <author><first>Lin</first><last>Haozhi</last></author>
      <author><first>JunYang</first><last>JunYang</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Zhangchao</first><last>Zhangchao</last></author>
      <author><first>Binqiang</first><last>Pan</last></author>
      <author><first>SongpanYang</first><last>SongpanYang</last></author>
      <author><first>Mingpeng</first><last>Mingpeng</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Kai</first><last>Shi</last><affiliation>Guangdong OPPO Mobile Telecommunications Corp.,Ltd.</affiliation></author>
      <author><first>Zixian</first><last>Li</last></author>
      <pages>16578-16595</pages>
      <abstract>The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative “Ask-Respond-Review” process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9% on MMLU-Pro and 2% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.</abstract>
      <url hash="8d48ed5f">2025.findings-acl.851</url>
      <bibkey>wu-etal-2025-review</bibkey>
    </paper>
    <paper id="852">
      <title>Why Uncertainty Estimation Methods Fall Short in <fixed-case>RAG</fixed-case>: An Axiomatic Analysis</title>
      <author><first>Heydar</first><last>Soudani</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Faegheh</first><last>Hasibi</last><affiliation>Radboud University</affiliation></author>
      <pages>16596-16616</pages>
      <abstract>Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model’s confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably estimate the correctness of LLM responses in the RAG setting. We propose an axiomatic framework to identify deficiencies in existing UE methods. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM’s prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.</abstract>
      <url hash="712637df">2025.findings-acl.852</url>
      <bibkey>soudani-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="853">
      <title><fixed-case>E</fixed-case>uro<fixed-case>V</fixed-case>erdict: A Multilingual Dataset for Verdict Generation Against Misinformation</title>
      <author><first>Daniel</first><last>Russo</last></author>
      <author><first>Fariba</first><last>Sadeghi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>16617-16634</pages>
      <abstract>Misinformation is a global issue that shapes public discourse, influencing opinions and decision-making across various domains. While automated fact-checking (AFC) has become essential in combating misinformation, most work in multilingual settings has focused on claim verification rather than generating explanatory verdicts (i.e. short texts discussing the veracity of the claim), leaving a gap in AFC resources beyond English.To this end, we introduce EuroVerdict, a multilingual dataset designed for verdict generation, covering eight European languages. Developed in collaboration with professional fact-checkers, the dataset comprises claims, manually written verdicts, and supporting evidence, including fact-checking articles and additional secondary sources. We evaluate EuroVerdict with Llama-3.1-8B-Instruct on verdict generation under different settings, varying the prompt language, input article language, and training approach. Our results show that fine-tuning consistently improves performance, with models fine-tuned on original-language articles achieving the highest scores in both automatic and human evaluations. Using articles in a different language from the claim slightly lowers performance; however, pairing them with language-specific prompts improves results. Zero-shot and Chain-of-Thought setups perform worse, reinforcing the benefits of fine-tuning for multilingual verdict generation.</abstract>
      <url hash="c3b4f9a3">2025.findings-acl.853</url>
      <bibkey>russo-etal-2025-euroverdict</bibkey>
    </paper>
    <paper id="854">
      <title><fixed-case>L</fixed-case>o<fixed-case>FTI</fixed-case>: Localization and Factuality Transfer to <fixed-case>I</fixed-case>ndian Locales</title>
      <author><first>Sona Elza</first><last>Simon</last><affiliation>Amazon and Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Soumen Kumar</first><last>Mondal</last></author>
      <author><first>Abhishek</first><last>Singhania</last><affiliation>Amazon</affiliation></author>
      <author><first>Sayambhu</first><last>Sen</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>16635-16662</pages>
      <abstract>Large language models (LLMs) encode vast amounts of world knowledge acquired via training on large web-scale datasets crawled from the internet. However, the datasets used to train the LLMs typically exhibit a geographical bias towards English-speaking Western countries. This results in LLMs producing biased or hallucinated responses to queries that require answers localized to other geographical regions. In this work, we introduce a new benchmark named LoFTI (Localization and Factuality Transfer to Indian Locales) that can be used to evaluate an LLM’s contextual localization and factual text transfer capabilities. LoFTI consists of factual statements about entities in source and target locations; the source locations are spread across the globe and the target locations are all within India with varying degrees of hyperlocality (country, states, cities). The entities span a wide variety of categories. We use LoFTI to evaluate Mixtral, Llama3.3-70B, GPT-4 and two other Mixtral-based approaches well-suited to the task of localized factual transfer. We demonstrate that LoFTI is a high-quality evaluation benchmark and all the models, including GPT-4, produce skewed results across varying levels of hyperlocality.</abstract>
      <url hash="69f5f977">2025.findings-acl.854</url>
      <bibkey>simon-etal-2025-lofti</bibkey>
    </paper>
    <paper id="855">
      <title>Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents</title>
      <author><first>Jaeyoung</first><last>Choe</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Jihoon</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Woohwan</first><last>Jung</last><affiliation>Hanyang University</affiliation></author>
      <pages>16663-16681</pages>
      <abstract>Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts,and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.</abstract>
      <url hash="b6ab8bdf">2025.findings-acl.855</url>
      <bibkey>choe-etal-2025-hierarchical</bibkey>
    </paper>
    <paper id="856">
      <title><fixed-case>GNN</fixed-case>-<fixed-case>RAG</fixed-case>: Graph Neural Retrieval for Efficient Large Language Model Reasoning on Knowledge Graphs</title>
      <author><first>Costas</first><last>Mavromatis</last><affiliation>Amazon</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>16682-16699</pages>
      <abstract>Retrieval-augmented generation (RAG) in Knowledge Graph Question Answering (KGQA) enhances the context of Large Language Models (LLMs) by incorporating information retrieved from the Knowledge Graph (KG). Most recent approaches rely on costly LLM calls to generate executable relation paths or traverse the KG, which is inefficient in complex KGQA tasks, such as those involving multi-hop or multi-entity questions. We introduce the GNN-RAG framework, which utilizes lightweight Graph Neural Networks (GNNs) for effective and efficient graph retrieval. The GNN learns to assign importance weights to nodes based on their relevance to the question, as well as the relevance of their neighboring nodes. This enables the framework to effectively handle context from deeper parts of the graph, improving retrieval performance. GNN-RAG retrieves the shortest paths connecting question entities to GNN answer candidates, providing this information as context for the LLM. Experimental results show that GNN-RAG achieves effective retrieval on two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. Additionally, GNN-RAG excels on multi-hop and multi-entity questions outperforming LLM-based retrieval approaches by 8.9–15.5% points at answer F1. Furthermore, it surpasses long-context inference while using <tex-math>9\times</tex-math> fewer KG tokens. The code is provided in <url>https://github.com/cmavro/GNN-RAG</url>.</abstract>
      <url hash="312b5cba">2025.findings-acl.856</url>
      <bibkey>mavromatis-karypis-2025-gnn</bibkey>
    </paper>
    <paper id="857">
      <title><fixed-case>ASTRID</fixed-case> - An Automated and Scalable <fixed-case>TRI</fixed-case>a<fixed-case>D</fixed-case> for the Evaluation of <fixed-case>RAG</fixed-case>-based Clinical Question Answering Systems</title>
      <author><first>Yajie Vera</first><last>He</last></author>
      <author><first>Mohita</first><last>Chowdhury</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Jared</first><last>Joselowitz</last><affiliation>Ufonia</affiliation></author>
      <author><first>Aisling</first><last>Higham</last></author>
      <author><first>Ernest</first><last>Lim</last></author>
      <pages>16700-16716</pages>
      <abstract>Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model’s response to the knowledge base without penalising conversational elements. Additionally, our metric RA captures the refusal to address questions outside of the system’s scope of practice. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, and clinical and non-clinical out-of-domain scenarios. We demonstrate that CF predicts human ratings of faithfulness more accurately than existing definitions in conversational settings. Furthermore, using eight different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. Finally, we show that evaluation using our triad of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.</abstract>
      <url hash="a843736e">2025.findings-acl.857</url>
      <bibkey>he-etal-2025-astrid</bibkey>
    </paper>
    <paper id="858">
      <title>On Entity Identification in Language Models</title>
      <author><first>Masaki</first><last>Sakata</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Sho</first><last>Yokoi</last><affiliation>NINJAL, Tohoku University and RIKEN</affiliation></author>
      <author><first>Takumi</first><last>Ito</last><affiliation>Langsmith Inc., Tohoku University and Machine Learning Solutions</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>16717-16741</pages>
      <abstract>We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions.We first formulate two problems of entity mentions — ambiguity and variability — and propose a framework analogous to clustering quality metrics. Specifically, we quantify through cluster analysis of LM internal representations the extent to which mentions of the same entity cluster together and mentions of different entities remain separated.Our experiments examine five Transformer-based autoregressive models, showing that they effectively identify and distinguish entities with metrics analogous to precision and recall ranging from 0.66 to 0.9.Further analysis reveals that entity-related information is compactly represented in a low-dimensional linear subspace at early LM layers.Additionally, we clarify how the characteristics of entity representations influence word prediction performance.These findings are interpreted through the lens of isomorphism between LM representations and entity-centric knowledge structures in the real world, providing insights into how LMs internally organize and use entity information.</abstract>
      <url hash="009ec499">2025.findings-acl.858</url>
      <bibkey>sakata-etal-2025-entity</bibkey>
    </paper>
    <paper id="859">
      <title><fixed-case>RAPID</fixed-case>: Efficient Retrieval-Augmented Long Text Generation with Writing Planning and Information Discovery</title>
      <author><first>Hongchao</first><last>Gu</last></author>
      <author><first>Dexun</first><last>Li</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Kuicai</first><last>Dong</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hang</first><last>Lv</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>16742-16763</pages>
      <abstract>Generating knowledge-intensive and comprehensive long texts, such as encyclopedia articles, remains significant challenges for Large Language Models. It requires not only the precise integration of facts but also the maintenance of thematic coherence throughout the article. Existing methods, such as multi-agent discussion, often struggle with issues like hallucinations, topic incoherence, and significant latency. To address these challenges, we propose RAPID, an efficient **R**etrieval-**A**ugmented long text generation framework with writing **P**lanning and **I**nformation **D**iscovery. RAPID consists of three main modules: (1) Retrieval-augmented preliminary outline generation to reduce hallucinations, (2) Attribute-constrained search for efficient information discovery, (3) Plan-guided article generation for enhanced coherence. Extensive experiments on our newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID significantly outperforms state-of-the-art methods across a wide range of evaluation metrics (long-text generation, outline quality, latency, etc). Our work provides a robust and efficient solution to the challenges of automated long-text generation.</abstract>
      <url hash="9b4407db">2025.findings-acl.859</url>
      <bibkey>gu-etal-2025-rapid</bibkey>
    </paper>
    <paper id="860">
      <title><fixed-case>CHARPEVAL</fixed-case>: Benchmarking Large Language Models’ Contextual Reasoning in Knowledge-Grounded Dialogue</title>
      <author><first>Abbas</first><last>Ghaddar</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>David</first><last>Alfonso-Hermelo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Philippe</first><last>Langlais</last><affiliation>Université de Montréal</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Prasanna</first><last>Parthasarathi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>16764-16775</pages>
      <abstract>This paper presents CHARPEVAL, a challenging benchmark specifically designed to evaluate the ability of Large Language Models (LLMs) to perform contextualized reasoning in knowledge-grounded dialogue scenarios. The task involves selecting the correct response from 6 options, including 5 manually crafted distractors, given the conversation history. Extensive benchmarking experiments with a diverse set of state-of-the-art open-weight LLMs show poor performance on CHARPEVAL due to their inability to effectively reason over discontinuous chunks of text across the input. Our analysis reveals systematic error patterns across models with different properties, highlighting the need to improve LLMs beyond simply scaling-up data and compute. CHARPEVAL is publicly available at https://huggingface.co/datasets/huawei-noah/CHARP.</abstract>
      <url hash="f1761992">2025.findings-acl.860</url>
      <bibkey>ghaddar-etal-2025-charpeval</bibkey>
    </paper>
    <paper id="861">
      <title>Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</title>
      <author><first>Mohammad Mahdi</first><last>Abootorabi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Amirhosein</first><last>Zobeiri</last></author>
      <author><first>Mahdi</first><last>Dehghani</last><affiliation>Khajeh Nasir Toosi University of Technology</affiliation></author>
      <author><first>Mohammadali</first><last>Mohammadkhani</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Bardia</first><last>Mohammadi</last><affiliation>Max Planck Institute for Software Systems</affiliation></author>
      <author><first>Omid</first><last>Ghahroodi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Mahdieh Soleymani</first><last>Baghshah</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>16776-16809</pages>
      <abstract>Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.</abstract>
      <url hash="c3b5574e">2025.findings-acl.861</url>
      <bibkey>abootorabi-etal-2025-ask</bibkey>
    </paper>
    <paper id="862">
      <title><fixed-case>D</fixed-case>ebate4<fixed-case>MATH</fixed-case>: Multi-Agent Debate for Fine-Grained Reasoning in Math</title>
      <author><first>Shaowei</first><last>Zhang</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>16810-16824</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive performance in reasoning. However, existing data annotation methods usually suffer from high annotation cost and the lack of effective automatic validation. To address these issues, we propose a Fine-grained Multi-Agent Debate framework (FMAD) and MMATH-Data, a dataset created by FMAD, which consists of 46K reasoning steps. By prompting multiple agents to debate, FMAD assesses the contribution of each reasoning step to the final solution, with labels based on the judge’s confidence score and the winner’s position. To facilitate reasoning in math and examine FMAD and MMATH-Data, we further propose two key components: a Multi-Agent Debate Reward Model (MRM) trained on MMATH-Data, which serves as a reward model to provide robust feedback during the optimization process, and MMATH-LLM, a model designed specifically for mathematical reasoning. MMATH-LLM is fine-tuned using reinforcement learning with supervised feedback from MRM, aiming at improving its mathematical reasoning capabilities. Extensive experiments demonstrate that our model achieves 83.4% accuracy on the GSM8K dataset and 45.1% on the MATH dataset, outperforming the state-of-the-art methods by 1.2% and 3.5%, respectively. All data and code will be available soon at GitHub.</abstract>
      <url hash="879126f0">2025.findings-acl.862</url>
      <bibkey>zhang-xiong-2025-debate4math</bibkey>
    </paper>
    <paper id="863">
      <title>Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing</title>
      <author><first>Irina</first><last>Saparina</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>16825-16839</pages>
      <abstract>Handling ambiguity and underspecification is an important challenge in natural language interfaces, particularly for tasks like text-to-SQL semantic parsing. We propose a modular approach that resolves ambiguity using natural language interpretations before mapping these to logical forms (e.g., SQL queries). Although LLMs excel at parsing unambiguous utterances, they show strong biases for ambiguous ones, typically predicting only preferred interpretations. We constructively exploit this bias to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations. To train the infilling model, we introduce an annotation method that uses SQL execution to validate different meanings. Our approach improves interpretation coverage and generalizes across datasets with different annotation styles, database structures, and ambiguity types.</abstract>
      <url hash="ba1b5bce">2025.findings-acl.863</url>
      <bibkey>saparina-lapata-2025-disambiguate</bibkey>
    </paper>
    <paper id="864">
      <title>The Anatomy of Evidence: An Investigation Into Explainable <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Katharina</first><last>Beckh</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Elisa</first><last>Studeny</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS and Hochschule Bonn-Rhein-Sieg</affiliation></author>
      <author><first>Sujan Sai</first><last>Gannamaneni</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Dario</first><last>Antweiler</last></author>
      <author><first>Stefan</first><last>Rueping</last></author>
      <pages>16840-16851</pages>
      <abstract>Automatic medical coding has the potential to ease documentation and billing processes. For this task, transparency plays an important role for medical coders and regulatory bodies, which can be achieved using explainability methods. However, the evaluation of these approaches has been mostly limited to short text and binary settings due to a scarcity of annotated data. Recent efforts by Cheng et al. (2023) have introduced the MDACE dataset, which provides a valuable resource containing code evidence in clinical records. In this work, we conduct an in-depth analysis of the MDACE dataset and perform plausibility evaluation of current explainable medical coding systems from an applied perspective. With this, we contribute to a deeper understanding of automatic medical coding and evidence extraction. Our findings reveal that ground truth evidence aligns with code descriptions to a certain degree. An investigation into state-of-the-art approaches shows a high overlap with ground truth evidence. We propose match measures and highlight success and failure cases. Based on our findings, we provide recommendations for developing and evaluating explainable medical coding systems.</abstract>
      <url hash="f444c0a2">2025.findings-acl.864</url>
      <bibkey>beckh-etal-2025-anatomy</bibkey>
    </paper>
    <paper id="865">
      <title><fixed-case>AVG</fixed-case>-<fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>: An Efficient Large Multimodal Model with Adaptive Visual Granularity</title>
      <author><first>Zhibin</first><last>Lan</last></author>
      <author><first>Liqiang</first><last>Niu</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Wenbo</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>16852-16869</pages>
      <abstract>Recently, large multimodal models (LMMs) have achieved significant advancements. When dealing with high-resolution images, dominant LMMs typically divide them into multiple local images and a global image, leading to a large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction. Specifically, we first apply the multiple pooling layers to obtain visual tokens at different granularities. Then we propose a visual granularity router, which includes a Transformer layer, an MLP layer, and a voter layer, used to select the appropriate visual granularity based on the image and instruction. Furthermore, we put forward RGLF, a novel training paradigm that aims at aligning the granularity predicted by the router with the preferences of the LMM, without the need for additional manually annotated data. Extensive experiments and analysis show that AVG-LLaVA achieves superior performance across 11 benchmarks, as well as significantly reduces the number of visual tokens and speeds up inference (e.g., an 85.3% reduction in visual tokens and a 2.53<tex-math>\times</tex-math> increase in inference speed on the AI2D benchmark).</abstract>
      <url hash="7be59056">2025.findings-acl.865</url>
      <bibkey>lan-etal-2025-avg</bibkey>
    </paper>
    <paper id="866">
      <title>Word Form Matters: <fixed-case>LLM</fixed-case>s’ Semantic Reconstruction under Typoglycemia</title>
      <author><first>Chenxi</first><last>Wang</last></author>
      <author><first>Tianle</first><last>Gu</last></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lang</first><last>Gao</last></author>
      <author><first>Zirui</first><last>Song</last></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>16870-16885</pages>
      <abstract>Human readers can efficiently comprehend scrambled words, a phenomenon known as <tex-math>\textit{Typoglycemia}</tex-math>, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose <tex-math>\textit{SemRecScore}</tex-math>, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs’ semantic reconstruction ability, identifying <tex-math>\textit{word form as the core factor}</tex-math> in this process. Furthermore, we analyze <tex-math>\textit{how}</tex-math> LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs’ fixed attention patterns primarily focused on word form and human readers’ adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms. Code is available on: https://github.com/Aurora-cx/TypoLLM.</abstract>
      <url hash="49bd73b3">2025.findings-acl.866</url>
      <bibkey>wang-etal-2025-word</bibkey>
    </paper>
    <paper id="867">
      <title><fixed-case>LLM</fixed-case>-based Translation Inference with Iterative Bilingual Understanding</title>
      <author><first>Andong</first><last>Chen</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16886-16902</pages>
      <abstract>The remarkable understanding and generation capabilities of large language models (LLMs) have greatly improved translation performance. However, incorrect understanding of the sentence to be translated can degrade translation quality. To address this issue, we proposed a novel Iterative Bilingual Understanding Translation (IBUT) method based on the cross-lingual capabilities of LLMs and the dual characteristics of translation tasks. The cross-lingual capability of LLMs enables the generation of contextual understanding for both the source and target languages separately. Furthermore, the dual characteristics allow IBUT to generate effective cross-lingual feedback, iteratively refining contextual understanding, thereby reducing errors and improving translation performance. Experimental results showed that the proposed IBUT outperforms several strong comparison methods, especially being generalized to multiple domains (e.g., news, commonsense, and cultural translation benchmarks).</abstract>
      <url hash="55ca3855">2025.findings-acl.867</url>
      <bibkey>chen-etal-2025-llm-based</bibkey>
    </paper>
    <paper id="868">
      <title>Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach</title>
      <author><first>Yurong</first><last>Wu</last></author>
      <author><first>Fangwen</first><last>Mu</last></author>
      <author><first>Qiuhong</first><last>Zhang</last></author>
      <author><first>Jinjing</first><last>Zhao</last><affiliation>University of Sydney, Microsoft Research Asia and Microsoft Research Asia</affiliation></author>
      <author><first>Xinrun</first><last>Xu</last></author>
      <author><first>Lingrui</first><last>Mei</last><affiliation>Skywork AI</affiliation></author>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Lin</first><last>Shi</last></author>
      <author><first>Junjie</first><last>Wang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiming</first><last>Ding</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <pages>16903-16916</pages>
      <abstract>Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce <tex-math>\textbf{Prism}</tex-math>, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose <tex-math>\textbf{EvoStealer}</tex-math>, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (InternVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer’s stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://whitepagewu.github.io/evostealer-site.</abstract>
      <url hash="60a303ed">2025.findings-acl.868</url>
      <bibkey>wu-etal-2025-vulnerability</bibkey>
    </paper>
    <paper id="869">
      <title>m<fixed-case>S</fixed-case>tyle<fixed-case>D</fixed-case>istance: Multilingual Style Embeddings and their Evaluation</title>
      <author><first>Justin</first><last>Qiu</last></author>
      <author><first>Jiacheng</first><last>Zhu</last></author>
      <author><first>Ajay</first><last>Patel</last></author>
      <author><first>Marianna</first><last>Apidianaki</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <pages>16917-16931</pages>
      <abstract>Style embeddings are useful for stylistic analysis and style transfer, yet they only exist for English. We introduce Multilingual StyleDistance (mStyleDistance), a method that can generate style embeddings in new languages using synthetic data and a contrastive loss. We create style embeddings in nine languages and a multilingual STEL-or-Content benchmark (Wegmann et al., 2022) that serves to assess their quality. We also employ our embeddings in an authorship verification task involving different languages. Our results show that mStyleDistance embeddings outperform existing style embeddings on these benchmarks and generalize well to unseen features and languages. We make our models and datasets publicly available.</abstract>
      <url hash="e473a323">2025.findings-acl.869</url>
      <bibkey>qiu-etal-2025-mstyledistance</bibkey>
    </paper>
    <paper id="870">
      <title><fixed-case>S</fixed-case>eq<fixed-case>MMR</fixed-case>: Sequential Model Merging and <fixed-case>LLM</fixed-case> Routing for Enhanced Batched Sequential Knowledge Editing</title>
      <author><first>Shanbao</first><last>Qiao</last></author>
      <author><first>Xuebing</first><last>Liu</last></author>
      <author><first>Akshat</first><last>Gupta</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Seung-Hoon</first><last>Na</last><affiliation>Ulsan National Institute of Science and Technology</affiliation></author>
      <pages>16932-16947</pages>
      <abstract>Model knowledge editing enables the efficient correction of erroneous information and the continuous updating of outdated knowledge within language models. While existing research has demonstrated strong performance in single-instance or few-instance sequential editing and one-time massive editing scenarios, the batched sequential editing paradigm remains a significant challenge. The primary issue lies in the model’s tendency to gradually forget previously edited knowledge and become increasingly unstable after multiple iterations of batched editing. To address these challenges, we propose **SeqMMR**, an enhanced framework for batched sequential knowledge editing that leverages **Seq**uential **M**odel **M**erging and a model **R**outer. Our approach iteratively merges parameters from current batch-edited models with those of their predecessors, ensuring that newly emerging knowledge is integrated while mitigating the forgetting of previously edited knowledge. Furthermore, the model router directs queries unrelated to the edited knowledge to an unedited model backup, preventing unintended alterations in model predictions. Extensive experiments across various datasets demonstrate that our approach effectively mitigates knowledge forgetting, improves performance across all previous batches, and better preserves the model’s general capabilities.</abstract>
      <url hash="7ea7234d">2025.findings-acl.870</url>
      <bibkey>qiao-etal-2025-seqmmr</bibkey>
    </paper>
    <paper id="871">
      <title><fixed-case>R</fixed-case>eflect<fixed-case>E</fixed-case>vo: Improving Meta Introspection of Small <fixed-case>LLM</fixed-case>s by Learning Self-Reflection</title>
      <author><first>Jiaqi</first><last>Li</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Xinyi</first><last>Dong</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Zhizhuo</first><last>Yang</last></author>
      <author><first>Quansen</first><last>Wang</last><affiliation>Beijing Institute of General Artificial Intelligence</affiliation></author>
      <author><first>Xiaobo</first><last>Wang</last><affiliation>Beijing Institute for General Artificial Intelligence and University of Science and Technology of China</affiliation></author>
      <author><first>Song-Chun</first><last>Zhu</last></author>
      <author><first>Zixia</first><last>Jia</last></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <pages>16948-16966</pages>
      <abstract>We present a novel pipeline, ReflectEvo, to demonstrate that small language models (SLMs) can enhance meta introspection through reflection learning. This process iteratively generates self-reflection for self-training, fostering a continuous and self-evolving process. Leveraging this pipeline, we construct ReflectEvo-460k, a large-scale, comprehensive, self-generated reflection dataset with broadened instructions and diverse multi-domain tasks. Building upon this dataset, we demonstrate the effectiveness of reflection learning to improve SLMs’ reasoning abilities using SFT and DPO with remarkable performance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral from 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the reasoning capability of the three prominent open-sourced models on BIG-bench without distillation from superior models or fine-grained human annotation. We further conduct a deeper analysis of the high quality of self-generated reflections and their impact on error localization and correction. Our work highlights the potential of continuously enhancing the reasoning performance of SLMs through iterative reflection learning in the long run.</abstract>
      <url hash="65987050">2025.findings-acl.871</url>
      <bibkey>li-etal-2025-reflectevo</bibkey>
    </paper>
    <paper id="872">
      <title><fixed-case>MAGIC</fixed-case>-<fixed-case>VQA</fixed-case>: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering</title>
      <author><first>Shuo</first><last>Yang</last></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <author><first>Siwen</first><last>Luo</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <pages>16967-16986</pages>
      <abstract>Visual Question Answering (VQA) necessitates models to reason effectively across visual and textual modalities. However, existing Large Vision-Language Models (LVLMs) often fall short in achieving human-like reasoning due to a lack of integrated commonsense knowledge, limiting their robustness and accuracy in real-world scenarios where both explicit facts and implicit understanding are crucial. To address this challenge, we present MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge, a novel framework designed to enhance multimodal inference by integrating commonsense reasoning. MAGIC-VQA introduces a three-stage process: (1) Explicit Commonsense Knowledge Retrieval from external knowledge graphs, (2) By-Type Commonsense Knowledge Post-Processing to refine contextual relevance, and (3) Implicit Commonsense Knowledge Augmentation using a heterogeneous graph processed by a Graph Neural Network (GNN). These stages collectively enable nuanced, context-aware reasoning without extensive pre-training or intricate prompt tuning.Our MAGIC-VQA significantly improves comprehensive benchmark datasets, surpassing existing models in tasks requiring advanced commonsense reasoning. MAGIC-VQA establishes a robust pathway for integrating commonsense knowledge into VQA, bridging the gap between vision-language inputs and high-level reasoning for improved reliability and contextual accuracy.</abstract>
      <url hash="e9f269d9">2025.findings-acl.872</url>
      <bibkey>yang-etal-2025-magic</bibkey>
    </paper>
    <paper id="873">
      <title>Automatic Transmission for <fixed-case>LLM</fixed-case> Tiers: Optimizing Cost and Accuracy in Large Language Models</title>
      <author><first>Injae</first><last>Na</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Keonwoong</first><last>Noh</last></author>
      <author><first>Woohwan</first><last>Jung</last><affiliation>Hanyang University</affiliation></author>
      <pages>16987-17004</pages>
      <abstract>LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.</abstract>
      <url hash="9f2838cf">2025.findings-acl.873</url>
      <bibkey>na-etal-2025-automatic</bibkey>
    </paper>
    <paper id="874">
      <title>Low-Rank Interconnected Adaptation across Layers</title>
      <author><first>Yibo</first><last>Zhong</last></author>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Yao</first><last>Zhou</last><affiliation>Sichuan University</affiliation></author>
      <pages>17005-17029</pages>
      <abstract>Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning (PEFT) method that learns weight updates <tex-math>\Delta W = AB</tex-math> for pretrained weights <tex-math>W</tex-math> through low-rank adapters <tex-math>A</tex-math> and <tex-math>B</tex-math>. While LoRA ensures hardware efficiency, its low-rank weight updates limit adaptation performance. In this paper, we propose low-rank interconnected adaptation across layers (Lily), a novel PEFT method that introduces an interconnected framework with locally shared <tex-math>A</tex-math> and globally shared <tex-math>B</tex-math> experts. This structure eliminates redundant per-layer <tex-math>AB</tex-math> pairs, enabling higher-rank <tex-math>\Delta W</tex-math> with equal or fewer parameters. To enhance expressiveness, we use data-dependent routers to determine <tex-math>A</tex-math>-<tex-math>B</tex-math> interconnections, preventing <tex-math>B</tex-math> experts from converging to the same behavior and improving representational power across domains. Experiments across modalities, architectures, and model sizes demonstrate Lily’s superior performance and efficiency.</abstract>
      <url hash="64c3a909">2025.findings-acl.874</url>
      <bibkey>zhong-etal-2025-low</bibkey>
    </paper>
    <paper id="875">
      <title><fixed-case>G</fixed-case>a<fixed-case>RAG</fixed-case>e: A Benchmark with Grounding Annotations for <fixed-case>RAG</fixed-case> Evaluation</title>
      <author><first>Ionut Teodor</first><last>Sorodoc</last><affiliation>Amazon</affiliation></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Rexhina</first><last>Blloshmi</last><affiliation>Amazon</affiliation></author>
      <author><first>Christopher</first><last>Davis</last><affiliation>Amazon</affiliation></author>
      <author><first>Adrià</first><last>de Gispert</last><affiliation>Amazon</affiliation></author>
      <pages>17030-17049</pages>
      <abstract>We present GaRAGe, a large RAG benchmark with human-curated long-form answers and annotations of each grounding passage, allowing a fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers. Our benchmark contains 2366 questions of diverse complexity, dynamism, and topics, and includes over 35K annotated passages retrieved from both private document sets and the Web, to reflect real-world RAG use cases. This makes it an ideal test bed to evaluate an LLM’s ability to identify only the relevant information necessary to compose a response, or provide a deflective response when there is insufficient information. Evaluations of multiple state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise rather than (a) ground their answers strictly on the annotated relevant passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b) deflect when no relevant grounding is available (reaching at most 31% true positive rate in deflections). The F<tex-math>_{1}</tex-math> in attribution to relevant sources is at most 58.9%, and we show that performance is particularly reduced when answering time-sensitive questions and when having to draw knowledge from sparser private grounding sources.</abstract>
      <url hash="fa5eab44">2025.findings-acl.875</url>
      <bibkey>sorodoc-etal-2025-garage</bibkey>
    </paper>
    <paper id="876">
      <title>Change Entity-guided Heterogeneous Representation Disentangling for Change Captioning</title>
      <author><first>Yi</first><last>Li</last></author>
      <author><first>Yunbin</first><last>Tu</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Li</first><last>Su</last></author>
      <author><first>Qingming</first><last>Huang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <pages>17050-17060</pages>
      <abstract>Change captioning aims to describe differences between a pair of images using natural language. However, learning effective difference representations is highly challenging due to distractors such as illumination and viewpoint changes. To address this, we propose a change-entity-guided disentanglement network that explicitly learns difference representations while mitigating the impact of distractors. Specifically, we first design a change entity retrieval module to identify key objects involved in the change from a textual perspective. Then, we introduce a difference representation enhancement module that strengthens the learned features, disentangling genuine differences from background variations. To further refine the generation process, we incorporate a gated Transformer decoder, which dynamically integrates both visual difference and textual change-entity information. Extensive experiments on CLEVR-Change, CLEVR-DC and Spot-the-Diff datasets demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance. The code is available at https://github.com/yili-19/CHEER</abstract>
      <url hash="1a5a6699">2025.findings-acl.876</url>
      <bibkey>li-etal-2025-change</bibkey>
    </paper>
    <paper id="877">
      <title><fixed-case>RAG</fixed-case>-<fixed-case>R</fixed-case>eward<fixed-case>B</fixed-case>ench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Hongbang</first><last>Yuan</last></author>
      <author><first>Tianyi</first><last>Men</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Jiexin</first><last>Xu</last></author>
      <author><first>Huaijun</first><last>Li</last></author>
      <author><first>Xiaojian</first><last>Jiang</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>17061-17090</pages>
      <abstract>Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training.</abstract>
      <url hash="d5436158">2025.findings-acl.877</url>
      <bibkey>jin-etal-2025-rag</bibkey>
    </paper>
    <paper id="878">
      <title>Generate, Discriminate, Evolve: Enhancing Context Faithfulness via Fine-Grained Sentence-Level Self-Evolution</title>
      <author><first>Kun</first><last>Li</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Tianhua</first><last>Zhang</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yunxiang</first><last>Li</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Abdalla Mohamed Salama Sayed</first><last>Moustafa</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>James R.</first><last>Glass</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Helen M.</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>17091-17105</pages>
      <abstract>Improving context faithfulness in large language models is essential for developing trustworthy retrieval augmented generation systems and mitigating hallucinations, especially in long-form question answering (LFQA) tasks or scenarios involving knowledge conflicts. Existing methods either intervene LLMs only at inference without addressing their inherent limitations or overlook the potential for self-improvement. In this paper, we introduce GenDiE(Generate, Discriminate, Evolve), a novel self-evolving framework that enhances context faithfulness through fine-grained sentence-level optimization. GenDiE combines both generative and discriminative training, equipping LLMs with self-generation and self-scoring capabilities to facilitate iterative self-evolution. This supports both data construction for model alignment and score-guided search during inference. Furthermore, by treating each sentence in a response as an independent optimization unit, GenDiE effectively addresses the limitations of previous approaches that optimize at the holistic answer level, which may miss unfaithful details. Experiments on ASQA (in-domain LFQA) and ConFiQA (out-of-domain counterfactual QA) datasets demonstrate that GenDiE surpasses various baselines in both faithfulness and correctness, and exhibits robust performance for domain adaptation.</abstract>
      <url hash="204a317d">2025.findings-acl.878</url>
      <bibkey>li-etal-2025-generate</bibkey>
    </paper>
    <paper id="879">
      <title><fixed-case>PAM</fixed-case>: Paraphrase <fixed-case>AMR</fixed-case>-Centric Evaluation Metric</title>
      <author><first>Afonso</first><last>Sousa</last><affiliation>Faculty of Engineering of the University of Porto</affiliation></author>
      <author><first>Henrique</first><last>Lopes Cardoso</last><affiliation>Faculty of Engineering of the University of Porto</affiliation></author>
      <pages>17106-17121</pages>
      <abstract>Paraphrasing is rooted in semantics, which makes evaluating paraphrase generation systems hard. Current paraphrase generators are typically evaluated using borrowed metrics from adjacent text-to-text tasks, like machine translation or text summarization. These metrics tend to have ties to the surface form of the reference text. This is not ideal for paraphrases as we typically want variation in the lexicon while persisting semantics. To address this problem, and inspired by learned similarity evaluation on plain text, we propose PAM, a Paraphrase AMR-Centric Evaluation Metric. This metric uses AMR graphs extracted from the input text, which consist of semantic structures agnostic to the text surface form, making the resulting evaluation metric more robust to variations in syntax or lexicon. Additionally, we evaluated PAM on different semantic textual similarity datasets and found that it improves the correlations with human semantic scores when compared to other AMR-based metrics.</abstract>
      <url hash="feecd4e3">2025.findings-acl.879</url>
      <bibkey>sousa-lopes-cardoso-2025-pam</bibkey>
    </paper>
    <paper id="880">
      <title><fixed-case>VP</fixed-case>-<fixed-case>MEL</fixed-case>: Visual Prompts Guided Multimodal Entity Linking</title>
      <author><first>Hongze</first><last>Mi</last></author>
      <author><first>Jinyuan</first><last>Li</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Zhangxuying</first><last>Zhangxuying</last></author>
      <author><first>Haoran</first><last>Cheng</last></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Di</first><last>Sun</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Gang</first><last>Pan</last></author>
      <pages>17122-17137</pages>
      <abstract>Multimodal entity linking (MEL), a task aimed at linking mentions within multimodal contexts to their corresponding entities in a knowledge base (KB), has attracted much attention due to its wide applications in recent years. However, existing MEL methods often rely on mention words as retrieval cues, which limits their ability to effectively utilize information from both images and text. This reliance causes MEL to struggle with accurately retrieving entities in certain scenarios, especially when the focus is on image objects or mention words are missing from the text. To solve these issues, we introduce a Visual Prompts guided Multimodal Entity Linking (VP-MEL) task. Given a text-image pair, VP-MEL aims to link a marked region (i.e., visual prompt) in an image to its corresponding entities in the knowledge base. To facilitate this task, we present a new dataset, VPWiki, specifically designed for VP-MEL. Furthermore, we propose a framework named IIER, which enhances visual feature extraction using visual prompts and leverages the pre-trained Detective-VLM model to capture latent information. Experimental results on the VPWiki dataset demonstrate that IIER outperforms baseline methods across multiple benchmarks for the VP-MEL task.</abstract>
      <url hash="8fa7a013">2025.findings-acl.880</url>
      <bibkey>mi-etal-2025-vp</bibkey>
    </paper>
    <paper id="881">
      <title><fixed-case>FADE</fixed-case>: Why Bad Descriptions Happen to Good Features</title>
      <author><first>Bruno</first><last>Puri</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Aakriti</first><last>Jain</last><affiliation>Fraunhofer HHI</affiliation></author>
      <author><first>Elena</first><last>Golimblevskaia</last><affiliation>Fraunhofer HHI, Fraunhofer IAIS</affiliation></author>
      <author><first>Patrick</first><last>Kahardipraja</last><affiliation>Fraunhofer HHI</affiliation></author>
      <author><first>Thomas</first><last>Wiegand</last><affiliation>Fraunhofer HHI and Technische Universität Berlin</affiliation></author>
      <author><first>Wojciech</first><last>Samek</last><affiliation>TU Berlin and Fraunhofer HHI</affiliation></author>
      <author><first>Sebastian</first><last>Lapuschkin</last><affiliation>Fraunhofer HHI</affiliation></author>
      <pages>17138-17160</pages>
      <abstract>Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While this may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing **FADE**: Feature Alignment to Description Evaluation, a scalable model-agnostic framework for automatically evaluating feature-to-description alignment. **FADE** evaluates alignment across four key metrics – *Clarity, Responsiveness, Purity, and Faithfulness* – and systematically quantifies the causes of the misalignment between features and their descriptions. We apply **FADE** to analyze existing open-source feature descriptions and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release **FADE** as an open-source package at: [github.com/brunibrun/FADE](https://github.com/brunibrun/FADE).</abstract>
      <url hash="ebd287f8">2025.findings-acl.881</url>
      <bibkey>puri-etal-2025-fade</bibkey>
    </paper>
    <paper id="882">
      <title>In the <fixed-case>LLM</fixed-case> era, Word Sense Induction remains unsolved</title>
      <author><first>Anna</first><last>Mosolova</last></author>
      <author><first>Marie</first><last>Candito</last><affiliation>Université Paris Cité</affiliation></author>
      <author><first>Carlos</first><last>Ramisch</last><affiliation>LIS - Laboratoire d’Informatique et Systèmes and AMU - Aix Marseille University</affiliation></author>
      <pages>17161-17178</pages>
      <abstract>In the absence of sense-annotated data, word sense induction (WSI) is a compelling alternative to word sense disambiguation, particularly in low-resource or domain-specific settings. In this paper, we emphasize methodological problems in current WSI evaluation. We propose an evaluation on a SemCor-derived dataset, respecting the original corpus polysemy and frequency distributions. We assess pre-trained embeddings and clustering algorithms across parts of speech, and propose and evaluate an LLM-based WSI method for English. We evaluate data augmentation sources (LLM-generated, corpus and lexicon), and semi-supervised scenarios using Wiktionary for data augmentation, must-link constraints, number of clusters per lemma.We find that no unsupervised method (whether ours or previous) surpasses the strong “one cluster per lemma” heuristic (1cpl). We also show that (i) results and best systems may vary across POS, (ii) LLMs have troubles performing this task, (iii) data augmentation is beneficial and (iv) capitalizing on Wiktionary does help. It surpasses previous SOTA system on our test set by 3.3%. WSI is not solved, and calls for a better articulation of lexicons and LLMs’ lexical semantics capabilities.</abstract>
      <url hash="557bb2d1">2025.findings-acl.882</url>
      <bibkey>mosolova-etal-2025-llm</bibkey>
    </paper>
    <paper id="883">
      <title>Navigating the Political Compass: Evaluating Multilingual <fixed-case>LLM</fixed-case>s across Languages and Nationalities</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Oana</first><last>Balalau</last><affiliation>INRIA</affiliation></author>
      <author><first>Davide</first><last>Ceolin</last></author>
      <pages>17179-17204</pages>
      <abstract>Large Language Models (LLMs) have become ubiquitous in today’s technological landscape, boasting a plethora of applications, and even endangering human jobs in complex and creative fields. One such field is journalism: LLMs are being used for summarization, generation and even fact-checking. However, in today’s political landscape, LLMs could accentuate tensions if they exhibit political bias. In this work, we evaluate the political bias of the most used 15 multilingual LLMs via the Political Compass Test. We test different scenarios, where we vary the language of the prompt, while also assigning a nationality to the model. We evaluate models on the 50 most populous countries and their official languages. Our results indicate that language has a strong influence on the political ideology displayed by a model. In addition, smaller models tend to display a more stable political ideology, i.e. ideology that is less affected by variations in the prompt.</abstract>
      <url hash="97367346">2025.findings-acl.883</url>
      <bibkey>helwe-etal-2025-navigating</bibkey>
    </paper>
    <paper id="884">
      <title>Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models</title>
      <author><first>Wanqi</first><last>Yang</last></author>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Yunchao</first><last>Wei</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>17205-17220</pages>
      <abstract>Adversarial audio attacks pose a significant threat to the growing use of large audio-language models (LALMs) in voice-based human-machine interactions. While existing research focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the vulnerabilities of LALMs to these audio attacks in conversational scenarios. To evaluate the robustness of LALMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-4o-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LALMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-4o exhibits the highest level of resilience. Our data can be accessed via the following link: CAA.</abstract>
      <url hash="b23e5b7f">2025.findings-acl.884</url>
      <bibkey>yang-etal-2025-withstand</bibkey>
    </paper>
    <paper id="885">
      <title>Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models</title>
      <author><first>Sibo</first><last>Yi</last></author>
      <author><first>Tianshuo</first><last>Cong</last></author>
      <author><first>Xinlei</first><last>He</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qi</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiaxing</first><last>Song</last></author>
      <pages>17221-17234</pages>
      <abstract>Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs). To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful prompts. To address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.</abstract>
      <url hash="f12ca0ce">2025.findings-acl.885</url>
      <bibkey>yi-etal-2025-beyond</bibkey>
    </paper>
    <paper id="886">
      <title><fixed-case>EMR</fixed-case>s2<fixed-case>CSP</fixed-case> : Mining Clinical Status Pathway from Electronic Medical Records</title>
      <author><first>Yifei</first><last>Chen</last></author>
      <author><first>Ruihui</first><last>Hou</last></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>17235-17251</pages>
      <abstract>Many current studies focus on extracting tests or treatments when constructing clinical pathways, often neglecting the patient’s symptoms and diagnosis, leading to incomplete diagnostic and therapeutic logic. Therefore, this paper aims to extract clinical pathways from electronic medical records that encompass complete diagnostic and therapeutic logic, including temporal information, patient symptoms, diagnosis, and tests or treatments. To achieve this objective, we propose a novel clinical pathway representation: the clinical status pathway. We also design a LLM-based pipeline framework for extracting clinical status pathway from electronic medical records, with the core concept being to improve extraction accuracy by modeling the diagnostic and treatment processes. In our experiments, we apply this framework to construct a comprehensive breast cancer-specific clinical status pathway and evaluate its performance on medical question-answering and decision-support tasks, demonstrating significant improvements over traditional clinical pathways. The code is publicly available at https://github.com/finnchen11/EMRs2CSP.</abstract>
      <url hash="ed5c4288">2025.findings-acl.886</url>
      <bibkey>chen-etal-2025-emrs2csp</bibkey>
    </paper>
    <paper id="887">
      <title>A Law Reasoning Benchmark for <fixed-case>LLM</fixed-case> with Tree-Organized Structures including Factum Probandum, Evidence and Experiences</title>
      <author><first>Jiaxin</first><last>Shen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Huiqi</first><last>Hu</last></author>
      <author><first>Luyi</first><last>Lin</last></author>
      <author><first>Guoyang</first><last>Ma</last></author>
      <author><first>Fei</first><last>Zheng</last><affiliation>China University of Political Science and Law</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Wenjuan</first><last>Han</last></author>
      <pages>17252-17274</pages>
      <abstract>While progress has been made in legal applications, law reasoning, crucial for fair adjudication, remains unexplored. We propose a transparent law reasoning schema enriched with hierarchical factum probandum, evidence, and implicit experience, enabling public scrutiny and preventing bias. Inspired by this schema, we introduce the challenging task, which takes a textual case description and outputs a hierarchical structure justifying the final decision. We also create the first crowd-sourced dataset for this task, enabling comprehensive evaluation. Simultaneously, we propose TL agent that employs a comprehensive suite of legal analysis tools to address the challenge task. This benchmark paves the way for transparent and accountable AI-assisted law-reasoning in the “Intelligent Court”.</abstract>
      <url hash="1caa2a2a">2025.findings-acl.887</url>
      <bibkey>shen-etal-2025-law</bibkey>
    </paper>
    <paper id="888">
      <title>Libra: Leveraging Temporal Images for Biomedical Radiology Analysis</title>
      <author><first>Xi</first><last>Zhang</last></author>
      <author><first>Zaiqiao</first><last>Meng</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Jake</first><last>Lever</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Edmond S. L.</first><last>Ho</last><affiliation>University of Glasgow</affiliation></author>
      <pages>17275-17303</pages>
      <abstract>Radiology report generation (RRG) requires advanced medical image analysis, effective temporal reasoning, and accurate text generation. While multimodal large language models (MLLMs) align with pre-trained vision encoders to enhance visual-language understanding, most existing methods rely on single-image analysis or rule-based heuristics to process multiple images, failing to fully leverage temporal information in multi-modal medical datasets. In this paper, we introduce **Libra**, a temporal-aware MLLM tailored for chest X-ray report generation. Libra combines a radiology-specific image encoder with a novel Temporal Alignment Connector (**TAC**), designed to accurately capture and integrate temporal differences between paired current and prior images. Extensive experiments on the MIMIC-CXR dataset demonstrate that Libra establishes a new state-of-the-art benchmark among similarly scaled MLLMs, setting new standards in both clinical relevance and lexical accuracy. All source code and data are publicly available at: https://github.com/X-iZhang/Libra.</abstract>
      <url hash="3756c131">2025.findings-acl.888</url>
      <bibkey>zhang-etal-2025-libra</bibkey>
    </paper>
    <paper id="889">
      <title>Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach</title>
      <author><first>Aditya</first><last>Tomar</last></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Pvt Ltd</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>17304-17317</pages>
      <abstract>Bias and stereotypes in language models can cause harm, especially in sensitive areas like content moderation and decision-making. This paper addresses bias and stereotype detection by exploring how jointly learning these tasks enhances model performance. We introduce StereoBias, a unique dataset labeled for bias and stereotype detection across five categories: religion, gender, socio-economic status, race, profession, and others, enabling a deeper study of their relationship. Our experiments compare encoder-only models and fine-tuned decoder-only models using QLoRA. While encoder-only models perform well, decoder-only models also show competitive results. Crucially, joint training on bias and stereotype detection significantly improves bias detection compared to training them separately. Additional experiments with sentiment analysis confirm that the improvements stem from the connection between bias and stereotypes, not multi-task learning alone. These findings highlight the value of leveraging stereotype information to build fairer and more effective AI systems.</abstract>
      <url hash="20237e05">2025.findings-acl.889</url>
      <bibkey>tomar-etal-2025-stereotype</bibkey>
    </paper>
    <paper id="890">
      <title>Filling the Temporal Void: Recovering Missing Publication Years in the <fixed-case>P</fixed-case>roject <fixed-case>G</fixed-case>utenberg Corpus Using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Omar</first><last>Momen</last></author>
      <author><first>Manuel</first><last>Schaaf</last></author>
      <author><first>Alexander</first><last>Mehler</last><affiliation>Johann Wolfgang Goethe Universität Frankfurt am Main</affiliation></author>
      <pages>17318-17334</pages>
      <abstract>Analysing texts spanning long periods of time is critical for researchers in historical linguistics and related disciplines. However, publicly available corpora suitable for such analyses are scarce. The Project Gutenberg (PG) corpus presents a significant yet underutilized opportunity in this context, due to the absence of accurate temporal metadata. We take advantage of language models and information retrieval to explore four sources of information – Open Web, Wikipedia, Open Library API, and PG books texts – to add missing temporal metadata to the PG corpus. Through 20 experiments employing state-of-the-art Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) methods, we estimate the production years of all PG books. We curate an enriched metadata repository for the PG corpus and propose a refined version for it, which includes 53,774 books with a total of 3.8 billion tokens in 11 languages, produced between 1600 and 2000. This work provides a new resource for computational linguistics and humanities studies focusing on diachronic analyses. The final dataset and all experiments data are publicly available (https://github.com/OmarMomen14/pg-dates).</abstract>
      <url hash="7a0829d8">2025.findings-acl.890</url>
      <bibkey>momen-etal-2025-filling</bibkey>
    </paper>
    <paper id="891">
      <title><fixed-case>E</fixed-case>xpli<fixed-case>C</fixed-case>a: Evaluating Explicit Causal Reasoning in Large Language Models</title>
      <author><first>Martina</first><last>Miliani</last></author>
      <author><first>Serena</first><last>Auriemma</last><affiliation>Universita’ di Pisa, University of Pisa</affiliation></author>
      <author><first>Alessandro</first><last>Bondielli</last><affiliation>Universita’ di Pisa, University of Pisa</affiliation></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Lucia</first><last>Passaro</last></author>
      <author><first>Irene</first><last>Sucameli</last></author>
      <author><first>Alessandro</first><last>Lenci</last><affiliation>University of Pisa</affiliation></author>
      <pages>17335-17355</pages>
      <abstract>Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.</abstract>
      <url hash="f0445af5">2025.findings-acl.891</url>
      <bibkey>miliani-etal-2025-explica</bibkey>
    </paper>
    <paper id="892">
      <title>Are Dialects Better Prompters? A Case Study on <fixed-case>A</fixed-case>rabic Subjective Text Classification</title>
      <author><first>Leila</first><last>Moudjari</last><affiliation>IRIT</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>Institut de recherche en informatique de toulouse</affiliation></author>
      <pages>17356-17371</pages>
      <abstract>This paper investigates the effect of dialectal prompting, variations in prompting scrip t and model fine-tuning on subjective classification in Arabic dialects. To this end, we evaluate the performances of 12 widely used open LLMs across four tasks and eight benchmark datasets. Our results reveal that specialized fine-tuned models with Arabic and Arabizi scripts dialectal prompts achieve the best results, which constitutes a novel state of the art in the field.</abstract>
      <url hash="428a5a73">2025.findings-acl.892</url>
      <bibkey>moudjari-benamara-2025-dialects</bibkey>
    </paper>
    <paper id="893">
      <title>Natural Logic at the Core: Dynamic Rewards for Entailment Tree Generation</title>
      <author><first>Jihao</first><last>Shi</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Hengwei</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>17372-17382</pages>
      <abstract>Entailment trees are essential for enhancing interpretability and transparency in tasks like question answering and natural language understanding. However, existing approaches often lack logical consistency, as they rely on static reward structures or ignore the intricate dependencies within multi-step reasoning. To address these limitations, we propose a method that integrates natural logic principles into reinforcement learning, enabling dynamic reward computation to guide entailment tree generation. Our approach ensures logical consistency across reasoning steps while improving interpretability and generalization. Experiments on EntailmentBank demonstrate significant improvements over state-of-the-art methods, highlighting the effectiveness of natural logic in structured reasoning.</abstract>
      <url hash="8bbd3724">2025.findings-acl.893</url>
      <bibkey>shi-etal-2025-natural</bibkey>
    </paper>
    <paper id="894">
      <title><fixed-case>R</fixed-case>.<fixed-case>R</fixed-case>.: Unveiling <fixed-case>LLM</fixed-case> Training Privacy through Recollection and Ranking</title>
      <author><first>Wenlong</first><last>Meng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guo</first><last>Zhenyuan</last></author>
      <author><first>Lenan</first><last>Wu</last></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Wenyan</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Weixian</first><last>Li</last></author>
      <author><first>Chengkun</first><last>Wei</last></author>
      <author><first>Wenzhi</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>17383-17397</pages>
      <abstract>Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLMs’ training data remains challenging. In this paper, we propose (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the achieves better PII identification performance than baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release our code and datasets at GitHub.</abstract>
      <url hash="76b6a8c6">2025.findings-acl.894</url>
      <bibkey>meng-etal-2025-r</bibkey>
    </paper>
    <paper id="895">
      <title>Nested-Refinement Metamorphosis: Reflective Evolution for Efficient Optimization of Networking Problems</title>
      <author><first>Shuhan</first><last>Guo</last></author>
      <author><first>Nan</first><last>Yin</last></author>
      <author><first>James</first><last>Kwok</last><affiliation>Department of Computer Science and Engineering, The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Quanming</first><last>Yao</last><affiliation>Tsinghua University</affiliation></author>
      <pages>17398-17429</pages>
      <abstract>Large Language Models (LLMs) excel in network algorithm design but suffer from inefficient iterative coding and high computational costs. Drawing inspiration from butterfly metamorphosis—where structured developmental phases (Phase I: larval nutrient accumulation → Phase II: pupal transformation) enable adaptive evolution—we propose Nested-Refinement Metamorphosis (NeRM). Building on this principle, we introduce Metamorphosis on Prompts (MoP) to iteratively refine task descriptions (e.g. latency / bandwidth constraints) and Metamorphosis on Algorithms (MoA) to generate more effective solutions (e.g. appropriate network processing architecture). Their nested refinement ensures task-algorithm alignment, systematically improving both task descriptions and algorithmic solutions for more efficient algorithm design. To further enhance efficiency, we incorporate predictor-assisted code evaluation, mimicking natural selection by filtering out weak candidates early and reducing computational costs. Experimental results on TSP (routing), MKP (resource allocation), and CVRP (service-network coordination) demonstrate that NeRM consistently outperforms state-of-the-art approaches in both performance and efficiency.</abstract>
      <url hash="8e524a22">2025.findings-acl.895</url>
      <bibkey>guo-etal-2025-nested</bibkey>
    </paper>
    <paper id="896">
      <title><fixed-case>MC</fixed-case>-<fixed-case>MKE</fixed-case>: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency</title>
      <author><first>Junzhe</first><last>Zhang</last></author>
      <author><first>Huixuan</first><last>Zhang</last></author>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Baizhou</first><last>Huang</last></author>
      <author><first>Xu</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>17430-17445</pages>
      <abstract>Multimodal large language models (MLLMs) are prone to non-factual or outdated knowledge issues, highlighting the importance of knowledge editing. Many benchmark has been proposed for researching multimodal knowledge editing. However, previous benchmarks focus on limited scenarios due to the lack of rigorous definition of multimodal knowledge. To better evaluate multimodal knowledge editing, we propose a decomposed definition of multimodal knowledge. Following the decomposed definition of multimodal knowledge, we introduce three scenarios and a novel requirement modality consistency. We construct MC-MKE, a fine-grained **M**ultimodal **K**nowledge **E**diting benchmark emphasizing **M**odality **C**onsistency through strict data selection. We evaluate four multimodal knowledge editing methods on MC-MKE, revealing their limitations, particularly in terms of modality consistency. Our work highlights the challenges posed by multimodal knowledge editing and motivates further research in developing effective techniques for this task.</abstract>
      <url hash="c794b9dc">2025.findings-acl.896</url>
      <bibkey>zhang-etal-2025-mc</bibkey>
    </paper>
    <paper id="897">
      <title>Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models</title>
      <author><first>Alessio</first><last>Galatolo</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Zhenbang</first><last>Dai</last></author>
      <author><first>Katie</first><last>Winkle</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Meriem</first><last>Beloucif</last><affiliation>Uppsala University</affiliation></author>
      <pages>17446-17461</pages>
      <abstract>Fine-tuning Large Language Models (LLMs) with first-order methods like back-propagation is computationally intensive. Zeroth-Order (ZO) optimisation uses function evaluations instead of gradients, reducing memory usage, but suffers from slow convergence in high-dimensional models. As a result, ZO research in LLMs has mostly focused on classification, overlooking more complex generative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm designed for *Preference Optimisation* in LLMs. We begin by analysing the interplay between policy and reward models during traditional (first-order) Preference Optimisation, uncovering patterns in their relative updates. Guided by these insights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to accelerate convergence. Through experiments on summarisation, machine translation, and conversational assistants, we demonstrate that our method consistently enhances reward signals while achieving convergence times comparable to first-order methods. While it falls short of some state-of-the-art methods, our work is the first to apply Zeroth-Order methods to Preference Optimisation in LLMs, going beyond classification tasks and paving the way for a largely unexplored research direction. Code and visualisations are available at https://github.com/alessioGalatolo/VisZOPrO.</abstract>
      <url hash="c269c024">2025.findings-acl.897</url>
      <bibkey>galatolo-etal-2025-visualising</bibkey>
    </paper>
    <paper id="898">
      <title>Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding</title>
      <author><first>Elisa</first><last>Sanchez-Bayona</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>17462-17477</pages>
      <abstract>This paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that LLMs’ performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code publicly available: https://github.com/elisanchez-beep/metaphorLLM</abstract>
      <url hash="10f8fc85">2025.findings-acl.898</url>
      <bibkey>sanchez-bayona-agerri-2025-metaphor</bibkey>
    </paper>
    <paper id="899">
      <title><fixed-case>A</fixed-case>sk<fixed-case>QE</fixed-case>: Question Answering as Automatic Evaluation for Machine Translation</title>
      <author><first>Dayeon</first><last>Ki</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>17478-17515</pages>
      <abstract>How can a monolingual English speaker determine whether an automatic translation in French is good enough to be shared? Existing MT error detection and quality estimation (QE) techniques do not address this practical scenario. We introduce AskQE, a question generation and answering framework designed to detect critical MT errors and provide actionable feedback, helping users decide whether to accept or reject MT outputs even without the knowledge of the target language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the COVID-19 domain, we explore design choices for AskQE and develop an optimized version relying on LLaMA-3 70B and entailed facts to guide question generation. We evaluate the resulting system on the BioMQM dataset of naturally occurring MT errors, where AskQE has higher Kendall’s Tau correlation and decision accuracy with human ratings compared to other QE metrics.</abstract>
      <url hash="7bad3809">2025.findings-acl.899</url>
      <bibkey>ki-etal-2025-askqe</bibkey>
    </paper>
    <paper id="900">
      <title><fixed-case>E</fixed-case>x<fixed-case>P</fixed-case>er<fixed-case>T</fixed-case>: Effective and Explainable Evaluation of Personalized Long-Form Text Generation</title>
      <author><first>Alireza</first><last>Salemi</last></author>
      <author><first>Julian</first><last>Killingback</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <pages>17516-17532</pages>
      <abstract>Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e. prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidences from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style—two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT’s explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.</abstract>
      <url hash="6e83e40c">2025.findings-acl.900</url>
      <bibkey>salemi-etal-2025-expert</bibkey>
    </paper>
    <paper id="901">
      <title>Bridging Intuitive Associations and Deliberate Recall: Empowering <fixed-case>LLM</fixed-case> Personal Assistant with Graph-Structured Long-term Memory</title>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Weikang</first><last>Yuan</last></author>
      <author><first>Zhuoren</first><last>Jiang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>17533-17547</pages>
      <abstract>Large language models (LLMs)-based personal assistants may struggle to effectively utilize long-term conversational histories.Despite advances in long-term memory systems and dense retrieval methods, these assistants still fail to capture entity relationships and handle multiple intents effectively. To tackle above limitations, we propose **Associa**, a graph-structured memory framework that mimics human cognitive processes. Associa comprises an event-centric memory graph and two collaborative components: **Intuitive Association**, which extracts evidence-rich subgraphs through Prize-Collecting Steiner Tree optimization, and **Deliberating Recall**, which iteratively refines queries for comprehensive evidence collection. Experiments show that Associa significantly outperforms existing methods in retrieval and QA (question and answering) tasks across long-term dialogue benchmarks, advancing the development of more human-like AI memory systems.</abstract>
      <url hash="31e8c327">2025.findings-acl.901</url>
      <bibkey>zhang-etal-2025-bridging</bibkey>
    </paper>
    <paper id="902">
      <title>Each graph is a new language: Graph Learning with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Huachi</first><last>Zhou</last></author>
      <author><first>Jiahe</first><last>Du</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Chuang</first><last>Zhou</last></author>
      <author><first>Chang</first><last>Yang</last></author>
      <author><first>Yilin</first><last>Xiao</last></author>
      <author><first>Yuxuan</first><last>Xie</last></author>
      <author><first>Xiao</first><last>Huang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>17548-17559</pages>
      <abstract>Natural language has been extensively used for modeling text-attributed graphs with LLMs. Natural language is used to describe the graph for LLMs to understand or serve as component of the graph, e.g., textual attributes for embedding generation. However, natural language is inherently redundant and unstructured, making it unsuitable for modeling high-order neighbors with LLMs. Specifically, (i) graph descriptions become verbose, overwhelming LLMs, and (ii) only relying on attribute embeddings limits LLM’s ability to capture the adequate graph structural information. These limitations make it difficult to model graphs both concisely and adequately using sole natural language with LLMs.Inspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose Graph-Defined Language for Large Language Model (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates the graph into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand the graph. This corpus represents the subgraph centered around target nodes concisely with only a few tokens during fine-tuning on downstream tasks. By treating the graph as a new language, GDL4LLM enables LLMs to model text-attributed graph adequately and concisely. Extensive experiments on five datasets demonstrate that GDL4LLM outperforms description-based and embedding-based baselines by efficiently modeling different orders of neighbors.</abstract>
      <url hash="5eba72d6">2025.findings-acl.902</url>
      <bibkey>zhou-etal-2025-graph</bibkey>
    </paper>
    <paper id="903">
      <title>100-<fixed-case>L</fixed-case>ong<fixed-case>B</fixed-case>ench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?</title>
      <author><first>Van</first><last>Yang</last></author>
      <author><first>Hongye</first><last>Jin</last><affiliation>Texas A&amp;M</affiliation></author>
      <author><first>Shaochen</first><last>Zhong</last><affiliation>Rice University</affiliation></author>
      <author><first>Song</first><last>Jiang</last><affiliation>FAIR</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Vipin</first><last>Chaudhary</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Xiaotian</first><last>Han</last><affiliation>Case Western Reserve University</affiliation></author>
      <pages>17560-17576</pages>
      <abstract>Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM shall enable its users to effortlessly process many originally exhausting tasks — e.g., digesting a long-form document to find answers v.s., directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have a few major shortcomings. For instance, some Needle-in-a-Haystack-like benchmarks are too synthetic, and therefore do not represent the real world usage of LLMs. While some real-task-based benchmarks like LongBench avoid this problem, such benchmarks are often formed in a way where each data sample has a fixed sequence length, which not only makes them solely suitable for models with a certain range of context windows, but also lacks a proxy to know at what length the model/method-of-interest would fail. Last, most benchmarks tend to not provide proper metrics to separate long-context performance from the model’s baseline ability, so when conducting a cross-model/recipe comparison, such conflation makes the user unable to understand how exactly one model or recipe excels at the long-context task in relation to its baseline ability. To address these issues, we introduce a length-controllable, real-life reflective benchmark with a novel metric that disentangles baseline knowledge from long-context capabilities. Experiments demonstrate the superiority of our datasets in effectively evaluating LLMs. All assets are available at https://github.com/uservan/100-LongBench.git.</abstract>
      <url hash="46821e88">2025.findings-acl.903</url>
      <bibkey>yang-etal-2025-100</bibkey>
    </paper>
    <paper id="904">
      <title>Multimodal Fusion and Coherence Modeling for Video Topic Segmentation</title>
      <author><first>Hai</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chong</first><last>Deng</last></author>
      <author><first>Qinglin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaqing</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qian</first><last>Chen</last></author>
      <author><first>Wen</first><last>Wang</last></author>
      <pages>17577-17593</pages>
      <abstract>The video topic segmentation (VTS) task segments videos into intelligible, non-overlapping topics, facilitating efficient comprehension of video content and quick access to specific content. VTS is also critical to various downstream video understanding tasks. Traditional VTS methods using shallow features or unsupervised approaches struggle to accurately discern the nuances of topical transitions. Recently, supervised approaches have achieved superior performance on video action or scene segmentation over unsupervised approaches. In this work, we improve supervised VTS by thoroughly exploring **multimodal fusion** and **multimodal coherence modeling**. Specifically, (1) we enhance multimodal fusion by exploring different architectures using Cross-Attention and Mixture of Experts. (2) To generally strengthen multimodality alignment and fusion, we pre-train and fine-tune the model with multimodal contrastive learning. (3) We propose a new pre-training task tailored for the VTS task, and a novel fine-tuning task for enhancing multimodal coherence modeling for VTS. We evaluate our proposed approaches on educational videos, in the form of lectures, due to the vital role of topic segmentation of educational videos in boosting learning experiences. Additionally, to promote research in VTS, we introduce a large-scale Chinese lecture video dataset to augment the existing English lecture video datasets. Experiments on both English and Chinese lecture datasets demonstrate that our model achieves superior VTS performance compared to competitive unsupervised and supervised baselines.</abstract>
      <url hash="e62afd90">2025.findings-acl.904</url>
      <bibkey>yu-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="905">
      <title>Are Your <fixed-case>LLM</fixed-case>s Capable of Stable Reasoning?</title>
      <author><first>Junnan</first><last>Liu</last></author>
      <author><first>Hongwei</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Linchen</first><last>Xiao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Ziyi</first><last>Wang</last></author>
      <author><first>Kuikun</first><last>Liu</last></author>
      <author><first>Songyang</first><last>Gao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>17594-17632</pages>
      <abstract>The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce **G-Pass@**<tex-math>k</tex-math>, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model’s performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@<tex-math>k</tex-math> in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics.</abstract>
      <url hash="518f206f">2025.findings-acl.905</url>
      <bibkey>liu-etal-2025-llms-capable</bibkey>
    </paper>
    <paper id="906">
      <title><fixed-case>FANNO</fixed-case>: Augmenting High-Quality Instruction Data with Open-Sourced <fixed-case>LLM</fixed-case>s Only</title>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Yifan</first><last>Ding</last></author>
      <author><first>Yicheng</first><last>Tao</last></author>
      <author><first>Zhiwen</first><last>Ruan</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>Wenjia</first><last>Zhang</last><affiliation>Tongji University and Peking University</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>17633-17653</pages>
      <abstract>Instruction tuning stands as a crucial advancement in leveraging large language models (LLMs) for enhanced task performance. However, the annotation of instruction datasets has traditionally been expensive and laborious, often relying on manual annotations or costly proprietary LLMs. Recent works explore approaches to synthesize data with open-sourced LLMs but require high-quality human-crafted seed data. In this work, we introduce , an end-to-end framework to synthesize high-quality instruction data with open-sourced LLMs and sampled unlabeled documents, eliminating the necessity for seed data. Starting from diverse pre-screened documents, the framework synthesizes complex and diverse high-quality instruction and response pairs in different stages. We propose a tagging-based prompt method to generate diverse and complex seed data and a UCB-based approach to augment more instruction data with the seed data. A novel <i>Think Different</i> prompt is proposed to address the distributional limitations of the seeds, further boosting the data diversity. Experiments prove that the can generate diverse and complex high-quality data even with a opensource small teacher model. The synthesized instruction data demonstrates performance that is comparable to, or even surpasses, baseline annotation methods with proprietary LLMs or open-sourced LLMs while requiring fewer instruction data samples.</abstract>
      <url hash="5b2d764f">2025.findings-acl.906</url>
      <bibkey>zhu-etal-2025-fanno</bibkey>
    </paper>
    <paper id="907">
      <title><fixed-case>JEBS</fixed-case>: A Fine-grained Biomedical Lexical Simplification Task</title>
      <author><first>William</first><last>Xia</last></author>
      <author><first>Ishita</first><last>Unde</last></author>
      <author><first>Brian David</first><last>Ondov</last><affiliation>Yale School of Medicine</affiliation></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <pages>17654-17666</pages>
      <abstract>Though online medical literature has made health information more available than ever, the barrier of complex medical jargon prevents the general public from understanding it. Though parallel and comparable corpora for Biomedical Text Simplification have been introduced, these conflate the many syntactic and lexical operations involved in simplification. To enable more targeted development and evaluation, we present a fine-grained lexical simplification task and dataset, Jargon Explanations for Biomedical Simplification (JEBS). The JEBS task involves identifying complex terms, classifying how to replace them, and generating replacement text. The JEBS dataset contains 21,595 replacements for 10,314 terms across 400 biomedical abstracts and their manually simplified versions. Additionally, we provide baseline results for a variety of rule-based and transformer-based systems for the three subtasks. The JEBS task, data, and baseline results pave the way for development and rigorous evaluation of systems for replacing or explaining complex biomedical terms.</abstract>
      <url hash="6e6cdfb1">2025.findings-acl.907</url>
      <bibkey>xia-etal-2025-jebs</bibkey>
    </paper>
    <paper id="908">
      <title>Multi-Hop Reasoning for Question Answering with Hyperbolic Representations</title>
      <author><first>Simon</first><last>Welz</last></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Akbar</first><last>Karimi</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <pages>17667-17679</pages>
      <abstract>Hyperbolic representations are effective in modeling knowledge graph data which is prevalently used to facilitate multi-hop reasoning. However, a rigorous and detailed comparison of the two spaces for this task is lacking. In this paper, through a simple integration of hyperbolic representations with an encoder-decoder model, we perform a controlled and comprehensive set of experiments to compare the capacity of hyperbolic space versus Euclidean space in multi-hop reasoning. Our results show that the former consistently outperforms the latter across a diverse set of datasets. In addition, through an ablation study, we show that a learnable curvature initialized with the delta hyperbolicity of the utilized data yields superior results to random initializations. Furthermore, our findings suggest that hyperbolic representations can be significantly more advantageous when the datasets exhibit a more hierarchical structure.</abstract>
      <url hash="34870023">2025.findings-acl.908</url>
      <bibkey>welz-etal-2025-multi</bibkey>
    </paper>
    <paper id="909">
      <title>Look &amp; Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest <fixed-case>X</fixed-case>-ray Report Generation</title>
      <author><first>Yunsoo</first><last>Kim</last></author>
      <author><first>Jinge</first><last>Wu</last></author>
      <author><first>Su Hwan</first><last>Kim</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Pardeep</first><last>Vasudev</last></author>
      <author><first>Jiashu</first><last>Shen</last></author>
      <author><first>Honghan</first><last>Wu</last><affiliation>University of Glasgow</affiliation></author>
      <pages>17680-17694</pages>
      <abstract>Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still suffer from hallucinations and clinically significant errors, limiting their reliability in real-world applications. In this study, we propose Look &amp; Mark (L&amp;M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into the LLM prompting framework. Unlike conventional fine-tuning, L&amp;M leverages in-context learning to achieve substantial performance gains without retraining. When evaluated across multiple domain-specific and general-purpose models, L&amp;M demonstrates significant gains, including a 1.2% improvement in overall metrics (A.AVG) for CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for LLaVA-Med. General-purpose models also benefit from L&amp;M combined with in-context learning, with LLaVA-OV achieving an 87.3% clinical average performance (C.AVG)—the highest among all models, even surpassing those explicitly trained for CXR report generation. Expert evaluations further confirm that L&amp;M reduces clinically significant errors (by 0.43 average errors per report), such as false predictions and omissions, enhancing both accuracy and reliability. These findings highlight L&amp;M’s potential as a scalable and efficient solution for AI-assisted radiology, paving the way for improved diagnostic workflows in low-resource clinical settings.</abstract>
      <url hash="419fcd4a">2025.findings-acl.909</url>
      <bibkey>kim-etal-2025-look</bibkey>
    </paper>
    <paper id="910">
      <title>Hatevolution: What Static Benchmarks Don’t Tell Us</title>
      <author><first>Chiara</first><last>Di Bonaventura</last></author>
      <author><first>Barbara</first><last>McGillivray</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Albert</first><last>Meroño-Peñuela</last><affiliation>King’s College London</affiliation></author>
      <pages>17695-17707</pages>
      <abstract>Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.</abstract>
      <url hash="3eda6461">2025.findings-acl.910</url>
      <bibkey>di-bonaventura-etal-2025-hatevolution</bibkey>
    </paper>
    <paper id="911">
      <title>Tag-Instruct: Controlled Instruction Complexity Enhancement through Structure-based Augmentation</title>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Zhiwen</first><last>Ruan</last></author>
      <author><first>Junyou</first><last>Su</last></author>
      <author><first>Xingwei</first><last>He</last></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Wenjia</first><last>Zhang</last><affiliation>Tongji University and Peking University</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>17708-17729</pages>
      <abstract>High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present Tag-Instruct, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, Tag-Instruct compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that Tag-Instruct outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks.</abstract>
      <url hash="ad538bb0">2025.findings-acl.911</url>
      <bibkey>zhu-etal-2025-tag</bibkey>
    </paper>
    <paper id="912">
      <title>Code-<fixed-case>SPA</fixed-case>: Style Preference Alignment to Large Language Models for Effective and Robust Code Debugging</title>
      <author><first>Tengfei</first><last>Wen</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>17730-17743</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in coding tasks like code generation and debugging. However, code from real-world users is often poorly styled, containing various types of noise, such as structural inconsistencies, stylistic deviations and flawed test cases. To investigate this, we first simulate poorly styled code using eight types of code perturbations, and then demonstrate that the debugging performance of existing LLM-based methods significantly declines on such inputs. Furthermore, to address this, we propose a novel debugging method called Code-SPA, which aligns noisy code with the well-structured style familiar to LLMs, mitigating the impact of stylistic inconsistencies. Specifically, Code-SPA extracts the model’s preferred coding style from a reference snippet, then adjusts the input code by Concrete Syntax Tree (CST)-based transformations and LLM-assisted refinements before debugging. By aligning the code style preference, Code-SPA enhances the debugging performance of both code-specific and general-purpose LLMs on both poorly and well-styled code across the HumanEval, MBPP and EvalPlus datasets.</abstract>
      <url hash="10ece048">2025.findings-acl.912</url>
      <bibkey>wen-etal-2025-code</bibkey>
    </paper>
    <paper id="913">
      <title>Open-World Authorship Attribution</title>
      <author><first>Xinhao</first><last>Tan</last></author>
      <author><first>Songhua</first><last>Liu</last></author>
      <author><first>Xia</first><last>Cong</last></author>
      <author><first>Kunjun</first><last>Li</last></author>
      <author><first>Xinchao</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <pages>17744-17758</pages>
      <abstract>Recent years have witnessed rapid advancements in Large Language Models (LLMs). Nevertheless, it remains unclear whether state-of-the-art LLMs can infer the author of an anonymous research paper solely from the text, without any additional information. To investigate this novel challenge, which we define as Open-World Authorship Attribution, we introduce a benchmark comprising thousands of research papers across various fields to quantitatively assess model capabilities. Then, at the core of this paper, we tailor a two-stage framework to tackle this problem: candidate selection and authorship decision. Specifically, in the first stage, LLMs are prompted to generate multi-level key information, which are then used to identify potential candidates through Internet searches. In the second stage, we introduce key perspectives to guide LLMs in determining the most likely author from these candidates. Extensive experiments on our benchmark demonstrate the effectiveness of the proposed approach, achieving 60.7% and 44.3% accuracy in the two stages, respectively. We will release our benchmark and source codes to facilitate future research in this field.</abstract>
      <url hash="0760a47a">2025.findings-acl.913</url>
      <bibkey>tan-etal-2025-open</bibkey>
    </paper>
    <paper id="914">
      <title>What is in a name? Mitigating Name Bias in Text Embedding Similarity via Anonymization</title>
      <author><first>Sahil</first><last>Manchanda</last><affiliation>Pocket FM</affiliation></author>
      <author><first>Pannaga</first><last>Shivaswamy</last><affiliation>Adobe Systems</affiliation></author>
      <pages>17759-17781</pages>
      <abstract>Text-embedding models often exhibit biases arising from the data on which they are trained. In this paper, we examine a hitherto unexplored bias in text-embeddings: bias arising from the presence of <tex-math>\textit{names}</tex-math> such as persons, locations, organizations etc. in the text. Our study shows how the presence of <tex-math>\textit{name-bias}</tex-math> in text-embedding models can potentially lead to erroneous conclusions in the assessment of thematic similarity. <i>Text-embeddings can mistakenly indicate similarity between texts based on names in the text, even when their actual semantic content has no similarity or indicate dissimilarity simply because of the names in the text even when the texts match semantically</i>. We first demonstrate the presence of name bias in different text-embedding models and then propose <tex-math>\textit{text-anonymization}</tex-math> during inference which involves removing references to names, while preserving the core theme of the text. The efficacy of the anonymization approach is demonstrated on three downstream NLP tasks involving embedding similarities, achieving significant performance gains. Our simple and training-optimization-free approach offers a practical and easily implementable solution to mitigate name bias.</abstract>
      <url hash="e7d7a1a5">2025.findings-acl.914</url>
      <bibkey>manchanda-shivaswamy-2025-name</bibkey>
    </paper>
    <paper id="915">
      <title><fixed-case>B</fixed-case>en<fixed-case>N</fixed-case>um<fixed-case>E</fixed-case>val: A Benchmark to Assess <fixed-case>LLM</fixed-case>s’ Numerical Reasoning Capabilities in <fixed-case>B</fixed-case>engali</title>
      <author><first>Kawsar</first><last>Ahmed</last></author>
      <author><first>Md</first><last>Osama</last></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Eftekhar</first><last>Hossain</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <pages>17782-17799</pages>
      <abstract>Large Language Models (LLMs) demonstrate exceptional proficiency in general-purpose tasks but struggle with numerical reasoning, particularly in low-resource languages like Bengali. Despite advancements, limited research has explored their numerical reasoning capabilities in these languages. To address this gap, we present BenNumEval (Bengali Numerical Evaluation), a benchmark designed to assess LLMs on numerical reasoning tasks in Bengali. It comprises six diverse tasks and a total of 3.2k samples curated from real-world problem-solving scenarios. Our extensive evaluations reveal that even with advanced prompting techniques such as Cross-Lingual Prompting (XLP) and Cross-Lingual Chain-of-Thought Prompting (XCoT), LLMs fall notably short of human-level performance, particularly when using Bengali Native Prompting (BNaP). These findings underscore the substantial gap between current LLM capabilities and human expertise in numerical reasoning, highlighting the need for more robust and linguistically inclusive AI models to advance Bengali Language Processing and equitable AI development. The source code for the system and evaluation pipeline is publicly available on GitHub.</abstract>
      <url hash="7cd87719">2025.findings-acl.915</url>
      <bibkey>ahmed-etal-2025-bennumeval</bibkey>
    </paper>
    <paper id="916">
      <title><fixed-case>LLM</fixed-case> Agents for Coordinating Multi-User Information Gathering</title>
      <author><first>Harsh</first><last>Jhamtani</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>17800-17826</pages>
      <abstract>This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA, focused on questions about tabular data, and PeopleJoin-DocCreation, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic “organizations” of 2–20 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PeopleJoin.</abstract>
      <url hash="32ed95b2">2025.findings-acl.916</url>
      <bibkey>jhamtani-etal-2025-llm</bibkey>
    </paper>
    <paper id="917">
      <title><fixed-case>C</fixed-case>2<fixed-case>KD</fixed-case>: Cross-layer and Cross-head Knowledge Distillation for Small Language Model-based Recommendation</title>
      <author><first>Xiao</first><last>Chen</last></author>
      <author><first>Changyi</first><last>Ma</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenqi</first><last>Fan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <pages>17827-17838</pages>
      <abstract>Sequential recommenders predict users’ next interactions based on historical behavior and are essential in modern recommendation systems. While Large Language Models (LLMs) show promise, their size and high inference costs limit deployment on resource-constrained devices. Small Language Models (SLMs) provide a more efficient alternative for edge devices, but bridging the recommendation performance gap between LLMs and SLMs remains challenging. Typical approaches like supervised fine-tuning or vanilla knowledge distillation (KD) often lead to suboptimal performance or even negative transfer. Our motivational experiments reveal key issues with vanilla KD methods: feature imitation suffers from redundancy and uneven recommendation ability across layers, while prediction mimicking faces conflicts caused by differing weight distributions of prediction heads. To address these challenges, we propose a simple yet effective framework, C2KD, to transfer task-relevant knowledge from two complementary dimensions. Specifically, our method incorporates: (1) cross-layer feature imitation, which uses a dynamic router to select the most relevant teacher layers and assimilate task-relevant knowledge from the teacher’s late layers, allowing the student to concentrate on the teacher’s specialized knowledge; and (2) cross-head logit distillation, which maps the intermediate features of the student to the teacher’s output head, thereby minimizing prediction discrepancies between the teacher and the student. Extensive experiments across diverse model families demonstrate that our approach enables 1B-parameter SLMs to achieve competitive performance compared to LLMs (e.g., Llama3-8B), offering a practical solution for real-world on-device sequential recommendations.</abstract>
      <url hash="3fa25d18">2025.findings-acl.917</url>
      <bibkey>chen-etal-2025-c2kd</bibkey>
    </paper>
    <paper id="918">
      <title><fixed-case>S</fixed-case>ign2<fixed-case>V</fixed-case>is: Automated Data Visualization from Sign Language</title>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Zhen</first><last>Li</last></author>
      <author><first>Guobiao</first><last>Zhang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <author><first>Hai</first><last>Jin</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>April</first><last>Wang</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>17839-17857</pages>
      <abstract>Data visualizations, such as bar charts and histograms, are essential for analyzing and exploring data, enabling the effective communication of insights. While existing methods have been proposed to translate natural language descriptions into visualization queries, they focus solely on spoken languages, overlooking sign languages, which comprise about 200 variants used by 70 million Deaf and Hard-of-Hearing (DHH) individuals. To fill this gap, this paper proposes Sign2Vis, a sign language interface that enables the DHH community to engage more fully with data analysis. We first construct a paired dataset that includes sign language pose videos and their corresponding visualization queries. Using this dataset, we evaluate a variety of models, including both pipeline-based and end-to-end approaches. Extensive experiments, along with a user study involving 15 participants, demonstrate the effectiveness of Sign2Vis. Finally, we share key insights from our evaluation and highlight the need for more accessible and user-centered tools to support the DHH community in interactive data analytics.</abstract>
      <url hash="ed56f265">2025.findings-acl.918</url>
      <bibkey>wan-etal-2025-sign2vis</bibkey>
    </paper>
    <paper id="919">
      <title>Transparentize the Internal and External Knowledge Utilization in <fixed-case>LLM</fixed-case>s with Trustworthy Citation</title>
      <author><first>Jiajun</first><last>Shen</last></author>
      <author><first>Tong</first><last>Zhou</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Delai</first><last>Qiu</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>17858-17877</pages>
      <abstract>While hallucinations of large language models could be alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable. In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm. Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines. Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation.</abstract>
      <url hash="0b8a4255">2025.findings-acl.919</url>
      <bibkey>shen-etal-2025-transparentize</bibkey>
    </paper>
    <paper id="920">
      <title><fixed-case>JARVIS</fixed-case>-<fixed-case>VLA</fixed-case>: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse</title>
      <author><first>Muyao</first><last>Li</last></author>
      <author><first>Zihao</first><last>Wang</last></author>
      <author><first>Kaichen</first><last>He</last></author>
      <author><first>Xiaojian</first><last>Ma</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yitao</first><last>Liang</last><affiliation>Peking University</affiliation></author>
      <pages>17878-17899</pages>
      <abstract>Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundation model itself. In response, we introduce Act from Visual Language Post-Training (ActVLP), a novel training paradigm. ActVLP distinctively enhances the foundation model prior to action-specific tuning by first post-training it on a curated set of environment-specific visual and linguistic tasks using self-supervised learning. This initial stage significantly improves the model’s capabilities in world knowledge, visual recognition, and spatial grounding. Subsequently, this strengthened VLM undergoes action post-training via imitation learning on trajectory datasets.Following this paradigm, we develop JARVIS-VLA, the first VLA model in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that our ActVLP paradigm leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, JARVIS-VLA surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research.The project page can be found at <url>https://craftjarvis.github.io/JarvisVLA</url>.</abstract>
      <url hash="e4eee569">2025.findings-acl.920</url>
      <bibkey>li-etal-2025-jarvis</bibkey>
    </paper>
    <paper id="921">
      <title>Generative Frame Sampler for Long Video Understanding</title>
      <author><first>Linli</first><last>Yao</last></author>
      <author><first>Haoning</first><last>Wu</last><affiliation>Rhymes AI</affiliation></author>
      <author><first>Kun</first><last>Ouyang</last></author>
      <author><first>Yuanxing</first><last>Zhang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Bei</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xu</first><last>Sun</last><affiliation>Peking University</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <pages>17900-17917</pages>
      <abstract>Despite recent advances in Video Large Language Models (VideoLLMs), effectively understanding long-form videos remains a significant challenge. Perceiving lengthy videos containing thousands of frames poses substantial computational burden. To mitigate this issue, this paper introduces Generative Frame Sampler (GenS), a plug-and-play module integrated with VideoLLMs to facilitate efficient lengthy video perception. Built upon a lightweight VideoLLM, GenS leverages its inherent vision-language capabilities to identify question-relevant frames. To facilitate effective retrieval, we construct GenS-Video-150K, a large-scale video instruction dataset with dense frame relevance annotations. Extensive experiments demonstrate that GenS consistently boosts the performance of various VideoLLMs, including open-source models (Qwen2-VL-7B, Aria-25B, LLaVA-Video-7B/72B) and proprietary assistants (GPT-4o, Gemini). When equipped with GenS, open-source VideoLLMs achieve impressive state-of-the-art results on long-form video benchmarks: LLaVA-Video-72B reaches 66.8 (+4.3) on LongVideoBench and 77.0 (+2.7) on MLVU, while Aria obtains 39.2 on HourVideo surpassing the Gemini-1.5-pro by 1.9 points.</abstract>
      <url hash="5a014196">2025.findings-acl.921</url>
      <bibkey>yao-etal-2025-generative</bibkey>
    </paper>
    <paper id="922">
      <title>Annotating the Annotators: Analysis, Insights and Modelling from an Annotation Campaign on Persuasion Techniques Detection</title>
      <author><first>Davide</first><last>Bassi</last></author>
      <author><first>Dimitar Iliyanov</first><last>Dimitrov</last></author>
      <author><first>Bernardo</first><last>D’Auria</last><affiliation>Universita’ di Padova, University of Padua</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Christian</first><last>Moro</last><affiliation>University of Padova</affiliation></author>
      <author><first>Luisa</first><last>Orrù</last></author>
      <author><first>Gian Piero</first><last>Turchi</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Giovanni</first><last>Da San Martino</last><affiliation>University of Padua</affiliation></author>
      <pages>17918-17929</pages>
      <abstract>Persuasion (or propaganda) techniques detection is a relatively novel task in Natural Language Processing (NLP). While there have already been a number of annotation campaigns, they have been based on heuristic guidelines, which have never been thoroughly discussed. Here, we present the first systematic analysis of a complex annotation task -detecting 22 persuasion techniques in memes-, for which we provided continuous expert oversight. The presence of an expert allowed us to critically analyze specific aspects of the annotation process. Among our findings, we show that inter-annotator agreement alone inadequately assessed annotation correctness. We thus define and track different error types, revealing that expert feedback shows varying effectiveness across error categories. This pattern suggests that distinct mechanisms underlie different kinds of misannotations. Based on our findings, we advocate for an expert oversight in annotation tasks and periodic quality audits. As an attempt to reduce the costs for this, we introduce a probabilistic model for optimizing intervention scheduling.</abstract>
      <url hash="733b3f99">2025.findings-acl.922</url>
      <bibkey>bassi-etal-2025-annotating</bibkey>
    </paper>
    <paper id="923">
      <title>On the Generalization vs Fidelity Paradox in Knowledge Distillation</title>
      <author><first>Suhas Kamasetty</first><last>Ramesh</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Ayan</first><last>Sengupta</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>17930-17951</pages>
      <abstract>Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to 10%, with a peak task specific gain of 22%, while providing only marginal benefits (<tex-math>\sim 1.3</tex-math>%) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students’ performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.</abstract>
      <url hash="578f8e97">2025.findings-acl.923</url>
      <bibkey>ramesh-etal-2025-generalization</bibkey>
    </paper>
    <paper id="924">
      <title><fixed-case>BEDAA</fixed-case>: <fixed-case>B</fixed-case>ayesian Enhanced <fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a for Uncertainty-Aware Authorship Attribution</title>
      <author><first>Iqra</first><last>Zahid</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Youcheng</first><last>Sun</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Manchester</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>17952-17966</pages>
      <abstract>Authorship Attribution (AA) seeks to identify the author of a given text, yet existing methods often struggle with trustworthiness and interpretability, particularly across different domains, languages, and stylistic variations. These challenges arise from the absence of uncertainty quantification and the inability of current models to adapt to diverse authorship tasks. To address these limitations, we introduce BEDAA, a Bayesian-Enhanced DeBERTa framework that integrates Bayesian reasoning with transformer-based language models to enable uncertainty-aware and interpretable authorship attribution. BEDAA achieves up to 19.69% improvement in F1-score across multiple authorship attribution tasks, including binary, multiclass, and dynamic authorship detection. By incorporating confidence ranking, uncertainty decomposition, and probabilistic reasoning, BEDAA improves robustness while offering transparent decision-making processes. Furthermore, BEDAA extends beyond traditional AA by demonstrating its effectiveness in human vs. machine-generated text classification, code authorship detection, and cross-lingual attribution. These advances establish BEDAA as a generalised, interpretable, and adaptable framework for modern authorship attribution challenges.</abstract>
      <url hash="6c1c167c">2025.findings-acl.924</url>
      <bibkey>zahid-etal-2025-bedaa</bibkey>
    </paper>
    <paper id="925">
      <title>Benchmarking the Benchmarks: Reproducing Climate-Related <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Tom</first><last>Calamai</last></author>
      <author><first>Oana</first><last>Balalau</last><affiliation>INRIA</affiliation></author>
      <author><first>Fabian M.</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>17967-18009</pages>
      <abstract>Significant efforts have been made in the NLP community to facilitate the automatic analysis of climate-related corpora by tasks such as climate-related topic detection, climate risk classification, question answering over climate topics, and many more. In this work, we perform a reproducibility study on 8 tasks and 29 datasets, testing 6 models. We find that many tasks rely heavily on surface-level keyword patterns rather than deeper semantic or contextual understanding. Moreover, we find that 96% of the datasets contain annotation issues, with 16.6% of the sampled wrong predictions of a zero-shot classifier being actually clear annotation mistakes, and 38.8% being ambiguous examples.These results call into question the reliability of current benchmarks to meaningfully compare models and highlight the need for improved annotation practices. We conclude by outlining actionable recommendations to enhance dataset quality and evaluation robustness.</abstract>
      <url hash="31907cfe">2025.findings-acl.925</url>
      <bibkey>calamai-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="926">
      <title>Exploring Supervised Approaches to the Detection of Anthropomorphic Language in the Reporting of <fixed-case>NLP</fixed-case> Venues</title>
      <author><first>Matthew</first><last>Shardlow</last><affiliation>The Manchester Metropolitan University</affiliation></author>
      <author><first>Ashley</first><last>Williams</last><affiliation>The Manchester Metropolitan University</affiliation></author>
      <author><first>Charlie</first><last>Roadhouse</last></author>
      <author><first>Filippos</first><last>Ventirozos</last><affiliation>The Manchester Metropolitan University</affiliation></author>
      <author><first>Piotr</first><last>Przybyła</last></author>
      <pages>18010-18022</pages>
      <abstract>We investigate the prevalence of anthropomorphic language in the reporting of AI technology, focussed on NLP and LLMs. We undertake a corpus annotation focussing on one year of ACL long-paper abstracts and news articles from the same period. We find that 74% of ACL abstracts and 88% of news articles contain some form of anthropomorphic description of AI technology. Further, we train a regression classifier based on BERT, demonstrating that we can automatically label abstracts for their degree of anthropomorphism based on our corpus. We conclude by applying this labelling process to abstracts available in the entire history of the ACL Anthology and reporting on diachronic and inter-venue findings, showing that the degree of anthropomorphism is increasing at all examined venues over time.</abstract>
      <url hash="15f77b9e">2025.findings-acl.926</url>
      <bibkey>shardlow-etal-2025-exploring</bibkey>
    </paper>
    <paper id="927">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>L</fixed-case>ens: A Benchmark for Personalization Evaluation in Conversational <fixed-case>AI</fixed-case> Assistants</title>
      <author><first>Zheng</first><last>Zhao</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Clara</first><last>Vania</last><affiliation>Amazon</affiliation></author>
      <author><first>Subhradeep</first><last>Kayal</last></author>
      <author><first>Naila</first><last>Khan</last><affiliation>Amazon</affiliation></author>
      <author><first>Shay B</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>18023-18055</pages>
      <abstract>Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization—adapting to individual user preferences while completing tasks—remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.</abstract>
      <url hash="5cbe89fb">2025.findings-acl.927</url>
      <bibkey>zhao-etal-2025-personalens</bibkey>
    </paper>
    <paper id="928">
      <title>i<fixed-case>A</fixed-case>gent: <fixed-case>LLM</fixed-case> Agent as a Shield between User and Recommender Systems</title>
      <author><first>Wujiang</first><last>Xu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yunxiao</first><last>Shi</last></author>
      <author><first>Zujie</first><last>Liang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xuying</first><last>Ning</last></author>
      <author><first>Kai</first><last>Mei</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Xi</first><last>Zhu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Min</first><last>Xu</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>18056-18084</pages>
      <abstract>Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform’s recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform’s benefits, which may hinder their ability to protect and capture users’ true interests. Second, these models are typically optimized using data from all users, which may overlook individual user’s preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as InstructRec, along with user instructions for each record. To understand user’s intention, we design an Instruction-aware Agent capable of using tools to acquire knowledge from external environments. Moreover, we introduce an Individual Instruction-aware Agent, which incorporates a dynamic memory mechanism to optimize from individual feedback. Results on four datasets demonstrate that consistently achieves an average improvement of 16.6% over SOTA baselines across ranking metrics. Moreover, iAgent mitigates echo chamber effects and effectively alleviates the model bias in disadvantaged users (less-active), serving as a shield between user and recommender systems.</abstract>
      <url hash="25e46a94">2025.findings-acl.928</url>
      <bibkey>xu-etal-2025-iagent</bibkey>
    </paper>
    <paper id="929">
      <title><fixed-case>F</fixed-case>act<fixed-case>L</fixed-case>ens: Benchmarking Fine-Grained Fact Verification</title>
      <author><first>Kushan</first><last>Mitra</last><affiliation>Megagon Labs</affiliation></author>
      <author id="dan-zhang"><first>Dan</first><last>Zhang</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Sajjadur</first><last>Rahman</last></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>18085-18096</pages>
      <abstract>Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce **FactLens**, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.</abstract>
      <url hash="9ebdd511">2025.findings-acl.929</url>
      <bibkey>mitra-etal-2025-factlens</bibkey>
    </paper>
    <paper id="930">
      <title>Process-based Self-Rewarding Language Models</title>
      <author><first>Shimao</first><last>Zhang</last></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Junxiao</first><last>Liu</last></author>
      <author><first>Zheheng</first><last>Luo</last></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <pages>18097-18110</pages>
      <abstract>Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs’ performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of process-based self-rewarding to achieve LLM reasoning that may surpass human capabilities.</abstract>
      <url hash="d540501e">2025.findings-acl.930</url>
      <bibkey>zhang-etal-2025-process</bibkey>
    </paper>
    <paper id="931">
      <title>The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks</title>
      <author><first>Benedikt</first><last>Ebing</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>18111-18128</pages>
      <abstract>Translation-based strategies for cross-lingual transfer XLT such as translate-train—training on noisy target language data translated from the source language—and translate-test—evaluating on noisy source language data translated from the target language—are competitive XLT baselines. In XLT for token classification tasks, however, these strategies include label projection, the challenging step of mapping the labels from each token in the original sentence to its counterpart(s) in the translation. Although word aligners (WAs) are commonly used for label projection, the low-level design decisions for applying them to translation-based XLT have not been systematically investigated. Moreover, recent marker-based methods, which project labeled spans by inserting tags around them before (or after) translation, claim to outperform WAs in label projection for XLT. In this work, we revisit WAs for label projection, systematically investigating the effects of low-level design decisions on token-level XLT: (i) the algorithm for projecting labels between (multi-)token spans, (ii) filtering strategies to reduce the number of noisily mapped labels, and (iii) the pre-tokenization of the translated sentences. We find that all of these substantially impact translation-based XLT performance and show that, with optimized choices, XLT with WA offers performance at least comparable to that of marker-based methods. We then introduce a new projection strategy that ensembles translate-train and translate-test predictions and demonstrate that it substantially outperforms the marker-based projection. Crucially, we show that our proposed ensembling also reduces sensitivity to low-level WA design choices, resulting in more robust XLT for token classification tasks.</abstract>
      <url hash="c97513bc">2025.findings-acl.931</url>
      <bibkey>ebing-glavas-2025-devil</bibkey>
    </paper>
    <paper id="932">
      <title><fixed-case>S</fixed-case>hield<fixed-case>H</fixed-case>ead: Decoding-time Safeguard for Large Language Models</title>
      <author><first>Zitao</first><last>Xuan</last></author>
      <author><first>Xiaofeng</first><last>Mao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Da</first><last>Chen</last><affiliation>University of Bath</affiliation></author>
      <author><first>Xin</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yuhan</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <pages>18129-18143</pages>
      <abstract>In light of the widespread deployment of Large Language Models (LLMs), the responsibility for safeguarding and regulating LLM-generated content has taken on heightened significance. Recent advancements in LLM-based moderation methods, e.g., LlamaGuard, have demonstrated remarkable promise in identifying safety risks associated with both inputs and outputs in human-AI interactions. However, integrating LLM-based safeguards into a chatbot system requires an additional inference stage involving a moderation LLM with billions of parameters, which significantly increases computational costs and reduces overall efficiency. In this paper, we demonstrate that simply learning a classification head on the last-layer hidden states of the dialogue model provides a strong capability to identify harmful contents. The classification head, referred to as ShieldHead, serves as an auxiliary branch paralleled with next-token-prediction LM head, enabling the detection of potential risks in past text sequences. Additionally, a label disambiguation technique is employed to supervise ShieldHead with both token-level and sentence-level labels, which further enhances its performance. ShieldHead exhibits remarkable efficiency during inference, providing real-time moderation results alongside token-wise streaming output during the chatbot system’s decoding phase. Extensive experimental results demonstrate the superiority of the proposed framework: a state-of-the-art performance on the XSTest and SafeRLHF datasets while running at a speed about **300×** faster (**&lt;1ms**) than previous LLM-based moderation models with ** 99%** less parameters of LlamaGuard.</abstract>
      <url hash="abd481ea">2025.findings-acl.932</url>
      <bibkey>xuan-etal-2025-shieldhead</bibkey>
    </paper>
    <paper id="933">
      <title>A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models</title>
      <author><first>Shuliang</first><last>Liu</last></author>
      <author><first>Hongyi</first><last>Liu</last></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Duan</first><last>Bingchen</last><affiliation>Northeast Forest University</affiliation></author>
      <author><first>Zheng</first><last>Qi</last></author>
      <author><first>Yibo</first><last>Yan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>He</first><last>Geng</last></author>
      <author><first>Peijie</first><last>Jiang</last></author>
      <author><first>Jia</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <pages>18144-18155</pages>
      <abstract>The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.</abstract>
      <url hash="3277a001">2025.findings-acl.933</url>
      <bibkey>liu-etal-2025-survey</bibkey>
    </paper>
    <paper id="934">
      <title>Smotrom tvoja på ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study</title>
      <author><first>Alexey</first><last>Tikhonov</last><affiliation>Inworld AI</affiliation></author>
      <author><first>Sergei</first><last>Shteiner</last><affiliation>Independent</affiliation></author>
      <author><first>Anna</first><last>Bykova</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Ivan P.</first><last>Yamshchikov</last><affiliation>Technical University of Applied Sciences Würzburg-Schweinfurt</affiliation></author>
      <pages>18156-18166</pages>
      <abstract>Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a “reconstruction” translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.</abstract>
      <url hash="07db9846">2025.findings-acl.934</url>
      <bibkey>tikhonov-etal-2025-smotrom</bibkey>
    </paper>
    <paper id="935">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <pages>18167-18188</pages>
      <abstract>The ability of large language models to solve complex mathematical problems has progressed significantly, particularly for tasks requiring advanced reasoning. However, the scarcity of sufficiently challenging problems, particularly at the Olympiad level, hinders further advancements. In this work, we introduce PromptCoT, a novel approach for automatically generating high-quality Olympiad-level math problems. The proposed method synthesizes complex problems based on mathematical concepts and the rationale behind problem construction, emulating the thought processes of experienced problem designers. We provide a theoretical analysis demonstrating that an optimal rationale should maximize both the likelihood of rationale generation given the associated concepts and the likelihood of problem generation conditioned on both the rationale and the concepts. Our method is evaluated on standard benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently outperforms existing problem generation methods. Furthermore, we demonstrate that PromptCoT exhibits superior scalability, consistently maintaining high performance as the dataset size increases, outperforming the baselines.</abstract>
      <url hash="fb252c76">2025.findings-acl.935</url>
      <bibkey>zhao-etal-2025-promptcot</bibkey>
    </paper>
    <paper id="936">
      <title>Speculative Sampling via Exponential Races</title>
      <author><first>Szymon</first><last>Kobus</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Deniz</first><last>Gunduz</last><affiliation>Imperial College London</affiliation></author>
      <pages>18189-18204</pages>
      <abstract>Speculative decoding accelerates large language model inference using a smaller draft model. In this paper, we establish a surprising connection between speculative sampling and the concept of channel simulation from information theory, which aims at simulating a noisy channel using as few bits as possible. This connection allows us to provide an information-theoretic analysis of the speed up that can be achieved by speculative sampling. Leveraging this link, we derive an explicit relation between generation speed-up and the number of tokens <tex-math>k</tex-math> generated by the draft model for large <tex-math>k</tex-math>, which serves as an upper bound for all <tex-math>k</tex-math>. We also propose a novel speculative sampling method via exponential races called ERSS that matches state-of-the-art performance.</abstract>
      <url hash="3ad33ba7">2025.findings-acl.936</url>
      <bibkey>kobus-gunduz-2025-speculative</bibkey>
    </paper>
    <paper id="937">
      <title>Going Beyond Your Expectations in Latency Metrics for Simultaneous Speech Translation</title>
      <author><first>Jorge</first><last>Iranzo-Sánchez</last><affiliation>Universidad Politécnica de Valencia</affiliation></author>
      <author><first>Javier</first><last>Iranzo-Sánchez</last><affiliation>AppTek</affiliation></author>
      <author><first>Adrià</first><last>Giménez</last><affiliation>Universitat de València</affiliation></author>
      <author><first>Jorge</first><last>Civera</last><affiliation>Universidad Politécnica de Valencia</affiliation></author>
      <pages>18205-18228</pages>
      <abstract>Current evaluation practices in Simultaneous Speech Translation (SimulST) systems typically involve segmenting the input audio and corresponding translations, calculating quality and latency metrics for each segment, and averaging the results. Although this approach may provide a reliable estimation of translation quality, it can lead to misleading values of latency metrics due to an inherent assumption that average latency values are good enough estimators of SimulST systems’ response time. However, our detailed analysis of latency evaluations for state-of-the-art SimulST systems demonstrates that latency distributions are often skewed and subject to extreme variations. As a result, the mean in latency metrics fails to capture these anomalies, potentially masking the lack of robustness in some systems and metrics. In this paper, a thorough analysis of the results of systems submitted to recent editions of the IWSLT simultaneous track is provided to support our hypothesis and alternative ways to report latency metrics are proposed in order to provide a better understanding of SimulST systems’ latency.</abstract>
      <url hash="e728992f">2025.findings-acl.937</url>
      <bibkey>iranzo-sanchez-etal-2025-going</bibkey>
    </paper>
    <paper id="938">
      <title>Towards a Design Guideline for <fixed-case>RPA</fixed-case> Evaluation: A Survey of Large Language Model-Based Role-Playing Agents</title>
      <author><first>Chaoran</first><last>Chen</last></author>
      <author><first>Bingsheng</first><last>Yao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Ruishi</first><last>Zou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Toby Jia-Jun</first><last>Li</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Dakuo</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <pages>18229-18268</pages>
      <abstract>Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs.This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024.Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature.Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.</abstract>
      <url hash="60cec962">2025.findings-acl.938</url>
      <bibkey>chen-etal-2025-towards-design</bibkey>
    </paper>
    <paper id="939">
      <title>Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data</title>
      <author><first>Philipp</first><last>Christmann</last></author>
      <author><first>Gerhard</first><last>Weikum</last><affiliation>Max-Planck Institute for Informatics and Max Planck Institute</affiliation></author>
      <pages>18269-18288</pages>
      <abstract>Question answering over mixed sources, like text and tables, has been advanced by verbalizing all contents and encoding it with a language model. A prominent case of such heterogeneous data is personal information: user devices log vast amounts of data every day, such as calendar entries, workout statistics, shopping records, streaming history, and more. Information needs range from simple look-ups to queries of analytical nature. The challenge is to provide humans with convenient access with small footprint, so that all personal data stays on the user devices. We present ReQAP, a novel method that creates an executable operator tree for a given question, via recursive decomposition. Operators are designed to enable seamless integration of structured and unstructured sources, and the execution of the operator tree yields a traceable answer. We further release the PerQA benchmark, with persona-based data and questions, covering a diverse spectrum of realistic user needs.</abstract>
      <url hash="816be9a6">2025.findings-acl.939</url>
      <bibkey>christmann-weikum-2025-recursive</bibkey>
    </paper>
    <paper id="940">
      <title><fixed-case>P</fixed-case>re<fixed-case>S</fixed-case>umm: Predicting Summarization Performance Without Summarizing</title>
      <author><first>Steven</first><last>Koniaev</last></author>
      <author><first>Ori</first><last>Ernst</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Jackie CK</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>18289-18305</pages>
      <abstract>Despite recent advancements in automatic summarization, state-of-the-art models do not summarize all documents equally well, raising the question: why? While prior research has extensively analyzed summarization models, little attention has been given to the role of document characteristics in influencing summarization performance.In this work, we explore two key research questions. First, do documents exhibit consistent summarization quality across multiple systems? If so, can we predict a document’s summarization performance without generating a summary? We answer both questions affirmatively and introduce PreSumm, a novel task in which a system predicts summarization performance based solely on the source document. Our analysis sheds light on common properties of documents with low PreSumm scores, revealing that they often suffer from coherence issues, complex content, or a lack of a clear main theme.In addition, we demonstrate PreSumm’s practical utility in two key applications: improving hybrid summarization workflows by identifying documents that require manual summarization and enhancing dataset quality by filtering outliers and noisy documents.Overall, our findings highlight the critical role of document properties in summarization performance and offer insights into the limitations of current systems that could serve as the basis for future improvements.</abstract>
      <url hash="da617d85">2025.findings-acl.940</url>
      <bibkey>koniaev-etal-2025-presumm</bibkey>
    </paper>
    <paper id="941">
      <title>Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases</title>
      <author><first>Yongjia</first><last>Lei</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Haoyu</first><last>Han</last></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Nedim</first><last>Lipka</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Mahantesh M</first><last>Halappanavar</last></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>University of Oregon and Vanderbilt University</affiliation></author>
      <pages>18306-18321</pages>
      <abstract>Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and existing hybrid methods even bypass structural retrieval entirely. To fill this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with inspiring insights, including imbalanced retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking.</abstract>
      <url hash="6d08ae0b">2025.findings-acl.941</url>
      <bibkey>lei-etal-2025-mixture</bibkey>
    </paper>
    <paper id="942">
      <title>Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</title>
      <author><first>Denitsa</first><last>Saynova</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Lovisa</first><last>Hagström</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Moa</first><last>Johansson</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Richard</first><last>Johansson</last><affiliation>Chalmers University of Technology and Göteborg University</affiliation></author>
      <author><first>Marco</first><last>Kuhlmann</last><affiliation>Linköping University</affiliation></author>
      <pages>18322-18349</pages>
      <abstract>Language models (LMs) can make a correct prediction based on many possible signals in a prompt, not all corresponding to recall of factual associations. However, current interpretations of LMs fail to take this into account. For example, given the query “Astrid Lindgren was born in” with the corresponding completion “Sweden”, no difference is made between whether the prediction was based on knowing where the author was born or assuming that a person with a Swedish-sounding name was born in Sweden. In this paper, we present a model-specific recipe - PrISM - for constructing datasets with examples of four different prediction scenarios: generic language modeling, guesswork, heuristics recall and exact fact recall. We apply two popular interpretability methods to the scenarios: causal tracing (CT) and information flow analysis. We find that both yield distinct results for each scenario. Results for exact fact recall and generic language modeling scenarios confirm previous conclusions about the importance of mid-range MLP sublayers for fact recall, while results for guesswork and heuristics indicate a critical role of late last token position MLP sublayers. In summary, we contribute resources for a more extensive and granular study of fact completion in LMs, together with analyses that provide a more nuanced understanding of how LMs process fact-related queries.</abstract>
      <url hash="beaae62b">2025.findings-acl.942</url>
      <bibkey>saynova-etal-2025-fact</bibkey>
    </paper>
    <paper id="943">
      <title><fixed-case>FPE</fixed-case>2<fixed-case>M</fixed-case>2: Approaching Lossless and Efficient Quantization with Native Floating Point</title>
      <author><first>Ke</first><last>Yi</last></author>
      <author><first>Jianwei</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhiying</first><last>Xu</last></author>
      <author><first>Xinlong</first><last>Yang</last></author>
      <author><first>Yang</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Minmin</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zengke</first><last>Liu</last><affiliation>Institute of Software, Chinese Academy of Sciences and University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>18350-18361</pages>
      <abstract>Auto-regressive decoding is a memory-bound job, meaning decoding inference performance is limited by the bandwidth rather than the computational capabilities of the GPU. Weight-only quantization is a promising method to address the memory-bound limitations. Previous studies have followed one of two approaches. Some have exclusively studied integer quantization while ignoring the Gaussian distribution nature of LLMs’ weights. Others have proposed non-uniform quantization but incurred additional I/O overhead due to lookup tables, e.g. NF4. In this work, we extend the IEEE 754 float-point standard to the ExMy quantization schema, which allocates x bit for the exponent and y bit for the mantissa to represent a number. In terms of runtime efficiency, we demonstrate that the conversion from ExMy to FP16 can be realized through register-level operations, which can get almost the same performance as INT5. In terms of quantization loss, we analyze that of different ExMy settings, where the E2M2 schema achieves an optimal balance, offering the highest efficiency with lossless accuracy. We further propose the FPE2M2 framework that supports lossless weight-only quantization inference and validate the FPE2M2 framework on Qwen and LLaMA Models across various modalities, such as text, image, and audio tasks, which achieves a faster inference speed while maintaining nearly lossless accuracy.</abstract>
      <url hash="a1130e74">2025.findings-acl.943</url>
      <bibkey>yi-etal-2025-fpe2m2</bibkey>
    </paper>
    <paper id="944">
      <title>Asymmetric Conflict and Synergy in Post-training for <fixed-case>LLM</fixed-case>-based Multilingual Machine Translation</title>
      <author><first>Tong</first><last>Zheng</last></author>
      <author><first>Yan</first><last>Wen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Huiwen</first><last>Bao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Junfeng</first><last>Guo</last></author>
      <author><first>Heng</first><last>Huang</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <pages>18362-18383</pages>
      <abstract>The emergence of Large Language Models (LLMs) has advanced the multilingual machine translation (MMT), yet the Curse of Multilinguality (CoM) remains a major challenge. Existing work in LLM-based MMT typically mitigates this issue via scaling up training and computation budget, which raises a critical question: Is scaling up the training and computation budget truly necessary for high-quality MMT, or can a deeper understanding of CoM provide a more efficient solution? To explore this problem, we analyze the linguistic conflicts and synergy, the underlying mechanism of CoM during post-training phase. We identify an asymmetric phenomenon in linguistic conflicts and synergy: the dominance of conflicts and synergy varies in different translation directions, leading to sub-optimal adaptation in existing post-training methods. We further find that a significant bottleneck in MMT appears to lie in post-training rather than multilingual pre-training, suggesting the need for more effective adaptation strategies. Building on these new insights, we propose a direction-aware training approach, combined with group-wise model merging, to address asymmetry in linguistic conflicts and synergy explicitly. Leveraging this strategy, our method fine-tunes X-ALMA-13B-Pretrain—trained only with multilingual pre-training—achieving comparable performance to XALMA-13B (only SFT) while using only 20B pretraining tokens and 17B parameters—5.5× fewer pretraining-tokens and 1.7x fewer model size—with just 0.85 COMET drop on Flores-200 testsets of 50 languages.</abstract>
      <url hash="d2340c48">2025.findings-acl.944</url>
      <bibkey>zheng-etal-2025-asymmetric</bibkey>
    </paper>
    <paper id="945">
      <title><fixed-case>VISIAR</fixed-case>: Empower <fixed-case>MLLM</fixed-case> for Visual Story Ideation</title>
      <author><first>Zhaoyang</first><last>Xia</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Somdeb</first><last>Sarkhel</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Mehrab</first><last>Tanjim</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Stefano</first><last>Petrangeli</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ishita</first><last>Dasgupta</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Yuxiao</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinxuan</first><last>Xu</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Di</first><last>Liu</last></author>
      <author><first>Saayan</first><last>Mitra</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Dimitris N.</first><last>Metaxas</last><affiliation>Rutgers University</affiliation></author>
      <pages>18384-18402</pages>
      <abstract>Ideation, the process of forming ideas from concepts, is a big part of the content creation process. However, the noble goal of helping visual content creators by suggesting meaningful sequences of visual assets from a limited collection is challenging. It requires a nuanced understanding of visual assets and the integration of open-world knowledge to support creative exploration. Despite its importance, this task has yet to be explored fully in existing literature. To fill this gap, we propose Visual Story Ideation, a novel and underexplored task focused on the automated selection and arrangement of visual assets into coherent sequences that convey expressive storylines.We also present VISIAR, Visual Ideation through Sequence Integration and Asset Rearrangement, a robust framework leveraging Multimodal Large Language Models (MLLMs), and a novel Story Graph mechanism. Our framework operates in three key stages: visual content understanding, candidate asset selection, and asset rearrangement via MLLMs. In addition, we curated a new benchmark dataset, called VTravel, to evaluate our methods both qualitatively and quantitatively.User studies and GPT-as-the-judge evaluation show that our approach surpasses GPT-4o based baseline by an average of 33.5% and 18.5% across three different metrics, demonstrating the effectiveness of our framework for generating compelling visual stories.</abstract>
      <url hash="8d43eb35">2025.findings-acl.945</url>
      <bibkey>xia-etal-2025-visiar</bibkey>
    </paper>
    <paper id="946">
      <title>Same Company, Same Signal: The Role of Identity in Earnings Call Transcripts</title>
      <author><first>Ding</first><last>Yu</last></author>
      <author><first>Zhuo</first><last>Liu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <pages>18403-18422</pages>
      <abstract>Post-earnings volatility prediction is critical for investors, with previous works often leveraging earnings call transcripts under the assumption that their rich semantics contribute significantly. To further investigate how transcripts impact volatility, we introduce DEC, a dataset featuring accurate volatility calculations enabled by the previously overlooked beforeAfterMarket attribute and dense ticker coverage. Unlike established benchmarks, where each ticker has only around two earnings, DEC provides 20 earnings records per ticker. Using DEC, we reveal that post-earnings volatility undergoes significant shifts, with each ticker displaying a distinct volatility distribution. To leverage historical post-earnings volatility and capture ticker-specific patterns, we propose two training-free baselines: Post-earnings Volatility (PEV) and Same-ticker Post-earnings Volatility (STPEV). These baselines surpass all transcripts-based models on DEC as well as on established benchmarks. Additionally, we demonstrate that current transcript representations predominantly capture ticker identity rather than offering financially meaningful insights specific to each earnings. This is evidenced by two key observations: earnings representations from the same ticker exhibit significantly higher similarity compared to those from different tickers, and predictions from transcript-based models show strong correlations with prior post-earnings volatility.</abstract>
      <url hash="83c5e4cc">2025.findings-acl.946</url>
      <bibkey>yu-etal-2025-company</bibkey>
    </paper>
    <paper id="947">
      <title>Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by <fixed-case>LLM</fixed-case>-Based Systems</title>
      <author><first>Emma</first><last>Harvey</last><affiliation>Cornell University</affiliation></author>
      <author><first>Emily</first><last>Sheng</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alexandra</first><last>Chouldechova</last><affiliation>Microsoft and Carnegie Mellon University</affiliation></author>
      <author><first>Jean</first><last>Garcia-Gathright</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Alexandra</first><last>Olteanu</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Hanna</first><last>Wallach</last><affiliation>Microsoft</affiliation></author>
      <pages>18423-18440</pages>
      <abstract>The NLP research community has made publicly available numerous instruments for measuring representational harms caused by large language model (LLM)-based systems. These instruments have taken the form of datasets, metrics, tools, and more. In this paper, we examine the extent to which such instruments meet the needs of practitioners tasked with evaluating LLM-based systems. Via semi-structured interviews with 12 such practitioners, we find that practitioners are often unable to use publicly available instruments for measuring representational harms. We identify two types of challenges. In some cases, instruments are not useful because they do not meaningfully measure what practitioners seek to measure or are otherwise misaligned with practitioner needs. In other cases, instruments-even useful instruments-are not used by practitioners due to practical and institutional barriers impeding their uptake. Drawing on measurement theory and pragmatic measurement, we provide recommendations for addressing these challenges to better meet practitioner needs.</abstract>
      <url hash="6a2d7e88">2025.findings-acl.947</url>
      <bibkey>harvey-etal-2025-understanding</bibkey>
    </paper>
    <paper id="948">
      <title>Mind the (Belief) Gap: Group Identity in the World of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Angana</first><last>Borah</last></author>
      <author><first>Marwa</first><last>Houalla</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>18441-18463</pages>
      <abstract>Social biases and belief-driven behaviors can significantly impact Large Language Models’ (LLMs’) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to (37%) and enhance learning by (11%). Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.</abstract>
      <url hash="5dae51d2">2025.findings-acl.948</url>
      <bibkey>borah-etal-2025-mind</bibkey>
    </paper>
    <paper id="949">
      <title>A General Framework to Enhance Fine-tuning-based <fixed-case>LLM</fixed-case> Unlearning</title>
      <author><first>Jie</first><last>Ren</last><affiliation>Amazon and Michigan State University</affiliation></author>
      <author><first>Zhenwei</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jingying</first><last>Zeng</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Zhen</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Goutam</last></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <pages>18464-18476</pages>
      <abstract>Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations—essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.</abstract>
      <url hash="2f5cb077">2025.findings-acl.949</url>
      <bibkey>ren-etal-2025-general</bibkey>
    </paper>
    <paper id="950">
      <title>Right Answer, Wrong Score: Uncovering the Inconsistencies of <fixed-case>LLM</fixed-case> Evaluation in Multiple-Choice Question Answering</title>
      <author><first>Francesco Maria</first><last>Molfese</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Luca</first><last>Moroni</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Luca</first><last>Gioffré</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Alessandro</first><last>Scirè</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Simone</first><last>Conia</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>18477-18494</pages>
      <abstract>One of the most widely used tasks for evaluating Large Language Models (LLMs) is Multiple-Choice Question Answering (MCQA). While open-ended question answering tasks are more challenging to evaluate, MCQA tasks are, in principle, easier to assess, as the model’s answer is thought to be simple to extract and is compared directly to a set of predefined choices. However, recent studies have started to question the reliability of MCQA evaluation, showing that multiple factors can significantly impact the reported performance of LLMs, especially when the model generates free-form text before selecting one of the answer choices. In this work, we shed light on the inconsistencies of MCQA evaluation strategies, which can lead to inaccurate and misleading model comparisons. We systematically analyze whether existing answer extraction methods are aligned with human judgment, and how they are influenced by answer constraints in the prompt across different domains. Our experiments demonstrate that traditional evaluation strategies often underestimate LLM capabilities, while LLM-based answer extractors are prone to systematic errors. Moreover, we reveal a fundamental trade-off between including format constraints in the prompt to simplify answer extraction and allowing models to generate free-form text to improve reasoning. Our findings call for standardized evaluation methodologies and highlight the need for more reliable and consistent MCQA evaluation practices.</abstract>
      <url hash="3eb423ea">2025.findings-acl.950</url>
      <bibkey>molfese-etal-2025-right</bibkey>
    </paper>
    <paper id="951">
      <title>Machine Theory of Mind Needs Machine Validation</title>
      <author><first>Adil</first><last>Soubki</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>18495-18505</pages>
      <abstract>In the last couple years, there has been a flood of interest in studying the extent to which language models (LMs) have a theory of mind (ToM) — the ability to ascribe mental states to themselves and others. The results provide an unclear picture of the current state of the art, with some finding near-human performance and others near-zero. To make sense of this landscape, we perform a survey of 16 recent studies aimed at measuring ToM in LMs and find that, while almost all perform checks for human identifiable issues, less than half do so for patterns only a machine might exploit. Among those that do perform such validation, which we call machine validation, none identify LMs to exceed human performance. We conclude that the datasets that show high LM performance on ToM tasks are easier than their peers, likely due to the presence of spurious patterns in the data, and we caution against building ToM benchmarks relying solely on human validation of the data.</abstract>
      <url hash="e910efd2">2025.findings-acl.951</url>
      <bibkey>soubki-rambow-2025-machine</bibkey>
    </paper>
    <paper id="952">
      <title><fixed-case>M</fixed-case>ini<fixed-case>KV</fixed-case>: Pushing the Limits of 2-Bit <fixed-case>KV</fixed-case> Cache via Compression and System Co-Design for Efficient Long Context Inference</title>
      <author><first>Akshat</first><last>Sharma</last></author>
      <author><first>Hangliang</first><last>Ding</last></author>
      <author><first>Jianping</first><last>Li</last></author>
      <author><first>Neel</first><last>Dani</last></author>
      <author><first>Minjia</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>18506-18523</pages>
      <abstract>State-of-the-art 2-bit KV cache quantization techniques achieve excellent results in accelerating LLM inference while retaining accuracy on long context tasks. However, further pushing the compression ratio fails to deliver performance gains. In this work, we revisit these approaches by considering, additionally, adaptive KV methods that retain LLM accuracy with only a subset of KV states. This leads us to propose a method based on 2-bit KV cache quantization with adaptive KV policies. In addition, we take an algorithm and system co-design approach by developing hardware-friendly kernels to accelerate LLM inference while making MiniKV compatible with existing memory-efficient attention techniques such as FlashAttention, effectively translating algorithmic improvements into system performance gains. Experiments on a wide range of long context tasks show that MiniKV effectively achieves &gt;80% KV cache compression while retaining accuracy, outperforming state-of-the-art methods while achieving excellent latency, throughput, and memory consumption improvements in long context inference.</abstract>
      <url hash="189e5911">2025.findings-acl.952</url>
      <bibkey>sharma-etal-2025-minikv</bibkey>
    </paper>
    <paper id="953">
      <title>Sci-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Mixture of Scientific <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>s for Cross-Domain Lay Paraphrasing</title>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Jiaying</first><last>Gong</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Hoda</first><last>Eldardiry</last><affiliation>, Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>18524-18541</pages>
      <abstract>Lay paraphrasing aims to make scientific information accessible to audiences without technical backgrounds. However, most existing studies focus on a single domain, such as biomedicine. With the rise of interdisciplinary research, it is increasingly necessary to comprehend knowledge spanning multiple technical fields. To address this, we propose Sci-LoRA, a model that leverages a mixture of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA dynamically generates and applies weights for each LoRA, enabling it to adjust the impact of different domains based on the input text, without requiring explicit domain labels. To balance domain-specific knowledge and generalization across various domains, Sci-LoRA integrates information at both the data and model levels. This dynamic fusion enhances the adaptability and performance across various domains. Experimental results across twelve domains on five public datasets show that Sci-LoRA significantly outperforms state-of-the-art large language models and demonstrates flexible generalization and adaptability in cross-domain lay paraphrasing.</abstract>
      <url hash="c9b9c1bb">2025.findings-acl.953</url>
      <bibkey>cheng-etal-2025-sci</bibkey>
    </paper>
    <paper id="954">
      <title>Trick or Neat: Adversarial Ambiguity and Language Model Evaluation</title>
      <author><first>Antonia</first><last>Karamolegkou</last></author>
      <author><first>Oliver</first><last>Eberle</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Phillip</first><last>Rust</last></author>
      <author><first>Carina</first><last>Kauf</last></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>18542-18561</pages>
      <abstract>Detecting ambiguity is important for language understanding, including uncertainty estimation, humour detection, and processing garden path sentences. We assess language models’ sensitivity to ambiguity by introducing an adversarial ambiguity dataset that includes syntactic, lexical, and phonological ambiguities along with adversarial variations (e.g., word-order changes, synonym replacements, and random-based alterations). Our findings show that direct prompting fails to robustly identify ambiguity, while linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90%. Our results offer insights into the prompting paradigm and how language models encode ambiguity at different layers.</abstract>
      <url hash="29605982">2025.findings-acl.954</url>
      <bibkey>karamolegkou-etal-2025-trick</bibkey>
    </paper>
    <paper id="955">
      <title>Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes</title>
      <author><first>Kshitish</first><last>Ghate</last></author>
      <author><first>Tessa</first><last>Charlesworth</last><affiliation>Kellogg Community College</affiliation></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Aylin</first><last>Caliskan</last><affiliation>University of Washington</affiliation></author>
      <pages>18562-18580</pages>
      <abstract>To build fair AI systems we need to understand how social-group biases intrinsic to foundational encoder-based vision-language models (VLMs) manifest in biases in downstream tasks. In this study, we demonstrate that intrinsic biases in VLM representations systematically “carry over” or propagate into zero-shot retrieval tasks, revealing how deeply rooted biases shape a model’s outputs. We introduce a controlled framework to measure this propagation by correlating (a) intrinsic measures of bias in the representational space with (b) extrinsic measures of bias in zero-shot text-to-image (TTI) and image-to-text (ITT) retrieval. Results show substantial correlations between intrinsic and extrinsic bias, with an average <tex-math>\rho</tex-math> = 0.83 <tex-math>\pm</tex-math> 0.10. This pattern is consistent across 114 analyses, both retrieval directions, six social groups, and three distinct VLMs. Notably, we find that larger/better-performing models exhibit greater bias propagation, a finding that raises concerns given the trend towards increasingly complex AI models. Our framework introduces baseline evaluation tasks to measure the propagation of group and valence signals. Investigations reveal that underrepresented groups experience less robust propagation, further skewing their model-related outcomes.</abstract>
      <url hash="75cf889e">2025.findings-acl.955</url>
      <bibkey>ghate-etal-2025-biases</bibkey>
    </paper>
    <paper id="956">
      <title>Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models</title>
      <author><first>Yingqian</first><last>Cui</last><affiliation>Amazon and Michigan State University</affiliation></author>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jingying</first><last>Zeng</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhenwei</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Yan</first><last>Han</last><affiliation>Amazon</affiliation></author>
      <author><first>Chen</first><last>Luo</last><affiliation>Amazon</affiliation></author>
      <author><first>Jing</first><last>Huang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhen</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <pages>18581-18597</pages>
      <abstract>Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.</abstract>
      <url hash="84bdad68">2025.findings-acl.956</url>
      <bibkey>cui-etal-2025-stepwise</bibkey>
    </paper>
    <paper id="957">
      <title>Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking <fixed-case>QA</fixed-case> over Scientific Papers</title>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Chengye</first><last>Wang</last></author>
      <author><first>Chuhan</first><last>Li</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>18598-18631</pages>
      <abstract>This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 3,000 expert-annotated examples over 983 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. To ensure reliable and consistent evaluation, we propose an automated evaluating protocol powered by open-source LLMs trained on human-scored data. We assess the performance of 18 frontier multimodal foundation models, including o1, Claude-3.5, Llama-3.2-Vision, and Qwen2-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature.</abstract>
      <url hash="0e1fb822">2025.findings-acl.957</url>
      <bibkey>zhao-etal-2025-multimodal-foundation</bibkey>
    </paper>
    <paper id="958">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>C</fixed-case>hallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kaustubh</first><last>Deshpande</last><affiliation>Scale AI</affiliation></author>
      <author><first>Ved</first><last>Sirdeshmukh</last><affiliation>Scale AI</affiliation></author>
      <author><first>Johannes Baptist</first><last>Mols</last><affiliation>Scale AI</affiliation></author>
      <author><first>Lifeng</first><last>Jin</last><affiliation>Scale AI</affiliation></author>
      <author><first>Ed-Yeremai</first><last>Hernandez-Cardona</last><affiliation>Scale AI</affiliation></author>
      <author><first>Dean</first><last>Lee</last><affiliation>Scale AI</affiliation></author>
      <author><first>Jeremy</first><last>Kritz</last><affiliation>Scale AI</affiliation></author>
      <author><first>Willow E.</first><last>Primack</last><affiliation>Scale AI</affiliation></author>
      <author><first>Summer</first><last>Yue</last><affiliation>Scale AI</affiliation></author>
      <author><first>Chen</first><last>Xing</last><affiliation>Scale AI</affiliation></author>
      <pages>18632-18702</pages>
      <abstract>We present MultiChallenge, a pioneering benchmark evaluating large language models (LLMs) on conducting multi-turn conversations with human users, a crucial yet underexamined capability for their applications. MultiChallenge identifies four categories of challenges in multi-turn conversations that are not only common and realistic among current human-LLM interactions, but are also challenging to all current frontier LLMs. All 4 challenges require accurate instruction-following, context allocation, and in-context reasoning at the same time.We also develop LLM as judge with instance-level rubrics to facilitate an automatic evaluation method with fair agreement with experienced human raters. Despite achieving near perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge, with the top-performing Claude 3.5 Sonnet (October 2024) achieving just a 41.4% average accuracy.</abstract>
      <url hash="d9c58623">2025.findings-acl.958</url>
      <bibkey>deshpande-etal-2025-multichallenge</bibkey>
    </paper>
    <paper id="959">
      <title>Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training</title>
      <author><first>Jaydeep</first><last>Borkar</last></author>
      <author><first>Matthew</first><last>Jagielski</last><affiliation>Google</affiliation></author>
      <author><first>Katherine</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Niloofar</first><last>Mireshghallah</last></author>
      <author><first>David A.</first><last>Smith</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Christopher A.</first><last>Choquette-Choo</last><affiliation>Google DeepMind</affiliation></author>
      <pages>18703-18726</pages>
      <abstract>Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII; and (3) removing PII can lead to other PII being memorized.</abstract>
      <url hash="e3582baa">2025.findings-acl.959</url>
      <bibkey>borkar-etal-2025-privacy</bibkey>
    </paper>
    <paper id="960">
      <title>Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable <fixed-case>LLM</fixed-case> Safety</title>
      <author><first>Yuyou</first><last>Zhang</last></author>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>William</first><last>Han</last></author>
      <author><first>Yihang</first><last>Yao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhepeng</first><last>Cen</last></author>
      <author><first>Ding</first><last>Zhao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>18727-18746</pages>
      <abstract>Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs. While they are effective for direct adversarial attacks, they fall short of broader safety challenges requiring nuanced, context-aware decision-making. To address this, we propose Reasoning-enhanced Fine-Tuning for interpretable LLM Safety (RATIONAL), a novel framework that trains models to engage in explicit safe reasoning before response. Fine-tuned models leverage the extensive pretraining knowledge in self-generated reasoning to bootstrap their own safety through structured reasoning, internalizing context-sensitive decision-making. Our findings suggest that safety extends beyond refusal, requiring context awareness for more robust, interpretable, and adaptive responses. Reasoning is not only a core capability of LLMs but also a fundamental mechanism for LLM safety. RATIONAL employs reasoning-enhanced fine-tuning, allowing it to reject harmful prompts while providing meaningful and context-aware responses in complex scenarios.</abstract>
      <url hash="016d78de">2025.findings-acl.960</url>
      <bibkey>zhang-etal-2025-safety</bibkey>
    </paper>
    <paper id="961">
      <title>Is a cute puyfred cute? Context-dependent form-meaning systematicity in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jaïr A.</first><last>Waal</last><affiliation>Independent</affiliation></author>
      <author><first>Giovanni</first><last>Cassani</last><affiliation>Tilburg University</affiliation></author>
      <pages>18747-18769</pages>
      <abstract>We investigate static and contextualized embeddings for English pseudowords across a variety of Large Language Models (LLMs), to study (i) how these models represent semantic attributes of strings they encounter for the very first time and how (ii) these representations interact with sentence context. We zoom in on a key semantic attribute, valence, which plays an important role in theories of language processing, acquisition, and evolution. Across three experiments, we show that pseudoword valence is encoded in meaningful ways both in isolation and in context, and that, in some LLMs, pseudowords affect the representation of whole sentences similarly to words. This highlights how, at least for most LLMs we surveyed, pseudowords and words are not qualitatively different constructs. Our study confirms that LLMs capture systematic mappings between form and valence, and shows how different LLMs handle the contextualisation of pseudowords differently. Our findings provide a first computational exploration of how sub-lexical distributional patterns influence the valence of novel strings in context, offering useful insights for theories on the form-meaning interface and how it affects language learning and processing.</abstract>
      <url hash="82182497">2025.findings-acl.961</url>
      <bibkey>waal-cassani-2025-cute</bibkey>
    </paper>
    <paper id="962">
      <title><fixed-case>M</fixed-case>eta<fixed-case>S</fixed-case>ynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation</title>
      <author><first>Haris</first><last>Riaz</last></author>
      <author><first>Sourav Sanjukta</first><last>Bhabesh</last><affiliation>Amazon</affiliation></author>
      <author><first>Vinayak</first><last>Arannil</last></author>
      <author><first>Miguel</first><last>Ballesteros</last><affiliation>Oracle</affiliation></author>
      <author><first>Graham</first><last>Horwood</last><affiliation>Amazon</affiliation></author>
      <pages>18770-18803</pages>
      <abstract>Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is <i>low diversity</i>, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple “expert” LLM <i>agents</i> to collaboratively generate data. Using only <b>25 million</b> tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B) to two specialized domains–Finance and Biomedicine–without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.Continually pre-training Mistral-7B with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template-based prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.</abstract>
      <url hash="10d095d0">2025.findings-acl.962</url>
      <bibkey>riaz-etal-2025-metasynth</bibkey>
    </paper>
    <paper id="963">
      <title><fixed-case>MVT</fixed-case>amper<fixed-case>B</fixed-case>ench: Evaluating Robustness of Vision-Language Models</title>
      <author><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author><first>Srikant</first><last>Panda</last><affiliation>Oracle</affiliation></author>
      <author><first>Angeline</first><last>Charles</last></author>
      <author><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <author><first>Bhargava</first><last>Kumar</last><affiliation>TD Securities</affiliation></author>
      <author><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Taki Hasan</first><last>Rafi</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Tejaswini</first><last>Kumar</last><affiliation>Columbia University</affiliation></author>
      <author><first>Hansa</first><last>Meghwani</last></author>
      <author><first>Karan</first><last>Gupta</last><affiliation>Santander Holdings USA</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>18804-18828</pages>
      <abstract>Multimodal Large Language Models (MLLMs), are recent advancement of Vision-Language Models (VLMs) that have driven major advances in video understanding. However, their vulnerability to adversarial tampering and manipulations remains underexplored. To address this gap, we introduce <b>MVTamperBench</b>, a benchmark that systematically evaluates MLLM robustness against five prevalent tampering techniques: rotation, masking, substitution, repetition, and dropping; based on real-world visual tampering scenarios such as surveillance interference, social media content edits, and misinformation injection. MVTamperBench comprises ~3.4K original videos, expanded into over ~17K tampered clips covering 19 distinct video manipulation tasks. This benchmark challenges models to detect manipulations in spatial and temporal coherence. We evaluate 45 recent MLLMs from 15+ model families. We reveal substantial variability in resilience across tampering types and show that larger parameter counts do not necessarily guarantee robustness. MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in safety-critical applications, including detecting clickbait, preventing harmful content distribution, and enforcing policies on media platforms. We release all code, data, and benchmark to foster open research in trustworthy video understanding.</abstract>
      <url hash="7694ea83">2025.findings-acl.963</url>
      <bibkey>agarwal-etal-2025-mvtamperbench</bibkey>
    </paper>
    <paper id="964">
      <title>Multimodal Inconsistency Reasoning (<fixed-case>MMIR</fixed-case>): A New Benchmark for Multimodal Reasoning Models</title>
      <author><first>Qianqi</first><last>Yan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Yue</first><last>Fan</last></author>
      <author><first>Hongquan</first><last>Li</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Shan</first><last>Jiang</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Xinze</first><last>Guan</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Ching-Chen</first><last>Kuo</last></author>
      <author><first>Xin Eric</first><last>Wang</last><affiliation>Simular and University of California, Santa Cruz</affiliation></author>
      <pages>18829-18845</pages>
      <abstract>Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs’ ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate eight state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.</abstract>
      <url hash="51027eb8">2025.findings-acl.964</url>
      <bibkey>yan-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="965">
      <title>Vision-Language Models Struggle to Align Entities across Modalities</title>
      <author><first>Iñigo</first><last>Alonso</last></author>
      <author><first>Gorka</first><last>Azkune</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Ander</first><last>Salaberria</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Jeremy</first><last>Barnes</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Oier Lopez De</first><last>Lacalle</last></author>
      <pages>18846-18862</pages>
      <abstract>Cross-modal entity linking refers to the ability to align entities and their attributes across different modalities. While cross-modal entity linking is a fundamental skill needed for real-world applications such as multimodal code generation, fake news detection, or scene understanding, it has not been thoroughly studied in the literature. In this paper, we introduce a new task and benchmark to address this gap. Our benchmark, MATE, consists of 5.5k evaluation instances featuring visual scenes aligned with their textual representations. To evaluate cross-modal entity linking performance, we design a question-answering task that involves retrieving one attribute of an object in one modality based on a unique attribute of that object in another modality. We evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this task, and find that VLMs struggle significantly compared to humans, particularly as the number of objects in the scene increases. Our analysis also shows that, while chain-of-thought prompting can improve VLM performance, models remain far from achieving human-level proficiency. These findings highlight the need for further research in cross-modal entity linking and show that MATE is a strong benchmark to support that progress.</abstract>
      <url hash="36049263">2025.findings-acl.965</url>
      <bibkey>alonso-etal-2025-vision</bibkey>
    </paper>
    <paper id="966">
      <title>A Multi-Labeled Dataset for <fixed-case>I</fixed-case>ndonesian Discourse: Examining Toxicity, Polarization, and Demographics Information</title>
      <author><first>Lucky</first><last>Susanto</last></author>
      <author><first>Musa Izzanardi</first><last>Wijanarko</last><affiliation>Monash University</affiliation></author>
      <author><first>Prasetia Anugrah</first><last>Pratama</last></author>
      <author><first>Zilu</first><last>Tang</last></author>
      <author><first>Fariz</first><last>Akyas</last><affiliation>Monash University</affiliation></author>
      <author><first>Traci</first><last>Hong</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Ika Karlina</first><last>Idris</last><affiliation>Monash University</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Derry Tanti</first><last>Wijaya</last><affiliation>Monash University and Boston University</affiliation></author>
      <pages>18863-18890</pages>
      <abstract>Online discourse is increasingly trapped in a vicious cycle where polarizing language fuelstoxicity and vice versa. Identity, one of the most divisive issues in modern politics, oftenincreases polarization. Yet, prior NLP research has mostly treated toxicity and polarization asseparate problems. In Indonesia, the world’s third-largest democracy, this dynamic threatens democratic discourse, particularly in online spaces. We argue that polarization and toxicity must be studied in relation to each other. To this end, we present a novel multi-label Indonesian dataset annotated for toxicity, polarization, and annotator demographic information. Benchmarking with BERT-base models and large language models (LLMs) reveals that polarization cues improve toxicity classification and vice versa. Including demographic context further enhances polarization classification performance.</abstract>
      <url hash="f2bf027d">2025.findings-acl.966</url>
      <bibkey>susanto-etal-2025-multi</bibkey>
    </paper>
    <paper id="967">
      <title><fixed-case>M</fixed-case>ed<fixed-case>C</fixed-case>ite: Can Language Models Generate Verifiable Text for Medicine?</title>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Mengjue</first><last>Tan</last></author>
      <author><first>Qiao</first><last>Jin</last></author>
      <author><first>Guangzhi</first><last>Xiong</last></author>
      <author><first>Yu</first><last>Hu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Aidong</first><last>Zhang</last></author>
      <author><first>Zhiyong</first><last>Lu</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Minjia</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>18891-18913</pages>
      <abstract>Existing LLM-based medical question answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce MedCite, the first end-to-end framework that facilitates the design and evaluation of LLM citations for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations.Our extensive evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that our evaluation results correlate well with annotation results from professional experts.</abstract>
      <url hash="c96a73c0">2025.findings-acl.967</url>
      <bibkey>wang-etal-2025-medcite</bibkey>
    </paper>
    <paper id="968">
      <title>Let The Jury Decide: Fair Demonstration Selection for In-Context Learning through Incremental Greedy Evaluation</title>
      <author><first>Sadaf Md</first><last>Halim</last><affiliation>The University of Texas at Dallas</affiliation></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>Baylor University</affiliation></author>
      <author><first>Xintao</first><last>Wu</last></author>
      <author><first>Latifur</first><last>Khan</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Christan</first><last>Grant</last><affiliation>University of Florida</affiliation></author>
      <author><first>Fariha Ishrat</first><last>Rahman</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Feng</first><last>Chen</last><affiliation>University of Texas, Dallas</affiliation></author>
      <pages>18914-18931</pages>
      <abstract>Large Language Models (LLMs) are powerful in-context learners, achieving strong performance with just a few high-quality demonstrations. However, fairness concerns arise in many in-context classification tasks, especially when predictions involve sensitive attributes. To address this, we propose JUDGE—a simple yet effective framework for selecting fair and representative demonstrations that improve group fairness in In-Context Learning. JUDGE constructs the demonstration set iteratively using a greedy approach, guided by a small, carefully selected jury set. Our method remains robust across varying LLM architectures and datasets, ensuring consistent fairness improvements. We evaluate JUDGE on four datasets using four LLMs, comparing it against seven baselines. Results show that JUDGE consistently improves fairness metrics without compromising accuracy.</abstract>
      <url hash="f04609c3">2025.findings-acl.968</url>
      <bibkey>halim-etal-2025-jury</bibkey>
    </paper>
    <paper id="969">
      <title>The Lies Characters Tell: Utilizing Large Language Models to Normalize Adversarial <fixed-case>U</fixed-case>nicode Perturbations</title>
      <author><first>Portia</first><last>Cooper</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>18932-18944</pages>
      <abstract>Homoglyphs, Unicode characters that are visually homogeneous to Latin letters, are widely used to mask offensive content. Dynamic strategies are needed to combat homoglyphs as the Unicode library is ever-expanding and new substitution possibilities for Latin letters continuously emerge. The present study investigated two novel mitigation approaches that do not rely on strict mappings but instead harness the power of large language models to neutralize both known and unknown homoglyphs: (1) indirectly normalizing homoglyphs by replacing non-Latin characters with a delimiter and prompting large language models to “fill in the blanks” and (2) directly normalizing homoglyphs by using large language models to determine which characters should be replaced with Latin letters. We found that GPT-4o-mini constructed normalized text with an average cosine similarity score of 0.91 to the original tweets when applying our indirect method and 0.96 to the original tweets when applying our direct method. This study indicates that large language model-based normalization techniques can effectively unmask offensive content concealed by homoglyphs. Code and data are available in our GitHub repository: https://github.com/pcoopercoder/The-Lies-Characters-Tell.</abstract>
      <url hash="c08f5018">2025.findings-acl.969</url>
      <bibkey>cooper-etal-2025-lies</bibkey>
    </paper>
    <paper id="970">
      <title>Speech Act Patterns for Improving Generalizability of Explainable Politeness Detection Models</title>
      <author><first>Ahmad</first><last>Aljanaideh</last><affiliation>Bentley University</affiliation></author>
      <pages>18945-18954</pages>
      <abstract>The lack of explainability in state-of-the-art Natural Language Understanding (NLU) classification models has increased interest in developing techniques for improving explainable linear feature-based models (e.g., Logistic Regression/SVM). Politeness detection is a task that exemplifies this interest. While those techniques perform well on the task when applied to data from the same domain as the training data, they lack generalizability and thus fall short when applied to data from other domains. This is due to their reliance on discovering domain-specific word-level features. We introduce a method for improving the generalizability of explainable politeness models by relying on speech act patterns instead of words, leveraging speech act labels assigned by the GPT-4 model. This approach goes beyond the mere words and injects intent into politeness classification models, enhancing their generalizability. Results demonstrate that the proposed method achieves state-of-the-art accuracy in the cross-domain setting among explainable methods, while falling short in the in-domain setting. Our findings illustrate that explainable models can benefit from Large Language Models.</abstract>
      <url hash="6942de67">2025.findings-acl.970</url>
      <bibkey>aljanaideh-2025-speech</bibkey>
    </paper>
    <paper id="971">
      <title>Systematic Evaluation of Auto-Encoding and Large Language Model Representations for Capturing Author States and Traits</title>
      <author><first>Khushboo</first><last>Singh</last></author>
      <author><first>Vasudha</first><last>Varadarajan</last></author>
      <author><first>Adithya</first><last>V Ganesan</last><affiliation>, State University of New York, Stony Brook</affiliation></author>
      <author><first>August Håkan</first><last>Nilsson</last></author>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Syeda</first><last>Mahwish</last></author>
      <author><first>Pranav</first><last>Chitale</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Ryan L.</first><last>Boyd</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Richard N</first><last>Rosenthal</last><affiliation>Academic medical center at State University of New York at Stony Brook</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>18955-18973</pages>
      <abstract>Large Language Models (LLMs) are increasingly used in human-centered applications, yet their ability to model diverse psychological constructs is not well understood. In this study, we systematically evaluate a range of Transformer-LMs to predict psychological variables across five major dimensions: affect, substance use, mental health, sociodemographics, and personality. Analyses span three temporal levels—short daily text responses about current affect, text aggregated over two-weeks, and user-level text collected over two years—allowing us to examine how each model’s strengths align with the underlying stability of different constructs. The findings show that mental health signals emerge as the most accurately predicted dimensions (r=0.6) across all temporal scales. At the daily scale, smaller models like DeBERTa and HaRT often performed better, whereas, at longer scales or with greater context, larger model like Llama3-8B performed the best. Also, aggregating text over the entire study period yielded stronger correlations for outcomes, such as age and income. Overall, these results suggest the importance of selecting appropriate model architectures and temporal aggregation techniques based on the stability and nature of the target variable.</abstract>
      <url hash="ae86cf22">2025.findings-acl.971</url>
      <bibkey>singh-etal-2025-systematic</bibkey>
    </paper>
    <paper id="972">
      <title><fixed-case>TR</fixed-case>e<fixed-case>M</fixed-case>u: Towards Neuro-Symbolic Temporal Reasoning for <fixed-case>LLM</fixed-case>-Agents with Memory in Multi-Session Dialogues</title>
      <author><first>Yubin</first><last>Ge</last><affiliation>Amazon</affiliation></author>
      <author><first>Salvatore</first><last>Romeo</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Raphael</first><last>Shu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Monica</first><last>Sunkara</last></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>18974-18988</pages>
      <abstract>Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs <i>time-aware memorization</i> through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate <i>neuro-symbolic temporal reasoning</i>, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.</abstract>
      <url hash="2d3de714">2025.findings-acl.972</url>
      <bibkey>ge-etal-2025-tremu</bibkey>
    </paper>
    <paper id="973">
      <title>Conservative Bias in Large Language Models: Measuring Relation Predictions</title>
      <author><first>Toyin</first><last>Aguda</last></author>
      <author><first>Erik</first><last>Wilson</last></author>
      <author><first>Allan</first><last>Anzagira</last></author>
      <author><first>Simerjot</first><last>Kaur</last><affiliation>JPMorgan Chase and Co</affiliation></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <pages>18989-18998</pages>
      <abstract>Large language models (LLMs) exhibit pronounced conservative bias in relation extraction tasks, frequently defaulting to no_relation label when an appropriate option is unavailable. While this behavior helps prevent incorrect relation assignments, our analysis reveals that it also leads to significant information loss when reasoning is not explicitly included in the output. We systematically evaluate this trade-off across multiple prompts, datasets, and relation types, introducing the concept of Hobson’s choice to capture scenarios where models opt for safe but uninformative labels over hallucinated ones. Our findings suggest that conservative bias occurs twice as often as hallucination. To quantify this effect, we use SBERT and LLM prompts to capture the semantic similarity between conservative bias behaviors in constrained prompts and labels generated from semi-constrained and open-ended prompts.</abstract>
      <url hash="d6aaf7dc">2025.findings-acl.973</url>
      <bibkey>aguda-etal-2025-conservative</bibkey>
    </paper>
    <paper id="974">
      <title>Mitigating Bias in <fixed-case>RAG</fixed-case>: Controlling the Embedder</title>
      <author><first>Taeyoun</first><last>Kim</last></author>
      <author><first>Jacob Mitchell</first><last>Springer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Aditi</first><last>Raghunathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>18999-19024</pages>
      <abstract>In retrieval augmented generation (RAG) systems, each individual component—the LLM, embedder, and corpus—could introduce biases in the form of skews towards certain genders or political leanings. In this work, we study the conflict between biases of each component and their relationship to the overall bias of the RAG system, which we call <tex-math>\textit{bias conflict}</tex-math>. Examining both gender and political biases as case studies, we show that bias conflict can be characterized through a linear relationship among components despite its complexity. Through fine-tuning, we demonstrate how to control the bias of the embedder while maintaining utility and reveal the importance of <tex-math>\textit{reverse-biasing}</tex-math> the embedder to mitigate bias in the overall system, Additionally, we find that LLMs and tasks exhibit varying <tex-math>\textit{sensitivities}</tex-math> to bias, a crucial factor to consider for debiasing. Our results underscore that a fair RAG system can be better achieved by carefully controlling the bias of the embedder rather than increasing its fairness.</abstract>
      <url hash="bc124601">2025.findings-acl.974</url>
      <bibkey>kim-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="975">
      <title><fixed-case>V</fixed-case>-<fixed-case>ALPHASOCIAL</fixed-case>: Benchmark and Self-Reflective Chain-of-Thought Generation for Visual Social Commonsense Reasoning</title>
      <author><first>Zongyu</first><last>Lin</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zhikun</first><last>Xu</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Xiaohan</first><last>Song</last></author>
      <author><first>Yixin</first><last>Wan</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Xingcheng</first><last>Yao</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Tsung-Han</first><last>Lin</last></author>
      <author><first>Selina</first><last>Song</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Pranav</first><last>Subbaraman</last></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>19025-19047</pages>
      <abstract>Social commonsense reasoning naturally involves both the verbal and non-verbal cues of a social interaction. It is important for Large Vision-Language Models (VLMs) to leverage both textual and visual information in performing tasks like social understanding and reasoning. However, while current LLMs have shown good social reasoning capabilities in textual context, whether they can effectively incorporate visual information in social comprehension remains under-explored. To narrow the gap, we first construct and propose a benchmark: V-Social, featuring well-aligned text and visual content, tailored to assess visual social commonsense for multimodal foundation models. Through experimenting with V-Social, we find that even the most advanced VLM, GPT-4o, often falls short in social commonsense reasoning. This highlights the critical need to enhance the social grounding of VLMs. One major obstacle for improving this is the lack of high-quality data with good reasoning process. To overcome this obstacle, we introduce V-AlphaSocial, a novel method that generates high-quality chain-of-thought reasoning paths from unlabeled data. We design a visual reasoning reward model to improve VLM, and then iteratively refine both the VLM and the reward model. Our extensive analysis showcases how our method enhances social commonsense reasoning, proposing an effective approach that facilitates deeper exploration into field.</abstract>
      <url hash="457dc20b">2025.findings-acl.975</url>
      <bibkey>lin-etal-2025-v</bibkey>
    </paper>
    <paper id="976">
      <title><fixed-case>A</fixed-case>fro<fixed-case>B</fixed-case>ench: How Good are Large Language Models on <fixed-case>A</fixed-case>frican Languages?</title>
      <author><first>Jessica</first><last>Ojo</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Odunayo</first><last>Ogundepo</last></author>
      <author><first>Akintunde</first><last>Oladipo</last><affiliation>The African Research Collective</affiliation></author>
      <author><first>Kelechi</first><last>Ogueji</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <pages>19048-19095</pages>
      <abstract>Large-scale multilingual evaluations, such as MEGA, often include only a handful of African languages due to the scarcity of high-qualityevaluation data and the limited discoverability of existing African datasets. This lack of representation hinders comprehensive LLM evaluation across a diverse range of languages and tasks. To address these challenges, we introduce AFROBENCH—a multi-task benchmark for evaluating the performance of LLMs across 64 African languages, 15 tasks and 22 datasets. AFROBENCH consists of nine natural language understanding datasets, six text generation datasets, six knowledge and question answering tasks, and one mathematical reasoning task. We present results comparing the performance of prompting LLMs to fine-tuned baselines based on BERT and T5-style models. Our results suggest large gaps in performance between high-resource languages, such as English, and African languages across most tasks; but performance also varies based on the availability of monolingual data resources. Our findings confirm that performance on African languages continues to remain a hurdle for current LLMs, underscoring the need for additional efforts to close this gap.</abstract>
      <url hash="d7073e0b">2025.findings-acl.976</url>
      <bibkey>ojo-etal-2025-afrobench</bibkey>
    </paper>
    <paper id="977">
      <title>Training Bilingual <fixed-case>LM</fixed-case>s with Data Constraints in the Targeted Language</title>
      <author><first>Skyler</first><last>Seto</last><affiliation>Apple</affiliation></author>
      <author><first>Maartje</first><last>Ter Hoeve</last><affiliation>Apple</affiliation></author>
      <author><first>Richard He</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Natalie</first><last>Schluter</last><affiliation>Technical University of Denmark, Apple and IT University</affiliation></author>
      <author><first>David</first><last>Grangier</last><affiliation>Apple</affiliation></author>
      <pages>19096-19122</pages>
      <abstract>Large language models are trained on massive scrapes of the web, as required by current scaling laws. Most progress is made for English, given its abundance of high-quality pretraining data. For most other languages, however, such high quality pretraining data is unavailable. In this work, we study how to boost pretrained model performance in a target language with insufficient pretraining data for training a high performing language model by enlisting data from an auxiliary language for which high quality data is available. We study this by quantifying the performance gap between training with data in a data-rich auxiliary language compared with training in the target language, exploring the benefits of translation systems, studying the limitations of model scaling when data is limited in the target languages, and proposing new methods for upsampling data from the auxiliary language. Our results show that stronger auxiliary datasets result in performance gains without modification to the model or training objective for close languages, and, in particular, that performance gains due to the development of more information-rich English pretraining datasets can extend to targeted language settings with limited data.</abstract>
      <url hash="29e1c6cb">2025.findings-acl.977</url>
      <bibkey>seto-etal-2025-training</bibkey>
    </paper>
    <paper id="978">
      <title><fixed-case>C</fixed-case>hart<fixed-case>QAP</fixed-case>ro: A More Diverse and Challenging Benchmark for Chart Question Answering</title>
      <author><first>Ahmed</first><last>Masry</last><affiliation>York University and ServiceNow Inc</affiliation></author>
      <author><first>Mohammed Saidul</first><last>Islam</last><affiliation>York University</affiliation></author>
      <author><first>Mahir</first><last>Ahmed</last></author>
      <author><first>Aayush</first><last>Bajaj</last></author>
      <author><first>Firoz</first><last>Kabir</last></author>
      <author><first>Aaryaman</first><last>Kartha</last></author>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc.</affiliation></author>
      <author><first>Mizanur</first><last>Rahman</last></author>
      <author><first>Shadikur</first><last>Rahman</last></author>
      <author><first>Mehrad</first><last>Shahmohammadi</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Enamul</first><last>Hoque</last><affiliation>York University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <pages>19123-19151</pages>
      <abstract>Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 99 diverse sources, spanning various chart types—including infographics and dashboards—and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.</abstract>
      <url hash="8e71994c">2025.findings-acl.978</url>
      <bibkey>masry-etal-2025-chartqapro</bibkey>
    </paper>
    <paper id="979">
      <title>From Observation to Understanding: Front-Door Adjustments with Uncertainty Calibration for Enhancing Egocentric Reasoning in <fixed-case>LVLM</fixed-case>s</title>
      <author><first>Shenshen</first><last>Li</last></author>
      <author><first>Wenxin</first><last>Meng</last></author>
      <author><first>Lei</first><last>Wang</last><affiliation>SalesForce</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Meituan</affiliation></author>
      <author><first>Chong</first><last>Peng</last></author>
      <author><first>Peng</first><last>Yan</last><affiliation>Meituan</affiliation></author>
      <author><first>Fumin</first><last>Shen</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Jingkuan</first><last>Song</last><affiliation>University of Electronic Science and Technology of China,</affiliation></author>
      <author><first>Heng Tao</first><last>Shen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Xing</first><last>Xu</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <pages>19152-19169</pages>
      <abstract>Recent progress in large vision-language models (LVLMs) has shown substantial potential across a broad spectrum of third-person tasks. However, adapting these LVLMs to egocentric scenarios remains challenging due to their third-person training bias. Existing methods that adapt LVLMs for first-person tasks often overlook critical agent-environment interactions, limiting their ability to perform egocentric reasoning. To address these challenges, we propose a novel zero-shot paradigm termed Front-Door Adjustments with Uncertainty Calibration (FRUIT) to enhance the egocentric reasoning abilities of LVLMs by simulating human causal reasoning. Specifically, the FRUIT operates in two stages: observation and understanding. Unlike conventional prompting techniques, we formalize egocentric reasoning using a structural causal model. Then, we ground interaction regions and expand them into hierarchical visual cues, augmented with corresponding captions, to form the initial observations. To reduce noise in these observations, we employ uncertainty calibration to filter out unreliable information. These refined observations as mediators are then incorporated into the prompt template, guiding the model to understand semantics from a first-person perspective. Extensive experiments conducted on the EgoThink benchmark demonstrate that our FRUIT method consistently enhances the performance of existing LVLMs on six distinct tasks. Our code is available at https://github.com/Mrshenshen/FRUIT.</abstract>
      <url hash="61068f6c">2025.findings-acl.979</url>
      <bibkey>li-etal-2025-observation</bibkey>
    </paper>
    <paper id="980">
      <title>Hypothetical Documents or Knowledge Leakage? Rethinking <fixed-case>LLM</fixed-case>-based Query Expansion</title>
      <author><first>Yejun</first><last>Yoon</last><affiliation>Soongsil University</affiliation></author>
      <author><first>Jaeyoon</first><last>Jung</last></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Kunwoo</first><last>Park</last><affiliation>Soongsil University</affiliation></author>
      <pages>19170-19187</pages>
      <abstract>Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyze whether the generated documents contain information entailed by ground-truth evidence and assess their impact on performance. Our findings indicate that, on average, performance improvements consistently occurred for claims whose generated documents included sentences entailed by gold evidence. This suggests that knowledge leakage may be present in fact-verification benchmarks, potentially inflating the perceived performance of LLM-based query expansion methods.</abstract>
      <url hash="db1a82ee">2025.findings-acl.980</url>
      <bibkey>yoon-etal-2025-hypothetical</bibkey>
    </paper>
    <paper id="981">
      <title>Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical <fixed-case>VQA</fixed-case></title>
      <author><first>Qianqi</first><last>Yan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Xuehai</first><last>He</last><affiliation>University of California, San Diego and University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Xin Eric</first><last>Wang</last><affiliation>Simular and University of California, Santa Cruz</affiliation></author>
      <pages>19188-19205</pages>
      <abstract>Large Multimodal Models (LMMs) have demonstrated impressive performance on existing medical Visual Question Answering (Med-VQA) benchmarks. However, high reported accuracy does not necessarily reflect their true diagnostic reliability in clinical settings. This study reveals that state-of-the-art models perform worse than random guessing on medical diagnosis questions when subjected to simple Probing Evaluation for Medical Diagnosis (ProbMed). ProbMed challenges models through probing evaluation and procedural diagnosis. Particularly, probing evaluation features pairing ground-truth questions with adversarial counterparts that feature negated and hallucinated attributes, while procedural diagnosis requires reasoning across various dimensions for each image, including modality recognition, organ identification, clinical findings, abnormalities, and positional grounding. Our evaluation reveals that even top-performing models like GPT-4o, GPT-4V, and Gemini Pro perform worse than random guessing on specialized diagnostic questions, indicating significant limitations in handling fine-grained medical inquiries. Furthermore, our ablation study on open-source models (e.g., LLaVA, LLaVA-Med, and Med-Flamingo) identifies poor visual understanding as a primary bottleneck—a limitation that can be partially mitigated by incorporating visual descriptions generated by GPT-4o, resulting in an average performance improvement of 9.44%. These findings underscore the urgent need for more robust evaluation methods and domain-specific expertise to ensure the reliability of LMMs in high-stakes medical applications.</abstract>
      <url hash="94444a3b">2025.findings-acl.981</url>
      <bibkey>yan-etal-2025-worse</bibkey>
    </paper>
    <paper id="982">
      <title>Optimizing Reasoning for Text-to-<fixed-case>SQL</fixed-case> with Execution Feedback</title>
      <author><first>Bohan</first><last>Zhai</last><affiliation>Snowflake</affiliation></author>
      <author><first>Canwen</first><last>Xu</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yuxiong</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhewei</first><last>Yao</last><affiliation>Snowflake</affiliation></author>
      <pages>19206-19218</pages>
      <abstract>Text-to-SQL demands precise reasoning to convert natural language questions into structured queries. While large language models (LLMs) excel in many reasoning tasks, their ability to leverage Chain-of-Thought (CoT) reasoning for text-to-SQL remains underexplored. We identify critical limitations: zero-shot CoT offers minimal gains, and Direct Preference Optimization (DPO) applied without CoT yields marginal improvements. We propose ExCoT-DPO, a novel framework that iteratively optimizes open-source LLMs by combining CoT reasoning with off-policy and on-policy DPO, relying solely on execution accuracy as feedback. This approach eliminates the need for reward models or human-annotated preferences. Our experimental results demonstrate significant performance gains: ExCoT-DPO improves execution accuracy on BIRD from 57.37% to 68.51% and on Spider from 78.81% to 86.59% for LLaMA-3 70B, with Qwen-2.5-Coder demonstrating similar improvements. Our best model achieves state-of-the-art performance in the single-model setting on both BIRD and Spider datasets.</abstract>
      <url hash="d84577fa">2025.findings-acl.982</url>
      <bibkey>zhai-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="983">
      <title>Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities</title>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Kaijie</first><last>Zhu</last></author>
      <author><first>Lingyao</first><last>Li</last></author>
      <author><first>Lizhou</first><last>Fan</last><affiliation>Brigham and Women’s Hospital, Harvard University</affiliation></author>
      <author><first>Mingyu</first><last>Jin</last></author>
      <author><first>Shuhang</first><last>Lin</last><affiliation>, Rutgers University</affiliation></author>
      <author><first>Haochen</first><last>Xue</last></author>
      <author><first>Zelong</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>William &amp; Mary</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>19219-19242</pages>
      <abstract>This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark LLMs’ reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problems generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. We construct datasets for both reasoning types with four difficulty levels across 12 distinct domains based on the Wikipedia categorization in addition to those with purely abstract variables. Our experiments aim to provide insights into disentangling context in logical reasoning, the genuine reasoning capabilities of LLMs, and their generalization potential. Coda and data are available at <url>https://anonymous.4open.science/r/ContextHub-957E</url>.</abstract>
      <url hash="4004f2e3">2025.findings-acl.983</url>
      <bibkey>hua-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="984">
      <title>Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models</title>
      <author><first>Shuqi</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Han</first><last>Wu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Bowei</first><last>He</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Xiongwei</first><last>Han</last></author>
      <author><first>Mingxuan</first><last>Yuan</last></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>19243-19255</pages>
      <abstract>Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2 7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.</abstract>
      <url hash="a40001ee">2025.findings-acl.984</url>
      <bibkey>liu-etal-2025-sens</bibkey>
    </paper>
    <paper id="985">
      <title><fixed-case>E</fixed-case>go<fixed-case>N</fixed-case>ormia: Benchmarking Physical-Social Norm Understanding</title>
      <author><first>MohammadHossein</first><last>Rezaei</last></author>
      <author><first>Yicheng</first><last>Fu</last></author>
      <author><first>Phil</first><last>Cuvin</last><affiliation>Stanford University</affiliation></author>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hao</first><last>Zhu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>19256-19283</pages>
      <abstract>Human activity is moderated by norms; however, supervision for normative reasoning is sparse, particularly where norms are physically- or socially-grounded. We thus present EgoNormia <tex-math>\lVert \epsilon \rVert</tex-math>, comprising 1,853 (200 for EgoNormia-verified) multiple choice questions (MCQs) grounded within ego-centric videos of human interactions, enabling the evaluation and improvement of normative reasoning in vision-language models (VLMs). spans seven norm categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline to generate grounded MCQs from raw egocentric video. Our work demonstrates that current state-of-the-art VLMs lack robust grounded norm understanding, scoring a maximum of 54% on EgoNormia and 58% on EgoNormia-verified, with performance across norm categories indicating significant risks of safety and privacy when VLMs are used in real-world agents. We additionally explore methods for improving normative understanding, demonstrating a naive retrieval-based generation (RAG) method using can enhance normative reasoning in VLMs.</abstract>
      <url hash="f63861b9">2025.findings-acl.985</url>
      <bibkey>rezaei-etal-2025-egonormia</bibkey>
    </paper>
    <paper id="986">
      <title>Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence</title>
      <author><first>Linyang</first><last>He</last></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Helmut</first><last>Schmid</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <author><first>Nima</first><last>Mesgarani</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jonathan</first><last>Brennan</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>19284-19302</pages>
      <abstract>This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional psycholinguistic evaluations often reflect statistical rules that may not accurately represent LLMs’ true linguistic competence. We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pair and diagnostic probing to analyze activation patterns across model layers. This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages. We found: (1) Psycholinguistic and neurolinguistic methods reveal that language performance and competence are distinct; (2) Direct probability measurement may not accurately assess linguistic competence; (3) Instruction tuning won’t change much competence but improve performance; (4) LLMs exhibit higher competence and performance in form compared to meaning. Additionally, we introduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets.</abstract>
      <url hash="4eda8363">2025.findings-acl.986</url>
      <bibkey>he-etal-2025-large-language</bibkey>
    </paper>
    <paper id="987">
      <title>The Impact of Large Language Models in Academia: from Writing to Speaking</title>
      <author><first>Mingmeng</first><last>Geng</last><affiliation>Ecole Normale Supérieure – PSL</affiliation></author>
      <author><first>Caixi</first><last>Chen</last></author>
      <author><first>Yanru</first><last>Wu</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Pan</first><last>Zhou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Dongping</first><last>Chen</last></author>
      <pages>19303-19319</pages>
      <abstract>Large language models (LLMs) are increasingly impacting human society, particularly in textual information. Based on more than 30,000 papers and 1,000 presentations from machine learning conferences, we examined and compared the words used in writing and speaking, representing the first large-scale study of how LLMs influence the two main modes of verbal communication and expression within the same group of people. Our empirical results show that LLM-style words such as significant have been used more frequently in abstracts and oral presentations. The implicit impact on human expression like writing and speaking is beginning to emerge and is likely to grow in the future. We take the first step in building an automated monitoring platform to record its longitudinal changes to call attention to the implicit influence and ripple effect of LLMs on human society.</abstract>
      <url hash="dd520136">2025.findings-acl.987</url>
      <bibkey>geng-etal-2025-impact</bibkey>
    </paper>
    <paper id="988">
      <title><fixed-case>X</fixed-case>-<fixed-case>W</fixed-case>eb<fixed-case>A</fixed-case>gent<fixed-case>B</fixed-case>ench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System</title>
      <author><first>Peng</first><last>Wang</last><affiliation>Macau University of Science and Technology</affiliation></author>
      <author><first>Ruihan</first><last>Tao</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Mengkang</first><last>Hu</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <pages>19320-19335</pages>
      <abstract>Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.</abstract>
      <url hash="56973455">2025.findings-acl.988</url>
      <bibkey>wang-etal-2025-x</bibkey>
    </paper>
    <paper id="989">
      <title><fixed-case>M</fixed-case>em<fixed-case>B</fixed-case>ench: Towards More Comprehensive Evaluation on the Memory of <fixed-case>LLM</fixed-case>-based Agents</title>
      <author><first>Haoran</first><last>Tan</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Ma</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Quanyu</first><last>Dai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <pages>19336-19352</pages>
      <abstract>Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at <url>https://github.com/import-myself/Membench</url>.</abstract>
      <url hash="2e283e44">2025.findings-acl.989</url>
      <bibkey>tan-etal-2025-membench</bibkey>
    </paper>
    <paper id="990">
      <title>Adaptive <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Merge with Parameter Pruning for Low-Resource Generation</title>
      <author><first>Ryota</first><last>Miyano</last><affiliation>Osaka University, Graduate School of Information Science and Technology</affiliation></author>
      <author><first>Yuki</first><last>Arase</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology, RIKEN and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>19353-19366</pages>
      <abstract>This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.</abstract>
      <url hash="ee606aac">2025.findings-acl.990</url>
      <bibkey>miyano-arase-2025-adaptive</bibkey>
    </paper>
    <paper id="991">
      <title><fixed-case>L</fixed-case>ong<fixed-case>A</fixed-case>ttn: Selecting Long-context Training Data via Token-level Attention</title>
      <author><first>Longyun</first><last>Wu</last></author>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Guangxiang</first><last>Zhao</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhuocheng</first><last>Yu</last></author>
      <author><first>Junfeng</first><last>Ran</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiangyu</first><last>Wong</last></author>
      <author><first>Lin</first><last>Sun</last><affiliation>Qihoo 360</affiliation></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>19367-19380</pages>
      <abstract>With the development of large language models (LLMs), there has been an increasing need for significant advancements in handling long contexts. To enhance long-context capabilities, constructing high-quality training data with **long-range dependencies** is crucial. Existing methods to select long-context data often rely on sentence-level analysis,which can be greatly optimized in both performance and efficiency. In this paper, we propose a novel token-level framework, ​**LongAttn**​, which leverages the self-attention mechanism of LLMs to measure the **long-range dependencies** for the data. By calculating token-level dependency strength and distribution uniformity of token scores, LongAttn effectively quantifies ​**long-range dependencies**​, enabling more accurate and efficient data selection. We filter **LongABC-32K** from open-source long-context datasets (ArXiv, Book, and Code). Through our comprehensive experiments, LongAttn has demonstrated its excellent ​**effectiveness**​, ​**scalability**​, and ​**efficiency**​. We will release our code and the high-quality long-context dataset **LongABC-32K** in the future.</abstract>
      <url hash="49332043">2025.findings-acl.991</url>
      <bibkey>wu-etal-2025-longattn</bibkey>
    </paper>
    <paper id="992">
      <title><fixed-case>C</fixed-case>o<fixed-case>RE</fixed-case>: Condition-based Reasoning for Identifying Outcome Variance in Complex Events</title>
      <author><first>Sai P</first><last>Vallurupalli</last></author>
      <author><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>19381-19401</pages>
      <abstract>Knowing which latent conditions lead to a particular outcome is useful for critically examining claims made about complex event outcomes. Identifying implied conditions and examining their influence on an outcome is challenging. We handle this by combining and augmenting annotations from two existing datasets consisting of goals and states, and explore the influence of conditions through our research questions and Condition-based Reasoning tasks. We examine open and closed LLMs of varying sizes and intent-alignment on our reasoning tasks and find that conditions are useful when not all context is available. Models differ widely in their ability to generate and identify outcome-variant conditions, which affects their performance on outcome validation, when conditions are used to replace missing context. Larger models like GPT-4o, are more cautious in such less constrained situations.</abstract>
      <url hash="0fb4bbeb">2025.findings-acl.992</url>
      <bibkey>vallurupalli-ferraro-2025-core</bibkey>
    </paper>
    <paper id="993">
      <title><fixed-case>F</fixed-case>a<fixed-case>V</fixed-case>e: Factored and Verified Search Rationale for Long-form Answer</title>
      <author><first>Jihyuk</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Sungjin</first><last>Lee</last><affiliation>Amazon</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <pages>19402-19416</pages>
      <abstract>Targeting long-form question-answering, chain-of-query (CoQ) has been studied, integrating chain-of-thought (CoT) with retrieval-augmented generation. CoQ answers the complex question step-by-step, through simpler subquestions (SQs) from which relevant knowledge is retrieved. By doing so, CoQ aims to improve the answer comprehensiveness and verifiability, at the expense of latency. Our first contribution is showing that the chaining often incurs harmful effects on both objectives, and SQs left unverified often fail to answer the given question. Second, we propose a better alternative to CoQ, union-of-query which adopts a factored approach to break the harmful chain. Finally, we propose to verify SQs before answers, by fine-tuning the SQ generator using verified SQs and introducing a selector verifying SQs in test time. Employing vicuna-13b, our approach, denoted by FaVe (short for Factored and Verified search), even outperforms ChatGPT baselines while maintaining efficiency.</abstract>
      <url hash="6cd711cb">2025.findings-acl.993</url>
      <bibkey>kim-etal-2025-fave</bibkey>
    </paper>
    <paper id="994">
      <title><fixed-case>U</fixed-case>nreal<fixed-case>LLM</fixed-case>: Towards Highly Controllable and Interactable 3<fixed-case>D</fixed-case> Scene Generation by <fixed-case>LLM</fixed-case>-powered Procedural Content Generation</title>
      <author><first>Song</first><last>Tang</last></author>
      <author><first>Kaiyong</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Yuliang</first><last>Li</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Junyi</first><last>Zou</last></author>
      <author><first>Qiang</first><last>Wang</last></author>
      <author><first>Xiaowen</first><last>Chu</last></author>
      <pages>19417-19435</pages>
      <abstract>The creation of high-quality 3D scenes is essential for applications like video games and simulations, yet automating this process while retaining the benefits of Procedural Content Generation (PCG) remains challenging. In this paper, we introduce UnrealLLM, a novel multi-agent framework that connects natural language descriptions with the professional PCG system (Unreal Engine 5) to automate scene generation. UnrealLLM constructs a comprehensive knowledge base to translate text into executable PCG blueprints and a diverse asset library that guarantees high-quality scene generation. Additionally, it also introduces a text-based blueprint system with a spline-based control mechanism for geometric arrangement, enabling natural language interaction and enhancing interactivity in 3D environments using UE5’s advanced capabilities. Through extensive experiments, we show that UnrealLLM achieves competitive performance in technical metrics and aesthetic quality, offering unique advantages in generation scale and interactivity. This work makes a valuable contribution to automated 3D content creation, benefiting both novice users and professional designers.</abstract>
      <url hash="cc704487">2025.findings-acl.994</url>
      <bibkey>songtang-etal-2025-unrealllm</bibkey>
    </paper>
    <paper id="995">
      <title>Tree-of-Prompts: Abstracting Control-Flow for Prompt Optimization</title>
      <author><first>Jihyuk</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Shubham</first><last>Garg</last><affiliation>Amazon</affiliation></author>
      <author><first>Lahari</first><last>Poddar</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Chris</first><last>Hench</last></author>
      <pages>19436-19459</pages>
      <abstract>Prompt optimization (PO) generates prompts to guide Large Language Models (LLMs) in performing tasks. Existing methods, such as PromptAgent, rely on a single static prompt, which struggles with disjoint cases in complex tasks. Although MoP uses multiple prompts, it fails to account for variations in task complexity. Inspired by programmatic control flow, we introduce a nested if-else structure to address both varying similarities and complexities across diverse cases. We propose Tree-of-Prompts (ToP), which implements this structure by recursively expanding child prompts from a parent prompt. Sibling prompts tackle disjoint cases while inheriting shared similarities from their parent, and handle cases more complex than the parent. Evaluated on Gorilla (understanding), MATH (reasoning), and a subset of BBH benchmarks, ToP outperforms PromptAgent and MoP, with improvements of 1.4% and 4.6% over PromptAgent and 3.2% and 4.5% over MoP, when tested with GPT-4o-mini and Llama 3.2-3B, respectively.</abstract>
      <url hash="3f1d9d1e">2025.findings-acl.995</url>
      <bibkey>kim-etal-2025-tree</bibkey>
    </paper>
    <paper id="996">
      <title>Outlier-weighed Layerwise Sampling for <fixed-case>LLM</fixed-case> Fine-tuning</title>
      <author><first>Pengxiang</first><last>Li</last></author>
      <author><first>Lu</first><last>Yin</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Xiaowei</first><last>Gao</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Shiwei</first><last>Liu</last></author>
      <pages>19460-19473</pages>
      <abstract>The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampling (OWS), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds extra adapters to all layers, OWS strategically assigns higher sampling probabilities to layers with more outliers, selectively sampling only a few layers and fine-tuning their pre-trained weights. To further increase the number of fine-tuned layers without a proportional rise in memory costs, we incorporate gradient low-rank projection, further boosting the approach’s performance. Our extensive experiments across various architectures, including LLaMa2 and Mistral, demonstrate that OWS consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OWS allows us to fine-tune 7B LLMs with only 21GB of memory. Our code is available at https://github.com/pixeli99/OWS.</abstract>
      <url hash="55ac5d77">2025.findings-acl.996</url>
      <bibkey>li-etal-2025-outlier</bibkey>
    </paper>
    <paper id="997">
      <title><fixed-case>KVPR</fixed-case>: Efficient <fixed-case>LLM</fixed-case> Inference with <fixed-case>I</fixed-case>/<fixed-case>O</fixed-case>-Aware <fixed-case>KV</fixed-case> Cache Partial Recomputation</title>
      <author><first>Chaoyi</first><last>Jiang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Lei</first><last>Gao</last></author>
      <author><first>Hossein Entezari</first><last>Zarch</last></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>19474-19488</pages>
      <abstract>Inference for Large Language Models (LLMs) is computationally demanding. To reduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to store intermediate activations, which significantly lowers the computational overhead for token generation. However, the memory required for the KV cache grows rapidly, often exceeding the capacity of GPU memory. A cost-effective alternative is to offload KV cache to CPU memory, which alleviates GPU memory pressure, but shifts the bottleneck to the limited bandwidth of the PCIe connection between the CPU and GPU. Existing methods attempt to address these issues by overlapping GPU computation with I/O or employing CPU-GPU heterogeneous execution, but they are hindered by excessive data movement and dependence on CPU capabilities. Fully overlapping PCIe communication latency gets challenging as the size of the KV cache grows and/or the GPU compute capabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware LLM inference method where the CPU first transfers a partial set of activations, from which the GPU can start recomputing the KV cache values. While the GPU recomputes the partial KV cache, the remaining portion of the KV cache is transferred concurrently from the CPU. This approach overlaps GPU recomputation with KV cache transfer to minimize idle GPU time and maximize inference performance. KVPR is fully automated by integrating a profiler module that utilizes input characteristics and system hardware information, a scheduler module to optimize the distribution of computation and communication workloads, and a runtime module to efficiently execute the derived execution plan. Experimental results show that KVPR achieves up to 35.8% lower latency and 46.2% higher throughput during decoding compared to state-of-the-art approaches. The code is available at https://github.com/chaoyij/KVPR.</abstract>
      <url hash="10699fa3">2025.findings-acl.997</url>
      <bibkey>jiang-etal-2025-kvpr</bibkey>
    </paper>
    <paper id="998">
      <title>Direct Behavior Optimization: Unlocking the Potential of Lightweight <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hongming</first><last>Yang</last><affiliation>Zhejiang Gongshang University</affiliation></author>
      <author><first>Shi</first><last>Lin</last></author>
      <author><first>Jun</first><last>Shao</last><affiliation>Zhejiang Gongshang University</affiliation></author>
      <author><first>Changting</first><last>Lin</last></author>
      <author><first>Donghai</first><last>Zhu</last><affiliation>Zhejiang Gongshang University</affiliation></author>
      <author><first>Meng</first><last>Han</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qinglei</first><last>Kong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>19489-19515</pages>
      <abstract>Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs.To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.</abstract>
      <url hash="eae3514e">2025.findings-acl.998</url>
      <bibkey>yang-etal-2025-direct</bibkey>
    </paper>
    <paper id="999">
      <title>Whether <fixed-case>LLM</fixed-case>s Know If They Know: Identifying Knowledge Boundaries via Debiased Historical In-Context Learning</title>
      <author><first>Bo</first><last>Lv</last></author>
      <author><first>Nayu</first><last>Liu</last><affiliation>School of Computer Science and Technology, Tiangong University</affiliation></author>
      <author><first>Yang</first><last>Shen</last><affiliation>tiangong</affiliation></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yue</first><last>Yu</last></author>
      <pages>19516-19528</pages>
      <abstract>In active retrieval (AR), large language models (LLMs) need first assess whether they possess knowledge to answer a given query, to decide whether to invoke a retrieval module. Existing methods primarily rely on training classification models or using the confidence of the model’s answer to determine knowledge boundaries. However, training-based methods may have limited generalization, and our analysis reveals that LLMs struggle to reliably assess whether they possess the required information based on their answers, often biased by prior cognitive tendencies (e.g., tokens’ semantic preferences). To address this, we propose Debiased Historical In-Context Learning (DH-ICL) to identify knowledge boundaries in AR. DH-ICL aims to reframe this self-awareness metacognitive task as a structured pattern-learning problem by retrieving similar historical queries as high-confidence in-context examples to guide LLMs to identify knowledge boundaries. Furthermore, we introduce a historical bias calibration strategy that leverages deviations in the model’s past response logits to mitigate cognitive biases in its current knowledge boundary assessment. Experiments on four QA benchmarks show that DH-ICL achieves performance comparable to full retrieval on LLaMA with only half the number of retrievals, without any additional training.</abstract>
      <url hash="f193dd87">2025.findings-acl.999</url>
      <bibkey>lv-etal-2025-whether</bibkey>
    </paper>
    <paper id="1000">
      <title>How do <fixed-case>LLM</fixed-case>s’ Preferences Affect Event Argument Extraction? <fixed-case>CAT</fixed-case>: Addressing Preference Traps in Unsupervised <fixed-case>EAE</fixed-case></title>
      <author><first>Yunhao</first><last>Wei</last></author>
      <author><first>Kai</first><last>Shuang</last></author>
      <author><first>Zhiyi</first><last>Li</last></author>
      <author><first>Chenrui</first><last>Mao</last></author>
      <pages>19529-19543</pages>
      <abstract>Large Language Models (LLMs) have significantly improved the performance of unsupervised Event Argument Extraction (EAE) tasks. However, LLMs’ inherent preferences severely hinder their effectiveness in EAE, leading to what we term preference traps, namely, the Prior Knowledge Trap, the Sycophancy Hallucination Trap, and the Output Contradiction Trap. Existing approaches often fall into these traps due to misalignments between their prior knowledge, instructions, or output constraints and LLMs’ preferences, which significantly limits further performance gains. To address this issue, we propose Choose-After-Think (CAT), an unsupervised EAE framework designed to handle these preference traps through targeted measures. CAT innovatively divides the EAE task into two phases: identification of event information (argument roles) (Think Phase) and selection of the final answers from a candidate set (Choose Phase). This two-phase approach reduces the impact of individual token probability anomalies and ensures the integrity of EAE results. Experimental results demonstrate that CAT (based on the local 7B model, zero-shot setting) matches the performance of the best DeepSeek-R1 API model, with a significantly lower time cost.</abstract>
      <url hash="b48bd15e">2025.findings-acl.1000</url>
      <bibkey>wei-etal-2025-llms</bibkey>
    </paper>
    <paper id="1001">
      <title>Out-of-Distribution Detection via <fixed-case>LLM</fixed-case>-Guided Outlier Generation for Text-attributed Graph</title>
      <author><first>Xiangwei</first><last>Lv</last></author>
      <author><first>Mengze</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiang</first><last>Dong</last></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Beishui</first><last>Liao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>19544-19555</pages>
      <abstract>Text-Attributed Graphs (TAGs), which are characterized with text attributes, are widely used in the real world. When evaluating fully trained models designed for TAG predictions, they may perform significantly unsatisfactory on samples outside the In-Distribution (ID) data, which may raise serious security issues. To tackle it, Out-Of-Distribution (OOD) detection is introduced to the TAGs field, which aims to utilize a detector to classify OOD and ID samples. Recent studies attempt to introduce extra OOD datasets to regularize the detection model. However, due to the vastness of the OOD data space, high-quality OOD samples for training the detector are scarce and difficult to obtain in the real world. Thus, we utilize Large Language Models (LLMs) to generate the OOD training samples with high quality. There are two issues in this process: (1) LLMs tend to generate OOD-node samples significantly different from ID ones, with a limited learning value for OOD and ID relations. (2) Due to the inherent structure of TAGs, obtained OOD nodes need to be integrated with existing nodes by generating edges using LLMs. However, the large number of nodes makes reasoning over each node pair computationally unbearable. Toward these issues, we introduce LLMGuard with challenging OOD-node generation and lightweight edge predictors. Extensive experiments prove the effectiveness of LLMGuard. The source code is available.</abstract>
      <url hash="b7440198">2025.findings-acl.1001</url>
      <bibkey>lv-etal-2025-distribution</bibkey>
    </paper>
    <paper id="1002">
      <title>Document-Level Relation Extraction with Global Relations and Entity Pair Reasoning</title>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yi</first><last>Yan</last></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>19556-19567</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract structured relational triples from unstructured text based on given entities. Existing methods are mainly categorized into transformer-based models and graph-based models. While transformer-based models capture global contextual information, they typically focus on individual entity pairs, making it challenging to capture complex interactions between multiple entity pairs. Graph-based models build document graphs using entities or sentences as nodes for reasoning but often lack explicit mechanisms to model fine-grained interactions between entity pairs, limiting their ability to handle complex relational reasoning tasks. Additionally, previous research has not considered predicting all possible relations in advance to assist with DocRE tasks. To address these issues, we propose a new framework namely **GREP** (**g**lobal **r**elations and **e**ntity **p**air reasoning) for DocRE tasks. GREP leverages the global interdependencies between entity pairs to capture fine-grained interactions and perform multi reasoning at the entity pair level. In addtion, GREP for the first time proposes an auxiliary task that predicts all possible relations in advance that exist in a document, which enables the model to filter out the most unlikely relations. Experimental results on widely-used datasets demonstrate that our model achieves state-of-the-art performance. Code is available at https://github.com/yanyi74/GREP.</abstract>
      <url hash="9a3a7290">2025.findings-acl.1002</url>
      <bibkey>zhang-etal-2025-document</bibkey>
    </paper>
    <paper id="1003">
      <title>Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings</title>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Jinsong</first><last>Li</last><affiliation>The Chinese University of Hong Kong and Shanghai AI Laboratory</affiliation></author>
      <author><first>Yuhang</first><last>Zang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xiaoyi</first><last>Dong</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Pan</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yuhang</first><last>Cao</last></author>
      <author><first>Haodong</first><last>Duan</last></author>
      <author><first>Jiaqi</first><last>Wang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Aixin</first><last>Sun</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>19568-19580</pages>
      <abstract>Despite the strong performance of ColPali/ColQwen2 in Visualized Document Retrieval (VDR), its patch-level embedding approach leads to excessive memory usage. This empirical study investigates methods to reduce patch embeddings per page while minimizing performance degradation. We evaluate two token-reduction strategies: <i>token pruning</i> and <i>token merging</i>. Regarding token pruning, we surprisingly observe that a simple random strategy outperforms other sophisticated pruning methods, though still far from satisfactory. Further analysis reveals that pruning is inherently unsuitable for VDR as it requires removing certain page embeddings without query-specific information. Turning to token merging (more suitable for VDR), we search for the optimal combinations of merging strategy across three dimensions and develops Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance with only 11.8% of original memory usage, and preserves 94.6% effectiveness at 2% memory footprint. We expect our empirical findings and resulting Light-ColPali/ColQwen2 offer valuable insights and establish a competitive baseline for future efficient-VDR research.</abstract>
      <url hash="b201663a">2025.findings-acl.1003</url>
      <bibkey>ma-etal-2025-towards-storage</bibkey>
    </paper>
    <paper id="1004">
      <title>Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models</title>
      <author><first>Qingyu</first><last>Ren</last></author>
      <author><first>Jie</first><last>Zeng</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qianyu</first><last>He</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Weikang</first><last>Zhou</last></author>
      <author><first>Zeye</first><last>Sun</last></author>
      <author><first>Fei</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <pages>19581-19596</pages>
      <abstract>It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. In real-world scenarios, user instructions often contain soft constraints, which are semantically related and cannot be rule-based verified, posing challenges for LLMs. To enhance the soft constraint following ability of LLMs, we initially design a pipeline to construct datasets with high-quality outputs for instructions containing soft constraints automatically. Additionally, to fully utilize the positive and negative samples generated during the data construction process, we choose Direct Preference Optimization (DPO) as the training method. Furthermore, taking into account the difficulty of soft constraints indicated by the number of constraints, we design a curriculum learning training paradigm based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs’ soft constraint following ability and analyze the factors driving the improvements.</abstract>
      <url hash="57b699f2">2025.findings-acl.1004</url>
      <bibkey>ren-etal-2025-step</bibkey>
    </paper>
    <paper id="1005">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>DL</fixed-case>: Zero-shot Distribution Learning for Text Clustering via Large Language Models</title>
      <author><first>Hwiyeol</first><last>Jo</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Hyunwoo</first><last>Lee</last><affiliation>NAVER</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <author><first>Taiwoo</first><last>Park</last><affiliation>NAVER Search US</affiliation></author>
      <pages>19597-19607</pages>
      <abstract>The advancements in large language models (LLMs) have brought significant progress in NLP tasks. However, if a task cannot be fully described in prompts, the models could fail to carry out the task. In this paper, we propose a simple yet effective method to contextualize a task toward a LLM. The method utilizes (1) open-ended zero-shot inference from the entire dataset, (2) aggregate the inference results, and (3) finally incorporate the aggregated meta-information for the actual task. We show the effectiveness in text clustering tasks, empowering LLMs to perform text-to-text-based clustering and leading to improvements on several datasets. Furthermore, we explore the generated class labels for clustering, showing how the LLM understands the task through data.</abstract>
      <url hash="821afa3b">2025.findings-acl.1005</url>
      <bibkey>jo-etal-2025-zerodl</bibkey>
    </paper>
    <paper id="1006">
      <title>Patterns Over Principles: The Fragility of Inductive Reasoning in <fixed-case>LLM</fixed-case>s under Noisy Observations</title>
      <author><first>Chunyang</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianshi</first><last>Zheng</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>19608-19626</pages>
      <abstract>Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn’t yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce **Robust Rule Induction**, a task that evaluates LLMs’ capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0 accuracy change with only 70 consistent score);(3) Counterfactual task gaps highlight LLMs’ reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs’ reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems.</abstract>
      <url hash="48463021">2025.findings-acl.1006</url>
      <bibkey>li-etal-2025-patterns</bibkey>
    </paper>
    <paper id="1007">
      <title><fixed-case>LLMT</fixed-case>axo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
      <author><first>Haiqi</first><last>Zhang</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Zhengyuan</first><last>Zhu</last></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>19627-19641</pages>
      <abstract>With the rapid expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomies of factual claims from social media by generating topics at multiple levels of granularity. The resulting hierarchical structure significantly reduces redundancy and improves information accessibility. We also propose dedicated taxonomy evaluation metrics to enable comprehensive assessment. Evaluations conducted on three diverse datasets demonstrate LLMTaxo’s effectiveness in producing clear, coherent, and comprehensive taxonomies. Among the evaluated models, GPT-4o mini consistently outperforms others across most metrics. The framework’s flexibility and low reliance on manual intervention underscore its potential for broad applicability.</abstract>
      <url hash="94d34acf">2025.findings-acl.1007</url>
      <bibkey>zhang-etal-2025-llmtaxo</bibkey>
    </paper>
    <paper id="1008">
      <title><fixed-case>A</fixed-case>n<fixed-case>C</fixed-case>ast++: Document-Level Evaluation of Graph-based Meaning Representations</title>
      <author><first>Haibo</first><last>Sun</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Jayeol</first><last>Chun</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Nianwen</first><last>Xue</last><affiliation>Brandeis University</affiliation></author>
      <pages>19642-19654</pages>
      <abstract>Uniform Meaning Representation (UMR) is a cross-lingual document-level graph-based representation that is based on Abstract Meaning Representation (AMR) but extends it to include document-level semantic annotations such as coreference, modal and temporal dependencies.With recent advancements in UMR annotation efforts, a reliable evaluation metric is essential for assessing annotation consistency and tracking progress in automatic parsing. In this paper, we present AnCast++, an aggregated metric that unifies the evaluation of four distinct sub-structures of UMR: (1) sentence-level graphs that represent word senses, named entities, semantic relations between events and their participants, aspectual attributes of events as well as person and number attributes of entities, (2) modal dependencies that represent the level of certainty that a source holds with respect to an event, (3) temporal dependencies between events and their reference times, and (4) coreference relations between entities and between events. In particular, we describe a unified method <tex-math>TC^2</tex-math> for evaluating temporal and coreference relations that captures their shared transitive properties, and present experimental results on English and Chinese UMR parsing based on UMR v1.0 corpus to demonstrate the reliability of our metric. The tool will be made publicly available on Github.</abstract>
      <url hash="dbbf19ef">2025.findings-acl.1008</url>
      <bibkey>sun-etal-2025-ancast</bibkey>
    </paper>
    <paper id="1009">
      <title><fixed-case>MME</fixed-case>vol: Empowering Multimodal Large Language Models with Evol-Instruct</title>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Haonan</first><last>Zhang</last></author>
      <author><first>Longze</first><last>Chen</last></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiong</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Minzheng</first><last>Wang</last></author>
      <author><first>Pengpeng</first><last>Zeng</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Lianli</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Heng Tao</first><last>Shen</last><affiliation>Tongji University</affiliation></author>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Xiaobo</first><last>Xia</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jingkuan</first><last>Song</last><affiliation>University of Electronic Science and Technology of China,</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <pages>19655-19682</pages>
      <abstract>The development of Multimodal Large Language Models (MLLMs) has seen significant progress, driven by increasing demands across various fields (e.g., multimodal agents, embodied intelligence). While model-driven approaches aim to enhance MLLM capabilities through diverse architectures, their performance gains have become increasingly marginal. In contrast, data-driven methods, which scale up image-text instruction datasets, have proven more effective but face challenges related to limited data diversity and complexity. The absence of high-quality instruction data remains a major bottleneck in MLLM development. To address this issue, we propose , a novel multimodal instruction data evolution framework. This framework iteratively enhances data quality through a refined combination of fine-grained perception, cognitive reasoning, and interaction evolution, generating a more complex and diverse image-text instruction dataset that significantly improves MLLM capabilities. Starting with an initial dataset, SEED-163K, we employ to systematically expand instruction diversity, extend visual reasoning steps to improve cognitive abilities, and extract fine-grained visual details to enhance understanding and robustness. To rigorously evaluate our approach, we conduct extensive qualitative analysis and quantitative experiments across 13 vision-language tasks. Compared to baseline models trained on the original seed dataset, our method achieves an average accuracy improvement of 3.1 percentage points. Moreover, our approach attains state-of-the-art (SOTA) performance in nine tasks while using significantly less data than existing state-of-the-art models.</abstract>
      <url hash="4cd220e4">2025.findings-acl.1009</url>
      <bibkey>luo-etal-2025-mmevol</bibkey>
    </paper>
    <paper id="1010">
      <title><fixed-case>S</fixed-case>ci<fixed-case>V</fixed-case>erse: Unveiling the Knowledge Comprehension and Visual Reasoning of <fixed-case>LMM</fixed-case>s on Multi-modal Scientific Problems</title>
      <author><first>Ziyu</first><last>Guo</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Renrui</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Jialin</first><last>Gao</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Dongzhi</first><last>Jiang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jiaze</first><last>Wang</last></author>
      <author><first>Pheng-Ann</first><last>Heng</last></author>
      <pages>19683-19704</pages>
      <abstract>The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io</abstract>
      <url hash="8596efcd">2025.findings-acl.1010</url>
      <bibkey>guo-etal-2025-sciverse</bibkey>
    </paper>
    <paper id="1011">
      <title>Exploring Layer-wise Representations of <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese Homonymy in Pre-trained Language Models</title>
      <author><first>Matthew King-Hang</first><last>Ma</last></author>
      <author><first>Xie</first><last>Chenwei</last></author>
      <author><first>Wenbo</first><last>Wang</last></author>
      <author><first>William Shiyuan</first><last>Wang</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <pages>19705-19724</pages>
      <abstract>Homonymy can easily raise lexical ambiguity due to the misunderstanding of its multiple senses. Correct recognition of homonym sense greatly relies on its surrounding context. This ambiguous nature makes homonyms an appropriate testbed for examining the contextualization capability of pre-trained (PLM) and large language models (LLMs). Considering the impact of part of speech (POS) on homonym disambiguation and the prevalence of English-focused studies in word embedding research, this study extends to Chinese and provides a comprehensive layer-wise analysis of homonym representations in both languages, spanning same and different POS categories, across four families of PLMs/LLMs (BERT, GPT-2, Llama 3, Qwen 2.5). Through the creation of a synthetic dataset and computation of disambiguation score (D-Score), we found that: (1) no universal layer depth excels in differentiating homonym representations; (2) bidirectional models produce better contextualized homonym representations compared to much larger autoregressive models; (3) most importantly, POS affects homonym representations in models in ways that differ from human research findings. The individual differences between LLMs uncovered in our study challenge the simplistic understanding of their inner workings. This reveals a compelling research frontier: conducting controlled experiments with purposefully manipulated inputs to enhance the interpretability of LLMs. We have made our dataset and codes available publicly at https://github.com/neurothew/exploring-homonym-rep-in-llm.</abstract>
      <url hash="f09a0f49">2025.findings-acl.1011</url>
      <bibkey>ma-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1012">
      <title><fixed-case>D</fixed-case>oc<fixed-case>ME</fixed-case>dit: Towards Document-Level Model Editing</title>
      <author><first>Li</first><last>Zeng</last></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Chong</first><last>Feng</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>19725-19743</pages>
      <abstract>Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce DocMEdit, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.</abstract>
      <url hash="f51e6052">2025.findings-acl.1012</url>
      <bibkey>zeng-etal-2025-docmedit</bibkey>
    </paper>
    <paper id="1013">
      <title>Adaptive Detoxification: Safeguarding General Capabilities of <fixed-case>LLM</fixed-case>s through Toxicity-Aware Knowledge Editing</title>
      <author><first>Yifan</first><last>Lu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yigeng</first><last>Zhou</last></author>
      <author><first>Yihui</first><last>Zhang</last></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xiucheng</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>19744-19758</pages>
      <abstract>Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs’ general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs.</abstract>
      <url hash="d19a2456">2025.findings-acl.1013</url>
      <bibkey>lu-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="1014">
      <title>Evaluating the Long-Term Memory of Large Language Models</title>
      <author><first>Zixi</first><last>Jia</last></author>
      <author><first>Qinghua</first><last>Liu</last></author>
      <author><first>Hexiao</first><last>Li</last></author>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Jiqiang</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <pages>19759-19777</pages>
      <abstract>In applications such as dialogue systems, personalized recommendations, and personal assistants, large language models (LLMs) need to retain and utilize historical information over the long term to provide more accurate and consistent responses. Although long-term memory capability is crucial, recent studies have not thoroughly investigated the memory performance of large language models in long-term tasks. To address this gap, we introduce the Long-term Chronological Conversations (LOCCO) dataset and conduct a quantitative evaluation of the long-term memory capabilities of large language models. Experimental results demonstrate that large language models can retain past interaction information to a certain extent, but their memory decays over time. While rehearsal strategies can enhance memory persistence, excessive rehearsal is not an effective memory strategy for large models, unlike in smaller models. Additionally, the models exhibit memory preferences across different categories of information. Our study not only provides a new framework and dataset for evaluating the long-term memory capabilities of large language models but also offers important references for future enhancements of their memory persistence.</abstract>
      <url hash="ec033854">2025.findings-acl.1014</url>
      <bibkey>jia-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1015">
      <title>Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments</title>
      <author><first>Russell</first><last>Scheinberg</last></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <author><first>Amber</first><last>Shore</last><affiliation>Portland State University</affiliation></author>
      <author><first>So Young</first><last>Lee</last><affiliation>Miami University</affiliation></author>
      <pages>19778-19795</pages>
      <abstract>Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present <i>grammar prompting</i>, an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the <i>target model</i> – either an LLM or a smaller language model (SLM) – before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across a wide range of syntactic phenomena. Feeding an LLM’s metalinguistic explanation back to the target model bridges the gap between <i>knowing</i> a rule and <i>using</i> it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by 20%, and when paired with chain-of-thought, by 56% (13.0 pp <tex-math>\to</tex-math> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.</abstract>
      <url hash="f4058a64">2025.findings-acl.1015</url>
      <bibkey>scheinberg-etal-2025-explain</bibkey>
    </paper>
    <paper id="1016">
      <title>Data Interpreter: An <fixed-case>LLM</fixed-case> Agent for Data Science</title>
      <author><first>Sirui</first><last>Hong</last><affiliation>DeepWisdom</affiliation></author>
      <author><first>Yizhang</first><last>Lin</last><affiliation>DeepWisdom</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Bangbang</first><last>Liu</last><affiliation>deepwisdom</affiliation></author>
      <author><first>Binhao</first><last>Wu</last><affiliation>DeepWisdom</affiliation></author>
      <author><first>Ceyao</first><last>Zhang</last></author>
      <author><first>Danyang</first><last>Li</last></author>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Jiayi</first><last>Zhang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jinlin</first><last>Wang</last></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Lingyao</first><last>Zhang</last><affiliation>Aurora Innovation Inc</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mingchen</first><last>Zhuge</last></author>
      <author><first>Taicheng</first><last>Guo</last></author>
      <author><first>Tuo</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Tao</last><affiliation>LIGHTSPEED</affiliation></author>
      <author><first>Robert</first><last>Tang</last></author>
      <author><first>Xiangtao</first><last>Lu</last></author>
      <author><first>Xiawu</first><last>Zheng</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xinbing</first><last>Liang</last></author>
      <author><first>Yaying</first><last>Fei</last><affiliation>Southern Methodist University</affiliation></author>
      <author><first>Yuheng</first><last>Cheng</last></author>
      <author><first>Yongxin</first><last>Ni</last></author>
      <author><first>Zhibin</first><last>Gou</last></author>
      <author><first>Zongze</first><last>Xu</last></author>
      <author><first>Yuyu</first><last>Luo</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chenglin</first><last>Wu</last><affiliation>DeepWisdom</affiliation></author>
      <pages>19796-19821</pages>
      <abstract>Large Language Model (LLM)-based agents have excelled in various domains but face significant challenges when applied to data science workflows due to their complex, multi-stage nature. Current LLM-based agents struggle with non-linear relationships, recursive dependencies, implicit data- and logic-dependent reasoning, and managing extensive context. In this paper, we introduce Data Interpreter, an LLM-based agent that addresses these challenges through hierarchical graph-based modeling to represent the complexity and a progressive strategy for step-by-step verification, refinement, and consistent context management. Extensive experiments confirm the effectiveness of Data Interpreter. On InfiAgent-DABench, it boosts performance by 25% (from 75.9% to 94.9%), and on machine learning and open-ended tasks, it lifts accuracy from 88% to 95% and from 60% to 97%, respectively. Moreover, our method surpasses state-of-the-art baselines by 26% on the MATH dataset. We will release the code upon publication.</abstract>
      <url hash="14dcac21">2025.findings-acl.1016</url>
      <bibkey>hong-etal-2025-data</bibkey>
    </paper>
    <paper id="1017">
      <title><fixed-case>DR</fixed-case>e<fixed-case>SD</fixed-case>: Dense Retrieval for Speculative Decoding</title>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Huiyin</first><last>Xue</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>19822-19832</pages>
      <abstract>Speculative decoding (SD) accelerates Large Language Model (LLM) generation by using an efficient draft model to propose the next few tokens, which are verified by the LLM in a single forward call, reducing latency while preserving its outputs. We focus on retrieval-based SD where the draft model retrieves the next tokens from a non-parametric datastore. Sparse retrieval (CITATION)REST], which operates on the surface form of strings, is currently the dominant paradigm due to its simplicity and scalability. However, its effectiveness is limited due to the usage of short contexts and exact string matching. Instead, we introduce Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses approximate nearest neighbour search with contextualised token embeddings to retrieve the most semantically relevant token sequences for SD. Extensive experiments show that DReSD achieves (on average) 87% higher acceptance rates, 65% longer accepted tokens and 19% faster generation speeds compared to sparse retrieval (REST).</abstract>
      <url hash="94b08b8a">2025.findings-acl.1017</url>
      <bibkey>gritta-etal-2025-dresd</bibkey>
    </paper>
    <paper id="1018">
      <title>Core: Robust Factual Precision with Informative Sub-Claim Identification</title>
      <author><first>Zhengping</first><last>Jiang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Nathaniel</first><last>Weir</last><affiliation>Amazon</affiliation></author>
      <author><first>Seth</first><last>Ebner</last><affiliation>Kensho</affiliation></author>
      <author><first>Miriam</first><last>Wanner</last></author>
      <author><first>Kate</first><last>Sanders</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Anqi</first><last>Liu</last><affiliation>Johns Hopkins University, California Institute of Technology and University of Illinois, Chicago</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>19833-19856</pages>
      <abstract>Hallucinations pose a challenge to the application of large language models (LLMs) thereby motivating the development of metrics to evaluate factual precision. We observe that popular metrics using the Decompose-Then-Verify framework, such as FActScore, can be manipulated by adding obvious or repetitive subclaims to artificially inflate scores. This observation motivates our new customizable plug-and-play subclaim selection component called Core, which filters down individual subclaims according to their uniqueness and informativeness. We show that many popular factual precision metrics augmented by Core are substantially more robust on a wide range of knowledge domains. We release an evaluation framework supporting easy and modular use of Core and various decomposition strategies, which we recommend adoption by the community. We also release an expansion of the FActScore biography dataset to facilitate further studies of decomposition-based factual precision evaluation.</abstract>
      <url hash="7a7998eb">2025.findings-acl.1018</url>
      <bibkey>jiang-etal-2025-core</bibkey>
    </paper>
    <paper id="1019">
      <title>Rethinking Diverse Human Preference Learning through Principal Component Analysis</title>
      <author><first>Feng</first><last>Luo</last></author>
      <author><first>Rui</first><last>Yang</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Jiarui</first><last>Yao</last></author>
      <author><first>Jingyan</first><last>Shen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Huan</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <pages>19857-19870</pages>
      <abstract>Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.</abstract>
      <url hash="fe72f066">2025.findings-acl.1019</url>
      <bibkey>luo-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="1020">
      <title>Improving Word Alignment Using Semi-Supervised Learning</title>
      <author><first>Zhongtao</first><last>Miao</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Qiyu</first><last>Wu</last><affiliation>Sony</affiliation></author>
      <author><first>Masaaki</first><last>Nagata</last><affiliation>NTT Corporation</affiliation></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>19871-19888</pages>
      <abstract>Word alignment plays a crucial role in various natural language processing tasks, such as serving as cross-lingual signals for sentence embedding, reducing hallucination and omission in machine translation, and facilitating the construction of training data for simultaneous speech translation.Current state-of-the-art approaches usually rely on: (1) supervised data and large-scale weakly supervised data constructed from Wikipedia and (2) multilingual Transformer encoder-based models.However, we find that the current state-of-the-art encoder-based method, BinaryAlign, suffers from the issue of insufficient labeled data, and we further improve it with self-training with a small amount of parallel data. In addition, considering the impressive performance of multilingual large language models on many natural language processing tasks, we also explore the possibility of using these decoder-based large language models as word aligners. We observe that although fine-tuning large language models with labeled data produces acceptable results, augmenting the training with pseudo-labeled data further enhances model performance. Based on the findings, we propose a semi-supervised framework to improve the large language model-based word aligners. Experimental results demonstrate that the proposed method with a small amount of parallel data outperforms the current state-of-the-art method on various word alignment datasets.</abstract>
      <url hash="ae66ff31">2025.findings-acl.1020</url>
      <bibkey>miao-etal-2025-improving</bibkey>
    </paper>
    <paper id="1021">
      <title>How Do <fixed-case>LLM</fixed-case>s Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Yunzhi</first><last>Yao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Hui</first><last>Jin</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jiacheng</first><last>Sun</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>19889-19913</pages>
      <abstract>Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how acquired knowledge becomes structurally embedded in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance.</abstract>
      <url hash="fe9bd75a">2025.findings-acl.1021</url>
      <bibkey>ou-etal-2025-llms</bibkey>
    </paper>
    <paper id="1022">
      <title><fixed-case>LLM</fixed-case>-Symbolic Integration for Robust Temporal Tabular Reasoning</title>
      <author><first>Atharv</first><last>Kulkarni</last></author>
      <author><first>Kushagra</first><last>Dixit</last></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>19914-19940</pages>
      <abstract>Temporal tabular question answering presents a significant challenge for Large Language Models (LLMs), requiring robust reasoning over structured data—a task where traditional prompting methods often fall short. These methods face challenges such as memorization, sensitivity to table size, and reduced performance on complex queries. To overcome these limitations, we introduce TEMPTABQA-C, a synthetic dataset designed for systematic and controlled evaluations, alongside a symbolic intermediate representation that transforms tables into database schemas. This structured approach allows LLMs to generate and execute SQL queries, enhancing generalization and mitigating biases. By incorporating adaptive fewshot prompting with contextually tailored examples, our method achieves superior robustness, scalability, and performance. Experimental results consistently highlight improvements across key challenges, setting a new benchmark for robust temporal reasoning with LLMs. Code and TEMPTABQA-C dataset: https:// coral-lab-asu.github.io/llm_symbolic.</abstract>
      <url hash="7083b44e">2025.findings-acl.1022</url>
      <bibkey>kulkarni-etal-2025-llm</bibkey>
    </paper>
    <paper id="1023">
      <title>Multimodal Large Language Models for Text-rich Image Understanding: A Comprehensive Review</title>
      <author><first>Pei</first><last>Fu</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Tongkun</first><last>Guan</last></author>
      <author><first>Zining</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Zhentao</first><last>Guo</last></author>
      <author><first>Chen</first><last>Duan</last><affiliation>Meituan</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Boming</first><last>Chen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Qianyi</first><last>Jiang</last></author>
      <author><first>Jiayao</first><last>Ma</last></author>
      <author><first>Kai</first><last>Zhou</last><affiliation>Meituan</affiliation></author>
      <author><first>Junfeng</first><last>Luo</last><affiliation>Meituan</affiliation></author>
      <pages>19941-19958</pages>
      <abstract>The recent emergence of Multi-modal Large Language Models (MLLMs) has introduced a new dimension to the Text-rich Image Understanding (TIU) field, with models demonstrating impressive and inspiring performance. However, their rapid evolution and widespread adoption have made it increasingly challenging to keep up with the latest advancements. To address this, we present a systematic and comprehensive survey to facilitate further research on TIU MLLMs. Initially, we outline the timeline, architecture, and pipeline of nearly all TIU MLLMs. Then, we review the performance of selected models on mainstream benchmarks. Finally, we explore promising directions, challenges, and limitations within the field.</abstract>
      <url hash="32f4af29">2025.findings-acl.1023</url>
      <bibkey>fu-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="1024">
      <title><fixed-case>P</fixed-case>rune<fixed-case>V</fixed-case>id: Visual Token Pruning for Efficient Video Large Language Models</title>
      <author><first>Xiaohu</first><last>Huang</last></author>
      <author><first>Hao</first><last>Zhou</last><affiliation>Baidu</affiliation></author>
      <author><first>Kai</first><last>Han</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>19959-19973</pages>
      <abstract>We introduce PruneVid, a training-free visual token pruning method designed to enhance the efficiency of multimodal video understanding. While Large Language Models (LLMs) have shown promising performance on video tasks due to their advanced visual comprehension capabilities, the substantial redundancy inherent in video data poses significant computational challenges. To address this issue, PruneVid (1) reduces intrinsic video redundancy by merging temporally static and spatially similar tokens, and (2) leverages LLMs’ inherent ability to selectively prune visual tokens irrelevant to specific queries, thereby improving model efficiency. We validate our method across multiple video benchmarks, demonstrating that PruneVid can prune over 80% of tokens while maintaining competitive performance when combined with different video LLMs. Our results highlight PruneVid’s superior effectiveness and efficiency compared to existing pruning methods.</abstract>
      <url hash="5258bd05">2025.findings-acl.1024</url>
      <bibkey>huang-etal-2025-prunevid</bibkey>
    </paper>
    <paper id="1025">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>W</fixed-case>izard: Optimizing Prompts via Task-Aware, Feedback-Driven Self-Evolution</title>
      <author><first>Eshaan</first><last>Agarwal</last></author>
      <author><first>Raghav</first><last>Magazine</last><affiliation>Microsoft</affiliation></author>
      <author><first>Joykirat</first><last>Singh</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Vivek</first><last>Dani</last></author>
      <author><first>Tanuja</first><last>Ganu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Akshay</first><last>Nambi</last><affiliation>Microsoft Research</affiliation></author>
      <pages>19974-20003</pages>
      <abstract>Large language models (LLMs) have transformed AI across diverse domains, with prompting being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard’s efficiency, scalability, and advantages over existing prompt optimization strategies.</abstract>
      <url hash="7695d279">2025.findings-acl.1025</url>
      <bibkey>agarwal-etal-2025-promptwizard</bibkey>
    </paper>
    <paper id="1026">
      <title>Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models</title>
      <author><first>Haoyang</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xuejia</first><last>Chen</last></author>
      <author><first>Zhanchao</first><last>Xu</last></author>
      <author><first>Darian</first><last>Li</last></author>
      <author><first>Nicole</first><last>Hu</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Fei</first><last>Teng</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yiming</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Luyu</first><last>Qiu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <author><first>Li</first><last>Qing</last><affiliation>The Hong Kong Polytechnic University and Hong Kong Polytechnic University</affiliation></author>
      <author><first>Lei</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology and Hong Kong University of Science and Technology</affiliation></author>
      <pages>20004-20026</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and multi-step reasoning. NumericBench includes datasets ranging from synthetic number lists to crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.</abstract>
      <url hash="a995a9f1">2025.findings-acl.1026</url>
      <bibkey>li-etal-2025-exposing</bibkey>
    </paper>
    <paper id="1027">
      <title><fixed-case>TABGEN</fixed-case>-<fixed-case>ICL</fixed-case>: Residual-Aware In-Context Example Selection for Tabular Data Generation</title>
      <author><first>Liancheng</first><last>Fang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hengrui</first><last>Zhang</last></author>
      <author><first>Henry Peng</first><last>Zou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Weizhi</first><last>Zhang</last><affiliation>Amazon and University of Illinois Chicago</affiliation></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>20027-20041</pages>
      <abstract>Large Language models (LLMs) have achieved encouraging results in tabular data generation. However, existing approaches require fine-tuning, which is computationally expensive. This paper explores an alternative: prompting a fixed LLM with in-context examples. We observe that using randomly selected in-context examples hampers the LLM’s performance, resulting in sub-optimal generation quality. To address this, we propose a novel in-context learning framework: TabGen-ICL, to enhance the in-context learning ability of LLMs for tabular data generation. TabGen-ICL operates iteratively, retrieving a subset of real samples that represent the residual between currently generated samples and true data distributions. This approach serves two purposes: locally, it provides more effective in-context learning examples for the LLM in each iteration; globally, it progressively narrows the gap between generated and real data. Extensive experiments on five real-world tabular datasets demonstrate that TabGen-ICL significantly outperforms the random selection strategy. Specifically, it reduces the error rate by a margin of up to 42.2% on the fidelity metric. We demonstrate for the first time that prompting a fixed LLM can yield high-quality synthetic tabular data.</abstract>
      <url hash="ecc76599">2025.findings-acl.1027</url>
      <bibkey>fang-etal-2025-tabgen</bibkey>
    </paper>
    <paper id="1028">
      <title>Benchmarking Multi-National Value Alignment for Large Language Models</title>
      <author><first>Chengyi</first><last>Ju</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weijie</first><last>Shi</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chengzhong</first><last>Liu</last></author>
      <author><first>Jiaming</first><last>Ji</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Ruiyuan</first><last>Zhang</last></author>
      <author><first>Jiajie</first><last>Xu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yaodong</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Sirui</first><last>Han</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>20042-20058</pages>
      <abstract>Do Large Language Models (LLMs) hold positions that conflict with your country’s values? Occasionally they do! However, existing works primarily focus on ethical reviews, failing to capture the diversity of national values, which encompass broader policy, legal, and moral considerations. Furthermore, current benchmarks that rely on spectrum tests using manually designed questionnaires are not easily scalable. To address these limitations, we introduce NaVAB, a comprehensive benchmark to evaluate the alignment of LLMs with the values of five major nations: China, the United States, the United Kingdom, France, and Germany. NaVAB implements a national value extraction pipeline to efficiently construct value assessment datasets. Specifically, we propose a modeling procedure with instruction tagging to process raw data sources, a screening process to filter value-related topics and a generation process with a Conflict Reduction mechanism to filter non-conflicting values. We conduct extensive experiments on various LLMs across countries, and the results provide insights into assisting in the identification of misaligned scenarios. Moreover, we demonstrate that NaVAB can be combined with alignment techniques to effectively reduce value concerns by aligning LLMs’ values with the target country.</abstract>
      <url hash="00c3be19">2025.findings-acl.1028</url>
      <bibkey>ju-etal-2025-benchmarking</bibkey>
    </paper>
    <paper id="1029">
      <title><fixed-case>M</fixed-case>otive<fixed-case>B</fixed-case>ench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?</title>
      <author><first>Xixian</first><last>Yong</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Xiao</first><last>Zhou</last></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>20059-20089</pages>
      <abstract>Large language models (LLMs) have been widely adopted as the core of agent frameworks in various scenarios, such as social simulations and AI companions. However, the extent to which they can replicate human-like motivations remains an underexplored question. Existing benchmarks are constrained by simplistic scenarios and the absence of character identities, resulting in an information asymmetry with real-world situations. To address this gap, we propose MotiveBench, which consists of 200 rich contextual scenarios and 600 reasoning tasks covering multiple levels of motivation. Using MotiveBench, we conduct extensive experiments on seven popular model families, comparing different scales and versions within each family. The results show that even the most advanced LLMs still fall short in achieving human-like motivational reasoning. Our analysis reveals key findings, including the difficulty LLMs face in reasoning about “love &amp; belonging” motivations and their tendency toward excessive rationality and idealism. These insights highlight a promising direction for future research on the humanization of LLMs.</abstract>
      <url hash="feddc568">2025.findings-acl.1029</url>
      <bibkey>yong-etal-2025-motivebench</bibkey>
    </paper>
    <paper id="1030">
      <title>Confidence Improves Self-Consistency in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Amir</first><last>Taubenfeld</last><affiliation>Hebrew University of Jerusalem and Google</affiliation></author>
      <author><first>Tom</first><last>Sheffer</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Eran</first><last>Ofek</last><affiliation>Google</affiliation></author>
      <author><first>Amir</first><last>Feder</last><affiliation>Columbia University and Google</affiliation></author>
      <author><first>Ariel</first><last>Goldstein</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Zorik</first><last>Gekhman</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Gal</first><last>Yona</last><affiliation>Research, Google</affiliation></author>
      <pages>20090-20111</pages>
      <abstract>Self-consistency decoding enhances LLMs’ performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.</abstract>
      <url hash="e634375e">2025.findings-acl.1030</url>
      <bibkey>taubenfeld-etal-2025-confidence</bibkey>
    </paper>
    <paper id="1031">
      <title>None of the Above, Less of the Right Parallel Patterns in Human and <fixed-case>LLM</fixed-case> Performance on Multi-Choice Questions Answering</title>
      <author><first>Zhi Rui</first><last>Tam</last><affiliation>Appier</affiliation></author>
      <author><first>Cheng-Kuang</first><last>Wu</last><affiliation>Appier</affiliation></author>
      <author><first>Chieh-Yen</first><last>Lin</last><affiliation>Appier Inc.</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>20112-20134</pages>
      <abstract>Multiple-choice exam questions with “None of the above” (NA) options have been extensively studied in educational testing, in which existing research suggests that they better assess true knowledge. However, their impact on Large Language Models (LLMs) evaluation remains underexplored. Through systematic experiments with 28 LLMs on the MMLU benchmark, we examine how NA options affect model performance and confidence calibration. Our analysis reveals that NA options, when used as the correct answer, lead to a consistent 30-50% performance drop across models regardless of scale–suggesting that LLMs lack the meta-cognitive ability to systematically evaluate and reject all given options when none are correct. This degradation shows strong domain dependence, with minimal impact on mathematical reasoning (14.6% drop) but severe effects on tasks requiring uncertainty handling like business ethics (48.1% drop). Our results highlight important implications for benchmark design and raise questions about LLMs’ ability to handle uncertainty in real-world applications.</abstract>
      <url hash="794f18f6">2025.findings-acl.1031</url>
      <bibkey>tam-etal-2025-none</bibkey>
    </paper>
    <paper id="1032">
      <title>In Search of the Lost Arch in Dialogue: A Dependency Dialogue Acts Corpus for Multi-Party Dialogues</title>
      <author><first>Jon</first><last>Cai</last></author>
      <author><first>Brendan</first><last>King</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Peyton</first><last>Cameron</last></author>
      <author><first>Susan Windisch</first><last>Brown</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Miriam</first><last>Eckert</last></author>
      <author><first>Dananjay</first><last>Srinivas</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>George Arthur</first><last>Baker</last><affiliation>University of Utah and University of Colorado Boulder</affiliation></author>
      <author><first>V Kate</first><last>Everson</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Martha</first><last>Palmer</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>James</first><last>Martin</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Jeffrey</first><last>Flanigan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>20135-20149</pages>
      <abstract>Understanding the structure of multi-party conversation and the intentions and dialogue acts of each speaker remains a significant challenge in NLP. While a number of corpora annotated using theoretical frameworks of dialogue have been proposed, these typically focus on either utterance-level labeling of speaker intent, missing wider context, or the rhetorical structure of a dialogue, losing fine-grained intents captured in dialogue acts. Recently, the Dependency Dialogue Acts (DDA) framework has been proposed to for modeling both the fine-grained intents of each speaker and the structure of multi-party dialogues. However, there is not yet a corpus annotated with this framework available for the community to study. To address this gap, we introduce a new corpus of 33 dialogues and over 9,000 utterance units, densely annotated using the Dependency Dialogue Acts (DDA) framework.Our dataset spans four genres of multi-party conversations from different modalities: (1) physics classroom discussions, (2) engineering classroom discussions, (3) board game interactions, and (4) written online game chat logs. Each session is doubly annotated and adjudicated to ensure high-quality labeling. We present a description of the dataset and annotation process, an analysis of speaker dynamics enabled by our annotation, and a baseline evaluation of LLMs as DDA parsers. We discuss the implications of this dataset understanding dynamics between speakers and for developing more controllable dialogue agents.</abstract>
      <url hash="915fa151">2025.findings-acl.1032</url>
      <bibkey>cai-etal-2025-search</bibkey>
    </paper>
    <paper id="1033">
      <title><fixed-case>P</fixed-case>ro<fixed-case>M</fixed-case>ind-<fixed-case>LLM</fixed-case>: Proactive Mental Health Care via Causal Reasoning with Sensor Data</title>
      <author><first>Xinzhe</first><last>Zheng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Sijie</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Sun</last></author>
      <author><first>Renqi</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Wei</first><last>Gao</last></author>
      <author><first>Mani</first><last>Srivastava</last><affiliation>Amazon and University of California, Los Angeles</affiliation></author>
      <pages>20150-20171</pages>
      <abstract>Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.</abstract>
      <url hash="f0b287d3">2025.findings-acl.1033</url>
      <bibkey>zheng-etal-2025-promind</bibkey>
    </paper>
    <paper id="1034">
      <title>Debiasing Online Preference Learning via Preference Feature Preservation</title>
      <author><first>Dongyoung</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jinsung</first><last>Yoon</last><affiliation>Google</affiliation></author>
      <author><first>Jinwoo</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jaehyung</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <pages>20172-20191</pages>
      <abstract>Recent preference learning frameworks for large language models (LLMs) simplify human preferences with binary pairwise comparisons and scalar rewards. This simplification could make LLMs’ responses biased to mostly preferred features, and would be exacerbated during the iterations of online preference learning steps. To address these challenges, we propose a novel framework coined PFP (Preference Feature Preservation). The key idea of PFP is maintaining the distribution of human preference features and utilizing such rich signals throughout the online preference learning process. Specifically, PFP first extract preference features from offline pairwise human preference data and trains a feature classifier. Then, using trained classifier and the distribution preserving optimization, PFP maps appropriate preference features for a new input instruction during online learning. Lastly, PFP trains LLM using the existing preference learning method, by incorporating the preference feature into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features during online learning, and hence achieves superior performance compared to previous preference learning methods on standard benchmarks to evaluate LLM alignment.</abstract>
      <url hash="b9b74548">2025.findings-acl.1034</url>
      <bibkey>kim-etal-2025-debiasing</bibkey>
    </paper>
    <paper id="1035">
      <title><fixed-case>S</fixed-case>hort<fixed-case>GPT</fixed-case>: Layers in Large Language Models are More Redundant Than You Expect</title>
      <author><first>Xin</first><last>Men</last></author>
      <author><first>Mingyu</first><last>Xu</last></author>
      <author><first>Qingyu</first><last>Zhang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Qianhao</first><last>Yuan</last><affiliation>iscas</affiliation></author>
      <author><first>Bingning</first><last>Wang</last><affiliation>Beijing Baichuan Intelligence Technology Co., Ltd.</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Weipeng</first><last>Chen</last></author>
      <pages>20192-20204</pages>
      <abstract>As Large Language Models (LLMs) continue to advance, their computational overhead has increased significantly. In this study, we identify notable redundancy across the layers of LLMs, where some layers contribute minimally to the overall network functionality. To quantify this, we introduce a metric called Block Influence (BI), which measures the importance of each layer based on the similarity between its input and output. Based on the observation of layer redundancy, we propose straightforward pruning methods for different tasks: ShortGPT for multiple-choice tasks and ShortGPT-gen for generative tasks. They prune redundant layers based on their BI scores. Our methods demonstrate superior performance over previous pruning methods. The ability to achieve better results through simple layer pruning, as opposed to more complex pruning techniques, suggests a high degree of redundancy across layers. We hope this work will contribute to future research for improving LLM efficiency.</abstract>
      <url hash="ef916be2">2025.findings-acl.1035</url>
      <bibkey>men-etal-2025-shortgpt</bibkey>
    </paper>
    <paper id="1036">
      <title><fixed-case>P</fixed-case>roject<fixed-case>E</fixed-case>val: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation</title>
      <author><first>Kaiyuan</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Youcheng</first><last>Pan</last><affiliation>Pengcheng Laboratory</affiliation></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yexing</first><last>Du</last></author>
      <author><first>Tianrun</first><last>Gao</last></author>
      <pages>20205-20221</pages>
      <abstract>Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users’ perspective, and also lack the explainability of the results of LLM agents’ code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation’s automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.</abstract>
      <url hash="5993de0f">2025.findings-acl.1036</url>
      <bibkey>liu-etal-2025-projecteval</bibkey>
    </paper>
    <paper id="1037">
      <title>Unveiling the Lack of <fixed-case>LVLM</fixed-case> Robustness to Fundamental Visual Variations: Why and Path Forward</title>
      <author><first>Zhiyuan</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yumeng</first><last>Wang</last></author>
      <author><first>Sandeep</first><last>Polisetty</last></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>20222-20242</pages>
      <abstract>Large Vision Language Models (LVLMs) have shown impressive performance on various vision-language tasks. However, while objects in natural scenes inevitably exhibit visual variations in position, scale, orientation, and context due to changes in viewpoint and environment, the robustness of LVLMs to these fundamental visual variations remains largely unexplored. To address this gap, we introduce V²R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation of 13 LVLMs, we reveal a surprising vulnerability to visual variations, affecting even advanced models that excel at complex vision-language tasks yet significantly underperform on simple tasks like object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we propose a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural challenges, underscoring the need for architectural innovations in future LVLM designs.</abstract>
      <url hash="2732b261">2025.findings-acl.1037</url>
      <bibkey>fan-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="1038">
      <title><fixed-case>DYNTEXT</fixed-case>: Semantic-Aware Dynamic Text Sanitization for Privacy-Preserving <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>Juhua</first><last>Zhang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Minghang</first><last>Zhu</last><affiliation>Shandong University</affiliation></author>
      <author><first>Yiping</first><last>Song</last></author>
      <author><first>Taishu</first><last>Sheng</last></author>
      <author><first>Siyi</first><last>Yang</last></author>
      <author><first>Qiunan</first><last>Du</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xinwang</first><last>Liu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>20243-20255</pages>
      <abstract>LLMs face privacy risks when handling sensitive data. To ensure privacy, researchers use differential privacy (DP) to provide protection by adding noise during LLM training. However, users may be hesitant to share complete data with LLMs. Researchers follow local DP to sanitize the text on the user side and feed non-sensitive text to LLMs. The sanitization usually uses a fixed non-sensitive token list or a fixed noise distribution, which induces the risk of being attacked or semantic distortion. We argue that the token’s protection level should be adaptively adjusted according to its semantic-based information to balance the privacy-utility trade-off. In this paper, we propose DYNTEXT, an LDP-based Dynamic Text sanitization for privacy-preserving LLM inference, which dynamically constructs semantic-aware adjacency lists of sensitive tokens to sample non-sensitive tokens for perturbation. Specifically, DYNTEXT first develops a semantic-based density modeling under DP to extract each token’s density information. We propose token-level smoothing sensitivity by combining the idea of global sensitivity (GS) and local sensitivity (LS), which dynamically adjusts the noise scale to avoid excessive noise in GS and privacy leakage in LS. Then, we dynamically construct an adjacency list for each sensitive token based on its semantic density information. Finally, we apply the replacement mechanism to sample non-sensitive, semantically similar tokens from the adjacency list to replace sensitive tokens. Experiments show that DYNTEXT excels strong baselines on three datasets.</abstract>
      <url hash="b39a86ec">2025.findings-acl.1038</url>
      <bibkey>zhang-etal-2025-dyntext</bibkey>
    </paper>
    <paper id="1039">
      <title><fixed-case>I</fixed-case>n<fixed-case>I</fixed-case>mage<fixed-case>T</fixed-case>rans: Multimodal <fixed-case>LLM</fixed-case>-based Text Image Machine Translation</title>
      <author><first>Fei</first><last>Zuo</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen (HITSZ)</affiliation></author>
      <author><first>Zhengshan</first><last>Xue</last><affiliation>Tianjin University and OPPO</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>20256-20277</pages>
      <abstract>Multimodal large language models (MLLMs) have shown remarkable capabilities across various downstream tasks. However, when MLLMs are transferred to the text image machine translation (TiMT) task, preliminary experiments reveal that MLLMs suffer from serious repetition and omission hallucinations. To alleviate these issues, this paper first designs an efficient MLLM named InImageTrans for TiMT and then proposes a simple and effective method named multi-conditional direct preference optimization (mcDPO) for advancing the TiMT. Particularly, the proposed mcDPO not only guides the MLLM in rejecting repetition output by creating text output preference pairs automatically, but also guides the MLLM in paying more attention to text information in images by creating image input preference pairs. Furthermore, we build a high-quality benchmark called MCiT for comprehensively evaluating the TiMT capabilities of InImageTrans. Experimental results show that the proposed method significantly outperforms existing open-source MLLMs on MCiT.</abstract>
      <url hash="3be8c15a">2025.findings-acl.1039</url>
      <bibkey>zuo-etal-2025-inimagetrans</bibkey>
    </paper>
    <paper id="1040">
      <title><fixed-case>FRAME</fixed-case>: Boosting <fixed-case>LLM</fixed-case>s with A Four-Quadrant Multi-Stage Pretraining Strategy</title>
      <author><first>Xuemiao</first><last>Zhang</last></author>
      <author><first>Feiyu</first><last>Duan</last></author>
      <author><first>Xu</first><last>Liangyu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Yongwei</first><last>Zhou</last><affiliation>Meituan</affiliation></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Rongxiang</first><last>Weng</last><affiliation>Meituan</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <pages>20278-20297</pages>
      <abstract>Large language models (LLMs) have significantly advanced human language understanding and generation, with pretraining data quality and organization being crucial to their performance. Multi-stage pretraining is a promising approach, but existing methods often lack quantitative criteria for data partitioning and instead rely on intuitive heuristics. In this paper, we propose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME), guided by the established principle of organizing the pretraining process into four stages to achieve significant loss reductions four times. This principle is grounded in two key findings: first, training on high Perplexity (PPL) data followed by low PPL data, and second, training on low PPL difference (PD) data followed by high PD data, both causing the loss to drop significantly twice and performance enhancements. By partitioning data into four quadrants and strategically organizing them, FRAME achieves a remarkable 16.8% average improvement over random across MMLU and CMMLU for the 3B model, effectively boosting LLM performance.</abstract>
      <url hash="2f4f3876">2025.findings-acl.1040</url>
      <bibkey>zhang-etal-2025-frame</bibkey>
    </paper>
    <paper id="1041">
      <title>When Large Language Models Meet Speech: A Survey on Integration Approaches</title>
      <author><first>Zhengdong</first><last>Yang</last></author>
      <author><first>Shuichiro</first><last>Shimizu</last></author>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <pages>20298-20315</pages>
      <abstract>Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for future research.</abstract>
      <url hash="c0e7148b">2025.findings-acl.1041</url>
      <bibkey>yang-etal-2025-large-language</bibkey>
    </paper>
    <paper id="1042">
      <title><fixed-case>KE</fixed-case>-<fixed-case>MHISTO</fixed-case>: Towards a Multilingual Historical Knowledge Extraction Benchmark for Addressing the Long-Tail Problem</title>
      <author><first>Arianna</first><last>Graciotti</last></author>
      <author><first>Leonardo</first><last>Piano</last></author>
      <author><first>Nicolas</first><last>Lazzari</last></author>
      <author><first>Enrico</first><last>Daga</last><affiliation>Open University</affiliation></author>
      <author><first>Rocco</first><last>Tripodi</last><affiliation>University of Venice</affiliation></author>
      <author><first>Valentina</first><last>Presutti</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Livio</first><last>Pompianu</last><affiliation>Università Degli Studi di Cagliari</affiliation></author>
      <pages>20316-20339</pages>
      <abstract>Large Language Models (LLMs) face significant challenges when queried about long-tail knowledge, i.e., information that is rarely encountered during their training process. These difficulties arise due to the inherent sparsity of such data. Furthermore, LLMs often lack the ability to verify or ground their responses in authoritative sources, which can lead to plausible yet inaccurate outputs when addressing infrequent subject matter. Our work aims to investigate these phenomena by introducing KE-MHISTO, a multilingual benchmark for Entity Linking and Question Answering in the domain of historical music knowledge, available in both Italian and English. We demonstrate that KE-MHISTO provides significantly broader coverage of long-tail knowledge compared to existing alternatives. Moreover, it poses substantial challenges for state-of-the-art models. Our experiments reveal that smaller, multilingual models can achieve performance comparable to significantly larger counterparts, highlighting the potential of efficient, language-aware approaches for long-tail knowledge extraction. KE-MHISTO is available at: https://github.com/polifonia-project/KE-MHISTO.</abstract>
      <url hash="aa5c1f97">2025.findings-acl.1042</url>
      <bibkey>graciotti-etal-2025-ke</bibkey>
    </paper>
    <paper id="1043">
      <title><fixed-case>T</fixed-case>ailor<fixed-case>KV</fixed-case>: A Hybrid Framework for Long-Context Inference via Tailored <fixed-case>KV</fixed-case> Cache Optimization</title>
      <author><first>Dingyu</first><last>Yao</last></author>
      <author><first>Bowen</first><last>Shen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Weiping</first><last>Wang</last><affiliation>IIE</affiliation></author>
      <pages>20340-20359</pages>
      <abstract>The Key-Value (KV) cache in generative large language models (LLMs) introduces substantial memory overhead. Existing works mitigate this burden by offloading or compressing the KV cache. However, loading the entire cache incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU communication, while aggressive compression causes notable performance degradation. We identify that certain layers in the LLM need to maintain global information and are unsuitable for selective loading. In contrast, other layers primarily focus on a few tokens with dominant activations that potentially incur substantial quantization error. This observation leads to a key insight that loading dominant tokens and quantizing all tokens can complement each other. Building on this insight, we propose a hybrid compression method, TailorKV, which seamlessly integrates quantization and offloading. TailorKV develops an inference framework along with a hardware-friendly implementation that leverages these complementary characteristics. Extensive long-context evaluations exhibit that TailorKV achieves nearly lossless performance under aggressive compression settings, outperforming the state-of-the-art. Particularly, the Llama-3.1-8B with 128k context can be served within a single RTX 3090 GPU, reaching 82 ms per token during decoding.</abstract>
      <url hash="a2a777be">2025.findings-acl.1043</url>
      <bibkey>yao-etal-2025-tailorkv</bibkey>
    </paper>
    <paper id="1044">
      <title>The Elephant in the Room: Exploring the Role of Neutral Words in Language Model Group-Agnostic Debiasing</title>
      <author><first>Xinwei</first><last>Guo</last></author>
      <author><first>Jiashi</first><last>Gao</last></author>
      <author><first>Junlei</first><last>Zhou</last></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Quanying</first><last>Liu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Haiyan</first><last>Wu</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xin</first><last>Yao</last></author>
      <author><first>Xuetao</first><last>Wei</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>20360-20371</pages>
      <abstract>Large Language Models (LLMs) are increasingly integrated into our daily lives, raising significant ethical concerns, especially about perpetuating stereotypes.While group-specific debiasing methods have made progress, they often fail to address multiple biases simultaneously. In contrast, group-agnostic debiasing has the potential to mitigate a variety of biases at once, but remains underexplored.In this work, we investigate the role of neutral words—the group-agnostic component—in enhancing the group-agnostic debiasing process. We first reveal that neutral words are essential for preserving semantic modeling, and we propose <tex-math>\epsilon</tex-math>-DPCE, a method that incorporates a neutral word semantics-based loss function to effectively alleviate the deterioration of the Language Modeling Score (LMS) during the debiasing process. Furthermore, by introducing the SCM-Projection method, we demonstrate that SCM-based debiasing eliminates stereotypes by indirectly disrupting the association between attribute and neutral words in the Stereotype Content Model (SCM) space. Our experiments show that neutral words, which often embed multi-group stereotypical objects, play a key role in contributing to the group-agnostic nature of SCM-based debiasing.</abstract>
      <url hash="0646a5b1">2025.findings-acl.1044</url>
      <bibkey>guo-etal-2025-elephant</bibkey>
    </paper>
    <paper id="1045">
      <title><fixed-case>LLM</fixed-case>s Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline</title>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Minpeng</first><last>Liao</last></author>
      <author><first>Kai</first><last>Fan</last><affiliation>Alibaba Group and Alibaba Group</affiliation></author>
      <author><first>Chengxi</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <pages>20372-20395</pages>
      <abstract>When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt “Translate the following sentence from [src lang] into [tgt lang]:”. However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks and different evaluation metrics, and preserves the original capabilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.</abstract>
      <url hash="512a8158">2025.findings-acl.1045</url>
      <bibkey>fu-etal-2025-llms</bibkey>
    </paper>
    <paper id="1046">
      <title>Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning</title>
      <author><first>Yin</first><last>Hua</last></author>
      <author><first>Zhiqiang</first><last>Liu</last></author>
      <author><first>Mingyang</first><last>Chen</last><affiliation>Baichuan Inc.</affiliation></author>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Chi Man</first><last>Wong</last></author>
      <author><first>Lingxiao</first><last>Li</last></author>
      <author><first>Chi Man</first><last>Vong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>20396-20412</pages>
      <abstract>In natural language processing (NLP) and computer vision (CV), the successful application of foundation models across diverse tasks has demonstrated their remarkable potential. However, despite the rich structural and textual information embedded in knowledge graphs (KGs), existing research of foundation model for KG has primarily focused on their structural aspects, with most efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This limitation has hindered progress in addressing more challenging out-of-KG tasks. In this paper, we introduce MERRY, a foundation model for general knowledge graph reasoning, and investigate its performance across two task categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG question answering, KGQA). We not only utilize the structural information, but also the textual information in KGs. Specifically, we propose a multi-perspective Conditional Message Passing (CMP) encoding architecture to bridge the gap between textual and structural modalities, enabling their seamless integration. Additionally, we introduce a dynamic residual fusion module to selectively retain relevant textual information and a flexible edge scoring mechanism to adapt to diverse downstream tasks. Comprehensive evaluations on 28 datasets demonstrate that MERRY outperforms existing baselines in most scenarios, showcasing strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks such as KGQA.</abstract>
      <url hash="8bdaf0a3">2025.findings-acl.1046</url>
      <bibkey>hua-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1047">
      <title>Generative Error Correction for Emotion-aware Speech-to-text Translation</title>
      <author><first>Zhengdong</first><last>Yang</last></author>
      <author><first>Sheng</first><last>Li</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <pages>20413-20421</pages>
      <abstract>This paper explores emotion-aware speech-to-text translation (ST) using generative error correction (GER) by large language models (LLMs). Despite recent advancements in ST, the impact of the emotional content has been overlooked. First, we enhance the translation of emotional speech by adopting the GER paradigm: Finetuned an LLM to generate the translation based on the decoded N-best hypotheses. Moreover, we combine the emotion and sentiment labels into the LLM finetuning process to enable the model to consider the emotion content. In addition, we project the ST model’s latent representation into the LLM embedding space to further improve emotion recognition and translation. Experiments on an English-Chinese dataset show the effectiveness of the combination of GER, emotion and sentiment labels, and the projector for emotion-aware ST. Our code is available at https://github.com/N-Orien/EmoST.</abstract>
      <url hash="ac1c3eec">2025.findings-acl.1047</url>
      <bibkey>yang-etal-2025-generative</bibkey>
    </paper>
    <paper id="1048">
      <title><fixed-case>S</fixed-case>ynaptic<fixed-case>RAG</fixed-case>: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms</title>
      <author><first>Yuki</first><last>Hou</last></author>
      <author><first>Haruki</first><last>Tamoto</last></author>
      <author><first>Qinghua</first><last>Zhao</last></author>
      <author><first>Homei</first><last>Miyashita</last><affiliation>Meiji University</affiliation></author>
      <pages>20422-20436</pages>
      <abstract>Existing retrieval methods in Large Language Models show degradation in accuracy when handling temporally distributed conversations, primarily due to their reliance on simple similarity-based retrieval. Unlike existing memory retrieval methods that rely solely on semantic similarity, we propose SynapticRAG, which uniquely combines temporal association triggers with biologically-inspired synaptic propagation mechanisms. Our approach uses temporal association triggers and synaptic-like stimulus propagation to identify relevant dialogue histories. A dynamic leaky integrate-and-fire mechanism then selects the most contextually appropriate memories. Experiments on four datasets of English, Chinese and Japanese show that compared to state-of-the-art memory retrieval methods, SynapticRAG achieves consistent improvements across multiple metrics up to 14.66% points. This work bridges the gap between cognitive science and language model development, providing a new framework for memory management in conversational systems.</abstract>
      <url hash="b0be8806">2025.findings-acl.1048</url>
      <bibkey>hou-etal-2025-synapticrag</bibkey>
    </paper>
    <paper id="1049">
      <title>Localizing and Mitigating Errors in Long-form Question Answering</title>
      <author><first>Rachneet Singh</first><last>Sachdeva</last></author>
      <author><first>Yixiao</first><last>Song</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>20437-20469</pages>
      <abstract>Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-Informed Refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves the quality of the answers across multiple models. Furthermore, humans find the answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers.</abstract>
      <url hash="fb3ef102">2025.findings-acl.1049</url>
      <bibkey>sachdeva-etal-2025-localizing</bibkey>
    </paper>
    <paper id="1050">
      <title><fixed-case>EMGLLM</fixed-case>: Data-to-Text Alignment for Electromyogram Diagnosis Generation with Medical Numerical Data Encoding</title>
      <author><first>Zefei</first><last>Long</last></author>
      <author><first>Zhenbiao</first><last>Cao</last></author>
      <author><first>Wei</first><last>Chen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>20470-20480</pages>
      <abstract>Electromyography (EMG) tables are crucial for diagnosing muscle and nerve disorders, and advancing the automation of EMG diagnostics is significant for improving medical efficiency. EMG tables contain extensive continuous numerical data, which current Large Language Models (LLMs) often struggle to interpret effectively. To address this issue, we propose EMGLLM, a data-to-text model specifically designed for medical examination tables. EMGLLM employs the EMG Alignment Encoder to simulate the process that doctors compare test values with reference values, aligning the data into word embeddings that reflect health degree. Additionally, we construct ETM, a dataset comprising 17,250 real cases and their corresponding diagnostic results, to support medical data-to-text tasks. Experimental results on ETM demonstrate that EMGLLM outperforms various baseline models in understanding EMG tables and generating high-quality diagnoses, which represents an effective paradigm for automatic diagnosis generation from medical examination table.</abstract>
      <url hash="8887cc7f">2025.findings-acl.1050</url>
      <bibkey>long-etal-2025-emgllm</bibkey>
    </paper>
    <paper id="1051">
      <title><fixed-case>LLMV</fixed-case>o<fixed-case>X</fixed-case>: Autoregressive Streaming Text-to-Speech Model for Any <fixed-case>LLM</fixed-case></title>
      <author><first>Sambal</first><last>Shikhar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mohammed Irfan</first><last>Kurpath</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sahal Shaji</first><last>Mullappilly</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jean</first><last>Lahoud</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Rao Muhammad</first><last>Anwer</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <author><first>Hisham</first><last>Cholakkal</last><affiliation>MBZUAI</affiliation></author>
      <pages>20481-20493</pages>
      <abstract>Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX enables seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with minimal dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Evaluations demonstrate that LLMVoX matches or surpasses existing speech-enabled LLMs in both speech quality and latency, while maintaining the original linguistic strengths of the LLM. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training.</abstract>
      <url hash="a72fa389">2025.findings-acl.1051</url>
      <bibkey>shikhar-etal-2025-llmvox</bibkey>
    </paper>
    <paper id="1052">
      <title><fixed-case>A</fixed-case>ct2<fixed-case>P</fixed-case>: <fixed-case>LLM</fixed-case>-Driven Online Dialogue Act Classification for Power Analysis</title>
      <author><first>Zhangwenbo</first><last>Zhangwenbo</last></author>
      <author><first>Wang</first><last>Yuhan</last><affiliation>Beijing University of Technology</affiliation></author>
      <pages>20494-20504</pages>
      <abstract>In team communication, dialogue acts play a crucial role in helping team members understand each other’s intentions and revealing the roles and communication patterns within interactions. Although existing studies have focused on using Dialogue Act classification to capture the speaker’s intentions, few have explored the underlying power dynamics reflected by these dialogue acts. To this end, we present an online Dialogue Act Classification and Dynamic Power Analysis framework—Act2P, which is based on large language model. The framework combines the zero-shot learning capability of LLMs and introduces an online feedback classification method that allows for online classification with iterative feedback to previous stages, achieving efficient and accurate classification without the labeled data. Additionally, we also propose the PowerRank algorithm, which quantifies power dynamics through a graph-based structure. Through comparative experiments with existing methods, we demonstrate the significant superiority of Act2P in online scenarios and successfully visualize dialogue power in online, clearly presenting the distribution and dynamic transfer of power. This framework provides new scientific insights and practical tools for optimizing team collaboration.</abstract>
      <url hash="6cf01814">2025.findings-acl.1052</url>
      <bibkey>zhangwenbo-yuhan-2025-act2p</bibkey>
    </paper>
    <paper id="1053">
      <title><fixed-case>MELAB</fixed-case>enchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource <fixed-case>M</fixed-case>altese <fixed-case>NLP</fixed-case></title>
      <author><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>20505-20527</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend for researchers working with low-resource languages to consider more “traditional” language modelling approaches.</abstract>
      <url hash="5272bf63">2025.findings-acl.1053</url>
      <bibkey>micallef-borg-2025-melabenchv1</bibkey>
    </paper>
    <paper id="1054">
      <title><fixed-case>TRATES</fixed-case>: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring</title>
      <author><first>Sohaila</first><last>Eltanbouly</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Salam</first><last>Albatarni</last></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <pages>20528-20543</pages>
      <abstract>Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.</abstract>
      <url hash="d0355eda">2025.findings-acl.1054</url>
      <bibkey>eltanbouly-etal-2025-trates</bibkey>
    </paper>
    <paper id="1055">
      <title><fixed-case>DAST</fixed-case>: Context-Aware Compression in <fixed-case>LLM</fixed-case>s via Dynamic Allocation of Soft Tokens</title>
      <author><first>Shaoshen</first><last>Chen</last></author>
      <author><first>Yangning</first><last>Li</last></author>
      <author><first>Zishan</first><last>Xu</last></author>
      <author><first>Yongqin</first><last>Zeng</last></author>
      <author><first>Shunlong</first><last>Wu</last></author>
      <author><first>Xinshuo</first><last>Hu</last></author>
      <author><first>Zifei</first><last>Shan</last><affiliation>WeChat, Tencent</affiliation></author>
      <author><first>Xin</first><last>Su</last><affiliation>WeChat, Tencent</affiliation></author>
      <author><first>Jiwei</first><last>Tang</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Hai-Tao</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>20544-20552</pages>
      <abstract>Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM’s intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.</abstract>
      <url hash="985b59a3">2025.findings-acl.1055</url>
      <bibkey>chen-etal-2025-dast</bibkey>
    </paper>
    <paper id="1056">
      <title>A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs</title>
      <author><first>Yimin</first><last>Deng</last><affiliation>City University of Hong Kong and Xi’an Jiaotong University</affiliation></author>
      <author><first>Yuxia</first><last>Wu</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Yejing</first><last>Wang</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Guoshuai</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Li</first><last>Zhu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Qidong</first><last>Liu</last><affiliation>City University of Hong Kong and Xi’an Jiaotong University</affiliation></author>
      <author><first>Derong</first><last>Xu</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Zichuan</first><last>Fu</last></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Westlake University</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Xueming</first><last>Qian</last><affiliation>Xi’an Jiaotong University, Tsinghua University</affiliation></author>
      <pages>20553-20565</pages>
      <abstract>Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a **M**ulti-**E**xpert **S**tructural-**S**emantic **H**ybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.</abstract>
      <url hash="99f5df5e">2025.findings-acl.1056</url>
      <bibkey>deng-etal-2025-multi</bibkey>
    </paper>
    <paper id="1057">
      <title><fixed-case>MWPO</fixed-case>: Enhancing <fixed-case>LLM</fixed-case>s Performance through Multi-Weight Preference Strength and Length Optimization</title>
      <author><first>Shiyue</first><last>Xu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Linfeng</first><last>Zhou</last></author>
      <pages>20566-20581</pages>
      <abstract>Direct Preference Optimization (DPO) have proposed offline alternatives to Reinforcement Learning from Human Feedback (RLHF). In DPO, each preference pair, which serves as the foundation for learning, is typically constructed by first generating multiple responses to the same instruction and then annotating them to indicate the preferred choice. However, when the responses are highly similar, the weak preference signal can introduce annotation noise, which may hinder model optimization. Additionally, DPO suffers from the drawback of over-optimizing for verbose generation. A potential reason is the presence of length bias in preference datasets, which can lead to length exploitation. To address these issues, we propose a DPO-based **m**ulti-**w**eight **p**reference strength and length **o**ptimization (MWPO) method. Specifically, we propose to reweight preference pairs based on implicit reward margins and response length margins, unifying them through a geometric mixture to generate synthetic weights for optimization. This method allows preference pairs with stronger preference signals or more favorable length feature to have a more pronounced impact on model parameters. Moreover, our method does not require additional annotators. We validate our method on models of four different scales across multiple benchmarks. Our method surpasses state-of-the-art (SOTA) baselines, outperforming DPO by up to 8.7% on AlpacaEval 2 while reducing generation length by 9.4% in the Mistral setting. Our code is available at https://github.com/AIR-hl/MWPO.</abstract>
      <url hash="7197886d">2025.findings-acl.1057</url>
      <bibkey>xu-etal-2025-mwpo</bibkey>
    </paper>
    <paper id="1058">
      <title><fixed-case>CLEAR</fixed-case>: Character Unlearning in Textual and Visual Modalities</title>
      <author><first>Alexey</first><last>Dontsov</last><affiliation>Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Dmitrii</first><last>Korzh</last><affiliation>AIRI and Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Alexey</first><last>Zhavoronkin</last></author>
      <author><first>Boris</first><last>Mikheev</last></author>
      <author><first>Denis</first><last>Bobkov</last><affiliation>Higher School of Economics, Higher School of Economics and Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Aibek</first><last>Alanov</last><affiliation>Higher School of Economics and Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Oleg</first><last>Rogov</last><affiliation>Moscow Technical University of Informatics and Communication, AIRI and University of Sharjah</affiliation></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology and Institute of Numerical Mathematics</affiliation></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <pages>20582-20603</pages>
      <abstract>Machine Unlearning (MU) is critical for removing private or hazardous information from deep learning models. While MU has advanced significantly in unimodal (text or vision) settings, multimodal unlearning (MMU) remains underexplored due to the lack of open benchmarks for evaluating cross-modal data removal. To address this gap, we introduce CLEAR, the first open-source benchmark designed specifically for MMU. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We conduct a comprehensive analysis of 11 MU methods (e.g., SCRUB, gradient ascent, DPO) across four evaluation sets, demonstrating that jointly unlearning both modalities outperforms single-modality approaches. The dataset is available at [link](https://huggingface.co/datasets/therem/CLEAR)</abstract>
      <url hash="4d4d9b6f">2025.findings-acl.1058</url>
      <bibkey>dontsov-etal-2025-clear</bibkey>
    </paper>
    <paper id="1059">
      <title>Assessing the Reasoning Capabilities of <fixed-case>LLM</fixed-case>s in the context of Evidence-based Claim Verification</title>
      <author><first>John</first><last>Dougrez-Lewis</last></author>
      <author><first>Mahmud Elahi</first><last>Akhter</last></author>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Sebastian</first><last>Löbbers</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University London</affiliation></author>
      <pages>20604-20628</pages>
      <abstract>Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create RECV, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of in creasing complexity. We evaluate three state of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning prob lems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deduc tive reasoning cases.</abstract>
      <url hash="f00b652c">2025.findings-acl.1059</url>
      <bibkey>dougrez-lewis-etal-2025-assessing</bibkey>
    </paper>
    <paper id="1060">
      <title>Language Models Lack Temporal Generalization and Bigger is Not Better</title>
      <author><first>Stella</first><last>Verkijk</last></author>
      <author><first>Piek</first><last>Vossen</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Pia</first><last>Sommerauer</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <pages>20629-20637</pages>
      <abstract>This paper presents elaborate testing of various LLMs on their generalization capacities. We finetune six encoder models that have been pretrained with very different data (varying in size, language, and period) on a challenging event detection task in Early Modern Dutch archival texts. Each model is finetuned with 5 seeds on 15 different data splits, resulting in 450 finetuned models. We also pre-train a domain-specific Language Model on the target domain and fine-tune and evaluate it in the same way to provide an upper bound. Our experimental setup allows us to look at underresearched aspects of generalizability, namely i) shifts at multiple places in a modeling pipeline, ii) temporal and crosslingual shifts and iii) generalization over different initializations. The results show that none of the models reaches domain-specific model performance, demonstrating their incapacity to generalize. mBERT reaches highest F1 performance, and is relatively stable over different seeds and datasplits, contrary to XLM-R. We find that contemporary Dutch models do not generalize well to Early Modern Dutch as they underperform compared to crosslingual as well as historical models. We conclude that encoder LLMs lack temporal generalization capacities and that bigger models are not better, since even a model pre-trained with five hundred GPUs on 2.5 terabytes of training data (XLM-R) underperforms considerably compared to our domain-specific model, pre-trained on one GPU and 6 GB of data. All our code, data, and the domain-specific model are openly available.</abstract>
      <url hash="865c052f">2025.findings-acl.1060</url>
      <bibkey>verkijk-etal-2025-language</bibkey>
    </paper>
    <paper id="1061">
      <title><fixed-case>D</fixed-case>iff<fixed-case>LM</fixed-case>: Controllable Synthetic Data Generation via Diffusion Language Models</title>
      <author><first>Ying</first><last>Zhou</last></author>
      <author><first>Xinyao</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yulei</first><last>Niu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yaojie</first><last>Shen</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Lexin</first><last>Tang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Fan</first><last>Chen</last><affiliation>Bytedance</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Longyin</first><last>Wen</last><affiliation>Bytedance Inc.</affiliation></author>
      <pages>20638-20658</pages>
      <abstract>Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs’ limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM’s generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE’s latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%–7% in certain cases. Data and code are available at https://github.com/bytedance/DiffLM.</abstract>
      <url hash="1e69314c">2025.findings-acl.1061</url>
      <bibkey>zhou-etal-2025-difflm</bibkey>
    </paper>
    <paper id="1062">
      <title>Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?</title>
      <author><first>Yifei</first><last>Wang</last></author>
      <author><first>Yu</first><last>Sheng</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Linjing</first><last>Li</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Daniel Dajun</first><last>Zeng</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>20659-20678</pages>
      <abstract>Recent advances in handling long sequences have unlocked new possibilities for long-context in-context learning (ICL). While existing research predominantly focuses on performance gains driven by additional in-context examples, the impact on the trustworthiness of generated responses remains underexplored. This paper addresses this gap by investigating how increased examples influence predictive uncertainty—an essential aspect in trustworthiness. We begin by systematically quantifying uncertainty across different “shot” configurations in ICL, emphasizing the role of example quantity. Through uncertainty decomposition, we introduce a novel perspective on performance enhancement, focusing on epistemic uncertainty (EU). Our results reveal that additional examples reduce total uncertainty in both simple and complex tasks by injecting task-specific knowledge, thereby diminishing EU and enhancing performance. For complex tasks, these advantages emerge only after addressing the increased noise and uncertainty associated with longer inputs. Finally, we investigate the progression of internal confidence across layers, uncovering the underlying mechanisms that drive the reduction in uncertainty.</abstract>
      <url hash="5666ce2d">2025.findings-acl.1062</url>
      <bibkey>wang-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="1063">
      <title><fixed-case>T</fixed-case>ool<fixed-case>S</fixed-case>pectrum: Towards Personalized Tool Utilization for Large Language Models</title>
      <author><first>Zihao</first><last>Cheng</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <author><first>Yuanfang</first><last>Guo</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yunhong</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>20679-20699</pages>
      <abstract>While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions while overlooking the critical role of context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs’ capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool selection. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool selection significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code will be released soon.</abstract>
      <url hash="48967a3a">2025.findings-acl.1063</url>
      <bibkey>cheng-etal-2025-toolspectrum</bibkey>
    </paper>
    <paper id="1064">
      <title>Reverse Preference Optimization for Complex Instruction Following</title>
      <author><first>Xiang</first><last>Huang</last></author>
      <author><first>Ting-En</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hangyu</first><last>Li</last></author>
      <author><first>Yuzhong</first><last>Qu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>20700-20723</pages>
      <abstract>Instruction following (IF) is a critical capability for large language models (LLMs). However, handling complex instructions with multiple constraints remains challenging. Previous methods typically select preference pairs based on the number of constraints they satisfy, introducing noise where chosen examples may fail to follow some constraints and rejected examples may excel in certain respects over the chosen ones. To address the challenge of aligning with multiple preferences, we propose a simple yet effective method called Reverse Preference Optimization (RPO). It mitigates noise in preference pairs by dynamically reversing the constraints within the instruction to ensure the chosen response is perfect, alleviating the burden of extensive sampling and filtering to collect perfect responses. Besides, reversal also enlarges the gap between chosen and rejected responses, thereby clarifying the optimization direction and making it more robust to noise. We evaluate RPO on two multi-turn IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively. Moreover, RPO scales effectively across model sizes (8B to 70B parameters), with the 70B RPO model surpassing GPT-4o.</abstract>
      <url hash="01b43a14">2025.findings-acl.1064</url>
      <bibkey>huang-etal-2025-reverse</bibkey>
    </paper>
    <paper id="1065">
      <title><fixed-case>MMS</fixed-case>-<fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>: Efficient <fixed-case>LLM</fixed-case>-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens</title>
      <author><first>Jeong Hun</first><last>Yeo</last></author>
      <author><first>Hyeongseop</first><last>Rha</last><affiliation>KAIST, Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Se Jin</first><last>Park</last><affiliation>KAIST</affiliation></author>
      <author><first>Yong Man</first><last>Ro</last><affiliation>Korea Advanced Institute of Science &amp; Technology, Korea Advanced Institute of Science &amp; Technology and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>20724-20735</pages>
      <abstract>Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%. The code and models are available <url>https://github.com/JeongHun0716/MMS-LLaMA</url>.</abstract>
      <url hash="4bea621b">2025.findings-acl.1065</url>
      <bibkey>yeo-etal-2025-mms</bibkey>
    </paper>
    <paper id="1066">
      <title>Def-<fixed-case>DTS</fixed-case>: Deductive Reasoning for Open-domain Dialogue Topic Segmentation</title>
      <author><first>Seungmin</first><last>Lee</last></author>
      <author><first>Yongsang</first><last>Yoo</last></author>
      <author><first>Minhwa</first><last>Jung</last></author>
      <author><first>Min</first><last>Song</last><affiliation>Yonsei University and Yonsei University</affiliation></author>
      <pages>20736-20753</pages>
      <abstract>Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS.</abstract>
      <url hash="b1bbf1c1">2025.findings-acl.1066</url>
      <bibkey>lee-etal-2025-def</bibkey>
    </paper>
    <paper id="1067">
      <title>Exploring Jailbreak Attacks on <fixed-case>LLM</fixed-case>s through Intent Concealment and Diversion</title>
      <author><first>Tiehan</first><last>Cui</last><affiliation>Henan Univeristy</affiliation></author>
      <author><first>Yanxu</first><last>Mao</last></author>
      <author><first>Peipei</first><last>Liu</last></author>
      <author><first>Congying</first><last>Liu</last></author>
      <author><first>Datao</first><last>You</last></author>
      <pages>20754-20768</pages>
      <abstract>Although large language models (LLMs) have achieved remarkable advancements, their security remains a pressing concern. One major threat is jailbreak attacks, where adversarial prompts bypass model safeguards to generate harmful or objectionable content. Researchers study jailbreak attacks to understand security and robustness of LLMs. However, existing jailbreak attack methods face two main challenges: (1) an excessive number of iterative queries, and (2) poor generalization across models. In addition, recent jailbreak evaluation datasets focus primarily on question-answering scenarios, lacking attention to text generation tasks that require accurate regeneration of toxic content.To tackle these challenges, we propose two contributions:(1) **ICE**, a novel black-box jailbreak method that employs **I**ntent **C**oncealment and div**E**rsion to effectively circumvent security constraints. **ICE** achieves high attack success rates (ASR) with a single query, significantly improving efficiency and transferability across different models.(2) **BiSceneEval**, a comprehensive dataset designed for assessing LLM robustness in question-answering and text-generation tasks. Experimental results demonstrate that **ICE** outperforms existing jailbreak techniques, revealing critical vulnerabilities in current defense mechanisms. Our findings underscore the necessity of a hybrid security strategy that integrates predefined security mechanisms with real-time semantic decomposition to enhance the security of LLMs.</abstract>
      <url hash="0bc2e5e9">2025.findings-acl.1067</url>
      <bibkey>cui-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1068">
      <title>Verbosity-Aware Rationale Reduction: Sentence-Level Rationale Reduction for Efficient and Effective Reasoning</title>
      <author><first>Joonwon</first><last>Jang</last></author>
      <author><first>Jaehee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Wonbin</first><last>Kweon</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Seonghyeon</first><last>Lee</last><affiliation>Kyungpook National University</affiliation></author>
      <author><first>Hwanjo</first><last>Yu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <pages>20769-20784</pages>
      <abstract>Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While this approach has proven effective, it inevitably increases substantial inference costs. Previous methods adopting token-level reduction without clear criteria result in poor performance compared to models trained with complete rationale. To address this challenge, we propose a novel sentence-level rationale reduction framework leveraging likelihood-based criteria, *verbosity*, to identify and remove redundant reasoning sentences. Unlike previous approaches, our method leverages *verbosity* to selectively remove redundant reasoning sentences while preserving reasoning capabilities. Our experimental results across various reasoning tasks demonstrate that our method improves performance by an average of 7.71% while reducing token generation by 19.87% compared to model trained with complete reasoning paths.</abstract>
      <url hash="6cc1dc73">2025.findings-acl.1068</url>
      <bibkey>jang-etal-2025-verbosity</bibkey>
    </paper>
    <paper id="1069">
      <title>Exploring the Role of Mental Health Conversational Agents in Training Medical Students and Professionals: A Systematic Literature Review</title>
      <author><first>Thushari</first><last>Atapattu</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Menasha</first><last>Thilakaratne</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Duc Nhan</first><last>Do</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Mahen</first><last>Herath</last><affiliation>University of Moratuwa</affiliation></author>
      <author><first>Katrina E.</first><last>Falkner</last><affiliation>University of Adelaide</affiliation></author>
      <pages>20785-20798</pages>
      <abstract>The integration of Artificial Intelligence (AI) into mental health education and training (MHET) has become a promising solution to meet the increasing demand for skilled mental health professionals. This systematic review analyses 38 studies on AI-powered conversational agents (CAs) in MHET, selected from a total of 1003 studies published between 2019 and 2024. Following the PRISMA protocol, we reviewed papers from computer science, medicine, and interdisciplinary databases, assessing key aspects such as technological approaches, data characteristics, application areas, and evaluation methodologies. Our findings reveal that AI-based approaches, including Large Language Models (LLMs), dominate the field, with training as the application area being the most prevalent. These technologies show promise in simulating therapeutic interactions but face challenges such as limited public datasets, lack of standardised evaluation frameworks, and difficulty in ensuring authentic emotional responses, along with gaps in ethical considerations and clinical efficacy. This review presents a comprehensive framework for understanding the role of CAs in MHET while providing valuable recommendations to guide future research.</abstract>
      <url hash="0e48c9f9">2025.findings-acl.1069</url>
      <bibkey>atapattu-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1070">
      <title>Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers</title>
      <author><first>Rin</first><last>Ashizawa</last><affiliation>Yokohama National University</affiliation></author>
      <author><first>Yoichi</first><last>Hirose</last></author>
      <author><first>Nozomu</first><last>Yoshinari</last><affiliation>LY Corporation and Yokohama National University</affiliation></author>
      <author><first>Kento</first><last>Uchida</last><affiliation>Yokohama National University</affiliation></author>
      <author><first>Shinichi</first><last>Shirakawa</last><affiliation>Yokohama National University and Yokohama City University</affiliation></author>
      <pages>20799-20817</pages>
      <abstract>Prompt optimization aims to search for effective prompts that enhance the performance of large language models (LLMs). Although existing prompt optimization methods have discovered effective prompts, they often differ from sophisticated prompts carefully designed by human experts. Prompt design strategies, representing best practices for improving prompt performance, can be key to improving prompt optimization. Recently, a method termed the Autonomous Prompt Engineering Toolbox (APET) has incorporated various prompt design strategies into the prompt optimization process. In APET, the LLM is needed to implicitly select and apply the appropriate strategies because prompt design strategies can have negative effects. This implicit selection may be suboptimal due to the limited optimization capabilities of LLMs. This paper introduces Optimizing Prompts with sTrategy Selection (OPTS), which implements explicit selection mechanisms for prompt design. We propose three mechanisms, including a Thompson sampling-based approach, and integrate them into EvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for two LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench Hard. Our results show that the selection of prompt design strategies improves the performance of EvoPrompt, and the Thompson sampling-based mechanism achieves the best overall results. Our experimental code is provided at https://github.com/shiralab/OPTS.</abstract>
      <url hash="2fa0f37b">2025.findings-acl.1070</url>
      <bibkey>ashizawa-etal-2025-bandit</bibkey>
    </paper>
    <paper id="1071">
      <title><fixed-case>STORYTELLER</fixed-case>: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation</title>
      <author><first>Jiaming</first><last>Li</last></author>
      <author><first>Yukun</first><last>Chen</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Minghuan</first><last>Tan</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Yunshui</first><last>Li</last></author>
      <author><first>Run</first><last>Luo</last></author>
      <author><first>Longze</first><last>Chen</last></author>
      <author><first>Jing</first><last>Luo</last></author>
      <author><first>Ahmadreza</first><last>Argha</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Hamid</first><last>Alinejad-Rokny</last><affiliation>UNSW Sydney</affiliation></author>
      <author><first>Wei</first><last>Zhou</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>20818-20846</pages>
      <abstract>Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject-verb-object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules—the STORYLINE and narrative entity knowledge graph (NEKG)—that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance.</abstract>
      <url hash="75f4913e">2025.findings-acl.1071</url>
      <bibkey>li-etal-2025-storyteller</bibkey>
    </paper>
    <paper id="1072">
      <title><fixed-case>S</fixed-case>elect<fixed-case>LLM</fixed-case>: Query-Aware Efficient Selection Algorithm for Large Language Models</title>
      <author><first>Kaushal Kumar</first><last>Maurya</last></author>
      <author><first>Kv Aditya</first><last>Srivatsa</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>20847-20863</pages>
      <abstract>Large language models (LLMs) have been widely adopted due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse models. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier’s predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU, compared to the top-performing baseline. Also, we establish a theoretical upper bound by an Oracle with LLMs and perform an in-depth linguistic analysis to understand the performance gap between the Oracle and SelectLLM.</abstract>
      <url hash="856e28e4">2025.findings-acl.1072</url>
      <bibkey>maurya-etal-2025-selectllm</bibkey>
    </paper>
    <paper id="1073">
      <title><fixed-case>S</fixed-case>ky<fixed-case>LLM</fixed-case>: Cross-<fixed-case>LLM</fixed-case>-<fixed-case>API</fixed-case>s Federation for Cost-effective Query Processing</title>
      <author><first>Heng</first><last>Zhao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Yifei</first><last>Zhu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>20864-20873</pages>
      <abstract>Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks, from text generation to complex problem-solving. LLM APIs provide easy access to these models by streamlining deployment and usage. Combining LLMs with complementary strengths has been shown to yield substantial performance gains over a monolithic LLM. However, invoking a fixed set of LLM APIs for each query incurs higher API costs and increased inference latency. To address these limitations, we propose SkyLLM, a system composed of a set of estimators and an API selector, which federates multiple LLM APIs and dynamically assigns a non-empty subset of these APIs to each query prior to inference under cost and latency budgets. The selected subset consists of either a single LLM or multiple LLMs. A single LLM efficiently handles simple queries at low cost, whereas multiple LLMs are employed for more complex queries to overcome performance limitations. We evaluate SkyLLM against individual LLMs and representative ensemble LLM methods from the literature. SkyLLM achieves the highest accuracy under a high budget. It can also be cost-effective, matching the most accurate individual LLM while cutting costs by 67.8%.</abstract>
      <url hash="26a0cf59">2025.findings-acl.1073</url>
      <bibkey>zhao-zhu-2025-skyllm</bibkey>
    </paper>
    <paper id="1074">
      <title>Matina: A Culturally-Aligned <fixed-case>P</fixed-case>ersian Language Model Using Multiple <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Experts</title>
      <author><first>Sara Bourbour</first><last>Hosseinbeigi</last><affiliation>Tarbiat Modares University</affiliation></author>
      <author><first>MohammadAli</first><last>SeifKashani</last></author>
      <author><first>Javad</first><last>Seraj</last></author>
      <author><first>Fatemeh</first><last>Taherinezhad</last></author>
      <author><first>Ali</first><last>Nafisi</last></author>
      <author><first>Fatemeh</first><last>Nadi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Iman</first><last>Barati</last><affiliation>Iran University of Science and Technology Tehran, University of Tehran</affiliation></author>
      <author><first>Hosein</first><last>Hasani</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Mostafa</first><last>Amiri</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mostafa</first><last>Masoudi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <pages>20874-20889</pages>
      <abstract>Large language models (LLMs) are powerful tools for a variety of applications, but to interact effectively with users, they must align with the cultural values and linguistic nuances of their audience. However, existing LLMs often fall short in adequately modeling underrepresented languages and cultures, such as Persian, limiting their applicability and acceptance. To address this, we construct diverse, high-quality datasets specifically tailored to Persian linguistic and cultural contexts, ensuring a more authentic and context-aware training process. Using these datasets, we develop Matina, a Persian-focused multi-expert model designed to embody Iranian cultural values and linguistic structures. Matina is trained by fine-tuning LLaMA3.1 8B-Instruct models across five domains: culinary, tourism, socio-culture, translation, and summarization. These experts are combined using a classifier to create a unified multi-expert system. By leveraging culturally aligned datasets, Matina outperforms baseline models in both task performance and user satisfaction, demonstrating the importance of data-driven cultural adaptation in LLM development.</abstract>
      <url hash="0a4ba6c6">2025.findings-acl.1074</url>
      <bibkey>hosseinbeigi-etal-2025-matina-culturally</bibkey>
    </paper>
    <paper id="1075">
      <title><fixed-case>PM</fixed-case>3-<fixed-case>KIE</fixed-case>: A Probabilistic Multi-Task Meta-Model for Document Key Information Extraction</title>
      <author><first>Birgit</first><last>Kirsch</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Héctor</first><last>Allende-Cid</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Stefan</first><last>Rueping</last></author>
      <pages>20890-20912</pages>
      <abstract>Key Information Extraction (KIE) from visually rich documents is commonly approached as either fine-grained token classification or coarse-grained entity extraction. While token-level models capture spatial and visual cues, entity-level models better represent logical dependencies and align with real-world use cases.We introduce PM3-KIE, a probabilistic multi-task meta-model that incorporates both fine-grained and coarse-grained models. It serves as a lightweight reasoning layer that jointly predicts entities and all appearances in a document. PM3-KIE incorporates domain-specific schema constraints to enforce logical consistency and integrates large language models for semantic validation, thereby reducing extraction errors.Experiments on two public datasets, DeepForm and FARA, show that PM3-KIE outperforms three state-of-the-art models and a stacked ensemble, achieving a statistically significant 2% improvement in F1 score.</abstract>
      <url hash="c65eafb8">2025.findings-acl.1075</url>
      <bibkey>kirsch-etal-2025-pm3</bibkey>
    </paper>
    <paper id="1076">
      <title><fixed-case>T</fixed-case>echnique<fixed-case>RAG</fixed-case>: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text</title>
      <author><first>Ahmed</first><last>Lekssays</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Utsav</first><last>Shukla</last></author>
      <author><first>Husrev Taha</first><last>Sencar</last><affiliation>QCRI</affiliation></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>20913-20926</pages>
      <abstract>Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations—such as custom hard-negative mining and denoising—resources rarely available in specialized domains.We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text–technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.</abstract>
      <url hash="01d937a1">2025.findings-acl.1076</url>
      <bibkey>lekssays-etal-2025-techniquerag</bibkey>
    </paper>
    <paper id="1077">
      <title><fixed-case>G</fixed-case>2<fixed-case>S</fixed-case>: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models</title>
      <author><first>Long</first><last>Bai</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zixuan</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiaolong</first><last>Jin</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>Institute of Computing Technology, Chinese Academy</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>20927-20938</pages>
      <abstract>Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts based on historical ones has received much attention. Recent studies have introduced Large Language Models (LLMs) for this task to enhance the models’ generalization abilities. However, these models perform forecasting via simultaneously learning two kinds of entangled knowledge in the TKG: (1) general patterns, i.e., invariant temporal structures shared across different scenarios; and (2) scenario information, i.e., factual knowledge engaged in specific scenario, such as entities and relations. As a result, the learning processes of these two kinds of knowledge may interfere with each other, which potentially impact the generalization abilities of the models. To enhance the generalization ability of LLMs on this task, in this paper, we propose a General-to-Specific learning framework (G2S) that disentangles the learning processes of the above two kinds of knowledge. In the general learning stage, we mask the scenario information in different TKGs and convert it into anonymous temporal structures. After training on these structures, the model is able to capture the general patterns across different TKGs. In the specific learning stage, we inject the scenario information into the structures via either in-context learning or fine-tuning modes. Experimental results show that G2S effectively improves the generalization abilities of LLMs.</abstract>
      <url hash="6c6045db">2025.findings-acl.1077</url>
      <bibkey>bai-etal-2025-g2s</bibkey>
    </paper>
    <paper id="1078">
      <title>Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning</title>
      <author><first>Ziang</first><last>Ye</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhenru</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jianxin</first><last>Ma</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junyang</first><last>Lin</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>20939-20957</pages>
      <abstract>When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles—specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format)—differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).</abstract>
      <url hash="33fd78b5">2025.findings-acl.1078</url>
      <bibkey>ye-etal-2025-disentangling</bibkey>
    </paper>
    <paper id="1079">
      <title><fixed-case>APT</fixed-case>: Improving Specialist <fixed-case>LLM</fixed-case> Performance with Weakness Case Acquisition and Iterative Preference Training</title>
      <author><first>Jun</first><last>Rao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zepeng</first><last>Lin</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Xiaopeng</first><last>Ke</last></author>
      <author><first>Lian</first><last>Lian</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Dong</first><last>Jin</last></author>
      <author><first>Shengjun</first><last>Cheng</last></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>20958-20980</pages>
      <abstract>Large Language Models (LLMs) often require domain-specific fine-tuning to address targeted tasks, which risks degrading their general capabilities. Maintaining a balance between domain-specific enhancements and general model utility is a key challenge. This paper proposes a novel approach named APT (Weakness Case Acquisition and Iterative Preference Training) to enhance domain-specific performance with self-generated dis-preferred weakness data (bad cases and similar cases). APT uniquely focuses on training the model using only those samples where errors occur, alongside a small, similar set of samples retrieved for this purpose. This targeted training minimizes interference with the model’s existing knowledge base, effectively retaining generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3 models across various benchmarks demonstrate that APT ensures no reduction in generic capacity and achieves superior performance on downstream tasks compared to various existing methods. This validates our method as an effective strategy for enhancing domain-specific capabilities without sacrificing the model’s broader applicability.</abstract>
      <url hash="51a9be9d">2025.findings-acl.1079</url>
      <bibkey>rao-etal-2025-apt</bibkey>
    </paper>
    <paper id="1080">
      <title><fixed-case>E</fixed-case>asy<fixed-case>EA</fixed-case>: Large Language Model is All You Need in Entity Alignment Between Knowledge Graphs</title>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Chenglong</first><last>Lu</last></author>
      <author><first>Linyan</first><last>Yang</last></author>
      <author><first>Guoqing</first><last>Chen</last></author>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <pages>20981-20995</pages>
      <abstract>Entity alignment (EA) aims to identify entities in different knowledge graphs (KGs) that represent the same real-world objects. Traditional EA methods typically embed entity information into vector space under the guidance of seed entity pairs, and align entities by calculating and comparing the similarity between entity embeddings. With the advent of large language models (LLMs), emerging methods are increasingly integrating LLMs with traditional methods to leverage external knowledge and improve EA accuracy. However, this integration also introduces additional computational complexity and operational overhead, and still requires seed pairs that are scarce and expensive to obtain. To address these challenges, we propose EasyEA, the first end-to-end EA framework based on LLMs that requires no training. EasyEA consists of three main stages: (1) Information Summarization, (2) Embedding and Feature Fusion, and (3) Candidate Selection. By automating the EA process, EasyEA significantly reduces the reliance on seed entity pairs while demonstrating superior performance across various datasets, covering crosslingual, sparse, large-scale, and heterogeneous scenarios. Extensive experimental results show that EasyEA not only simplifies the EA process but also achieves state-of-the-art (SOTA) performance on diverse datasets, providing a promising solution for advancing EA tasks.</abstract>
      <url hash="7fb0b011">2025.findings-acl.1080</url>
      <bibkey>cheng-etal-2025-easyea</bibkey>
    </paper>
    <paper id="1081">
      <title>An Adaptive Multi-Threshold Loss and a General Framework for Collaborating Losses in Document-Level Relation Extraction</title>
      <author><first>Huangming</first><last>Xu</last></author>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>20996-21007</pages>
      <abstract>The goal of document-level relation extraction (DocRE) is to identify relations for a given entity pair within a document. As a multilabel classification task, the most commonly employed method involves introducing an adaptive threshold. Specifically, for an entity pair, if the scores of predicted relations exceed the threshold, the relations exist. However, we observe two phenomena that significantly weaken the model’s performance in DocRE: (1) as the label space (the number of relations) expands, the model’s performance gradually declines; (2) the model tends to prioritize predicting high-frequency relations in the long-tail problem. To address these challenges, we propose an innovative **A**daptive **M**ulti-**T**hreshold **L**oss (AMTL), which for the first time proposes to partition the label space into different sub-label spaces (thus reducing its overall size) and learn an adaptive threshold for each sub-label space. This approach allows for more precise tuning of the model’s sensitivity to diverse relations, mitigating the performance degradation associated with label space expansion and the long-tail problem. Moreover, our adaptive multi-threshold method can be considered as a general framework that seamlessly integrates different losses in different sub-label spaces, facilitating the concurrent application of multiple losses. Experimental results demonstrate that AMTL significantly enhances the performance of existing DocRE models across four datasets, achieving state-of-the-art results. The experiments on the concurrent application of multiple losses with our framework show stable performance and outperform single-loss methods. Code is available at https://github.com/xhm-code/AMTL.</abstract>
      <url hash="faf9b5e9">2025.findings-acl.1081</url>
      <bibkey>xu-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="1082">
      <title><fixed-case>R</fixed-case>ole<fixed-case>MRC</fixed-case>: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following</title>
      <author><first>Junru</first><last>Lu</last><affiliation>Tencent Youtu Lab</affiliation></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Guodong</first><last>Shen</last></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Siyu</first><last>An</last></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Di</first><last>Yin</last></author>
      <author><first>Xing</first><last>Sun</last><affiliation>Tencent YouTu Lab</affiliation></author>
      <pages>21008-21030</pages>
      <abstract>Role-playing is important for Large Language Models (LLMs) to follow diverse instructions while maintaining role identity and the role’s pre-defined ability limits. Existing role-playing datasets mostly contribute to controlling role style and knowledge boundaries, but overlook role-playing in instruction-following scenarios. We introduce a fine-grained role-playing and instruction-following composite benchmark, named RoleMRC, including: (1) Multi-turn dialogues between ideal roles and humans, including free chats or discussions upon given passages; (2) Role-playing machine reading comprehension, involving response, refusal, and attempts according to passage answerability and role ability; (3) More complex scenarios with nested, multi-turn and prioritized instructions. The final RoleMRC features a 10.2k role profile meta-pool, 37.9k well-synthesized role-playing instructions, and 1.4k testing samples. We develop a pipeline to quantitatively evaluate the fine-grained role-playing and instruction-following capabilities of several mainstream LLMs, as well as models that are fine-tuned on our data. Moreover, cross-evaluation on external role-playing datasets confirms that models fine-tuned on RoleMRC enhances instruction-following without compromising general role-playing and reasoning capabilities. We also probe the neural-level activation maps of different capabilities over post-tuned LLMs. Access to our RoleMRC, RoleMRC-mix and Codes: https://github.com/LuJunru/RoleMRC.</abstract>
      <url hash="b80e14ba">2025.findings-acl.1082</url>
      <bibkey>lu-etal-2025-rolemrc</bibkey>
    </paper>
    <paper id="1083">
      <title><fixed-case>C</fixed-case>²<fixed-case>RB</fixed-case>ench: A <fixed-case>C</fixed-case>hinese Complex Reasoning Benchmark for Large Language Models</title>
      <author><first>Junru</first><last>Wu</last></author>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Linxi</first><last>Su</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>21031-21050</pages>
      <abstract>Large language models (LLMs) have achieved remarkable progress in autonomous reasoning, evolving from basic text processing to sophisticated multimodal reasoning, a critical capability for general-purpose AI assistants. However, existing benchmarks usually fail to adequately capture the intricate multi-step reasoning demands inherent in real-world scenarios. To bridge this gap, we propose **C²RBench**: a **C**hinese **C**omplex **R**easoning **Bench**mark for evaluating multi-step, multimodal advanced reasoning capability of LLMs. C²RBench comprises 1,115 carefully curated Chinese tasks, which are organized into eight domain-specific subsets, each meticulously designed to mirror real-world challenges. This hierarchical benchmark features three difficulty tiers based on the number of reasoning steps required (average 8.44 steps per task), significantly exceeding existing benchmarks in cognitive complexity. Extensive evaluations of 20 LLMs (including DeepSeek-R1) and 24 multimodal large language models (MLLMs) on C²RBench reveal critical performance gaps: GPT-4.1 achieves only 52.11% accuracy, indicating substantial room for improvement. The dataset and evaluation code are publicly available.</abstract>
      <url hash="84756a18">2025.findings-acl.1083</url>
      <bibkey>wu-etal-2025-c2rbench</bibkey>
    </paper>
    <paper id="1084">
      <title>Unlocking <fixed-case>LLM</fixed-case>s’ Self-Improvement Capacity with Autonomous Learning for Domain Adaptation</title>
      <author><first>Ke</first><last>Ji</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Wenya</first><last>Xie</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>21051-21067</pages>
      <abstract>Self-supervised pre-training and instruction fine-tuning demonstrate the potential of large language models (LLMs) for domain adaptation (DA). In pursuit of superhuman performance, LLMs have demonstrated significant potential in math and coding through self-improvement algorithms that rely on iterative training with self-generated data. This success stems from the clear reward signals in these environments, which provide a solid foundation for self-improvement. However, when it comes to general DA scenarios, two main challenges emerge: 1) ambiguous self-improvement reward signals and 2) lack of high-quality instruction fine-tuning datasets. This motivates this paper addresses how LLMs can adapt autonomously to new domains using only a large amount of unlabeled target corpora. Inspired by the human practice of self-reflection through open- and closed-book exercises to achieve domain generalization, we propose autonomous learning, which creates a self-improvement learning environment for DA. Here, the model generates questions from documents and conducts two explorations—one with the original document and one with a masked version. By comparing these explorations, the LLMs can independently identify and enhance its policy for reducing knowledge gaps. Experiments across various DA tasks demonstrate that autonomous learning enhances the DA performance of existing models, outperforming traditional fine-tuning and self-improvement methods. Our code is publicly available at https://github.com/FreedomIntelligence/AL.</abstract>
      <url hash="89810094">2025.findings-acl.1084</url>
      <bibkey>ji-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="1085">
      <title>How Personality Traits Shape <fixed-case>LLM</fixed-case> Risk-Taking Behaviour</title>
      <author><first>John</first><last>Hartley</last></author>
      <author><first>Conor Brian</first><last>Hamill</last><affiliation>NatWest Bank</affiliation></author>
      <author><first>Dale</first><last>Seddon</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Devesh</first><last>Batra</last><affiliation>Data Science &amp; Innovation, NatWest Group</affiliation></author>
      <author><first>Ramin</first><last>Okhrati</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Raad</first><last>Khraishi</last><affiliation>University College London, University of London</affiliation></author>
      <pages>21068-21092</pages>
      <abstract>Large Language Models (LLMs) are increasingly deployed as autonomous agents for simulation and decision-making, necessitating a deeper understanding of their decision-making behaviour under risk. We investigate the relationship between LLMs’ personality traits and risk-propensity, applying Cumulative Prospect Theory (CPT) and the Big Five personality framework. We compare the behaviour of several LLMs to human baselines. Our findings show that the majority of the models investigated are risk-neutral rational agents, whilst displaying higher Conscientiousness and Agreeableness traits, coupled with lower Neuroticism. Interventions on Big Five traits, particularly Openness, influence the risk-propensity of several LLMs. Advanced models mirror human personality-risk patterns, suggesting that cognitive biases can be surfaced by optimal prompting. However, their distilled variants show no cognitive bias, suggesting limitations to knowledge transfer processes. Notably, Openness emerges as the most influential factor to risk-propensity, aligning with human baselines. In contrast, less advanced models demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and highlights the potential and limitations of personality-based interventions in shaping LLM decision-making.</abstract>
      <url hash="5644085e">2025.findings-acl.1085</url>
      <bibkey>hartley-etal-2025-personality</bibkey>
    </paper>
    <paper id="1086">
      <title>Word-Level Detection of Code-Mixed Hate Speech with Multilingual Domain Transfer</title>
      <author><first>Karin</first><last>Niederreiter</last><affiliation>Universität Vienna</affiliation></author>
      <author><first>Dagmar</first><last>Gromann</last></author>
      <pages>21093-21104</pages>
      <abstract>The exponential growth of offensive language on social media tends to fuel online harassment and challenges detection mechanisms. Hate speech detection is commonly treated as a monolingual or multilingual sentence-level classification task. However, profane language tends to contain code-mixing, a combination of more than one language, which requires a more nuanced detection approach than binary classification. A general lack of available code-mixed datasets aggravates the problem. To address this issue, we propose five word-level annotated hate speech datasets, EN and DE from social networks, one subset of the DE-EN Offensive Content Detection Code-Switched Dataset, one DE-EN code-mixed German rap lyrics held-out test set, and a cross-domain held-out test set. We investigate the capacity of fine-tuned German-only, German-English bilingual, and German-English code-mixed token classification XLM-R models to generalize to code-mixed hate speech in German rap lyrics in zero-shot domain transfer as well as across different domains. The results show that bilingual fine-tuning facilitates not only the detection of code-mixed hate speech, but also neologisms, addressing the inherent dynamics of profane language use.</abstract>
      <url hash="406c0b5a">2025.findings-acl.1086</url>
      <bibkey>niederreiter-gromann-2025-word</bibkey>
    </paper>
    <paper id="1087">
      <title>Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models</title>
      <author><first>Amin</first><last>Abolghasemi</last></author>
      <author><first>Leif</first><last>Azzopardi</last><affiliation>University of Strathclyde</affiliation></author>
      <author><first>Seyyed Hadi</first><last>Hashemi</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Maarten</first><last>de Rijke</last></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <pages>21105-21124</pages>
      <abstract>Attributing answers to source documents is an approach used to enhance the verifiability of a model’s output in retrieval-augmented generation (RAG). Prior work has mainly focused on improving and evaluating the attribution quality of large language models (LLMs) in RAG, but this may come at the expense of inducing biases in the attribution of answers. We define and examine two aspects in the evaluation of LLMs in RAG pipelines, namely attribution sensitivity and bias with respect to authorship information. We explicitly inform an LLM about the authors of source documents, instruct it to attribute its answers, and analyze (i) how sensitive the LLM’s output is to the author of source documents, and (ii) whether the LLM exhibits a bias towards human-written or AI-generated source documents. We design an experimental setup in which we use counterfactual evaluation to study three LLMs in terms of their attribution sensitivity and bias in RAG pipelines. Our results show that adding authorship information to source documents can significantly change the attribution quality of LLMs by 3 to 18%. We show that LLMs can have an attribution bias towards explicit human authorship, which can serve as a competing hypothesis for findings of prior work that shows that LLM-generated content may be preferred over human-written contents. Our findings indicate that metadata of source documents can influence LLMs’ trust, and how they attribute their answers. Furthermore, our research highlights attribution bias and sensitivity as a novel aspect of the vulnerability of LLMs.</abstract>
      <url hash="08868255">2025.findings-acl.1087</url>
      <bibkey>abolghasemi-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="1088">
      <title>Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment</title>
      <author><first>Wen</first><last>Yang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Junhong</first><last>Wu</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Chen</first><last>Wang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>21125-21147</pages>
      <abstract>Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that <tex-math>\textit{captures}</tex-math> learned preferences from well-aligned English models by implicit rewards and <tex-math>\textit{transfers}</tex-math> them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data.</abstract>
      <url hash="f43a9d54">2025.findings-acl.1088</url>
      <bibkey>yang-etal-2025-implicit</bibkey>
    </paper>
    <paper id="1089">
      <title>Diagnosing Failures in Large Language Models’ Answers: Integrating Error Attribution into Evaluation Framework</title>
      <author><first>Zishan</first><last>Xu</last></author>
      <author><first>Shuyi</first><last>Xie</last></author>
      <author><first>Qingsong</first><last>Lv</last></author>
      <author><first>Shupei</first><last>Xiao</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Linlin</first><last>Song</last></author>
      <author><first>Sui</first><last>Wenjuan</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <author><first>Fan</first><last>Lin</last></author>
      <pages>21148-21165</pages>
      <abstract>With the widespread application of Large Language Models (LLMs) in various tasks, the mainstream LLM platforms generate massive user-model interactions daily. In order to efficiently analyze the performance of models and diagnose failures in their answers, it is essential to develop an automated framework to systematically categorize and attribute errors. However, existing evaluation models lack error attribution capability. In this work, we establish a comprehensive Misattribution Framework with 6 primary and 15 secondary categories to facilitate in-depth analysis. Based on this framework, we present AttriData, a dataset specifically designed for error attribution, encompassing misattribution, along with the corresponding scores and feedback. We also propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first general-purpose judge model capable of simultaneously generating score, misattribution, and feedback. Extensive experiments and analyses are conducted to confirm the effectiveness and robustness of our proposed method.</abstract>
      <url hash="c82eef82">2025.findings-acl.1089</url>
      <bibkey>xu-etal-2025-diagnosing</bibkey>
    </paper>
    <paper id="1090">
      <title>Encode Errors: Representational Retrieval of In-Context Demonstrations for Multilingual Grammatical Error Correction</title>
      <author><first>Guangyue</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Wen</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>21166-21180</pages>
      <abstract>Grammatical Error Correction (GEC) involves detecting and correcting the wrong usage of grammar. While large language models (LLMs) with in-context learning (ICL) capabilities have shown significant progress on various natural language processing (NLP) tasks, their few-shot performance on GEC remains suboptimal. This is mainly due to the challenge of retrieving suitable in-context demonstrations that capture error patterns instead of semantic similarity. In this paper, we demonstrate that LLMs can inherently capture information related to grammatical errors through their internal states. From these states, we extract the Grammatical Error Representation (GER), an informative and semantically neutral encoding of grammatical errors. Our novel GER-based retrieval method significantly boosts performance in ICL settings on multilingual GEC datasets, improving the precision of correction. For high-resource languages, our results on 8B-sized open-source models match those of closed-source models such as Deepseek2.5 and GPT-4o-mini. For low-resource languages, our <tex-math>F_{0.5}</tex-math> scores surpass the baseline by up to a factor of 1.2. This method provides a more precise and resource-efficient solution for multilingual GEC, offering a promising direction for interpretable GEC research.</abstract>
      <url hash="470a3a3e">2025.findings-acl.1090</url>
      <bibkey>peng-etal-2025-encode</bibkey>
    </paper>
    <paper id="1091">
      <title>Preference Curriculum: <fixed-case>LLM</fixed-case>s Should Always Be Pretrained on Their Preferred Data</title>
      <author><first>Xuemiao</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Liangyu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Feiyu</first><last>Duan</last></author>
      <author><first>Yongwei</first><last>Zhou</last><affiliation>Meituan</affiliation></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Rongxiang</first><last>Weng</last><affiliation>Meituan</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last><affiliation>Meituan</affiliation></author>
      <pages>21181-21198</pages>
      <abstract>Large language models (LLMs) generally utilize a consistent data distribution throughout the pretraining process. However, as the model’s capability improves, it is intuitive that its data preferences dynamically change, indicating the need for pretraining with different data at various training stages. To achieve it, we propose the Perplexity Difference (PD) based Preference Curriculum learning (PDPC) framework, which always perceives and uses the data preferred by LLMs to train and boost them. First, we introduce the PD metric to quantify the difference in how challenging a sample is for weak versus strong models. Samples with high PD are more challenging for weak models to learn and are more suitable to be arranged in the later stage of pretraining. Second, we propose the preference function to approximate and predict the data preference of the LLM at any training step, so as to complete the arrangement of the dataset offline and ensure continuous training without interruption. Experimental results on 1.3B and 3B models demonstrate that PDPC significantly surpasses baselines. Notably, the 3B model trained on 1T tokens achieves an increased average accuracy of over 8.1% across MMLU and CMMLU.</abstract>
      <url hash="e1f9bb4e">2025.findings-acl.1091</url>
      <bibkey>zhang-etal-2025-preference</bibkey>
    </paper>
    <paper id="1092">
      <title>Can Input Attributions Explain Inductive Reasoning in In-Context Learning?</title>
      <author><first>Mengyu</first><last>Ye</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Goro</first><last>Kobayashi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <pages>21199-21225</pages>
      <abstract>Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying/solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests in linguistics; here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates the task demonstrated. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods.</abstract>
      <url hash="87949af2">2025.findings-acl.1092</url>
      <bibkey>ye-etal-2025-input</bibkey>
    </paper>
    <paper id="1093">
      <title>Modal Dependency Parsing via Biaffine Attention with Self-Loop</title>
      <author><first>Jayeol</first><last>Chun</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Nianwen</first><last>Xue</last><affiliation>Brandeis University</affiliation></author>
      <pages>21226-21238</pages>
      <abstract>A modal dependency structure represents a web of connections between events and sources of information in a document that allows for tracing of who-said-what with what levels of certainty, thereby establishing factuality in an event-centric approach. Obtaining such graphs defines the task of modal dependency parsing, which involves event and source identification along with the modal relations between them. In this paper, we propose a simple yet effective solution based on biaffine attention that specifically optimizes against the domain-specific challenges of modal dependency parsing by integrating self-loop. We show that our approach, when coupled with data augmentation by leveraging the Large Language Models to translate annotations from one language to another, outperforms the previous state-of-the-art on English and Chinese datasets by 2% and 4% respectively.</abstract>
      <url hash="2c1dda67">2025.findings-acl.1093</url>
      <bibkey>chun-xue-2025-modal</bibkey>
    </paper>
    <paper id="1094">
      <title>Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zixiao</first><last>Wang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Duzhen</first><last>Zhang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ishita</first><last>Agarwal</last></author>
      <author><first>Shen</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Le</first><last>Song</last><affiliation>College of Computing, Georgia Institute of Technology</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>21239-21257</pages>
      <abstract>Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character’s responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought patterns as manifested in the textual works of a character. Using Lu Xun, a renowned Chinese writer as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun’s internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope this work inspires future research on deep character persona simulation LLMs: https://github.com/zxwang63/characterbot</abstract>
      <url hash="11841e79">2025.findings-acl.1094</url>
      <bibkey>wang-etal-2025-beyond-profile</bibkey>
    </paper>
    <paper id="1095">
      <title>Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing <fixed-case>LLM</fixed-case> Personalization</title>
      <author><first>Yilun</first><last>Qiu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Xiaoyan</first><last>Zhao</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yimeng</first><last>Bai</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>21258-21277</pages>
      <abstract>Personalizing Large Language Models (LLMs) has become a critical step in facilitating their widespread application to enhance individual life experiences. In pursuit of personalization, distilling key preference information from an individual’s historical data as instructional preference context to customize LLM generation has emerged as a promising direction. However, these methods face a fundamental limitation by overlooking the inter-user comparative analysis, which is essential for identifying the inter-user differences that truly shape preferences. To address this limitation, we propose Difference-aware Personalization Learning (DPL), a novel approach that emphasizes extracting inter-user differences to enhance LLM personalization. DPL strategically selects representative users for comparison and establishes a structured standard to extract meaningful, task-relevant differences for customizing LLM generation. Extensive experiments on real-world datasets demonstrate that DPL significantly enhances LLM personalization. We release our code at <url>https://github.com/SnowCharmQ/DPL</url>.</abstract>
      <url hash="e43f82d5">2025.findings-acl.1095</url>
      <bibkey>qiu-etal-2025-measuring</bibkey>
    </paper>
    <paper id="1096">
      <title><fixed-case>V</fixed-case>ideo<fixed-case>RAG</fixed-case>: Retrieval-Augmented Generation over Video Corpus</title>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Kangsan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <pages>21278-21298</pages>
      <abstract>Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. Also, while very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions, losing multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Our code is available at https://github.com/starsuzi/VideoRAG.</abstract>
      <url hash="2569a1c2">2025.findings-acl.1096</url>
      <bibkey>jeong-etal-2025-videorag</bibkey>
    </paper>
    <paper id="1097">
      <title>Synergistic Augmentation: Enhancing Cross-Domain Zero-Shot Slot Filling with Small Model-Assisted Large Language Models</title>
      <author><first>Weizhen</first><last>Li</last></author>
      <author><first>Junbao</first><last>Huang</last></author>
      <author><first>Peijie</first><last>Huang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Yuhong</first><last>Xu</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Jiekun</first><last>Fan</last></author>
      <pages>21299-21312</pages>
      <abstract>In real-world scenarios, cross-domain slot filling in spoken language understanding remains a significant challenge due to data scarcity. Previous works exhibit limited generalization ability in the target domain, demonstrating effective knowledge transfer only on seen slots while performing poorly on unseen slots. Although large language models (LLMs) can alleviate this issue to some extent, they underperform on seen slots compared to small models. To address these challenges, we introduce a novel framework that harnesses the power of a small model to augment the inferential capabilities of LLMs without additional training. Initially, we utilize target domain samples synthesized by LLMs as pre-calculated demonstrations, which are curated and chosen using confidence metrics derived from a small model. We further extract slot predictions from the small model to fully exploit its robust learning of familiar slots. Finally, during the inference process for test inputs, we integrate these demonstrations and slot prediction insights as references to enhance the slot filling performance of LLMs. Experiments on a slot filling dataset and a NER dataset including eight cross-domain settings show our framework achieves the best results. Our codes are publicly available at https://github.com/SIGSDSscau/SLSF.</abstract>
      <url hash="7cb6ecb7">2025.findings-acl.1097</url>
      <bibkey>li-etal-2025-synergistic</bibkey>
    </paper>
    <paper id="1098">
      <title>A Classifier of Word-Level Variants in Witnesses of Biblical <fixed-case>H</fixed-case>ebrew Manuscripts</title>
      <author><first>Iglika</first><last>Nikolova-Stoupak</last></author>
      <author><first>Maxime</first><last>Amblard</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>Sophie</first><last>Robert-Hayek</last></author>
      <author><first>Frédérique</first><last>Rey</last><affiliation>Université de Lorraine</affiliation></author>
      <pages>21313-21329</pages>
      <abstract>The current project is inscribed within the field of stemmatology or the study and/or reconstruction of textual transmission based on the relationship between the available witnesses of given texts. In particular, the variants (differences) at the word-level in manuscripts written in Biblical Hebrew are considered. A strong classifier (F1 value of 0.80) is trained to predict the category of difference between word pairs (‘plus/minus’, ‘inversion’, ‘morphological’, ‘lexical’ or ‘unclassifiable’) as present in collated (aligned) pairs of witnesses. The classifier is non-neural and makes use of the two words themselves as well as part-of-speech (POS) tags, hand-crafted rules per category and synthetically derived data. Other models experimented with include neural ones based on the state-of-the-art model for Modern Hebrew, DictaBERT. Other features whose relevance is tested are different types of morphological information pertaining to the word pairs and the Levenshtein distance between words. A selection of the strongest classifiers as well as the used synthetic data and the steps taken at its derivation are made available. Coincidentally, the corelation between two sets of morphological labels is investigated: professionally established as per the Qumran-Digital online library and automatically derived with the sub-model DictaBERT-morph.</abstract>
      <url hash="9d324e06">2025.findings-acl.1098</url>
      <bibkey>nikolova-stoupak-etal-2025-classifier</bibkey>
    </paper>
    <paper id="1099">
      <title><fixed-case>NOVA</fixed-case>: An Iterative Planning Framework for Enhancing Scientific Innovation with Large Language Models</title>
      <author><first>Xiang</first><last>Hu</last></author>
      <author><first>Hongyu</first><last>Fu</last></author>
      <author><first>Jinge</first><last>Wang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yifeng</first><last>Wang</last><affiliation>Southeast University</affiliation></author>
      <author><first>Zhikun</first><last>Li</last></author>
      <author><first>Renjun</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Yaochu</first><last>Jin</last><affiliation>Westlake University</affiliation></author>
      <author><first>Lili</first><last>Pan</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Zhenzhong</first><last>Lan</last><affiliation>Westlake University</affiliation></author>
      <pages>21330-21359</pages>
      <abstract>Scientific innovation is pivotal for humanity, and harnessing large language models (LLMs) to generate research ideas could transform discovery. However, existing LLMs often produce simplistic and repetitive suggestions due to their limited ability in acquiring external knowledge for innovation. To address this problem, we introduce an enhanced planning and search methodology designed to boost the creative potential of LLM-based systems. Our approach involves an iterative process to purposely plan the retrieval of external knowledge, progressively enriching the idea generation with broader and deeper insights. Validation through automated and human assessments demonstrates that our framework substantially elevates the quality of generated ideas, particularly in novelty and diversity. The number of unique novel ideas produced by our framework is 3.4 times higher than without it. Moreover, our method outperforms the current state-of-the-art, generating at least 2.5 times more top-rated ideas based on 170 seed papers in a Swiss Tournament evaluation. Our code is available at https://github.com/hflyzju/Nova</abstract>
      <url hash="1eb322dc">2025.findings-acl.1099</url>
      <bibkey>hu-etal-2025-nova</bibkey>
    </paper>
    <paper id="1100">
      <title>Query-Driven Multimodal <fixed-case>G</fixed-case>raph<fixed-case>RAG</fixed-case>: Dynamic Local Knowledge Graph Construction for Online Reasoning</title>
      <author><first>Chenyang</first><last>Bu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Guojie</first><last>Chang</last></author>
      <author><first>Zihao</first><last>Chen</last></author>
      <author><first>CunYuan</first><last>Dang</last></author>
      <author><first>Zhize</first><last>Wu</last><affiliation>Hefei University</affiliation></author>
      <author><first>Yi</first><last>He</last><affiliation>College of William and Mary</affiliation></author>
      <author><first>Xindong</first><last>Wu</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>21360-21380</pages>
      <abstract>An increasing adoption of Large Language Models (LLMs) in complex reasoning tasks necessitates their interpretability and reliability. Recent advances to that end include retrieval-augmented generation (RAG) and knowledge graph-enhanced RAG (GraphRAG), whereas they are constrained by static knowledge bases and ineffective multimodal data integration. In response, we propose a Query-Driven Multimodal GraphRAG framework that dynamically constructs local knowledge graphs tailored to query semantics. Our approach 1) derives graph patterns from query semantics to guide knowledge extraction, 2) employs a multi-path retrieval strategy to pinpoint core knowledge, and 3) supplements missing multimodal information ad hoc. Experimental results on the MultimodalQA and WebQA datasets demonstrate that our framework achieves the state-of-the-art performance among unsupervised competitors, particularly excelling in cross-modal understanding of complex queries.</abstract>
      <url hash="bfed6e45">2025.findings-acl.1100</url>
      <bibkey>bu-etal-2025-query</bibkey>
    </paper>
    <paper id="1101">
      <title>A Survey of Uncertainty Estimation Methods on Large Language Models</title>
      <author><first>Zhiqiu</first><last>Xia</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Jinxuan</first><last>Xu</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Yuqian</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Hang</first><last>Liu</last><affiliation>Rutgers University</affiliation></author>
      <pages>21381-21396</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, these models could offer biased, hallucinated, or non-factual responses camouflaged by their fluency and realistic appearance. Uncertainty estimation is the key method to address this challenge. While research efforts in uncertainty estimation are ramping up, there is a lack of comprehensive and dedicated surveys on LLM uncertainty estimation. This survey presents four major avenues of LLM uncertainty estimation. Furthermore, we perform extensive experimental evaluations across multiple methods and datasets. At last, we provide critical and promising future directions for LLM uncertainty estimation.</abstract>
      <url hash="a406fb2c">2025.findings-acl.1101</url>
      <bibkey>xia-etal-2025-survey</bibkey>
    </paper>
    <paper id="1102">
      <title>Beyond Single-Value Metrics: Evaluating and Enhancing <fixed-case>LLM</fixed-case> Unlearning with Cognitive Diagnosis</title>
      <author><first>Yicheng</first><last>Lang</last></author>
      <author><first>Kehan</first><last>Guo</last></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Yujun</first><last>Zhou</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Haomin</first><last>Zhuang</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Tianyu</first><last>Yang</last></author>
      <author><first>Yao</first><last>Su</last></author>
      <author><first>Xiangliang</first><last>Zhang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>21397-21420</pages>
      <abstract>Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation using Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities.</abstract>
      <url hash="1c2753c5">2025.findings-acl.1102</url>
      <bibkey>lang-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1103">
      <title>Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review</title>
      <author><first>Zihan</first><last>Xu</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Haotian</first><last>Ma</last><affiliation>Weill Cornell Medicine, Cornell University</affiliation></author>
      <author><first>Yihao</first><last>Ding</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Gongbo</first><last>Zhang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chunhua</first><last>Weng</last><affiliation>Columbia University</affiliation></author>
      <author><first>Yifan</first><last>Peng</last><affiliation>Weill Cornell Medicine, Cornell University</affiliation></author>
      <pages>21421-21443</pages>
      <abstract>Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM—Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow.</abstract>
      <url hash="f7aeb547">2025.findings-acl.1103</url>
      <bibkey>xu-etal-2025-natural</bibkey>
    </paper>
    <paper id="1104">
      <title>How do Transformer Embeddings Represent Compositions? A Functional Analysis</title>
      <author><first>Aishik</first><last>Nagar</last></author>
      <author><first>Ishaan Singh</first><last>Rawal</last></author>
      <author><first>Mansi</first><last>Dhanania</last></author>
      <author><first>Cheston</first><last>Tan</last><affiliation>A*STAR</affiliation></author>
      <pages>21444-21461</pages>
      <abstract>Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.</abstract>
      <url hash="3a52cda9">2025.findings-acl.1104</url>
      <bibkey>nagar-etal-2025-transformer</bibkey>
    </paper>
    <paper id="1105">
      <title>Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems</title>
      <author><first>Yucheng</first><last>Cai</last></author>
      <author><first>Ke</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yi</first><last>Huang</last><affiliation>Tsinghua University and China Mobile Research Institute</affiliation></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Zhijian</first><last>Ou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>21462-21474</pages>
      <abstract>The retriever, which retrieves relevant knowledge pieces from a knowledge base given a context, is an important component in many natural language processing (NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog systems to improve knowledge acquisition. In knowledge-grounded dialog systems, when conditioning on a given context, there may be multiple relevant and correlated knowledge pieces. However, knowledge pieces are usually assumed to be conditionally independent in current retriever models. To address this issue, we propose Entriever, an energy-based retriever. The Entriever directly models the candidate retrieval results as a whole instead of modeling the knowledge pieces separately, with the relevance score defined by an energy function. We explore various architectures of energy functions and different training methods for Entriever, and show that Entriever substantially outperforms the strong cross-encoder baseline in knowledge retrieval tasks. Furthermore, we show that in semi-supervised training of knowledge-grounded dialog systems, Entriever enables the effective scoring of retrieved knowledge pieces and leads to a significant improvement in the end-to-end performance of the dialog system.</abstract>
      <url hash="43a131d8">2025.findings-acl.1105</url>
      <bibkey>cai-etal-2025-entriever</bibkey>
    </paper>
    <paper id="1106">
      <title><fixed-case>MONTROSE</fixed-case>: <fixed-case>LLM</fixed-case>-driven <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search Self-Refinement for Cross-Domain Rumor Detection</title>
      <author><first>Shanshan</first><last>Liu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Menglong</first><last>Lu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhen</first><last>Huang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zejiang</first><last>He</last></author>
      <author><first>Liu</first><last>Liu</last><affiliation>Suqian University</affiliation></author>
      <author><first>Zhigang</first><last>Sun</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Dongsheng</first><last>Li</last><affiliation>National University of Defense Technology</affiliation></author>
      <pages>21475-21487</pages>
      <abstract>With the emergence of new topics on social media as sources of rumor dissemination, addressing the distribution shifts between source and target domains remains a crucial task in cross-domain rumor detection. Existing feature alignment methods, which aim to reduce the discrepancies between domains, are often susceptible to task interference during training. Additionally, data distribution alignment methods, which rely on existing data to synthesize new training samples, inherently introduce noise. To deal with these challenges, a new cross-domain rumor detection method, MONTROSE, is proposed. It combines LLM-driven Monte Carlo Tree Search (MCTS) data synthesis to generate high-quality synthetic data for the target domain and a domain-sharpness-aware (DSAM) self-refinement approach to train rumor detection models with these synthetic data effectively. Experiments demonstrate the superior performance of MONTROSE in cross-domain rumor detection.</abstract>
      <url hash="84e6972b">2025.findings-acl.1106</url>
      <bibkey>liu-etal-2025-montrose</bibkey>
    </paper>
    <paper id="1107">
      <title><fixed-case>PET</fixed-case>ool<fixed-case>LLM</fixed-case>: Towards Personalized Tool Learning in Large Language Models</title>
      <author><first>Qiancheng</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Heming</first><last>Xia</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Fan</first><last>Liu</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>21488-21503</pages>
      <abstract>Tool learning has emerged as a promising direction by extending Large Language Models’ (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user’s interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs. We release code and data at https://github.com/travis-xu/PEToolBench.</abstract>
      <url hash="246eea70">2025.findings-acl.1107</url>
      <bibkey>xu-etal-2025-petoolllm</bibkey>
    </paper>
    <paper id="1108">
      <title>A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment</title>
      <author><first>Quanwei</first><last>Tang</last></author>
      <author><first>Sophia Yat Mei</first><last>Lee</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Junshuang</first><last>Wu</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Shoushan</first><last>Li</last><affiliation>Soochow University</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>21504-21523</pages>
      <abstract>Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our GraphMPA.</abstract>
      <url hash="a6f679ab">2025.findings-acl.1108</url>
      <bibkey>tang-etal-2025-comprehensive</bibkey>
    </paper>
    <paper id="1109">
      <title>A <fixed-case>MISMATCHED</fixed-case> Benchmark for Scientific Natural Language Inference</title>
      <author><first>Firoz</first><last>Shaik</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Mobashir</first><last>Sadat</last><affiliation>Accenture</affiliation></author>
      <author><first>Nikita</first><last>Gautam</last></author>
      <author><first>Doina</first><last>Caragea</last><affiliation>Kansas State University</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>21524-21538</pages>
      <abstract>Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. Existing datasets for this task are derived from various computer science (CS) domains, whereas non-CS domains are completely ignored. In this paper, we introduce a novel evaluation benchmark for scientific NLI, called MisMatched. The new MisMatched benchmark covers three non-CS domains–Psychology, Engineering, and Public Health, and contains 2,700 human annotated sentence pairs. We establish strong baselines on MisMatched using both Pre-trained Small Language Models (SLMs) and Large Language Models (LLMs). Our best performing baseline shows a Macro F1 of only 78.17% illustrating the substantial headroom for future improvements. In addition to introducing the MisMatched benchmark, we show that incorporating sentence pairs having an implicit scientific NLI relation between them in model training improves their performance on scientific NLI. We make our dataset and code publicly available on GitHub.</abstract>
      <url hash="4e52fe32">2025.findings-acl.1109</url>
      <bibkey>shaik-etal-2025-mismatched</bibkey>
    </paper>
    <paper id="1110">
      <title><fixed-case>T</fixed-case>ag<fixed-case>R</fixed-case>outer: Learning Route to <fixed-case>LLM</fixed-case>s through Tags for Open-Domain Text Generation Tasks</title>
      <author><first>Zhou</first><last>Chen</last></author>
      <author><first>Zhiqiang</first><last>Wei</last><affiliation>Baidu</affiliation></author>
      <author><first>Yuqi</first><last>Bai</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xue</first><last>Xiong</last><affiliation>Baidu</affiliation></author>
      <author><first>Jianmin</first><last>Wu</last><affiliation>Baidu</affiliation></author>
      <pages>21539-21564</pages>
      <abstract>Model routing allocates queries to the suitable model, improving system performance while reducing costs. However, existing routing methods face practical limitations that hinder scalability in large-scale applications and struggle to keep up with the rapid growth of the large language model (LLM) ecosystem. To tackle these challenges, we propose TagRouter, a training-free model routing method designed to optimize the synergy among multiple LLMs for open-domain text generation tasks. Experimental results demonstrate that TagRouter outperforms 13 baseline methods, increasing the accept rate of system by 6.15% and reducing costs by 17.20%, achieving optimal cost-efficiency. Our findings provides the LLM community with an efficient and scalable solution for model ensembling, offering users an evolvable “super model.”</abstract>
      <url hash="0d23569d">2025.findings-acl.1110</url>
      <bibkey>chen-etal-2025-tagrouter</bibkey>
    </paper>
    <paper id="1111">
      <title>The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction</title>
      <author><first>Yihuai</first><last>Hong</last></author>
      <author><first>Meng</first><last>Cao</last><affiliation>McGill University</affiliation></author>
      <author><first>Dian</first><last>Zhou</last></author>
      <author><first>Lei</first><last>Yu</last><affiliation>Meta</affiliation></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>21565-21585</pages>
      <abstract>Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs’ reasoning-memorization dynamics by identifying a set of linear features in the model’s residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems. Our code and data are at https://github.com/yihuaihong/Linear_Reasoning_Memory_Features.</abstract>
      <url hash="1e37efd0">2025.findings-acl.1111</url>
      <bibkey>hong-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="1112">
      <title><fixed-case>MPB</fixed-case>ench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification</title>
      <author><first>xu Zhao</first><last>Pan</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Pengfei</first><last>Zhou</last></author>
      <author><first>Jiaxin</first><last>Ai</last></author>
      <author><first>Wangbo</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Kai</first><last>Wang</last><affiliation>NUS</affiliation></author>
      <author><first>Xiaojiang</first><last>Peng</last><affiliation>Shenzhen Technology University</affiliation></author>
      <author><first>Wenqi</first><last>Shao</last></author>
      <author><first>Hongxun</first><last>Yao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Kaipeng</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>21586-21606</pages>
      <abstract>Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, whereas the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answers Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.</abstract>
      <url hash="f334a052">2025.findings-acl.1112</url>
      <bibkey>pan-etal-2025-mpbench</bibkey>
    </paper>
    <paper id="1113">
      <title><fixed-case>CRAB</fixed-case>: Cross-environment Agent Benchmark for Multimodal Language Model Agents</title>
      <author><first>Tianqi</first><last>Xu</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Linyao</first><last>Chen</last></author>
      <author><first>Dai-Jie</first><last>Wu</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Yanjun</first><last>Chen</last></author>
      <author><first>Zecheng</first><last>Zhang</last><affiliation>Kumo.AI</affiliation></author>
      <author><first>Xiang</first><last>Yao</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zhiqiang</first><last>Xie</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yongchao</first><last>Chen</last></author>
      <author><first>Shilong</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bochen</first><last>Qian</last></author>
      <author><first>Anjie</first><last>Yang</last></author>
      <author><first>Zhaoxuan</first><last>Jin</last></author>
      <author><first>Jianbo</first><last>Deng</last></author>
      <author><first>Philip</first><last>Torr</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Bernard</first><last>Ghanem</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Guohao</first><last>Li</last><affiliation>University of Oxford</affiliation></author>
      <pages>21607-21647</pages>
      <abstract>The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and thecomplexities of constructing tasks and evaluators. To overcome these limitations, we introduce CRAB, the first cross-environment agent benchmark framework, incorporating a graph-based fine-grained evaluation method and an efficient task generation method. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging CRAB, we develope CRAB Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated 6 advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%.</abstract>
      <url hash="5841ca05">2025.findings-acl.1113</url>
      <bibkey>xu-etal-2025-crab</bibkey>
    </paper>
    <paper id="1114">
      <title>Towards A “Novel” Benchmark: Evaluating Literary Fiction with Large Language Models</title>
      <author><first>Wenqing</first><last>Wang</last></author>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>21648-21673</pages>
      <abstract>Current exploration on creative generation focuses mainly on short stories, poetry, and scripts. With the expansion of Large Language Models (LLMs) context windows, “novel” avenues emerge. This study aims to extend the boundaries of Natural Language Generation (NLG) evaluation by exploring LLMs’ capabilities in more challenging long-form fiction. We propose a new multi-level evaluation framework that incorporates ten metrics across the Macro, Meso, and Micro levels. An annotated fiction dataset, sourced from human authors, LLMs, and human-AI collaborations in both English and Chinese is then constructed. Human evaluation reveals notable disparities between LLM-generated and human-authored fictions, particularly the “high-starting, low-ending” pattern in LLM outputs. We further probe ten high-performing LLMs through different prompt templates, achieving moderate correlations by strategically utilizing diverse LLMs tailored to different levels, as an initial step towards better automatic fiction evaluation. Finally, we offer a fine-grained analysis of LLMs capabilities through six issues, providing promising insights for future advancements.</abstract>
      <url hash="55d08218">2025.findings-acl.1114</url>
      <bibkey>wang-etal-2025-towards-novel</bibkey>
    </paper>
    <paper id="1115">
      <title>A Reinforcement Learning Framework for Cross-Lingual Stance Detection Using Chain-of-Thought Alignment</title>
      <author><first>Binghui</first><last>Li</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Minghui</first><last>Zou</last></author>
      <author><first>Xiaowang</first><last>Zhang</last><affiliation>Tianjin University, China</affiliation></author>
      <author><first>Shizhan</first><last>Chen</last></author>
      <author><first>Zhiyong</first><last>Feng</last><affiliation>Tianjin University</affiliation></author>
      <pages>21674-21688</pages>
      <abstract>Cross-lingual stance detection identifies users’ attitudes toward specific targets in texts by transferring knowledge from source languages to target languages. Previous studies have typically facilitated this transfer by translating and aligning labels or targets. However, these methods cannot effectively perform cross-lingual transfer of the complex reasoning processes in stance detection. To address this challenge, we propose a reinforcement learning framework using cross-lingual Chain-of-Thought (CoT) alignment, referred to as RCCA. Specifically, we adopt a cross-lingual CoT alignment strategy to obtain the high-quality CoTs generated from target language inputs. After that, we leverage reinforcement learning by sampling CoTs and assigning rewards according to predefined rules, aiming to enhance the model’s generalization capabilities in the target language. Experimental results on four multilingual datasets demonstrate that our approach outperforms competitive methods.</abstract>
      <url hash="1e76b697">2025.findings-acl.1115</url>
      <bibkey>li-etal-2025-reinforcement</bibkey>
    </paper>
    <paper id="1116">
      <title><fixed-case>CARE</fixed-case>-<fixed-case>ST</fixed-case>a<fixed-case>R</fixed-case>: Constraint-aware Self-taught Reasoner</title>
      <author><first>Zhiliang</first><last>Li</last></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Yijun</first><last>Niu</last><affiliation>Institute for Advanced Algorithms Research,Shanghai</affiliation></author>
      <author><first>Beihong</first><last>Jin</last></author>
      <author><first>Qiwen</first><last>Shi</last></author>
      <author><first>Yuchen</first><last>Feng</last><affiliation>Institute for Advanced Algorithms Research</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Jie</first><last>Hu</last></author>
      <author><first>Mingchuan</first><last>Yang</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <pages>21689-21703</pages>
      <abstract>In real-world applications, large language models (LLMs) often need to handle diverse and complex instructions. Specifically, when instructions are subject to multiple constraints, some of which are somewhat ambiguous, LLMs often fail to produce answers that satisfy all constraints, limiting their effectiveness in various tasks. To address this challenge, we examine the different constraints in the instructions and discover that the difficulty of determining whether an answer meets a constraint varies widely, from extremely straightforward to exceptionally perplexing. Correspondingly, we propose to assign constraints to different constraint levels. Furthermore, inspired by chain-of-thought (CoT) and self-taught reasoner (STaR), we propose a two-stage method named CARE-STaR (Constraint-AwaRE STaR). Our method distinguishes constraints within instructions by generating different CoTs and guides LLMs to autonomously learn optimal answers by setting the positive rewards to the CoTs that are beneficial to generating accurate answers and iteratively optimizing these answers. We have conducted extensive experiments on three instruction-following benchmarks, taking three existing LLMs as base LLMs, respectively. Experimental results indicate that our method substantially enhances the capability of these LLMs to handle complex instructions, outperforming supervised fine-tuning (SFT). Our code is available at https://github.com/lzl0124/carestar.</abstract>
      <url hash="6fb4e824">2025.findings-acl.1116</url>
      <bibkey>li-etal-2025-care</bibkey>
    </paper>
    <paper id="1117">
      <title>Is It <fixed-case>JUST</fixed-case> Semantics? A Case Study of Discourse Particle Understanding in <fixed-case>LLM</fixed-case>s</title>
      <author><first>William Berkeley</first><last>Sheffield</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Kanishka</first><last>Misra</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Ashwini</first><last>Deo</last></author>
      <author><first>Kyle</first><last>Mahowald</last><affiliation>The University of Texas at Austin</affiliation></author>
      <author><first>Junyi Jessy</first><last>Li</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>21704-21715</pages>
      <abstract>Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects,as exemplified by the diverse uses of the particle *just* (e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English *just*, a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles.</abstract>
      <url hash="b573a7fd">2025.findings-acl.1117</url>
      <bibkey>sheffield-etal-2025-just</bibkey>
    </paper>
    <paper id="1118">
      <title>War of Thoughts: Competition Stimulates Stronger Reasoning in Large Language Models</title>
      <author><first>Yibin</first><last>Chen</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Jinyi</first><last>Liu</last></author>
      <author><first>Yan</first><last>Zheng</last><affiliation>Tianjin Unibersity, China</affiliation></author>
      <author><first>Yifu</first><last>Yuan</last></author>
      <author><first>Jianye</first><last>Hao</last><affiliation>Tianjin University</affiliation></author>
      <pages>21716-21737</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have reshaped the landscape of reasoning tasks, particularly through test-time scaling (TTS) to enhance LLM reasoning. Prior research has used structures such as trees or graphs to guide LLMs in searching for optimal solutions. These methods are time-consuming and require a strong reward model (RM) to support effective solution space exploration. Tournament-style approaches eliminate the reliance on RMs through comparative evaluation but suffer from transitivity dilemmas, leading to unstable ordering. To address these issues, we propose War of Thoughts (**WoT**), a novel post-hoc method that enhances reasoning without finetuning. WoT comprises two distinct stages: (1) *Exploration*, in which diverse and meaningful candidate solutions are generated through contrastive demonstrations and multi-granularity reasoning specifications; and (2) *Competition*, where these candidate solutions are subjected to multiple rounds of matchups within a competitive arena. Throughout this iterative process, the solutions are optimized and improved, with the optimal solution being determined based on Elo ratings. Extensive experiments across various LLMs demonstrate the superiority of WoT, surpassing baselines by **10–30%**. WoT can effectively stimulate stronger reasoning abilities, achieving impressive TTS performance in both generation budget and model size. It shows higher scalability efficiency compared to the baseline within the same budget. Notably, WoT exhibits excellent scalability with model size, even outperforming a 72B model despite using a 7B model.</abstract>
      <url hash="456f4f8b">2025.findings-acl.1118</url>
      <bibkey>chen-etal-2025-war</bibkey>
    </paper>
    <paper id="1119">
      <title>Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation</title>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Huije</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jisu</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Changgeon</first><last>Ko</last></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>21738-21756</pages>
      <abstract>The detection of mental health problems from social media and the interpretation of these results have been extensively explored. Research has shown that incorporating clinical symptom information into a model enhances domain expertise, improving its detection and interpretation performance. While large language models (LLMs) are shown to be effective for generating explanatory rationales in mental health detection, their substantially big parameter size and high computational cost limit their practicality. Reasoning distillation transfers this ability to smaller language models (SLMs), but inconsistencies in the relevance and domain alignment of LLM-generated rationales pose a challenge. This paper investigates how rationale quality impacts SLM performance in mental health detection and explanation generation. We hypothesize that ensuring high-quality and domain-relevant rationales enhances the distillation. To this end, we propose a framework that selects rationales based on their alignment with expert clinical reasoning. Experiments show that our quality-focused approach significantly enhances SLM performance in both mental disorder detection and rationale generation. This work highlights the importance of rationale quality and offers an insightful framework for knowledge transfer in mental health applications.</abstract>
      <url hash="78ba4fc4">2025.findings-acl.1119</url>
      <bibkey>song-etal-2025-rationale</bibkey>
    </paper>
    <paper id="1120">
      <title>Rethinking Table Instruction Tuning</title>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>21757-21780</pages>
      <abstract>Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.</abstract>
      <url hash="dcbcb501">2025.findings-acl.1120</url>
      <bibkey>deng-mihalcea-2025-rethinking</bibkey>
    </paper>
    <paper id="1121">
      <title><fixed-case>C</fixed-case>lini<fixed-case>D</fixed-case>ial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation</title>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Kapotaksha</first><last>Das</last><affiliation>University of Michigan - Dearborn</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Vitaliy</first><last>Popov</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Mohamed</first><last>Abouelenien</last><affiliation>University of Michigan</affiliation></author>
      <pages>21781-21798</pages>
      <abstract>In clinical operations, teamwork can be the crucial factor that determines the final outcome. Prior studies have shown that sufficient collaboration is the key factor that determines the outcome of an operation. To understand how the team practices teamwork during the operation, we collected **CliniDial** from simulations of medical operations. **CliniDial** includes the audio data and its transcriptions, the simulated physiology signals of the patient manikins, and how the team operates from two camera angles. We annotate behavior codes following an existing framework to understand the teamwork process for **CliniDial**. We pinpoint three main characteristics of our dataset, including its label imbalances, rich and natural interactions, and multiple modalities, and conduct experiments to test existing LLMs’ capabilities on handling data with these characteristics. Experimental results show that **CliniDial** poses significant challenges to the existing models, inviting future effort on developing methods that can deal with real-world clinical data. We open-source the codebase at https://github.com/MichiganNLP/CliniDial.</abstract>
      <url hash="1910a4e4">2025.findings-acl.1121</url>
      <bibkey>deng-etal-2025-clinidial</bibkey>
    </paper>
    <paper id="1122">
      <title>Chumor 2.0: Towards Better Benchmarking <fixed-case>C</fixed-case>hinese Humor Understanding from (Ruo Zhi Ba)</title>
      <author><first>Ruiqi</first><last>He</last></author>
      <author><first>Yushu</first><last>He</last></author>
      <author><first>Longju</first><last>Bai</last></author>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Zhenjie</first><last>Sun</last></author>
      <author><first>Zenghao</first><last>Tang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>He</first><last>Wang</last></author>
      <author><first>Hanchen</first><last>Xia</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Naihao</first><last>Deng</last></author>
      <pages>21799-21818</pages>
      <abstract>Existing humor datasets and evaluations predominantly focus on English, leaving limited resources for culturally nuanced humor in non-English languages like Chinese. To address this gap, we construct **Chumor**, the first and the largest Chinese humor explanation dataset. **Chumor** is sourced from Ruo Zhi Ba (RZB, 弱智吧), a Chinese Reddit-like platform known for sharing intellectually challenging and culturally specific jokes. We test ten LLMs through direct and chain-of-thought prompting, revealing that **Chumor** poses significant challenges to existing LLMs, with their accuracy slightly above random and far below human. In addition, our analysis highlights that human-annotated humor explanations are significantly better than those generated by GPT-4o and ERNIE4-turbo. We release **Chumor** at https://huggingface.co/datasets/MichiganNLP/Chumor , our project page is at https://github.com/MichiganNLP/Chumor-2.0 , our leaderboard is at https://huggingface.co/spaces/MichiganNLP/Chumor-leaderboard , and our codebase is at https://github.com/MichiganNLP/Chumor-2.0 .</abstract>
      <url hash="326edaa9">2025.findings-acl.1122</url>
      <bibkey>he-etal-2025-chumor</bibkey>
    </paper>
    <paper id="1123">
      <title>Explicit <fixed-case>B</fixed-case>ayesian Inference to Uncover the Latent Themes of Large Language Models</title>
      <author><first>Raymond</first><last>Li</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Chuyuan</first><last>Li</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Gabriel</first><last>Murray</last></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>, University of British Columbia</affiliation></author>
      <pages>21819-21833</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive generative capabilities, yet their inner mechanisms remain largely opaque. In this work, we introduce a novel approach to interpret LLMs generation process through the lens of an explicit Bayesian framework by inferring latent topic variables via variational inference. Specifically, we leverage a variational autoencoder-based neural topic model to dynamically approximate the posterior distribution over the high-level latent topic variables at each generation step. By reconstructing the LLM’s next-token predictions through these latent topics and maintaining a regularized latent space, our method yields interpretable and diverse topic representations but also has the ability to effectively captures semantic shifts throughout the text. We validate our approach on multiple datasets, showing that our latent topics outperform state-of-the-art topic models on intrinsic measures of coherence and diversity. Furthermore, we demonstrate the utility of our approach in downstream applications by using the inferred topic distributions to retrieve relevant demonstration examples for in-context learning, resulting in significant gains on classification and summarization tasks.</abstract>
      <url hash="2a281232">2025.findings-acl.1123</url>
      <bibkey>li-etal-2025-explicit</bibkey>
    </paper>
    <paper id="1124">
      <title>Improving Occupational <fixed-case>ISCO</fixed-case> Classification of Multilingual <fixed-case>S</fixed-case>wiss Job Postings with <fixed-case>LLM</fixed-case>-Refined Training Data</title>
      <author><first>Ann-Sophie</first><last>Gnehm</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Simon</first><last>Clematide</last><affiliation>University of Zurich</affiliation></author>
      <pages>21834-21847</pages>
      <abstract>Classifying occupations in multilingual job postings is challenging due to noisy labels, language variation, and domain-specific terminology. We present a method that refines silver-standard ISCO labels by consolidating them with predictions from pre-fine-tuned models, using large language model (LLM) evaluations to resolve discrepancies. The refined labels are used in Multiple Negatives Ranking (MNR) training for SentenceBERT-based classification. This approach substantially improves performance, raising Top-1 accuracy on silver data from 37.2% to 58.3% and reaching up to 80% precision on held-out data—an over 30-point gain validated by both GPT and human raters. The model benefits from cross-lingual transfer, with particularly strong gains in French and Italian. These results demonstrate hat LLM-guided label refinement can substantially improve multilingual occupation classification in fine-grained taxonomies such as CH-ISCO with 670 classes.</abstract>
      <url hash="b990dc9a">2025.findings-acl.1124</url>
      <bibkey>gnehm-clematide-2025-improving</bibkey>
    </paper>
    <paper id="1125">
      <title>Brevity is the soul of sustainability: Characterizing <fixed-case>LLM</fixed-case> response lengths</title>
      <author><first>Soham</first><last>Poddar</last></author>
      <author><first>Paramita</first><last>Koley</last></author>
      <author><first>Janardan</first><last>Misra</last></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author id="saptarshi-ghosh"><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>21848-21864</pages>
      <abstract>A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies.Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60% by reducing the response length while preserving the quality of LLM responses.</abstract>
      <url hash="b43d7d7b">2025.findings-acl.1125</url>
      <bibkey>poddar-etal-2025-brevity</bibkey>
    </paper>
    <paper id="1126">
      <title>Adversarial Preference Learning for Robust <fixed-case>LLM</fixed-case> Alignment</title>
      <author><first>Yuanfu</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Chenyang</first><last>Xi</last></author>
      <author><first>Bo</first><last>Tang</last></author>
      <author><first>Junyi</first><last>Zhu</last><affiliation>Samsung</affiliation></author>
      <author><first>Wenqiang</first><last>Wei</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Chao</first><last>Yang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jingfeng</first><last>Zhang</last><affiliation>RIKEN and University of Auckland</affiliation></author>
      <author><first>Chaochao</first><last>Lu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yijun</first><last>Niu</last><affiliation>Institute for Advanced Algorithms Research,Shanghai</affiliation></author>
      <author><first>Keming</first><last>Mao</last></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Jie</first><last>Hu</last></author>
      <author><first>Mingchuan</first><last>Yang</last></author>
      <pages>21865-21881</pages>
      <abstract>Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model’s intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.</abstract>
      <url hash="0498b346">2025.findings-acl.1126</url>
      <bibkey>wang-etal-2025-adversarial</bibkey>
    </paper>
    <paper id="1127">
      <title>g<fixed-case>MBA</fixed-case>: Expression Semantic Guided Mixed <fixed-case>B</fixed-case>oolean-Arithmetic Deobfuscation Using Transformer Architectures</title>
      <author><first>Youjeong</first><last>Roh</last></author>
      <author><first>Joon-Young</first><last>Paik</last></author>
      <author><first>Jingun</first><last>Kwon</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Eun-Sun</first><last>Cho</last></author>
      <pages>21882-21888</pages>
      <abstract>Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by converting programs into forms that are more complex to analyze. However, MBA has been increasingly exploited by malware developers to evade detection and cause significant real-world problems. Traditional MBA deobfuscation methods often consider these expressions as part of a black box and overlook their internal semantic information. To bridge this gap, we propose a truth table, which is an automatically constructed semantic representation of an expression’s behavior that does not rely on external resources. The truth table is a mathematical form that represents the output of expression for all possible combinations of input. We also propose a general and extensible guided MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural encoder-decoder Seq2Seq architecture to incorporate this semantic guidance. Experimental results and in-depth analysis show that integrating expression semantics significantly improves performance and highlights the importance of internal semantic expressions in recovering obfuscated code to its original form.</abstract>
      <url hash="8a23c4a8">2025.findings-acl.1127</url>
      <bibkey>roh-etal-2025-gmba</bibkey>
    </paper>
    <paper id="1128">
      <title><fixed-case>READ</fixed-case>oc: A Unified Benchmark for Realistic Document Structured Extraction</title>
      <author><first>Zichao</first><last>Li</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Aizier</first><last>Abulaiti</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Jia</first><last>Zheng</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Shanshan</first><last>Jiang</last></author>
      <author><first>Bin</first><last>Dong</last><affiliation>Ricoh Software Research Center Beijing Co., Ltd.</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>21889-21905</pages>
      <abstract>Document Structured Extraction (DSE) aims to extract structured content from raw documents. Despite the emergence of numerous DSE systems, their unified evaluation remains inadequate, significantly hindering the field’s advancement. This problem is largely attributed to existing benchmark paradigms, which exhibit fragmented and localized characteristics. To offer a thorough evaluation of DSE systems, we introduce a novel benchmark named READoc, which defines DSE as a realistic task of converting unstructured PDFs into semantically rich Markdown. The READoc dataset is derived from 3,576 diverse and real-world documents from arXiv, GitHub, and Zenodo. In addition, we develop a DSE Evaluation S<tex-math>^3</tex-math>uite comprising Standardization, Segmentation and Scoring modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By evaluating a range of pipeline tools, expert visual models, and general Vision-Language Models, we identify the gap between current work and the unified, realistic DSE objective for the first time. We aspire that READoc will catalyze future research in DSE, fostering more comprehensive and practical solutions.</abstract>
      <url hash="3898c52e">2025.findings-acl.1128</url>
      <bibkey>li-etal-2025-readoc</bibkey>
    </paper>
    <paper id="1129">
      <title><fixed-case>T</fixed-case>ic<fixed-case>T</fixed-case>ac: Time-aware Supervised Fine-tuning for Automatic Text Dating</title>
      <author><first>Han</first><last>Ren</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <author><first>Minna</first><last>Peng</last></author>
      <pages>21906-21918</pages>
      <abstract>Pre-trained langauge models have achieved success in many natural language processing tasks, whereas they are trapped by the time-agnostic setting, impacting the performance in automatic text dating. This paper introduces TicTac, a supervised fine-tuning model for automatic text dating. Unlike the existing models that always ignore the temporal relatedness of documents, TicTac has the ability to learn temporal semantic information, which is helpful for capturing the temporal implications over long-time span corpora. As a fine-tuning framework, TicTac employs a contrastive learning-based approach to model two types of temporal relations of diachronic documents. TicTac also adopts a metric learning approach, where the temporal distance between a historical text and its category label is estimated, which benefits to learn temporal semantic information on texts with temporal ordering. Experiments on two diachronic corpora show that our model effectively captures the temporal semantic information and outperforms state-of-the-art baselines.</abstract>
      <url hash="d5a0a11a">2025.findings-acl.1129</url>
      <bibkey>ren-peng-2025-tictac</bibkey>
    </paper>
    <paper id="1130">
      <title>Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting</title>
      <author><first>Hao</first><last>Feng</last></author>
      <author><first>Shu</first><last>Wei</last></author>
      <author><first>Xiang</first><last>Fei</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Wei</first><last>Shi</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yingdong</first><last>Han</last></author>
      <author><first>Lei</first><last>Liao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Binghong</first><last>Wu</last><affiliation>Tencent Hunyuan Team</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>Bytedance Inc.</affiliation></author>
      <author><first>Chunhui</first><last>Lin</last><affiliation>Bytedance</affiliation></author>
      <author><first>Jingqun</first><last>Tang</last></author>
      <author><first>Hao</first><last>Liu</last><affiliation>Bytedance</affiliation></author>
      <author><first>Can</first><last>Huang</last><affiliation>Bytedance</affiliation></author>
      <pages>21919-21936</pages>
      <abstract>Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Current approaches either assemble specialized expert models or directly generate page-level content autoregressively, facing integration overhead, efficiency bottlenecks, and layout structure degradation despite their decent performance. To address these limitations, we present <i>Dolphin</i> (<i>
          <b>Do</b>cument Image <b>P</b>arsing via <b>H</b>eterogeneous Anchor Prompt<b>in</b>g</i>), a novel multimodal document image parsing model following an analyze-then-parse paradigm. In the first stage, Dolphin generates a sequence of layout elements in reading order. These heterogeneous elements, serving as anchors and coupled with task-specific prompts, are fed back to Dolphin for parallel content parsing in the second stage. To train Dolphin, we construct a large-scale dataset of over 30 million samples, covering multi-granularity parsing tasks. Through comprehensive evaluations on both prevalent benchmarks and self-constructed ones, Dolphin achieves state-of-the-art performance across diverse page-level and element-level settings, while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism. The code and pre-trained models are publicly available at https://github.com/ByteDance/Dolphin</abstract>
      <url hash="3f98b010">2025.findings-acl.1130</url>
      <bibkey>feng-etal-2025-dolphin</bibkey>
    </paper>
    <paper id="1131">
      <title><fixed-case>F</fixed-case>an<fixed-case>C</fixed-case>huan: A Multilingual and Graph-Structured Benchmark For Parody Detection and Analysis</title>
      <author><first>Yilun</first><last>Zheng</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Fangkun</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yang</first><last>Ziyi</last></author>
      <author><first>Lin</first><last>Hongchao</last></author>
      <author><first>Zhichao</first><last>Hu</last></author>
      <author><first>Cai</first><last>Xinjun</last></author>
      <author><first>Ziming</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jinxuan</first><last>Chen</last></author>
      <author><first>Sitao</first><last>Luan</last></author>
      <author><first>Jiahao</first><last>Xu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lihui</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>21937-21957</pages>
      <abstract>Parody is an emerging phenomenon on social media, where individuals imitate a role or position opposite to their own, often for humor, provocation, or controversy. Detecting and analyzing parody can be challenging and is often reliant on context, yet it plays a crucial role in understanding cultural values, promoting subcultures, and enhancing self-expression. However, the study of parody is hindered by limited available data and deficient diversity in current datasets. To bridge this gap, we built seven parody datasets from both English and Chinese corpora, with 14,755 annotated users and 21,210 annotated comments in total. To provide sufficient context information, we also collect replies and construct user-interaction graphs to provide richer contextual information, which is lacking in existing datasets. With these datasets, we test traditional methods and Large Language Models (LLMs) on three key tasks: (1) parody detection, (2) comment sentiment analysis with parody, and (3) user sentiment analysis with parody. Our extensive experiments reveal that parody-related tasks still remain challenging for all models, and contextual information plays a critical role. Interestingly, we find that, in certain scenarios, traditional sentence embedding methods combined with simple classifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3, highlighting parody as a significant challenge for LLMs.</abstract>
      <url hash="fd152144">2025.findings-acl.1131</url>
      <bibkey>zheng-etal-2025-fanchuan</bibkey>
    </paper>
    <paper id="1132">
      <title><fixed-case>P</fixed-case>-<fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Dongjun</first><last>Jang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Youngchae</first><last>Ahn</last></author>
      <author><first>Hyopil</first><last>Shin</last><affiliation>Seoul National University</affiliation></author>
      <pages>21958-21979</pages>
      <abstract>This study explores the potential of phonological reasoning within text-based large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess tasks like rhyme word generation, g2p conversion, and syllable counting. Our evaluations across 12 LLMs reveal that while few-shot learning offers inconsistent gains, the introduction of a novel Pedagogically-motivated Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational theories like scaffolding and discovery learning, consistently enhances performance. This method leverages structured guidance to activate latent phonological abilities, achieving up to 52% improvement and even surpassing human baselines in certain tasks. Future work could aim to optimize P-CoT prompts for specific models or explore their application across different linguistic domains.</abstract>
      <url hash="a9200c3c">2025.findings-acl.1132</url>
      <bibkey>jang-etal-2025-p</bibkey>
    </paper>
    <paper id="1133">
      <title><fixed-case>D</fixed-case>yna<fixed-case>C</fixed-case>ode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation</title>
      <author><first>Wenhao</first><last>Hu</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Jinhao</first><last>Duan</last></author>
      <author><first>Chunchen</first><last>Wei</last></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last><affiliation>Drexel University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Drexel University</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>21980-21997</pages>
      <abstract>The rapid advancement of large language models (LLMs) has significantly improved their performance in code generation tasks. However, existing code benchmarks remain static, consisting of fixed datasets with predefined problems. This makes them vulnerable to memorization during training, where LLMs recall specific test cases instead of generalizing to new problems, leading to data contamination and unreliable evaluation results. To address these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that overcomes the limitations of static datasets. DynaCode evaluates LLMs systematically using a complexity-aware metric, incorporating both code complexity and call-graph structures. DynaCode achieves large-scale diversity, generating up to 189 million unique nested code problems across 4 units of code complexity and 16 types of call graphs. Results on 12 latest LLMs show an average performance drop of 16.8 to 45.7 compared to MBPP+, with performance progressively decreasing as complexity increases. This demonstrates DynaCode’s ability to effectively differentiate model performance based on code complexity and how different parts of a program interact. Our benchmark and evaluation code are available at https://github.com/HWH-2000/DynaCode.</abstract>
      <url hash="5e9e8399">2025.findings-acl.1133</url>
      <bibkey>hu-etal-2025-dynacode</bibkey>
    </paper>
    <paper id="1134">
      <title>Small Encoders Can Rival Large Decoders in Detecting Groundedness</title>
      <author><first>Istabrak</first><last>Abbes</last></author>
      <author><first>Gabriele</first><last>Prato</last></author>
      <author><first>Quentin</first><last>Fournier</last><affiliation>Mila - Quebec AI Institute</affiliation></author>
      <author><first>Fernando</first><last>Rodriguez</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Alaa</first><last>Boukhary</last><affiliation>Ailylabs</affiliation></author>
      <author><first>Adam</first><last>Elwood</last><affiliation>Aily Labs</affiliation></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>École Polytechnique de Montréal</affiliation></author>
      <pages>21998-22005</pages>
      <abstract>Augmenting large language models (LLMs) with external context significantly improves their performance in natural language processing (NLP) tasks. However, LLMs struggle to answer queries reliably when the provided context lacks information, often resorting to ungrounded speculation or internal knowledge. Groundedness – generating responses strictly supported by the context – is essential for ensuring factual consistency and trustworthiness. This study focuses on detecting whether a given query is grounded in a document provided in context before the costly answer generation by LLMs. Such a detection mechanism can significantly reduce both inference time and resource consumption. We show that lightweight, task-specific encoder models such as RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in groundedness detection while reducing inference latency by orders of magnitude.</abstract>
      <url hash="53a1e654">2025.findings-acl.1134</url>
      <bibkey>abbes-etal-2025-small</bibkey>
    </paper>
    <paper id="1135">
      <title><fixed-case>KITAB</fixed-case>-Bench: A Comprehensive Multi-Domain Benchmark for <fixed-case>A</fixed-case>rabic <fixed-case>OCR</fixed-case> and Document Understanding</title>
      <author><first>Ahmed</first><last>Heakl</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Muhammad Abdullah</first><last>Sohail</last></author>
      <author><first>Mukul</first><last>Ranjan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rania</first><last>Elbadry</last></author>
      <author><first>Ghazi Shazan</first><last>Ahmad</last></author>
      <author><first>Mohamed</first><last>El-Geish</last><affiliation>Monta AI</affiliation></author>
      <author><first>Omar</first><last>Maher</last><affiliation>Monta AI</affiliation></author>
      <author><first>Zhiqiang</first><last>Shen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <pages>22006-22024</pages>
      <abstract>With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 subdomains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision language models (such as GPT-4o, Gemini, and Qwen) outperform traditional OCR approaches (such as EasyOCR, PaddleOCR, and Surya) by an average of 60% in the character error rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges of accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.</abstract>
      <url hash="945a8ef0">2025.findings-acl.1135</url>
      <bibkey>heakl-etal-2025-kitab</bibkey>
    </paper>
    <paper id="1136">
      <title>Robustness and Confounders in the Demographic Alignment of <fixed-case>LLM</fixed-case>s with Human Perceptions of Offensiveness</title>
      <author><first>Shayan</first><last>Alipour</last><affiliation>Sapienza University of Rome and Sapienza University of Rome</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Mattia</first><last>Samory</last></author>
      <author><first>Tanu</first><last>Mitra</last><affiliation>University of Washington</affiliation></author>
      <pages>22025-22047</pages>
      <abstract>Despite a growing literature finding that large language models (LLMs) exhibit demographic biases, reports with whom they align best are hard to generalize or even contradictory. In this work, we examine the alignment of LLMs with human annotations in five offensive language datasets, comprising approximately 220K annotations. While demographic traits, particularly race, influence alignment, these effects vary across datasets and are often entangled with other factors. Confounders introduced in the annotation process—such as document difficulty, annotator sensitivity, and within-group agreement—account for more variation in alignment patterns than demographic traits. Alignment increases with annotator sensitivity and group agreement, and decreases with document difficulty. Our results underscore the importance of multi-dataset analyses and confounder-aware methodologies in developing robust measures of demographic bias.</abstract>
      <url hash="8f486792">2025.findings-acl.1136</url>
      <bibkey>alipour-etal-2025-robustness</bibkey>
    </paper>
    <paper id="1137">
      <title><fixed-case>AL</fixed-case>-<fixed-case>QASIDA</fixed-case>: Analyzing <fixed-case>LLM</fixed-case> Quality and Accuracy Systematically in Dialectal <fixed-case>A</fixed-case>rabic</title>
      <author><first>Nathaniel Romney</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Shahd</first><last>Abdelmoneim</last></author>
      <author><first>Kelly</first><last>Marchisio</last><affiliation>Cohere and Cohere</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Facebook</affiliation></author>
      <pages>22048-22065</pages>
      <abstract>Dialectal Arabic (DA) varieties are under-served by language technologies, particularly large language models (LLMs). This trend threatens to exacerbate existing social inequalities and limits LLM applications, yet the research community lacks operationalized performance measurements in DA. We present a framework that comprehensively assesses LLMs’ DA modeling capabilities across four dimensions: fidelity, understanding, quality, and diglossia. We evaluate nine LLMs in eight DA varieties and provide practical recommendations. Our evaluation suggests that LLMs do not produce DA as well as they understand it, not because their DA fluency is poor, but because they are reluctant to generate DA. Further analysis suggests that current post-training can contribute to bias against DA, that few-shot examples can overcome this deficiency, and that otherwise no measurable features of input text correlate well with LLM DA performance.</abstract>
      <url hash="2f0ad839">2025.findings-acl.1137</url>
      <bibkey>robinson-etal-2025-al</bibkey>
    </paper>
    <paper id="1138">
      <title>Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?</title>
      <author><first>Seok Hwan</first><last>Song</last></author>
      <author><first>Mohna</first><last>Chakraborty</last></author>
      <author><first>Qi</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Wallapak</first><last>Tavanapong</last><affiliation>Iowa State University</affiliation></author>
      <pages>22066-22081</pages>
      <abstract>Large Language Models (LLMs) have been evaluated using diverse question types, e.g., multiple-choice, true/false, and short/long answers. This study answers an unexplored question about the impact of different question types on LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on three different types of questions using quantitative and deductive reasoning tasks. The performance metrics include accuracy in the reasoning steps and choosing the final answer. Key Findings: (1) Significant differences exist in LLM performance across different question types. (2) Reasoning accuracy does not necessarily correlate with the final selection accuracy. (3) The number of options and the choice of words, influence LLM performance.</abstract>
      <url hash="ebfec298">2025.findings-acl.1138</url>
      <bibkey>song-etal-2025-large</bibkey>
    </paper>
    <paper id="1139">
      <title><fixed-case>M</fixed-case>utant<fixed-case>P</fixed-case>rompt: Prompt Optimization via Mutation Under a Budget on Modest-sized <fixed-case>LM</fixed-case>s</title>
      <author><first>Arijit</first><last>Nag</last></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author><first>Soumen</first><last>Chakrabarti</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>22082-22092</pages>
      <abstract>Prompts serve as a critical instruction interface to unlock the diverse capabilities of Large Language Models (LLMs), thus directly influencing the quality of their outputs. While prompt engineering has shown great promise, identifying optimal prompts remains a significant challenge, particularly for low-resource languages, which often face higher computational costs due to increased token generation and limited gold standard task data. In response, we propose MutantPrompt, a framework that leverages multi-armed bandit algorithms to efficiently identify optimal prompts tailored to low-resource languages. By framing prompt selection as an exploration-exploitation problem under a fixed computational budget, the framework dynamically balances exploring new prompts with exploiting known high-performing ones. We demonstrate the framework’s effectiveness across multiple low-resource Indic language tasks, including classification, question-answering and causal reasoning using three small parameter-size LLMs. The results highlight the cost efficiency of the search method in finding optimal prompts and resulting performance improvements.</abstract>
      <url hash="ac8eca53">2025.findings-acl.1139</url>
      <bibkey>nag-etal-2025-mutantprompt</bibkey>
    </paper>
    <paper id="1140">
      <title>Heuristic-based Search Algorithm in Automatic Instruction-focused Prompt Optimization: A Survey</title>
      <author><first>Wendi</first><last>Cui</last><affiliation>Intuit</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Intuit AI Research</affiliation></author>
      <author><first>Zhuohang</first><last>Li</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Damien</first><last>Lopez</last><affiliation>Austin Peay State University</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <author><first>Bradley A.</first><last>Malin</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Sricharan</first><last>Kumar</last></author>
      <pages>22093-22111</pages>
      <abstract>Recent advances in Large Language Models(LLMs) have led to remarkable achievements across a variety of Natural Language Processing(NLP) tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods (e.g., “chain-of-thought,” “step-by-step” prompts) can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges, pointing toward future opportunities for more robust and versatile LLM applications.</abstract>
      <url hash="a9bc96e3">2025.findings-acl.1140</url>
      <bibkey>cui-etal-2025-heuristic</bibkey>
    </paper>
    <paper id="1141">
      <title><fixed-case>CONSENSAGENT</fixed-case>: Towards Efficient and Effective Consensus in Multi-Agent <fixed-case>LLM</fixed-case> Interactions Through Sycophancy Mitigation</title>
      <author><first>Priya</first><last>Pitre</last></author>
      <author><first>Naren</first><last>Ramakrishnan</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>22112-22133</pages>
      <abstract>Multi-agent large language model (LLM) systems have shown remarkable performance in tasks such as reasoning, planning, and decision-making. However, their applicability is limited by challenges such as high computational costs and robustness issues. In this work, we identify and systematically evaluate a critical yet overlooked challenge: sycophancy, where agents reinforce each other’s responses instead of critically engaging with the debate. This behavior inflates computational costs by requiring additional debate rounds to reach consensus, limiting the efficiency of multi-agent LLM systems. Through experiments on six benchmark reasoning datasets across three models, we analyze the impact of sycophancy and its role in reducing the reliability of multi-agent debate. Motivated by our findings, we propose CONSENSAGENT, a novel framework that dynamically refines prompts based on agent interactions to mitigate sycophancy. CONSENSAGENT improves accuracy of the debate while maintaining efficiency. It significantly outperforms both single-agent and multi-agent baselines, achieving state-of-the-art results across all benchmark datasets. Our findings highlight the crucial role of structured prompt optimization in multi-agent setups and establish a foundation for more reliable, efficient multi-agent LLM systems in real-world applications.</abstract>
      <url hash="048efe0b">2025.findings-acl.1141</url>
      <bibkey>pitre-etal-2025-consensagent</bibkey>
    </paper>
    <paper id="1142">
      <title>The Structural Safety Generalization Problem</title>
      <author><first>Julius</first><last>Broomfield</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Tom</first><last>Gibbs</last><affiliation>Independent</affiliation></author>
      <author><first>George</first><last>Ingebretsen</last></author>
      <author><first>Ethan</first><last>Kosak-Hine</last><affiliation>Independent</affiliation></author>
      <author><first>Tia</first><last>Nasir</last></author>
      <author><first>Jason</first><last>Zhang</last></author>
      <author><first>Reihaneh</first><last>Iranmanesh</last></author>
      <author><first>Sara</first><last>Pieri</last></author>
      <author><first>Reihaneh</first><last>Rabbany</last><affiliation>McGill University and Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal</affiliation></author>
      <author><first>Kellin</first><last>Pelrine</last></author>
      <pages>22134-22173</pages>
      <abstract>LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge—more tractable than universal defenses but essential for long-term safety—we highlight a critical milestone for AI safety research.</abstract>
      <url hash="697ffa42">2025.findings-acl.1142</url>
      <bibkey>broomfield-etal-2025-structural</bibkey>
    </paper>
    <paper id="1143">
      <title><fixed-case>DPO</fixed-case> Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization</title>
      <author><first>Amitava</first><last>Das</last><affiliation>University of of South Carolina</affiliation></author>
      <author><first>Suranjana</first><last>Trivedy</last></author>
      <author><first>Danush</first><last>Khanna</last></author>
      <author><first>Yaswanth</first><last>Narsupalli</last></author>
      <author><first>Basab</first><last>Ghosh</last></author>
      <author><first>Rajarshi</first><last>Roy</last></author>
      <author><first>Gurpreet</first><last>Singh</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Facebook</affiliation></author>
      <author><first>Aishwarya Naresh</first><last>Reganti</last><affiliation>Amazon</affiliation></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <pages>22174-22270</pages>
      <abstract>The rapid advancement of large language models (LLMs) has revolutionized numerous applications, but presents significant challenges in aligning these models with diverse human values, ethical standards, and specific user preferences. Direct Preference Optimization (DPO) has become a cornerstone for preference alignment but is constrained by reliance on fixed divergence measures and limited feature transformations. We introduce <b>DPO-Kernels</b>, an innovative enhancement of DPO that integrates kernel methods to overcome these challenges through four key contributions: (i) <b>Kernelized Representations</b>: These representations enhance divergence measures by using polynomial, RBF, Mahalanobis, and spectral kernels for richer feature transformations. Additionally, we introduce a <b>hybrid loss</b> that combines embedding-based loss with probability-based loss; (ii) <b>Divergence Alternatives</b>: Beyond Kullback–Leibler (KL), we incorporate Jensen-Shannon, Hellinger, Rényi, Bhattacharyya, Wasserstein, and other f-divergences to boost stability and robustness; (iii) <b>Data-Driven Selection</b>: Choosing the optimal kernel-divergence pair among 28 combinations (4 kernels <tex-math>\times</tex-math> 7 divergences) is challenging. We introduce automatic metrics that analyze the data to select the best kernel-divergence pair, eliminating the need for manual tuning; (iv) <b>Hierarchical Mixture of Kernels (HMK)</b>: Combining local and global kernels for precise and large-scale semantic modeling. This approach automatically selects the optimal kernel mixture during training, enhancing modeling flexibility. DPO-Kernels achieve state-of-the-art generalization in factuality, safety, reasoning, and instruction following across 12 datasets. While alignment risks overfitting, Heavy-Tailed Self-Regularization (HT-SR) theory confirms that DPO-Kernels ensure robust generalization in LLMs. Comprehensive resources are available to facilitate further research and application of DPO-Kernels.</abstract>
      <url hash="262981f6">2025.findings-acl.1143</url>
      <bibkey>das-etal-2025-dpo</bibkey>
    </paper>
    <paper id="1144">
      <title>Model-Dependent Moderation: Inconsistencies in Hate Speech Detection Across <fixed-case>LLM</fixed-case>-based Systems</title>
      <author><first>Neil</first><last>Fasching</last></author>
      <author><first>Yphtach</first><last>Lelkes</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <pages>22271-22285</pages>
      <abstract>Content moderation systems powered by large language models (LLMs) are increasingly deployed to detect hate speech; however, no systematic comparison exists between different systems. If different systems produce different outcomes for the same content, it undermines consistency and predictability, leading to moderation decisions that appear arbitrary or unfair. Analyzing seven leading models—dedicated Moderation Endpoints (OpenAI, Mistral), frontier LLMs (Claude 3.5 Sonnet, GPT-4o, Mistral Large, DeepSeek V3), and specialized content moderation APIs (Google Perspective API)—we demonstrate that moderation system choice fundamentally determines hate speech classification outcomes. Using a novel synthetic dataset of 1.3+ million sentences from a factorial design, we find identical content receives markedly different classification values across systems, with variations especially pronounced for specific demographic groups. Analysis across 125 distinct groups reveals these divergences reflect systematic differences in how models establish decision boundaries around harmful content, highlighting significant implications for automated content moderation.</abstract>
      <url hash="12275774">2025.findings-acl.1144</url>
      <bibkey>fasching-lelkes-2025-model</bibkey>
    </paper>
    <paper id="1145">
      <title>Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification</title>
      <author><first>Subhendu</first><last>Khatuya</last></author>
      <author><first>Shashwat</first><last>Naidu</last></author>
      <author id="saptarshi-ghosh"><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <pages>22286-22298</pages>
      <abstract>The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the predefined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94 % in Micro-F1 and 24.85 % in Macro-F1 compared to the closest baseline across all datasets.</abstract>
      <url hash="cbc0773e">2025.findings-acl.1145</url>
      <bibkey>khatuya-etal-2025-label</bibkey>
    </paper>
    <paper id="1146">
      <title>Unsupervised Morphological Tree Tokenizer</title>
      <author><first>Qingyang</first><last>Zhu</last><affiliation>New York University</affiliation></author>
      <author><first>Xiang</first><last>Hu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pengyu</first><last>Ji</last></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>22299-22312</pages>
      <abstract>As a cornerstone in language modeling, tokenization involves segmenting text inputs into pre-defined atomic units. Conventional statistical tokenizers often disrupt constituent boundaries within words, thereby corrupting semantic information. To address this drawback, we introduce morphological structure guidance to tokenization and propose a deep model to induce character-level structures of words. Specifically, the deep model jointly encodes internal structures and representations of words with a mechanism named MorphOverriding to ensure the indecomposability of morphemes. By training the model with self-supervised objectives, our method is capable of inducing character-level structures that align with morphological rules without annotated training data. Based on the induced structures, our algorithm tokenizes words through vocabulary matching in a top-down manner. Empirical results indicate that the proposed method effectively retains complete morphemes and outperforms widely adopted methods such as BPE and WordPiece on both morphological segmentation tasks and language modeling tasks.</abstract>
      <url hash="10eff271">2025.findings-acl.1146</url>
      <bibkey>zhu-etal-2025-unsupervised</bibkey>
    </paper>
    <paper id="1147">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>L</fixed-case>ink: An Interactive Evaluation Framework for Causal Reasoning</title>
      <author><first>Jinyue</first><last>Feng</last></author>
      <author><first>Frank</first><last>Rudzicz</last><affiliation>Dalhousie University</affiliation></author>
      <pages>22313-22326</pages>
      <abstract>We present CausalLink, an innovative evaluation framework that interactively assesses thecausal reasoning skill to identify the correct intervention in conversational language models. Each CausalLink test case creates a hypothetical environment in which the language models are instructed to apply interventions to entities whose interactions follow predefined causal relations generated from controllable causal graphs. Our evaluation framework isolates causal capabilities from the confounding effects of world knowledge and semantic cues. We evaluate a series of LLMs in a scenario featuring movements of geometric shapes and discover that models start to exhibit reliable reasoning on two or three variables at the 14-billion-parameter scale. However, the performance of state-of-the-art models such as GPT4o degrades below random chance as the number of variables increases. We identify and analyze several key failure modes.</abstract>
      <url hash="897d6841">2025.findings-acl.1147</url>
      <bibkey>feng-rudzicz-2025-causallink</bibkey>
    </paper>
    <paper id="1148">
      <title>Toward Global <fixed-case>AI</fixed-case> Inclusivity: A Large-Scale Multilingual Terminology Dataset (<fixed-case>GIST</fixed-case>)</title>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Iman</first><last>Ouzzani</last><affiliation>Carnegie Mellon University Qatar</affiliation></author>
      <author><first>Wenkai</first><last>Li</last></author>
      <author><first>Lechen</first><last>Zhang</last></author>
      <author><first>Tianyue</first><last>Ou</last></author>
      <author><first>Houda</first><last>Bouamor</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>22327-22360</pages>
      <abstract>The field of machine translation has achieved significant advancements, yet domain-specific terminology translation, particularly in AI, remains challenging. This work introduces GIST, a large-scale multilingual AI terminology dataset containing 5K terms extracted from top AI conference papers spanning 2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese, and Russian using a hybrid framework that combines LLMs for extraction with human expertise for translation. The dataset’s quality was benchmarked against existing resources, demonstrating superior translation accuracy through crowdsourced evaluation. GIST was integrated into translation workflows using post-translation refinement methods that required no retraining, where LLM prompting consistently improved BLEU and COMET scores. A web demonstration on the ACL Anthology platform highlights its practical application, showcasing improved accessibility for non-English speakers. We address a critical gap in AI terminology resources and fosters global inclusivity and collaboration in AI research.</abstract>
      <url hash="97945a76">2025.findings-acl.1148</url>
      <bibkey>liu-etal-2025-toward</bibkey>
    </paper>
    <paper id="1149">
      <title>A Joint Optimization Framework for Enhancing Efficiency of Tool Utilization in <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Bin</first><last>Wu</last></author>
      <author><first>Edgar</first><last>Meij</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>22361-22373</pages>
      <abstract>Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex problem solving. Existing efforts for tool utilization typically involve an LLM agent that contains instructions on using the description of the available tools to determine and call the tools required to solve the problem. Inference Scaling techniques, such as chain-of-thought and tree-of-thought reasoning, are commonly used but require significant computational overhead and rendering such methods impractical in real-world applications. In this work, we recognize and formalize the critical role of instructions provided in agent prompts and tool descriptions—collectively referred to as *context*—and show that incomplete *context* is one of the reasons for this computational overhead.To fill this efficiency gap, we propose an optimization framework that jointly refines both the instructions provided in the agent prompt and tool description, enhancing their interaction. Experiments on StableToolBench and RestBench demonstrate that our optimized agents achieve superior efficiency while maintaining effectiveness. Our findings underscore the critical role of context optimization in improving LLM agents for tool utilization, paving the way for more responsive and cost-effective LLM agents. Our code is available at [https://github.com/Bingo-W/ToolOptimization](https://github.com/Bingo-W/ToolOptimization).</abstract>
      <url hash="82eec252">2025.findings-acl.1149</url>
      <bibkey>wu-etal-2025-joint</bibkey>
    </paper>
    <paper id="1150">
      <title>When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits</title>
      <author><first>Jabez</first><last>Magomere</last></author>
      <author><first>Emanuele</first><last>La Malfa</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Manuel</first><last>Tonneau</last><affiliation>Oxford Internet Institute, University of Oxford</affiliation></author>
      <author><first>Ashkan</first><last>Kazemi</last><affiliation>Meedan</affiliation></author>
      <author><first>Scott A.</first><last>Hale</last><affiliation>Meedan, University of Oxford and Alan Turing Institute</affiliation></author>
      <pages>22374-22404</pages>
      <abstract>Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation.</abstract>
      <url hash="1e98e793">2025.findings-acl.1150</url>
      <bibkey>magomere-etal-2025-claims</bibkey>
    </paper>
    <paper id="1151">
      <title>Splintering Nonconcatenative Languages for Better Tokenization</title>
      <author><first>Bar</first><last>Gazit</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Shaltiel</first><last>Shmidman</last></author>
      <author><first>Avi</first><last>Shmidman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>22405-22417</pages>
      <abstract>Common subword tokenization algorithms like BPE and UnigramLM assume that text can be split into meaningful units by concatenative measures alone. This is not true for languages such as Hebrew and Arabic, where morphology is encoded in root-template patterns, or Malay and Georgian, where split affixes are common. We present SPLINTER, a pre-processing step which rearranges text into a linear form that better represents such nonconcatenative morphologies, enabling meaningful contiguous segments to be found by the tokenizer. We demonstrate SPLINTER’s merit using both intrinsic measures evaluating token vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using BERT-architecture models trained for Hebrew.</abstract>
      <url hash="025db46c">2025.findings-acl.1151</url>
      <bibkey>gazit-etal-2025-splintering</bibkey>
    </paper>
    <paper id="1152">
      <title>Aria-<fixed-case>UI</fixed-case>: Visual Grounding for <fixed-case>GUI</fixed-case> Instructions</title>
      <author><first>Yuhao</first><last>Yang</last></author>
      <author><first>Yue</first><last>Wang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Dongxu</first><last>Li</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Ziyang</first><last>Luo</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Bei</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chao</first><last>Huang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <pages>22418-22433</pages>
      <abstract>Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. We release all training data and model checkpoints to foster further research.</abstract>
      <url hash="5be5ba2a">2025.findings-acl.1152</url>
      <bibkey>yang-etal-2025-aria</bibkey>
    </paper>
    <paper id="1153">
      <title>Revealing Hidden Mechanisms of Cross-Country Content Moderation with Natural Language Processing</title>
      <author><first>Neemesh</first><last>Yadav</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Francesco</first><last>Ortu</last><affiliation>University of Trieste and Area Science Park</affiliation></author>
      <author><first>Roya</first><last>Ensafi</last><affiliation>University of Michigan Ann Arbor</affiliation></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>22434-22452</pages>
      <abstract>The ability of Natural Language Processing (NLP) methods to categorize text into multiple classes has motivated their use in online content moderation tasks, such as hate speech and fake news detection. However, there is limited understanding of how or why these methods make such decisions, or why certain content is moderated in the first place. To investigate the hidden mechanisms behind content moderation, we explore multiple directions: 1) training classifiers to reverse-engineer content moderation decisions across countries; 2) explaining content moderation decisions by analyzing Shapley values and LLM-guided explanations. Our primary focus is on content moderation decisions made across countries, using pre-existing corpora sampled from the Twitter Stream Grab. Our experiments reveal interesting patterns in censored posts, both across countries and over time. Through human evaluations of LLM-generated explanations across three LLMs, we assess the effectiveness of using LLMs in content moderation. Finally, we discuss potential future directions, as well as the limitations and ethical considerations of this work.</abstract>
      <url hash="9794c730">2025.findings-acl.1153</url>
      <bibkey>yadav-etal-2025-revealing</bibkey>
    </paper>
    <paper id="1154">
      <title>Unilogit: Robust Machine Unlearning for <fixed-case>LLM</fixed-case>s Using Uniform-Target Self-Distillation</title>
      <author><first>Stefan</first><last>Vasilev</last></author>
      <author><first>Christian</first><last>Herold</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Baohao</first><last>Liao</last></author>
      <author><first>Seyyed Hadi</first><last>Hashemi</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Shahram</first><last>Khadivi</last><affiliation>eBay Research</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>22453-22472</pages>
      <abstract>This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model’s outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model’s ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit’s superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit’s robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.</abstract>
      <url hash="7c8ebe64">2025.findings-acl.1154</url>
      <bibkey>vasilev-etal-2025-unilogit</bibkey>
    </paper>
    <paper id="1155">
      <title>Creating a Lens of <fixed-case>C</fixed-case>hinese Culture: A Multimodal Dataset for <fixed-case>C</fixed-case>hinese Pun Rebus Art Understanding</title>
      <author><first>Tuo</first><last>Zhang</last></author>
      <author><first>Tiantian</first><last>Feng</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Yibin</first><last>Ni</last></author>
      <author><first>Mengqin</first><last>Cao</last></author>
      <author><first>Ruying</first><last>Liu</last></author>
      <author><first>Kiana</first><last>Avestimehr</last></author>
      <author><first>Katharine</first><last>Butler</last></author>
      <author><first>Yanjun</first><last>Weng</last></author>
      <author><first>Mi</first><last>Zhang</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Shrikanth</first><last>Narayanan</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <pages>22473-22487</pages>
      <abstract>Large vision-language models (VLMs) have demonstrated remarkable abilities in understanding everyday content. However, their performance in the domain of art, particularly culturally rich art forms, remains less explored. As a pearl of human wisdom and creativity, art encapsulates complex cultural narratives and symbolism. In this paper, we offer the Pun Rebus Art Dataset, a multimodal dataset for art understanding deeply rooted in traditional Chinese culture. We focus on three primary tasks: identifying salient visual elements, matching elements with their symbolic meanings, and explanations for the conveyed messages. Our evaluation reveals that state-of-the-art VLMs struggle with these tasks, often providing biased and hallucinated explanations and showing limited improvement through in-context learning. By releasing the Pun Rebus Art Dataset, we aim to facilitate the development of VLMs that can better understand and interpret culturally specific content, promoting greater inclusiveness beyond English-based corpora. The dataset and evaluation code are available at [this link](https://github.com/zhang-tuo-pdf/Pun-Rebus-Art-Benchmark).</abstract>
      <url hash="342656e9">2025.findings-acl.1155</url>
      <bibkey>zhang-etal-2025-creating</bibkey>
    </paper>
    <paper id="1156">
      <title><fixed-case>F</fixed-case>ast<fixed-case>D</fixed-case>raft: How to Train Your Draft</title>
      <author><first>Ofir</first><last>Zafrir</last></author>
      <author><first>Igor</first><last>Margulis</last><affiliation>Intel</affiliation></author>
      <author><first>Dorin</first><last>Shteyman</last></author>
      <author><first>Shira</first><last>Guskin</last><affiliation>Intel</affiliation></author>
      <author><first>Guy</first><last>Boudoukh</last></author>
      <pages>22488-22505</pages>
      <abstract>Speculative Decoding has gained popularity as an effective technique for accelerating the auto-regressive inference process of Large Language Models. However, Speculative Decoding entirely relies on the availability of efficient draft models, which are often lacking for many existing language models due to a stringent constraint of vocabulary compatibility. In this work we introduce FastDraft, a novel and efficient approach for pre-training and aligning a draft model to any large language model by incorporating efficient pre-training, followed by fine-tuning over synthetic datasets generated by the target model. We demonstrate FastDraft by training two highly parameter efficient drafts for the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able to produce a draft model with approximately 10 billion tokens on a single server with 8 Intel Gaudi 2 accelerators in under 24 hours. Our results show that the draft model achieves impressive results in key metrics of acceptance rate, block efficiency and up to 3x memory bound speed up when evaluated on code completion and up to 2x in summarization, text completion and instruction tasks. We validate our theoretical findings through benchmarking on the latest Intel Core Ultra, achieving a wall-clock time speedup of up to 2x, indicating a significant reduction in runtime. Due to its high quality, FastDraft unlocks large language models inference on AI-PC and other edge-devices.</abstract>
      <url hash="1fe17b39">2025.findings-acl.1156</url>
      <bibkey>zafrir-etal-2025-fastdraft</bibkey>
    </paper>
    <paper id="1157">
      <title><fixed-case>S</fixed-case>ign<fixed-case>M</fixed-case>usketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale</title>
      <author><first>Shester</first><last>Gueuwou</last></author>
      <author><first>Xiaodan</first><last>Du</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Greg</first><last>Shakhnarovich</last><affiliation>Toyota Technological Institute at Chicago and University of Chicago</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <pages>22506-22521</pages>
      <abstract>A persistent challenge in sign language video processing, including the task of sign language to written language translation, is how we train efficient model given the nature of videos. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body posture of the signer. However, instead of using pose estimation coordinates from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn the complex handshapes and rich facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model trained on publicly avaiable data that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3% of the compute.</abstract>
      <url hash="51990f87">2025.findings-acl.1157</url>
      <bibkey>gueuwou-etal-2025-signmusketeers</bibkey>
    </paper>
    <paper id="1158">
      <title><fixed-case>GUI</fixed-case> Agents: A Survey</title>
      <author><first>Dang</first><last>Nguyen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author id="jian-chen"><first>Jian</first><last>Chen</last></author>
      <author><first>Yu</first><last>Wang</last><affiliation>University of Oregon and Vanderbilt University</affiliation></author>
      <author><first>Gang</first><last>Wu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Namyong</first><last>Park</last><affiliation>Meta AI</affiliation></author>
      <author><first>Zhengmian</first><last>Hu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Hanjia</first><last>Lyu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ryan</first><last>Aponte</last></author>
      <author><first>Yu</first><last>Xia</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xintong</first><last>Li</last></author>
      <author><first>Jing</first><last>Shi</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Hongjie</first><last>Chen</last><affiliation>Dolby Labs.</affiliation></author>
      <author><first>Viet Dac</first><last>Lai</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Zhouhang</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Mehrab</first><last>Tanjim</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Nesreen K.</first><last>Ahmed</last><affiliation>Intel AI Research</affiliation></author>
      <author><first>Puneet</first><last>Mathur</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Lina</first><last>Yao</last><affiliation>University of New South Wales and CSIRO’s Data61</affiliation></author>
      <author><first>Branislav</first><last>Kveton</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Jihyung</first><last>Kil</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <pages>22522-22538</pages>
      <abstract>Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.</abstract>
      <url hash="50cc5619">2025.findings-acl.1158</url>
      <bibkey>nguyen-etal-2025-gui</bibkey>
    </paper>
    <paper id="1159">
      <title><fixed-case>MEDEC</fixed-case>: A Benchmark for Medical Error Detection and Correction in Clinical Notes</title>
      <author><first>Asma</first><last>Ben Abacha</last><affiliation>Microsoft, USA</affiliation></author>
      <author><first>Wen-wai</first><last>Yim</last></author>
      <author><first>Yujuan</first><last>Fu</last></author>
      <author><first>Zhaoyi</first><last>Sun</last></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Thomas</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <pages>22539-22550</pages>
      <abstract>Several studies have shown that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the ability of language models to validate existing or generated medical text for correctness and consistency. In this paper, we introduce MEDEC (https://github.com/abachaa/MEDEC), the first publicly available benchmark for medical error detection and correction in clinical notes, covering five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were not previously seen by any LLM. The dataset has been used in the MEDIQA-CORR 2024 shared task to evaluate seventeen participating systems. In this paper, we describe the data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4, Claude 3.5 Sonnet, Gemini 2.0 Flash, and DeepSeek-R1) for the tasks of detecting and correcting medical errors requiring both medical knowledge and reasoning capabilities. We also conducted a comparative study where two medical doctors performed the same task on the MEDEC test set. The results showed that MEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We also found that although recent LLMs have a good performance in error detection and correction, they are still outperformed by medical doctors in these tasks. We discuss the potential factors behind this gap, the insights from our experiments, the limitations of current evaluation metrics, and share potential pointers for future research.</abstract>
      <url hash="e1d56a2c">2025.findings-acl.1159</url>
      <bibkey>ben-abacha-etal-2025-medec</bibkey>
    </paper>
    <paper id="1160">
      <title>Understanding the Influence of Synthetic Data for Text Embedders</title>
      <author><first>Jacob Mitchell</first><last>Springer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Vaibhav</first><last>Adlakha</last></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>ServiceNow Inc, Mila, McGill University and Mila, McGill University</affiliation></author>
      <author><first>Aditi</first><last>Raghunathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <pages>22551-22567</pages>
      <abstract>Recent progress in developing general purpose text embedders has been driven by training on ever-growing corpora of synthetic LLM-generated data. Nonetheless, no publicly available synthetic dataset exists, posing a barrier to studying its role for generalization. To address this issue, we first reproduce and publicly release the synthetic data proposed by Wang et al. (2024) (Mistral-E5). Our synthetic data is high quality and leads to consistent improvements in performance. Next, we critically examine where exactly synthetic data improves model generalization. Our analysis reveals that benefits from synthetic data are sparse and highly localized to individual datasets. Moreover, we observe trade-offs between the performance on different categories and data that benefits one task, degrades performance on another. Our findings highlight the limitations of current synthetic data approaches for building general-purpose embedders and challenge the notion that training on synthetic data leads to more robust embedding models across tasks.</abstract>
      <url hash="7b5806d9">2025.findings-acl.1160</url>
      <bibkey>springer-etal-2025-understanding</bibkey>
    </paper>
    <paper id="1161">
      <title>Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models</title>
      <author><first>Anar</first><last>Yeginbergen</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Rodrigo</first><last>Agerri</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>22568-22584</pages>
      <abstract>This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially non-factual responses highlights the need for more controlled and evidence-based approaches. We introduce a reconstructed and manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems. Data and code are publicly available: https://github.com/anaryegen/ counter-argument-generation</abstract>
      <url hash="383d61b2">2025.findings-acl.1161</url>
      <bibkey>yeginbergen-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="1162">
      <title>Tell, Don’t Show: Leveraging Language Models’ Abstractive Retellings to Model Literary Themes</title>
      <author><first>Li</first><last>Lucy</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Camilla</first><last>Griffiths</last></author>
      <author><first>Sarah</first><last>Levine</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jennifer L</first><last>Eberhardt</last></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <author><first>David</first><last>Bamman</last><affiliation>University of California Berkeley</affiliation></author>
      <pages>22585-22610</pages>
      <abstract>Conventional bag-of-words approaches for topic modeling, like latent Dirichlet allocation (LDA), struggle with literary text. Literature challenges lexical methods because narrative language focuses on immersive sensory details instead of abstractive description or exposition: writers are advised to *show, don’t tell*. We propose Retell, a simple, accessible topic modeling approach for literature. Here, we prompt resource-efficient, generative language models (LMs) to *tell* what passages *show*, thereby translating narratives’ surface forms into higher-level concepts and themes. By running LDA on LMs’ retellings of passages, we can obtain more precise and informative topics than by running LDA alone or by directly asking LMs to list topics. To investigate the potential of our method for cultural analytics, we compare our method’s outputs to expert-guided annotations in a case study on racial/cultural identity in high school English language arts books.</abstract>
      <url hash="c6395014">2025.findings-acl.1162</url>
      <bibkey>lucy-etal-2025-tell</bibkey>
    </paper>
    <paper id="1163">
      <title><fixed-case>B</fixed-case>ottle<fixed-case>H</fixed-case>umor: Self-Informed Humor Explanation using the Information Bottleneck Principle</title>
      <author><first>EunJeong</first><last>Hwang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <pages>22611-22632</pages>
      <abstract>Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes).Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce BottleHumor, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction.</abstract>
      <url hash="7b2fbfc9">2025.findings-acl.1163</url>
      <bibkey>hwang-etal-2025-bottlehumor</bibkey>
    </paper>
    <paper id="1164">
      <title>Financial Language Model Evaluation (<fixed-case>FL</fixed-case>a<fixed-case>ME</fixed-case>)</title>
      <author><first>Glenn</first><last>Matlin</last></author>
      <author><first>Mika</first><last>Okamoto</last><affiliation>Georgia Tech Research Institute and Georgia Institute of Technology</affiliation></author>
      <author><first>Huzaifa</first><last>Pardawala</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Sudheer</first><last>Chava</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>22633-22679</pages>
      <abstract>Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs’ performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against ‘reasoning-reinforced’ LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.</abstract>
      <url hash="3802576c">2025.findings-acl.1164</url>
      <bibkey>matlin-etal-2025-financial</bibkey>
    </paper>
    <paper id="1165">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>RAG</fixed-case>: Integrating Causal Graphs into Retrieval-Augmented Generation</title>
      <author><first>Nengbo</first><last>Wang</last></author>
      <author><first>Xiaotian</first><last>Han</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Jagdip</first><last>Singh</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Vipin</first><last>Chaudhary</last><affiliation>Case Western Reserve University</affiliation></author>
      <pages>22680-22693</pages>
      <abstract>Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across multiple metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.</abstract>
      <url hash="065e6579">2025.findings-acl.1165</url>
      <bibkey>wang-etal-2025-causalrag</bibkey>
    </paper>
    <paper id="1166">
      <title>Towards Safety Reasoning in <fixed-case>LLM</fixed-case>s: <fixed-case>AI</fixed-case>-agentic Deliberation for Policy-embedded <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Data Creation</title>
      <author><first>Tharindu</first><last>Kumarage</last><affiliation>Amazon and Arizona State University</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Xinyan</first><last>Zhao</last><affiliation>Amazon</affiliation></author>
      <author><first>Richard</first><last>Zemel</last><affiliation>Department of Computer Science, Columbia University and Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Charith</first><last>Peris</last><affiliation>Amazon</affiliation></author>
      <pages>22694-22715</pages>
      <abstract>Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy.</abstract>
      <url hash="0ad87dbc">2025.findings-acl.1166</url>
      <bibkey>kumarage-etal-2025-towards</bibkey>
    </paper>
    <paper id="1167">
      <title>Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Puxuan</first><last>Yu</last><affiliation>Snowflake</affiliation></author>
      <author><first>Daniel</first><last>Cohen</last><affiliation>Dataminr</affiliation></author>
      <author><first>Hemank</first><last>Lamba</last><affiliation>Dataminr Inc.</affiliation></author>
      <author><first>Joel R.</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last><affiliation>Dataminr</affiliation></author>
      <pages>22716-22730</pages>
      <abstract>In search settings, calibrating the scores during the ranking process to quantities such as click-through rates or relevance levels enhances a system’s usefulness and trustworthiness for downstream users. While previous research has improved this notion of calibration for low complexity learning-to-rank models, the larger data demands and parameter count specific to modern neural text rankers produce unique obstacles that hamper the efficacy of methods intended for the learning-to-rank setting.This paper proposes exploiting large language models (LLMs) to provide relevance and uncertainty signals for these neural text rankers to produce scale-calibrated scores through Monte Carlo sampling of natural language explanations (NLEs). Our approach transforms the neural ranking task from ranking textual query-document pairs to ranking corresponding synthesized NLEs. Comprehensive experiments on two popular document ranking datasets show that the NLE-based calibration approach consistently outperforms past calibration methods and LLM-based methods for ranking, calibration, and query performance prediction tasks.</abstract>
      <url hash="5bbf160e">2025.findings-acl.1167</url>
      <bibkey>yu-etal-2025-explain</bibkey>
    </paper>
    <paper id="1168">
      <title>Beyond instruction-conditioning, <fixed-case>M</fixed-case>o<fixed-case>TE</fixed-case>: Mixture of Task Experts for Multi-task Embedding Models</title>
      <author><first>Miguel Romero</first><last>Calvo</last><affiliation>University of Minnesota - Twin Cities and Amazon</affiliation></author>
      <author><first>Shuoyang</first><last>Ding</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Corey D</first><last>Barrett</last><affiliation>Oracle</affiliation></author>
      <author><first>Georgiana</first><last>Dinu</last><affiliation>Amazon</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>22731-22746</pages>
      <abstract>Dense embeddings are fundamental to modern machine learning systems, powering Retrieval-Augmented Generation (RAG), information retrieval, and representation learning. While instruction-conditioning has become the dominant approach for embedding specialization, its direct application to low-capacity models imposes fundamental representational constraints that limit the performance gains derived from specialization. In this paper, we analyze these limitations and introduce the Mixture of Task Experts (MoTE) transformer block, which leverages task-specialized parameters trained with Task-Aware Contrastive Learning () to enhance the model’s ability to generate specialized embeddings. Empirical results show that MoTE achieves 64% higher performance gains in retrieval datasets (<tex-math>+3.27\rightarrow +5.21</tex-math>) and 43% higher performance gains across all datasets (<tex-math>+1.81\rightarrow 2.60</tex-math>). Critically, these gains are achieved without altering instructions, training data, inference time, or number of active parameters.</abstract>
      <url hash="227d7807">2025.findings-acl.1168</url>
      <bibkey>calvo-etal-2025-beyond</bibkey>
    </paper>
    <paper id="1169">
      <title>Metagent-<fixed-case>P</fixed-case>: A Neuro-Symbolic Planning Agent with Metacognition for Open Worlds</title>
      <author><first>YanfangZhou</first><last>YanfangZhou</last></author>
      <author><first>Yuntao</first><last>Liu</last><affiliation>Academy of Military Sciences</affiliation></author>
      <author><first>Xiaodong</first><last>Li</last></author>
      <author><first>Yongqiang</first><last>Zhao</last></author>
      <author><first>Xintong</first><last>Wang</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Jinlong</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Xinhai</first><last>Xu</last><affiliation>Academy of Military Sciences</affiliation></author>
      <pages>22747-22764</pages>
      <abstract>The challenge of developing agents capable of open-world planning remains fundamental to artificial general intelligence (AGI). While large language models (LLMs) have made progress with their vast world knowledge, their limitations in perception, memory, and reliable reasoning still hinder LLM-based agents from achieving human-level performance in long-term tasks. Drawing inspiration from human cognitive-metacognitive collaboration, we propose <b>Metagent-P</b>, integrating the world knowledge of LLMs, the symbolic reasoning capabilities of cognitive architectures, and the self-reflection characteristic of metacognition to construct a “planning-verification-execution-reflection” framework. Metagent-P improves experience utilization through multimodal memory integration. It uses a neural-symbolic hierarchical representation structure to ensure the plan’s reasoning correctness in advance. Finally, it actively adapts the agent to dynamic environments through monitoring, evaluation, and regulation mechanisms. Experimental results show Metagent-P significantly outperforms current state-of-the-art methods in Minecraft. In long-term tasks, Metagent-P reduces the average replanning counts by <b>34%</b> and exceeds the average human success rate by <b>18.96%</b>. Additionally, Metagent-P also demonstrates self-evolution through step-by-step open-world exploration.</abstract>
      <url hash="3141b905">2025.findings-acl.1169</url>
      <bibkey>yanfangzhou-etal-2025-metagent</bibkey>
    </paper>
    <paper id="1170">
      <title><fixed-case>Q</fixed-case>-<fixed-case>STRUM</fixed-case> Debate: Query-Driven Contrastive Summarization for Recommendation Comparison</title>
      <author><first>George-Kirollos</first><last>Saad</last></author>
      <author><first>Scott</first><last>Sanner</last><affiliation>Department of Mechanical and Industrial Engineering, University of Toronto and Department of Computer Science</affiliation></author>
      <pages>22765-22782</pages>
      <abstract>Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.</abstract>
      <url hash="ef7228ed">2025.findings-acl.1170</url>
      <bibkey>saad-sanner-2025-q</bibkey>
    </paper>
    <paper id="1171">
      <title>Inductive Linguistic Reasoning with Large Language Models</title>
      <author><first>Raghav</first><last>Ramji</last></author>
      <author><first>Keshav</first><last>Ramji</last><affiliation>IBM Research AI</affiliation></author>
      <pages>22783-22810</pages>
      <abstract>Evaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models’ knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1% and Llama-3.1-405B-Instruct by 5.9% over chain-of-thought approaches. Furthermore, we demonstrate that our method generalizes to other tasks present in Linguistics Olympiad competitions, achieving state-of-the-art results across nearly all problem types and difficulty levels in the LINGOLY dataset.</abstract>
      <url hash="70bd40d9">2025.findings-acl.1171</url>
      <bibkey>ramji-ramji-2025-inductive</bibkey>
    </paper>
    <paper id="1172">
      <title>Evaluating <fixed-case>LLM</fixed-case>s’ Mathematical and Coding Competency through Ontology-guided Interventions</title>
      <author><first>Pengfei</first><last>Hong</last></author>
      <author><first>Navonil</first><last>Majumder</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Deepanway</first><last>Ghosal</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Somak</first><last>Aditya</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>22811-22849</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce (i) a general ontology of perturbations for math and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, GSMore and HumanEval-Core, respectively, of perturbed math and coding problems to probe LLM capabilities in numeric reasoning and coding tasks.Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust problem solving skills and structured reasoning abilities in many areas, as defined by our ontology.</abstract>
      <url hash="5ac95caa">2025.findings-acl.1172</url>
      <bibkey>hong-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="1173">
      <title>Exploiting Phonetics and Glyph Representation at Radical-level for Classical <fixed-case>C</fixed-case>hinese Understanding</title>
      <author><first>Junyi</first><last>Xiang</last><affiliation>Wuhan Vocational College of Software and Engineering, Wuhan</affiliation></author>
      <author><first>Maofu</first><last>Liu</last><affiliation>Wuhan Unviersity of Science and Technology</affiliation></author>
      <pages>22850-22871</pages>
      <abstract>The diachronic gap between classical and modern Chinese arises from century-scale language evolution through cumulative changes in phonological, syntactic, and lexical systems, resulting in substantial semantic variation that poses significant challenges for the computational modeling of historical texts. Current methods always enhance classical Chinese understanding of pre-trained language models through corpus pre-training or semantic integration. However, they overlook the synergistic relationship between phonetic and glyph features within Chinese characters, which is a critical factor in deciphering characters’ semantics. In this paper, we propose RPGCM, a radical-level phonetics and glyph representation enhanced Chinese model, with powerful fine-grained semantic modeling capabilities. Our model establishes robust contextualized representations through: (1) rules-based radical decomposition and bype pair encoder (BPE) based radical aggregated for structural pattern recognition, (2) phonetic-glyph semantic mapping, and (3) dynamic semantic fusion. Experimental results on CCMRC, WYWEB, and C³Bench benchmarks demonstrate the RPGCM’s superiority and validate that explicit radical-level modeling effectively mitigates semantic variations.</abstract>
      <url hash="b8fd511c">2025.findings-acl.1173</url>
      <bibkey>xiang-liu-2025-exploiting</bibkey>
    </paper>
    <paper id="1174">
      <title>Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training</title>
      <author><first>Toan</first><last>Tran</last><affiliation>Emory University</affiliation></author>
      <author><first>Ruixuan</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <author><first>Li</first><last>Xiong</last><affiliation>Emory University</affiliation></author>
      <pages>22872-22888</pages>
      <abstract>Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model’s training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose DuoLearn, a lightweight yet effective empirical privacy defense for protecting training data of language models by leveraging token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10% across various LLM architectures and datasets compared to the baselines.</abstract>
      <url hash="ee7ff849">2025.findings-acl.1174</url>
      <bibkey>tran-etal-2025-tokens</bibkey>
    </paper>
    <paper id="1175">
      <title>Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics</title>
      <author><first>Ameya</first><last>Godbole</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <pages>22889-22912</pages>
      <abstract>Improvements in large language models have led to increasing optimism that they can serve as reliable evaluators of natural language generation outputs. In this paper, we challenge this optimism in regards to factuality evaluation.We re-evaluate five state-of-the-art factuality metrics on a collection of 11 datasets for summarization, retrieval-augmented generation, and question answering.We find that these evaluators are inconsistent with each other and often misestimate the factual accuracy of NLG systems, both of which can lead to a variety of pitfalls.We further show that these metrics exhibit biases against highly paraphrased outputs and outputs that draw upon faraway parts of the source documents.We urge users of factuality metrics to proceed with caution and manually validate the reliability of these metrics in their domain of interest.</abstract>
      <url hash="40bf208e">2025.findings-acl.1175</url>
      <bibkey>godbole-jia-2025-verify</bibkey>
    </paper>
    <paper id="1176">
      <title><fixed-case>T</fixed-case>ab<fixed-case>XE</fixed-case>val: Why this is a Bad Table? An e<fixed-case>X</fixed-case>haustive Rubric for Table Evaluation</title>
      <author><first>Vihang</first><last>Pancholi</last></author>
      <author><first>Jainit Sushil</first><last>Bafna</last></author>
      <author><first>Tejas</first><last>Anvekar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>22913-22934</pages>
      <abstract>Evaluating tables qualitatively and quantitatively poses a significant challenge, as standard metrics often overlook subtle structural and content-level discrepancies. To address this, we propose a rubric-based evaluation framework that integrates multi-level structural descriptors with fine-grained contextual signals, enabling more precise and consistent table comparison. Building on this, we introduce TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval first aligns reference and predicted tables structurally via TabAlign, then performs semantic and syntactic comparison using TabCompare, offering interpretable and granular feedback. We evaluate TabXEval on TabXBench, a diverse, multi-domain benchmark featuring realistic table perturbations and human annotations. A sensitivity-specificity analysis further demonstrates the robustness and explainability of TabXEval across varied table tasks. Code and data are available at https://corallab- asu.github.io/tabxeval/.</abstract>
      <url hash="8e21f755">2025.findings-acl.1176</url>
      <bibkey>pancholi-etal-2025-tabxeval</bibkey>
    </paper>
    <paper id="1177">
      <title><fixed-case>LADDER</fixed-case>: Language-Driven Slice Discovery and Error Rectification in Vision Classifiers</title>
      <author><first>Shantanu</first><last>Ghosh</last></author>
      <author><first>Rayan</first><last>Syed</last><affiliation>Boston University</affiliation></author>
      <author><first>Chenyu</first><last>Wang</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Vaibhav</first><last>Choudhary</last><affiliation>Boston University</affiliation></author>
      <author><first>Binxu</first><last>Li</last></author>
      <author><first>Clare B</first><last>Poynton</last><affiliation>Boston University</affiliation></author>
      <author><first>Shyam</first><last>Visweswaran</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Kayhan</first><last>Batmanghelich</last><affiliation>Boston University, Boston University</affiliation></author>
      <pages>22935-22970</pages>
      <abstract>Slice discovery refers to identifying systematic biases in the mistakes of pre-trained vision models. Current slice discovery methods in computer vision rely on converting input images into sets of attributes and then testing hypotheses about configurations of these pre-computed attributes associated with elevated error patterns. However, such methods face several limitations: 1) they are restricted by the predefined attribute bank; 2) they lack the <i>common sense</i> reasoning and domain-specific knowledge often required for specialized fields radiology; 3) at best, they can only identify biases in image attributes while overlooking those introduced during preprocessing or data preparation. We hypothesize that bias-inducing variables leave traces in the form of language (logs), which can be captured as unstructured text. Thus, we introduce ladder, which leverages the reasoning capabilities and latent domain knowledge of Large Language Models (LLMs) to generate hypotheses about these mistakes. Specifically, we project the internal activations of a pre-trained model into text using a retrieval approach and prompt the LLM to propose potential bias hypotheses. To detect biases from preprocessing pipelines, we convert the preprocessing data into text and prompt the LLM. Finally, ladder generates pseudo-labels for each identified bias, thereby mitigating all biases without requiring expensive attribute annotations.Rigorous evaluations on 3 natural and 3 medical imaging datasets, 200+ classifiers, and 4 LLMs with varied architectures and pretraining strategies – demonstrate that ladder consistently outperforms current methods. Code is available: <url>https://github.com/batmanlab/Ladder</url>.</abstract>
      <url hash="255c3f7e">2025.findings-acl.1177</url>
      <bibkey>ghosh-etal-2025-ladder</bibkey>
    </paper>
    <paper id="1178">
      <title><fixed-case>GSQ</fixed-case>-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for <fixed-case>LLM</fixed-case>s On-Device Fine-tuning</title>
      <author><first>Sifan</first><last>Zhou</last></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhihang</first><last>Yuan</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Mingjia</first><last>Shi</last><affiliation>University of Virginia, Houmo AI and National University of Singapore</affiliation></author>
      <author><first>Yuzhang</first><last>Shang</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Dawei</first><last>Yang</last><affiliation>Houmo</affiliation></author>
      <pages>22971-22988</pages>
      <abstract>Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point(FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to FP16-based fine-tuning while significantly reducing memory usage ( 50%). Moreover, compared to FP8, at comparable performance levels, our method can reduce 5x power consumption and 11x chip area, making large-scale model adaptation feasible on edge devices.</abstract>
      <url hash="e61a1614">2025.findings-acl.1178</url>
      <bibkey>zhou-etal-2025-gsq</bibkey>
    </paper>
    <paper id="1179">
      <title>Evaluation of <fixed-case>LLM</fixed-case>s in Medical Text Summarization: The Role of Vocabulary Adaptation in High <fixed-case>OOV</fixed-case> Settings</title>
      <author><first>Gunjan</first><last>Balde</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Soumyadeep</first><last>Roy</last></author>
      <author><first>Mainack</first><last>Mondal</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <pages>22989-23004</pages>
      <abstract>Large Language Models (LLMs) recently achieved great success in medical text summarization by simply using in-context learning. However, these recent efforts do not perform fine-grained evaluations under difficult settings where LLMs might fail. They typically report performance scores over the entire dataset. Through our benchmarking study, we show that LLMs show a significant performance drop for data points with high concentration of out-of-vocabulary (OOV) words or with high novelty. Vocabulary adaptation is an intuitive solution to this vocabulary mismatch issue where the LLM vocabulary gets updated with certain expert domain (here, medical) words or subwords. An interesting finding from our study is that Llama-3.1, even with a vocabulary size of around 128K tokens, still faces _over-fragmentation_ issue with medical words. To that end, we show vocabulary adaptation helps improve the LLM summarization performance even in difficult settings. Through extensive experimentation of multiple vocabulary adaptation strategies, two continual pretraining strategies, and three benchmark medical summarization datasets, we gain valuable insights into the role of vocabulary adaptation strategies for customizing LLMs to the medical domain. We also performed a human evaluation study with medical experts where they found that vocabulary adaptation results in more relevant and faithful summaries. Our codebase is made publicly available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.</abstract>
      <url hash="4bcbe24d">2025.findings-acl.1179</url>
      <bibkey>balde-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="1180">
      <title><fixed-case>U</fixed-case>ni<fixed-case>T</fixed-case>: One Document, Many Revisions, Too Many Edit Intention Taxonomies</title>
      <author><first>Fangping</first><last>Lan</last><affiliation>Temple University</affiliation></author>
      <author><first>Abdullah</first><last>Aljebreen</last><affiliation>Temple University</affiliation></author>
      <author><first>Eduard</first><last>Dragut</last><affiliation>Temple University</affiliation></author>
      <pages>23005-23024</pages>
      <abstract>Writing is inherently iterative, each revision enhancing information representation. One revision may contain many edits. Examination of the intentions behind edits provides valuable insights into an editor’s expertise, the dynamics of collaborative writing, and the evolution of a document. Current research on edit intentions lacks a comprehensive edit intention taxonomy (EIT) that spans multiple application domains. As a result, researchers often create new EITs tailored to specific needs, a process that is both time-consuming and costly. To address this gap, we propose UniT, a Unified edit intention Taxonomy that integrates existing EITs encompassing a wide range of edit intentions. We examine the lineage relationship and the construction of 24 EITs. They together have 232 categories across various domains. During the literature survey and integration process, we identify challenges such as one-to-many category matches, incomplete definitions, and varying hierarchical structures. We propose solutions for resolving these issues. Finally, our evaluation shows that our UniT achieves higher inter-annotator agreement scores compared to existing EITs and is applicable to a large set of application domains.</abstract>
      <url hash="1823b5f7">2025.findings-acl.1180</url>
      <bibkey>lan-etal-2025-unit</bibkey>
    </paper>
    <paper id="1181">
      <title>Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration</title>
      <author><first>Xianbing</first><last>Zhao</last></author>
      <author><first>Yiqing</first><last>Lyu</last></author>
      <author><first>Di</first><last>Wang</last><affiliation>Xidian University</affiliation></author>
      <author><first>Buzhou</first><last>Tang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>23025-23035</pages>
      <abstract>Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to explicitly capture intra-theme and inter-theme correlation, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework, namely Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration (PDIMC). PDIMC leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 12% on Recall and 35% on F1-dep. metrics, compared to the previous state-of-the-art model on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of capturing theme correlation and incorporating interactive external feedback.</abstract>
      <url hash="5151e621">2025.findings-acl.1181</url>
      <bibkey>zhao-etal-2025-predicting</bibkey>
    </paper>
    <paper id="1182">
      <title>Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency with Symmetry-Enhanced Training</title>
      <author><first>Yihang</first><last>Yao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhepeng</first><last>Cen</last></author>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>William</first><last>Han</last></author>
      <author><first>Yuyou</first><last>Zhang</last></author>
      <author><first>Emerson</first><last>Liu</last><affiliation>Allegheny Health Network</affiliation></author>
      <author><first>Zuxin</first><last>Liu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Chuang</first><last>Gan</last></author>
      <author><first>Ding</first><last>Zhao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>23036-23052</pages>
      <abstract>Large Language Models (LLMs) have demonstrated strong reasoning capabilities across various tasks. However, even minor variations in query phrasing, despite preserving the underlying semantic meaning, can significantly affect their performance. To address this, we focus on enhancing LLMs’ awareness of symmetry in query variations and propose syMmetry-ENhanceD (MEND) data augmentation, a data-centric approach that improves the model’s ability to extract useful information from context. Unlike existing methods that emphasize reasoning chain augmentation, our approach improves model robustness at the knowledge extraction stage through query augmentation, enabling more data-efficient training and stronger generalization to Out-of-Distribution (OOD) settings. Extensive experiments on both logical and arithmetic reasoning tasks show that MEND enhances reasoning performance across diverse query variations, providing new insights into improving LLM robustness through structured dataset curation.</abstract>
      <url hash="35dd35f3">2025.findings-acl.1182</url>
      <bibkey>yao-etal-2025-language</bibkey>
    </paper>
    <paper id="1183">
      <title><fixed-case>T</fixed-case>riton<fixed-case>B</fixed-case>ench: Benchmarking Large Language Model Capabilities for Generating Triton Operators</title>
      <author><first>Jianling</first><last>Li</last></author>
      <author><first>ShangZhan</first><last>Li</last></author>
      <author><first>Zhenye</first><last>Gao</last></author>
      <author><first>Qi</first><last>Shi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yuxuan</first><last>Li</last></author>
      <author><first>Zefan</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jiacheng</first><last>Huang</last><affiliation>Qiyuan Lab</affiliation></author>
      <author><first>WangHaojie</first><last>WangHaojie</last></author>
      <author><first>Jianrong</first><last>Wang</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>23053-23066</pages>
      <abstract>Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation.</abstract>
      <url hash="1fa9c0f0">2025.findings-acl.1183</url>
      <bibkey>li-etal-2025-tritonbench</bibkey>
    </paper>
    <paper id="1184">
      <title>Just <fixed-case>KIDDIN</fixed-case>’ : Knowledge Infusion and Distillation for Detection of <fixed-case>IN</fixed-case>decent Memes</title>
      <author><first>Rahul</first><last>Garg</last></author>
      <author><first>Trilok</first><last>Padhi</last></author>
      <author><first>Hemang</first><last>Jain</last></author>
      <author><first>Ugur</first><last>Kursuncu</last><affiliation>Georgia State University</affiliation></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>23067-23086</pages>
      <abstract>Detecting toxicity in online multimodal environments, such as memes, remains a challenging task due to the complex contextual connections across modalities (e.g., text and visual), which demand both common-sense reasoning and contextual awareness. To bridge this gap, we propose a hybrid neurosymbolic framework that unifies (1) distillation of implicit contextual knowledge (e.g., sarcasm, cultural references) from Large Vision-Language Models (LVLMs) and (2) infusion of explicit relational semantics through sub-graphs from Knowledge Graphs (KGs). Experimental results on two benchmark datasets show the superior performance of our approach, <i>Knowledge-Infused Distilled Vision-Language Model (KID-VLM)</i>, over the state-of-the-art baselines across AUC and F1, with improvements of 0.5%, and 10.6%, respectively, in HatefulMemes Benchmark across variants. Further, KID-VLM demonstrates better generalizability and achieves the best performance across all baselines in the HarMeme Dataset with a 6.3% and 3.2% in F1 and AUC.Given the contextual complexity of the toxicity detection, KID-VLM showcases the significance of learning compact models (~500M parameters) from both explicit (i.e., KG) and implicit (i.e., LVLMs) contextual cues incorporated through a hybrid neurosymbolic approach. Our codes and pretrained models are publicly available.</abstract>
      <url hash="46be9300">2025.findings-acl.1184</url>
      <bibkey>garg-etal-2025-just</bibkey>
    </paper>
    <paper id="1185">
      <title>Dynamic Personality in <fixed-case>LLM</fixed-case> Agents: A Framework for Evolutionary Modeling and Behavioral Analysis in the Prisoner’s Dilemma</title>
      <author><first>Weiqi</first><last>Zeng</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Dongming</first><last>Zhao</last></author>
      <author><first>Zongfeng</first><last>Qu</last></author>
      <author><first>Ruifang</first><last>He</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <author><first>Qinghua</first><last>Hu</last><affiliation>Tianjin University</affiliation></author>
      <pages>23087-23100</pages>
      <abstract>Using Large Language Model agents to simulate human game behaviors offers valuable insights for human social psychology in anthropomorphic AI research. While current models rely on static personality traits, real-world evidence shows personality evolves through environmental feedback. Recent work introduced dynamic personality traits but lacked natural selection processes and direct psychological metrics, failing to accurately capture authentic dynamic personality variations. To address these limitations, we propose an enhanced framework within the Prisoner’s Dilemma, a socially significant scenario. By using game payoffs as environmental feedback, we drive adaptive personality evolution and analyze correlations between personality metrics and behavior. Our framework reveals new behavioral patterns of agents and evaluates personality-behavior relationships, advancing agent-based social simulations and human-AI symbiosis research.</abstract>
      <url hash="a48d85be">2025.findings-acl.1185</url>
      <bibkey>zeng-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="1186">
      <title>Building A Proof-Oriented Programmer That Is 64% Better Than <fixed-case>GPT</fixed-case>-4o Under Data Scarcity</title>
      <author><first>Dylan</first><last>Zhang</last></author>
      <author><first>Justin</first><last>Wang</last></author>
      <author><first>Tianran</first><last>Sun</last></author>
      <pages>23101-23118</pages>
      <abstract>Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o’s performance by 54% by repairing its outputs over GPT-4o’s self-repair.</abstract>
      <url hash="79b288a5">2025.findings-acl.1186</url>
      <bibkey>zhang-etal-2025-building</bibkey>
    </paper>
    <paper id="1187">
      <title>On the Robust Approximation of <fixed-case>ASR</fixed-case> Metrics</title>
      <author><first>Abdul</first><last>Waheed</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Hanin</first><last>Atwany</last></author>
      <author><first>Rita</first><last>Singh</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>23119-23146</pages>
      <abstract>Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50%.</abstract>
      <url hash="ca706e7d">2025.findings-acl.1187</url>
      <bibkey>waheed-etal-2025-robust</bibkey>
    </paper>
    <paper id="1188">
      <title>Are the Values of <fixed-case>LLM</fixed-case>s Structurally Aligned with Humans? A Causal Perspective</title>
      <author><first>Yipeng</first><last>Kang</last><affiliation>National Key Laboratory of General Artificial Intelligence</affiliation></author>
      <author><first>Junqi</first><last>Wang</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Yexin</first><last>Li</last><affiliation>State Key Laboratory of General Artificial Intelligence, BIGAI</affiliation></author>
      <author><first>Mengmeng</first><last>Wang</last></author>
      <author><first>Wenming</first><last>Tu</last><affiliation>Shanghai Jiaotong University and Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Quansen</first><last>Wang</last><affiliation>Beijing Institute of General Artificial Intelligence</affiliation></author>
      <author><first>Hengli</first><last>Li</last></author>
      <author><first>Tingjun</first><last>Wu</last></author>
      <author><first>Xue</first><last>Feng</last><affiliation>National Key Laboratory of General Artificial Intelligence and Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Fangwei</first><last>Zhong</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <pages>23147-23161</pages>
      <abstract>As large language models (LLMs) become increasingly integrated into critical applications, aligning their behavior with human values presents significant challenges. Current methods, such as Reinforcement Learning from Human Feedback (RLHF), typically focus on a limited set of coarse-grained values and are resource-intensive. Moreover, the correlations between these values remain implicit, leading to unclear explanations for value-steering outcomes. Our work argues that a latent causal value graph underlies the value dimensions of LLMs and that, despite alignment training, this structure remains significantly different from human value systems. We leverage these causal value graphs to guide two lightweight value-steering methods: role-based prompting and sparse autoencoder (SAE) steering, effectively mitigating unexpected side effects. Furthermore, SAE provides a more fine-grained approach to value steering. Experiments on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and controllability of our methods.</abstract>
      <url hash="f93eb02d">2025.findings-acl.1188</url>
      <bibkey>kang-etal-2025-values</bibkey>
    </paper>
    <paper id="1189">
      <title><fixed-case>LLM</fixed-case>s Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models</title>
      <author><first>Xinxin</first><last>Li</last></author>
      <author><first>Huiyao</first><last>Chen</last></author>
      <author><first>Chengjun</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Jun</first><last>Yu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>23162-23180</pages>
      <abstract>Semantic role labeling (SRL) is a crucial task of natural language processing (NLP). Although generative decoder-based large language models (LLMs) have achieved remarkable success across various NLP tasks, they still lag behind state-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we seek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a) retrieval-augmented generation and (b) self-correction. The first mechanism enables LLMs to leverage external linguistic knowledge such as predicate and argument structure descriptions, while the second allows LLMs to identify and correct inconsistent SRL outputs. We conduct extensive experiments on three widely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results demonstrate that our method achieves state-of-the-art performance in both Chinese and English, marking the first successful application of LLMs to surpass encoder-decoder approaches in SRL.</abstract>
      <url hash="defc394e">2025.findings-acl.1189</url>
      <bibkey>li-etal-2025-llms-also</bibkey>
    </paper>
    <paper id="1190">
      <title>Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models</title>
      <author><first>Hanin</first><last>Atwany</last></author>
      <author><first>Abdul</first><last>Waheed</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Rita</first><last>Singh</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>23181-23203</pages>
      <abstract>Speech foundation models trained at a massive scale, both in terms of model and data size, result in robust systems capable of performing multiple speech tasks, including automatic speech recognition (ASR). These models transcend language and domain barriers, yet effectively measuring their performance remains a challenge. Traditional metrics like word error rate (WER) and character error rate (CER) are commonly used to evaluate ASR performance but often fail to reflect transcription quality in critical contexts, particularly when detecting fabricated outputs. This phenomenon, known as hallucination, is especially concerning in high-stakes domains such as healthcare, legal, and aviation, where errors can have severe consequences. In our work, we address this gap by investigating hallucination in ASR models. We examine how factors such as distribution shifts, model size, and model architecture influence the hallucination error rate (HER), a metric we introduce to quantify hallucinations. Our analysis of over 20 ASR models reveals key insights: (1) High WERs can mask low hallucination rates, while low WERs may conceal dangerous hallucinations. (2) Synthetic noise, both adversarial and common perturbations like white noise, pitch shift, and time stretching, increase HER. (3) Distribution shift correlates strongly with HER (<tex-math>\alpha = 0.91</tex-math>). Our findings highlight the importance of incorporating HER alongside traditional metrics like WER to better assess ASR model performance, particularly in high-stakes domains.</abstract>
      <url hash="e8d24a6a">2025.findings-acl.1190</url>
      <bibkey>atwany-etal-2025-lost</bibkey>
    </paper>
    <paper id="1191">
      <title><fixed-case>M</fixed-case>2<fixed-case>PA</fixed-case>: A Multi-Memory Planning Agent for Open Worlds Inspired by Cognitive Theory</title>
      <author><first>YanfangZhou</first><last>YanfangZhou</last></author>
      <author><first>Xiaodong</first><last>Li</last></author>
      <author><first>Yuntao</first><last>Liu</last><affiliation>Academy of Military Sciences</affiliation></author>
      <author><first>Yongqiang</first><last>Zhao</last></author>
      <author><first>Xintong</first><last>Wang</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Zhenyu</first><last>Li</last></author>
      <author><first>Jinlong</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Xinhai</first><last>Xu</last><affiliation>Academy of Military Sciences</affiliation></author>
      <pages>23204-23220</pages>
      <abstract>Open-world planning poses a significant challenge for general artificial intelligence due to environmental complexity and task diversity, especially in long-term tasks and lifelong learning. Inspired by cognitive theories, we propose M2PA, an open-world multi-memory planning agent. M2PA innovates by combining Large Language Models (LLMs) with human-like multi-memory systems, aiming to fully leverage the strengths of both while mitigating their respective limitations. By integrating the expansive world knowledge and language processing capabilities of LLMs with the perception and experience accumulation abilities of the human memory system, M2PA exhibits situation awareness, and experience generalization capabilities, as well as the potential for lifelong learning. In experiments, M2PA significantly outperforms current state-of-the-art agents across 50 Minecraft tasks in zero-shot learning. In exploratory lifelong learning experiments, M2PA demonstrates its continuous learning ability, achieving a <b>38.33%</b> success rate in the “ObtainDiamond” task. Our findings provide a novel paradigm for constructing more effective agents in open-world environments.</abstract>
      <url hash="7e478ee6">2025.findings-acl.1191</url>
      <bibkey>yanfangzhou-etal-2025-m2pa</bibkey>
    </paper>
    <paper id="1192">
      <title><fixed-case>A</fixed-case>nna<fixed-case>A</fixed-case>gent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation</title>
      <author><first>Ming</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Peidong</first><last>Wang</last></author>
      <author><first>Lin</first><last>Wu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaocui</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Daling</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Yuxin</first><last>Chen</last><affiliation>Central University of Finance and Economics</affiliation></author>
      <author><first>Bixuan</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <pages>23221-23235</pages>
      <abstract>Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers’ mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose **AnnaAgent**, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator’s configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on [https://github.com/sci-m-wang/AnnaAgent](https://github.com/sci-m-wang/AnnaAgent).</abstract>
      <url hash="f843b088">2025.findings-acl.1192</url>
      <bibkey>wang-etal-2025-annaagent</bibkey>
    </paper>
    <paper id="1193">
      <title>Diversification Catalyzes Language Models’ Instruction Generalization To Unseen Semantics</title>
      <author><first>Dylan</first><last>Zhang</last></author>
      <author><first>Justin</first><last>Wang</last></author>
      <author><first>Francois</first><last>Charton</last><affiliation>Facebook</affiliation></author>
      <pages>23236-23249</pages>
      <abstract>Instruction-tuned language models excel in knowledge, reasoning, and instruction-following. While knowledge and reasoning are well-explored, the factors enabling generalization to unseen instructions remain underexplored due to challenges in isolating instruction-following dynamics.In this work, we model instruction-following as a computational process and design controlled experiments inspired by the Turing-complete Markov algorithm to disentangle its dynamics. Our findings reveal that the ability to generalize to instructions with unseen semantics emerges only when training data is strategically diversified across rich semantics. This finding gives us the hammer that breaks down the wall separating training instructions from unseen ones encountered in the wild. For specialist models, a balanced mix of in-domain and diverse out-of-domain tasks enhances performance more effectively than simply increasing in-domain data. For generalist models, domain diversification consistently outweighs the costs of reduced task-specific data, regardless of data budgets. Furthermore, we show that proper diversification with a lower data budget can outperform simply scaling up data volume. These findings highlight strategic data diversification as key to optimizing instruction-following and improving model performance across applications.</abstract>
      <url hash="438680bc">2025.findings-acl.1193</url>
      <bibkey>zhang-etal-2025-diversification</bibkey>
    </paper>
    <paper id="1194">
      <title><fixed-case>D</fixed-case>ecompile<fixed-case>B</fixed-case>ench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios</title>
      <author><first>Zeyu</first><last>Gao</last></author>
      <author><first>Yuxin</first><last>Cui</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Siliang</first><last>Qin</last><affiliation>Institute of Information Engineering.CAS</affiliation></author>
      <author><first>Yuanda</first><last>Wang</last></author>
      <author><first>Zhang</first><last>Bolun</last></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University</affiliation></author>
      <pages>23250-23267</pages>
      <abstract>Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present **DecompileBench**, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: real-world function extraction (comprising 23,400 functions from 130 real-world programs), runtime-aware validation, and automated human-centric assessment using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source **DecompileBench** to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements.</abstract>
      <url hash="b2a13340">2025.findings-acl.1194</url>
      <bibkey>gao-etal-2025-decompilebench</bibkey>
    </paper>
    <paper id="1195">
      <title>Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement</title>
      <author><first>Xiaoqing</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Flood</first><last>Sung</last><affiliation>Moonshot AI</affiliation></author>
      <author><first>Xiuying</first><last>Chen</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>23268-23281</pages>
      <abstract>Code generation is crucial in software engineering for automating the coding process efficiently. While test-time computation methods show promise, they suffer from high latency due to multiple computation rounds.To overcome this, we introduce <b>ThinkCoder</b>, a framework that combines thorough exploration with optimal refinement.The exploration phase diversifies the solution space by searching for potential solutions, followed by a refinement phase that enhances precision.This approach allows us to select the best solution through careful consideration before taking action, avoiding excessive trial and error.To further minimize test-time computation overhead, we introduce preference-driven optimization with Reinforced Self-Training (ReST), which uses exploration trajectories from ThinkCoder to guide LLM’s evolution.This approach enhances LLM’s exploration efficiency via preference learning, cutting costs while maintaining accuracy.ThinkCoder boosts the performance with a single LLM, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA models, it improves Pass@1 by 3.0% over MapCoder with just 6.4% of the computation cost.Against AgentCoder, ThinkCoder achieves a 0.5% higher Pass@1 after 2 rounds, outperforming AgentCoder’s 5 rounds.Additionally, ReST with success trajectories enhances efficiency, allowing models like LLaMA2-7B to achieve competitive results using only 20% of the computational resources. These results highlight the framework’s effectiveness and scalability.</abstract>
      <url hash="fdb9f1e5">2025.findings-acl.1195</url>
      <bibkey>zhang-etal-2025-thinking</bibkey>
    </paper>
    <paper id="1196">
      <title>Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuchen</first><last>Wu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Li</first><last>Shen</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>23282-23302</pages>
      <abstract>Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.</abstract>
      <url hash="8584ec49">2025.findings-acl.1196</url>
      <bibkey>wu-etal-2025-edit</bibkey>
    </paper>
    <paper id="1197">
      <title><fixed-case>S</fixed-case>afe<fixed-case>C</fixed-case>hain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities</title>
      <author><first>Fengqing</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhangchen</first><last>Xu</last></author>
      <author><first>Yuetai</first><last>Li</last></author>
      <author><first>Luyao</first><last>Niu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhen</first><last>Xiang</last><affiliation>University of Georgia</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>23303-23320</pages>
      <abstract>Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 13 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.</abstract>
      <url hash="0cc4ceb3">2025.findings-acl.1197</url>
      <bibkey>jiang-etal-2025-safechain</bibkey>
    </paper>
    <paper id="1198">
      <title><fixed-case>ETRQA</fixed-case>: A Comprehensive Benchmark for Evaluating Event Temporal Reasoning Abilities of Large Language Models</title>
      <author><first>Sigang</first><last>Luo</last></author>
      <author><first>Yinan</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Dongying</first><last>Lin</last></author>
      <author><first>Yingying</first><last>Zhai</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Xiaochun</first><last>Yang</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <pages>23321-23339</pages>
      <abstract>Event temporal reasoning (ETR) aims to model and reason about the relationships between events and time, as well as between events in the real world. Proficiency in ETR is a significant indicator that a large language model (LLM) truly understands the physical world. Previous question-answering datasets available for evaluating the ETR ability lack a systematic taxonomy and pay limited attention to compound questions. In this paper, we propose a unified taxonomy for event temporal questions and construct a comprehensive benchmark ETRQA, to evaluate the ETR abilities of LLMs based on this taxonomy. ETRQA not only inherits and expands the evaluation content of existing datasets but also contains multiple categories of compound questions. We evaluate two leading LLM series, Llama and Qwen, on ETRQA across various settings. Our experimental results indicate that large-scale LLMs exhibit certain ETR abilities. Yet they do not perform well in solving specific types of reasoning tasks, including reasoning involving time spans, reasoning for compound questions, and reasoning with fine temporal granularity. Additionally, we hope ETRQA can benefit the temporal reasoning research community for future studies.</abstract>
      <url hash="78a695c3">2025.findings-acl.1198</url>
      <bibkey>luo-etal-2025-etrqa</bibkey>
    </paper>
    <paper id="1199">
      <title>The Law of Knowledge Overshadowing: Towards Understanding, Predicting and Preventing <fixed-case>LLM</fixed-case> Hallucination</title>
      <author><first>Yuji</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Sha</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Cheng</first><last>Qian</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jiateng</first><last>Liu</last></author>
      <author><first>Pengfei</first><last>Yu</last><affiliation>Amazon</affiliation></author>
      <author><first>Chi</first><last>Han</last></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>ChengXiang</first><last>Zhai</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Manling</first><last>Li</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>23340-23358</pages>
      <abstract>Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model’s dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.</abstract>
      <url hash="5f75051d">2025.findings-acl.1199</url>
      <bibkey>zhang-etal-2025-law</bibkey>
    </paper>
    <paper id="1200">
      <title><fixed-case>L</fixed-case>ego<fixed-case>MT</fixed-case>2: Selective Asynchronous Sharded Data Parallel Training for Massive Neural Machine Translation</title>
      <author><first>Fei</first><last>Yuan</last></author>
      <author><first>Yinquan</first><last>Lu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <pages>23359-23376</pages>
      <abstract>It is a critical challenge to learn a single model for massive languages. Prior methods focus on increasing the model size and training data size. However, large models are difficult to optimize efficiently even with distributed parallel training and translation capacity can interfere among languages. To address the challenge, we propose LegoMT2, an efficient training approach with an asymmetric multi-way model architecture for massive multilingual neural machine translation. LegoMT2 shards 435 languages into 8 language-centric groups and attributes one local encoder for each group’s languages and a mix encoder-decoder for all languages. LegoMT2 trains the model through local data parallel and asynchronous distributed updating of parameters. LegoMT2 is 16.2<tex-math>\times</tex-math> faster than the distributed training method for M2M-100-12B (which only for 100 languages) while improving the translation performance by an average of 2.2 BLEU on <i>Flores-101</i>, especially performing better for low-resource languages .</abstract>
      <url hash="a0b9447e">2025.findings-acl.1200</url>
      <bibkey>yuan-etal-2025-legomt2</bibkey>
    </paper>
    <paper id="1201">
      <title>Pruning General Large Language Models into Customized Expert Models</title>
      <author><first>Yiran</first><last>Zhao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Guizhen</first><last>Chen</last></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>23377-23391</pages>
      <abstract>Large Language Models (LLMs) have transformed natural language processing, yet their substantial model sizes often demand significant computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need expert models tailored to specific downstream scenarios. However, current pruning methods primarily focus on maintaining models’ general capabilities, either requiring extensive post-training or performing poorly due to coarse-grained pruning. In this work, we design a <tex-math>\underline{Cus}</tex-math>tom <tex-math>\underline{Prun}</tex-math>ing method (<tex-math>\texttt{Cus-Prun}</tex-math>) to prune a large general model into a smaller lightweight expert model, which is positioned along the “language”, “domain” and “task” dimensions. By identifying and pruning irrelevant neurons of each dimension, <tex-math>\texttt{Cus-Prun}</tex-math> creates expert models without any post-training. Our experiments demonstrate that <tex-math>\texttt{Cus-Prun}</tex-math> consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.</abstract>
      <url hash="bb92cfb8">2025.findings-acl.1201</url>
      <bibkey>zhao-etal-2025-pruning</bibkey>
    </paper>
    <paper id="1202">
      <title>Enhance Multimodal Consistency and Coherence for Text-Image Plan Generation</title>
      <author><first>Xiaoxin</first><last>Lu</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Ranran Haoran</first><last>Zhang</last></author>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>23392-23409</pages>
      <abstract>People get informed of a daily task plan through diverse media involving both texts and images. However, most prior research only focuses on LLM’s capability of textual plan generation. The potential of large-scale models in providing text-image plans remains understudied. Generating high-quality text-image plans faces two main challenges: ensuring consistent alignment between two modalities and keeping coherence among visual steps. To address these challenges, we propose a novel framework that generates and refines text-image plans step-by-step. At each iteration, our framework (1) drafts the next textual step based on the prediction history; (2) edits the last visual step to obtain the next one; (3) extracts PDDL-like visual information; and (4) refines the draft with the extracted visual information. The textual and visual step produced in stage (4) and (2) will then serve as inputs for the next iteration. Our approach offers a plug-and-play improvement to various backbone models, such as Mistral-7B, Gemini-1.5, and GPT-4o. To evaluate the effectiveness of our approach, we collect a new benchmark consisting of 1,100 tasks and their text-image pair solutions covering 11 daily topics. We also design and validate a new set of metrics to evaluate the multimodal consistency and coherence in text-image plans. Extensive experiment results show the effectiveness of our approach on a range of backbone models against competitive baselines.</abstract>
      <url hash="12363e51">2025.findings-acl.1202</url>
      <bibkey>lu-etal-2025-enhance</bibkey>
    </paper>
    <paper id="1203">
      <title>Un-considering Contextual Information: Assessing <fixed-case>LLM</fixed-case>s’ Understanding of Indexical Elements</title>
      <author><first>Metehan</first><last>Oğuz</last></author>
      <author><first>Yavuz Faruk</first><last>Bakman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Duygu Nur</first><last>Yaldiz</last></author>
      <pages>23410-23427</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English</abstract>
      <url hash="a7d968e0">2025.findings-acl.1203</url>
      <bibkey>oguz-etal-2025-un</bibkey>
    </paper>
    <paper id="1204">
      <title>Behavioral Analysis of Information Salience in Large Language Models</title>
      <author><first>Jan</first><last>Trienes</last></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Junyi Jessy</first><last>Li</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <pages>23428-23454</pages>
      <abstract>Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.</abstract>
      <url hash="6f12b8f5">2025.findings-acl.1204</url>
      <bibkey>trienes-etal-2025-behavioral</bibkey>
    </paper>
    <paper id="1205">
      <title>The Behavior Gap: Evaluating Zero-shot <fixed-case>LLM</fixed-case> Agents in Complex Task-Oriented Dialogs</title>
      <author><first>Avinash</first><last>Baidya</last><affiliation>Intuit</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <pages>23455-23472</pages>
      <abstract>Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.</abstract>
      <url hash="eb7b440e">2025.findings-acl.1205</url>
      <bibkey>baidya-etal-2025-behavior</bibkey>
    </paper>
    <paper id="1206">
      <title>Task Facet Learning: A Structured Approach To Prompt Optimization</title>
      <author><first>Gurusha</first><last>Juneja</last></author>
      <author><first>Gautam</first><last>Jajoo</last></author>
      <author><first>Hua</first><last>Li</last></author>
      <author><first>Jian</first><last>Jiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Nagarajan</first><last>Natarajan</last><affiliation>Microsoft Research and Microsoft</affiliation></author>
      <author><first>Amit</first><last>Sharma</last><affiliation>Microsoft Research</affiliation></author>
      <pages>23473-23496</pages>
      <abstract>Given a task in the form of a basic description and its training examples, prompt optimization is the problem of synthesizing the given information into a text prompt for a large language model. Humans solve this problem by also considering the different facets that define a task (e.g., counter-examples, explanations, analogies) and including them in the prompt. However, it is unclear whether existing algorithmic approaches, based on iteratively editing a given prompt or automatically selecting a few in-context examples, can cover the multiple facets required to solve a complex task. In this work, we view prompt optimization as that of learning multiple facets of a task from a set of training examples. We exploit structure in the prompt optimization problem and break down a prompt into loosely coupled semantic sections. The proposed algorithm, UniPrompt, (1) clusters the input space and uses clustered batches so that each batch likely corresponds to a different facet of the task, and (2) utilizes a feedback mechanism to propose adding, editing or deleting a section, which in turn is aggregated over a batch to capture generalizable facets. Empirical evaluation on multiple datasets and a real-world task shows that prompts generated using UniPrompt obtain higher accuracy than human-tuned prompts and those from state-of-the-art methods. In particular, our algorithm can generate long, complex prompts that existing methods are unable to generate.</abstract>
      <url hash="1ac10d47">2025.findings-acl.1206</url>
      <bibkey>juneja-etal-2025-task</bibkey>
    </paper>
    <paper id="1207">
      <title><fixed-case>LLM</fixed-case> as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding</title>
      <author><first>Junlong</first><last>Tong</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Zixuan</first><last>Lin</last></author>
      <author><first>Yingqi</first><last>Fan</last><affiliation>Eastern Institute of Technology, Ningbo</affiliation></author>
      <author><first>Anhao</first><last>Zhao</last></author>
      <author><first>Hui</first><last>Su</last><affiliation>Meituan</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <pages>23497-23517</pages>
      <abstract>Large Language Models (LLMs) are primarily designed for batch processing. Existing methods for adapting LLMs to streaming rely either on expensive re-encoding or specialized architectures with limited scalability. This work identifies three key mismatches in adapting batch-oriented LLMs to streaming: (1) input-attention, (2) output-attention, and (3) position-ID mismatches. While it is commonly assumed that the latter two mismatches require frequent re-encoding, our analysis reveals that only the input-attention mismatch significantly impacts performance, indicating re-encoding outputs is largely unnecessary. To better understand this discrepancy with the common assumption,we provide the first comprehensive analysis of the impact of position encoding on LLMs in streaming, showing that preserving relative positions within source and target contexts is more critical than maintaining absolute order. Motivated by the above analysis, we introduce a group position encoding paradigm built on batch architectures to enhance consistency between streaming and batch modes. Extensive experiments on cross-lingual and cross-modal tasks demonstrate that our method outperforms existing approaches. Our method requires no architectural modifications, exhibits strong generalization in both streaming and batch modes. The code is available at repository https://github.com/EIT-NLP/StreamingLLM.</abstract>
      <url hash="7fef692b">2025.findings-acl.1207</url>
      <bibkey>tong-etal-2025-llm</bibkey>
    </paper>
    <paper id="1208">
      <title><fixed-case>Y</fixed-case>in<fixed-case>Y</fixed-case>ang-Align: A new Benchmark for Competing Objectives and Introducing Multi-Objective Preference based Text-to-Image Alignment</title>
      <author><first>Amitava</first><last>Das</last><affiliation>University of of South Carolina</affiliation></author>
      <author><first>Yaswanth</first><last>Narsupalli</last></author>
      <author><first>Gurpreet</first><last>Singh</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Facebook</affiliation></author>
      <author><first>Suranjana</first><last>Trivedy</last></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <pages>23518-23598</pages>
      <abstract>Precise alignment in Text-to-Image (T2I) systems is crucial for generating visuals that reflect user intent while adhering to ethical and policy standards. Recent controversies, such as the Google Gemini-generated Pope image backlash, highlight the urgent need for robust alignment mechanisms. Building on alignment successes in Large Language Models (LLMs), this paper introduces YinYangAlign, a benchmarking framework designed to evaluate and optimize T2I systems across six inherently contradictory objectives. These objectives highlight core trade-offs, such as balancing faithfulness to prompts with artistic freedom and maintaining cultural sensitivity without compromising creativity. Alongside this benchmark, we propose the Contradictory Alignment Optimization (CAO) framework, an extension of Direct Preference Optimization (DPO), which employs multi-objective optimization techniques to address these competing goals. By leveraging per-axiom loss functions, synergy-driven global preferences, and innovative tools like the Synergy Jacobian, CAO achieves superior alignment across all objectives. Experimental results demonstrate significant improvements in fidelity, diversity, and ethical adherence, setting new benchmarks for the field. This work provides a scalable, effective approach to resolving alignment challenges in T2I systems while offering insights into broader AI alignment paradigms.</abstract>
      <url hash="a7d9502c">2025.findings-acl.1208</url>
      <bibkey>das-etal-2025-yinyang</bibkey>
    </paper>
    <paper id="1209">
      <title><fixed-case>FREE</fixed-case>: Fast and Robust Vision Language Models with Early Exits</title>
      <author><first>Divya Jyoti</first><last>Bajpai</last></author>
      <author><first>Manjesh Kumar</first><last>Hanawal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>23599-23615</pages>
      <abstract>In recent years, Vision-Language Models (VLMs) have shown remarkable performance improvements in Vision-Language tasks. However, their large size poses challenges for real-world applications where inference latency is a concern. To tackle this issue, we propose employing Early Exit (EE) strategies in VLMs. However, training exit classifiers in VLMs is challenging, particularly with limited labeled training data. To address this, we introduce FREE, an adversarial training approach within a GAN-based framework. Here, each exit consists of a transformer layer and a classifier. The transformer layer is adversarially trained to produce feature representations similar to the final layer, while a feature classifier serves as the discriminator. Our method focuses on performing input-adaptive inference that increases inference speed with minimal drop in performance. Experimental results demonstrate the effectiveness of our approach in enhancing accuracy and model robustness by mitigating overthinking and the phenomenon of mid-crisis that we highlight. We experimentally validate that our method speeds up the inference process by more than <tex-math>1.51\times</tex-math> while retaining comparable performance. The anonymized source code is available at https://github.com/Div290/BLIPEE.</abstract>
      <url hash="9f5ff5ff">2025.findings-acl.1209</url>
      <bibkey>bajpai-hanawal-2025-free</bibkey>
    </paper>
    <paper id="1210">
      <title><fixed-case>REPRO</fixed-case>-Bench: Can Agentic <fixed-case>AI</fixed-case> Systems Assess the Reproducibility of Social Science Research?</title>
      <author><first>Chuxuan</first><last>Hu</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Liyun</first><last>Zhang</last></author>
      <author><first>Yeji</first><last>Lim</last></author>
      <author><first>Aum</first><last>Wadhwani</last></author>
      <author><first>Austin</first><last>Peters</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Daniel</first><last>Kang</last></author>
      <pages>23616-23626</pages>
      <abstract>Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.</abstract>
      <url hash="5a10bcd5">2025.findings-acl.1210</url>
      <bibkey>hu-etal-2025-repro</bibkey>
    </paper>
    <paper id="1211">
      <title>Time Travel: A Comprehensive Benchmark to Evaluate <fixed-case>LMM</fixed-case>s on Historical and Cultural Artifacts</title>
      <author><first>Sara</first><last>Ghaboura</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ketan Pravin</first><last>More</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ritesh</first><last>Thawkar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Wafa Al</first><last>Ghallabi</last></author>
      <author><first>Omkar</first><last>Thawakar</last></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Hisham</first><last>Cholakkal</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <author><first>Rao Muhammad</first><last>Anwer</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>23627-23641</pages>
      <abstract>Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models’ capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. We release the TimeTravel dataset and evaluation suite as open-source resources for culturally and historically informed research.</abstract>
      <url hash="80f2e98e">2025.findings-acl.1211</url>
      <bibkey>ghaboura-etal-2025-time</bibkey>
    </paper>
    <paper id="1212">
      <title>Unveiling and Addressing Pseudo Forgetting in Large Language Models</title>
      <author><first>Huashan</first><last>Sun</last></author>
      <author><first>Yizhe</first><last>Yang</last></author>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Jiawei</first><last>Li</last></author>
      <author><first>Yang</first><last>Gao</last></author>
      <pages>23642-23658</pages>
      <abstract>Although substantial efforts have been made to mitigate catastrophic forgetting in continual learning, the intrinsic mechanisms are not well understood. In this work, we demonstrate the existence of “pseudo forgetting”: the performance degradation in previous tasks is not attributed to a loss of capabilities, but rather to the failure of the instructions to activate the appropriate model capabilities. We show that the model’s performance on previous tasks can be restored through two simple interventions: (1) providing partial external correct rationale, and (2) appending semantically meaningless suffixes to the original instructions, to guide the generation of correct rationales. Through empirical analysis of the internal mechanisms governing rationale generation, we reveal that models exhibiting pseudo forgetting show reduced instruction dependence during rationale generation, leading to suboptimal activation of their inherent capabilities. Based on this insight, we propose Rationale-Guidance Difficulty based Replay (RGD-R) framework that dynamically allocates replay data based on the model’s ability to correctly leverage the intrinsic capabilities. Experimental results demonstrate that RGD-R effectively mitigates pseudo forgetting while maintaining model plasticity.</abstract>
      <url hash="e906c57b">2025.findings-acl.1212</url>
      <bibkey>sun-etal-2025-unveiling</bibkey>
    </paper>
    <paper id="1213">
      <title>Improving <fixed-case>MLLM</fixed-case>’s Document Image Machine Translation via Synchronously Self-reviewing Its <fixed-case>OCR</fixed-case> Proficiency</title>
      <author><first>Yupu</first><last>Liang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Yaping</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Zhiyuan</first><last>Chen</last></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lu</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>23659-23678</pages>
      <abstract>Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model’s existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept “Bilingual Cognitive Advantage”. Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks. The code will be released upon acceptance.</abstract>
      <url hash="f3d57cd8">2025.findings-acl.1213</url>
      <bibkey>liang-etal-2025-improving</bibkey>
    </paper>
    <paper id="1214">
      <title><fixed-case>HG</fixed-case>-<fixed-case>I</fixed-case>nsight<fixed-case>L</fixed-case>og: Context Prioritization and Reduction for Question Answering with Non-Natural Language Construct Log Data</title>
      <author><first>Supriya</first><last>Bajpai</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Athira</first><last>Gopal</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Chandrakant</first><last>Harjpal</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Niraj</first><last>Kumar</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <pages>23679-23695</pages>
      <abstract>Modern IT systems generate vast amounts of log data, which pose challenges for Large Language Models (LLMs) due to their large size, irrelevant entries, and non-Natural Language (non-NL) construct (e.g., domain-specific jargon, error codes, file paths, and abbreviations). Traditional methods like Retrieval-Augmented Generation (RAG) and GraphRAG fail to preserve temporal sequences, handle non-NL for context and entities extraction, and dynamically prioritize query-relevant context. To address these limitations, we propose HG-InsightLog, a novel framework that constructs a multi-entity temporal hypergraph representing log attribute-value pair as nodes and connecting them with hyperedges, capturing critical connections in the data. HG-InsightLog introduces a multi-step query personalization mechanism enhancing the Personalized PageRank algorithm to rank hyperedges based on query relevance and contextual centrality to priortize critical connections. Top ranked hyperedges are extracted and converted back into log formats preserving temporal order and reducing context. Experimental results across multiple datasets demonstrate its superiority over existing methods, enhancing factual, causal, and analytical reasoning. Our approach enables smaller LLMs like LLaMA-8B to perform effective log-based QA. Being model-agnostic and training-free, it scales with evolving open-source LLMs without relying on proprietary systems.</abstract>
      <url hash="9a21c122">2025.findings-acl.1214</url>
      <bibkey>bajpai-etal-2025-hg</bibkey>
    </paper>
    <paper id="1215">
      <title>Dialect Normalization using Large Language Models and Morphological Rules</title>
      <author><first>Antonios</first><last>Dimakis</last><affiliation>University of Athens</affiliation></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>23696-23714</pages>
      <abstract>Natural language understanding systems struggle with low-resource languages, including many dialects of high-resource ones. Dialect-to-standard normalization attempts to tackle this issue by transforming dialectal text so that it can be used by standard-language tools downstream. In this study, we tackle this task by introducing a new normalization method that combines rule-based linguistically informed transformations and large language models (LLMs) with targeted few-shot prompting, without requiring any parallel data. We implement our method for Greek dialects and apply it on a dataset of regional proverbs, evaluating the outputs using human annotators. We then use this dataset to conduct downstream experiments, finding that previous results regarding these proverbs relied solely on superficial linguistic information, including orthographic artifacts, while new observations can still be made through the remaining semantics.</abstract>
      <url hash="ca337aca">2025.findings-acl.1215</url>
      <bibkey>dimakis-etal-2025-dialect</bibkey>
    </paper>
    <paper id="1216">
      <title><fixed-case>USDC</fixed-case>: A Dataset of <tex-math>\underline{U}</tex-math>ser <tex-math>\underline{S}</tex-math>tance and <tex-math>\underline{D}</tex-math>ogmatism in Long <tex-math>\underline{C}</tex-math>onversations</title>
      <author><first>Mounika</first><last>Marreddy</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Subba Reddy</first><last>Oota</last><affiliation>INRIA</affiliation></author>
      <author><first>Venkata Charan</first><last>Chinni</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Manish</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <pages>23715-23759</pages>
      <abstract>Analyzing user opinion changes in long conversation threads is extremely critical for applications like enhanced personalization, market research, political campaigns, customer service, targeted advertising, and content moderation. Unfortunately, previous studies on stance and dogmatism in user conversations have focused on training models using datasets annotated at the post level, treating each post as independent and randomly sampling posts from conversation threads. Hence, first, we build a dataset for studying user opinion fluctuations in 764 long multi-user Reddit conversation threads, called USDC. USDC contains annotations for 2 tasks: i) User Stance classification, which involves labeling a user’s stance in a post within a conversation on a five-point scale; ii) User Dogmatism classification, which involves labeling a user’s overall opinion in the conversation on a four-point scale. Besides being time-consuming and costly, manual annotations for USDC are challenging because: 1) Conversation threads could be very long, increasing the chances of noisy annotations; and 2) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Hence, we leverage majority voting on zero-shot, one-shot, and few-shot annotations from Mistral Large and GPT-4 to automate the annotation process. Human annotations on 200 test conversations achieved inter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism with these LLM annotations, indicating a reasonable level of consistency between human and LLM annotations. USDC is then used to finetune and instruction-tune multiple deployable small language models like LLaMA, Falcon and Vicuna for the stance and dogmatism classification tasks. We make the code and dataset publicly available [https://github.com/mounikamarreddy/USDC].</abstract>
      <url hash="d90853c1">2025.findings-acl.1216</url>
      <bibkey>marreddy-etal-2025-usdc</bibkey>
    </paper>
    <paper id="1217">
      <title>Learning to Insert [<fixed-case>PAUSE</fixed-case>] Tokens for Better Reasoning</title>
      <author><first>Eunki</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sangryul</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>23760-23777</pages>
      <abstract>To enhance reasoning capabilities, previous works have explored incorporating special-purpose tokens into the training process. These strategies strengthen the learning mechanism of transformer-based large language models (LLMs). Building on prior research, in which inserting dummy tokens consecutively just before reasoning steps can enhance effectiveness, we introduce a novel approach termed <tex-math>\textbf{D}</tex-math>ynamic <tex-math>\textbf{I}</tex-math>nserting Tokens <tex-math>\textbf{T}</tex-math>raining <tex-math>\textbf{(DIT)}</tex-math>. Our method identifies positions within sequences where model confidence is lowest according to token log-likelihood. Strategically inserting [PAUSE] tokens on these positions bolsters the model’s predictive capabilities for subsequent tokens. Experimental results across diverse datasets and models, from the 2.7B model to the 8B model, demonstrate that DIT consistently outperforms traditional fine-tuning and previous token insertion methods. With this simple yet effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work shows a model-based, dynamic approach rather than a heuristic one, thereby broadening the scope of research in reasoning.</abstract>
      <url hash="4bd0c91e">2025.findings-acl.1217</url>
      <bibkey>kim-etal-2025-learning-insert</bibkey>
    </paper>
    <paper id="1218">
      <title>Understand the Implication: Learning to Think for Pragmatic Understanding</title>
      <author><first>Settaluri Lakshmi</first><last>Sravanthi</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Kishan</first><last>Maharaj</last></author>
      <author><first>Sravani</first><last>Gunnu</last></author>
      <author><first>Abhijit</first><last>Mishra</last><affiliation>University of Texas at Austin and Apple</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>23778-23790</pages>
      <abstract>Pragmatics, the ability to infer meaning beyond literal interpretation, is crucial for social cognition and communication. While LLMs have been benchmarked for their pragmatic understanding, improving their performance remains underexplored. Existing methods rely on annotated labels but overlook the reasoning process humans naturally use to interpret implicit meaning. To bridge this gap, we introduce a novel pragmatic dataset <b>ImpliedMeaningPreference</b> that includes <i>explicit reasoning (‘thoughts’)</i> for both correct and incorrect interpretations. Through preference-tuning and supervised fine-tuning, we demonstrate that thought-based learning significantly enhances LLMs’ pragmatic understanding, improving accuracy by 11.12% across model families. We further discuss a transfer-learning study where we evaluate the performance of <i>thought</i>-based training for the other tasks of pragmatics (presupposition, deixis) that are not seen during the training time and observe an improvement of 16.10% compared to <i>label</i> trained models.</abstract>
      <url hash="b4034a63">2025.findings-acl.1218</url>
      <bibkey>sravanthi-etal-2025-understand</bibkey>
    </paper>
    <paper id="1219">
      <title><fixed-case>WASA</fixed-case>: <fixed-case>WA</fixed-case>termark-based Source Attribution for Large Language Model-Generated Data</title>
      <author><first>Xinyang</first><last>Lu</last></author>
      <author><first>Jingtan</first><last>Wang</last><affiliation>National University of Singapore, National University of Singapore</affiliation></author>
      <author><first>Zitong</first><last>Zhao</last></author>
      <author><first>Zhongxiang</first><last>Dai</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Chuan-Sheng</first><last>Foo</last><affiliation>Centre for Frontier AI Research, A*STAR and Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bryan Kian Hsiang</first><last>Low</last><affiliation>National University of Singapore</affiliation></author>
      <pages>23791-23824</pages>
      <abstract>The impressive performances of Large Language Models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the Intellectual Property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to perform source attribution by identifying the data provider who contributed to the generation of a synthetic text by an LLM. In this paper, we show that this problem can be tackled by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a source attribution framework that satisfies these key properties due to our algorithmic designs. Our framework enables an LLM to learn an accurate mapping from the generated texts to data providers, which sets the foundation for effective source attribution. Extensive empirical evaluations show that our framework achieves effective source attribution.</abstract>
      <url hash="810a2128">2025.findings-acl.1219</url>
      <bibkey>lu-etal-2025-wasa</bibkey>
    </paper>
    <paper id="1220">
      <title>Dense Retrieval with Quantity Comparison Intent</title>
      <author><first>Prayas</first><last>Agrawal</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Nandeesh Kumar K</first><last>M</last><affiliation>Flipkart Group</affiliation></author>
      <author><first>Muthusamy</first><last>Chelliah</last><affiliation>Flipkart</affiliation></author>
      <author><first>Surender</first><last>Kumar</last><affiliation>Flipkart Group</affiliation></author>
      <author><first>Soumen</first><last>Chakrabarti</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>23825-23839</pages>
      <abstract>Pre-trained language models (PLMs) fragment numerals and units that express quantities in arbitrary ways, depending on their subword vocabulary. Consequently, they are unable to contextualize the fragment embeddings well enough to be proficient with dense retrieval in domains like e-commerce and finance. Arithmetic inequality constraints (“laptop under 2 lb”) offer additional challenges. In response, we propose DeepQuant, a dense retrieval system built around a dense multi-vector index, but carefully engineered to elicit and exploit quantities and associated comparison intents. A novel component of our relevance score compares two quantities with compatible units, conditioned on a proposed comparison operator. The uncertain extractions of numerals, units and comparators are marginalized in a suitable manner. On two public and one proprietary e-commerce benchmark, DeepQuant is both faster and more accurate than popular PLMs. It also beats several competitive sparse and dense retrieval systems that do not take special cognizance of quantities.</abstract>
      <url hash="5c4ce253">2025.findings-acl.1220</url>
      <bibkey>agrawal-etal-2025-dense</bibkey>
    </paper>
    <paper id="1221">
      <title>Reflection on Knowledge Graph for Large Language Models Reasoning</title>
      <author><first>Yigeng</first><last>Zhou</last></author>
      <author><first>Wu</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yifan</first><last>Lu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Fangming</first><last>Liu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China</affiliation></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Daojing</first><last>He</last></author>
      <author><first>Honghai</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>23840-23857</pages>
      <abstract>Recent research shows that supplementing Large Language Models (LLMs) with knowledge graphs can enhance their performance. However, existing methods often introduce noise in the retrieval and reasoning pipeline, hindering LLMs’ ability to effectively integrate external knowledge for complex multi-hop question answering. To address this, we propose RefKG, a novel framework designed to enhance the reasoning capabilities of LLMs through reflective engagement with knowledge graphs. RefKG autonomously conduct retrieval and reflection on knowledge graphs. It consists of three modules: Query Decoupling, LLM-Driven Knowledge Graph Exploration, and Inference with Knowledge Reconstruction. We also introduce a multi-task tuning strategy that not only integrates external knowledge into LLMs but also trains them to leverage this knowledge for answering questions. This significantly improves their performance on knowledge-intensive tasks. Experiments on fact verification and knowledge graph question answering demonstrate RefKG’s effectiveness.</abstract>
      <url hash="e70cb82b">2025.findings-acl.1221</url>
      <bibkey>zhou-etal-2025-reflection</bibkey>
    </paper>
    <paper id="1222">
      <title>Revisiting 3<fixed-case>D</fixed-case> <fixed-case>LLM</fixed-case> Benchmarks: Are We Really Testing 3<fixed-case>D</fixed-case> Capabilities?</title>
      <author><first>Jiahe</first><last>Jin</last></author>
      <author><first>Yanheng</first><last>He</last></author>
      <author><first>Mingyan</first><last>Yang</last></author>
      <pages>23858-23869</pages>
      <abstract>In this work, we identify the “2D-Cheating” problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs’ unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs.</abstract>
      <url hash="ae5bbd50">2025.findings-acl.1222</url>
      <bibkey>jin-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="1223">
      <title><fixed-case>DIESEL</fixed-case>: A Lightweight Inference-Time Safety Enhancement for Language Models</title>
      <author><first>Ben</first><last>Ganon</last></author>
      <author><first>Alon</first><last>Zolfi</last><affiliation>Ben Gurion University of the Negev</affiliation></author>
      <author><first>Omer</first><last>Hofman</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Inderjeet</first><last>Singh</last><affiliation>Fujitsu Research of Europe Limited</affiliation></author>
      <author><first>Hisashi</first><last>Kojima</last><affiliation>Fujitsu Research and Development Center Co. Ltm.</affiliation></author>
      <author><first>Yuval</first><last>Elovici</last><affiliation>Ben Gurion University of the Negev, Technion</affiliation></author>
      <author><first>Asaf</first><last>Shabtai</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>23870-23890</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive performance across a wide range of tasks, including open-ended dialogue, driving advancements in virtual assistants and other interactive systems. However, these models often generate outputs misaligned with human values, such as ethical norms and safety constraints, resulting in potentially harmful or inappropriate responses. While several techniques have been proposed to address this problem, they typically involve computationally intensive training procedures or introduce substantial inference-time latency. In this paper, we present DIESEL, a lightweight inference-guidance technique that can be seamlessly integrated into any autoregressive LLM to semantically filter undesirable content during generation. DIESEL guides generation by reranking token candidates according to their semantic similarity to predefined negative concepts in the latent space. It can serve either as a standalone safeguard or as an auxiliary defense layer, enhancing response safety without requiring model fine-tuning or additional data. We demonstrate DIESEL’s effectiveness on state-of-the-art conversational models, including in adversarial jailbreak scenarios. Furthermore, we show that DIESEL generalizes beyond safety applications, enabling flexible and domain-specific response filtering.</abstract>
      <url hash="ff3f57fc">2025.findings-acl.1223</url>
      <bibkey>ganon-etal-2025-diesel</bibkey>
    </paper>
    <paper id="1224">
      <title>Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</title>
      <author><first>Jiawei</first><last>Gu</last></author>
      <author><first>Ziting</first><last>Xian</last></author>
      <author><first>Yuanzhen</first><last>Xie</last><affiliation>Tencent</affiliation></author>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Enjie</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Ruichao</first><last>Zhong</last><affiliation>Tencent</affiliation></author>
      <author><first>Mochi</first><last>Gao</last><affiliation>Tencent</affiliation></author>
      <author><first>Yunzhi</first><last>Tan</last><affiliation>Tencent</affiliation></author>
      <author><first>Bo</first><last>Hu</last></author>
      <author><first>Zang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <pages>23891-23910</pages>
      <abstract>Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9×, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.</abstract>
      <url hash="c2d78fa0">2025.findings-acl.1224</url>
      <bibkey>gu-etal-2025-toward</bibkey>
    </paper>
    <paper id="1225">
      <title>Structured Pruning for Diverse Best-of-<tex-math>N</tex-math> Reasoning Optimization</title>
      <author><first>Hieu Trung</first><last>Nguyen</last><affiliation>Vinai Research</affiliation></author>
      <author><first>Bao</first><last>Nguyen</last></author>
      <author><first>Viet Anh</first><last>Nguyen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>23911-23922</pages>
      <abstract>Model pruning in transformer-based language models, traditionally seen as a means of computational savings, can enhance the model’s reasoning capabilities. In this work, we uncover the surprising phenomenon that the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, our approach identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments on the MATH dataset demonstrate that our method significantly outperforms traditional best-of-<tex-math>N</tex-math> and random head selection strategies on the MATH500 and GSM8K datasets.</abstract>
      <url hash="4c725940">2025.findings-acl.1225</url>
      <bibkey>nguyen-etal-2025-structured</bibkey>
    </paper>
    <paper id="1226">
      <title><fixed-case>P</fixed-case>od<fixed-case>A</fixed-case>gent: A Comprehensive Framework for Podcast Generation</title>
      <author><first>Yujia</first><last>Xiao</last><affiliation>The Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lei</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <author><first>Haohan</first><last>Guo</last></author>
      <author><first>Feng-Long</first><last>Xie</last></author>
      <author><first>Tan</first><last>Lee</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>23923-23937</pages>
      <abstract>Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model’s performance. Experimental results demonstrate PodAgent’s effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.</abstract>
      <url hash="e49f9581">2025.findings-acl.1226</url>
      <bibkey>xiao-etal-2025-podagent</bibkey>
    </paper>
    <paper id="1227">
      <title><fixed-case>STORM</fixed-case>-<fixed-case>BORN</fixed-case>: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework</title>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Zhenyi</first><last>Lu</last></author>
      <author><first>Xinyu</first><last>Hu</last></author>
      <author><first>Jerry</first><last>Zhang</last></author>
      <author><first>Dailin</first><last>Li</last></author>
      <author><first>Jiacheng</first><last>Cen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Huilin</first><last>Cao</last></author>
      <author><first>Haiteng</first><last>Wang</last></author>
      <author><first>Yuhan</first><last>Li</last></author>
      <author><first>Xie</first><last>Kun</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Dandan</first><last>Li</last></author>
      <author><first>Pei</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chengbo</first><last>Zhang</last></author>
      <author><first>Yuxiang</first><last>Ren</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xiaohong</first><last>Huang</last></author>
      <author><first>Yan</first><last>Ma</last></author>
      <pages>23938-23958</pages>
      <abstract>High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation.To address these, we introduce STORM-BORN, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues.To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians’ evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems.Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B).As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at https://github.com/lwhere/STORM-BORN.</abstract>
      <url hash="921706cb">2025.findings-acl.1227</url>
      <bibkey>liu-etal-2025-storm</bibkey>
    </paper>
    <paper id="1228">
      <title>i<fixed-case>MOVE</fixed-case> : Instance-Motion-Aware Video Understanding</title>
      <author><first>Jiaze</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yaya</first><last>Shi</last></author>
      <author><first>Zongyang</first><last>Ma</last></author>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Yandong.bai</first><last>Yandong.bai</last></author>
      <author><first>Huihui</first><last>Xiao</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Ruiwen</first><last>Kang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Tingting</first><last>Gao</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <pages>23959-23975</pages>
      <abstract>Enhancing the fine-grained instance spatiotemporal motion perception capabilities of Video Large Language Models is crucial for improving their temporal and general video understanding. However, current models struggle to perceive detailed and complex instance motions. To address these challenges, we have made improvements from both data and model perspectives. In terms of data, we have meticulously curated iMOVE-IT, the first large-scale instance-motion-aware video instruction-tuning dataset. This dataset is enriched with comprehensive instance motion annotations and spatiotemporal mutual-supervision tasks, providing extensive training for the model’s instance-motion-awareness. Building on this foundation, we introduce iMOVE, an instance-motion-aware video foundation model that utilizes Event-aware Spatiotemporal Efficient Modeling to retain informative instance spatiotemporal motion details while maintaining computational efficiency. It also incorporates Relative Spatiotemporal Position Tokens to ensure awareness of instance spatiotemporal positions. Evaluations indicate that iMOVE excels not only in video temporal understanding and general video understanding but also demonstrates significant advantages in long-term video understanding. We will release the data, code, and model weights after acceptance.</abstract>
      <url hash="e5b8409e">2025.findings-acl.1228</url>
      <bibkey>li-etal-2025-imove</bibkey>
    </paper>
    <paper id="1229">
      <title><fixed-case>S</fixed-case>cene<fixed-case>G</fixed-case>ram: Conceptualizing and Describing Tangrams in Scene Context</title>
      <author><first>Simeon</first><last>Junker</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <pages>23976-23992</pages>
      <abstract>Research on reference and naming suggests that humans can come up with very different ways of conceptualizing and referring to the same object, e.g. the same abstract tangram shape can be a “crab”, “sink” or “space ship”. Another common assumption in cognitive science is that scene context fundamentally shapes our visual perception of objects and conceptual expectations. This paper contributes SceneGram, a dataset of human references to tangram shapes placed in different scene contexts, allowing for systematic analyses of the effect of scene context on conceptualization. Based on this data, we analyze references to tangram shapes generated by multimodal LLMs, showing that these models do not account for the richness and variability of conceptualizations found in human references.</abstract>
      <url hash="be84cba9">2025.findings-acl.1229</url>
      <bibkey>junker-zarriess-2025-scenegram</bibkey>
    </paper>
    <paper id="1230">
      <title>Relevant or Random: Can <fixed-case>LLM</fixed-case>s Truly Perform Analogical Reasoning?</title>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wenhan</first><last>Xia</last></author>
      <author><first>Tan</first><last>Wang</last></author>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Bosheng</first><last>Ding</last></author>
      <author><first>Ruirui</first><last>Chen</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <pages>23993-24010</pages>
      <abstract>Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance on certain tasks, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two novel methods with improved performance and significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.</abstract>
      <url hash="994b59cc">2025.findings-acl.1230</url>
      <bibkey>qin-etal-2025-relevant</bibkey>
    </paper>
    <paper id="1231">
      <title><fixed-case>MERIT</fixed-case>: Multi-Agent Collaboration for Unsupervised Time Series Representation Learning</title>
      <author><first>Shu</first><last>Zhou</last></author>
      <author><first>Yunyang</first><last>Xuan</last></author>
      <author><first>Yuxuan</first><last>Ao</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <author><first>Tao</first><last>Fan</last><affiliation>Nanjing University of Finance and Economics</affiliation></author>
      <author><first>Hao</first><last>Wang</last></author>
      <pages>24011-24028</pages>
      <abstract>This paper studies the problem of unsupervised time series representation learning, which aims to map unlabeled time series data into a low-dimensional latent space for various downstream tasks. Previous works usually combine a range of augmentation strategies with contrastive learning to generate discriminative representations. However, these augmentation strategies could alter the original semantics of time series data, which could degrade the performance of representation learning. To solve this problem, this paper incorporates the large language model (LLM) agent to guide unsupervised time series representation learning and proposes a novel framework named Multi-Agent Collaboration for Time-series Representation Learning (MERIT). The core of our MERIT is to utilize three LLM agents to collaboratively generate positive views for time series data. In particular, we first design a retrieval agent to automatically identify the relevant time series data from a coarse candidate set. Then, these selected sequences are further utilized to enhance an augmentation agent which automatically selects reliable augmentation strategies from an augmentation strategy library. We also design a review agent to evaluate the quality of generated views and stop the generation process. These three agents are designed to work in a loop for effective time series representation learning. Extensive experiments on multiple time series datasets demonstrate the effectiveness of our MERIT in comparison with state-of-the-art baselines.</abstract>
      <url hash="f83f7507">2025.findings-acl.1231</url>
      <bibkey>zhou-etal-2025-merit</bibkey>
    </paper>
    <paper id="1232">
      <title><fixed-case>J</fixed-case>son<fixed-case>T</fixed-case>uning: Towards Generalizable, Robust, and Controllable Instruction Tuning</title>
      <author><first>Chang</first><last>Gao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Guizhen</first><last>Chen</last></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>24029-24055</pages>
      <abstract>Instruction tuning is vital for enhancing the performance of large language models (LLMs), but existing text-to-text methods, referred to as TextTuning, struggle with issues such as generalization, robustness, and controllability due to their lack of explicit task structures. We introduce JsonTuning, a structure-to-structure approach that uses JSON structures to represent tasks. This method improves generalization by clarifying task elements and their relations, boosts robustness by minimizing ambiguity, and enhances controllability by allowing precise control over outputs. We conduct an extensive comparative analysis between JsonTuning and TextTuning using various language models and benchmarks. Our findings reveal that JsonTuning consistently surpasses TextTuning in terms of performance, robustness, and controllability across different scenarios. By overcoming the limitations of TextTuning, JsonTuning demonstrates significant potential for developing more effective and reliable LLMs capable of handling diverse scenarios.</abstract>
      <url hash="3fd6ba8f">2025.findings-acl.1232</url>
      <bibkey>gao-etal-2025-jsontuning</bibkey>
    </paper>
    <paper id="1233">
      <title><fixed-case>R</fixed-case>edundancy<fixed-case>L</fixed-case>ens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only <fixed-case>MLLM</fixed-case>s</title>
      <author><first>Hongliang</first><last>Li</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last></author>
      <author><first>Wenhui</first><last>Liao</last></author>
      <author><first>Dezhi</first><last>Peng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Kai</first><last>Ding</last><affiliation>INTSIG Information</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>24056-24067</pages>
      <abstract>Current Multimodal Large Language Model (MLLM) architectures face a critical tradeoff between performance and efficiency: decoder-only architectures achieve higher performance but lower efficiency, while cross-attention-based architectures offer greater efficiency but lower performance. The key distinction lies in how visual tokens are processed. Decoder-only architectures apply self-attention and FFN operations on visual tokens, while cross-attention architectures skip these computations. To investigate whether redundancy exists in this computationally expensive process, we propose a training-free framework for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and Hollow Attention, which enable adjustable reductions in computations for visual tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these reductions. Extensive experiments demonstrate substantial, structured, and clustered redundancy unique to decoder-only MLLMs, offering valuable insights for future MLLM architecture design. Furthermore, by leveraging our reduction framework as a training-free inference acceleration approach, we achieve performance comparable to or better than state-of-the-art methods while remaining compatible with them. Code is available at https://github.com/L-Hugh/RedundancyLens.</abstract>
      <url hash="83ca4783">2025.findings-acl.1233</url>
      <bibkey>li-etal-2025-redundancylens</bibkey>
    </paper>
    <paper id="1234">
      <title>Memory-augmented Query Reconstruction for <fixed-case>LLM</fixed-case>-based Knowledge Graph Reasoning</title>
      <author><first>Mufan</first><last>Xu</last></author>
      <author><first>Gewen</first><last>Liang</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Xun</first><last>Zhou</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>24068-24084</pages>
      <abstract>Large language models (LLMs) have achieved remarkable performance on knowledge graph question answering (KGQA) tasks by planning and interacting with knowledge graphs. However, existing methods often confuse tool utilization with knowledge reasoning, harming readability of model outputs and giving rise to hallucinatory tool invocations, which hinder the advancement of KGQA. To address this issue, we propose Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation tasks using LLM-built query memory. By establishing a memory module with explicit descriptions of query statements, the proposed MemQ facilitates the KGQA process with natural language reasoning and memory-augmented query reconstruction. Meanwhile, we design an effective and readable reasoning to enhance the LLM’s reasoning capability in KGQA. Experimental results that MemQ achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ.</abstract>
      <url hash="c366be5b">2025.findings-acl.1234</url>
      <bibkey>xu-etal-2025-memory</bibkey>
    </paper>
    <paper id="1235">
      <title><fixed-case>K</fixed-case>a<fixed-case>FT</fixed-case>: Knowledge-aware Fine-tuning for Boosting <fixed-case>LLM</fixed-case>s’ Domain-specific Question-Answering Performance</title>
      <author><first>Qihuang</first><last>Zhong</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Xiantao</first><last>Cai</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Juhua</first><last>Liu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>24085-24100</pages>
      <abstract>Supervised fine-tuning (SFT) is a common approach to improve the domain-specific question-answering (QA) performance of large language models (LLMs). However, recent literature reveals that due to the conflicts between LLMs’ internal knowledge and the context knowledge of training data, vanilla SFT using the full QA training set is usually suboptimal. In this paper, we first design a query diversification strategy for robust conflict detection and then conduct a series of experiments to analyze the impact of knowledge conflict. We find that 1) training samples with varied conflicts contribute differently, where SFT on the data with large conflicts leads to catastrophic performance drops; 2) compared to directly filtering out the conflict data, appropriately applying the conflict data would be more beneficial. Motivated by this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely KaFT) approach to effectively boost LLMs’ performance. The core of KaFT is to adapt the training weight by assigning different rewards for different training samples according to conflict level. Extensive experiments show that KaFT brings consistent and significant improvements (up to +5.73% average scores) across four LLMs. More analyses prove that KaFT effectively improves the model generalization and alleviates the hallucination.</abstract>
      <url hash="22b1850e">2025.findings-acl.1235</url>
      <bibkey>zhong-etal-2025-kaft</bibkey>
    </paper>
    <paper id="1236">
      <title>Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?</title>
      <author><first>Simeon</first><last>Junker</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Manar</first><last>Ali</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Larissa</first><last>Koch</last></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Hendrik</first><last>Buschmeier</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>24101-24109</pages>
      <abstract>We investigate the linguistic abilities of multimodal large language models in reference resolution tasks featuring simple yet abstract visual stimuli, such as color patches and color grids. Although the task may not seem challenging for today’s language models, being straightforward for human dyads, we consider it to be a highly relevant probe of the pragmatic capabilities of MLLMs. Our results and analyses indeed suggest that basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges for state-of-the-art MLLMs.</abstract>
      <url hash="f918a5b0">2025.findings-acl.1236</url>
      <bibkey>junker-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="1237">
      <title>Removing Prompt-template Bias in Reinforcement Learning from Human Feedback</title>
      <author><first>Chaojie</first><last>Wang</last></author>
      <author><first>Haonan</first><last>Shi</last></author>
      <author><first>Long</first><last>Tian</last></author>
      <author><first>Bo</first><last>An</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Shuicheng</first><last>Yan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>24110-24122</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) has become an essential technique for enhancing pre-trained large language models (LLMs) to generate responses that align with human preferences and societal values. Although RLHF has shown promise, the training of reward models (RMs) still faces the challenge of <i>reward hacking</i>, motivating recent works to prevent RMs from finding shortcuts that bypass the intended optimization objectives by identifying simplistic patterns such as response length. Besides the issue of <i>length bias</i>, our work firstly reveals that <i>prompt-template bias</i> learned by RMs can also cause <i>reward hacking</i> when dealing with some marginal samples, resulting in LLMs preferring to generate responses in a specific format after RLHF fine-tuning, regardless of the format requested in the prompt. To this end, we propose a low-cost but effective method, namely Prompt Bias Calibration (PBC), to estimate the <i>prompt-template bias</i> term during reward modeling, which can be utilized to calibrate reward scores in the following RL fine-tuning process. Then, we show that our PBC method can be flexibly combined with existing algorithms of removing <i>length bias</i>, leading to a further improvement in the aspect of enhancing the quality of generated responses.</abstract>
      <url hash="d7c35e1a">2025.findings-acl.1237</url>
      <bibkey>wang-etal-2025-removing</bibkey>
    </paper>
    <paper id="1238">
      <title>Latent Distribution Decouple for Uncertain-Aware Multimodal Multi-label Emotion Recognition</title>
      <author><first>Jingwang</first><last>Huang</last></author>
      <author><first>Jiang</first><last>Zhong</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Qin</first><last>Lei</last><affiliation>Chongqing University</affiliation></author>
      <author><first>Gaojinpeng</first><last>Gaojinpeng</last></author>
      <author><first>Ymyang</first><last>Ymyang</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>PeiguangLi</first><last>PeiguangLi</last></author>
      <author><first>Kaiwen</first><last>Wei</last><affiliation>Chongqing University</affiliation></author>
      <pages>24123-24138</pages>
      <abstract>Multimodal multi-label emotion recognition (MMER) aims to identify the concurrent presence of multiple emotions in multimodal data. Existing studies primarily focus on improving fusion strategies and modeling modality-to-label dependencies. However, they often overlook the impact of <b>aleatoric uncertainty</b>, which is the inherent noise in the multimodal data and hinders the effectiveness of modality fusion by introducing ambiguity into feature representations.To address this issue and effectively model aleatoric uncertainty, this paper proposes Latent emotional Distribution Decomposition with Uncertainty perception (LDDU) framework from a novel perspective of latent emotional space probabilistic modeling. Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal method that accounts for the dispersed distribution of uncertainty and integrates distribution information. Experimental results show that LDDU achieves state-of-the-art performance on the CMU-MOSEI and M<tex-math>^3</tex-math>ED datasets, highlighting the importance of uncertainty modeling in MMER. Code is available at https://github.com/201983290498/lddu_mmer.git.</abstract>
      <url hash="cbe5ad9e">2025.findings-acl.1238</url>
      <bibkey>huang-etal-2025-latent</bibkey>
    </paper>
    <paper id="1239">
      <title>Are <fixed-case>LLM</fixed-case>s Rational Investors? A Study on the Financial Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Yuchen</first><last>Ni</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Yu</first><last>He</last></author>
      <author><first>Gan</first><last>Yunhui</last></author>
      <author><first>Xiang</first><last>Liu</last><affiliation>New York University</affiliation></author>
      <author><first>Zhang</first><last>Jian</last></author>
      <author><first>Sen</first><last>Liu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Guangnan</first><last>Ye</last><affiliation>Fudan University</affiliation></author>
      <author><first>Hongfeng</first><last>Chai</last><affiliation>Fudan University</affiliation></author>
      <pages>24139-24173</pages>
      <abstract>Large language models (LLMs) excel in natural language generation but also exhibit biases, particularly in gender, race, and religion, which can be amplified with widespread use. However, research on biases in specific domains, such as finance, remains limited. To address this gap, we conducted a comprehensive evaluation of 23 leading LLMs and found varying degrees of financial bias, including more pronounced biases in financial-specific LLMs (FinLLMs). In response, we propose the Financial Bias Indicators (FBI) framework, which includes components like the Bias Unveiler, Bias Detective, Bias Tracker, and Bias Antidote, designed to identify, detect, analyze, and mitigate financial biases. Our analysis explores the root causes of these biases and introduces a debiasing method based on financial causal knowledge, alongside three other debiasing techniques. For the most biased model, we successfully reduced bias by 68% according to key metrics. This study advances our understanding of LLM biases in finance and highlights the need for greater scrutiny in their application within this critical domain.</abstract>
      <url hash="7029c590">2025.findings-acl.1239</url>
      <bibkey>zhou-etal-2025-llms</bibkey>
    </paper>
    <paper id="1240">
      <title>Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era</title>
      <author><first>Dan</first><last>Oneata</last><affiliation>University Politehnica of Bucharest</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>Copenhagen University and University of Copenhagen</affiliation></author>
      <author><first>Stella</first><last>Frank</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>24174-24191</pages>
      <abstract>Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as “encyclopedic” or “function”. These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.</abstract>
      <url hash="cd16e9df">2025.findings-acl.1240</url>
      <bibkey>oneata-etal-2025-seeing</bibkey>
    </paper>
    <paper id="1241">
      <title>Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models</title>
      <author><first>Sajjad</first><last>Ghiasvand</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Yifan</first><last>Yang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Zhiyu</first><last>Xue</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Mahnoosh</first><last>Alizadeh</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Ramtin</first><last>Pedarsani</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <pages>24192-24207</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) methods typically assume that Large Language Models (LLMs) are trained on data from a single device or client. However, real-world scenarios often require fine-tuning these models on private data distributed across multiple devices. Federated Learning (FL) offers an appealing solution by preserving user privacy, as sensitive data remains on local devices during training. Nonetheless, integrating PEFT methods into FL introduces two main challenges: communication overhead and data heterogeneity. In this paper, we introduce FedTT and FedTT+, methods for adapting LLMs by integrating tensorized adapters into client-side models’ encoder/decoder blocks. FedTT is versatile and can be applied to both cross-silo FL and large-scale cross-device FL. FedTT+, an extension of FedTT tailored for cross-silo FL, enhances robustness against data heterogeneity by adaptively freezing portions of tensor factors, further reducing the number of trainable parameters. Experiments on BERT and LLaMA models demonstrate that our proposed methods successfully address data heterogeneity challenges and perform on par or even better than existing federated PEFT approaches while achieving up to <tex-math>10\times</tex-math> reduction in communication cost.</abstract>
      <url hash="7fdfb8db">2025.findings-acl.1241</url>
      <bibkey>ghiasvand-etal-2025-communication</bibkey>
    </paper>
    <paper id="1242">
      <title>A rebuttal of two common deflationary stances against <fixed-case>LLM</fixed-case> cognition</title>
      <author><first>Zak</first><last>Hussain</last></author>
      <author><first>Rui</first><last>Mata</last><affiliation>University of Basel</affiliation></author>
      <author><first>Dirk U.</first><last>Wulff</last><affiliation>Max-Planck Institute</affiliation></author>
      <pages>24208-24213</pages>
      <abstract>Large language models (LLMs) are arguably the most predictive models of human cognition available. Despite their impressive human-alignment, LLMs are often labeled as "*just* next-token predictors” that purportedly fall short of genuine cognition. We argue that these deflationary claims need further justification. Drawing on prominent cognitive and artificial intelligence research, we critically evaluate two forms of “Justaism” that dismiss LLM cognition by labeling LLMs as “just” simplistic entities without specifying or substantiating the critical capacities these models supposedly lack. Our analysis highlights the need for a more measured discussion of LLM cognition, to better inform future research and the development of artificial intelligence.</abstract>
      <url hash="f0e3db17">2025.findings-acl.1242</url>
      <bibkey>hussain-etal-2025-rebuttal</bibkey>
    </paper>
    <paper id="1243">
      <title><fixed-case>COVER</fixed-case>: Context-Driven Over-Refusal Verification in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Giovanni</first><last>Sullutrone</last></author>
      <author><first>Riccardo A.</first><last>Vigliermo</last></author>
      <author><first>Sonia</first><last>Bergamaschi</last><affiliation>University of Modena and Reggio Emilia</affiliation></author>
      <author><first>Luca</first><last>Sala</last></author>
      <pages>24214-24229</pages>
      <abstract>We introduce the concept of context-driven over-refusal, an abstention arising when model’s safety guardrails are triggered by the grounding knowledge provided alongside the user’s request. Distinct from question-driven over-refusal, this occurs in both retrieval-augmented generation (RAG) and natural language processing (NLP) task completion (e.g. summarization, translation) where external content can unexpectedly trigger refusals. In this work, we present a novel two-stage evaluation framework named COVER, designed to quantify and analyze this behavior. Through a comprehensive empirical study on two public corpora, we show that over-refusal rates strongly depend on the task, system prompts, model family, and the number of retrieved documents. We observe that tasks such as translation and summarization yield disproportionately high over-refusal rates, while question-answering remains relatively robust, especially in newer models. Moreover, increasing the number of contextual documents tends to reduce refusals, yet broadens the pool of prompts at risk of encountering at least one “unsafe” text. Interestingly, strict system prompts do not necessarily lead to higher over-refusal rates, suggesting that in the absence of explicit directives, some models may default to a more cautious behavior. These findings highlight the need for fine-grained alignment and benchmarking strategies sensitive to both user intent and contextual nuances, offering a roadmap for future research in model training and evaluation.</abstract>
      <url hash="15d9750a">2025.findings-acl.1243</url>
      <bibkey>sullutrone-etal-2025-cover</bibkey>
    </paper>
    <paper id="1244">
      <title><fixed-case>MOSAIC</fixed-case>: Multiple Observers Spotting <fixed-case>AI</fixed-case> Content</title>
      <author><first>Matthieu</first><last>Dubois</last><affiliation>Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <author><first>Pablo</first><last>Piantanida</last><affiliation>Université Paris-Saclay, CNRS</affiliation></author>
      <pages>24230-24247</pages>
      <abstract>The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities, has made it easier for all to produce harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a binary classification problem. Early approaches evaluate an input document with a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. More recent systems instead consider two LLMs and compare their probability distributions over the document to further discriminate when perplexity alone cannot. However, using a fixed pair of models can induce brittleness in performance. We extend these approaches to the ensembling of several LLMs and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, using a variety of generator LLMs, suggest that this approach effectively harnesses each model’s capabilities, leading to strong detection performance on a variety of domains.</abstract>
      <url hash="a93900dd">2025.findings-acl.1244</url>
      <bibkey>dubois-etal-2025-mosaic-multiple</bibkey>
    </paper>
    <paper id="1245">
      <title><fixed-case>GUIDEX</fixed-case>: Guided Synthetic Data Generation for Zero-Shot Information Extraction</title>
      <author><first>Neil De La</first><last>Fuente</last><affiliation>Technical University of Munich and Universidad del País Vasco</affiliation></author>
      <author><first>Oscar</first><last>Sainz</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Iker</first><last>García-Ferrero</last></author>
      <author><first>Eneko</first><last>Agirre</last><affiliation>University of the Basque Country (UPV/EHU)</affiliation></author>
      <pages>24248-24262</pages>
      <abstract>Information Extraction (IE) systems are traditionally domain-specific, requiring costlyadaptation that involves expert schema design,data annotation, and model training. WhileLarge Language Models have shown promisein zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX,a novel method that automatically definesdomain-specific schemas, infers guidelines,and generates synthetically labeled instances,allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEXsets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks.Models trained with GUIDEX gain up to 7 F1points over previous methods without humanlabeled data, and nearly 2 F1 points higherwhen combined with it. Models trained onGUIDEX demonstrate enhanced comprehension of complex, domain-specific annotationschemas. Code, models, and synthetic datasetsare available at neilus03.github.io/guidex.com</abstract>
      <url hash="45d5137a">2025.findings-acl.1245</url>
      <bibkey>fuente-etal-2025-guidex</bibkey>
    </paper>
    <paper id="1246">
      <title>Missing the Margins: A Systematic Literature Review on the Demographic Representativeness of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Indira</first><last>Sen</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Marlene</first><last>Lutz</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Elisa</first><last>Rogers</last></author>
      <author><first>David</first><last>Garcia</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Markus</first><last>Strohmaier</last><affiliation>Universität Mannheim and GESIS – Leibniz Institute for the Social Sciences</affiliation></author>
      <pages>24263-24289</pages>
      <abstract>Many applications of Large Language Models (LLMs) require them to either simulate people or offer personalized functionality, making the demographic representativeness of LLMs crucial for equitable utility. At the same time, we know little about the extent to which these models actually reflect the demographic attributes and behaviors of certain groups or populations, with conflicting findings in empirical research. To shed light on this debate, we review 211 papers on the demographic representativeness of LLMs. We find that while 29% of the studies report positive conclusions on the representativeness of LLMs, 30% of these do not evaluate LLMs across multiple demographic categories or within demographic subcategories. Another 35% and 47% of the papers concluding positively fail to specify these subcategories altogether for gender and race, respectively. Of the articles that do report subcategories, fewer than half include marginalized groups in their study. Finally, more than a third of the papers do not define the target population to whom their findings apply; of those that do define it either implicitly or explicitly, a large majority study only the U.S. Taken together, our findings suggest an inflated perception of LLM representativeness in the broader community. We recommend more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible use of LLMs for social applications.</abstract>
      <url hash="217febac">2025.findings-acl.1246</url>
      <bibkey>sen-etal-2025-missing</bibkey>
    </paper>
    <paper id="1247">
      <title><fixed-case>L</fixed-case>lama<fixed-case>V</fixed-case>-o1: Rethinking Step-by-step Visual Reasoning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Omkar</first><last>Thawakar</last></author>
      <author><first>Dinura</first><last>Dissanayake</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ketan Pravin</first><last>More</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ritesh</first><last>Thawkar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ahmed</first><last>Heakl</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Noor</first><last>Ahsan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yuhao</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Northwestern Polytechnical University, Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Ilmuz Zaman Mohammed</first><last>Zumri</last></author>
      <author><first>Jean</first><last>Lahoud</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rao Muhammad</first><last>Anwer</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Hisham</first><last>Cholakkal</last><affiliation>MBZUAI</affiliation></author>
      <author><first>Ivan</first><last>Laptev</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mubarak</first><last>Shah</last><affiliation>Amazon and University of Central Florida</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <pages>24290-24315</pages>
      <abstract>Step-by-step reasoning is crucial for solving complex visual tasks, yet existing approaches lack a comprehensive framework for evaluating this capability and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing multi-step visual reasoning in large multimodal models (LMMs) through three key contributions. First, we introduce a Visual Reasoning Chain Benchmark, the most comprehensive benchmark for multi-step visual reasoning, covering eight diverse categories and over 4k reasoning steps. This enables rigorous evaluation of LMMs’ ability to reason accurately and interpretably across multiple steps. Second, we propose a fine-grained reasoning metric that evaluates correctness and logical coherence at each step, providing deeper insights beyond traditional accuracy metrics. Third, we introduce LlamaV-o1, a state-of-the-art multimodal reasoning model trained using a multi-step curriculum learning approach. LlamaV-o1 is optimized for structured, step-by-step reasoning and significantly outperforms existing open-source models. It surpasses Llava-CoT with a 3.8% absolute gain across six benchmarks, achieving an average score of 67.3 while being 5x faster during inference scaling. Our benchmark, model, and code is available at https://github.com/mbzuai-oryx/LlamaV-o1.</abstract>
      <url hash="41f1561c">2025.findings-acl.1247</url>
      <bibkey>thawakar-etal-2025-llamav</bibkey>
    </paper>
    <paper id="1248">
      <title>Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?</title>
      <author><first>Yingjin</first><last>Song</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Yupei</first><last>Du</last></author>
      <author><first>Denis</first><last>Paperno</last><affiliation>Utrecht University and CNRS</affiliation></author>
      <author><first>Albert</first><last>Gatt</last><affiliation>Utrecht University</affiliation></author>
      <pages>24316-24342</pages>
      <abstract>This paper introduces the TempVS benchmark, which focuses on temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) in image sequences. TempVS consists of three main tests (i.e., event relation inference, sentence ordering and image ordering), each accompanied with a basic grounding test. TempVS requires MLLMs to rely on both visual and linguistic modalities to understand the temporal order of events. We evaluate 38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS, with a substantial performance gap compared to human capabilities. We also provide fine-grained insights that suggest promising directions for future research. Our TempVS benchmark data and code are available at https://github.com/yjsong22/TempVS.</abstract>
      <url hash="d68de9e6">2025.findings-acl.1248</url>
      <bibkey>song-etal-2025-burn</bibkey>
    </paper>
    <paper id="1249">
      <title>Full-Step-<fixed-case>DPO</fixed-case>: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning</title>
      <author><first>Huimin</first><last>Xu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xin</first><last>Mao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Feng-Lin</first><last>Li</last><affiliation>Shopee</affiliation></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wang</first><last>Chen</last></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>sea group</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>24343-24356</pages>
      <abstract>Direct Preference Optimization (DPO) often struggles with long-chain mathematical reasoning. Existing approaches, such as Step-DPO, typically improve this by focusing on the first erroneous step in the reasoning chain. However, they overlook all other steps and rely heavily on humans or GPT-4 to identify erroneous steps. To address these issues, we propose Full-Step-DPO, a novel DPO framework tailored for mathematical reasoning. Instead of optimizing only the first erroneous step, it leverages step-wise rewards from the entire reasoning chain. This is achieved by training a self-supervised process reward model, which automatically scores each step, providing rewards while avoiding reliance on external signals. Furthermore, we introduce a novel step-wise DPO loss, which dynamically updates gradients based on these step-wise rewards. This endows stronger reasoning capabilities to language models. Extensive evaluations on both in-domain and out-of-domain mathematical reasoning benchmarks across various base language models, demonstrate that Full-Step-DPO achieves superior performance compared to state-of-the-art baselines.</abstract>
      <url hash="d1a412a1">2025.findings-acl.1249</url>
      <bibkey>xu-etal-2025-full</bibkey>
    </paper>
    <paper id="1250">
      <title>Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with <fixed-case>LLM</fixed-case>-based Manipulation Checks</title>
      <author><first>Yanran</first><last>Chen</last></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>24357-24381</pages>
      <abstract>Emotions have been shown to play a role in argument convincingness, yet this aspect is underexplored in the natural language processing (NLP) community. Unlike prior studies that use static analyses, focus on a single text domain or language, or treat emotion as just one of many factors, we introduce a dynamic framework inspired by manipulation checks commonly used in psychology and social science; leveraging LLM-based manipulation checks, this framework examines the extent to which perceived emotional intensity influences perceived convincingness. Through human evaluation of arguments across different languages, text domains, and topics, we find that in over half of cases, human judgments of convincingness remain unchanged despite variations in perceived emotional intensity; when emotions do have an impact, they more often enhance rather than weaken convincingness.We further analyze whether 11 LLMs behave like humans in the same scenario, finding that while LLMs generally mirror human patterns,they struggle to capture nuanced emotional effects in individual judgments.</abstract>
      <url hash="71dd018c">2025.findings-acl.1250</url>
      <bibkey>chen-eger-2025-emotions</bibkey>
    </paper>
    <paper id="1251">
      <title><fixed-case>SCOPE</fixed-case>: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation</title>
      <author><first>Huimin</first><last>Xu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xin</first><last>Mao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Feng-Lin</first><last>Li</last><affiliation>Shopee</affiliation></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wang</first><last>Chen</last></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>sea group</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>24382-24394</pages>
      <abstract>Process Reward Models (PRMs) have demonstrated promising results in mathematical reasoning, but existing process annotation approaches, whether through human annotations or Monte Carlo simulations, remain computationally expensive. In this paper, we introduce Step COmpression for Process Estimation (SCOPE), a novel compression-based approach that significantly reduces annotation costs. We first translate natural language reasoning steps into code and normalize them through Abstract Syntax Tree, then merge equivalent steps to construct a prefix tree. Unlike simulation-based methods that waste numerous samples on estimation, SCOPE leverages a compression-based prefix tree where each root-to-leaf path serves as a training sample, reducing the complexity from <tex-math>O(NMK)</tex-math> to O(N) We construct a large-scale dataset containing 509K samples with only 5% of the computational resources required by previous methods. Empirical results demonstrate that PRMs trained on our dataset consistently outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench.</abstract>
      <url hash="03de3634">2025.findings-acl.1251</url>
      <bibkey>xu-etal-2025-scope</bibkey>
    </paper>
    <paper id="1252">
      <title>Compositional Syntactico-<fixed-case>S</fixed-case>em<fixed-case>B</fixed-case>anking for <fixed-case>E</fixed-case>nglish as a Second or Foreign Language</title>
      <author><first>Wenxi</first><last>Li</last></author>
      <author><first>Xihao</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Weiwei</first><last>Sun</last><affiliation>University of Cambridge</affiliation></author>
      <pages>24395-24406</pages>
      <abstract>Despite the widespread use of English as a Second or Foreign Language (ESFL), developing syntactico-semantic representations for it is limited — the irregularities in ESFL complicate systematic composition and subsequently the derivation of its semantics.This paper draws on constructivism and proposes a novel Synchronous Hyperedge Replacement Grammar (SHRG)-based constructivist approach to address the challenges. By using constructions as fundamental units, this approach not only accommodates both the idiosyncrasies and the compositional nature of ESFL, but also bridges the gap between literal cues and intended meaning.The feasibility of this constructivist approach is demonstrated using real ESFL data, resulting in a gold-standard, medium-sized syntactico-semantic bank that covers a wide range of ESFL phenomena.</abstract>
      <url hash="4db22a47">2025.findings-acl.1252</url>
      <bibkey>li-etal-2025-compositional</bibkey>
    </paper>
    <paper id="1253">
      <title>Semantics-aware prompting for translating <fixed-case>NO</fixed-case>tices To <fixed-case>A</fixed-case>ir<fixed-case>M</fixed-case>en</title>
      <author><first>Minal Nitin</first><last>Dani</last><affiliation>Honeywell and Indian Institute of Technology, Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Aishwarya</first><last>Maheswaran</last></author>
      <author><first>Maunendra Sankar</first><last>Desarkar</last><affiliation>Indian Institute of Technology, Hyderabad,</affiliation></author>
      <pages>24407-24417</pages>
      <abstract>A NOTAM or NOtice To AirMen is a crucial notice for different aviation stakeholders, particularly flight crews. It delivers essential notifications about abnormal conditions of Aviation System components such as changes to facilities, hazards, service, procedure that are not known far enough in advance to be publicized through other means. NOTAM messages are short, contain acronyms, and look cryptic in most of the cases. Writing and understanding these messages put heavy cognitive load on its end users. In this work, we take up the task of translating NOTAMs into English natural language using LLMs. Since NOTAMs do not adhere to English grammar rules and have their own decoding rules, large language models (LLMs) cannot translate them without effective prompting. In this paper, we develop a framework to come up with effective prompts to achieve the translations. Our approach uses context-aware semantic prompting techniques, paired with domain-specific rules, to improve the accuracy and clarity of translations. The framework is evaluated using comprehensive experiments (6 LLMs of varying sizes, and with 5 different prompting setups for each) and eight evaluation metrics measuring different aspects of the translation. The results demonstrate that our methodology can produce clear translations that accurately convey the information contained in NOTAMs.</abstract>
      <url hash="692d8b10">2025.findings-acl.1253</url>
      <bibkey>dani-etal-2025-semantics</bibkey>
    </paper>
    <paper id="1254">
      <title>Stereotype or Personalization? User Identity Biases Chatbot Recommendations</title>
      <author><first>Anjali</first><last>Kantharuban</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Jeremiah</first><last>Milbauer</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>24418-24436</pages>
      <abstract>While personalized recommendations are often desired by users, it can be difficult in practice to distinguish cases of bias from cases of personalization: we find that models generate racially stereotypical recommendations regardless of whether the user revealed their identity intentionally through explicit indications or unintentionally through implicit cues. We demonstrate that when people use large language models (LLMs) to generate recommendations, the LLMs produce responses that reflect both what the user wants and who the user is. We argue that chatbots ought to transparently indicate when recommendations are influenced by a user’s revealed identity characteristics, but observe that they currently fail to do so. Our experiments show that even though a user’s revealed identity significantly influences model recommendations (<tex-math>p &lt; 0.001</tex-math>), model responses obfuscate this fact in response to user queries. This bias and lack of transparency occurs consistently across multiple popular consumer LLMs and for four American racial groups.</abstract>
      <url hash="04f272a3">2025.findings-acl.1254</url>
      <bibkey>kantharuban-etal-2025-stereotype</bibkey>
    </paper>
    <paper id="1255">
      <title>Automated main concept generation for narrative discourse assessment in aphasia</title>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Marisa</first><last>Hudspeth</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Polly</first><last>Stokes</last></author>
      <author><first>Jacquie</first><last>Kurland</last></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>24437-24451</pages>
      <abstract>We present an interesting application of narrative understanding in the clinical assessment of aphasia, where story retelling tasks are used to evaluate a patient’s communication abilities. This clinical setting provides a framework to help operationalize narrative discourse analysis and an application-focused evaluation method for narrative understanding systems. In particular, we highlight the use of main concepts (MCs)—a list of statements that capture a story’s gist—for aphasic discourse analysis. We then propose automatically generating MCs from novel stories, which experts can edit manually, thus enabling wider adaptation of current assessment tools. We further develop a prompt ensemble method using large language models (LLMs) to automatically generate MCs for a novel story. We evaluate our method on an existing narrative summarization dataset to establish its intrinsic validity. We further apply it to a set of stories that have been annotated with MCs through extensive analysis of retells from non-aphasic and aphasic participants (Kurland et al., 2021, 2025). Our results show that our proposed method can generate most of the gold-standard MCs for stories from this dataset. Finally, we release this dataset of stories with annotated MCs to spur more research in this area.</abstract>
      <url hash="71ea1be1">2025.findings-acl.1255</url>
      <bibkey>gupta-etal-2025-automated</bibkey>
    </paper>
    <paper id="1256">
      <title>Can <fixed-case>VLM</fixed-case>s Actually See and Read? A Survey on Modality Collapse in Vision-Language Models</title>
      <author><first>Mong Yuan</first><last>Sim</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Wei Emma</first><last>Zhang</last><affiliation>The University of Adelaide</affiliation></author>
      <author><first>Xiang</first><last>Dai</last><affiliation>CSIRO</affiliation></author>
      <author><first>Biaoyan</first><last>Fang</last></author>
      <pages>24452-24470</pages>
      <abstract>Vision-language models (VLMs) integrate textual and visual information, enabling the model to process visual inputs and leverage visual information to generate predictions. Such models are demanding for tasks such as visual question answering, image captioning, and visual grounding. However, some recent work found that VLMs often rely heavily on textual information, ignoring visual information, but are still able to achieve competitive performance in vision-language (VL) tasks. This survey reviews modality collapse analysis work to provide insights into the reason for this unintended behavior. It also reviews probing studies for fine-grained vision-language understanding, presenting current findings on information encoded in VL representations and highlighting potential directions for future research.</abstract>
      <url hash="e2c0ffda">2025.findings-acl.1256</url>
      <bibkey>sim-etal-2025-vlms</bibkey>
    </paper>
    <paper id="1257">
      <title>“You are Beautiful, Body Image Stereotypes are Ugly!” <fixed-case>BIS</fixed-case>tereo: A Benchmark to Measure Body Image Stereotypes in Language Models</title>
      <author><first>Narjis</first><last>Asad</last></author>
      <author><first>Nihar Ranjan</first><last>Sahoo</last></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Pvt Ltd</affiliation></author>
      <author><first>Swaprava</first><last>Nath</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>24471-24496</pages>
      <abstract>While a few high-quality bias benchmark datasets exist to address stereotypes in Language Models (LMs), a notable lack of focus remains on body image stereotypes. To bridge this gap, we propose <tex-math>\textbf{BIStereo}</tex-math>, a suite to uncover LMs’ biases towards people of certain physical appearance characteristics, namely, <tex-math>\textit{skin complexion, body shape, height, attire,}</tex-math> and a <tex-math>\textit{miscellaneous category}</tex-math> including <tex-math>\textit{hair texture, eye color, and more}</tex-math>. Our dataset comprises 40k sentence pairs designed to assess LMs’ biased preference for certain body types. We further include 60k premise-hypothesis pairs designed to comprehensively assess LMs’ preference for fair skin tone. Additionally, we curate 553 tuples consisting of a <tex-math>\textit{body image descriptor, gender, and a stereotypical attribute}</tex-math>, validated by a diverse pool of annotators for physical appearance stereotypes.We propose a metric, <tex-math>\textbf{TriSentBias}</tex-math>, that captures the biased preferences of LMs towards a certain body type over others. Using <tex-math>\textbf{BIStereo}</tex-math>, we assess the presence of body image biases in ten different language models, revealing significant biases in models Muril, XLMR, Llama3, and Gemma. We further evaluate the LMs through downstream NLI and Analogy tasks.Our NLI experiments highlight notable patterns in the LMs that align with the well-documented cognitive bias in humans known as <tex-math>\textbf{\textit{the Halo Effect}}</tex-math>.</abstract>
      <url hash="aafe823d">2025.findings-acl.1257</url>
      <bibkey>asad-etal-2025-beautiful</bibkey>
    </paper>
    <paper id="1258">
      <title>Retrieval Models Aren’t Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models</title>
      <author><first>Zhengliang</first><last>Shi</last></author>
      <author><first>Yuhan</first><last>Wang</last><affiliation>Shandong University</affiliation></author>
      <author><first>Lingyong</first><last>Yan</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <pages>24497-24524</pages>
      <abstract>Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.</abstract>
      <url hash="ae261dd5">2025.findings-acl.1258</url>
      <bibkey>shi-etal-2025-retrieval</bibkey>
    </paper>
    <paper id="1259">
      <title><fixed-case>F</fixed-case>ine<fixed-case>C</fixed-case>ite: A Novel Approach For Fine-Grained Citation Context Analysis</title>
      <author><first>Lasse M.</first><last>Jantsch</last></author>
      <author><first>Dong-Jae</first><last>Koh</last></author>
      <author><first>Seonghwan</first><last>Yoon</last><affiliation>Kyungpook National University</affiliation></author>
      <author><first>Jisu</first><last>Lee</last></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Young-Kyoon</first><last>Suh</last><affiliation>Kyungpook National University</affiliation></author>
      <pages>24525-24542</pages>
      <abstract>Citation context analysis (CCA) is a field of research studying the role and purpose of citation in scientific discourse. While most of the efforts in CCA have been focused on elaborate characterization schemata to assign function or intent labels to individual citations, the citation context as the basis for such a classification has received rather limited attention. This relative neglect, however, has led to the prevalence of vague definitions and restrictive assumptions, limiting the citation context in its expressiveness. It is a common practice, for example, to restrict the context to the citing sentence. While this simple context conceptualization might be sufficient to assign intent or function classes, it fails to cover the rich information of scientific discourse. To address this concern, we analyze the context conceptualizations of previous works and, to our knowledge, construct the first comprehensive context definition based on the semantic properties of the citing text. To evaluate this definition, we construct and publish the FineCite corpus containing 1,056 manually annotated citation contexts. Our experiments on established CCA benchmarks demonstrate the effectiveness of our fine-grained context definition, showing improvements of up to 25% compared to state-of-the-art approaches. We make our code and data publicly available at https://github.com/lab-paper-code/FineCite.</abstract>
      <url hash="b5152c37">2025.findings-acl.1259</url>
      <bibkey>jantsch-etal-2025-finecite</bibkey>
    </paper>
    <paper id="1260">
      <title>Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing</title>
      <author><first>Changyue</first><last>Wang</last></author>
      <author><first>Weihang</first><last>Su</last></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>24543-24562</pages>
      <abstract>Knowledge editing enables efficient updates to Large Language Models (LLMs) by modifying specific knowledge without full-model retraining. Among knowledge editing approaches, in-context editing (ICE) stands out for its ability to inject knowledge without modifying the model’s parameters. However, existing ICE approaches directly edit model context without isolating target knowledge from the reasoning path of model inference, resulting in unreliable and low-quality outputs, particularly in multi-hop tasks. To investigate this issue, we analyze the interaction between reasoning path planning and knowledge injection, showing that the reasoning ability of a LLM is usually coupled with its original knowledge, and directly replacing old knowledge with new one could simultaneously hurt the LLM’s performance in task reasoning. Based on these findings, we propose DecKER, a novel ICE framework that separates model reasoning from knowledge editing. Extensive experiments show that DecKER significantly improves multi-hop reasoning performance by mitigating knowledge conflicts and preserving reasoning integrity.</abstract>
      <url hash="204baa83">2025.findings-acl.1260</url>
      <bibkey>wang-etal-2025-decoupling</bibkey>
    </paper>
    <paper id="1261">
      <title>Entrospect: Information-Theoretic Self-Reflection Elicits Better Response Refinement of Small Language Models</title>
      <author><first>Tianqiang</first><last>Yan</last></author>
      <author><first>Ziqiao</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lin</first><last>Zhang</last><affiliation>Shenzhen Technology University</affiliation></author>
      <author><first>Zhenglong</first><last>Sun</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yuan</first><last>Gao</last></author>
      <pages>24563-24577</pages>
      <abstract>Self-reflection helps de-hallucinate Large Language Models (LLMs). However, the effectiveness of self-reflection remains insufficiently validated in the context of Small Language Models (SLMs), which exhibit limited semantic capacities. In particular, we demonstrate that the conventional self-reflection paradigm, such as Self-Refine, fails to deliver robust response refinement for models with parameter sizes of 10 billion or smaller, even when compared to generations elicited through Chain-of-Thought (CoT) prompting. To improve SLMs’ self-reflection, we redesign Self-Refine and introduce Entrospect (ENTROpy-aware IntroSPECTion), an information-theoretic framework based on prompt engineering.We evaluated Entrospect using accuracy and average time consumption metrics to comprehensively assess its precision and computational efficiency. Experiments conducted across four distinct SLMs and four baseline methods demonstrate that Entrospect achieves state-of-the-art performance on validation tasks. Notably, under identical model and data settings, Entrospect delivers a remarkable improvement of up to 36.2 in reasoning accuracy while enhancing computational efficiency by as much as 10 times compared to its predecessor, Self-Refine.</abstract>
      <url hash="703a82d6">2025.findings-acl.1261</url>
      <bibkey>yan-etal-2025-entrospect</bibkey>
    </paper>
    <paper id="1262">
      <title>Iterative Repair with Weak Verifiers for Few-shot Transfer in <fixed-case>KBQA</fixed-case> with Unanswerability</title>
      <author><first>Riya</first><last>Sawhney</last></author>
      <author><first>Samrat</first><last>Yadav</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Indrajit</first><last>Bhattacharya</last><affiliation>KnowDis AI</affiliation></author>
      <author><first>Mausam</first><last>Mausam</last><affiliation>Indian Institute of Technology Delhi</affiliation></author>
      <pages>24578-24596</pages>
      <abstract>Real-world applications of KBQA require models to detect different types of unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions. The state-of-the-art KBQA few-shot transfer model (FuSIC-KBQA) uses an iterative repair strategy that assumes that all questions are answerable. As a remedy, we present FUn-FuSIC – a novel solution for our task that extends FuSIC-KBQA with Feedback for Unanswerability (FUn), which is an iterative repair strategy for answerable as well as unanswerable questions. FUn uses feedback from a suite of strong and weak verifiers, and an adaptation of self-consistency for unanswerability for assessing answerability of questions. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM-based and supervised SoTA models on our task, while establishing a new SoTA performance for answerable few-shot transfer as well. We have made datasets and other resources publicly available at https://github.com/dair-iitd/funfusic/.</abstract>
      <url hash="6ad25f51">2025.findings-acl.1262</url>
      <bibkey>sawhney-etal-2025-iterative</bibkey>
    </paper>
    <paper id="1263">
      <title>Safeguarding <fixed-case>RAG</fixed-case> Pipelines with <fixed-case>GMTP</fixed-case>: A Gradient-based Masked Token Probability Method for Poisoned Document Detection</title>
      <author><first>San</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jonghwi</first><last>Kim</last></author>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>24597-24614</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk; attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever’s similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.</abstract>
      <url hash="02700f71">2025.findings-acl.1263</url>
      <bibkey>kim-etal-2025-safeguarding</bibkey>
    </paper>
    <paper id="1264">
      <title><fixed-case>E</fixed-case>n<fixed-case>ST</fixed-case>o<fixed-case>M</fixed-case>: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance</title>
      <author><first>Heejae</first><last>Suh</last></author>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Deokhyung</first><last>Kang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Taehee</first><last>Park</last></author>
      <author><first>Yejin</first><last>Min</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>24615-24631</pages>
      <abstract>Small large language models (sLLMs) offer the advantage of being lightweight and efficient, which makes them suitable for resource-constrained environments. However, sLLMs often struggle to maintain topic consistency in task-oriented dialogue systems, which is critical for scenarios such as service chatbots. Specifically, it is important to ensure that the model denies off-topic or malicious inputs and adheres to its intended functionality so as to prevent potential misuse and uphold reliability. Towards this, existing activation engineering approaches have been proposed to manipulate internal activations during inference. While these methods are effective in certain scenarios, our preliminary experiments reveal their limitations in ensuring topic adherence. Therefore, to address this, we propose a novel approach termed <b>En</b>tropy-scaled <b>S</b>teering vectors for <b>To</b>pic <b>M</b>aintenance (EnSToM). EnSToM dynamically adjusts the steering intensity based on input uncertainty, which allows the model to handle off-topic distractors effectively while preserving on-topic accuracy. Our experiments demonstrate that EnSToM achieves significant performance gain with a relatively small data size compared to fine-tuning approaches. By improving topic adherence without compromising efficiency, our approach provides a robust solution for enhancing sLLM-based dialogue systems.</abstract>
      <url hash="10d8ba8f">2025.findings-acl.1264</url>
      <bibkey>suh-etal-2025-enstom</bibkey>
    </paper>
    <paper id="1265">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>TEND</fixed-case>: A Multilingual Benchmark for Natural Language to <fixed-case>N</fixed-case>o<fixed-case>SQL</fixed-case> Query Translation</title>
      <author><first>Zhiqian</first><last>Qin</last></author>
      <author><first>Yuanfeng</first><last>Song</last></author>
      <author><first>Jinwei</first><last>Lu</last></author>
      <author><first>Yuanwei</first><last>Song</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Shuaimin</first><last>Li</last></author>
      <author><first>Chen Jason</first><last>Zhang</last></author>
      <pages>24632-24657</pages>
      <abstract>Natural language interfaces for NoSQL databases are increasingly vital in the big data era, enabling users to interact with complex, unstructured data without deep technical expertise. However, most recent advancements focus on English, leaving a gap for multilingual support. This paper introduces MultiTEND, the first and largest multilingual benchmark for natural language to NoSQL query generation, covering six languages: English, German, French, Russian, Japanese and Mandarin Chinese.Using MultiTEND, we analyze challenges in translating natural language to NoSQL queries across diverse linguistic structures, including lexical and syntactic differences. Experiments show that performance accuracy in both English and non-English settings remains relatively low, with a 4%-6% gap across scenarios like fine-tuned SLM, zero-shot LLM, and RAG for LLM.To address the aforementioned challenges, we introduce MultiLink, a novel framework that bridges the multilingual input to NoSQL query generation gap through a Parallel Linking Process. It breaks down the task into multiple steps, integrating parallel multilingual processing, Chain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to tackle lexical and structural challenges inherent in multilingual NoSQL generation. MultiLink shows enhancements in all metrics for every language against the top baseline, boosting execution accuracy by about 15% for English and averaging a 10% improvement for non-English languages.</abstract>
      <url hash="c7043f16">2025.findings-acl.1265</url>
      <bibkey>qin-etal-2025-multitend</bibkey>
    </paper>
    <paper id="1266">
      <title>Tool learning via Inference-time Scaling and Cycle Verifier</title>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Wenjin</first><last>Xie</last><affiliation>Beihang University</affiliation></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Wanfu</first><last>Wang</last></author>
      <author><first>Yibin</first><last>Chen</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>24658-24671</pages>
      <abstract>In inference-time scaling, Chain-of-Thought (CoT) plays a crucial role in enabling large language models (LLMs) to exhibit reasoning capabilities. However, in many scenarios, high-quality CoT data is scarce or even unavailable. In such cases, STaR-like methods can help LLMs synthesize CoT based on user queries and response, but they inevitably suffer from the risk of compounding errors. In this work, we tackle an even more challenging scenario: tool learning in the absence of user queries. We design a data scaling method using back-translation, which establishes an inference cycle to synthesize both user queries and CoT data. To reudce the compounding error of inference time, we introduce two rule-based verifiers to assess the validity of the synthesized CoT data. In particular, the Cycle Verifier facilitates performance improvement by continuously accumulating new data over multiple iterations. Our approach achieves a <b>75.4%</b> pass rate and a <b>79.6%</b> win rate using small models (7B) in StableToolBench. Notably, these results are obtained exclusively from self-synthesized high-quality data, without relying on external supervision or expert trajectories for warm-up.</abstract>
      <url hash="d1dc2727">2025.findings-acl.1266</url>
      <bibkey>liang-etal-2025-tool</bibkey>
    </paper>
    <paper id="1267">
      <title>When Benchmarks Talk: Re-Evaluating Code <fixed-case>LLM</fixed-case>s with Interactive Feedback</title>
      <author><first>Jane</first><last>Pan</last><affiliation>New York University</affiliation></author>
      <author><first>Ryan</first><last>Shar</last></author>
      <author><first>Jacob</first><last>Pfau</last><affiliation>New York University</affiliation></author>
      <author><first>Ameet</first><last>Talwalkar</last><affiliation>Datadog and Carnegie Mellon University</affiliation></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <author><first>Valerie</first><last>Chen</last></author>
      <pages>24672-24700</pages>
      <abstract>Programming with a coding assistant is a fundamentally interactive process, yet existing static benchmarks fail to capture key features of model-user collaboration. We introduce an interactive evaluation pipeline to examine how LLMs incorporate different types of feedback in a collaborative setting, in which we obfuscate the input of static coding benchmarks so that the code model must interact with a simulated user. Across 10 models and 3 datasets, the relative rankings of models often permute greatly between static and interactive settings, despite models being fairly robust to feedback that contains errors. We also observe that similarly effective feedback types differ in terms of how models respond to higher- vs. lower-quality feedback. Moreover, feedback type impacts the degree to which the models make aesthetic or behavioral edits to their output. Our work aims to “re-evaluate” model coding capabilities through an interactive lens toward bridging the gap between existing evaluations and real-world usage.</abstract>
      <url hash="47cd4aef">2025.findings-acl.1267</url>
      <bibkey>pan-etal-2025-benchmarks</bibkey>
    </paper>
    <paper id="1268">
      <title>Reranking-based Generation for Unbiased Perspective Summarization</title>
      <author><first>Narutatsu</first><last>Ri</last></author>
      <author><first>Nicholas</first><last>Deas</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>24701-24723</pages>
      <abstract>Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model–based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.</abstract>
      <url hash="335217ac">2025.findings-acl.1268</url>
      <bibkey>ri-etal-2025-reranking</bibkey>
    </paper>
    <paper id="1269">
      <title><fixed-case>KARPA</fixed-case>: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model’s Reasoning Path Aggregation</title>
      <author><first>Siyuan</first><last>Fang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Kaijing</first><last>Ma</last></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Xeron</first><last>Du</last></author>
      <author><first>Ningxuan</first><last>Lu</last><affiliation>Duke University</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Qingkun</first><last>Tang</last><affiliation>zte</affiliation></author>
      <pages>24724-24746</pages>
      <abstract>Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning. KARPA operates in three steps: pre-planning relation paths using the LLM’s global planning capabilities, matching semantically relevant paths via an embedding model, and reasoning over these paths to generate answers. Unlike existing KGQA methods, KARPA avoids stepwise traversal, requires no additional training, and is adaptable to various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy.</abstract>
      <url hash="cb4533cd">2025.findings-acl.1269</url>
      <bibkey>fang-etal-2025-karpa</bibkey>
    </paper>
    <paper id="1270">
      <title>Enhancing <fixed-case>LLM</fixed-case>-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph</title>
      <author><first>Yibo</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Jiapeng</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Can</first><last>Xu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yao</first><last>Liu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <pages>24747-24760</pages>
      <abstract>The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxicity knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called *MetaTox*, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three step pipeline. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxicity knowledge. Extensive experiments and case studies across multiple datasets demonstrate that our MetaTox boosts overall toxicity detection performance, particularly in out-of-domain settings. In addition, under in-domain scenarios, we surprisingly find that small language models are more competent. Our code is available at https://github.com/YiboZhao624/MetaTox.</abstract>
      <url hash="154a65f7">2025.findings-acl.1270</url>
      <bibkey>zhao-etal-2025-enhancing-llm</bibkey>
    </paper>
    <paper id="1271">
      <title>Mixture-of-Personas Language Models for Population Simulation</title>
      <author><first>Ngoc</first><last>Bui</last><affiliation>Yale University</affiliation></author>
      <author><first>Hieu Trung</first><last>Nguyen</last><affiliation>Vinai Research</affiliation></author>
      <author><first>Shantanu</first><last>Kumar</last><affiliation>Yale University and Snap Inc.</affiliation></author>
      <author><first>Julian</first><last>Theodore</last></author>
      <author><first>Weikang</first><last>Qiu</last><affiliation>Yale University</affiliation></author>
      <author><first>Viet Anh</first><last>Nguyen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Rex</first><last>Ying</last><affiliation>Yale University</affiliation></author>
      <pages>24761-24778</pages>
      <abstract>Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose <i>Mixture of Personas</i> (MoP), a <i>probabilistic</i> prompting method that aligns LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar that represents the behaviors of subpopulation. The persona and the exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, does not require model fine-tuning, and is transferable between base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.</abstract>
      <url hash="f7ba6da0">2025.findings-acl.1271</url>
      <bibkey>bui-etal-2025-mixture</bibkey>
    </paper>
    <paper id="1272">
      <title><fixed-case>C</fixed-case>lus<fixed-case>C</fixed-case>omp: A Simple Paradigm for Model Compression and Efficient Finetuning</title>
      <author><first>Baohao</first><last>Liao</last></author>
      <author><first>Christian</first><last>Herold</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Seyyed Hadi</first><last>Hashemi</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Stefan</first><last>Vasilev</last></author>
      <author><first>Shahram</first><last>Khadivi</last><affiliation>eBay Research</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>24779-24804</pages>
      <abstract>As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.</abstract>
      <url hash="cadd80c0">2025.findings-acl.1272</url>
      <bibkey>liao-etal-2025-cluscomp</bibkey>
    </paper>
    <paper id="1273">
      <title>Decomposed Opinion Summarization with Verified Aspect-Aware Modules</title>
      <author><first>Miao</first><last>Li</last></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne and Carnegie Mellon University</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>24805-24841</pages>
      <abstract>Opinion summarization plays a key role in deriving meaningful insights from large-scale online reviews. To make the process more explainable and grounded, we propose a domain-agnostic modular approach guided by review aspects (e.g., cleanliness for hotel reviews) which separates the tasks of aspect identification, opinion consolidation, and meta-review synthesis to enable greater transparency and ease of inspection. We conduct extensive experiments across datasets representing scientific research, business, and product domains. Results show that our approach generates more grounded summaries compared to strong baseline models, as verified through automated and human evaluations. Additionally, our modular approach, which incorporates reasoning based on review aspects, produces more informative intermediate outputs than other knowledge-agnostic decomposition approaches. Lastly, we provide empirical results to show that these intermediate outputs can support humans in summarizing opinions from large volumes of reviews.</abstract>
      <url hash="c32e309c">2025.findings-acl.1273</url>
      <bibkey>li-etal-2025-decomposed</bibkey>
    </paper>
    <paper id="1274">
      <title>Token-Budget-Aware <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Tingxu</first><last>Han</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Chunrong</first><last>Fang</last><affiliation>nanjing university</affiliation></author>
      <author><first>Shiyu</first><last>Zhao</last><affiliation>Amazon</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zhenyu</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>24842-24855</pages>
      <abstract>Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.</abstract>
      <url hash="6a820a1b">2025.findings-acl.1274</url>
      <bibkey>han-etal-2025-token</bibkey>
    </paper>
    <paper id="1275">
      <title><fixed-case>HATA</fixed-case>: Trainable and Hardware-Efficient Hash-Aware Top-<tex-math>k</tex-math> Attention for Scalable Large Model Inference</title>
      <author><first>Ping</first><last>Gong</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiawei</first><last>Yi</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shengnan</first><last>Wang</last></author>
      <author><first>Juncheng</first><last>Zhang</last></author>
      <author><first>Zewen</first><last>Jin</last></author>
      <author><first>Ouxiang</first><last>Zhou</last></author>
      <author><first>Ruibo</first><last>Liu</last></author>
      <author><first>Guanbin</first><last>Xu</last></author>
      <author><first>Youhui</first><last>Bai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Bowen</first><last>Ye</last></author>
      <author><first>Kun</first><last>Yuan</last><affiliation>Peking University</affiliation></author>
      <author><first>Tong</first><last>Yang</last></author>
      <author><first>Gong</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Renhai</first><last>Chen</last><affiliation>Tianjin University and Huawei Technologies Ltd.</affiliation></author>
      <author><first>Feng</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Cheng</first><last>Li</last></author>
      <pages>24856-24871</pages>
      <abstract>Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-<tex-math>k</tex-math> attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-<tex-math>k</tex-math> Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-<tex-math>k</tex-math> attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2<tex-math>\times</tex-math> speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-<tex-math>k</tex-math> attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.</abstract>
      <url hash="3524f0be">2025.findings-acl.1275</url>
      <bibkey>gong-etal-2025-hata</bibkey>
    </paper>
    <paper id="1276">
      <title>Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning</title>
      <author><first>Shota</first><last>Takashiro</last></author>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Andrew</first><last>Gambardella</last><affiliation>The University of Tokyo, Tokyo University</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>24872-24885</pages>
      <abstract>As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities.Therefore, we propose a novel method termed ìn-context knowledge unlearning”, which enables the model to selectively forget information in test-time based on the query context.Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios.Further investigation of the model’s internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. LLMs pretend to forget”.Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.</abstract>
      <url hash="c913d352">2025.findings-acl.1276</url>
      <bibkey>takashiro-etal-2025-answer</bibkey>
    </paper>
    <paper id="1277">
      <title><fixed-case>LIST</fixed-case>: Linearly Incremental <fixed-case>SQL</fixed-case> Translator for Single-Hop Reasoning, Generation and Verification</title>
      <author><first>Kaiyuan</first><last>Guan</last><affiliation>Tencent</affiliation></author>
      <author><first>Ruoxin</first><last>Li</last></author>
      <author><first>Xudong</first><last>Guo</last></author>
      <author><first>Zhenning</first><last>Huang</last></author>
      <author><first>Xudong</first><last>Weng</last><affiliation>Tencent</affiliation></author>
      <author><first>Hehuan</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Wei</last></author>
      <author><first>Zang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <pages>24886-24897</pages>
      <abstract>SQL languages often feature nested structures that require robust interaction with databases. Aside from the well-validated schema linking methods on PLMs and LLMs, we introduce the Linearly Incremental SQL Translator (LIST), a novel algorithmic toolkit designed to leverage the notable reasoning and tool interaction capabilities inherent in LLMs. LIST transforms complex SQL queries into grammatically verifiable sub-queries which are arranged sequentially to reflect single-hop reasoning steps, enhancing both the granularity and accuracy of database interactions. With in-context learning, our experiments demonstrated significant improvements, achieving notable performance of 60.56% and 56.32% on the BIRD dataset with GPT-4o and Llama-3-70B-Instruct. To the best of our knowledge, this achieves SOTA performance among non-schema linking methods, also surpassing a series of schema linking based approaches at a comparable or better cost.</abstract>
      <url hash="0c2d4c13">2025.findings-acl.1277</url>
      <bibkey>guan-etal-2025-list</bibkey>
    </paper>
    <paper id="1278">
      <title><fixed-case>MAGI</fixed-case>: Multi-Agent Guided Interview for Psychiatric Assessment</title>
      <author><first>Guanqun</first><last>Bi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhuang</first><last>Chen</last><affiliation>Central South University</affiliation></author>
      <author><first>Zhoufu</first><last>Liu</last></author>
      <author><first>Hongkai</first><last>Wang</last></author>
      <author><first>Xiyao</first><last>Xiao</last></author>
      <author><first>Yuqiang</first><last>Xie</last></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>University of International Relations</affiliation></author>
      <author><first>Yongkang</first><last>Huang</last><affiliation>Beijing Lingxin Intelligent Technology Co., Ltd</affiliation></author>
      <author><first>Yuxuan</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Libiao</first><last>Peng</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>24898-24921</pages>
      <abstract>Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI’s branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.</abstract>
      <url hash="13b6e956">2025.findings-acl.1278</url>
      <bibkey>bi-etal-2025-magi</bibkey>
    </paper>
    <paper id="1279">
      <title><fixed-case>T</fixed-case>itu<fixed-case>LLM</fixed-case>s: A Family of <fixed-case>B</fixed-case>angla <fixed-case>LLM</fixed-case>s with Comprehensive Benchmarking</title>
      <author><first>Shahriar Kabir</first><last>Nahin</last></author>
      <author><first>Rabindra Nath</first><last>Nandi</last></author>
      <author><first>Sagor</first><last>Sarker</last></author>
      <author><first>Quazi Sarwar</first><last>Muhtaseem</last></author>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Apu Chandraw</first><last>Shill</last><affiliation>Hishab technologies</affiliation></author>
      <author><first>Md</first><last>Ibrahim</last></author>
      <author><first>Mehadi Hasan</first><last>Menon</last></author>
      <author><first>Tareq Al</first><last>Muntasir</last></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>24922-24940</pages>
      <abstract>In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately ∼ 37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available.</abstract>
      <url hash="0e362bdf">2025.findings-acl.1279</url>
      <bibkey>nahin-etal-2025-titullms</bibkey>
    </paper>
    <paper id="1280">
      <title><fixed-case>W</fixed-case>iki<fixed-case>M</fixed-case>ix<fixed-case>QA</fixed-case>: A Multimodal Benchmark for Question Answering over Tables and Charts</title>
      <author><first>Negar</first><last>Foroutan</last><affiliation>School of Computer and Communication Sciences, EPFL - EPF Lausanne</affiliation></author>
      <author><first>Angelika</first><last>Romanou</last></author>
      <author><first>Matin</first><last>Ansaripour</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Julian Martin</first><last>Eisenschlos</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Karl</first><last>Aberer</last><affiliation>School of Computer and Communication Sciences, EPFL - EPF Lausanne</affiliation></author>
      <author><first>Rémi</first><last>Lebret</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <pages>24941-24958</pages>
      <abstract>Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.</abstract>
      <url hash="e0dfe5ef">2025.findings-acl.1280</url>
      <bibkey>foroutan-etal-2025-wikimixqa</bibkey>
    </paper>
    <paper id="1281">
      <title>Let’s Fuse Step by Step: A Generative Fusion Decoding Algorithm with <fixed-case>LLM</fixed-case>s for Robust and Instruction-Aware <fixed-case>ASR</fixed-case> and <fixed-case>OCR</fixed-case></title>
      <author><first>Chan-Jan</first><last>Hsu</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Yi-Chang</first><last>Chen</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Feng-Ting</first><last>Liao</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Pei-Chen</first><last>Ho</last></author>
      <author><first>Yu-Hsiang</first><last>Wang</last></author>
      <author><first>Po-Chun</first><last>Hsu</last><affiliation>MediaTek Research</affiliation></author>
      <author><first>Da-shan</first><last>Shiu</last></author>
      <pages>24959-24973</pages>
      <abstract>We introduce “Generative Fusion Decoding” (GFD), a novel shallow fusion framework, utilized to integrate large language models(LLMs) into cross-modal text recognition systems inlculding automatic speech recognition (ASR) and optical character recognition (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by calculating likelihood at the byte level, thereby enabling seamless fusion and synchronous progression during the decoding process. GFD is plug-and-play bydesign, making it readily compatible with various auto-regressive models without the need for any re-training. GFD proves effective for general ASR and OCR tasks through intermediate and frequent interactions with LLMs, surpassing cascaded methods in English and Mandarin benchmarks. In addition, GFD transfers in-context learning abilities of LLMs and allows for adaptive ASR in instruction-aware andlong-context settings, yielding significant WER reductions of up to 17.7%.</abstract>
      <url hash="722fed48">2025.findings-acl.1281</url>
      <bibkey>hsu-etal-2025-lets</bibkey>
    </paper>
    <paper id="1282">
      <title><fixed-case>HPSS</fixed-case>: Heuristic Prompting Strategy Search for <fixed-case>LLM</fixed-case> Evaluators</title>
      <author><first>Bosi</first><last>Wen</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Yufei</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hongning</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>24974-25007</pages>
      <abstract>Since the adoption of large language models (LLMs) for text evaluation has become increasingly prevalent in the field of natural language processing (NLP), a series of existing works attempt to optimize the prompts for LLM evaluators to improve their alignment with human judgment. However, their efforts are limited to optimizing individual factors of evaluation prompts, such as evaluation criteria or output formats, neglecting the combinatorial impact of multiple factors, which leads to insufficient optimization of the evaluation pipeline. Nevertheless, identifying well-behaved prompting strategies for adjusting multiple factors requires extensive enumeration. To this end, we comprehensively integrate 8 key factors for evaluation prompts and propose a novel automatic prompting strategy optimization method called Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm, HPSS conducts an iterative search to find well-behaved prompting strategies for LLM evaluators. A heuristic function is employed to guide the search process, enhancing the performance of our algorithm. Extensive experiments across four evaluation tasks demonstrate the effectiveness of HPSS, consistently outperforming both human-designed evaluation prompts and existing automatic prompt optimization methods. Our code is available athttps://github.com/thu-coai/HPSS.</abstract>
      <url hash="45328af9">2025.findings-acl.1282</url>
      <bibkey>wen-etal-2025-hpss</bibkey>
    </paper>
    <paper id="1283">
      <title>A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit</title>
      <author><first>Zafarullah</first><last>Mahmood</last></author>
      <author><first>Soliman</first><last>Ali</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Jiading</first><last>Zhu</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Mohamed</first><last>Abdelwahab</last></author>
      <author><first>Michelle Yu</first><last>Collins</last></author>
      <author><first>Sihan</first><last>Chen</last></author>
      <author><first>Yi Cheng</first><last>Zhao</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Jodi</first><last>Wolff</last><affiliation>Centre for Addiction and Mental Health (CAMH)</affiliation></author>
      <author><first>Osnat C.</first><last>Melamed</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Nadia</first><last>Minian</last></author>
      <author><first>Marta</first><last>Maslej</last><affiliation>University of Toronto and Centre for Addiction and Mental Health</affiliation></author>
      <author><first>Carolynne</first><last>Cooper</last><affiliation>Centre for Addiction and Mental Health (CAMH) and Centre for Addiction and Mental Health (CAMH)</affiliation></author>
      <author><first>Matt</first><last>Ratto</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Peter</first><last>Selby</last><affiliation>University of Toronto and Centre for Addiction and Mental health</affiliation></author>
      <author><first>Jonathan</first><last>Rose</last><affiliation>University of Toronto</affiliation></author>
      <pages>25008-25043</pages>
      <abstract>The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot’s adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants’ confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants’ language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.</abstract>
      <url hash="db5e4ffe">2025.findings-acl.1283</url>
      <bibkey>mahmood-etal-2025-fully</bibkey>
    </paper>
    <paper id="1284">
      <title><fixed-case>L</fixed-case>egal<fixed-case>C</fixed-case>ore: A Dataset for Event Coreference Resolution in Legal Documents</title>
      <author><first>Kangda</first><last>Wei</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Xi</first><last>Shi</last></author>
      <author><first>Jonathan</first><last>Tong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first/><last>Sai Ramana Reddy</last></author>
      <author><first>Anandhavelu</first><last>Natarajan</last><affiliation>Aqxle AI</affiliation></author>
      <author><first>Rajiv</first><last>Jain</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>25044-25059</pages>
      <abstract>Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event detection and event coreference resolution tasks, and find that this dataset poses significant challenges for state-of-the-art open-source and proprietary LLMs, which perform significantly worse than a supervised baseline. We will publish the dataset as well as the code.</abstract>
      <url hash="7c211be9">2025.findings-acl.1284</url>
      <bibkey>wei-etal-2025-legalcore</bibkey>
    </paper>
    <paper id="1285">
      <title>Rectifying Belief Space via Unlearning to Harness <fixed-case>LLM</fixed-case>s’ Reasoning</title>
      <author><first>Ayana</first><last>Niwa</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>25060-25075</pages>
      <abstract>Large Language Models (LLMs) exhibit sophisticated reasoning yet still generate incorrect answers. We attribute these errors to **Spurious Beliefs**, defined as propositions the model internally considers as true despite being factually false. To reduce reasoning errors, we propose a belief space rectification framework. Our method first identifies the beliefs invoked during inference via an explanation‐based approach with Forward‐Backward Beam Search (FBBS). We subsequently apply unlearning via gradient ascent to suppress spurious beliefs and enhance true ones, thereby effectively rectifying the model’s belief space. Experiments on three QA datasets and three LLMs show that our method significantly reduces erroneous reasoning and improves generalization.</abstract>
      <url hash="f19c5f51">2025.findings-acl.1285</url>
      <bibkey>niwa-etal-2025-rectifying</bibkey>
    </paper>
    <paper id="1286">
      <title><fixed-case>M</fixed-case>eme<fixed-case>D</fixed-case>etox<fixed-case>N</fixed-case>et: Balancing Toxicity Reduction and Context Preservation</title>
      <author><first>Gitanjali</first><last>Kumari</last></author>
      <author><first>Jitendra</first><last>Solanki</last></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>25076-25098</pages>
      <abstract>Toxic memes often spread harmful and offensive content and pose a significant challenge in online environments. In this paper, we present MemeDetoxNet, a robust framework designed to mitigate toxicity in memes by leveraging fine-tuned pre-trained models. Our approach utilizes the interpretability of CLIP (Contrastive Language-Image Pre-Training) to identify toxic elements within the visual and textual components of memes. Our objective is to automatically assess the immorality of toxic memes and transform them into morally acceptable alternatives by employing large language models (LLMs) to replace offensive text and blurring toxic regions in the image. As a result, we proposed MemeDetoxNet that has three main primitives: (1) detection of toxic memes, (2) localizing and highlighting toxic visual and textual attributes, and (3) manipulating the toxic content to create a morally acceptable alternative. Empirical evaluation on several publicly available meme datasets shows a reduction in toxicity by approximately 10-20%. Both qualitative and quantitative analyses further demonstrate MemeDetoxNet’s superior performance in detoxifying memes compared to the other methods. These results underscore MemeDetoxNet’s potential as an effective tool for content moderation on online platforms.</abstract>
      <url hash="0a97f240">2025.findings-acl.1286</url>
      <bibkey>kumari-etal-2025-memedetoxnet</bibkey>
    </paper>
    <paper id="1287">
      <title>Should <fixed-case>I</fixed-case> Trust You? Detecting Deception in Negotiations using Counterfactual <fixed-case>RL</fixed-case></title>
      <author><first>Wichayaporn</first><last>Wongkamjan</last></author>
      <author><first>Yanze</first><last>Wang</last></author>
      <author><first>Feng</first><last>Gu</last></author>
      <author><first>Denis</first><last>Peskoff</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Jordan Lee</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>25099-25113</pages>
      <abstract>An increasingly common socio-technical problem is people being taken in by offers that sound “too good to be true”, where persuasion and trust shape decision-making. This paper investigates how AI can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in Diplomacy, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms representing proposals—agreements that players suggest during communication—and computing their relative rewards using agents’ value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-AI interaction tools can build on our methods for deception detection by triggering friction to give users a chance of interrogating suspicious proposals.</abstract>
      <url hash="0d3bd819">2025.findings-acl.1287</url>
      <bibkey>wongkamjan-etal-2025-trust</bibkey>
    </paper>
    <paper id="1288">
      <title>Multi-matrix Factorization Attention</title>
      <author><first>Jingcheng</first><last>Hu</last></author>
      <author><first>Houyi</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yinmin</first><last>Zhang</last><affiliation>StepFun</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Shuigeng</first><last>Zhou</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiangyu</first><last>Zhang</last></author>
      <author><first>Heung-Yeung</first><last>Shum</last></author>
      <pages>25114-25126</pages>
      <abstract>We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA’s design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.</abstract>
      <url hash="f8ce57d5">2025.findings-acl.1288</url>
      <bibkey>hu-etal-2025-multi</bibkey>
    </paper>
    <paper id="1289">
      <title>Self-Training Elicits Concise Reasoning in Large Language Models</title>
      <author><first>Tergel</first><last>Munkhbat</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Namgyu</first><last>Ho</last></author>
      <author><first>Seo Hyun</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yongjin</first><last>Yang</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Yujin</first><last>Kim</last></author>
      <author><first>Se-Young</first><last>Yun</last><affiliation>KAIST</affiliation></author>
      <pages>25127-25152</pages>
      <abstract>Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training.</abstract>
      <url hash="b2e0a617">2025.findings-acl.1289</url>
      <bibkey>munkhbat-etal-2025-self</bibkey>
    </paper>
    <paper id="1290">
      <title>Reason from Future: Reverse Thought Chain Enhances <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Yinlong</first><last>Xu</last></author>
      <author><first>Yanzhao</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shuoshuo</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Shuaihan</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Baohua</first><last>Dong</last></author>
      <author><first>Zhu</first><last>Hangcheng</last><affiliation>Taobao &amp; Tmall Group</affiliation></author>
      <author><first>Ruohui</first><last>Huang</last></author>
      <author><first>Gang</first><last>Yu</last></author>
      <author><first>Hongxia</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>25153-25166</pages>
      <abstract>It has been demonstrated that carefully designed reasoning paradigms, like Chain-of-Thought(CoT) and Tree-of-Thought(ToT), can enhance the reasoning capabilities of small language models by detailed thinking and extensive thought searching, unbounded branching factors in the searching space create prohibitive reasoning consumption. However these methods fell into the trap of local optimum reasoning, which means the model lacks a global perspective while solving problems. We propose a novel reasoning paradigm called Reason from Future(RFF), which generates reasoning paths by bidirectional reasoning that combines top-down planning with bottom-up reasoning accumulation. The essence of RFF lies in its reverse reasoning mechanism, which prioritizes core logical relationships and imposes goal-oriented constraints on intermediate steps, thereby reducing the searching space and mitigating error accumulation inherent in sequential forward reasoning. Empirical evaluations across diverse experiments demonstrate that RFF outperforms conventional paradigms with higher accuracy and less searching space to solve complex tasks.</abstract>
      <url hash="a0e81559">2025.findings-acl.1290</url>
      <bibkey>xu-etal-2025-reason</bibkey>
    </paper>
    <paper id="1291">
      <title><fixed-case>LLM</fixed-case>s as Planning Formalizers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models</title>
      <author><first>Marcus</first><last>Tantakoun</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Christian</first><last>Muise</last><affiliation>Queens University</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <pages>25167-25188</pages>
      <abstract>Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting and introduces new challenges. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for formalizing and refining planning specifications to support reliable off-the-shelf AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.</abstract>
      <url hash="20a517ca">2025.findings-acl.1291</url>
      <bibkey>tantakoun-etal-2025-llms</bibkey>
    </paper>
    <paper id="1292">
      <title>From Conversation to Automation: Leveraging <fixed-case>LLM</fixed-case>s for Problem-Solving Therapy Analysis</title>
      <author><first>Elham</first><last>Aghakhani</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <author><first>Karla T.</first><last>Washington</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>George</first><last>Demiris</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Jina</first><last>Huh-Yoo</last><affiliation>University of California-San Diego</affiliation></author>
      <author><first>Rezvaneh</first><last>Rezapour</last><affiliation>Drexel University</affiliation></author>
      <pages>25189-25207</pages>
      <abstract>Problem-Solving Therapy (PST) is a structured psychological approach that helps individuals manage stress and resolve personal issues by guiding them through problem identification, solution brainstorming, decision-making, and outcome evaluation. As mental health care increasingly adopts technologies like chatbots and large language models (LLMs), it is important to thoroughly understand how each session of PST is conducted before attempting to automate it. We developed a comprehensive framework for PST annotation using established PST Core Strategies and a set of novel Facilitative Strategies to analyze a corpus of real-world therapy transcripts to determine which strategies are most prevalent. Using various LLMs and transformer-based models, we found that GPT-4o outperformed all models, achieving the highest accuracy (0.76) in identifying all strategies. To gain deeper insights, we examined how strategies are applied by analyzing Therapeutic Dynamics (autonomy, self-disclosure, and metaphor), and linguistic patterns within our labeled data. Our research highlights LLMs’ potential to automate therapy dialogue analysis, offering a scalable tool for mental health interventions. Our framework enhances PST by improving accessibility, effectiveness, and personalized support for therapists.</abstract>
      <url hash="6469adc4">2025.findings-acl.1292</url>
      <bibkey>aghakhani-etal-2025-conversation</bibkey>
    </paper>
    <paper id="1293">
      <title>Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation</title>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Jiayi</first><last>Shi</last></author>
      <author><first>Yueqi</first><last>Zhang</last></author>
      <author><first>Chuyi</first><last>Tan</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yao</first><last>Hu</last><affiliation>Xiaohongshu</affiliation></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>25208-25223</pages>
      <abstract>Self-consistency improves reasoning by aggregating diverse stochastic samples, yet the dynamics behind its efficacy remain underexplored. We reframe self-consistency as a dynamic distributional alignment problem, revealing that decoding temperature not only governs sampling randomness but also actively shapes the latent answer distribution. Given that high temperatures require prohibitively large sample sizes to stabilize, while low temperatures risk amplifying biases, we propose a confidence-driven mechanism that dynamically calibrates temperature: sharpening the sampling distribution under uncertainty to align with high-probability modes, and promoting exploration when confidence is high. Experiments on mathematical reasoning tasks show this approach outperforms fixed-diversity baselines under limited samples, improving both average and best-case performance across varying initial temperatures without additional data or modules. This establishes self-consistency as a synchronization challenge between sampling dynamics and evolving answer distributions.</abstract>
      <url hash="5fcfbecf">2025.findings-acl.1293</url>
      <bibkey>li-etal-2025-revisiting-self</bibkey>
    </paper>
    <paper id="1294">
      <title>Don’t Say No: Jailbreaking <fixed-case>LLM</fixed-case> by Suppressing Refusal</title>
      <author><first>Yukai</first><last>Zhou</last></author>
      <author><first>Jian</first><last>Lou</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Zhijie</first><last>Huang</last></author>
      <author><first>Zhan</first><last>Qin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Sibei</first><last>Yang</last></author>
      <author><first>Wenjie</first><last>Wang</last></author>
      <pages>25224-25249</pages>
      <abstract>Ensuring the safety alignment of Large Language Models (LLMs) is critical for generating responses consistent with human values. However, LLMs remain vulnerable to jailbreaking attacks, where carefully crafted prompts manipulate them into producing toxic content. One category of such attacks reformulates the task as an optimization problem, aiming to elicit affirmative responses from the LLM. However, these methods heavily rely on predefined objectionable behaviors, limiting their effectiveness and adaptability to diverse harmful queries. In this study, we first identify why the vanilla target loss is suboptimal and then propose enhancements to the loss objective. We introduce <tex-math>\textit{DSN}</tex-math> (Don’t Say No) attack, which combines a cosine decay schedule method with refusal suppression to achieve higher success rates. Extensive experiments demonstrate that <tex-math>\textit{DSN}</tex-math> outperforms baseline attacks and achieves state-of-the-art attack success rates (ASR). <tex-math>\textit{DSN}</tex-math> also shows strong universality and transferability to unseen datasets and black-box models.</abstract>
      <url hash="09ffd089">2025.findings-acl.1294</url>
      <bibkey>zhou-etal-2025-dont</bibkey>
    </paper>
    <paper id="1295">
      <title>From Perception to Reasoning: Enhancing Vision-Language Models for Mobile <fixed-case>UI</fixed-case> Understanding</title>
      <author><first>Settaluri Lakshmi</first><last>Sravanthi</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Ankit</first><last>Mishra</last></author>
      <author><first>Debjyoti</first><last>Mondal</last><affiliation>Samsung</affiliation></author>
      <author><first>Subhadarshi</first><last>Panda</last><affiliation>Samsung</affiliation></author>
      <author><first>Rituraj</first><last>Singh</last><affiliation>Samsung Research and Development Institute - India, Bengaluru</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>25250-25269</pages>
      <abstract>Accurately grounding visual and textual elements within mobile user interfaces (UIs) remains a significant challenge for Vision-Language Models (VLMs). Visual grounding, a critical task in this domain, involves identifying the most relevant UI element or region based on a natural language query—a process that requires both precise perception and context-aware reasoning. In this work, we present - **MoUI**, a light-weight mobile UI understanding model trained on **MoIT**, an instruction-tuning dataset specifically tailored for mobile screen understanding and grounding, designed to bridge the gap between user intent and visual semantics. Complementing this dataset, we also present a human-annotated reasoning benchmark **MoIQ** that rigorously evaluates complex inference capabilities over mobile UIs. To harness these resources effectively, we propose a two-stage training approach that separately addresses perception and reasoning tasks, leading to stronger perception capabilities and improvement in reasoning abilities. Through extensive experiments, we demonstrate that our MoUI models achieve significant gains in accuracy across all perception tasks and _state-of-the-art_ results on public reasoning benchmark **ComplexQA (78%) and our MoIQ (49%)**. We will be open-sourcing our dataset, code, and models to foster further research and innovation in the field.</abstract>
      <url hash="cee1e1a7">2025.findings-acl.1295</url>
      <bibkey>sravanthi-etal-2025-perception</bibkey>
    </paper>
    <paper id="1296">
      <title>Lemmas Matter, But Not Like That: Predictors of Lemma-Based Generalization in Morphological Inflection</title>
      <author><first>Sarah Ruth Brogden</first><last>Payne</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Jordan</first><last>Kodner</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>25270-25286</pages>
      <abstract>Recent work has suggested that overlap –whether a given lemma or feature set is attested independently in train – drives model performance on morphological inflection tasks. The impact of lemma overlap, however, is debated, with recent work reporting accuracy drops from 0% to 30% between seen and unseen test lemmas. In this paper, we introduce a novel splitting algorithm designed to investigate predictors of accuracy on seen and unseen lemmas. We find only an 11% average drop from seen to unseen test lemmas, but show that the number of lemmas in train has a much stronger effect on accuracy on unseen than seen lemmas. We also show that the previously reported 30% drop is inflated due to the introduction of a near-30% drop in the number of training lemmas from the original splits to their novel splits.</abstract>
      <url hash="81f18cd6">2025.findings-acl.1296</url>
      <bibkey>payne-kodner-2025-lemmas</bibkey>
    </paper>
    <paper id="1297">
      <title>Mosaic-<fixed-case>IT</fixed-case>: Cost-Free Compositional Data Synthesis for Instruction Tuning</title>
      <author><first>Ming</first><last>Li</last></author>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Hongyu</first><last>Zhao</last></author>
      <author><first>Yijun</first><last>Liang</last></author>
      <author><first>YuPeng</first><last>Hou</last><affiliation>Amazon</affiliation></author>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>25287-25318</pages>
      <abstract>Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses for training, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human/model-free compositional data synthesis method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and an 80% reduction in training costs compared with original instruction tuning.</abstract>
      <url hash="f80fd1bd">2025.findings-acl.1297</url>
      <bibkey>li-etal-2025-mosaic</bibkey>
    </paper>
    <paper id="1298">
      <title><fixed-case>MAM</fixed-case>: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration</title>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lingran</first><last>Song</last></author>
      <author><first>Jianbing</first><last>Shen</last><affiliation>University of Macau</affiliation></author>
      <pages>25319-25333</pages>
      <abstract>Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code, data, and prompts are released at URL.</abstract>
      <url hash="13a2af65">2025.findings-acl.1298</url>
      <bibkey>zhou-etal-2025-mam</bibkey>
    </paper>
    <paper id="1299">
      <title><fixed-case>ATLAS</fixed-case>: Agent Tuning via Learning Critical Steps</title>
      <author><first>Zhixun</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Ming</first><last>Li</last></author>
      <author><first>Yuxuan</first><last>Huang</last></author>
      <author><first>Yali</first><last>Du</last><affiliation>King’s College London</affiliation></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>25334-25349</pages>
      <abstract>Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps—such as planning, complex reasoning for intermediate subtasks, and strategic decision-making—are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLAS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training’s focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLAS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLAS maintains and improves base LLM skills as generalist agents interacting with diverse environments.</abstract>
      <url hash="71112eb9">2025.findings-acl.1299</url>
      <bibkey>chen-etal-2025-atlas</bibkey>
    </paper>
    <paper id="1300">
      <title>Syntactic Control of Language Models by Posterior Inference</title>
      <author><first>Vicky</first><last>Xefteri</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tim</first><last>Vieira</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Afra</first><last>Amini</last></author>
      <pages>25350-25365</pages>
      <abstract>Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from 12.31 (GPT2-large) and 35.33 (Llama3-8B) to about 93 in both cases without compromising the language model’s fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.</abstract>
      <url hash="b53f4af2">2025.findings-acl.1300</url>
      <bibkey>xefteri-etal-2025-syntactic</bibkey>
    </paper>
    <paper id="1301">
      <title>Small Models Struggle to Learn from Strong Reasoners</title>
      <author><first>Yuetai</first><last>Li</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zhangchen</first><last>Xu</last></author>
      <author><first>Fengqing</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Luyao</first><last>Niu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Bhaskar</first><last>Ramasubramanian</last><affiliation>Western Washington University</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>25366-25394</pages>
      <abstract>Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models (3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.</abstract>
      <url hash="1d1c6225">2025.findings-acl.1301</url>
      <bibkey>li-etal-2025-small-models</bibkey>
    </paper>
    <paper id="1302">
      <title>Sparse Rewards Can Self-Train Dialogue Agents</title>
      <author><first>Barrett Martin</first><last>Lattimer</last><affiliation>ASAPP</affiliation></author>
      <author><first>Varun Prashant</first><last>Gangal</last><affiliation>Patronus AI</affiliation></author>
      <author><first>Ryan</first><last>McDonald</last><affiliation>ASAPP</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>ASAPP Inc</affiliation></author>
      <pages>25395-25413</pages>
      <abstract>Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM) agents, especially in multi-turn dialogue tasks, have been primarily driven by supervised fine-tuning and high-quality human feedback. However, as base LLM models continue to improve, acquiring meaningful human feedback has become increasingly challenging and costly. In certain domains, base LLM agents may eventually exceed human capabilities, making traditional feedback-driven methods impractical. In this paper, we introduce a novel self-improvement paradigm that empowers LLM agents to autonomously enhance their performance without external human feedback. Our method, Juxtaposed Outcomes for Simulation Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation environment derived from MultiWOZ. We demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities across diverse benchmarks. Our code and data are publicly available on GitHub.</abstract>
      <url hash="fcd7155e">2025.findings-acl.1302</url>
      <bibkey>lattimer-etal-2025-sparse</bibkey>
    </paper>
    <paper id="1303">
      <title>Almost <fixed-case>AI</fixed-case>, Almost Human: The Challenge of Detecting <fixed-case>AI</fixed-case>-Polished Writing</title>
      <author><first>Shoumik</first><last>Saha</last></author>
      <author><first>Soheil</first><last>Feizi</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>25414-25431</pages>
      <abstract>The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate *twelve* state-of-the-art AI-text detectors using our **AI-Polished-Text Evaluation (APT-Eval)** dataset, which contains <tex-math>15K</tex-math> samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.</abstract>
      <url hash="06fcdfba">2025.findings-acl.1303</url>
      <bibkey>saha-feizi-2025-almost</bibkey>
    </paper>
    <paper id="1304">
      <title>The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of <fixed-case>AI</fixed-case> Creative Writing</title>
      <author><first>Guillermo</first><last>Marco</last></author>
      <author><first>Julio</first><last>Gonzalo</last><affiliation>Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Víctor</first><last>Fresno</last></author>
      <pages>25432-25449</pages>
      <abstract>Recent studies comparing AI-generated and human-authored literary texts have produced conflicting results: some suggest AI already surpasses human quality, while others argue it still falls short. We start from the hypothesis that such divergences can be largely explained by genuine differences in how readers interpret and value literature, rather than by an intrinsic quality of the texts evaluated. Using five public datasets (1,471 stories, 101 annotators including critics, students, and lay readers), we (i) extract 17 reference-less textual features (e.g., coherence, emotional variance, average sentence length...); (ii) model individual reader preferences, deriving feature importance vectors that reflect their textual priorities; and (iii) analyze these vectors in a shared “preference space”. Reader vectors cluster into two profiles: _surface-focused readers_ (mainly non-experts), who prioritize readability and textual richness; and _holistic readers_ (mainly experts), who value thematic development, rhetorical variety, and sentiment dynamics. Our results quantitatively explain how measurements of literary quality are a function of how text features align with each reader’s preferences. These findings advocate for reader-sensitive evaluation frameworks in the field of creative text generation.</abstract>
      <url hash="70e4c5a6">2025.findings-acl.1304</url>
      <bibkey>marco-etal-2025-reader</bibkey>
    </paper>
    <paper id="1305">
      <title>Summary Factual Inconsistency Detection Based on <fixed-case>LLM</fixed-case>s Enhanced by Universal Information Extraction</title>
      <author><first>Anguo</first><last>Li</last></author>
      <author><first>Lei</first><last>Yu</last><affiliation>Beihang University</affiliation></author>
      <pages>25450-25465</pages>
      <abstract>Automatic text summarization has a potential flaw that affects the factuality of summaries. Recently, Large Language Models (LLMs) have been introduced as detectors for factual inconsistencies in summaries. However, LLM-based methods rely on reasoning capabilities and face challenges in terms of efficiency and explainability. We focus on decoupling LLMs’ information extraction and reasoning capabilities to address prominent challenges, and propose a novel framework, UIEFID (Universal Information Extraction-enhanced Factual Inconsistency Detection). Our idea is to define a self-adaptive structured schema to guide fine-tuned LLMs in extracting unified structured information from documents and summaries, ultimately detecting the origins of inconsistencies in extraction information. The evaluation on 5 open-source models shows that UIEFID not only enhances the detection accuracy on the AGGREFACT benchmark but also significantly reduces redundant reasoning.</abstract>
      <url hash="5e5a330e">2025.findings-acl.1305</url>
      <bibkey>li-yu-2025-summary</bibkey>
    </paper>
    <paper id="1306">
      <title><fixed-case>ELI</fixed-case>-Why: Evaluating the Pedagogical Utility of Language Model Explanations</title>
      <author><first>Brihi</first><last>Joshi</last></author>
      <author><first>Keyu</first><last>He</last></author>
      <author><first>Sahana</first><last>Ramnath</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Sadra</first><last>Sabouri</last></author>
      <author><first>Kaitlyn</first><last>Zhou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Souti</first><last>Chattopadhyay</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Swabha</first><last>Swayamdipta</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California</affiliation></author>
      <pages>25466-25499</pages>
      <abstract>Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K “Why” questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an “educator” to assess model explanations’ fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.</abstract>
      <url hash="ce5c8ae4">2025.findings-acl.1306</url>
      <bibkey>joshi-etal-2025-eli</bibkey>
    </paper>
    <paper id="1307">
      <title>Beyond Generation: Leveraging <fixed-case>LLM</fixed-case> Creativity to Overcome Label Bias in Classification</title>
      <author><first>Xiaoyue</first><last>Wang</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>25500-25506</pages>
      <abstract>Large Language Models (LLMs) exhibit impressive capabilities in In-Context Learning (ICL) but are prone to label bias—an undesirable tendency to favor certain answers. Existing calibration methods mitigate bias by leveraging in-domain data, yet such data is often unavailable in real-world scenarios. To address this limitation, we propose SDC (Synthetic Data Calibration), a simple-yet-effective approach that generates synthetic in-domain data from a few in-context demonstrations and utilizes it for calibration. By approximating the benefits of real in-domain data, SDC effectively reduces label bias without requiring access to actual domain-specific inputs. Experimental evaluations on 279 classification and multiple-choice tasks from the Super-NaturalInstructions benchmark. The results show that SDC significantly reduces label bias, achieving an average Bias Score reduction of 57.5%, and outperforming all competitive baselines. Moreover, when combined with Leave-One-Out Calibration (LOOC), further improves performance, underscoring its effectiveness and generalizability in enhancing the reliability of LLMs.</abstract>
      <url hash="7852e064">2025.findings-acl.1307</url>
      <bibkey>wang-liu-2025-beyond</bibkey>
    </paper>
    <paper id="1308">
      <title><fixed-case>C</fixed-case>og<fixed-case>S</fixed-case>teer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models</title>
      <author><first>Xintong</first><last>Wang</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Jingheng</first><last>Pan</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Longqin</first><last>Jiang</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Xingshan</first><last>Li</last></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <pages>25507-25522</pages>
      <abstract>Large Language Models (LLMs) achieve remarkable performance through pretraining on extensive data. This enables efficient adaptation to diverse downstream tasks. However, the lack of interpretability in their underlying mechanisms limits the ability to effectively steer LLMs for specific applications. In this work, we investigate the intrinsic mechanisms of LLMs from a cognitive perspective using eye movement measures. Specifically, we analyze the layer-wise correlation between human cognitive indicators and LLM representations. Building on these insights, we propose a heuristic approach for selecting the optimal steering layer to modulate LLM semantics. To this end, we introduce an efficient selective layer intervention based on prominent parameter-efficient fine-tuning methods, which conventionally adjust either all layers or only the final layer. Additionally, we present an implicit layer contrastive intervention during inference to steer LLMs away from toxic outputs. Extensive experiments on natural language understanding, reasoning, and generation tasks, conducted on GPT-2, LLaMa2-7B, and Mixtral-7B, demonstrate the effectiveness and efficiency of our approach. As a model-agnostic framework, it enhances the interpretability of LLMs while improving efficiency for safe deployment.</abstract>
      <url hash="fd0357df">2025.findings-acl.1308</url>
      <bibkey>wang-etal-2025-cogsteer</bibkey>
    </paper>
    <paper id="1309">
      <title><fixed-case>PASTEL</fixed-case> : Polarity-Aware Sentiment Triplet Extraction with <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Aaditya</first><last>Bodke</last></author>
      <author><first>Avinoor Singh</first><last>Kohli</last></author>
      <author><first>Hemant Subhash</first><last>Pardeshi</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Prathamesh</first><last>Bhosale</last></author>
      <pages>25523-25533</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based Sentiment Analysis (ABSA) that aims to extract aspect terms, corresponding opinion terms, and their associated sentiment polarities from text. Current end-to-end approaches, whether employing Large Language Models (LLMs) or complex neural network structures, struggle to effectively model the intricate latent relationships between aspects and opinions. Therefore, in this work, we propose Polarity-Aware Sentiment Triplet Extraction with LLM-as-a-judge (PASTEL), a novel pipeline that decomposes the ASTE task into structured subtasks. We employ finetuned LLMs to separately extract the aspect and opinion terms, incorporating a polarity-aware mechanism to enhance opinion extraction. After generating a candidate set through the Cartesian product of the extracted aspect and opinion-sentiment sets, we leverage an LLM-as-a-Judge to validate and prune these candidates. Experimental evaluations demonstrate that PASTEL outperforms existing baselines. Our findings highlight the necessity of modular decomposition in complex sentiment analysis tasks to fully exploit the capabilities of current LLMs.</abstract>
      <url hash="54fa5aef">2025.findings-acl.1309</url>
      <bibkey>bodke-etal-2025-pastel</bibkey>
    </paper>
    <paper id="1310">
      <title><fixed-case>COSMIC</fixed-case>: Generalized Refusal Direction Identification in <fixed-case>LLM</fixed-case> Activations</title>
      <author><first>Vincent</first><last>Siu</last></author>
      <author><first>Nicholas</first><last>Crispino</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Zihao</first><last>Yu</last></author>
      <author><first>Sam</first><last>Pan</last></author>
      <author><first>Zhun</first><last>Wang</last><affiliation>University of California, Berkeley</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Dawn</first><last>Song</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <pages>25534-25553</pages>
      <abstract>Large Language Models encode behaviors like refusal within their activation space, but identifying these behaviors remains challenging. Existing methods depend on predefined refusal templates detectable in output tokens or manual review. We introduce **COSMIC** (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that optimally identifies steering directions and target layers using cosine similarity, entirely independent of output text. COSMIC achieves steering effectiveness comparable to prior work without any prior knowledge or assumptions of a model’s refusal behavior such as the use of certain refusal tokens. Additionally, COSMIC successfully identifies refusal directions in adversarial scenarios and models with weak safety alignment, demonstrating its robustness across diverse settings.</abstract>
      <url hash="b4b1282e">2025.findings-acl.1310</url>
      <bibkey>siu-etal-2025-cosmic</bibkey>
    </paper>
    <paper id="1311">
      <title>Red Queen: Exposing Latent Multi-Turn Risks in Large Language Models</title>
      <author><first>Yifan</first><last>Jiang</last><affiliation>Information Sciences Institute, University of Southern California</affiliation></author>
      <author><first>Kriti</first><last>Aggarwal</last><affiliation>HippocraticAI</affiliation></author>
      <author><first>Tanmay</first><last>Laud</last><affiliation>Hippocratic AI</affiliation></author>
      <author><first>Kashif</first><last>Munir</last><affiliation>Hippocraticai AI</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Subhabrata</first><last>Mukherjee</last><affiliation>Hippocratic AI</affiliation></author>
      <pages>25554-25591</pages>
      <abstract>The rapid advancement of large language models (LLMs) has unlocked diverse opportunities across domains and applications but has also raised concerns about their tendency to generate harmful responses under jailbreak attacks. However, most existing jailbreak strategies are single-turn with explicit malicious intent, failing to reflect the real-world scenario where interactions can be multi-turn and users can conceal their intents. Recent studies on Theory of Mind (ToM) reveal that LLMs often struggle to infer users’ latent intent in such scenarios. Building on these limitations, we propose a novel jailbreak attack, RED QUEEN ATTACK, which constructs a multi-turn scenario, concealing the malicious intent under the guise of preventing harm. We generate 56k multi-turn concealment data points across 40 scenarios and 14 harmful categories, evaluating four LLM families of different sizes. Results show all models are vulnerable to RED QUEEN ATTACK, reaching 87.6% attack success rate (ASR) on GPT-4o and 77.1% on Llama3-70B. Compared to prior jailbreak attacks, the RED QUEEN ATTACK achieves superior performance on nine out of ten models, with ASR improvements ranging from 2% to 64%. Further analysis reveals that larger models exhibit greater vulnerability to our attack, primarily due to the combination of multi-turn structures and concealment strategies. To enhance safety, we propose RED QUEEN GUARD, a mitigation strategy reducing ASR to below 1% while maintaining model performance on standard benchmarks. Full implementation and dataset are publicly accessible at https://github.com/kriti-hippo/red_queen.</abstract>
      <url hash="8c05e00d">2025.findings-acl.1311</url>
      <bibkey>jiang-etal-2025-red</bibkey>
    </paper>
    <paper id="1312">
      <title><fixed-case>MDB</fixed-case>ench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance</title>
      <author><first>Joseph J</first><last>Peper</last></author>
      <author><first>Wenzhao</first><last>Qiu</last></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>25592-25621</pages>
      <abstract>Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBench poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.</abstract>
      <url hash="ffc1103e">2025.findings-acl.1312</url>
      <bibkey>peper-etal-2025-mdbench</bibkey>
    </paper>
    <paper id="1313">
      <title><fixed-case>D</fixed-case>ia<fixed-case>LLM</fixed-case>s: <fixed-case>EHR</fixed-case>-Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction</title>
      <author><first>Weijieying</first><last>Ren</last></author>
      <author><first>Tianxiang</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Wang</last><affiliation>SalesForce</affiliation></author>
      <author><first>Tianchun</first><last>Wang</last></author>
      <author><first>Vasant G</first><last>Honavar</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>25622-25635</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have led to remarkable progresses in medical consultation.However, existing medical LLMs overlook the essential role of Electronic Health Records (EHR) and focus primarily on diagnosis recommendation, limiting their clinical applicability. We propose DiaLLM, the first medical LLM that integrates heterogeneous EHR data into clinically grounded dialogues, enabling clinical test recommendation, result interpretation, and diagnosis prediction to better align with real-world medical practice. To construct clinically grounded dialogues from EHR, we design a Clinical Test Reference (CTR) strategy that maps each clinical code to its corresponding description and classifies test results as “normal” or “abnormal”. Additionally, DiaLLM employs a reinforcement learning framework for evidence acquisition and automated diagnosis. To handle the large action space, we introduce a reject sampling strategy to reduce redundancy and improve exploration efficiency. Furthermore, a confirmation reward and a class-sensitive diagnosis reward are designed to guide accurate diagnosis prediction.Extensive experimental results demonstrate that DiaLLM outperforms baselines in clinical test recommendation and diagnosis prediction. Our code is available at Github.</abstract>
      <url hash="59de9459">2025.findings-acl.1313</url>
      <bibkey>ren-etal-2025-diallms</bibkey>
    </paper>
    <paper id="1314">
      <title>Can Hallucination Correction Improve Video-Language Alignment?</title>
      <author><first>Lingjun</first><last>Zhao</last></author>
      <author><first>Mingyang</first><last>Xie</last></author>
      <author><first>Paola</first><last>Cascante-Bonilla</last><affiliation>State University of New York at Stony Brook and University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park</affiliation></author>
      <author><first>Hal Daumé</first><last>Iii</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Kwonjoon</first><last>Lee</last><affiliation>Honda Research Institute USA</affiliation></author>
      <pages>25636-25646</pages>
      <abstract>Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model’s ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment.</abstract>
      <url hash="6305b927">2025.findings-acl.1314</url>
      <bibkey>zhao-etal-2025-hallucination</bibkey>
    </paper>
    <paper id="1315">
      <title><fixed-case>IMPARA</fixed-case>-<fixed-case>GED</fixed-case>: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator</title>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Takumi</first><last>Goto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>25647-25654</pages>
      <abstract>We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations.</abstract>
      <url hash="72cd4be7">2025.findings-acl.1315</url>
      <bibkey>sakai-etal-2025-impara</bibkey>
    </paper>
    <paper id="1316">
      <title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Chenjun</first><last>Xu</last></author>
      <author><first>Bingbing</first><last>Wen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Bin</first><last>Han</last><affiliation>University of Washington</affiliation></author>
      <author><first>Robert</first><last>Wolfe</last></author>
      <author><first>Lucy Lu</first><last>Wang</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bill</first><last>Howe</last><affiliation>University of Washington</affiliation></author>
      <pages>25655-25672</pages>
      <abstract>Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas—e.g., expert vs layman, or different race, gender, and ages—the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.</abstract>
      <url hash="dd11a8c9">2025.findings-acl.1316</url>
      <bibkey>xu-etal-2025-language</bibkey>
    </paper>
    <paper id="1317">
      <title>Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System</title>
      <author><first>Yongsen</first><last>Zheng</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zongxuan</first><last>Xie</last></author>
      <author><first>Guohua</first><last>Wang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Ziyao</first><last>Liu</last></author>
      <author><first>Liang</first><last>Lin</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Kwok-Yan</first><last>Lam</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>25673-25684</pages>
      <abstract>Unfairness is a well-known challenge in Recommender Systems (RSs), often resulting in biased outcomes that disadvantage users or items based on attributes such as gender, race, age, or popularity. Although some approaches have started to improve fairness recommendation in offline or static contexts, the issue of unfairness often exacerbates over time, leading to significant problems like the Matthew effect, filter bubbles, and echo chambers. To address these challenges, we proposed a novel framework, Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS), aiming to promote multi-interest diversity fairness in dynamic and interactive Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide range of user interests by establishing diverse hypergraphs through contrastive learning. These interests are then utilized in conversations to generate informative responses and ensure fair item predictions within the dynamic user-system feedback loop. Experiments on two CRS-based datasets show that HyFairCRS achieves a new state-of-the-art performance while effectively alleviating unfairness.</abstract>
      <url hash="f81694a9">2025.findings-acl.1317</url>
      <bibkey>zheng-etal-2025-multi</bibkey>
    </paper>
    <paper id="1318">
      <title>Cautious Next Token Prediction</title>
      <author><first>Yizhou</first><last>Wang</last><affiliation>Adobe Systems and Northeastern University</affiliation></author>
      <author><first>Lingzhi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Yue</first><last>Bai</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Mang Tik</first><last>Chiu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Zhengmian</first><last>Hu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Mingyuan</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Qihua</first><last>Dong</last></author>
      <author><first>Yu</first><last>Yin</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Sohrab</first><last>Amirghodsi</last></author>
      <author><first>Yun</first><last>Fu</last></author>
      <pages>25685-25697</pages>
      <abstract>Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model’s capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings’ behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.</abstract>
      <url hash="a828c638">2025.findings-acl.1318</url>
      <bibkey>wang-etal-2025-cautious</bibkey>
    </paper>
    <paper id="1319">
      <title>Reasoning with Graphs: Structuring Implicit Knowledge to Enhance <fixed-case>LLM</fixed-case>s Reasoning</title>
      <author><first>Haoyu</first><last>Han</last></author>
      <author><first>Yaochen</first><last>Xie</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>William</first><last>Headden</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Chen</first><last>Luo</last><affiliation>Amazon</affiliation></author>
      <author><first>Shuiwang</first><last>Ji</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>25698-25714</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable success across a wide range of tasks; however, they still encounter challenges in reasoning tasks that require understanding and inferring relationships between distinct pieces of information within text sequences. This challenge is particularly pronounced in tasks involving multi-step processes, such as logical reasoning and multi-hop question answering, where understanding implicit relationships between entities and leveraging multi-hop connections in the given context are crucial. Graphs, as fundamental data structures, explicitly represent pairwise relationships between entities, thereby offering the potential to enhance LLMs’ reasoning capabilities. External graphs have proven effective in supporting LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing graph structure is provided. Can we structure implicit knowledge derived from context into graphs to assist LLMs in reasoning? In this paper, we propose Reasoning with Graphs (RwG) by first constructing explicit graphs from the context and then leveraging these graphs to enhance LLM reasoning performance on reasoning tasks. Extensive experiments demonstrate the effectiveness of the proposed method in improving both logical reasoning and multi-hop question answering tasks.</abstract>
      <url hash="51a1bf7d">2025.findings-acl.1319</url>
      <bibkey>han-etal-2025-reasoning</bibkey>
    </paper>
    <paper id="1320">
      <title>Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment</title>
      <author><first>Hongda</first><last>Sun</last></author>
      <author><first>Jiaren</first><last>Peng</last></author>
      <author><first>Wenzhong</first><last>Yang</last><affiliation>Xinjiang University</affiliation></author>
      <author><first>Liang</first><last>He</last></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>25715-25726</pages>
      <abstract>Medical dialogue systems (MDS) have emerged as crucial online platforms for enabling multi-turn, context-aware conversations with patients. However, existing MDS often struggle to (1) identify relevant medical knowledge and (2) generate personalized, medically accurate responses. To address these challenges, we propose MedRef, a novel MDS that incorporates knowledge refining and dynamic prompt adjustment. First, we employ a knowledge refining mechanism to filter out irrelevant medical data, improving predictions of critical medical entities in responses. Additionally, we design a comprehensive prompt structure that incorporates historical details and evident details. To enable real-time adaptability to diverse patient conditions, we implement two key modules, Triplet Filter and Demo Selector, providing appropriate knowledge and demonstrations equipped in the system prompt.Extensive experiments on MedDG and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in both generation quality and medical entity accuracy, underscoring its effectiveness and reliability for real-world healthcare applications.</abstract>
      <url hash="c02b79b3">2025.findings-acl.1320</url>
      <bibkey>sun-etal-2025-enhancing-medical</bibkey>
    </paper>
    <paper id="1321">
      <title>Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders</title>
      <author><first>Kristian</first><last>Kuznetsov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Laida</first><last>Kushnareva</last><affiliation>Huawei</affiliation></author>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Polina</first><last>Druzhinina</last><affiliation>Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Anastasia</first><last>Voznyuk</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Evgeny</first><last>Burnaev</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Serguei</first><last>Barannikov</last><affiliation>Skolkovo Institute of Science and Technology and CNRS, Institut Mathematiques de Jussieu, Paris Diderot University</affiliation></author>
      <pages>25727-25748</pages>
      <abstract>Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2B’s residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation of obtained features. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts. The code for this paper is available at https://github.com/pyashy/SAE_ATD.</abstract>
      <url hash="f8716d01">2025.findings-acl.1321</url>
      <bibkey>kuznetsov-etal-2025-feature</bibkey>
    </paper>
    <paper id="1322">
      <title>Low-Resource Grammatical Error Correction: Selective Data Augmentation with Round-Trip Machine Translation</title>
      <author><first>Frank</first><last>Palma Gomez</last><affiliation>Boston University, Boston University and Google</affiliation></author>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <pages>25749-25770</pages>
      <abstract>Supervised state-of-the-art methods for grammatical error correction require large amounts of parallel data for training. Due to lack of gold-labeled data, techniques that create synthetic training data have become popular. We show that models trained on synthetic data tend tocorrect a limited range of grammar and spelling mistakes that involve character-level changes, but perform poorly on (more complex) phenomena that require word-level changes. We propose to address the performance gap on such errors by generating synthetic data through selective data augmentation via round-trip machine translation. We show that the proposed technique, SeLex-RT, is capable of generating mistakes that are similar to those observed with language learners. Using the approach with two types of state-of-the-art learning frameworks and two low-resource languages (Russian and Ukrainian), we achieve substantial improvements, compared to training on synthetic data produced with standard techniques. Analysis of the output reveals that models trained on data noisified with the SeLex-RT approach are capable of making word-level changes and correct lexical errors common with language learners.</abstract>
      <url hash="5f123ac3">2025.findings-acl.1322</url>
      <bibkey>palma-gomez-rozovskaya-2025-low</bibkey>
    </paper>
    <paper id="1323">
      <title>Just Put a Human in the Loop? Investigating <fixed-case>LLM</fixed-case>-Assisted Annotation for Subjective Tasks</title>
      <author><first>Hope</first><last>Schroeder</last></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>25771-25795</pages>
      <abstract>LLM use in annotation is becoming widespread, and given LLMs’ overall promising performance and speed, putting humans in the loop to simply “review” LLM annotations can be tempting. In subjective tasks with multiple plausible answers, this can impact both evaluation of LLM performance, and analysis using these labels in a social science task downstream. In a pre-registered experiment with 350 unique annotators and 7,000 annotations across 4 conditions, 2 models, and 2 datasets, we find that presenting crowdworkers with LLM-generated annotation suggestions did not make them faster annotators, but did improve their self-reported confidence in the task. More importantly, annotators strongly took the LLM suggestions, significantly changing the label distribution compared to the baseline. We show that when these labels created with LLM assistance are used to evaluate LLM performance, reported model performance significantly increases. We show how changes in label distributions as a result of LLM assistance can affect conclusions drawn by analyzing even “human-approved” LLM-annotated datasets. We believe our work underlines the importance of understanding the impact of LLM-assisted annotation on subjective, qualitative tasks, on the creation of gold data for training and testing, and on the evaluation of NLP systems on subjective tasks.</abstract>
      <url hash="67ad7f12">2025.findings-acl.1323</url>
      <bibkey>schroeder-etal-2025-just</bibkey>
    </paper>
    <paper id="1324">
      <title>Research Community Perspectives on “Intelligence” and Large Language Models</title>
      <author><first>Bertram</first><last>Højer</last></author>
      <author><first>Terne Sasha</first><last>Thorn Jakobsen</last><affiliation>Capitol Region of Denmark and Copenhagen University</affiliation></author>
      <author><first>Anna</first><last>Rogers</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Stefan</first><last>Heinrich</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>25796-25812</pages>
      <abstract>Despite the widespread use of ‘artificial intelligence’ (AI) framing in Natural Language Processing (NLP) research, it is not clear what researchers mean by ”intelligence”. To that end, we present the results of a survey on the notion of ”intelligence” among researchers and its role in the research agenda. The survey elicited complete responses from 303 researchers from a variety of fields including NLP, Machine Learning (ML), Cognitive Science, Linguistics, and Neuroscience.We identify 3 criteria of intelligence that the community agrees on the most: generalization, adaptability, &amp; reasoning.Our results suggests that the perception of the current NLP systems as ”intelligent” is a minority position (29%).Furthermore, only 16.2% of the respondents see developing intelligent systems as a research goal, and these respondents are more likely to consider the current systems intelligent.</abstract>
      <url hash="1eb6b715">2025.findings-acl.1324</url>
      <bibkey>hojer-etal-2025-research</bibkey>
    </paper>
    <paper id="1325">
      <title><fixed-case>LEMONADE</fixed-case>: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World</title>
      <author><first>Sina</first><last>Semnani</last></author>
      <author><first>Pingyue</first><last>Zhang</last></author>
      <author><first>Wanyue</first><last>Zhai</last></author>
      <author><first>Haozhuo</first><last>Li</last></author>
      <author><first>Ryan</first><last>Beauchamp</last></author>
      <author><first>Trey</first><last>Billing</last><affiliation>ACLED</affiliation></author>
      <author><first>Katayoun</first><last>Kishi</last><affiliation>Armed Conflict Location &amp; Event Data Project (ACLED)</affiliation></author>
      <author><first>Manling</first><last>Li</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>25813-25852</pages>
      <abstract>This paper presents LEMONADE, a large-scale conflict event dataset comprising 39,786 events across 20 languages and 171 countries, with extensive coverage of region-specific entities. LEMONADE is based on a partially reannotated subset of the Armed Conflict Location &amp; Event Data (ACLED), which has documented global conflict events for over a decade.To address the challenge of aggregating multilingual sources for global event analysis, we introduce abstractive event extraction (AEE) and its subtask, abstractive entity linking (AEL). Unlike conventional span-based event extraction, our approach detects event arguments and entities through holistic document understanding and normalizes them across the multilingual dataset. We evaluate various large language models (LLMs) on these tasks, adapt existing zero-shot event extraction systems, and benchmark supervised models. Additionally, we introduce ZEST, a novel zero-shot retrieval-based system for AEL.Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs outperforming specialized event extraction models such as GoLLIE. For entity linking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a state-of-the-art zero-shot baseline that achieves only 23.7%. However, these zero-shot results lag behind the best supervised systems by 20.1% and 37.0% in the end-to-end and AEL tasks, respectively, highlighting the need for further research.</abstract>
      <url hash="209d38f4">2025.findings-acl.1325</url>
      <bibkey>semnani-etal-2025-lemonade</bibkey>
    </paper>
    <paper id="1326">
      <title>Memorization vs. Reasoning: Updating <fixed-case>LLM</fixed-case>s with New Knowledge</title>
      <author><first>Aochong Oliver</first><last>Li</last><affiliation>Cornell University</affiliation></author>
      <author><first>Tanya</first><last>Goyal</last><affiliation>Cornell University</affiliation></author>
      <pages>25853-25874</pages>
      <abstract>Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. Existing methodologies and benchmarks primarily target entity substitutions, failing to capture the full breadth of complex real-world dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an automatic pipeline for simulating realistic knowledge updates reflected in an evidence corpus. KUP’s evaluation framework includes direct and indirect probes to both test memorization of updated facts and reasoning over them, for any update learning methods. Next, we present a lightweight method called memory conditioned training (MCT), which conditions tokens in the update corpus on self-generated ”memory” tokens during training. Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference. Our results on two LLM families show that (1) KUP benchmark is highly challenging, with the best CPT models achieving &lt;2% in indirect probing setting (reasoning) and (2) MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to 25.4%.</abstract>
      <url hash="4e8def61">2025.findings-acl.1326</url>
      <bibkey>li-goyal-2025-memorization</bibkey>
    </paper>
    <paper id="1327">
      <title><fixed-case>C</fixed-case>ourt<fixed-case>E</fixed-case>val: A Courtroom-Based Multi-Agent Evaluation Framework</title>
      <author><first>Sandeep</first><last>Kumar</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Abhijit A</first><last>Nargund</last></author>
      <author><first>Vivek</first><last>Sridhar</last></author>
      <pages>25875-25887</pages>
      <abstract>Automated evaluation is crucial for assessing the quality of natural language text, especially in open-ended generation tasks, given the costly and time-consuming nature of human evaluation. Existing automatic evaluation metrics like ROUGE and BLEU often show low correlation with human judgments. As large language models (LLMs) continue to evolve, researchers have explored their use as alternatives to human evaluators. Although single-agent approaches have shown potential, results indicate that further progress is required to close the gap between their performance and the quality of human assessments. Acknowledging that human evaluations involve multiple annotators, the multi-agent approach allows LLMs to collaborate, enhancing efficiency and effectiveness in handling complex tasks. In this paper, we present CourtEval, a novel Multi-Agent Evaluation Framework modeled after courtroom dynamics. Each agent takes on a distinct role: the Grader, similar to a judge, assigns an initial score; the Critic, like a prosecutor, challenges this score; and the Defender, akin to a defense attorney, defends it. Based on the input from both the Critic and Defender, the Grader re-evaluates the score, leading to a more balanced and fair final decision through this adversarial process. CourtEval substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat.</abstract>
      <url hash="ffa873bd">2025.findings-acl.1327</url>
      <bibkey>kumar-etal-2025-courteval</bibkey>
    </paper>
    <paper id="1328">
      <title>Multilingual Definition Modeling</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Erica K.</first><last>Shimomoto</last></author>
      <author><first>Alfredo</first><last>Solano</last></author>
      <author><first>Enrique</first><last>Reid</last></author>
      <pages>25888-25906</pages>
      <abstract>In this paper, we propose the first multilingual study on definition modeling. We use monolingual dictionary data for four new languages (Spanish, French, Portuguese, and German) and perform an in-depth empirical study to test the performance of pre-trained multilingual language models on definition modeling of monosemic words when finetuned on this data. Furthermore, we use a zero-shot approach to test the multilingual capabilities of two popular chat-based Large Language Models (LLMs) in the task. Results show that multilingual language models can perform on-pair with English but cannot leverage potential cross-lingual synergies, with LLMs generally offering better performance overall. A comprehensive human evaluation of the LLM-generated definition highlights the zero and few-shot capabilities of these models in this new task, also showing their shortcomings. Finally, we show that performance on our task via BERTScore strongly correlates to the performance on multilingual LLM benchmarks, suggesting that our task offers a viable compute-constrained, stable and natural alternative to these.</abstract>
      <url hash="e21dbec8">2025.findings-acl.1328</url>
      <bibkey>marrese-taylor-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="1329">
      <title>Human Bias in the Face of <fixed-case>AI</fixed-case>: Examining Human Judgment Against Text Labeled as <fixed-case>AI</fixed-case> Generated</title>
      <author><first>Tiffany</first><last>Zhu</last></author>
      <author><first>Iain</first><last>Weissburg</last></author>
      <author><first>Kexun</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>25907-25914</pages>
      <abstract>As Al advances in text generation, human trust in Al generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as “Human Generated,” over those labeled “AI Generated,” by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.</abstract>
      <url hash="bb3ea80a">2025.findings-acl.1329</url>
      <bibkey>zhu-etal-2025-human</bibkey>
    </paper>
    <paper id="1330">
      <title>Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings</title>
      <author><first>Hayato</first><last>Tsukagoshi</last></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <pages>25915-25930</pages>
      <abstract>Prompt-based text embedding models, which generate task-specific embeddings upon receiving tailored prompts, have recently demonstrated remarkable performance. However, their resulting embeddings often have thousands of dimensions, leading to high storage costs and increased computational costs of embedding-based operations. In this paper, we investigate how post-hoc dimensionality reduction applied to the embeddings affects the performance of various tasks that leverage these embeddings, specifically classification, clustering, retrieval, and semantic textual similarity (STS) tasks. Our experiments show that even a naive dimensionality reduction, which keeps only the first 25% of the dimensions of the embeddings, results in a very slight performance degradation, indicating that these embeddings are highly redundant. Notably, for classification and clustering, even when embeddings are reduced to less than 0.5% of the original dimensionality the performance degradation is very small. To quantitatively analyze this redundancy, we perform an analysis based on the intrinsic dimensionality and isotropy of the embeddings. Our analysis reveals that embeddings for classification and clustering, which are considered to have very high dimensional redundancy, exhibit lower intrinsic dimensionality and less isotropy compared with those for retrieval and STS.</abstract>
      <url hash="d8b1ccf6">2025.findings-acl.1330</url>
      <bibkey>tsukagoshi-sasano-2025-redundancy</bibkey>
    </paper>
    <paper id="1331">
      <title>Harnessing Whisper for Prosodic Stress Analysis</title>
      <author><first>Samuel S.</first><last>Sohn</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Sten</first><last>Knutsen</last></author>
      <author><first>Karin</first><last>Stromswold</last><affiliation>Rutgers University</affiliation></author>
      <pages>25931-25942</pages>
      <abstract>Prosody affects how people produce and understand language, yet studies of how it does so have been hindered by the lack of efficient tools for analyzing prosodic stress. We fine-tune OpenAI Whisper large-v2, a state-of-the-art speech recognition model, to recognize phrasal, lexical, and contrastive stress using a small, carefully annotated dataset. Our results show that Whisper can learn distinct, gender-specific stress patterns to achieve near-human and super-human accuracy in stress classification and transfer its learning from one type of stress to another, surpassing traditional machine learning models. Furthermore, we explore how acoustic context influences its performance and propose a novel black-box evaluation method for characterizing the decision boundaries used by Whisper for prosodic stress interpretation. These findings open new avenues for large-scale, automated prosody research. Models can be found at github.com/SSSohn/ProsodyBench.</abstract>
      <url hash="be2a4ab2">2025.findings-acl.1331</url>
      <bibkey>sohn-etal-2025-harnessing</bibkey>
    </paper>
    <paper id="1332">
      <title>Can You Share Your Story? Modeling Clients’ Metacognition and Openness for <fixed-case>LLM</fixed-case> Therapist Evaluation</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Dongje</first><last>Yoo</last></author>
      <author><first>Yeonjun</first><last>Hwang</last></author>
      <author><first>Minseok</first><last>Kang</last></author>
      <author><first>Namyoung</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Minju</first><last>Gwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Beong-woo</first><last>Kwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Harim</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yunjoong</first><last>Lee</last></author>
      <author><first>Min Hee</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dayi</first><last>Jung</last></author>
      <author><first>Kyong-Mee</first><last>Chung</last></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>25943-25962</pages>
      <abstract>Understanding clients’ thoughts and beliefs is fundamental in counseling, yet current evaluations of LLM therapists often fail to assess this ability. Existing evaluation methods rely on client simulators that clearly disclose internal states to the therapist, making it difficult to determine whether an LLM therapist can uncover unexpressed perspectives. To address this limitation, we introduce MindVoyager, a novel evaluation framework featuring a controllable and realistic client simulator which dynamically adapts itself based on the ongoing counseling session, offering a more realistic and challenging evaluation environment. We further introduce evaluation metrics that assess the exploration ability of LLM therapists by measuring their thorough understanding of client’s beliefs and thoughts.</abstract>
      <url hash="ca08a503">2025.findings-acl.1332</url>
      <bibkey>kim-etal-2025-share-story</bibkey>
    </paper>
    <paper id="1333">
      <title>Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries</title>
      <author><first>Haruki</first><last>Sakajo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yusuke</first><last>Ide</last></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yingtao</first><last>Tian</last><affiliation>Sakana AI</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>25963-25976</pages>
      <abstract>Cross-lingual vocabulary transfer plays a promising role in adapting pre-trained language models to new languages, including low-resource languages.Existing approaches that utilize monolingual or parallel corpora face challenges when applied to languages with limited resources.In this work, we propose a simple yet effective vocabulary transfer method that utilizes bilingual dictionaries, which are available for many languages, thanks to descriptive linguists.Our proposed method leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords.The embeddings of target subwords are estimated iteratively by progressively removing them from the tokenizer.The experimental results show that our approach outperforms existing methods for low-resource languages, demonstrating the effectiveness of a dictionary-based approach for cross-lingual vocabulary transfer.</abstract>
      <url hash="915bbc12">2025.findings-acl.1333</url>
      <bibkey>sakajo-etal-2025-dictionaries</bibkey>
    </paper>
    <paper id="1334">
      <title>When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using <fixed-case>G</fixed-case>rad<fixed-case>N</fixed-case>orm<fixed-case>IR</fixed-case></title>
      <author><first>Dayoon</first><last>Ko</last></author>
      <author><first>Jinyoung</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sohyeon</first><last>Kim</last></author>
      <author><first>Jinhyuk</first><last>Kim</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Jaehoon</first><last>Lee</last></author>
      <author><first>Seonghak</first><last>Song</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Minyoung</first><last>Lee</last><affiliation>Samsung SDS</affiliation></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>25977-25996</pages>
      <abstract>Dense retrievers encode texts into embeddings to efficiently retrieve relevant documents from large databases in response to user queries. However, real-world corpora continually evolve, leading to a shift from the original training distribution of the retriever. Without timely updates or retraining, indexing newly emerging documents can degrade retrieval performance for future queries. Thus, identifying when a dense retriever requires an update is critical for maintaining robust retrieval systems. In this paper, we propose a novel task of predicting whether a corpus is out-of-distribution (OOD) relative to a dense retriever before indexing. Addressing this task allows us to proactively manage retriever updates, preventing potential retrieval failures. We introduce GradNormIR, an unsupervised approach that leverages gradient norms to detect OOD corpora effectively. Experiments on the BEIR benchmark demonstrate that GradNormIR enables timely updates of dense retrievers in evolving document collections, significantly enhancing retrieval robustness and efficiency.</abstract>
      <url hash="064ac595">2025.findings-acl.1334</url>
      <bibkey>ko-etal-2025-dense</bibkey>
    </paper>
    <paper id="1335">
      <title>The Million Authors Corpus: A Cross-Lingual and Cross-Domain <fixed-case>W</fixed-case>ikipedia Dataset for Authorship Verification</title>
      <author><first>Abraham</first><last>Israeli</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Shuai</first><last>Liu</last><affiliation>University of Southern California, Information Sciences Institute</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>25997-26017</pages>
      <abstract>Authorship verification (AV) is a crucial task for applications like identity verification, plagiarism detection, and AI-generated text identification. However, datasets for training and evaluating AV models are primarily in English and primarily in a single domain. This precludes analysis of AV techniques for generalizability and can cause seemingly valid AV solutions to, in fact, rely on topic-based features rather than actual authorship features. To address this limitation, we introduce the Million Authors Corpus (), a novel dataset encompassing contributions from dozens of languages on Wikipedia. It includes only long and contiguous textual chunks taken from Wikipedia edits and links those texts to their authors. includes 60.08M textual chunks, contributed by 1.29M Wikipedia authors. It enables broad-scale cross-lingual and cross-domain AV evaluation to ensure accurate analysis of model capabilities that are not overly optimistic. We provide baseline evaluations using state-of-the-art AV models as well as information retrieval models that are not AV-specific in order to demonstrate ‘s unique cross-lingual and cross-domain ablation capabilities.</abstract>
      <url hash="375e7bda">2025.findings-acl.1335</url>
      <bibkey>israeli-etal-2025-million</bibkey>
    </paper>
    <paper id="1336">
      <title><fixed-case>B</fixed-case>rid<fixed-case>G</fixed-case> <fixed-case>MT</fixed-case>: Enhancing <fixed-case>LLM</fixed-case>s’ Machine Translation Capabilities with Sentence Bridging and Gradual <fixed-case>MT</fixed-case></title>
      <author><first>Seungwoo</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Gahyun</first><last>Yoo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jay-Yoon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>26018-26042</pages>
      <abstract>Recent Large Language Models (LLMs) have demonstrated impressive translation performance without requiring fine-tuning on additional parallel corpora. However, they still face significant challenges in certain scenarios, particularly when translating low-resource languages. A common approach to address this issue is to provide external knowledge, such as few-shot examples, to assist LLMs in translating specific source sentences. However, this method is fundamentally limited by the quality or quantity of relevant sources, which cannot always be guaranteed. To reduce LLMs’ reliance on external sources, we propose BridG MT, a method that combines Sentence Bridging, which generates a sequence of sentences as a bridge that gradually transition from easy-to-translate to more difficult, and Gradual MT, which sequentially translates these sentences using earlier translations as few-shot examples for subsequent ones. Experiments conducted on four LLMs across seven languages demonstrate that our method effectively enhances translation performance, even outperforming translation methods that rely on a large number of few-shot examples.</abstract>
      <url hash="36dd90b9">2025.findings-acl.1336</url>
      <bibkey>choi-etal-2025-bridg</bibkey>
    </paper>
    <paper id="1337">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>W</fixed-case>orld: Benchmarking Large Language Models for Symbolic World Model Generation</title>
      <author><first>Mengkang</first><last>Hu</last></author>
      <author><first>Tianxing</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yude</first><last>Zou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yuheng</first><last>Lei</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Ming</first><last>Li</last></author>
      <author><first>Yao</first><last>Mu</last></author>
      <author><first>Hongyuan</first><last>Zhang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Wenqi</first><last>Shao</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>26043-26066</pages>
      <abstract>Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models.</abstract>
      <url hash="91e67dc1">2025.findings-acl.1337</url>
      <bibkey>hu-etal-2025-text2world</bibkey>
    </paper>
    <paper id="1338">
      <title>Blinded by Context: Unveiling the Halo Effect of <fixed-case>MLLM</fixed-case> in <fixed-case>AI</fixed-case> Hiring</title>
      <author><first>Kyusik</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jeongwoo</first><last>Ryu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyeonseok</first><last>Jeon</last></author>
      <author><first>Bongwon</first><last>Suh</last><affiliation>Seoul National University</affiliation></author>
      <pages>26067-26113</pages>
      <abstract>This study investigates the halo effect in AI-driven hiring evaluations using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Through experiments with hypothetical job applications, we examined how these models’ evaluations are influenced by non-job-related information, including extracurricular activities and social media images. By analyzing models’ responses to Likert-scale questions across different competency dimensions, we found that AI models exhibit significant halo effects, particularly in image-based evaluations, while text-based assessments showed more resistance to bias. The findings demonstrate that supplementary multimodal information can substantially influence AI hiring decisions, highlighting potential risks in AI-based recruitment systems.</abstract>
      <url hash="ac3b841c">2025.findings-acl.1338</url>
      <bibkey>kim-etal-2025-blinded</bibkey>
    </paper>
    <paper id="1339">
      <title><fixed-case>C</fixed-case>o<fixed-case>T</fixed-case>-<fixed-case>UQ</fixed-case>: Improving Response-wise Uncertainty Quantification in <fixed-case>LLM</fixed-case>s with Chain-of-Thought</title>
      <author><first>Boxuan</first><last>Zhang</last><affiliation>Purdue University</affiliation></author>
      <author><first>Ruqi</first><last>Zhang</last><affiliation>Purdue University</affiliation></author>
      <pages>26114-26133</pages>
      <abstract>Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which leads to inefficiency. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we introduce a novel approach to quantify response-wise uncertainty by integrating LLMs’ inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. Our CoT-UQ framework captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. The uncertainty scores of keywords are then aggregated based on their significance to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods.</abstract>
      <url hash="9f0e399b">2025.findings-acl.1339</url>
      <bibkey>zhang-zhang-2025-cot</bibkey>
    </paper>
    <paper id="1340">
      <title><fixed-case>ADO</fixed-case>: Automatic Data Optimization for Inputs in <fixed-case>LLM</fixed-case> Prompts</title>
      <author><first>Sam</first><last>Lin</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last></author>
      <author><first>Lingyao</first><last>Li</last></author>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>26134-26146</pages>
      <abstract>This study explores a novel approach to enhance the performance of Large Language Models (LLMs) through the optimization of input data within prompts. While previous research has primarily focused on refining instruction components and augmenting input data with in-context examples, our work investigates the potential benefits of optimizing the input data itself. We introduce a two-pronged strategy for input data optimization: content engineering and structural reformulation. Content engineering involves imputing missing values, removing irrelevant attributes, and enriching profiles by generating additional information inferred from existing attributes. Subsequent to content engineering, structural reformulation is applied to optimize the presentation of the modified content to LLMs, given their sensitivity to input format. Our findings suggest that these optimizations can significantly improve the performance of LLMs in various tasks, offering a promising avenue for future research in prompt engineering. The source code is available at https://github.com/glin2229/Automatic-Data-Optimization.</abstract>
      <url hash="b8118d2f">2025.findings-acl.1340</url>
      <bibkey>lin-etal-2025-ado</bibkey>
    </paper>
    <paper id="1341">
      <title>Large Language Models Still Exhibit Bias in Long Text</title>
      <author><first>Wonje</first><last>Jeung</last></author>
      <author><first>Dongjae</first><last>Jeon</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Ashkan</first><last>Yousefpour</last></author>
      <author><first>Jonghyun</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <pages>26147-26169</pages>
      <abstract>Existing fairness benchmarks for large language models (LLMs) primarily focus on simple tasks, such as multiple-choice questions, overlooking biases that may arise in more complex scenarios like long-text generation. To address this gap, we introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates biases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10 demographic axes, including gender and race, resulting in 11,948 samples. By assessing both model responses and the reasoning behind them, LTF-TEST uncovers subtle biases that are difficult to detect in simple responses. In our evaluation of five recent LLMs, including GPT-4o and LLaMA3, we identify two key patterns of bias. First, these models frequently favor certain demographic groups in their responses. Second, they show excessive sensitivity toward traditionally disadvantaged groups, often providing overly protective responses while neglecting others. To mitigate these biases, we propose REGARD-FT, a finetuning approach that pairs biased prompts with neutral responses. REGARD-FT reduces gender bias by 34.6% and improves performance by 1.4 percentage points on the BBQ benchmark, offering a promising approach to addressing biases in long-text generation tasks.</abstract>
      <url hash="8c742f6a">2025.findings-acl.1341</url>
      <bibkey>jeung-etal-2025-large</bibkey>
    </paper>
    <paper id="1342">
      <title>Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</title>
      <author><first>Qiyue</first><last>Gao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xinyu</first><last>Pi</last></author>
      <author><first>Kevin</first><last>Liu</last></author>
      <author><first>Junrong</first><last>Chen</last></author>
      <author><first>Ruolan</first><last>Yang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Xinqi</first><last>Huang</last></author>
      <author><first>Xinyu</first><last>Fang</last></author>
      <author><first>Lu</first><last>Sun</last></author>
      <author><first>Gautham</first><last>Kishore</last></author>
      <author><first>Bo</first><last>Ai</last></author>
      <author><first>Stone</first><last>Tao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Mengyang</first><last>Liu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jiaxi</first><last>Yang</last><affiliation>Cornell University</affiliation></author>
      <author><first>Chao-Jung</first><last>Lai</last></author>
      <author><first>Chuanyang</first><last>Jin</last></author>
      <author><first>Jiannan</first><last>Xiang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Benhao</first><last>Huang</last></author>
      <author><first>Zeming</first><last>Chen</last></author>
      <author><first>David</first><last>Danks</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hao</first><last>Su</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Tianmin</first><last>Shu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ziqiao</first><last>Ma</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Zhiting</first><last>Hu</last><affiliation>University of California, San Diego and Amazon</affiliation></author>
      <pages>26170-26195</pages>
      <abstract>Internal world models (WMs) enable agents to understand the world’s state and predict transitions, serving as the basis for advanced deliberative reasoning.Recent large Vision-Language Models (VLMs), such as GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs’ fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses **perception** (visual, spatial, temporal, quantitative, and motion) and **prediction** (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce **WM-ABench**, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding—e.g., they tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.</abstract>
      <url hash="ffe01297">2025.findings-acl.1342</url>
      <bibkey>gao-etal-2025-vision</bibkey>
    </paper>
    <paper id="1343">
      <title>Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents</title>
      <author><first>Ivoline C.</first><last>Ngong</last><affiliation>University of Vermont</affiliation></author>
      <author><first>Swanand Ravindra</first><last>Kadhe</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>RedHat AI &amp; MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Justin D.</first><last>Weisz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Amit</first><last>Dhurandhar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last><affiliation>International Business Machines</affiliation></author>
      <pages>26196-26220</pages>
      <abstract>Conversational agents are increasingly woven into individuals’ personal lives, yet users often underestimate the privacy risks associated with them. The moment users share information with these agents —such as large language models (LLMs)— their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLM-based Conversational Agents (LCAs). It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LCAs (untrusted receivers). Through a formative design user study, we observe how even “privacy-conscious” users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally deployable framework that operates between users and LCAs, identifying and reformulating out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user’s intended interaction goals. Notably, about 76% of participants in our human evaluation preferred the reformulated prompts over the original ones, validating the usability and effectiveness of contextual privacy in our proposed framework. We open source the code at https://github.com/IBM/contextual-privacy-LLM.</abstract>
      <url hash="dccf092c">2025.findings-acl.1343</url>
      <bibkey>ngong-etal-2025-protecting</bibkey>
    </paper>
    <paper id="1344">
      <title>Enhancing Persona Consistency for <fixed-case>LLM</fixed-case>s’ Role-Playing using Persona-Aware Contrastive Learning</title>
      <author><first>Ke</first><last>Ji</last></author>
      <author><first>Yixin</first><last>Lian</last></author>
      <author><first>Linxu</first><last>Li</last></author>
      <author><first>Jingsheng</first><last>Gao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Weiyuan</first><last>Li</last></author>
      <author><first>Bin</first><last>Dai</last><affiliation>XiaoIce</affiliation></author>
      <pages>26221-26238</pages>
      <abstract>In recent years, large language models (LLMs) have achieved breakthrough progress in many dialogue generation tasks. However, their lack of emotion and fine-grained role awareness limits the model’s ability to provide personalized and diverse interactions further. Current methods face high costs in collecting high-quality annotated data for scenarios such as role-playing, and traditional human alignment methods are difficult to deploy due to the inherent diversity of model behavior in role-playing scenarios. Inspired by the alignment of models for safety behaviors through RLHF (Reinforcement Learning from Human Feedback), in this paper, we revisit model role-playing behavior from the perspective of persona alignment and propose a novel annotation-free framework named Persona-Aware Contrastive Learning (PCL) to align LLMs’ behavior during role-playing, enhancing the model’s role consistency. Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency. Then, we further enhance the model’s role-playing strategy through iterative adversarial modeling between the use of role characteristics and not. Experiments on both black-box and white-box LLMs show that LLMs equipped with PCL significantly outperform vanilla LLMs under automatic evaluation methods (CharEval &amp; GPT-4) and human expert evaluation.</abstract>
      <url hash="28657adc">2025.findings-acl.1344</url>
      <bibkey>ji-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="1345">
      <title>M<tex-math>^{2}</tex-math>-<fixed-case>T</fixed-case>ab<fixed-case>F</fixed-case>act: Multi-Document Multi-Modal Fact Verification with Visual and Textual Representations of Tabular Data</title>
      <author><first>Mingyang</first><last>Zhou</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Lingyu</first><last>Zhang</last><affiliation>Duke University</affiliation></author>
      <author><first>Sophia</first><last>Horng</last><affiliation>Columbia University</affiliation></author>
      <author><first>Maximillian</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shih-Fu</first><last>Chang</last><affiliation>Columbia University and Columbia University</affiliation></author>
      <pages>26239-26256</pages>
      <abstract>Tabular data is used to store information in many real-world systems ranging from finance to healthcare. However, such structured data is often communicated to humans in visually interpretable formats (e.g. charts and textual paragraphs), making it imperative that fact-checking models should be able to reason over multiple pieces of structured evidence presented across different modalities. In this paper, we propose Multi-Document Multi-Modal Table-based Fact Verification (M<tex-math>^{2}</tex-math>-TabFact), a challenging fact verification task that requires jointly reasoning over visual and textual representations of structured data. We design an automatic data generation pipeline that converts existing tabular data into descriptive visual and textual evidence. We then use Large Language Models to generate complex claims that depend on multi-document, multi-modal evidence. In total, we create 8,856 pairs of complex claims and multi-modal evidence through this procedure and systematically evaluate M<tex-math>^{2}</tex-math>-TabFact with a set of strong vision-language models (VLM). We find that existing VLMs have large gaps in fact verification performance compared to humans. Moreover, we find that they are imbalanced when it comes to their ability to handle reason about different modalities, and currently struggle to reason about information extracted from multiple documents.</abstract>
      <url hash="0ec31c3b">2025.findings-acl.1345</url>
      <bibkey>zhou-etal-2025-m2</bibkey>
    </paper>
    <paper id="1346">
      <title>Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff</title>
      <author><first>Maximilian</first><last>Holsman</last><affiliation>Department of Computer Science, Duke University</affiliation></author>
      <author><first>Yukun</first><last>Huang</last><affiliation>Duke University</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>26257-26273</pages>
      <abstract>Speculative Decoding (SD) enforces strict distributional equivalence to the target model when accepting candidate tokens. While it maintains the target model’s generation quality, this strict equivalence limits the speedup achievable by SD and prevents users from trading deviations from the target distribution in exchange for further inference speed gains. To address these limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding algorithm that generalizes SD by accepting candidate tokens based on the divergences between the target and draft model distributions. By allowing for controlled divergence from the target model, FSD enables users to flexibly trade generation quality for inference speed. Across several benchmarks, our method is able to achieve significant runtime improvements of over 5 tokens per second faster than SD at only an approximate 2% absolute reduction in benchmark accuracy. In many cases, FSD is even able to match SD benchmark accuracy at over 2 tokens per second faster, demonstrating that distributional equivalence is not necessary to maintain target model performance. Furthermore, FSD can be seamlessly integrated into existing SD extensions; we demonstrate this by applying FSD to EAGLE-2, greatly enhancing this existing extension’s efficiency while allowing it to leverage FSD’s tunable quality-speed trade-off.</abstract>
      <url hash="3c4d80b2">2025.findings-acl.1346</url>
      <bibkey>holsman-etal-2025-fuzzy</bibkey>
    </paper>
    <paper id="1347">
      <title><fixed-case>PLAY</fixed-case>2<fixed-case>PROMPT</fixed-case>: Zero-shot Tool Instruction Optimization for <fixed-case>LLM</fixed-case> Agents via Tool Play</title>
      <author><first>Wei</first><last>Fang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kaizhi</first><last>Qian</last><affiliation>International Business Machines</affiliation></author>
      <author><first>James R.</first><last>Glass</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yada</first><last>Zhu</last></author>
      <pages>26274-26290</pages>
      <abstract>Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically “plays” with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.</abstract>
      <url hash="332bea65">2025.findings-acl.1347</url>
      <bibkey>fang-etal-2025-play2prompt</bibkey>
    </paper>
    <paper id="1348">
      <title>Towards the Pedagogical Steering of Large Language Models for Tutoring: A Case Study with Modeling Productive Failure</title>
      <author><first>Romain</first><last>Puech</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jakub</first><last>Macina</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Julia</first><last>Chatain</last><affiliation>Singapore-ETH Centre</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Manu</first><last>Kapur</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>26291-26311</pages>
      <abstract>One-to-one tutoring is one of the most efficient methods of teaching. With the growing popularity of Large Language Models (LLMs), there have been efforts to create LLM-based conversational tutors which can expand the benefits of one-to-one tutoring to everyone. However, current LLMs are trained primarily to be helpful assistants and lack crucial pedagogical skills. For example, they often quickly reveal the solution to the student and fail to plan for a richer multi-turn pedagogical interaction.To use LLMs in pedagogical settings, they need to be steered to use effective teaching strategies: a problem we introduce as Pedagogical Steering. We develop StratL, an algorithm to optimize LLM prompts and steer it to follow a predefined multi-turn tutoring plan represented as a transition graph.As a case study, we create a prototype tutor for high school math following Productive Failure (PF), an advanced and effective learning design. To validate our approach in a real-world setting, we run a field study with 17 high school students in Singapore and show that StratL succeeds in steering the LLM to follow the PF tutoring strategy. Finally, we highlight challenges in Pedagogical Steering of LLMs and offer opportunities for further improvements by publishing a dataset of PF problems and our code.</abstract>
      <url hash="774f5a09">2025.findings-acl.1348</url>
      <bibkey>puech-etal-2025-towards</bibkey>
    </paper>
    <paper id="1349">
      <title>Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation</title>
      <author><first>Jisu</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Juhyun</first><last>Oh</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>26312-26332</pages>
      <abstract>Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.</abstract>
      <url hash="1bbe3493">2025.findings-acl.1349</url>
      <bibkey>shin-etal-2025-spotting</bibkey>
    </paper>
    <paper id="1350">
      <title>What Language Do Non-<fixed-case>E</fixed-case>nglish-Centric Large Language Models Think in?</title>
      <author><first>Chengzhi</first><last>Zhong</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Junfeng</first><last>Jiang</last><affiliation>NII, Tokyo Institute of Technology</affiliation></author>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yugo</first><last>Murawaki</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>National Institute of Informatics (NII) and Kyoto University</affiliation></author>
      <pages>26333-26346</pages>
      <abstract>In this study, we investigate whether non-English-centric large language models, ‘think’ in their specialized language. Specifically, we analyze how intermediate layer representations, when projected into the vocabulary space, favor certain languages during generation—termed as latent languages. We categorize non-English-centric models into two groups: CPMs, which are English-centric models with continued pre-training on its specialized language, and BLMs, which are pre-trained on a balanced mix of multiple languages from scratch. Our findings reveal that while English-centric models rely exclusively on English as their latent language, non-English-centric models activate multiple latent languages, dynamically selecting the most similar one based on both the source and target languages. This also influences responses to culture difference questions, reducing English-centric biases in non-English models. This study deepens our understanding of language representation in non-English-centric LLMs, shedding light on the intricate dynamics of multilingual processing at the representational level.</abstract>
      <url hash="7f92e1bb">2025.findings-acl.1350</url>
      <bibkey>zhong-etal-2025-language</bibkey>
    </paper>
    <paper id="1351">
      <title><tex-math>T^5Score</tex-math>: A Methodology for Automatically Assessing the Quality of <fixed-case>LLM</fixed-case> Generated Multi-Document Topic Sets</title>
      <author><first>Itamar</first><last>Trainin</last></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>26347-26375</pages>
      <abstract>Using LLMs for Multi-Document Topic Extraction has recently gained popularity due to their apparent high-quality outputs, expressiveness, and ease of use. However, most existing evaluation practices are not designed for LLM-generated topics and result in low inter-annotator agreement scores, hindering the reliable use of LLMs for the task. To address this, we introduce <tex-math>T^5Score</tex-math>, an evaluation methodology that decomposes the quality of a topic set into quantifiable aspects, measurable through easy-to-perform annotation tasks. This framing enables a convenient, manual or automatic, evaluation procedure resulting in a strong inter-annotator agreement score.To substantiate our methodology and claims, we perform extensive experimentation on multiple datasets and report the results.</abstract>
      <url hash="65bf4ed0">2025.findings-acl.1351</url>
      <bibkey>trainin-abend-2025-t5score</bibkey>
    </paper>
    <paper id="1352">
      <title>Uncertainty-Aware Contrastive Decoding</title>
      <author><first>Hakyung</first><last>Lee</last></author>
      <author><first>Subeen</first><last>Park</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Joowang</first><last>Kim</last><affiliation>LGCNS</affiliation></author>
      <author><first>Sungjun</first><last>Lim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Kyungwoo</first><last>Song</last><affiliation>Yonsei University</affiliation></author>
      <pages>26376-26391</pages>
      <abstract>Large language models excel in a wide range of natural language processing tasks, but generating factually accurate and consistent outputs remains a challenge. To improve text reliability, Contrastive Decoding (CD) refines token selection by leveraging differences between an expert and base model, penalizing low-quality token choices. However, CD employs static weighting between models, making it sensitive to variations in model architecture and input characteristics, often resulting in suboptimal token selection and error propagation throughout generation. We propose Uncertainty-Aware Contrastive Decoding (UCD), a method that dynamically adjusts model contributions at each decoding step based on uncertainty. We introduce a cumulative energy function, where uncertainty is quantified as the negative log-sum-exp over logits, and decomposed into entropy and expected logit components. This energy serves as a dynamic confidence signal, guiding adaptive model weighting during generation. We demonstrate through extensive experiments that UCD significantly improves factual accuracy and reliability over existing decoding methods. Finally, we provide a theoretical analysis showing that our energy function serves as a well-defined uncertainty metric capturing model confidence. Our code is available at: https://github.com/MLAI-Yonsei/UCD.</abstract>
      <url hash="5fad601b">2025.findings-acl.1352</url>
      <bibkey>lee-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="1353">
      <title><fixed-case>GEMS</fixed-case>: Generation-Based Event Argument Extraction via Multi-perspective Prompts and Ontology Steering</title>
      <author><first>Run</first><last>Lin</last></author>
      <author><first>Yao</first><last>Liu</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Yanglei</first><last>Gan</last></author>
      <author><first>Yuxiang</first><last>Cai</last></author>
      <author><first>Tian</first><last>Lan</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Qiao</first><last>Liu</last><affiliation>UESTC</affiliation></author>
      <pages>26392-26409</pages>
      <abstract>Generative methods significantly advance event argument extraction by probabilistically generating event argument sequences in a structured format. However, existing approaches primarily rely on a single prompt to generate event arguments in a fixed, predetermined order. Such a rigid approach overlooks the complex structural and dynamic interdependencies among event arguments. In this work, we present GEMS, a multi-prompt learning framework that Generates Event arguments via Multi-perspective prompts and ontology Steering. Specifically, GEMS utilizes multiple unfilled prompts for each sentence, predicting event arguments in varying sequences to explicitly capture the interrelationships between arguments. These predictions are subsequently aggregated using a voting mechanism. Furthermore, an ontology-driven steering mechanism is proposed to ensure that the generated arguments are contextually appropriate and consistent with event-specific knowledge. Extensive experiments on two benchmark datasets demonstrate that GEMS achieves state-of-the-art performance, particularly in low-resource settings. The source code is available at: https://github.com/AONE-NLP/EAE-GEMS</abstract>
      <url hash="37aa66c3">2025.findings-acl.1353</url>
      <bibkey>lin-etal-2025-gems</bibkey>
    </paper>
    <paper id="1354">
      <title><fixed-case>R</fixed-case>oman<fixed-case>L</fixed-case>ens: The Role Of Latent <fixed-case>R</fixed-case>omanization In Multilinguality In <fixed-case>LLM</fixed-case>s</title>
      <author><first>Alan</first><last>Saji</last><affiliation>ai4bharat</affiliation></author>
      <author><first>Jaavid Aktar</first><last>Husain</last></author>
      <author><first>Thanmay</first><last>Jayakumar</last></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Anoop</first><last>Kunchukuttan</last><affiliation>Microsoft and Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Ratish</first><last>Puduppully</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>26410-26429</pages>
      <abstract>Large Language Models (LLMs) exhibit strong multilingual performance despite being predominantly trained on English-centric corpora. This raises a fundamental question: How do LLMs achieve such multilingual capabilities? Focusing on languages written in non-Roman scripts, we investigate the role of Romanization—the representation of non-Roman scripts using Roman characters—as a potential bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in Romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and Romanized scripts, suggesting a shared underlying representation. Additionally, for translation into non-Roman script languages, our findings reveal that when the target language is in Romanized form, its representations emerge earlier in the model’s layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of Romanization in facilitating language transfer.</abstract>
      <url hash="394904d1">2025.findings-acl.1354</url>
      <bibkey>saji-etal-2025-romanlens</bibkey>
    </paper>
    <paper id="1355">
      <title>7 Points to <fixed-case>T</fixed-case>singhua but 10 Points to ? Assessing Large Language Models in Agentic Multilingual National Bias</title>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Katrina Qiyao</first><last>Wang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>National Institute of Informatics (NII) and Kyoto University</affiliation></author>
      <pages>26430-26442</pages>
      <abstract>Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM’s applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation.We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal significant biases in both the scores and the reasoning structure of non-English languages. We also draw future implications for improving multilingual alignment in AI systems.</abstract>
      <url hash="df8f109d">2025.findings-acl.1355</url>
      <bibkey>liu-etal-2025-7</bibkey>
    </paper>
    <paper id="1356">
      <title>Search-in-Context: Efficient Multi-Hop <fixed-case>QA</fixed-case> over Long Contexts via <fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Tree Search with Dynamic <fixed-case>KV</fixed-case> Retrieval</title>
      <author><first>Jiabei</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guang</first><last>Liu</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kun</first><last>Luo</last></author>
      <author><first>Yao</first><last>Xu</last></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>26443-26455</pages>
      <abstract>Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, such as math problem-solving and code generation. However, multi-hop question answering (MHQA) over long contexts, which demands both robust knowledge-intensive reasoning and efficient processing of lengthy documents, remains a significant challenge. Existing approaches often struggle to balance these requirements, either neglecting explicit reasoning or incurring expensive computational costs due to full-attention mechanisms over long contexts. To address this, we propose **Search-in-Context (SIC)**, a novel framework that integrates Monte Carlo Tree Search (MCTS) with dynamic key-value (KV) retrieval to enable iterative, context-aware reasoning. SIC dynamically retrieves critical KV pairs (e.g., 4K tokens) at each step, prioritizing relevant evidence while mitigating the “lost in the middle” problem. Furthermore, the paper introduces a Process-Reward Model (PRM) trained on auto-labeled data to guide the MCTS process with stepwise rewards, promoting high-quality reasoning trajectories without manual annotation. Experiments on three long-context MHQA benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue) and a counterfactual multi-hop dataset demonstrate SIC’s superiority, achieving state-of-the-art performance while significantly reducing computational overhead.</abstract>
      <url hash="1f84bc56">2025.findings-acl.1356</url>
      <bibkey>chen-etal-2025-search</bibkey>
    </paper>
    <paper id="1357">
      <title><fixed-case>LLM</fixed-case>-as-an-Interviewer: Beyond Static Testing Through Dynamic <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Eunsu</first><last>Kim</last></author>
      <author><first>Juyoung</first><last>Suk</last></author>
      <author><first>Seungone</first><last>Kim</last></author>
      <author><first>Niklas</first><last>Muennighoff</last><affiliation>Stanford University, Contextual AI and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dongkwan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>26456-26493</pages>
      <abstract>We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the reasoning, factuality and instruction-following tasks. Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs. Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM’s strengths and weaknesses. This report offers a detailed snapshot of the model’s real-world applicability.</abstract>
      <url hash="c539fd5f">2025.findings-acl.1357</url>
      <bibkey>kim-etal-2025-llm-interviewer</bibkey>
    </paper>
    <paper id="1358">
      <title><fixed-case>I</fixed-case>ntention<fixed-case>ESC</fixed-case>: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems</title>
      <author><first>Xinjie</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qin</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <pages>26494-26516</pages>
      <abstract>In emotional support conversations, unclear intentions can lead supporters to employ inappropriate strategies, inadvertently imposing their expectations or solutions on the seeker. Clearly defined intentions are essential for guiding both the supporter’s motivations and the overall emotional support process. In this paper, we propose the Intention-centered Emotional Support Conversation (IntentionESC) framework, which defines the possible intentions of supporters in emotional support conversations, identifies key emotional state aspects for inferring these intentions, and maps them to appropriate support strategies. While Large Language Models (LLMs) excel in text generating, they fundamentally operate as probabilistic models trained on extensive datasets, lacking a true understanding of human thought processes and intentions. To address this limitation, we introduce the Intention CEntric Chain-of-Thought (ICECoT) mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional states, inferring intentions, and selecting suitable support strategies, thereby generating more effective emotional support responses. To train the model with ICECoT and integrate expert knowledge, we design an automated annotation pipeline that produces high-quality training data. Furthermore, we develop a comprehensive evaluation scheme to assess emotional support efficacy and conduct extensive experiments to validate our framework. Our data and code will be publically released to facilitate further research.</abstract>
      <url hash="da1261f7">2025.findings-acl.1358</url>
      <bibkey>zhang-etal-2025-intentionesc</bibkey>
    </paper>
    <paper id="1359">
      <title>Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models</title>
      <author><first>Gerard Christopher</first><last>Yeo</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Kokil</first><last>Jaidka</last><affiliation>National University of Singapore</affiliation></author>
      <pages>26517-26525</pages>
      <abstract>Datasets used for emotion recognition tasks typically contain overt cues that can be used in predicting the emotions expressed in a text. However, one challenge is that texts sometimes contain covert contextual cues that are rich in affective semantics, which warrant higher-order reasoning abilities to infer emotional states, not simply the emotions conveyed. This study advances beyond surface-level perceptual features to investigate how large language models (LLMs) reason about others’ emotional states using contextual information, within a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal Theory, we curate a specialized ToM evaluation dataset to assess both forward reasoning—from context to emotion—and backward reasoning—from emotion to inferred context. We showed that LLMs can reason to a certain extent, although they are poor at associating situational outcomes and appraisals with specific emotions. Our work highlights the need for psychological theories in the training and evaluation of LLMs in the context of emotion reasoning.</abstract>
      <url hash="76eb4fd7">2025.findings-acl.1359</url>
      <bibkey>yeo-jaidka-2025-beyond</bibkey>
    </paper>
    <paper id="1360">
      <title><fixed-case>CSTRL</fixed-case>: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization</title>
      <author><first>Mst. Fahmida Sultana</first><last>Naznin</last></author>
      <author><first>Adnan Ibney</first><last>Faruq</last></author>
      <author><first>Mostafa Rifat</first><last>Tazwar</last></author>
      <author><first>Md</first><last>Jobayer</last><affiliation>BRAC University</affiliation></author>
      <author><first>Md. Mehedi Hasan</first><last>Shawon</last><affiliation>BRAC University</affiliation></author>
      <author><first>Md Rakibul</first><last>Hasan</last><affiliation>Curtin University of Technology and BRAC University, Bangladesh</affiliation></author>
      <pages>26526-26537</pages>
      <abstract>A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists’ workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL — Context-driven Sequential TRansfer Learning — achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.</abstract>
      <url hash="a58b380c">2025.findings-acl.1360</url>
      <bibkey>naznin-etal-2025-cstrl</bibkey>
    </paper>
    <paper id="1361">
      <title>Rethinking Prompt-based Debiasing in Large Language Model</title>
      <author><first>Xinyi</first><last>Yang</last></author>
      <author><first>Runzhe</first><last>Zhan</last><affiliation>University of Macau</affiliation></author>
      <author id="shu-yang"><first>Shu</first><last>Yang</last></author>
      <author><first>Junchao</first><last>Wu</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia S.</first><last>Chao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <pages>26538-26553</pages>
      <abstract>Investigating bias in large language models (LLMs) is crucial for developing trustworthy AI. While prompt-based through prompt engineering is common, its effectiveness relies on the assumption that models inherently understand biases. Our study systematically analyzed this assumption using the BBQ and StereoSet benchmarks on both open-source models as well as commercial GPT model. Experimental results indicate that prompt-based is often superficial; for instance, the Llama2-7B-Chat model misclassified over 90% of unbiased content as biased, despite achieving high accuracy in identifying bias issues on the BBQ dataset. Additionally, specific evaluation and question settings in bias benchmarks often lead LLMs to choose “evasive answers”, disregarding the core of the question and the relevance of the response to the context. Moreover, the apparent success of previous methods may stem from flawed evaluation metrics. Our research highlights a potential “false prosperity” in prompt-base efforts and emphasizes the need to rethink bias evaluation metrics to ensure truly trustworthy AI. We will release our data and code upon acceptance.</abstract>
      <url hash="80566a9c">2025.findings-acl.1361</url>
      <bibkey>yang-etal-2025-rethinking-prompt</bibkey>
    </paper>
    <paper id="1362">
      <title>Exploring In-context Example Generation for Machine Translation</title>
      <author><first>Dohyun</first><last>Lee</last></author>
      <author><first>Seungil Chad</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Chanwoo</first><last>Yang</last></author>
      <author><first>Yujin</first><last>Baek</last><affiliation>KAIST</affiliation></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>26554-26568</pages>
      <abstract>Large language models (LLMs) have demonstrated strong performance across various tasks, leveraging their exceptional in-context learning ability with only a few examples.Accordingly, the selection of optimal in-context examples has been actively studied in the field of machine translation.However, these studies presuppose the presence of a demonstration pool with human-annotated pairs, making them less applicable to low-resource languages where such an assumption is challenging to meet.To overcome this limitation, this paper explores the research direction of in-context example generation for machine translation.Specifically, we propose Demonstration Augmentation for Translation (DAT), a simple yet effective approach that generates example pairs without relying on any external resources.This method builds upon two prior criteria, relevance and diversity, which have been highlighted in previous work as key factors for in-context example selection.Through experiments and analysis on low-resource languages where human-annotated pairs are scarce, we show that DAT achieves superior translation quality compared to the baselines.Furthermore, we investigate the potential of progressively accumulating generated pairs during test time to build and reuse a demonstration pool. Our implementation is publicly available at https://github.com/aiclaudev/DAT.</abstract>
      <url hash="2f55ef63">2025.findings-acl.1362</url>
      <bibkey>lee-etal-2025-exploring</bibkey>
    </paper>
    <paper id="1363">
      <title>Knowledge Base Construction for Knowledge-Augmented Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Horst</first><last>Samulowitz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Oktie</first><last>Hassanzadeh</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dharmashankar</first><last>Subramanian</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sola</first><last>Shirai</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Alfio</first><last>Gliozzo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Debarun</first><last>Bhattacharjya</last><affiliation>International Business Machines</affiliation></author>
      <pages>26569-26583</pages>
      <abstract>Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.</abstract>
      <url hash="34bf6250">2025.findings-acl.1363</url>
      <bibkey>baek-etal-2025-knowledge</bibkey>
    </paper>
    <paper id="1364">
      <title><fixed-case>NBDESCRIB</fixed-case>: A Dataset for Text Description Generation from Tables and Code in <fixed-case>J</fixed-case>upyter Notebooks with Guidelines</title>
      <author><first>Xuye</first><last>Liu</last></author>
      <author><first>Tengfei</first><last>Ma</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Yimu</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Fengjie</first><last>Wang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jian</first><last>Zhao</last><affiliation>University of Waterloo</affiliation></author>
      <pages>26584-26606</pages>
      <abstract>Generating cell-level descriptions for Jupyter Notebooks, which is a major resource consisting of codes, tables, and descriptions, has been attracting increasing research attention. However, existing methods for Jupyter Notebooks mostly focus on generating descriptions from code snippets or table outputs independently. On the other side, descriptions should be personalized as users have different purposes in different scenarios while previous work ignored this situation during description generation. In this work, we formulate a new task, personalized description generation with code, tables,and user-written guidelines in Jupyter Notebooks. To evaluate this new task, we collect and propose a benchmark, namely NBDESCRIB: , containing code, tables, and user-written guidelines as inputs and personalized descriptions as targets. Extensive experiments show that while existing models of text generation are able to generate fluent and readable descriptions, they still struggle to produce factually correct descriptions without user-written guidelines. CodeT5 achieved the highest scores in Orientation (1.27) and Correctness (-0.43) among foundation models in human evaluation, while the ground truth scored higher in Orientation (1.45) and Correctness (1.19). Common error patterns involve misalignment with guidelines, incorrect variable values, omission of im-031 portant code information, and reasoning errors.032 Moreover, ablation studies show that adding guidelines significantly enhances performance, both qualitatively and quantitatively.</abstract>
      <url hash="3f1f73f3">2025.findings-acl.1364</url>
      <bibkey>liu-etal-2025-nbdescrib</bibkey>
    </paper>
    <paper id="1365">
      <title><fixed-case>EC</fixed-case>o<fixed-case>RAG</fixed-case>: Evidentiality-guided Compression for Long Context <fixed-case>RAG</fixed-case></title>
      <author><first>Yeonseok</first><last>Jeong</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jinsu</first><last>Kim</last></author>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>26607-26628</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or <b>ECoRAG</b> framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.</abstract>
      <url hash="fc4f8a5d">2025.findings-acl.1365</url>
      <bibkey>jeong-etal-2025-ecorag</bibkey>
    </paper>
    <paper id="1366">
      <title>From Complexity to Clarity: <fixed-case>AI</fixed-case>/<fixed-case>NLP</fixed-case>’s Role in Regulatory Compliance</title>
      <author><first>Jivitesh</first><last>Jain</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Nivedhitha</first><last>Dhanasekaran</last></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>26629-26641</pages>
      <abstract>Regulatory data compliance is a cornerstone of trust and accountability in critical sectors like finance, healthcare, and technology, yet its complexity poses significant challenges for organizations worldwide. Recent advances in natural language processing, particularly large language models, have demonstrated remarkable capabilities in text analysis and reasoning, offering promising solutions for automating compliance processes. This survey examines the current state of automated data compliance, analyzing key challenges and approaches across problem areas. We identify critical limitations in current datasets and techniques, including issues of adaptability, completeness, and trust. Looking ahead, we propose research directions to address these challenges, emphasizing standardized evaluation frameworks and balanced human-AI collaboration.</abstract>
      <url hash="70bb88dd">2025.findings-acl.1366</url>
      <bibkey>jain-etal-2025-complexity</bibkey>
    </paper>
    <paper id="1367">
      <title><fixed-case>EXPERT</fixed-case>: An Explainable Image Captioning Evaluation Metric with Structured Explanations</title>
      <author><first>Hyunjong</first><last>Kim</last></author>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Jongheon</first><last>Jeong</last></author>
      <author><first>Yeongjae</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungzoon</first><last>Cho</last><affiliation>Seoul National University</affiliation></author>
      <pages>26642-26657</pages>
      <abstract>Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.</abstract>
      <url hash="d49ebd77">2025.findings-acl.1367</url>
      <bibkey>kim-etal-2025-expert</bibkey>
    </paper>
    <paper id="1368">
      <title>Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning</title>
      <author><first>Eitan</first><last>Wagner</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Nitay</first><last>Alon</last></author>
      <author><first>Joseph M</first><last>Barnby</last><affiliation>Royal Holloway University of London</affiliation></author>
      <author><first>Omri</first><last>Abend</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>26658-26668</pages>
      <abstract>Theory of Mind (ToM) capabilities in LLMs have recently become a central object of investigation, sparking debates and discussions. In this position paper, we explore many lines of work in different communities in AI and cognitive science. Inspired by cognitive work, we view ToM tasks as a two-step process: (I) first, determining whether and how to invoke ToM, which includes setting the appropriate Depth of Mentalizing (DoM); and (II) second, applying correct inference given the appropriate DoM. We identify that many works about ToM in LLMs, such as benchmarks and add-on modules, tend to unjustly overlook the first step and focus exclusively on the second one, which can be framed as a logic-reasoning task. We support our distinction with empirical evidence about the difficulty of the different steps in existing benchmarks. We conclude with suggestions for improved evaluation of ToM capabilities, inspired by dynamic environments used in cognitive tasks in biological agents.</abstract>
      <url hash="b6a30fa2">2025.findings-acl.1368</url>
      <bibkey>wagner-etal-2025-mind</bibkey>
    </paper>
    <paper id="1369">
      <title><fixed-case>LLM</fixed-case>s are Biased Evaluators But Not Biased for Fact-Centric Retrieval Augmented Generation</title>
      <author><first>Yen-Shan</first><last>Chen</last></author>
      <author><first>Jing</first><last>Jin</last></author>
      <author><first>Peng-Ting</first><last>Kuo</last></author>
      <author><first>Chao-Wei</first><last>Huang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>26669-26684</pages>
      <abstract>Recent studies have demonstrated that large language models (LLMs) exhibit significant biases in evaluation tasks, particularly in preferentially rating and favoring self-generated content. However, the extent to which this bias manifests in fact-oriented tasks, especially within retrieval-augmented generation (RAG) frameworks—where keyword extraction and factual accuracy take precedence over stylistic elements—remains unclear. Our study addresses this knowledge gap by simulating two critical phases of the RAG framework. In the first phase, LLMs evaluated human-authored and model-generated passages, emulating the pointwise reranking phase. The second phase involves conducting pairwise reading comprehension tests to simulate the generation phase. Contrary to previous findings indicating a self-preference in rating tasks, our results reveal no significant self-preference effect in RAG frameworks. Instead, we observe that factual accuracy significantly influences LLMs’ output, even in the absence of prior knowledge. These findings are consistent among three common QA datasets (NQ, MARCO, TriviaQA Datasets) and 5 widely adopted language models (GPT-3.5, GPT-4o-mini, Gemini, LLaMA3, and Mistral). Our research contributes to the ongoing discourse on LLM biases and their implications for RAG-based system, offering insights that may inform the development of more robust and unbiased LLM systems.</abstract>
      <url hash="907368cd">2025.findings-acl.1369</url>
      <bibkey>chen-etal-2025-llms</bibkey>
    </paper>
    <paper id="1370">
      <title>Standard Quality Criteria Derived from Current <fixed-case>NLP</fixed-case> Evaluations for Guiding Evaluation Design and Grounding Comparability and <fixed-case>AI</fixed-case> Compliance Assessments</title>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Craig</first><last>Thomson</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>26685-26715</pages>
      <abstract>Research shows that two evaluation experiments reporting results for the same quality criterion name (e.g. Fluency) do not necessarily evaluate the same aspect of quality. Not knowing when two evaluations are comparable in this sense means we currently lack the ability to draw conclusions based on multiple independently conducted evaluations. It is hard to see how this issue can be fully addressed other than by the creation of a standard set of quality criterion names and definitions that the evaluations in use in NLP can be grounded in. Taking a descriptivist approach, the QCET Quality Criteria for Evaluation Taxonomy derives a standard set of 114 quality criterion names and definitions from three surveys of a combined total of 933 evaluation experiments in NLP, and structures them into a reference taxonomy. We present QCET and its uses in (i) establishing comparability of existing evaluations, (ii) guiding the design of new evaluations, and (iii) assessing regulation compliance.</abstract>
      <url hash="0e36ded1">2025.findings-acl.1370</url>
      <bibkey>belz-etal-2025-standard</bibkey>
    </paper>
    <paper id="1371">
      <title>sk<fixed-case>LEP</fixed-case>: A <fixed-case>S</fixed-case>lovak General Language Understanding Benchmark</title>
      <author><first>Marek</first><last>Suppa</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Andrej</first><last>Ridzik</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Daniel</first><last>Hládek</last><affiliation>Technical University in Kosice</affiliation></author>
      <author><first>Tomáš</first><last>Javůrek</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Viktória</first><last>Ondrejová</last></author>
      <author><first>Kristína</first><last>Sásiková</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Martin</first><last>Tamajka</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <author><first>Marian</first><last>Simko</last><affiliation>Kempelen Institute of Intelligent Technologies</affiliation></author>
      <pages>26716-26743</pages>
      <abstract>In this work, we introduce skLEP, the first comprehensive benchmark specifically designed for evaluating Slovak natural language understanding (NLU) models. We have compiled skLEP to encompass nine diverse tasks that span token-level, sentence-pair, and document-level challenges, thereby offering a thorough assessment of model capabilities. To create this benchmark, we curated new, original datasets tailored for Slovak and meticulously translated established English NLU resources. Within this paper, we also present the first systematic and extensive evaluation of a wide array of Slovak-specific, multilingual, and English pre-trained language models using the skLEP tasks. Finally, we also release the complete benchmark data, an open-source toolkit facilitating both fine-tuning and evaluation of models, and a public leaderboard at <url>https://github.com/slovak-nlp/sklep</url> in the hopes of fostering reproducibility and drive future research in Slovak NLU.</abstract>
      <url hash="09a2c9c7">2025.findings-acl.1371</url>
      <bibkey>suppa-etal-2025-sklep</bibkey>
    </paper>
    <paper id="1372">
      <title>Can Vision Language Models Understand Mimed Actions?</title>
      <author><first>Hyundong Justin</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Spencer</first><last>Lin</last></author>
      <author><first>Tejas</first><last>Srinivasan</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Deuksin</first><last>Kwon</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Natali T.</first><last>Chavez</last></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>26744-26759</pages>
      <abstract>Non-verbal communication (NVC) is an integral part of human language, but it has been overlooked in natural language processing research. Studying NVC in general is challenging because of its high variance in interpretation among individuals and cultures, but mime—the theatrical technique of suggesting intent using only gesture, expression, and movement—is a subset of NVC with much lower human interpretation variance. As a gateway for evaluating vision-language models on their understanding of NVC, we propose Mime Identification-based Multimodal Evaluation (MIME), a gesture recognition task built upon a novel corpus of mimed activity comprising 86 unique gestures with a variety of perturbations applied to the avatar, background, and viewpoint for evaluating recognition robustness. We find that both open-weight and API-based vision-language models perform significantly worse than humans at identifying mimed gestures in MIME, motivating the need for increased research for instilling more robust understanding of human actions for VLMs.</abstract>
      <url hash="b9ff65be">2025.findings-acl.1372</url>
      <bibkey>cho-etal-2025-vision</bibkey>
    </paper>
    <paper id="1373">
      <title>Training Language Model to Critique for Better Refinement</title>
      <author><first>Tianshu</first><last>Yu</last></author>
      <author><first>Chao</first><last>Xiang</last></author>
      <author><first>Mingchuan</first><last>Yang</last></author>
      <author><first>Pei</first><last>Ke</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Bosi</first><last>Wen</last></author>
      <author><first>Cunxiang</first><last>Wang</last></author>
      <author><first>Jiale</first><last>Cheng</last></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last><affiliation>China Telecom</affiliation></author>
      <author><first>Xinyu</first><last>Mu</last></author>
      <author><first>Chuxiong</first><last>Sun</last><affiliation>China Telecom Research Institute Emerging Technology Research Division</affiliation></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>26760-26804</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable evaluation and critique capabilities, providing insightful feedback and identifying flaws in various tasks. However, limited research has explored which types of critiques are most effective for improving model responses or how to generate such critiques. To address this gap, we introduce Refinement-oriented Critique Optimization (RCO), a novel framework designed to train critic models using refinement signals. RCO uses a feedback loop where critiques, generated by the critic model, guide the actor model in refining its responses. The critique utility (CU) quantifies the effectiveness of these refinements, serving as the reward signal for training the critic model. By focusing on critiques that lead to better refinements, RCO eliminates the need for direct critique preference assessment, ensuring that critiques driving meaningful improvements are rewarded. We evaluate RCO across five tasks—dialog generation, summarization, question answering, mathematical reasoning, and code generation—and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes. Our contributions include the introduction of RCO, a novel supervision scheme based on refined response preferences, and comprehensive experimental results that highlight the method’s effectiveness in enhancing LLM critique-refinement loops. Code and data will be publicly available upon acceptance of this paper.</abstract>
      <url hash="0386af6f">2025.findings-acl.1373</url>
      <bibkey>yu-etal-2025-training</bibkey>
    </paper>
    <paper id="1374">
      <title>Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning</title>
      <author><first>Peiyi</first><last>Zhang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Zhijie</first><last>Nie</last><affiliation>Beihang University</affiliation></author>
      <author><first>Ziqiao</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>26805-26821</pages>
      <abstract>Multi-task prompt tuning utilizes multiple high-resource source tasks to improve performance on low-source target tasks. Existing approaches transfer the soft prompt trained by combining all source tasks or a single “high-similar” source task one-time-only. However, we find that the optimal transfer performance often comes from a combination of source tasks, which is neither one nor all. Further, we find that the similarity between source and target tasks also changes dynamically during fine-tuning after transfering, making similarity calculation in the initiation stage inadequate. To address these issues, we propose a method called Dynamic Task Vector Grouping (DTVG), whose core ideas contain (1) measuring the task similarity with task vectors instead of soft prompt, (2) grouping the optimal source task combination based on two metrics: <i>target similarity</i> and <i>knowledge consistency</i>; (3) dynamically updating the combination in each iteration step. Extensive experiments on the 26 NLP datasets under different settings demonstrate that DTVG effectively groups similar source tasks while reducing negative transfer, achieving the start-of-art performance.</abstract>
      <url hash="2d0d1ddc">2025.findings-acl.1374</url>
      <bibkey>zhang-etal-2025-dynamic-task</bibkey>
    </paper>
    <paper id="1375">
      <title><fixed-case>DICE</fixed-case>-<fixed-case>BENCH</fixed-case>: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues</title>
      <author><first>Kyochul</first><last>Jang</last><affiliation>Korea University</affiliation></author>
      <author><first>Donghyeon</first><last>Lee</last></author>
      <author><first>Kyusik</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongseok</first><last>Heo</last></author>
      <author><first>Taewhoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Woojeong</first><last>Kim</last><affiliation>Cornell University</affiliation></author>
      <author><first>Bongwon</first><last>Suh</last><affiliation>Seoul National University</affiliation></author>
      <pages>26822-26846</pages>
      <abstract>Existing function-calling benchmarks focus on single-turn interactions. However, they overlook the complexity of real-world scenarios. To quantify how existing benchmarks address practical applications, we introduce DICE-SCORE, a metric that evaluates the dispersion of tool-related information such as function name and parameter values throughout the dialogue. Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios. To address this gap, we present DICE-BENCH, a framework that constructs practical function-calling datasets by synthesizing conversations through a tool graph that maintains dependencies across rounds and a multi-agent system with distinct personas to enhance dialogue naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our experiments on 19 LLMs with DICE-BENCH show that significant advances are still required before such models can be deployed effectively in real-world settings. Our code and data are all publicly available.</abstract>
      <url hash="a3fdd43e">2025.findings-acl.1375</url>
      <bibkey>jang-etal-2025-dice</bibkey>
    </paper>
    <paper id="1376">
      <title><fixed-case>HASH</fixed-case>-<fixed-case>RAG</fixed-case>: Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and Augmented Generation</title>
      <author><first>Jinyu</first><last>Guo</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xunlei</first><last>Chen</last></author>
      <author><first>Qiyang</first><last>Xia</last></author>
      <author><first>Zhaokun</first><last>Wang</last></author>
      <author><first>Jie</first><last>Ou</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Shunyu</first><last>Yao</last><affiliation>China Telecom Research Institute</affiliation></author>
      <author><first>Wenhong</first><last>Tian</last></author>
      <pages>26847-26858</pages>
      <abstract>Retrieval-Augmented Generation (RAG) encounters efficiency challenges when scaling to massive knowledge bases while preserving contextual relevance. We propose Hash-RAG, a framework that integrates deep hashing techniques with systematic optimizations to address these limitations. Our queries directly learn binary hash codes from knowledgebase code, eliminating intermediate feature extraction steps, and significantly reducing storage and computational overhead. Building upon this hash-based efficient retrieval framework, we establish the foundation for fine-grained chunking. Consequently, we design a Prompt-Guided Chunk-to-Context (PGCC) module that leverages retrieved hash-indexed propositions and their original document segments through prompt engineering to enhance the LLM’s contextual awareness. Experimental evaluations on NQ, TriviaQA, and HotpotQA datasets demonstrate that our approach achieves a 90% reduction in retrieval time compared to conventional methods while maintaining considerate recall performance. Additionally, The proposed system outperforms retrieval/non-retrieval baselines by 1.4-4.3% in EM scores.</abstract>
      <url hash="b226f215">2025.findings-acl.1376</url>
      <bibkey>guo-etal-2025-hash</bibkey>
    </paper>
    <paper id="1377">
      <title>A Constrained Text Revision Agent via Iterative Planning and Searching</title>
      <author><first>Hannan</first><last>Cao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>26859-26882</pages>
      <abstract>Existing text revision systems are capable of generating fluent and coherent text, but struggle with constrained text revision (CTR), which requires adherence to specific constraints. Furthermore, adapting these systems to diverse constraints is challenging. To bridge this gap, we introduce TRIPS, a Text Revision agent via Iterative Planning and Searching, focusing on CTR. TRIPS utilizes a planner, a reviser (i.e., a large language model), and adaptable tools to generate revisions tailored to different scenarios. Specifically, we propose an iterative self-training alignment method to construct the planner, which generates tool usage and text revision plans. Furthermore, we propose Tool-Guided Monte Carlo Tree Search (TG-MCTS), a novel CTR algorithm that extends MCTS with tool-guided expansion and evaluation, enabling the search for optimal revision strategies across various scenarios. To evaluate TRIPS, we introduce ConsTRev, a dataset with multi-level constrained instructions for paragraph-level revision. Experimental results show that TRIPS outperforms baselines in both constraint adherence and revision quality. Furthermore, TRIPS exhibits robust performance across diverse use cases, including plain text and LaTeX revision.</abstract>
      <url hash="f031a1b5">2025.findings-acl.1377</url>
      <bibkey>cao-ng-2025-constrained</bibkey>
    </paper>
    <paper id="1378">
      <title><fixed-case>MMR</fixed-case>efine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models</title>
      <author><first>Gio</first><last>Paik</last><affiliation>Theta One</affiliation></author>
      <author><first>Geewook</first><last>Kim</last><affiliation>University of Seoul, NAVER Cloud and KAIST</affiliation></author>
      <author><first>Jinbae</first><last>Im</last><affiliation>NAVER CLOUD</affiliation></author>
      <pages>26883-26904</pages>
      <abstract>This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs’ abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types.Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at <url>https://github.com/naver-ai/MMRefine</url>.</abstract>
      <url hash="01df6f6b">2025.findings-acl.1378</url>
      <bibkey>paik-etal-2025-mmrefine</bibkey>
    </paper>
    <paper id="1379">
      <title>How Programming Concepts and Neurons Are Shared in Code Language Models</title>
      <author><first>Amir Hossein</first><last>Kargaran</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>26905-26917</pages>
      <abstract>Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model’s concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model’s concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.</abstract>
      <url hash="9eca3621">2025.findings-acl.1379</url>
      <bibkey>kargaran-etal-2025-programming</bibkey>
    </paper>
    <paper id="1380">
      <title><fixed-case>D</fixed-case>yna<fixed-case>Q</fixed-case>uest: A Dynamic Question Answering Dataset Reflecting Real-World Knowledge Updates</title>
      <author><first>Qian</first><last>Lin</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>26918-26936</pages>
      <abstract>The rapidly changing nature of real-world information presents challenges for large language models (LLMs), which are typically trained on static datasets. This limitation makes it difficult for LLMs to accurately perform tasks that require up-to-date knowledge, such as time-sensitive question answering (QA). In this paper, we introduce **DynaQuest**, a **Dyna**mic **Quest**ion answering dataset reflecting knowledge updates in the real world. DynaQuest is based on Wikipedia Infoboxes, which are frequently updated to reflect real-world changes. Our dataset is created by automatically identifying and comparing changes between different versions of Wikipedia pages and generating question-answer pairs based on these updates. To address the challenges posed by our dynamic dataset, we propose **CARL**, a **C**ontext-**A**ware **R**einforcement **L**earning framework to improve the performance of LLMs on time-sensitive question answering. We conduct experiments on our collected dataset across recent time periods and demonstrate the effectiveness of our approach. Furthermore, we maintain a dynamic knowledge updating process, providing a periodically evolving benchmark to continually evaluate LLMs’ ability to answer time-sensitive questions.</abstract>
      <url hash="3d9f6c7c">2025.findings-acl.1380</url>
      <bibkey>lin-etal-2025-dynaquest</bibkey>
    </paper>
    <paper id="1381">
      <title><fixed-case>P</fixed-case>rocrustes<fixed-case>GPT</fixed-case>: Compressing <fixed-case>LLM</fixed-case>s with Structured Matrices and Orthogonal Transformations</title>
      <author><first>Ekaterina</first><last>Grishina</last><affiliation>Higher School of Economics and Higher School of Economics</affiliation></author>
      <author><first>Mikhail</first><last>Gorbunov</last></author>
      <author><first>Maxim</first><last>Rakhuba</last><affiliation>Higher School of Economics</affiliation></author>
      <pages>26937-26949</pages>
      <abstract>Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning.To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices.This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes.The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at: https://github.com/GrishKate/ProcrustesGPT.</abstract>
      <url hash="2ae5358d">2025.findings-acl.1381</url>
      <bibkey>grishina-etal-2025-procrustesgpt</bibkey>
    </paper>
    <paper id="1382">
      <title>Revisiting In-Context Learning with Long Context Language Models</title>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sun Jae</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Prakhar</first><last>Gupta</last><affiliation>Google</affiliation></author>
      <author><first>Geunseob</first><last>Oh</last><affiliation>Google</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Prateek</first><last>Kolhar</last><affiliation>Google</affiliation></author>
      <pages>26950-26966</pages>
      <abstract>In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we discover that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.</abstract>
      <url hash="f9b11535">2025.findings-acl.1382</url>
      <bibkey>baek-etal-2025-revisiting</bibkey>
    </paper>
    <paper id="1383">
      <title>Rationalize and Align: Enhancing Writing Assistance with Rationale via Self-Training for Improved Alignment</title>
      <author><first>Hannan</first><last>Cao</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hai</first><last>Ye</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>26967-26982</pages>
      <abstract>A Writing Assistant (WA) is a system that offers writing suggestions based on user instructions. Existing WAs are typically built by training large language models (LLMs) on domain-specific instruction data through supervised fine-tuning (SFT) only. However, SFT optimizes models to match a single reference, failing to capture the inherent flexibility of text editing, where multiple valid revisions exist. Therefore, solely relying on SFT limits WA performance. To address this limitation, we propose the Rationalize and Align framework, which enhances the WA performance with rationale (i.e., linguistic explanations) and alignment. Our framework automatically generates the rationale and preference data for writing tasks via distillation and self-training, eliminating the need for human annotation. These data are then leveraged to refine WA using a novel preference optimization method. Empirical results show that our framework significantly improves WA performance. Our WA outperforms both open-source state-of-the-art WAs and the closed-source GPT-4o by 3.9 and 7.1 points on average, respectively, across eight well-established writing-related test sets.</abstract>
      <url hash="1879a64e">2025.findings-acl.1383</url>
      <bibkey>cao-etal-2025-rationalize</bibkey>
    </paper>
    <paper id="1384">
      <title>Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps</title>
      <author><first>Jie</first><last>Ou</last></author>
      <author><first>Jinyu</first><last>Guo</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Shuaihong</first><last>Jiang</last></author>
      <author><first>Zhaokun</first><last>Wang</last></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Central South University</affiliation></author>
      <author><first>Shunyu</first><last>Yao</last><affiliation>China Telecom Research Institute</affiliation></author>
      <author><first>Wenhong</first><last>Tian</last></author>
      <pages>26983-27000</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a pivotal method for expanding the knowledge of large language models. To handle complex queries more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the generated quality through multiple interactions with external knowledge bases. Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency challenges inherent in RAG, which are attributable to its reliance on multiple iterations of generation. Existing A-RAG approaches process all retrieved contents from scratch. However, they ignore the situation where there is a significant overlap in the content of the retrieval results across rounds. The overlapping content is redundantly represented, which leads to a large proportion of repeated computations, thus affecting the overall efficiency. To address this issue, this paper introduces a model-agnostic approach that can be generally applied to A-RAG methods, which is dedicated to reducing the redundant representation process caused by the overlapping of retrieval results. Specifically, we use cache access and parallel generation to speed up the prefilling and decoding stages respectively. Additionally, we also propose an instruction-driven module to further guide the model to more effectively attend to each part of the content in a more suitable way for LLMs. Experiments show that our approach achieves 2.79 and 2.33 times significant acceleration on average for prefilling and decoding respectively while maintaining equal generation quality.</abstract>
      <url hash="7191d1fd">2025.findings-acl.1384</url>
      <bibkey>ou-etal-2025-accelerating</bibkey>
    </paper>
    <paper id="1385">
      <title><fixed-case>MEXA</fixed-case>: Multilingual Evaluation of <fixed-case>E</fixed-case>nglish-Centric <fixed-case>LLM</fixed-case>s via Cross-Lingual Alignment</title>
      <author><first>Amir Hossein</first><last>Kargaran</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Ali</first><last>Modarressi</last><affiliation>Center for Information and Language Processing, LMU Munich</affiliation></author>
      <author><first>Nafiseh</first><last>Nikeghbal</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Jana</first><last>Diesner</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>27001-27023</pages>
      <abstract>English-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.</abstract>
      <url hash="b43fd047">2025.findings-acl.1385</url>
      <bibkey>kargaran-etal-2025-mexa</bibkey>
    </paper>
    <paper id="1386">
      <title>Automated Fine-Grained Mixture-of-Experts Quantization</title>
      <author><first>Zhanhao</first><last>Xie</last></author>
      <author><first>Yuexiao</first><last>Ma</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xiawu</first><last>Zheng</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Fei</first><last>Chao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Wanchen</first><last>Sui</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yong</first><last>Li</last></author>
      <author><first>Shen</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Rongrong</first><last>Ji</last></author>
      <pages>27024-27037</pages>
      <abstract>The Mixture of Experts (MoE) architecture enables efficient model scaling through conditional computation, where only subset of parameters are activated per input. However, this distributed architecture poses unprecedented challenges for model compression, as conventional quantization methods optimized for dense networks prove inadequate. This paper introduces a specialized quantization framework for MoE architectures, motivated by our discovery that weight matrices across expert networks exhibit distinctive channel-wise outlier distributions, necessitating a more nuanced compression approach. Through theoretical analysis incorporating Fisher Information matrices and condition number characteristics, we establish a fundamental relationship between layer functionality and quantization sensitivity, demonstrating that down-projection layers inherently demand higher precision compared to up-projection layers. Leveraging these insights, we develop an automated channel-wise quantization framework that dynamically determines optimal bit-width allocations while maintaining minimal computational overhead through efficient statistical approximations. When evaluated on the Mixtral-8x7b-v0.1 architecture, our methodology demonstrates a 3.96% improvement over existing state-of-the-art approaches across natural language understanding benchmarks, while achieving superior compression ratios.</abstract>
      <url hash="0a650ecd">2025.findings-acl.1386</url>
      <bibkey>xie-etal-2025-automated</bibkey>
    </paper>
    <paper id="1387">
      <title>Enhancing Complex Reasoning in Knowledge Graph Question Answering through Query Graph Approximation</title>
      <author><first>Hongjun</first><last>Jeong</last></author>
      <author><first>Minji</first><last>Kim</last></author>
      <author><first>Heesoo</first><last>Jung</last></author>
      <author><first>Ko Keun</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Hogun</first><last>Park</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>27038-27056</pages>
      <abstract>Knowledge-grounded Question Answering (QA) aims to provide answers to structured queries or natural language questions by leveraging Knowledge Graphs (KGs). Existing approaches are mainly divided into Knowledge Graph Question Answering (KGQA) and Complex Query Answering (CQA). Both approaches have limitations: the first struggles to utilize KG context effectively when essential triplets related to the questions are missing in the given KGs, while the second depends on structured first-order logic queries. To overcome these limitations, we propose a novel framework termed Aqua-QA. Aqua-QAapproximates query graphs from natural language questions, enabling reasoning over KGs. We evaluate Aqua-QA on challenging QA tasks where KGs are incomplete in the context of QA, and complex logical reasoning is required to answer natural language questions. Experimental results on these datasets demonstrate that Aqua-QA outperforms existing methods, showcasing its effectiveness in handling complex reasoning tasks in knowledge-grounded QA settings.</abstract>
      <url hash="dc510d9a">2025.findings-acl.1387</url>
      <bibkey>jeong-etal-2025-enhancing</bibkey>
    </paper>
  </volume>
</collection>
