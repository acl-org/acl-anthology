<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.findings">
  <volume id="naacl" ingest-date="2025-04-25" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: NAACL 2025</booktitle>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>April</month>
      <year>2025</year>
      <url hash="7a54f87b">2025.findings-naacl</url>
      <venue>findings</venue>
      <isbn>979-8-89176-195-7</isbn>
    </meta>
    <frontmatter>
      <url hash="a93538d7">2025.findings-naacl.0</url>
      <bibkey>findings-2025-naacl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>From Lazy to Prolific: Tackling Missing Labels in Open Vocabulary Extreme Classification by Positive-Unlabeled Sequence Learning</title>
      <author><first>Ranran Haoran</first><last>Zhang</last></author>
      <author><first>Bensu</first><last>Uçar</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Soumik</first><last>Dey</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Hansi</first><last>Wu</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Binbin</first><last>Li</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1-16</pages>
      <url hash="be65fe89">2025.findings-naacl.1</url>
      <bibkey>zhang-etal-2025-lazy</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>D</fixed-case>iff<fixed-case>ZOO</fixed-case>: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization</title>
      <author><first>Pucheng</first><last>Dang</last></author>
      <author><first>Xing</first><last>Hu</last></author>
      <author><first>Dong</first><last>Li</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Institute of Computing Technology, CAS</affiliation></author>
      <author><first>Qi</first><last>Guo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>17-31</pages>
      <abstract>Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I model’s capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of purely black-box attacks without prior knowledge of the T2l model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T2I diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2l models.</abstract>
      <url hash="6d25059a">2025.findings-naacl.2</url>
      <bibkey>dang-etal-2025-diffzoo</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>M</fixed-case>ed<fixed-case>O</fixed-case>dyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200<fixed-case>K</fixed-case> Tokens</title>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Hongli</first><last>Sun</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>32-56</pages>
      <abstract>Numerous advanced Large Language Models (LLMs) now support context lengths up to 128K, and some extend to 200K. Some benchmarks in the generic domain have also followed up on evaluating long-context capabilities. In the medical domain, tasks are distinctive due to the unique contexts and need for domain expertise, necessitating further evaluation. However, despite the frequent presence of long texts in medical scenarios, evaluation benchmarks of long-context capabilities for LLMs in this field are still rare. In this paper, we propose MedOdyssey, the first medical long-context benchmark with seven length levels ranging from 4K to 200K tokens. MedOdyssey consists of two primary components: the medical-context “needles in a haystack” task and a series of tasks specific to medical applications, together comprising 10 datasets. The first component includes challenges such as counter-intuitive reasoning and novel (unknown) facts injection to mitigate knowledge leakage and data contamination of LLMs. The second component confronts the challenge of requiring professional medical expertise. Especially, we design the ‘“Maximum Identical Context” principle to improve fairness by guaranteeing that different LLMs observe as many identical contexts as possible. Our experiment evaluates advanced proprietary and open-source LLMs tailored for processing long contexts and presents detailed performance analyses. This highlights that LLMs still face challenges and need for further research in this area. Our code and data are released in the repository: <url>https://github.com/JOHNNY-fans/MedOdyssey</url>.</abstract>
      <url hash="7929ab46">2025.findings-naacl.3</url>
      <bibkey>fan-etal-2025-medodyssey</bibkey>
    </paper>
    <paper id="4">
      <title>Can <fixed-case>LLM</fixed-case>s Learn Macroeconomic Narratives from Social Media?</title>
      <author><first>Almog</first><last>Gueta</last><affiliation>Google</affiliation></author>
      <author><first>Amir</first><last>Feder</last><affiliation>Columbia University and Google</affiliation></author>
      <author><first>Zorik</first><last>Gekhman</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Ariel</first><last>Goldstein</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Roi</first><last>Reichart</last><affiliation>Technion, Israel Institute of Technology</affiliation></author>
      <pages>57-78</pages>
      <abstract>This study empirically tests the <tex-math>Narrative Economics</tex-math> hypothesis, which posits that narratives (ideas that are spread virally and affect public beliefs) can influence economic fluctuations. We introduce two curated datasets containing posts from X (formerly Twitter) which capture economy-related narratives (Data will be shared upon paper acceptance). Employing Natural Language Processing (NLP) methods, we extract and summarize narratives from the tweets. We test their predictive power for <tex-math>macroeconomic</tex-math> forecasting by incorporating the tweets’ or the extracted narratives’ representations in downstream financial prediction tasks. Our work highlights the challenges in improving macroeconomic models with narrative data, paving the way for the research community to realistically address this important challenge. From a scientific perspective, our investigation offers valuable insights and NLP tools for narrative extraction and summarization using Large Language Models (LLMs), contributing to future research on the role of narratives in economics.</abstract>
      <url hash="7e2299a8">2025.findings-naacl.4</url>
      <bibkey>gueta-etal-2025-llms</bibkey>
    </paper>
    <paper id="5">
      <title>Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency</title>
      <author><first>Leonidas</first><last>Gee</last></author>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ignacio</first><last>Iacobacci</last><affiliation>Elm Europe</affiliation></author>
      <pages>79-94</pages>
      <abstract>Code Language Models have been trained togenerate accurate solutions, typically with noregard for runtime. On the other hand, previousworks that explored execution optimisationhave observed corresponding drops infunctional correctness. To that end, we introduceCode-Optimise, a framework that incorporatesboth correctness (passed, failed) andruntime (quick, slow) as learning signals viaself-generated preference data. Our frameworkis both lightweight and robust as it dynamicallyselects solutions to reduce overfitting whileavoiding a reliance on larger models for learningsignals. Code-Optimise achieves significantimprovements in pass@k while decreasingthe competitive baseline runtimes by anadditional 6% for in-domain data and up to3% for out-of-domain data. As a by-product,the average length of the generated solutionsis reduced by up to 48% on MBPP and 23%on HumanEval, resulting in faster and cheaperinference. The generated data and codebaseis open-sourced at https://github.com/huawei-noah/HEBO/tree/Code_Optimise.</abstract>
      <url hash="dfed7a1a">2025.findings-naacl.5</url>
      <bibkey>gee-etal-2025-code</bibkey>
    </paper>
    <paper id="6">
      <title>People will agree what <fixed-case>I</fixed-case> think: Investigating <fixed-case>LLM</fixed-case>’s False Consensus Effect</title>
      <author><first>Junhyuk</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Yeseon</first><last>Hong</last></author>
      <author><first>Bugeun</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>95-126</pages>
      <abstract>Large Language Models (LLMs) have been recently adopted in interactive systems requiring communication. As the false belief in a model can harm the usability of such systems, LLMs should not have cognitive biases that humans have. Psychologists especially focus on the False Consensus Effect (FCE), a cognitive bias where individuals overestimate the extent to which others share their beliefs or behaviors, because FCE can distract smooth communication by posing false beliefs. However, previous studies have less examined FCE in LLMs thoroughly, which needs more consideration of confounding biases, general situations, and prompt changes. Therefore, in this paper, we conduct two studies to examine the FCE phenomenon in LLMs. In Study 1, we investigate whether LLMs have FCE. In Study 2, we explore how various prompting styles affect the demonstration of FCE. As a result of these studies, we identified that popular LLMs have FCE. Also, the result specifies the conditions when FCE becomes more or less prevalent compared to normal usage.</abstract>
      <url hash="7f8c866f">2025.findings-naacl.6</url>
      <bibkey>choi-etal-2025-people</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>L</fixed-case>aw<fixed-case>I</fixed-case>nstruct: A Resource for Studying Language Model Adaptation to the Legal Domain</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>Harvey</affiliation></author>
      <author><first>Lucia</first><last>Zheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Arya D.</first><last>McCarthy</last><affiliation>Scaled Cognition</affiliation></author>
      <author><first>Christopher</first><last>Hahn</last><affiliation>X, the moonshot factory</affiliation></author>
      <author><first>Brian M</first><last>Rosen</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Henderson</last><affiliation>Princeton University</affiliation></author>
      <author><first>Daniel E.</first><last>Ho</last><affiliation>Stanford University</affiliation></author>
      <author><first>Garrett</first><last>Honke</last></author>
      <author><first>Percy</first><last>Liang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher D</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <pages>127-152</pages>
      <abstract>Instruction tuning is an important step in making language models useful for direct user interaction. However, the legal domain is underrepresented in typical instruction datasets (e.g., only 10 out of 1600+ tasks in Super-NaturalInstructions). To study whether instruction tuning on legal datasets is necessary for strong legal reasoning, we aggregate 58 annotated legal datasets and write instructions for each, creating LawInstruct. LawInstruct covers 17 global jurisdictions, 24 languages and a total of 12M examples across diverse tasks such as legal QA, summarization of court cases, and legal argument mining. We evaluate our models on LegalBench, measuring legal reasoning across five categories in 162 challenging and realistic legal tasks, and MMLU, to measure potential drops in general reasoning capabilities. We find that legal-specific instruction tuning on Flan-T5 – yielding FLawN-T5 – improves performance on LegalBench across all model sizes, with an aggregate increase of 15 points or 50% over Flan-T5 for the base size. No model size shows performance drops in MMLU. We publish LawInstruct as a resource for further study of instruction tuning in the legal domain.</abstract>
      <url hash="62c4ba93">2025.findings-naacl.7</url>
      <bibkey>niklaus-etal-2025-lawinstruct</bibkey>
    </paper>
    <paper id="8">
      <title>Stephanie: Step-by-Step Dialogues for Mimicking Human Interactions in Social Conversations</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Hongyuan</first><last>Lu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xinhua</first><last>Zeng</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Zhang</last><affiliation>facemind</affiliation></author>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Yumeng</first><last>Zhang</last></author>
      <author><first>Shan</first><last>Huang</last></author>
      <author><first>Yiran</first><last>Wei</last></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>153-166</pages>
      <abstract>In the rapidly evolving field of natural language processing, dialogue systems primarily employ a single-step dialogue paradigm. Although this paradigm is commonly adopted, it lacks the depth and fluidity of human interactions and does not appear natural. We introduce a novel **Step**-by-Step Dialogue Paradigm (Stephanie), designed to mimic the ongoing dynamic nature of human conversations. By employing a dual learning strategy and a further-split post-editing method, we generated and utilized a high-quality step-by-step dialogue dataset to fine-tune existing large language models, enabling them to perform step-by-step dialogues. We thoroughly present Stephanie. Tailored automatic and human evaluations are conducted to assess its effectiveness compared to the traditional single-step dialogue paradigm. We will release code, Stephanie datasets, and Stephanie LLMs to facilitate the future of chatbot eras.</abstract>
      <url hash="3cb66799">2025.findings-naacl.8</url>
      <bibkey>yang-etal-2025-stephanie</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>hift: Sense-based Language Variation Analysis using Flexible Alignment</title>
      <author><first>Clare</first><last>Arrington</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Mauricio</first><last>Gruppi</last><affiliation>Villanova University</affiliation></author>
      <author><first>Sibel</first><last>Adali</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>167-181</pages>
      <abstract>We introduce ConShift, a family of alignment-based algorithms that enable semantic variation analysis at the sense-level. Using independent senses of words induced from the context of tokens in two corpora, sense-enriched word embeddings are aligned using self-supervision and a flexible matching mechanism. This approach makes it possible to test for multiple sense-level language variations such as sense gain/presence, loss/absence and broadening/narrowing, while providing explanation of the changes through visualization of related concepts. We illustrate the utility of the method with sense- and word-level semantic shift detection results for multiple evaluation datasets in diachronic settings and dialect variation in the synchronic setting.</abstract>
      <url hash="c07ce6df">2025.findings-naacl.9</url>
      <bibkey>arrington-etal-2025-conshift</bibkey>
    </paper>
    <paper id="10">
      <title>Breaking the Stigma! Unobtrusively Probe Symptoms in Depression Disorder Diagnosis Dialogue</title>
      <author><first>Jieming</first><last>Cao</last></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yanan</first><last>Zhang</last></author>
      <author><first>Ruibo</first><last>Deng</last></author>
      <author><first>Jincheng</first><last>Zhang</last></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <pages>182-200</pages>
      <url hash="26aafd84">2025.findings-naacl.10</url>
      <bibkey>cao-etal-2025-breaking</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>T</fixed-case>o<fixed-case>V</fixed-case>o: Toxicity Taxonomy via Voting</title>
      <author><first>Tinh Son</first><last>Luong</last></author>
      <author><first>Thanh-Thien</first><last>Le</last></author>
      <author><first>Thang Viet</first><last>Doan</last></author>
      <author><first>Linh Ngo</first><last>Van</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien Huu</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Nguyen Thi Ngoc</first><last>Diep</last></author>
      <pages>201-212</pages>
      <abstract>Existing toxic detection models face significant limitations, such as lack of transparency, customization, and reproducibility. These challenges stem from the closed-source nature of their training data and the paucity of explanations for their evaluation mechanism. To address these issues, we propose a dataset creation mechanism that integrates voting and chain-of-thought processes, producing a high-quality open-source dataset for toxic content detection. Our methodology ensures diverse classification metrics for each sample and includes both classification scores and explanatory reasoning for the classifications.We utilize the dataset created through our proposed mechanism to train our model, which is then compared against existing widely-used detectors. Our approach not only enhances transparency and customizability but also facilitates better fine-tuning for specific use cases. This work contributes a robust framework for developing toxic content detection models, emphasizing openness and adaptability, thus paving the way for more effective and user-specific content moderation solutions.</abstract>
      <url hash="6c642c2f">2025.findings-naacl.11</url>
      <bibkey>luong-etal-2025-tovo</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>HALLUCANA</fixed-case>: Fixing <fixed-case>LLM</fixed-case> Hallucination with A Canary Lookahead</title>
      <author><first>Tianyi</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Erenay</first><last>Dayanik</last><affiliation>Amazon</affiliation></author>
      <author><first>Shubhi</first><last>Tyagi</last><affiliation>Amazon</affiliation></author>
      <author><first>Andrea</first><last>Pierleoni</last><affiliation>Alexa AI</affiliation></author>
      <pages>213-230</pages>
      <abstract>In this paper, we present HALLUCANA, a canary lookahead to detect and correct factual hallucinations of Large Language Models (LLMs) in long-form generation. HALLUCANA detects and intervenes as soon as traces of hallucination emerge, during and even before generation. To support timely detection, we exploit the internal factuality representation in the LLM hidden space, where we investigate various proxies to the LLMs’ factuality self-assessment, and discuss its relation to the models’ context familiarity from their pre-training. On biography generation, our method improves generation quality by up to 2.5x, while consuming over 6 times less compute.</abstract>
      <url hash="ec7c8f02">2025.findings-naacl.12</url>
      <bibkey>li-etal-2025-hallucana</bibkey>
    </paper>
    <paper id="13">
      <title>Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</title>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Aoyang</first><last>Zhou</last></author>
      <author><first>Kun</first><last>He</last><affiliation>Huazhong University of Sceince and Technology</affiliation></author>
      <pages>231-245</pages>
      <abstract>Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</abstract>
      <url hash="61689032">2025.findings-naacl.13</url>
      <bibkey>liu-etal-2025-enhancing-adversarial</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>D</fixed-case>is2<fixed-case>D</fixed-case>is: Explaining Ambiguity in Fact-Checking</title>
      <author><first>Ieva</first><last>Staliunaite</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>246-267</pages>
      <abstract>Ambiguity is a linguistic tool for encoding information efficiently, yet it also causes misunderstandings and disagreements. It is particularly relevant to the domain of misinformation, as fact-checking ambiguous claims is difficult even for experts. In this paper we argue that instead of predicting a veracity label for which there is genuine disagreement, it would be more beneficial to explain the ambiguity. Thus, this work introduces claim disambiguation, a constrained generation task, for explaining ambiguous claims in fact-checking. This involves editing them to spell out an interpretation that can then be unequivocally supported by the given evidence. We collect a dataset of 1501 such claim revisions and conduct experiments with sequence-to-sequence models. The performance is compared to a simple copy baseline and a Large Language Model baseline. The best results are achieved by employing Minimum Bayes Decoding, with a BertScore F1 of 92.22. According to human evaluation, the model successfully disambiguates the claims 72% of the time.</abstract>
      <url hash="83832e83">2025.findings-naacl.14</url>
      <bibkey>staliunaite-vlachos-2025-dis2dis</bibkey>
    </paper>
    <paper id="15">
      <title>Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement</title>
      <author><first>Xiyao</first><last>Wang</last></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Zhaoyang</first><last>Wang</last></author>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Yiyang</first><last>Zhou</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Tom</first><last>Goldstein</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Taha</first><last>Kass-Hout</last><affiliation>GE HealthCare</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <pages>268-282</pages>
      <abstract>Large vision-language models (LVLMs) have achieved impressive results in visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there remains significant room for improvement in aligning visual and language modalities. Existing methods often depend on external models or data, leading to uncontrollable and unstable alignment results. In this paper, we propose SIMA, a self-improvement framework that enhances visual and language modality alignment without external dependencies. SIMA leverages existing vision instruction tuning datasets to self-generate responses, incorporating an in-context self-critic mechanism that constructs preference pairs for tuning. Crucially, our approach allows LVLMs to act as critics by designing effective critic prompts, eliminating the need for additional fine-tuning with external instruction data. We introduce three novel visual metrics within the self-critic process to guide judgement, significantly improving the accuracy of self-critic. Through extensive experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA significantly improves LVLM’s performance and outperforms previous approaches, achieving superior modality alignment.</abstract>
      <url hash="51745fec">2025.findings-naacl.15</url>
      <bibkey>wang-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>R</fixed-case>e<fixed-case>PD</fixed-case>: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process</title>
      <author><first>Peiran</first><last>Wang</last></author>
      <author><first>Xiaogeng</first><last>Liu</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>283-294</pages>
      <abstract>In this study, we introduce RePD, an innovative attack Retrieval-based Prompt Decomposition framework designed to mitigate the risk of jailbreak attacks on large language models (LLMs). Despite rigorous pre-training and fine-tuning focused on ethical alignment, LLMs are still susceptible to jailbreak exploits. RePD operates on a one-shot learning model, wherein it accesses a database of pre-collected jailbreak prompt templates to identify and decompose harmful inquiries embedded within user prompts. This process involves integrating the decomposition of the jailbreak prompt into the user’s original query into a one-shot learning example to effectively teach the LLM to discern and separate malicious components. Consequently, the LLM is equipped to first neutralize any potentially harmful elements before addressing the user’s prompt in a manner that aligns with its ethical guidelines. RePD is versatile and compatible with a variety of open-source LLMs acting as agents. Through comprehensive experimentation with both harmful and benign prompts, we have demonstrated the efficacy of our proposed RePD in enhancing the resilience of LLMs against jailbreak attacks, without compromising their performance in responding to typical user requests.</abstract>
      <url hash="1572b1ed">2025.findings-naacl.16</url>
      <bibkey>wang-etal-2025-repd</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>C</fixed-case>hat<fixed-case>CRS</fixed-case>: Incorporating External Knowledge and Goal Guidance for <fixed-case>LLM</fixed-case>-based Conversational Recommender Systems</title>
      <author><first>Chuang</first><last>Li</last></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hengchang</first><last>Hu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>295-312</pages>
      <abstract>This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.</abstract>
      <url hash="2f68f51e">2025.findings-naacl.17</url>
      <bibkey>li-etal-2025-chatcrs</bibkey>
    </paper>
    <paper id="18">
      <title>Data-Efficiently Learn Large Language Model for Universal 3<fixed-case>D</fixed-case> Scene Perception</title>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Haifeng</first><last>Huang</last></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Ziang</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>313-333</pages>
      <abstract>3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. With limited data, Chat-3D achieves a 82.2% relative score compared with GPT-4 on the constructed instruction dataset, and comparable performance to state-of-the-art LLM-based methods.</abstract>
      <url hash="5e64585f">2025.findings-naacl.18</url>
      <bibkey>wang-etal-2025-data</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>U</fixed-case>nified<fixed-case>MLLM</fixed-case>: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model</title>
      <author><first>Zhaowei</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>YiQing</first><last>Cai</last></author>
      <author><first>Qi</first><last>Xu</last></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Hang</first><last>Song</last></author>
      <author><first>Botian</first><last>Jiang</last></author>
      <author><first>Zhida</first><last>Huang</last></author>
      <author><first>Tao</first><last>Wang</last><affiliation>Bytedance group</affiliation></author>
      <pages>334-344</pages>
      <abstract>Significant advancements has recently been achieved in the field of multi-modal large language models (MLLMs), demonstrating their remarkable capabilities in understanding and reasoning across diverse tasks. However, these models are often trained for specific tasks and rely on task-specific input-output formats, limiting their applicability to a broader range of tasks. This raises a fundamental question: Can we develop a unified approach to represent and handle different multi-modal tasks to maximize the generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a comprehensive model designed to represent various tasks using a unified representation. Our model exhibits strong capabilities in comprehending the implicit intent of user instructions and preforming reasoning. In addition to generating textual responses, our model also outputs task tokens and grounding tokens, serving as indicators of task types and task granularity. These outputs are subsequently routed through the task router and directed to specific expert models for task completion. To train our model, we construct a task-specific dataset and an 100k multi-task dataset encompassing complex scenarios. Employing a three-stage training strategy, we equip our model with robust reasoning and task processing capabilities while preserving its generalization capacity and knowledge reservoir. Extensive experiments showcase the impressive performance of our unified representation approach across various tasks, surpassing existing methodologies. Furthermore, our approach exhibits exceptional scalability and generality.</abstract>
      <url hash="0fb9f356">2025.findings-naacl.19</url>
      <bibkey>li-etal-2025-unifiedmllm</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>PEMV</fixed-case>: Improving Spatial Distribution for Emotion Recognition in Conversations Using Proximal Emotion Mean Vectors</title>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <pages>345-357</pages>
      <abstract>Emotion Recognition in Conversation (ERC) aims to identify the emotions expressed in each utterance within a dialogue. Existing research primarily focuses on the analysis of contextual structure in dialogue and the interactions between different emotions. Nonetheless, ERC datasets often contain difficult-to-classify samples and suffer from imbalanced label distributions, which pose challenges to the spatial distribution of dialogue features. To tackle this issue, we propose a method that generates Proximal Emotion Mean Vectors (PEMV) based on emotion feature queues to optimize the spatial representation of text features. We design a Center Loss based on PEMVs to pull hard-to-classify samples closer to their respective category centers and employ Angle Loss to maximize the angular separation between different PEMVs. Furthermore, we utilize PEMV as a classifier to better adapt to the spatial structure of dialogue features. Extensive experiments on three widely used benchmark datasets demonstrate that our method achieves state-of-the-art performance and validates its effectiveness in optimizing feature space representations.</abstract>
      <url hash="3ff6a62e">2025.findings-naacl.20</url>
      <bibkey>lin-etal-2025-pemv</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>D</fixed-case>iscover<fixed-case>GPT</fixed-case>: Multi-task Fine-tuning Large Language Model for Related Table Discovery</title>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiao</first><last>Qin</last><affiliation>Amazon</affiliation></author>
      <author><first>Chuan</first><last>Lei</last></author>
      <author><first>Asterios</first><last>Katsifodimos</last><affiliation>Delft University of Technology</affiliation></author>
      <author><first>Zhengyuan</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Balasubramaniam</first><last>Srinivasan</last><affiliation>Amazon</affiliation></author>
      <author><first>Huzefa</first><last>Rangwala</last><affiliation>Amazon and Computer Science, George Mason University</affiliation></author>
      <pages>358-373</pages>
      <abstract>Natural language understanding over tabular data has played a significant role in data discovery tasks such as joinable and unionable table search. State-of-the-art approaches adopt large language models (LLMs) pre-trained over massive text corpora to learn and evaluate the table semantic relatedness. Existing methods typically follow a pretrain-and-finetune paradigm, namely fine-tuning an LLM using tabular data with table relatedness labels. To enhance model’s understanding of tabular data, recent studies include auxiliary tasks such as entity resolution and column type classification in the fine-tuning phase. In spite of achieving performance gain from these supervisions, there is a lack of study on how these supervisions complement or even contrast each other, leading to a subpar performance on the final data discovery tasks. In this paper, we propose a simple yet effective multi-task fine-tuning framework named DiscoverGPT that holistically discovers and leverages the intricate relationships among the supervisions to optimize the performance on the data discovery task. Moreover, DiscoverGPT is plug-and-play that allows a broad range of open-domain auxiliary tasks to be incorporated, by utilizing the generative power of LLMs. We demonstrate the usability and effectiveness of DiscoverGPT with baseline comparisons and ablation studies. DiscoverGPT outperforms the best performing baseline by up to 7% in F1 score.</abstract>
      <url hash="17142b89">2025.findings-naacl.21</url>
      <bibkey>hu-etal-2025-discovergpt</bibkey>
    </paper>
    <paper id="22">
      <title>Can <fixed-case>GPT</fixed-case>-4 Sway Experts’ Investment Decisions?</title>
      <author><first>Takehiro</first><last>Takayanagi</last></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Kiyoshi</first><last>Izumi</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>374-383</pages>
      <abstract>In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers’ decisions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers’ decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience decisions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the decisions of readers. We release our dataset to assist future research.</abstract>
      <url hash="66f617d6">2025.findings-naacl.22</url>
      <bibkey>takayanagi-etal-2025-gpt</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>P</fixed-case>oly<fixed-case>J</fixed-case>oin: Semantic Multi-key Joinable Table Search in Data Lakes</title>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chuan</first><last>Lei</last></author>
      <author><first>Xiao</first><last>Qin</last><affiliation>Amazon</affiliation></author>
      <author><first>Asterios</first><last>Katsifodimos</last><affiliation>Delft University of Technology</affiliation></author>
      <author><first>Christos</first><last>Faloutsos</last><affiliation>Amazon and Carnegie Mellon University</affiliation></author>
      <author><first>Huzefa</first><last>Rangwala</last><affiliation>Amazon and Computer Science, George Mason University</affiliation></author>
      <pages>384-395</pages>
      <abstract>Given a query table, how can we effectively discover multi-key joinable tables on the web? This can be seen as a retrieval task, where users can lookup on the web for tables related to an existing one. Searching and discovering such joinable tables is critical to data analysts and data scientists for reporting, establishing correlations and training machine learning models. Existing joinable table search methods have mostly focused on single key (unary) joins, where a single column is the join key. However, these methods are ineffective when dealing with join keys composed of multiple columns (n-ary joins), which are prevalent on web table corpora. In this paper, we introduce PolyJoin, which finds multi-key semantically-joinable tables on the web, given a query table. PolyJoin employs a multi-key encoder and a novel self-supervised training method to generate the representations of multiple join keys, preserving the alignment across multiple columns. In particular, PolyJoin is equipped with a hierarchical contrastive learning technique to further enhance the model’s semantic understanding of multi-key joinable tables. PolyJoin outperforms the state-of-the-art methods by 2.89% and 3.67% with respect to MAP@30 and R@30 on two real-world web table benchmarks, respectively.</abstract>
      <url hash="fda0431d">2025.findings-naacl.23</url>
      <bibkey>hu-etal-2025-polyjoin</bibkey>
    </paper>
    <paper id="24">
      <title>Marrying <fixed-case>LLM</fixed-case>s with Dynamic Forecasting: A Graph Mixture-of-expert Perspective</title>
      <author><first>Dapeng</first><last>Jiang</last></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>396-410</pages>
      <abstract>Dynamical system modeling is a crucial area of research in machine learning with extensive applications in physics and social science. Recent data-driven approaches often employ graph neural networks (GNNs) to learn relationships in dynamical systems using message passing mechanisms. Despite their advancements, these methods often suffer from performance degradation when it comes to potential environmental change with distribution shifts in real-world applications. In this work, we propose a new perspective which leverages large language models (LLMs) to enhance the generalization capabilities of dynamical system modeling. In particular, we develop a novel framework named LLM Judge with Graph Mixture-of-expert LEGO which incorporates multiple graph experts to learn diverse dynamics within the systems. More importantly, LEGO utilizes LLMs with hierarchical prompts at object, edge, and system levels as a context-aware routing function to determine which experts carry the most relevant information to different environments. The whole framework is optimized by updating the weights and expert parameters in an alternative fashion. Extensive experiments across various datasets demonstrate the effectiveness of our proposed LEGO in comparison to extensive baselines.</abstract>
      <url hash="fa7d5aaf">2025.findings-naacl.24</url>
      <bibkey>jiang-luo-2025-marrying</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>G</fixed-case>en: Multi-modal Interactive Dialogue System with Multi-turn Text-Image Generation</title>
      <author><first>Minbin</first><last>Huang</last></author>
      <author><first>Yanxin</first><last>Long</last><affiliation>Tencent Data Platform</affiliation></author>
      <author><first>Xinchi</first><last>Deng</last></author>
      <author><first>Ruihang</first><last>Chu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jiangfeng</first><last>Xiong</last><affiliation>Tencent Data Platform</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Hong</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qinglin</first><last>Lu</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <pages>411-426</pages>
      <abstract>Text-to-image (T2I) generation models have significantly advanced in recent years. However, effective interaction with these models is challenging for average users due to the need for specialized prompt engineering knowledge and the inability to perform multi-turn image generation, hindering a dynamic and iterative creation process. Recent attempts have tried to equip Multi-modal Large Language Models (MLLMs) with T2I models to bring the user’s natural language instructions into reality. Hence, the output modality of MLLMs is extended, and the multi-turn generation quality of T2I models is enhanced thanks to the strong multi-modal comprehension ability of MLLMs. However, many of these works face challenges in identifying correct output modalities and generating coherent images accordingly as the number of output modalities increases and the conversations go deeper. Therefore, we propose DialogGen, an effective pipeline to align off-the-shelf MLLMs and T2I models to build a Multi-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image generation. It is composed of drawing prompt alignment, careful training data curation, and error correction. Moreover, as the field of MIDS flourishes, comprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms of output modality correctness and multi-modal output coherence. To address this issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a comprehensive bilingual benchmark designed to assess the ability of MLLMs to generate accurate and coherent multi-modal content that supports image editing. It contains two evaluation metrics to measure the model’s ability to switch modalities and the coherence of the output images. Our extensive experiments on DialogBen and user study demonstrate the effectiveness of DialogGen in producing correct output modalities and coherent multi-modal outputs compared with other State-of-the-Art models. We hope that DialogBen can contribute to the community for building more powerful MIDS.</abstract>
      <url hash="7d49678c">2025.findings-naacl.25</url>
      <bibkey>huang-etal-2025-dialoggen</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>REL</fixed-case>ex<fixed-case>ED</fixed-case>: Retrieval-Enhanced Legal Summarization with Exemplar Diversity</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Chen</first><last>Jia</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Patrick</first><last>Goroncy</last><affiliation>Department of Informatics, Technische Universität München</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>427-434</pages>
      <abstract>This paper addresses the task of legal summarization, which involves distilling complex legal documents into concise, coherent summaries. Current approaches often struggle with content theme deviation and inconsistent writing styles due to their reliance solely on source documents. We propose RELexED, a retrieval-augmented framework that utilizes exemplar summaries along with the source document to guide the model. RELexED employs a two-stage exemplar selection strategy, leveraging a determinantal point process to balance the trade-off between similarity of exemplars to the query and diversity among exemplars, with scores computed via influence functions. Experimental results on two legal summarization datasets demonstrate that RELexED significantly outperforms models that do not utilize exemplars and those that rely solely on similarity-based exemplar selection.</abstract>
      <url hash="cc8840d8">2025.findings-naacl.26</url>
      <bibkey>t-y-s-s-etal-2025-relexed</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>CL</fixed-case>a<fixed-case>MP</fixed-case> 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models</title>
      <author><first>Shangda</first><last>Wu</last></author>
      <author><first>Yashan</first><last>Wang</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Guo</first><last>Zhancheng</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Monan</first><last>Zhou</last></author>
      <author><first>Jing</first><last>Chen</last></author>
      <author><first>Xuefeng</first><last>Mu</last></author>
      <author><first>Yuejie</first><last>Gao</last></author>
      <author><first>Yuanliang</first><last>Dong</last></author>
      <author><first>Jiafeng</first><last>Liu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Xiaobing</first><last>Li</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Feng</first><last>Yu</last><affiliation>Central Conservatory of Music</affiliation></author>
      <author><first>Maosong</first><last>Sun</last><affiliation>Tsinghua University</affiliation></author>
      <pages>435-451</pages>
      <abstract>Challenges in managing linguistic diversity and integrating various musical modalities are faced by current music information retrieval systems. These limitations reduce their effectiveness in a global, multimodal music environment. To address these issues, we introduce CLaMP 2, a system compatible with 101 languages that supports both ABC notation (a text-based musical notation format) and MIDI (Musical Instrument Digital Interface) for music information retrieval. CLaMP 2, pre-trained on 1.5 million ABC-MIDI-text triplets, includes a multilingual text encoder and a multimodal music encoder aligned via contrastive learning. By leveraging large language models, we obtain refined and consistent multilingual descriptions at scale, significantly reducing textual noise and balancing language distribution. Our experiments show that CLaMP 2 achieves state-of-the-art results in both multilingual semantic search and music classification across modalities, thus establishing a new standard for inclusive and global music information retrieval.</abstract>
      <url hash="3ce836db">2025.findings-naacl.27</url>
      <bibkey>wu-etal-2025-clamp</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>L</fixed-case>og<fixed-case>R</fixed-case>ules: Enhancing Log Analysis Capability of Large Language Models through Rules</title>
      <author><first>Xin</first><last>Huang</last></author>
      <author><first>Ting</first><last>Zhang</last></author>
      <author><first>Wen</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <pages>452-470</pages>
      <abstract>Currently, large language models (LLMs) have achieved impressive performance in natural language processing tasks. However, LLMs still exhibit many hallucinations when analyzing system logs, which is due to the implicit knowledge and rules in logs that LLMs cannot capture. Based on this, we propose LogRules, a lightweight log analysis framework that generates and utilizes rules through LLMs. LogRules consists of three stages: an induction stage, an alignment stage, and a reasoning stage. Firstly, in the induction stage, an strong LLM (e.g., GPT-4o-mini) is tasked with generating a series of rules related to logs, which are then validated on the training set. When the rules are confirmed to produce correct reasoning results, they are added to a rule repository. Secondly, considering that the LLMs with small size (<tex-math>\approx</tex-math>8B parameters) still face challenges in utilizing rules, we design an alignment method based on rule-case contrastive preference optimization (CPO) to effectively enhance the rule reasoning capabilities of these LLMs. Finally, in the reasoning stage, the LLM constructs prompt using the rule repository and performs log analysis on the test set. Experiments show that LogRules outperforms LLM-based methods in log parsing and anomaly detection tasks, and achieves better performance compared to case-based methods.</abstract>
      <url hash="ecba03cd">2025.findings-naacl.28</url>
      <bibkey>huang-etal-2025-logrules</bibkey>
    </paper>
    <paper id="29">
      <title>Audio Description Generation in the Era of <fixed-case>LLM</fixed-case>s and <fixed-case>VLM</fixed-case>s: A Review of Transferable Generative <fixed-case>AI</fixed-case> Technologies</title>
      <author><first>Yingqiang</first><last>Gao</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lukas</first><last>Fischer</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Alexa</first><last>Lintner</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <pages>471-490</pages>
      <abstract>Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.</abstract>
      <url hash="51fba01b">2025.findings-naacl.29</url>
      <bibkey>gao-etal-2025-audio</bibkey>
    </paper>
    <paper id="30">
      <title>Adaptive Retrieval-Augmented Generation for Conversational Systems</title>
      <author><first>Xi</first><last>Wang</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Procheta</first><last>Sen</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>491-503</pages>
      <abstract>With the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. While many existing studies agree on the necessity of Retrieval Augmented Generation (RAG), further investigation into the necessity and value of applying RAG to every turn of the conversation is needed. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models, joined with well-rounded analyses of various conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying if system responses require RAG to generate high-quality responses with high confidence. This study also identifies and shows the correlation between the generation’s confidence level and the relevance of the augmented knowledge. We have also released the implementation code and resources in https://github.com/wangxieric/RAGate.</abstract>
      <url hash="a8962ab6">2025.findings-naacl.30</url>
      <bibkey>wang-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="31">
      <title>Multimodal Generation with Consistency Transferring</title>
      <author><first>Junxiang</first><last>Qiu</last></author>
      <author><first>Jinda</first><last>Lu</last></author>
      <author><first>Shuo</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>504-513</pages>
      <abstract>Multimodal content generation has become an area of considerable interest. However, existing methods are hindered by limitations related to model constraints and training strategies: (1) Most current approaches rely on training models from scratch, resulting in inefficient training processes when extending these models; (2) There is a lack of constraints on adjacent steps within the models, leading to slow sampling and poor generation stability across various sampling methods. To address the issues, we introduce Multimodal Generation with Consistency Transferring (MGCT). The method introduces two key improvements: (1) A Model Consistency Transferring (MCT) strategy to acquire low-cost prior knowledge, increasing training efficiency and avoiding error accumulation; (2) A Layer Consistency Transferring (LCT) between adjacent steps, enhancing denoising capabilities at each step and improving model stability across various generation methods. These strategies ensure the consistency of jointly generated multimodal content and improving training efficiency. Experiments show that the algorithm enhances the model’s ability to capture actions and depict backgrounds more effectively. In both the AIST++ and Landscape datasets, it improves video generation speed by approximately 40% and quality by about 39.3%, while also achieving a slight 3% improvement in audio quality over the baseline.</abstract>
      <url hash="cfc5f78e">2025.findings-naacl.31</url>
      <bibkey>qiu-etal-2025-multimodal</bibkey>
    </paper>
    <paper id="32">
      <title>On the Impact of Noise in Differentially Private Text Rewriting</title>
      <author><first>Stephen</first><last>Meisenbacher</last></author>
      <author><first>Maulik</first><last>Chevli</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>514-532</pages>
      <abstract>The field of text privatization often leverages the notion of *Differential Privacy* (DP) to provide formal guarantees in the rewriting or obfuscation of sensitive textual data. A common and nearly ubiquitous form of DP application necessitates the addition of calibrated noise to vector representations of text, either at the data- or model-level, which is governed by the privacy parameter <tex-math>\varepsilon</tex-math>. However, noise addition almost undoubtedly leads to considerable utility loss, thereby highlighting one major drawback of DP in NLP. In this work, we introduce a new sentence infilling privatization technique, and we use this method to explore the effect of noise in DP text rewriting. We empirically demonstrate that non-DP privatization techniques excel in utility preservation and can find an acceptable empirical privacy-utility trade-off, yet cannot outperform DP methods in empirical privacy protections. Our results highlight the significant impact of noise in current DP rewriting mechanisms, leading to a discussion of the merits and challenges of DP in NLP as well as the opportunities that non-DP methods present.</abstract>
      <url hash="355e2dd7">2025.findings-naacl.32</url>
      <bibkey>meisenbacher-etal-2025-impact</bibkey>
    </paper>
    <paper id="33">
      <title>Teaching Large Language Models Number-Focused Headline Generation With Key Element Rationales</title>
      <author><first>Zhen</first><last>Qian</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiaofei</first><last>Xu</last></author>
      <author><first>Feng</first><last>Xia</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>533-550</pages>
      <abstract>Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy.</abstract>
      <url hash="c8dda3f5">2025.findings-naacl.33</url>
      <bibkey>qian-etal-2025-teaching</bibkey>
    </paper>
    <paper id="34">
      <title>Zero-Shot Strategies for Length-Controllable Summarization</title>
      <author><first>Fabian</first><last>Retkowski</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>551-572</pages>
      <abstract>Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs’ length control capabilities across multiple measures and propose practical methods to improve controllability. Our experiments with LLaMA 3 reveal stark differences in length adherence across measures and highlight inherent biases of the model. To address these challenges, we introduce a set of methods: length approximation, target adjustment, sample filtering, and automated revisions. By combining these methods, we demonstrate substantial improvements in length compliance while maintaining or enhancing summary quality, providing highly effective zero-shot strategies for precise length control without the need for model fine-tuning or architectural changes. With our work, we not only advance our understanding of LLM behavior in controlled text generation but also pave the way for more reliable and adaptable summarization systems in real-world applications.</abstract>
      <url hash="ef02ede1">2025.findings-naacl.34</url>
      <bibkey>retkowski-waibel-2025-zero</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>SIMPLOT</fixed-case>: Enhancing Chart Question Answering by Distilling Essentials</title>
      <author><first>Wonjoong</first><last>Kim</last></author>
      <author><first>Sangwu</first><last>Park</last></author>
      <author><first>Yeonjun</first><last>In</last></author>
      <author><first>Seokwon</first><last>Han</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Chanyoung</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>573-593</pages>
      <abstract>Recently, interpreting complex charts with logical reasoning has emerged as challenges due to the development of vision-language models. A prior state-of-the-art (SOTA) model has presented an end-to-end method that leverages the vision-language model to convert charts into table format utilizing Large Language Model (LLM) for reasoning. However, unlike natural images, charts contain a mix of essential and irrelevant information required for chart reasoning, and we discover that this characteristic can lower the performance of chart-to-table extraction. In this paper, we introduce SIMPLOT, a method designed to extract only the elements necessary for chart reasoning. The proposed method involves two steps: 1) training to mimic a simple plot that contains only the essential information from a complex chart for table extraction, followed by 2) performing reasoning based on the table. Our model enables accurate chart reasoning without the need for additional annotations or datasets, and its effectiveness is demonstrated through various experiments.</abstract>
      <url hash="570c419a">2025.findings-naacl.35</url>
      <bibkey>kim-etal-2025-simplot</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>A</fixed-case>ny2<fixed-case>P</fixed-case>ix: Image Editing with Multi-Modal Prompts</title>
      <author><first>Shufan</first><last>Li</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Harkanwar</first><last>Singh</last></author>
      <author><first>Aditya</first><last>Grover</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>594-619</pages>
      <abstract>Image Editing has made incredible progress in recent years. Earliest work only supported caption-guided editing. Recently, free-form text instructions and reference images are incorporated to allow more flexibility. However, existing methods still struggle with complicated editing instructions involving multiple objects or reference images. We present InstructAny2Pix, a novel image editing model that leverages a multi-modal LLM to execute complicated edit instructions. Compared with previous, works, InstructAny2Pix extends the flexibility of edit instructions in three ways: First, it can perform complex instructions involving multiple object edits; Second, it supports interleaving text instructions with multiple reference images; Third, it supports audio and music inputs as part of edit prompts, unlocking many creative applications, such as album cover generation and music-inspired merchandise design. To evaluate the effectiveness of InstructAny2Pix, we propose two new benchmark datasets MM-Inst and Dream-booth++ consisting of human written, multi-modal prompts. InstructAny2Pix outperforms baselines in these two proposed multi-modal benchmarks, as well as conventional image editing benchmarks such as InstructPix2Pix.</abstract>
      <url hash="fe2eb128">2025.findings-naacl.36</url>
      <bibkey>li-etal-2025-instructany2pix</bibkey>
    </paper>
    <paper id="37">
      <title>Lost in Overlap: Exploring Logit-based Watermark Collision in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yiyang</first><last>Luo</last></author>
      <author><first>Ke</first><last>Lin</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Chao</first><last>Gu</last></author>
      <author><first>Jiahui</first><last>Hou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Luo</first><last>Ping</last></author>
      <pages>620-637</pages>
      <abstract>The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation.In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.</abstract>
      <url hash="e6fa5ae1">2025.findings-naacl.37</url>
      <bibkey>luo-etal-2025-lost</bibkey>
    </paper>
    <paper id="38">
      <title>Prompt-Guided Selective Masking Loss for Context-Aware Emotive Text-to-Speech</title>
      <author><first>Yejin</first><last>Jeon</last></author>
      <author><first>Youngjae</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jihyun</first><last>Lee</last></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>638-650</pages>
      <abstract>Emotional dialogue speech synthesis (EDSS) aims to generate expressive speech by leveraging the dialogue context between interlocutors. This is typically done by concatenating global representations of previous utterances as conditions for text-to-speech (TTS) systems. However, such approaches overlook the importance of integrating localized acoustic cues that convey emotion. To address this, we introduce a novel approach that utilizes a large language model (LLM) to generate holistic emotion tags based on prior dialogue context, while also pinpointing key words in the target utterance that align with the predicted emotional state. Furthermore, we enhance the emotional richness of synthesized speech by incorporating concentrated acoustic features of these key words through a novel selective audio masking loss function. This methodology not only improves emotional expressiveness, but also facilitates automatic emotion speech generation during inference by eliminating the need for manual emotion tag selection. Comprehensive subjective and objective evaluations and analyses demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="5f9d08ca">2025.findings-naacl.38</url>
      <bibkey>jeon-etal-2025-prompt</bibkey>
    </paper>
    <paper id="39">
      <title>Identifying and Mitigating Social Bias Knowledge in Language Models</title>
      <author><first>Ruizhe</first><last>Chen</last></author>
      <author><first>Yichen</first><last>Li</last></author>
      <author><first>Jianfei</first><last>Yang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Joey Tianyi</first><last>Zhou</last><affiliation>A*STAR Centre for Frontier AI Research</affiliation></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>651-672</pages>
      <abstract>Generating fair and accurate predictions plays a pivotal role in deploying pre-trained language models (PLMs) in the real world. However, existing debiasing methods may inevitably generate incorrect or nonsensical predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. This paper introduces a novel debiasing framework that first identifies the encoding locations of biases within language models and then applies the Fairness-Stamp (FAST). FAST focuses on fine-grained, individual bias mitigation and integrates a lightweight network into PLMs, specifically targeting identified biases while preserving essential knowledge and maintaining factual integrity. We also present BiaScope, a new benchmark comprising datasets and metrics designed to evaluate the retention of commonsense knowledge and the generalization across paraphrased social biases. Our extensive experiments across multiple datasets demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in PLMs. Code will be publicly available.</abstract>
      <url hash="7ec9a178">2025.findings-naacl.39</url>
      <bibkey>chen-etal-2025-identifying</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>D</fixed-case>ia<fixed-case>S</fixed-case>ynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications</title>
      <author><first>Sathya Krishnan</first><last>Suresh</last></author>
      <author><first>Wu</first><last>Mengjun</last></author>
      <author><first>Tushar</first><last>Pranav</last><affiliation>Singapore Institute of Technology</affiliation></author>
      <author><first>EngSiong</first><last>Chng</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>673-690</pages>
      <url hash="b12c1143">2025.findings-naacl.40</url>
      <bibkey>suresh-etal-2025-diasynth</bibkey>
    </paper>
    <paper id="41">
      <title>Do Not Design, Learn: A Trainable Scoring Function for Uncertainty Estimation in Generative <fixed-case>LLM</fixed-case>s</title>
      <author><first>Duygu Nur</first><last>Yaldiz</last></author>
      <author><first>Yavuz Faruk</first><last>Bakman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Baturalp</first><last>Buyukates</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Chenyang</first><last>Tao</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Dimitrios</first><last>Dimitriadis</last><affiliation>Amazon</affiliation></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <pages>691-713</pages>
      <abstract>Uncertainty estimation (UE) of generative large language models (LLMs) is crucial for evaluating the reliability of generated sequences. A significant subset of UE methods utilize token probabilities to assess uncertainty, aggregating multiple token probabilities into a single UE score using a scoring function. Existing scoring functions for probability-based UE, such as length-normalized scoring and semantic contribution-based weighting, are designed to solve certain aspects of the problem but exhibit limitations, including the inability to handle biased probabilities and complex semantic dependencies between tokens. To address these issues, in this work, we propose Learnable Response Scoring (LARS) function, a novel scoring function that leverages supervised data to capture complex dependencies between tokens and probabilities, thereby producing more reliable and calibrated response scores in computing the uncertainty of LLM generations. Our comprehensive experiments across question-answering and arithmetical reasoning tasks with various datasets demonstrate that LARS significantly outperforms existing scoring functions, achieving improvements of up to 16% AUROC score.</abstract>
      <url hash="ed0b7223">2025.findings-naacl.41</url>
      <bibkey>yaldiz-etal-2025-design</bibkey>
    </paper>
    <paper id="42">
      <title>Joint Learning Event-Specific Probe and Argument Library with Differential Optimization for Document-Level Multi-Event Extraction</title>
      <author><first>Jianpeng</first><last>Hu</last></author>
      <author><first>Chao</first><last>Xue</last></author>
      <author><first>Chunqing</first><last>Yu</last><affiliation>Tongji University</affiliation></author>
      <author><first>JiaCheng</first><last>Xu</last></author>
      <author><first>Chengxiang</first><last>Tan</last></author>
      <pages>714-726</pages>
      <abstract>Document-level multi-event extraction aims to identify a list of event types and corresponding arguments from the document. However, most of the current methods neglect the fine-grained difference among events in multi-event documents, which leads to event confusion and missing. This is also one of the reasons why the recall and F1-score of multi-event recognition are lower compared to single-event recognition. In this paper, we propose an event-specific probe-based method to sniff multiple events by querying each corresponding argument library, which uses a novel probe-label alignment method for differential optimization. In addition, the role contrastive loss and probe consistent loss are designed to fine-tune the fine-grained role differences and probe differences in each event. The experimental results on two general datasets show that our method outperforms the state-of-the-art method in the F1-score, especially in the recall of multi-events.</abstract>
      <url hash="ecd1a8b2">2025.findings-naacl.42</url>
      <bibkey>hu-etal-2025-joint</bibkey>
    </paper>
    <paper id="43">
      <title>Synonym-unaware Fast Adversarial Training against Textual Adversarial Attacks</title>
      <author><first>Yichen</first><last>Yang</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Kun</first><last>He</last><affiliation>Huazhong University of Sceince and Technology</affiliation></author>
      <pages>727-739</pages>
      <abstract>Numerous adversarial defense methods have been proposed to strengthen the robustness of Natural Language Processing (NLP) models against adversarial attacks. However, many of these methods rely on predetermined linguistic knowledge and assume that attackers’ synonym candidates are known, which is often unrealistic. In this work, we investigate adversarial training in the embedding space and introduce a Fast Adversarial Training (FAT) method to improve the model robustness without requiring synonym awareness. FAT leverages single-step perturbation generation and effective perturbation initialization based on two key insights: (1) adversarial perturbations generated by single-step and multi-step gradient ascent are similar, and (2) perturbations generated on the same training sample across successive epochs exhibit resemblance. By employing single-step gradient ascent and leveraging historical perturbation information, FAT not only expedites the training process but also efficiently initializes perturbations. Extensive experiments demonstrate that FAT significantly enhances the robustness of popular NLP models under scenarios where synonyms are unknown, outperforming other defense baselines under various character-level and word-level attacks.</abstract>
      <url hash="39944f86">2025.findings-naacl.43</url>
      <bibkey>yang-etal-2025-synonym</bibkey>
    </paper>
    <paper id="44">
      <title>Tethering Broken Themes: Aligning Neural Topic Models with Labels and Authors</title>
      <author><first>Mayank</first><last>Nagda</last></author>
      <author><first>Phil</first><last>Ostheimer</last><affiliation>RPTU Kaiserslautern-Landau</affiliation></author>
      <author><first>Sophie</first><last>Fellenz</last><affiliation>Universität Kaiserslautern</affiliation></author>
      <pages>740-760</pages>
      <abstract>Topic models are a popular approach for extracting semantic information from large document collections. However, recent studies suggest that the topics generated by these models often do not align well with human intentions. Although metadata such as labels and authorship information are available, it has not yet been effectively incorporated into neural topic models. To address this gap, we introduce FANToM, a novel method to align neural topic models with both labels and authorship information. FANToM allows for the inclusion of this metadata when available, producing interpretable topics and author distributions for each topic. Our approach demonstrates greater expressiveness than conventional topic models by learning the alignment between labels, topics, and authors. Experimental results show that FANToM improves existing models in terms of both topic quality and alignment. Additionally, it identifies author interests and similarities.</abstract>
      <url hash="4fc0d1e1">2025.findings-naacl.44</url>
      <bibkey>nagda-etal-2025-tethering</bibkey>
    </paper>
    <paper id="45">
      <title>Towards Zero-Shot Multimodal Machine Translation</title>
      <author><first>Matthieu</first><last>Futeral</last></author>
      <author><first>Cordelia</first><last>Schmid</last><affiliation>Google, INRIA and Inria</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>761-778</pages>
      <abstract>Current multimodal machine translation (MMT) systems rely on fully supervised data (i.e sentences with their translations and accompanying images), which is costly to collect and prevents the extension of MMT to language pairs with no such data. We propose a method to bypass the need for fully supervised data to train MMT systems, using multimodal English data only. Our method ( ZeroMMT) consists in adapting a strong text-only machine translation (MT) model by training it jointly on two objectives: visually conditioned masked language modelling and the Kullback-Leibler divergence between the original MT and new MMT outputs. We evaluate on standard MMT benchmarks and on CoMMuTE, a contrastive test set designed to evaluate how well models use images to disambiguate translations. ZeroMMT obtains disambiguation results close to state-of-the-art MMT models trained on fully supervised examples. To prove that ZeroMMT generalizes to languages with no fully supervised training data, we extend CoMMuTE to three new languages: Arabic, Russian and Chinese. We also show that we can control the trade-off between disambiguation capabilities and translation fidelity at inference time using classifier-free guidance and without any additional data. Our code, data and trained models are publicly accessible.</abstract>
      <url hash="e9cb5b52">2025.findings-naacl.45</url>
      <bibkey>futeral-etal-2025-towards</bibkey>
    </paper>
    <paper id="46">
      <title>Large-Scale Corpus Construction and Retrieval-Augmented Generation for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Poetry: New Method and Data Insights</title>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Lan</first><last>Lan</last></author>
      <author><first>Jiahuan</first><last>Cao</last></author>
      <author><first>Hiuyi</first><last>Cheng</last></author>
      <author><first>Kai</first><last>Ding</last><affiliation>INTSIG Information</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last></author>
      <pages>779-817</pages>
      <abstract>Ancient Chinese Poetry (ACP), a critical aspect of Chinese cultural heritage, presents unique challenges for Large Language Models (LLMs). One of the most pressing challenges is the significant hallucination issues faced by LLMs due to data scarcity and limited ability of general LLMs when dealing with ACP. To address these challenges, this paper constructs the ACP-Corpus, which encompasses 1.1 million ancient poems and 990K related texts, designed to enhance the training and performance of LLMs. Alongside this, we develop the ACP-QA dataset, comprising over 12 million question-answer pairs across 24 task categories, and the ACP-Eval dataset for rigorous evaluation purposes, containing 7,050 entries. Building on this resources, we propose the ACP-RAG framework, a specialized Retrieval-Augmented Generation (RAG) approach that significantly improves the performance of LLMs in the domain of ancient poetry from 49.2% to 89.0%. The ACP-RAG contains five modules of semantic coarse-grained retrieval, semantic fine-grained retrieval, keyword retrieval, keyword matching, and context filtering. Experiments show that ACP-RAG achieves a promising response accuracy of 89.0%, surpassing existing LLMs by a remarkable margin. We believe this work not only advances the capabilities of LLMs in processing ancient Chinese poetry but also contributes to the preservation and innovative development within this rich literary tradition. The datasets and code are available at https://github.com/SCUT-DLVCLab/ACP-RAG.</abstract>
      <url hash="84ec3ce1">2025.findings-naacl.46</url>
      <bibkey>liu-etal-2025-large</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>O</fixed-case>pen<fixed-case>B</fixed-case>io<fixed-case>NER</fixed-case>: Lightweight Open-Domain Biomedical Named Entity Recognition Through Entity Type Description</title>
      <author><first>Alessio</first><last>Cocchieri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Giacomo</first><last>Frisoni</last></author>
      <author><first>Marcos</first><last>Martínez Galindo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <author><first>Giuseppe</first><last>Tagliavini</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Francesco</first><last>Candoli</last><affiliation>University of Bologna</affiliation></author>
      <pages>818-837</pages>
      <abstract>Biomedical Named Entity Recognition (BioNER) faces significant challenges in real-world applications due to limited annotated data and the constant emergence of new entity types, making zero-shot learning capabilities crucial. While Large Language Models (LLMs) possess extensive domain knowledge necessary for specialized fields like biomedicine, their computational costs often make them impractical. To address these challenges, we introduce OpenBioNER, a lightweight BERT-based cross-encoder architecture that can identify any biomedical entity using only its description, eliminating the need for retraining on new, unseen entity types. Through comprehensive evaluation on established biomedical benchmarks, we demonstrate that OpenBioNER surpasses state-of-the-art baselines, including specialized 7B NER LLMs and GPT-4o, achieving up to 10% higher F1 scores while using 110M parameters only. Moreover, OpenBioNER outperforms existing small-scale models that match textual spans with entity types rather than descriptions, both in terms of accuracy and computational efficiency.</abstract>
      <url hash="24112436">2025.findings-naacl.47</url>
      <bibkey>cocchieri-etal-2025-openbioner</bibkey>
    </paper>
    <paper id="48">
      <title>Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum</title>
      <author><first>Ryan Soh-Eun</first><last>Shim</last><affiliation>Ludwig-Maximilians-Universität München, University of Stuttgart, Universität Stuttgart and Institute for Natural Language Processing, University of Stuttgart</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>838-849</pages>
      <abstract>There is increasing interest in looking at dialects in NLP. However, most work to date still treats dialects as discrete categories. For instance, evaluative work in variation-oriented NLP for English often works with Indian English or African-American Venacular English as homogeneous categories, yet even within one variety there is substantial variation. We examine within-dialect variation and show that performance critically varies within categories. We measure speech-to-text performance on Italian dialects, and empirically observe a geographical performance disparity. This disparity correlates substantially (-0.5) with linguistic similarity to the highest performing dialect variety. We cross-examine our results against dialectometry methods, and interpret the performance disparity to be due to a bias towards dialects that are more similar to the standard variety in the speech-to-text model examined. We additionally leverage geostatistical methods to predict zero-shot performance at unseen sites, and find the incorporation of geographical information to substantially improve prediction performance, indicating there to be geographical structure in the performance distribution.</abstract>
      <url hash="2adec0a7">2025.findings-naacl.48</url>
      <bibkey>shim-plank-2025-dialetto</bibkey>
    </paper>
    <paper id="49">
      <title>Linguistically Grounded Analysis of Language Models using Shapley Head Values</title>
      <author><first>Marcell</first><last>Fekete</last></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>850-865</pages>
      <abstract>Understanding how linguistic knowledge is encoded in language models is crucial for improving their generalisation capabilities. In this paper, we investigate the processing of morphosyntactic phenomena, by leveraging a recently proposed method for probing language models via Shapley Head Values (SHVs). Using the English language BLiMP dataset, we test our approach on two widely used models, BERT and RoBERTa, and compare how linguistic constructions such as anaphor agreement and filler-gap dependencies are handled. Through quantitative pruning and qualitative clustering analysis, we demonstrate that attention heads responsible for processing related linguistic phenomena cluster together. Our results show that SHV-based attributions reveal distinct patterns across both models, providing insights into how language models organize and process linguistic information. These findings support the hypothesis that language models learn subnetworks corresponding to linguistic theory, with potential implications for cross-linguistic model analysis and interpretability in Natural Language Processing (NLP).</abstract>
      <url hash="9d8a1eb9">2025.findings-naacl.49</url>
      <bibkey>fekete-bjerva-2025-linguistically</bibkey>
    </paper>
    <paper id="50">
      <title>How Do Large Language Models Perform in Dynamical System Modeling</title>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Binqi</first><last>Chen</last></author>
      <author><first>Haixin</first><last>Wang</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>866-880</pages>
      <abstract>This paper studies the problem of dynamical system modeling, which involves the evolution of multiple interacting objects. Recent data-driven methods often utilize graph neural networks (GNNs) to learn these interactions by optimizing the neural network in an end-to-end fashion. While large language models (LLMs) have shown exceptional zero-shot performance across various applications, their potential for modeling dynamical systems has not been extensively explored. In this work, we design prompting techniques for dynamical system modeling and systematically evaluate the capabilities of LLMs on two tasks, including dynamic forecasting and relational reasoning. An extensive benchmark LLM4DS across nine datasets is built for performance comparison. Our extensive experiments yield several key findings: (1) LLMs demonstrate competitive performance without training compared to state-of-the-art methods in dynamical system modeling. (2) LLMs effectively infer complex interactions among objects to capture system evolution. (3) Prompt engineering plays a crucial role in enabling LLMs to accurately understand and predict the evolution of systems.</abstract>
      <url hash="818cfa0c">2025.findings-naacl.50</url>
      <bibkey>luo-etal-2025-large</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>LMM</fixed-case>s-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
      <author><first>Kaichen</first><last>Zhang</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Peiyuan</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Fanyi</first><last>Pu</last></author>
      <author><first>Joshua Adrian</first><last>Cahyono</last></author>
      <author><first>Kairui</first><last>Hu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Shuai</first><last>Liu</last></author>
      <author><first>Yuanhan</first><last>Zhang</last></author>
      <author><first>Jingkang</first><last>Yang</last></author>
      <author><first>Chunyuan</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ziwei</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>881-916</pages>
      <abstract>The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models’ generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs.</abstract>
      <url hash="e64a10ea">2025.findings-naacl.51</url>
      <bibkey>zhang-etal-2025-lmms</bibkey>
    </paper>
    <paper id="52">
      <title>Pairwise Prompt-Based Tuning with Parameter Efficient Fast Adaptation for Generalized Zero-Shot Intent Detection</title>
      <author><first>Xiaotong</first><last>Zhang</last></author>
      <author><first>Qianru</first><last>Zhou</last></author>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>917-929</pages>
      <abstract>Generalized zero-shot intent detection (GZID) aims to recognize the labels of utterances from both seen and unseen intents by utilizing the knowledge learned from seen intents. Enhancing the generalization ability from seen intents to unseen intents is a key challenge in the GZID setting. Existing methods attempt to tackle this challenge by distinguishing unseen intents from seen intents or focusing on enhancing the model discriminability. However, the challenge is not solved substantially as they ignore to promote the representation learning ability of the model itself and neglect to strengthen the model adaptability to new tasks, resulting in overfitting on the seen intents. In this paper, we propose a pairwise prompt-based tuning model with parameter efficient fast adaptation which involves two training steps. In the first step, we leverage hybrid contrastive learning in discriminant space and masked language modeling to make predictions at both sentence and token levels, which can enhance the model discriminability and representation learning ability respectively. In the second step, we design a pipeline for generating and filtering unseen data by only providing unseen intent labels, and utilize parameter-efficient fine-tuning to quickly adapt to unseen intents. Experiments on four intent detection datasets demonstrate that our two-step training method has better comprehension and generalization capabilities.</abstract>
      <url hash="171b8196">2025.findings-naacl.52</url>
      <bibkey>zhang-etal-2025-pairwise</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>F</fixed-case>aithful<fixed-case>P</fixed-case>ersona: Balancing Faithfulness and Personalization in Code Explanations through Self-Critique</title>
      <author><first>Zhuang</first><last>Luo</last></author>
      <author><first>Yichuan</first><last>Li</last></author>
      <author><first>Zexing</first><last>Xu</last></author>
      <author><first>Kyumin</first><last>Lee</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>S. Rasoul</first><last>Etesami</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>930-944</pages>
      <abstract>Code explanations are crucial in real-world life, from educating students to aligning technical projects with business goals. However, existing approaches face challenges balancing faithfulness to the original code and personalization for diverse user needs. This paper addresses these challenges by introducing a novel benchmark and method for generating faithful personalized code explanations. Our benchmark, FaithfulPersonaCodeX, incorporates code samples and user profiles, employing various evaluation metrics to evaluate both faithfulness and personalization. We propose DISCO, a new method that uses a self-critique mechanism and two-stage optimization to balance faithfulness and personalization in code explanations, addressing the limitations of current large language model approaches. Our proposed model, DISCO, achieves a notable 3.7% improvement in Pass@5 compared to the strong baseline method, Self-Consistency, while maintaining high personalization with a 61.08% win rate in the LLM-as-a-Judge evaluation, effectively balancing faithfulness and user-specific needs in code explanations.</abstract>
      <url hash="c8dbc2e5">2025.findings-naacl.53</url>
      <bibkey>luo-etal-2025-faithfulpersona</bibkey>
    </paper>
    <paper id="54">
      <title>Efficient Multi-Agent Collaboration with Tool Use for Online Planning in Complex Table Question Answering</title>
      <author><first>Wei</first><last>Zhou</last><affiliation>Robert Bosch GmbH, Bosch</affiliation></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <pages>945-968</pages>
      <abstract>Complex table question answering (TQA) aims to answer questions that require complex reasoning, such as multi-step or multi-category reasoning, over data represented in tabular form. Previous approaches demonstrate notable performance by leveraging either closed-source large language models (LLMs) or fine-tuned open-weight LLMs. However, fine-tuning LLMs requires high-quality training data, which is costly to obtain. The use of closed-source LLMs poses accessibility challenges and leads to reproducibility issues. In this paper, we propose Multi Agent Collaboration with Tool use (MACT), a framework that requires neither fine-tuning nor closed-source models. In MACT, a planning agent and a coding agent that also make use of tools collaborate for TQA. MACT outperforms previous SoTA systems on three out of four benchmarks and performs comparably to the larger and more expensive closed-source model GPT-4 on two benchmarks, even when using only open-weight models without any fine-tuning. Our extensive analyses prove the effectiveness of MACT’s multi-agent collaboration in TQA. We release our code publicly.</abstract>
      <url hash="a0bc8540">2025.findings-naacl.54</url>
      <bibkey>zhou-etal-2025-efficient</bibkey>
    </paper>
    <paper id="55">
      <title>Ground Every Sentence: Improving Retrieval-Augmented <fixed-case>LLM</fixed-case>s with Interleaved Reference-Claim Generation</title>
      <author><first>Sirui</first><last>Xia</last></author>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <author><first>Weikang</first><last>Zhou</last></author>
      <author><first>Jiaji</first><last>Deng</last></author>
      <author><first>Fei</first><last>Yu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>969-988</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. To enhance credibility and verifiability in RAG systems, Attributed Text Generation (ATG) is proposed, which provides citations to retrieval knowledge in LLM-generated responses. Prior methods mainly adopt coarse-grained attributions, with passage-level or paragraph-level references or citations, which fall short in verifiability. This paper proposes ReClaim(Refer &amp; Claim), a fine-grained ATG method that alternates the generation of references and answers step by step. Different from previous coarse-grained attribution, ReClaim provides sentence-level citations in long-form question-answering tasks. With extensive experiments, we verify the effectiveness of ReClaim in extensive settings, achieving a citation accuracy rate of 90%.</abstract>
      <url hash="c12c279c">2025.findings-naacl.55</url>
      <bibkey>xia-etal-2025-ground</bibkey>
    </paper>
    <paper id="56">
      <title>Understanding the Role of Mental Models in User Interaction with an Adaptive Dialog Agent</title>
      <author><first>Lindsey Morgan</first><last>Vanderlyn</last></author>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Thang</first><last>Vu</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <pages>989-1015</pages>
      <abstract>Mental models play an important role in whether user interactions with intelligent systems, such as dialog agents, are successful. Adaptive dialog systems present the opportunity to align a dialog agent’s behavior with heterogeneous user expectations. However, there has been little research into what mental models users form when interacting with a task-oriented dialog system, how these models affect users’ interactions, or what role system adaptation can play in this process. This can make it challenging to avoid damage to human-AI partnership. In this work, we collect a new publicly available dataset for exploring user mental models of information seeking dialog systems. We demonstrate that users have a variety of conflicting mental models about such systems, the validity of which directly impacts the success and perception of their interactions. Furthermore, we show that adapting a dialog agent’s behavior to better align with users’ mental models, even when done implicitly, can improve dialog efficiency, success, and user perception of the interaction. This shows that implicit adaptation can be beneficial for task-oriented dialog systems, so long as developers understand the mental models of their users.</abstract>
      <url hash="d0d8364e">2025.findings-naacl.56</url>
      <bibkey>vanderlyn-etal-2025-understanding</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>C</fixed-case>o<fixed-case>PERL</fixed-case>ex: Content Planning with Event-based Representations for Legal Case Summarization</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Youssef</first><last>Farag</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1016-1032</pages>
      <abstract>Legal professionals often struggle with lengthy judgments and require efficient summarization for quick comprehension. To address this challenge, we investigate the need for structured planning in legal case summarization, particularly through event-centric representations that reflect the narrative nature of legal case documents. We propose our framework, CoPERLex, which operates in three stages: first, it performs content selection to identify crucial information from the judgment; second, the selected content is utilized to generate intermediate plans through event-centric representations modeled as Subject-Verb-Object tuples; and finally, it generates coherent summaries based on both the content and the structured plan. Our experiments on four legal summarization datasets demonstrate the effectiveness of integrating content selection and planning components, highlighting the advantages of event-centric plans over traditional entity-centric approaches in the context of legal judgements.</abstract>
      <url hash="22890085">2025.findings-naacl.57</url>
      <bibkey>t-y-s-s-etal-2025-coperlex</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>D</fixed-case>is<fixed-case>C</fixed-case>omp: A Two-Stage Prompt Optimization Framework Combining Task-Agnostic and Task-Aware Compression</title>
      <author><first>Liu</first><last>Quancai</last></author>
      <author><first>Haihui</first><last>Fan</last><affiliation>Institute of Information Engineering,CAS</affiliation></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Lixiangfang</first><last>Lixiangfang</last></author>
      <author><first>Lichuanrong</first><last>Lichuanrong</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>1033-1044</pages>
      <abstract>Large language models (LLMs) exhibit exceptional performance across a wide range of natural language processing tasks, often relying on lengthy prompts to harness their full capabilities. However, extended prompts can lead to substantial computational overhead and increased hardware demands, limiting the scalability and efficiency of such models. In this paper, we propose DisComp, a two-stage prompt compression framework based on knowledge distillation that combines task-agnostic and task-aware strategies, designed to efficiently compress prompt length without compromising performance.In the first stage, task-agnostic compression is achieved through knowledge distillation, transferring the summarization capabilities of a LLM to a smaller, more efficient model. The distillation process combines cross-entropy loss and keyword matching loss to ensure the smaller model generates concise and informative summaries. In the second stage, sentence-level pruning is applied, where sentences are ranked by relevance to the query, and irrelevant sentences are pruned to retain only task-critical information. We evaluate our method on three benchmark datasets, LongBench , ZeroSCROLLS and NaturalQuestions. The results show that DisComp significantly outperforms previous task-agnostic and task-specific compression approaches, and it is up to 6.56× faster at inference compared to the best token-level compression method.</abstract>
      <url hash="cd154829">2025.findings-naacl.58</url>
      <bibkey>quancai-etal-2025-discomp</bibkey>
    </paper>
    <paper id="59">
      <title>A Large-Scale Benchmark for <fixed-case>V</fixed-case>ietnamese Sentence Paraphrases</title>
      <author><first>Sang Quang</first><last>Nguyen</last></author>
      <author><first>Kiet Van</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <pages>1045-1060</pages>
      <abstract>This paper presents ViSP, a high-quality Vietnamese dataset for sentence paraphrasing, consisting of 1.2M original–paraphrase pairs collected from various domains. The dataset was constructed using a hybrid approach that combines automatic paraphrase generation with manual evaluation to ensure high quality. We conducted experiments using methods such as back-translation, EDA, and baseline models like BART and T5, as well as large language models (LLMs), including GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To the best of our knowledge, this is the first large-scale study on Vietnamese paraphrasing. We hope that our dataset and findings will serve as a valuable foundation for future research and applications in Vietnamese paraphrase tasks. The dataset is available for research purposes at <url>https://github.com/ngwgsang/ViSP</url>.</abstract>
      <url hash="7751d47e">2025.findings-naacl.59</url>
      <bibkey>nguyen-nguyen-2025-large</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>RAMQA</fixed-case>: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering</title>
      <author><first>Yang</first><last>Bai</last><affiliation>Facebook</affiliation></author>
      <author><first>Christan</first><last>Grant</last><affiliation>University of Florida</affiliation></author>
      <author><first>Daisy Zhe</first><last>Wang</last><affiliation>University of Florida</affiliation></author>
      <pages>1061-1076</pages>
      <abstract>Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Data and code will be made public once the paper is accepted.</abstract>
      <url hash="04f85d0e">2025.findings-naacl.60</url>
      <bibkey>bai-etal-2025-ramqa</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>CAT</fixed-case>: Multimodal Communication Annotations for Teams</title>
      <author><first>Adarsh</first><last>Pyarelal</last><affiliation>University of Arizona</affiliation></author>
      <author><first>John M</first><last>Culnan</last><affiliation>US Department of Veterans Affairs</affiliation></author>
      <author><first>Ayesha</first><last>Qamar</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Meghavarshini</first><last>Krishnaswamy</last></author>
      <author><first>Yuwei</first><last>Wang</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Cheonkam</first><last>Jeong</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Md Messal Monem</first><last>Miah</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shahriar</first><last>Hormozi</last></author>
      <author><first>Jonathan</first><last>Tong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>1077-1111</pages>
      <abstract>Successful teamwork requires team members to understand each other and communicate effectively, managing multiple linguistic and paralinguistic tasks at once. Because of the potential for interrelatedness of these tasks, it is important to have the ability to make multiple types of predictions on the same dataset. Here, we introduce Multimodal Communication Annotations for Teams (MultiCAT), a speech- and text-based dataset consisting of audio recordings, automated and hand-corrected transcriptions. MultiCAT builds upon data from teams working collaboratively to save victims in a simulated search and rescue mission, and consists of annotations and benchmark results for the following tasks: (1) dialog act classification, (2) adjacency pair detection, (3) sentiment and emotion recognition, (4) closed-loop communication detection, and (5) vocal (phonetic) entrainment detection. We also present exploratory analyses on the relationship between our annotations and team outcomes. We posit that additional work on these tasks and their intersection will further improve understanding of team communication and its relation to team performance. Code &amp; data: https://doi.org/10.5281/zenodo.14834835</abstract>
      <url hash="611d2ab3">2025.findings-naacl.61</url>
      <bibkey>pyarelal-etal-2025-multicat</bibkey>
    </paper>
    <paper id="62">
      <title>Prototype Tuning: A Meta-Learning Approach for Few-Shot Document-Level Relation Extraction with Large Language Models</title>
      <author><first>Dinghao</first><last>Pan</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Yuanyuan</first><last>Sun</last></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Jiru</first><last>Li</last></author>
      <author><first>Zhihao</first><last>Yang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Ling</first><last>Luo</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <author><first>Jian</first><last>Wang</last></author>
      <pages>1112-1128</pages>
      <abstract>Few-Shot Document-Level Relation Extraction (FSDLRE) aims to develop models capable of generalizing to new categories with minimal support examples. Although Large Language Models (LLMs) demonstrate exceptional In-Context Learning (ICL) capabilities on many few-shot tasks, their performance on FSDLRE tasks remains suboptimal due to the significant gap between the task format and the intrinsic capabilities of language models, coupled with the complexity of ICL prompts for document-level text. To address these challenges, we introduce a novel meta-training approach for LLMs termed Prototype Tuning. We construct simulated episodes using data with relation types that do not overlap with the test corpus, fundamentally enhancing the ICL capabilities of LLMs in FSDLRE through meta-learning. To further enhance the effects of meta-learning, we innovatively integrate the concept of prototype into the fine-tuning process of LLMs. This involves aggregating entity pairs from support documents into prototypes within the prompts and altering the way of determining relation categories to identifying the closest prototype. Experimental results demonstrate that our LLMs trained with this approach outperform all baselines. Our proposed approach markedly improves the ICL capabilities of LLMs in FSDLRE and mitigates the impact of relation semantic discrepancies between the training corpus and the test corpus on model performance.</abstract>
      <url hash="ced75298">2025.findings-naacl.62</url>
      <bibkey>pan-etal-2025-prototype</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>L</fixed-case>egal<fixed-case>S</fixed-case>eg: Unlocking the Structure of <fixed-case>I</fixed-case>ndian Legal Judgments Through Rhetorical Role Classification</title>
      <author><first>Shubham Kumar</first><last>Nigam</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Tanmay</first><last>Dubey</last></author>
      <author><first>Govind</first><last>Sharma</last></author>
      <author><first>Noel</first><last>Shallum</last><affiliation>Symbiosis Law School Pune</affiliation></author>
      <author><first>Kripabandhu</first><last>Ghosh</last><affiliation>Indian Institute of Science Education and Research Kolkata</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>1129-1144</pages>
      <abstract>In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce **LegalSeg**, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory **RhetoricLLaMA**, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.</abstract>
      <url hash="0000ac1f">2025.findings-naacl.63</url>
      <bibkey>nigam-etal-2025-legalseg</bibkey>
    </paper>
    <paper id="64">
      <title>Claim-Guided Textual Backdoor Attack for Practical Applications</title>
      <author><first>Minkyoo</first><last>Song</last></author>
      <author><first>Hanna</first><last>Kim</last></author>
      <author><first>Jaehan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Youngjin</first><last>Jin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Texas A&amp;M University - College Station</affiliation></author>
      <pages>1145-1159</pages>
      <abstract>Recent advances in natural language processing and the increased use of large language models have exposed new security vulnerabilities, such as backdoor attacks. Previous backdoor attacks require input manipulation after model distribution to activate the backdoor, posing limitations in real-world applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor Attack (CGBA), which eliminates the need for such manipulations by utilizing inherent textual claims as triggers. CGBA leverages claim extraction, clustering, and targeted training to trick models to misbehave on targeted claims without affecting their performance on clean data. CGBA demonstrates its effectiveness and stealthiness across various datasets and models, significantly enhancing the feasibility of practical backdoor attacks. Our code and data will be available at https://github.com/minkyoo9/CGBA.</abstract>
      <url hash="455380fe">2025.findings-naacl.64</url>
      <bibkey>song-etal-2025-claim</bibkey>
    </paper>
    <paper id="65">
      <title><fixed-case>T</fixed-case>ool<fixed-case>S</fixed-case>andbox: A Stateful, Conversational, Interactive Evaluation Benchmark for <fixed-case>LLM</fixed-case> Tool Use Capabilities</title>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Thomas</first><last>Holleis</last><affiliation>Apple</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Bernhard</first><last>Aumayer</last><affiliation>Apple</affiliation></author>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Haoping</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Shuang</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Shen</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Mengyu</first><last>Li</last><affiliation>Apple</affiliation></author>
      <author><first>Guoli</first><last>Yin</last><affiliation>Apple</affiliation></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <pages>1160-1183</pages>
      <abstract>Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over arbitrary trajectory. We show that open source and proprietary models has a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights to tool-use LLM capabilities. Datasets and evaluation scripts of ToolSandbox are released at &lt;placeholder&gt;.</abstract>
      <url hash="89e64cf1">2025.findings-naacl.65</url>
      <bibkey>lu-etal-2025-toolsandbox</bibkey>
    </paper>
    <paper id="66">
      <title><tex-math>SusGen-GPT</tex-math>: A Data-Centric <fixed-case>LLM</fixed-case> for Financial <fixed-case>NLP</fixed-case> and Sustainability Report Generation</title>
      <author><first>Qilong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiaoneng</first><last>Xiang</last><affiliation>Huawei Singapore Research Center</affiliation></author>
      <author><first>Huang</first><last>Hejia</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Yeo</first><last>Wei Jie</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Ranjan</first><last>Satapathy</last></author>
      <author><first>Ricardo Shirota</first><last>Filho</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Bharadwaj</first><last>Veeravalli</last></author>
      <pages>1184-1203</pages>
      <abstract>The rapid growth of the financial sector and the increasing focus on Environmental, Social, and Governance (ESG) considerations have created a pressing need for advanced natural language processing (NLP) tools. Despite recent advancements, there is still a notable absence of open-source Large Language Models (LLMs) that are proficient across both general finance and ESG domains, such as generating ESG reports. To address this gap, we introduce <tex-math>SusGen</tex-math>-<tex-math>30k</tex-math>, a high-quality, category-balanced dataset comprising seven financial NLP tasks. In addition, we propose <tex-math>TCFD</tex-math>-<tex-math>Bench</tex-math>, a benchmark designed to improve the evaluation of sustainability report generation. Our data-centric approach led to the development of a suite of models, <tex-math>SusGen</tex-math>-<tex-math>GPT</tex-math>, trained on the curated dataset. These models were evaluated across six adapted tasks and two off-the-shelf tasks, showing state-of-the-art performance, surpassing all other models except GPT-4. Remarkably, <tex-math>SusGen</tex-math>-<tex-math>GPT</tex-math> achieved an average score only 0.02 below GPT-4, despite using models with only 7-8B parameters compared to much larger GPT-4. This demonstrates the efficiency of our approach in delivering high performance with significantly fewer resources, addressing existing challenges and fostering further advancements in the financial and ESG research community.</abstract>
      <url hash="7d056bfd">2025.findings-naacl.66</url>
      <bibkey>wu-etal-2025-susgen</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>G</fixed-case>r<fixed-case>E</fixed-case>m<fixed-case>LI</fixed-case>n: A Repository of Green Baseline Embeddings for 87 Low-Resource Languages Injected with Multilingual Graph Knowledge</title>
      <author><first>Daniil</first><last>Gurgurov</last></author>
      <author><first>Rishu</first><last>Kumar</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>German Research Center for AI</affiliation></author>
      <pages>1204-1221</pages>
      <url hash="ba6a69da">2025.findings-naacl.67</url>
      <bibkey>gurgurov-etal-2025-gremlin</bibkey>
    </paper>
    <paper id="68">
      <title>In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation</title>
      <author><first>Armel Randy</first><last>Zebaze</last><affiliation>INRIA</affiliation></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>Inria</affiliation></author>
      <author><first>Rachel</first><last>Bawden</last><affiliation>Inria</affiliation></author>
      <pages>1222-1252</pages>
      <abstract>The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. In this paper, we focus on machine translation (MT), a task that has been shown to benefit from in-context translation examples. However no systematic studies have been published on how best to select examples, and mixed results have been reported on the usefulness of similarity-based selection over random selection, although these results have mainly been shown for high-resource languages only. We provide a study covering multiple LLMs and in-context example retrieval strategies. Contrarily to previously published results, we find that retrieval based on sentence embedding similarity can improve MT, especially for low-resource language directions, and we also discuss the balance between selection pool diversity and quality. Code and outputs will be made freely available.</abstract>
      <url hash="c0600ee4">2025.findings-naacl.68</url>
      <bibkey>zebaze-etal-2025-context</bibkey>
    </paper>
    <paper id="69">
      <title>Self-Training Large Language Models for Tool-Use Without Demonstrations</title>
      <author><first>Ne</first><last>Luo</last></author>
      <author><first>Aryo Pradipta</first><last>Gema</last><affiliation>Anthropic and University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Emile</first><last>Van Krieken</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Pietro</first><last>Lesci</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>1253-1271</pages>
      <abstract>Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.</abstract>
      <url hash="6ad5be04">2025.findings-naacl.69</url>
      <bibkey>luo-etal-2025-self</bibkey>
    </paper>
    <paper id="70">
      <title>Can Large Language Models Generate High-quality Patent Claims?</title>
      <author><first>Lekang</first><last>Jiang</last></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Pascal A.</first><last>Scherz</last></author>
      <author><first>Stefan</first><last>Goetz</last><affiliation>University of Cambridge and Duke University</affiliation></author>
      <pages>1272-1287</pages>
      <abstract>Large language models (LLMs) have shown exceptional performance across various text generation tasks, but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions’ features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.</abstract>
      <url hash="a6af09bb">2025.findings-naacl.70</url>
      <bibkey>jiang-etal-2025-large</bibkey>
    </paper>
    <paper id="71">
      <title>Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm</title>
      <author><first>Jaehan</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minkyoo</first><last>Song</last></author>
      <author><first>Seung Ho</first><last>Na</last></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Texas A&amp;M University - College Station</affiliation></author>
      <pages>1288-1307</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has become a key training strategy for large language models. However, its reliance on fewer trainable parameters poses security risks, such as task-agnostic backdoors. Despite their severe impact on a wide range of tasks, there is no practical defense solution available that effectively counters task-agnostic backdoors within the context of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor defense. We develop two techniques aimed at amplifying benign neurons within PEFT layers and penalizing the influence of trigger tokens. Our evaluations across three major PEFT architectures show that our method can significantly reduce the attack success rate of the state-of-the-art task-agnostic backdoors (83.6%<tex-math>\downarrow</tex-math>). Furthermore, our method exhibits robust defense capabilities against both task-specific backdoors and adaptive attacks. Source code will be obtained at https://github.com/jaehanwork/Obliviate.</abstract>
      <url hash="c5be84fc">2025.findings-naacl.71</url>
      <bibkey>kim-etal-2025-obliviate</bibkey>
    </paper>
    <paper id="72">
      <title><fixed-case>CORAL</fixed-case>: Benchmarking Multi-turn Conversational Retrieval-Augmented Generation</title>
      <author><first>Yiruo</first><last>Cheng</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Ziliang</first><last>Zhao</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Hongjin</first><last>Qian</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yongkang</first><last>Wu</last></author>
      <author><first>Tetsuya</first><last>Sakai</last><affiliation>NAVER and Waseda University</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1308-1330</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.</abstract>
      <url hash="585f2dfe">2025.findings-naacl.72</url>
      <bibkey>cheng-etal-2025-coral</bibkey>
    </paper>
    <paper id="73">
      <title>Beyond <fixed-case>E</fixed-case>nglish: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual <fixed-case>LLM</fixed-case>s</title>
      <author><first>Itai</first><last>Mondshine</last></author>
      <author><first>Tzuf</first><last>Paz-Argaman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>1331-1354</pages>
      <abstract>Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings</abstract>
      <url hash="1b12d9bb">2025.findings-naacl.73</url>
      <bibkey>mondshine-etal-2025-beyond</bibkey>
    </paper>
    <paper id="74">
      <title><fixed-case>Q</fixed-case>ua<fixed-case>LLM</fixed-case>: An <fixed-case>LLM</fixed-case>-based Framework to Extract Quantitative Insights from Online Forums</title>
      <author><first>Varun</first><last>Nagaraj Rao</last><affiliation>Princeton University</affiliation></author>
      <author><first>Eesha</first><last>Agarwal</last></author>
      <author><first>Samantha</first><last>Dalal</last></author>
      <author><first>Dana</first><last>Calacci</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Andrés</first><last>Monroy-Hernández</last><affiliation>Department of Computer Science, Princeton University</affiliation></author>
      <pages>1355-1369</pages>
      <abstract>Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methodologies used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting and human evaluation methodology. We applied this framework to analyze over one million comments from two of Reddit’s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.</abstract>
      <url hash="afdeea63">2025.findings-naacl.74</url>
      <bibkey>nagaraj-rao-etal-2025-quallm</bibkey>
    </paper>
    <paper id="75">
      <title>The Promises and Pitfalls of <fixed-case>LLM</fixed-case> Annotations in Dataset Labeling: a Case Study on Media Bias Detection</title>
      <author><first>Tomáš</first><last>Horych</last></author>
      <author><first>Christoph</first><last>Mandl</last><affiliation>Media Bias Group</affiliation></author>
      <author><first>Terry</first><last>Ruas</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Andre</first><last>Greiner-Petter</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Bela</first><last>Gipp</last><affiliation>Georg-August Universität Göttingen</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Timo</first><last>Spinde</last></author>
      <pages>1370-1386</pages>
      <abstract>High annotation costs from hiring or crowdsourcing complicate the creation of large, high-quality datasets needed for training reliable text classifiers. Recent research suggests using Large Language Models (LLMs) to automate the annotation process, reducing these costs while maintaining data quality. LLMs have shown promising results in annotating downstream tasks like hate speech detection and political framing. Building on the success in these areas, this study investigates whether LLMs are viable for annotating a complex task of media bias detection and whether a downstream media bias classifier can be trained on such data. We create Annolexical, the first large-scale dataset for media bias classification with over 48k synthetically annotated examples. Our classifier fine-tuned on it surpasses all of the annotator LLMs by 5-9% in Mathew’s Correlation Coefficient (MCC) and performs close to or outperforms the model trained on human-labeled data when evaluated on two media bias benchmark datasets (BABE and BASIL). This study demonstrates how our approach significantly reduces the cost of dataset creation in the media bias domain and, by extension - the development of the classifiers, while our subsequent behavioral stress-testing reveals some of its current limitations and trade-offs.</abstract>
      <url hash="9416a6ed">2025.findings-naacl.75</url>
      <bibkey>horych-etal-2025-promises</bibkey>
    </paper>
    <paper id="76">
      <title>Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning</title>
      <author><first>Lin</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Lijie</first><last>Hu</last><affiliation>KAUST</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>1387-1404</pages>
      <abstract>Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.</abstract>
      <url hash="bf371113">2025.findings-naacl.76</url>
      <bibkey>zhang-etal-2025-mechanistic</bibkey>
    </paper>
    <paper id="77">
      <title>Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities in Large Language Models</title>
      <author><first>Yuyi</first><last>Huang</last></author>
      <author><first>Runzhe</first><last>Zhan</last><affiliation>University of Macau</affiliation></author>
      <author><first>Derek F.</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia S.</first><last>Chao</last><affiliation>University of Macau</affiliation></author>
      <author><first>Ailin</first><last>Tao</last><affiliation>Guangzhou Medical University</affiliation></author>
      <pages>1405-1425</pages>
      <abstract>Large language models (LLMs) have significantly influenced various industries but suffer from a critical flaw, the potential sensitivity of generating harmful content, which poses severe societal risks. We developed and tested novel attack strategies on popular LLMs to expose their vulnerabilities in generating inappropriate content. These strategies, inspired by psychological phenomena such as the “Priming Effect”, “Safe Attention Shift”, and “Cognitive Dissonance”, effectively attack the models’ guarding mechanisms. Our experiments achieved an attack success rate (ASR) of 100% on various open-source models, including Meta’s Llama-3.2, Google’s Gemma-2, Mistral’s Mistral-NeMo, Falcon’s Falcon-mamba, Apple’s DCLM, Microsoft’s Phi3, and Qwen’s Qwen2.5, among others. Similarly, for closed-source models such as OpenAI’s GPT-4o, Google’s Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95% on the AdvBench dataset, which represents the current state-of-the-art. This study underscores the urgent need to reassess the use of generative models in critical applications to mitigate potential adverse societal impacts.</abstract>
      <url hash="9c51c543">2025.findings-naacl.77</url>
      <bibkey>huang-etal-2025-intrinsic</bibkey>
    </paper>
    <paper id="78">
      <title><fixed-case>A</fixed-case>d<fixed-case>P</fixed-case>araphrase: Paraphrase Dataset for Analyzing Linguistic Features toward Generating Attractive Ad Texts</title>
      <author><first>Soichiro</first><last>Murakami</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>1426-1439</pages>
      <abstract>Effective linguistic choices that attract potential customers play crucial roles in advertising success. This study aims to explore the linguistic features of ad texts that influence human preferences. Although the creation of attractive ad texts is an active area of research, progress in understanding the specific linguistic features that affect attractiveness is hindered by several obstacles. First, human preferences are complex and influenced by multiple factors, including their content, such as brand names, and their linguistic styles, making analysis challenging. Second, publicly available ad text datasets that include human preferences are lacking, such as ad performance metrics and human feedback, which reflect people’s interests. To address these problems, we present AdParaphrase, a paraphrase dataset that contains human preferences for pairs of ad texts that are semantically equivalent but differ in terms of wording and style. This dataset allows for preference analysis that focuses on the differences in linguistic features. Our analysis revealed that ad texts preferred by human judges have higher fluency, longer length, more nouns, and use of bracket symbols. Furthermore, we demonstrate that an ad text-generation model that considers these findings significantly improves the attractiveness of a given text. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase.</abstract>
      <url hash="50d0f480">2025.findings-naacl.78</url>
      <bibkey>murakami-etal-2025-adparaphrase</bibkey>
    </paper>
    <paper id="79">
      <title>Token Weighting for Long-Range Language Modeling</title>
      <author><first>Falko</first><last>Helm</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Nico</first><last>Daheim</last><affiliation>ETHZ - ETH Zurich and Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <pages>1440-1459</pages>
      <abstract>Many applications of large language models (LLMs) require long-context understanding, but models continue to struggle with such tasks. We hypothesize that conventional next-token prediction training could contribute to this, because each token is assigned equal weight. Yet, intuitively, the amount of context needed to predict the next token accurately varies greatly across different data. To reflect this, we propose various novel token-weighting schemes that assign different weights to each training token in the loss, thereby generalizing existing works. For this, we categorize token-weighting methods using a two-step framework which compares the confidences of a long-context and short-context model to score tokens. We evaluate all methods on multiple long-context understanding tasks and show that non-uniform loss weights are helpful to improve the long-context abilities of LLMs.Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained.All in all, this work contributes to a better understanding of the trade-offs long-context language modeling faces and provides guidelines for model steering via loss-weighting based on empirical evidence. The code can be found on [Github](https://github.com/UKPLab/naacl2025-token-weighting).</abstract>
      <url hash="2e8645cb">2025.findings-naacl.79</url>
      <bibkey>helm-etal-2025-token</bibkey>
    </paper>
    <paper id="80">
      <title>Learning to Explore and Select for Coverage-Conditioned Retrieval-Augmented Generation</title>
      <author><first>Takyoung</first><last>Kim</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Kyungjae</first><last>Lee</last><affiliation>University of Seoul</affiliation></author>
      <author><first>Young Rok</first><last>Jang</last></author>
      <author><first>Ji Yong</first><last>Cho</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <author><first>Minseok</first><last>Cho</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>LG Corporation and University of Illinois, Chicago</affiliation></author>
      <pages>1460-1480</pages>
      <abstract>Interactions with large language models (LLMs) often yield long and detailed responses, leveraging both parametric knowledge and retrieval-augmented generation (RAG). While these responses can provide rich insights, they often include redundant or less engaging content not aligned with user interests. This issue becomes apparent when users specify particular subtopics to include or exclude – termed **coverage-conditioned (<tex-math>C^2</tex-math>)** queries – as LLMs often struggle to provide tailored responses. To address this challenge, we investigate the role of query outlines, sequences of subqueries designed to guide LLMs in generating responses that meet specific user requirements. To systematically create and evaluate these outlines, we introduce **QTree**, a dataset of 10K hierarchical sets of information-seeking subqueries that define structured boundaries for outline creation and evaluation in <tex-math>C^2</tex-math> scenarios. Additionally, we develop **QPlanner**, a 7B language model trained to generate customized outlines within boundaries of QTree. We evaluate the effectiveness of the generated outlines through automatic and human judgements, focusing on their impact within retrieval-augmented generation (RAG) systems. Experimental results demonstrate that QPlanner, especially when trained with alignment techniques like DPO, generates higher-quality outlines that better fulfill diverse user needs.</abstract>
      <url hash="0eadc692">2025.findings-naacl.80</url>
      <bibkey>kim-etal-2025-learning</bibkey>
    </paper>
    <paper id="81">
      <title><fixed-case>L</fixed-case>ay<fixed-case>A</fixed-case>lign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy</title>
      <author><first>Zhiwen</first><last>Ruan</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Longyue</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba International Digital Commerce Group</affiliation></author>
      <author><first>Kaifu</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>1481-1495</pages>
      <abstract>Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder’s output, overlooking valuable information from other layers. We propose Layer-Wise Adaptive Fusion and Alignment Strategy (LayAlign), a framework that integrates representations from all encoder layers, coupled with the adaptive fusion-enhanced attention mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.</abstract>
      <url hash="6a5a650b">2025.findings-naacl.81</url>
      <bibkey>ruan-etal-2025-layalign</bibkey>
    </paper>
    <paper id="82">
      <title>On the Impacts of Contexts on Repository-Level Code Generation</title>
      <author><first>Nam Le</first><last>Hai</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Dung Manh</first><last>Nguyen</last></author>
      <author><first>Nghi D. Q.</first><last>Bui</last></author>
      <pages>1496-1524</pages>
      <abstract>CodeLLMs are widely used for code generation, yet their ability to handle repository-level dependencies remains underexplored. We introduce RepoExec, a benchmark for evaluating repository-level code generation, focusing on executability, functional correctness, and dependency utilization. Our study evaluates 18 models, revealing that retaining full dependency context yields the best performance, while smaller context sizes can be misleading. Pretrained LLMs excel in correctness but often reimplement dependencies, while instruction-tuned models better utilize dependencies but sometimes introduce unnecessary complexity. We propose an instruction-tuning dataset that improves dependency handling and introduce a new metric, Dependency Invocation Rate (DIR), to measure context utilization. Experiments show that instruction-tuned models improve DIR by over 10%, and multi-round debugging further enhances both correctness and dependency use. RepoExec provides a comprehensive framework to advance CodeLLMs for real-world applications. The dataset and source code are available at <url>https://github.com/FSoft-AI4Code/RepoExec</url>.</abstract>
      <url hash="ba83aa59">2025.findings-naacl.82</url>
      <bibkey>hai-etal-2025-impacts</bibkey>
    </paper>
    <paper id="83">
      <title>From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis</title>
      <author><first>Moritz</first><last>Plenz</last><affiliation>Institute for Computational Linguistics, Heidelberg University, Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Janosch</first><last>Gehring</last></author>
      <author><first>Philipp</first><last>Cimiano</last><affiliation>Bielefeld University and Bielefeld University</affiliation></author>
      <author><first>Anette</first><last>Frank</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <pages>1525-1553</pages>
      <abstract>Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer’s stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.</abstract>
      <url hash="6434720c">2025.findings-naacl.83</url>
      <bibkey>plenz-etal-2025-argumentation</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>LVLM</fixed-case>-Compress-Bench: Benchmarking the Broader Impact of Large Vision-Language Model Compression</title>
      <author><first>Souvik</first><last>Kundu</last><affiliation>Intel</affiliation></author>
      <author><first>Anahita</first><last>Bhiwandiwalla</last></author>
      <author><first>Sungduk</first><last>Yu</last></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <author><first>Tiep</first><last>Le</last><affiliation>Intel</affiliation></author>
      <author><first>Sharath</first><last>Nittur Sridhar</last><affiliation>Intel Labs</affiliation></author>
      <author><first>David</first><last>Cobbley</last><affiliation>Intel Labs</affiliation></author>
      <author><first>Hao</first><last>Kang</last></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel</affiliation></author>
      <pages>1554-1570</pages>
      <abstract>Despite recent efforts in understanding the compression impact on Large Language Models (LLMs) in terms of their downstream task performance and trustworthiness on relatively simpler uni-modal benchmarks (e.g. question answering, common sense reasoning), their detailed study on multi-modal Large Vision Language Models (LVLMs) is yet to be unveiled. Towards mitigating this gap, we present LVLM-Compress-Bench, a framework to first thorough study on the broad impact of compression on the generative performance of LVLMs on multi-modal input driven tasks. In specific, we consider two major classes of compression for autoregressive models, namely KV cache and weight compression, for the dynamically growing intermediate cache and static weights, respectively. We use four LVLM variants of the popular LLaVA framework to present our analysis to integrate various state-of-the-art KV and weight compression methods including uniform, outlier-reduced, and group quantization. With this framework we demonstrate on ten different multi-modal datasets with varied capabilities including recognition, knowledge, language generation, spatial awareness, visual reasoning, hallucination and visual illusion identification, toxicity, stereotypes and bias. In specific, our framework demonstrates the compression impact on both general and ethically critical metrics leveraging a combination of real world and synthetic datasets to encompass diverse societal intersectional attributes. Extensive experimental evaluations yield diverse and intriguing observations on the behavior of LVLMs at different quantization budget of KV and weights, in both maintaining and losing performance as compared to the baseline model with FP16 data format. We believe LVLM-Compress-Bench would help the community to have a deeper insight on the parting impact of compression and the societal impact the compressed models may pose. Code will be released soon.</abstract>
      <url hash="e4768d1b">2025.findings-naacl.84</url>
      <bibkey>kundu-etal-2025-lvlm</bibkey>
    </paper>
    <paper id="85">
      <title>Does Generative <fixed-case>AI</fixed-case> speak <fixed-case>N</fixed-case>igerian-<fixed-case>P</fixed-case>idgin?: Issues about Representativeness and Bias for Multilingualism in <fixed-case>LLM</fixed-case>s</title>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>McGill University</affiliation></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Ghent University</affiliation></author>
      <author><first>Iyanuoluwa</first><last>Shode</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <pages>1571-1583</pages>
      <abstract>Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.</abstract>
      <url hash="b99ac33c">2025.findings-naacl.85</url>
      <bibkey>adelani-etal-2025-generative</bibkey>
    </paper>
    <paper id="86">
      <title>A Guide To Effectively Leveraging <fixed-case>LLM</fixed-case>s for Low-Resource Text Summarization: Data Augmentation and Semi-supervised Approaches</title>
      <author><first>Gaurav</first><last>Sahu</last></author>
      <author><first>Olga</first><last>Vechtomova</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Issam H.</first><last>Laradji</last><affiliation>ServiceNow</affiliation></author>
      <pages>1584-1603</pages>
      <abstract>Existing approaches for low-resource text summarization primarily employ large language models (LLMs) like GPT-3 or GPT-4 at inference time to generate summaries directly; however, such approaches often suffer from inconsistent LLM outputs and are difficult to adapt to domain-specific data in low-resource scenarios. In this work, we propose two novel methods to effectively utilize LLMs for low-resource text summarization: 1) MixSumm, an LLM-based data augmentation regime that synthesizes high-quality documents (short and long) for few-shot text summarization, and 2) PPSL, a prompt-based pseudolabeling strategy for sample-efficient semi-supervised text summarization. Specifically, MixSumm leverages the open-source LLaMA-3-70b-Instruct model to generate new documents by mixing topical information derived from a small seed set, and PPSL leverages the LLaMA-3-70b-Instruct model to generate high-quality pseudo-labels in a semi-supervised learning setup. We evaluate our methods on the TweetSumm, WikiHow, and ArXiv/PubMed datasets and use L-Eval, a LLaMA-3-based evaluation metric, and ROUGE scores to measure the quality of generated summaries. Our experiments on extractive and abstractive summarization show that MixSumm and PPSL achieve competitive ROUGE scores as a fully supervised method with 5% of the labeled data. We release our codebase here: https://github.com/ServiceNow/text-summarization-with-llms/</abstract>
      <url hash="b5be357a">2025.findings-naacl.86</url>
      <bibkey>sahu-etal-2025-guide</bibkey>
    </paper>
    <paper id="87">
      <title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title>
      <author><first>Aashiq</first><last>Muhamed</last></author>
      <author><first>Mona T.</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Virginia</first><last>Smith</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1604-1635</pages>
      <abstract>Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and <tex-math>L_0</tex-math> sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5% increase in worst-group classification accuracy over the pretrained general-purpose SAE when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.</abstract>
      <url hash="f2e717fe">2025.findings-naacl.87</url>
      <bibkey>muhamed-etal-2025-decoding</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>MA</fixed-case>i<fixed-case>DE</fixed-case>-up: Multilingual Deception Detection of <fixed-case>AI</fixed-case>-generated Hotel Reviews</title>
      <author><first>Oana</first><last>Ignat</last><affiliation>Santa Clara University</affiliation></author>
      <author><first>Xiaomeng</first><last>Xu</last></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>1636-1653</pages>
      <abstract>Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews.</abstract>
      <url hash="e6812d65">2025.findings-naacl.88</url>
      <bibkey>ignat-etal-2025-maide</bibkey>
    </paper>
    <paper id="89">
      <title><fixed-case>L</fixed-case>e<fixed-case>C</fixed-case>o<fixed-case>PCR</fixed-case>: Legal Concept-guided Prior Case Retrieval for <fixed-case>E</fixed-case>uropean Court of Human Rights cases</title>
      <author><first>Santosh</first><last>T.y.s.s</last></author>
      <author><first>Isaac Misael Olguín</first><last>Nolasco</last></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>1654-1661</pages>
      <abstract>Prior case retrieval (PCR) is crucial for legal practitioners to find relevant precedent cases given the facts of a query case. Existing approaches often overlook the underlying semantic intent in determining relevance with respect to the query case. In this work, we propose LeCoPCR, a novel approach that explicitly generate intents in the form of legal concepts from a given query case facts and then augments the query with these concepts to enhance models understanding of semantic intent that dictates relavance. To overcome the unavailability of annotated legal concepts, We employ a weak supervision approach to extract key legal concepts from the reasoning section using Determinantal Point Process (DPP) to balance quality and diversity. Experimental results on the ECtHR-PCR dataset demonstrate the effectiveness of leveraging legal concepts and DPP-based key concept extraction.</abstract>
      <url hash="a00f2570">2025.findings-naacl.89</url>
      <bibkey>t-y-s-s-etal-2025-lecopcr</bibkey>
    </paper>
    <paper id="90">
      <title>How much do contextualized representations encode long-range context?</title>
      <author><first>Simeng</first><last>Sun</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Cheng-Ping</first><last>Hsieh</last><affiliation>NVIDIA</affiliation></author>
      <pages>1662-1679</pages>
      <abstract>We analyze contextual representations in neural autoregressive language models, emphasizing long-range contexts that span several thousand tokens. Our methodology employs a perturbation setup and the metric Anisotropy-Calibrated Cosine Similarity, to capture the degree of contextualization of long-range patterns from the perspective of representation geometry. We begin the analysis with a case study on standard decoder-only Transformers, demonstrating that similar perplexity can exhibit markedly different downstream task performance, which can be explained by the difference in contextualization of long-range content. Next, we extend the analysis to other models, covering recent novel architectural designs and various training configurations. The representation-level results illustrate a reduced capacity for high-complexity (i.e., less compressible) sequences across architectures, and that fully recurrent models rely heavily on local context, whereas hybrid models more effectively encode the entire sequence structure. Finally, preliminary analysis of model size and training configurations on the encoding of long-range context suggest potential directions for improving existing language models.</abstract>
      <url hash="53b5cd89">2025.findings-naacl.90</url>
      <bibkey>sun-hsieh-2025-much</bibkey>
    </paper>
    <paper id="91">
      <title>Data Poisoning for In-context Learning</title>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Makoto</first><last>Yamada</last><affiliation>Okinawa Institute of Science and Technology (OIST)</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>1680-1700</pages>
      <abstract>In-context learning (ICL) has emerged as a capability of large language models (LLMs), enabling them to adapt to new tasks using provided examples. While ICL has demonstrated its strong effectiveness, there is limited understanding of its vulnerability against potential threats. This paper examines ICL’s vulnerability to data poisoning attacks. We introduce ICLPoison, an attacking method specially designed to exploit ICL’s unique learning mechanisms by identifying discrete text perturbations that influence LLM hidden states. We propose three representative attack strategies, evaluated across various models and tasks. Our experiments, including those on GPT-4, show that ICL performance can be significantly compromised by these attacks, highlighting the urgent need for improved defense mechanisms to protect LLMs’ integrity and reliability.</abstract>
      <url hash="80817191">2025.findings-naacl.91</url>
      <bibkey>he-etal-2025-data</bibkey>
    </paper>
    <paper id="92">
      <title>Synthetic Audio Helps for Cognitive State Tasks</title>
      <author><first>Adil</first><last>Soubki</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>John</first><last>Murzaku</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Peter</first><last>Zeng</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>1701-1708</pages>
      <abstract>The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio.</abstract>
      <url hash="03416ad1">2025.findings-naacl.92</url>
      <bibkey>soubki-etal-2025-synthetic</bibkey>
    </paper>
    <paper id="93">
      <title><fixed-case>B</fixed-case>io<fixed-case>EL</fixed-case>: A Comprehensive Python Package for Biomedical Entity Linking</title>
      <author><first>Prasanth</first><last>Bathala</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Christophe</first><last>Ye</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Batuhan</first><last>Nursal</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Shubham</first><last>Lohiya</last></author>
      <author><first>David</first><last>Kartchner</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Cassie S.</first><last>Mitchell</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>1709-1721</pages>
      <url hash="13948d10">2025.findings-naacl.93</url>
      <bibkey>bathala-etal-2025-bioel</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>P</fixed-case>air<fixed-case>S</fixed-case>cale: Analyzing Attitude Change with Pairwise Comparisons</title>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Patrick Y.</first><last>Wu</last><affiliation>American University</affiliation></author>
      <author><first>Kristina</first><last>Miler</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Alexander Miserlis</first><last>Hoyle</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Philip</first><last>Resnik</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>1722-1738</pages>
      <abstract>We introduce a text-based framework for measuring attitudes in communities toward issues of interest, going beyond the pro/con/neutral of conventional stance detection to characterize attitudes on a continuous scale using both implicit and explicit evidence in language. The framework exploits LLMs both to extract attitude-related evidence and to perform pairwise comparisons that yield unidimensional attitude scores via the classic Bradley-Terry model. We validate the LLM-based steps using human judgments, and illustrate the utility of the approach for social science by examining the evolution of attitudes on two high-profile issues in U.S. politics in two political communities on Reddit over the period spanning from the 2016 presidential campaign to the 2022 mid-term elections. WARNING: Potentially sensitive political content.</abstract>
      <url hash="cd6347d1">2025.findings-naacl.94</url>
      <bibkey>sarkar-etal-2025-pairscale</bibkey>
    </paper>
    <paper id="95">
      <title>Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation</title>
      <author><first>Chenyu</first><last>Wang</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Weichao</first><last>Zhou</last></author>
      <author><first>Shantanu</first><last>Ghosh</last></author>
      <author><first>Kayhan</first><last>Batmanghelich</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Wenchao</first><last>Li</last><affiliation>Boston University</affiliation></author>
      <pages>1739-1754</pages>
      <abstract>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by 10%, achieved by rejecting 20% of reports on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an 82.9% success rate. Our implementation is open-source and available at https://github.com/BU-DEPEND-Lab/SCUQ-RRG.</abstract>
      <url hash="2840ec26">2025.findings-naacl.95</url>
      <bibkey>wang-etal-2025-semantic</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>R</fixed-case>eward<fixed-case>B</fixed-case>ench: Evaluating Reward Models for Language Modeling</title>
      <author><first>Nathan</first><last>Lambert</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <author><first>Jacob</first><last>Morrison</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Lester James Validad</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last><affiliation>Mistral AI</affiliation></author>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Tom</first><last>Zick</last><affiliation>Harvard University</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Noah A.</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>1755-1797</pages>
      <abstract>Reward models (RMs) are at the crux of successfully using RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. Resources for reward model training and understanding are sparse in the nascent open-source community around them. To enhance scientific understanding of reward models, we present RewardBench, a benchmark dataset and code-base for evaluation. The RewardBench dataset is a collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We create specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate RMs trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO). We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.</abstract>
      <url hash="fd90914b">2025.findings-naacl.96</url>
      <bibkey>lambert-etal-2025-rewardbench</bibkey>
    </paper>
    <paper id="97">
      <title>Evaluating Vision-Language Models for Emotion Recognition</title>
      <author><first>Sree</first><last>Bhattacharyya</last></author>
      <author><first>James Z.</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1798-1820</pages>
      <abstract>Large Vision-Language Models (VLMs) have achieved unprecedented success in several objective multimodal reasoning tasks. However, to further enhance their capabilities of empathetic and effective communication with humans, improving how VLMs process and understand emotions is crucial. Despite significant research attention on improving affective understanding, there is a lack of detailed evaluations of VLMs for emotion-related tasks, which can potentially help inform downstream fine-tuning efforts. In this work, we present the first comprehensive evaluation of VLMs for recognizing evoked emotions from images. We create a benchmark for the task of evoked emotion recognition and study the performance of VLMs for this task, from perspectives of correctness and robustness. Through several experiments, we demonstrate important factors that emotion recognition performance depends on, and also characterize the various errors made by VLMs in the process. Finally, we pinpoint potential causes for errors through a human evaluation study. We use our experimental results to inform recommendations for the future of emotion research in the context of VLMs.</abstract>
      <url hash="b64188aa">2025.findings-naacl.97</url>
      <bibkey>bhattacharyya-wang-2025-evaluating</bibkey>
    </paper>
    <paper id="98">
      <title>Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?</title>
      <author><first>Crystina</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jing</first><last>Lu</last><affiliation>Google</affiliation></author>
      <author><first>Vinh Q.</first><last>Tran</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google DeepMind and Google</affiliation></author>
      <author><first>Donald</first><last>Metzler</last><affiliation>Google</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>1821-1837</pages>
      <abstract>Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form “semantic tokens” by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.</abstract>
      <url hash="07382737">2025.findings-naacl.98</url>
      <bibkey>zhang-etal-2025-tomato</bibkey>
    </paper>
    <paper id="99">
      <title>Open Domain Question Answering with Conflicting Contexts</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Qiang</first><last>Ning</last><affiliation>Jump Trading</affiliation></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Qi</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Phu Mon</first><last>Htut</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>AWS AI</affiliation></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1838-1854</pages>
      <abstract>Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts. We publicly release our dataset and code to promote research along this line.</abstract>
      <url hash="6062af07">2025.findings-naacl.99</url>
      <bibkey>liu-etal-2025-open</bibkey>
    </paper>
    <paper id="100">
      <title>The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models</title>
      <author><first>Artem</first><last>Kirsanov</last><affiliation>New York University</affiliation></author>
      <author><first>Chi-Ning</first><last>Chou</last></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>SueYeon</first><last>Chung</last><affiliation>New York University and Flatiron Institute / Simons Foundation</affiliation></author>
      <pages>1855-1888</pages>
      <abstract>Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights critical geometric effects of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.</abstract>
      <url hash="e3f72365">2025.findings-naacl.100</url>
      <bibkey>kirsanov-etal-2025-geometry</bibkey>
    </paper>
    <paper id="101">
      <title>Biases in Opinion Dynamics in Multi-Agent Systems of Large Language Models: A Case Study on Funding Allocation</title>
      <author><first>Pedro</first><last>Cisneros-Velarde</last><affiliation>VMware Research</affiliation></author>
      <pages>1889-1916</pages>
      <abstract>We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM’s tendency to find consensus with the other LLM’s opinion, display caution when specifying funding, and consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus is mostly attained. When agents are aware of past opinions, they seek to maintain consistency with them, changing the opinion dynamics. Our study is performed using Llama 3 and Mistral LLMs.</abstract>
      <url hash="d4f35917">2025.findings-naacl.101</url>
      <bibkey>cisneros-velarde-2025-biases</bibkey>
    </paper>
    <paper id="102">
      <title><fixed-case>C</fixed-case>ase<fixed-case>S</fixed-case>umm: A Large-Scale Dataset for Long-Context Summarization from <fixed-case>U</fixed-case>.<fixed-case>S</fixed-case>. <fixed-case>S</fixed-case>upreme <fixed-case>C</fixed-case>ourt Opinions</title>
      <author><first>Mourad</first><last>Heddaya</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Kyle</first><last>MacMillan</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Hongyuan</first><last>Mei</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Anup</first><last>Malani</last><affiliation>University of Chicago</affiliation></author>
      <pages>1917-1942</pages>
      <abstract>This paper introduces CaseSumm, a novel dataset for long-context summarization in the legal domain that addresses the need for longer and more complex datasets for summarization evaluation. We collect 25.6K U.S. Supreme Court (SCOTUS) opinions and their official summaries, known as “syllabuses.” Our dataset is the largest open legal case summarization dataset, and is the first to include summaries of SCOTUS decisions dating back to 1815.We also present a comprehensive evaluation of LLM-generated summaries using both automatic metrics and expert human evaluation, revealing discrepancies between these assessment methods. Our evaluation shows Mistral 7b, a smaller open-source model, outperforms larger models on most automatic metrics and successfully generates syllabus-like summaries. In contrast, human expert annotators indicate that Mistral summaries contain hallucinations. The annotators consistently rank GPT-4 summaries as clearer and exhibiting greater sensitivity and specificity. We find that LLM-based evaluations are not more correlated with human evaluations than traditional automatic metrics. Furthermore, our analysis identifies specific hallucinations in generated summaries, including precedent citation errors and misrepresentations of case facts. These findings demonstrate the limitations of current automatic evaluation methods for legal summarization and highlight the critical role of human evaluation in assessing summary quality, particularly in complex, high-stakes domains.</abstract>
      <url hash="eb603744">2025.findings-naacl.102</url>
      <bibkey>heddaya-etal-2025-casesumm</bibkey>
    </paper>
    <paper id="103">
      <title>Chasing Random: Instruction Selection Strategies Fail to Generalize</title>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daphne</first><last>Ippolito</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1943-1957</pages>
      <url hash="5997d475">2025.findings-naacl.103</url>
      <bibkey>diddee-ippolito-2025-chasing</bibkey>
    </paper>
    <paper id="104">
      <title>Can’t Hide Behind the <fixed-case>API</fixed-case>: Stealing Black-Box Commercial Embedding Models</title>
      <author><first>Manveer Singh</first><last>Tamber</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jasper</first><last>Xian</last></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>1958-1969</pages>
      <abstract>Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being “hidden” behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to “steal” these models for retrieval by training thief models on text–embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.</abstract>
      <url hash="0d6a758b">2025.findings-naacl.104</url>
      <bibkey>tamber-etal-2025-cant</bibkey>
    </paper>
    <paper id="105">
      <title><fixed-case>CAMEL</fixed-case>-Bench: A Comprehensive <fixed-case>A</fixed-case>rabic <fixed-case>LMM</fixed-case> Benchmark</title>
      <author><first>Sara</first><last>Ghaboura</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ahmed</first><last>Heakl</last></author>
      <author><first>Omkar</first><last>Thawakar</last></author>
      <author><first>Ali Husain Salem Abdulla</first><last>Alharthi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ines</first><last>Riahi</last></author>
      <author><first>Abduljalil</first><last>Radman</last></author>
      <author><first>Jorma</first><last>Laaksonen</last><affiliation>Aalto University</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <author><first>Rao Muhammad</first><last>Anwer</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1970-1980</pages>
      <abstract>Recent years have witnessed a significant interest in developing large multi-modal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the bestopen-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark will be publicly released.</abstract>
      <url hash="a585ce65">2025.findings-naacl.105</url>
      <bibkey>ghaboura-etal-2025-camel</bibkey>
    </paper>
    <paper id="106">
      <title><fixed-case>P</fixed-case>roxy<fixed-case>LM</fixed-case>: Predicting Language Model Performance on Multilingual Tasks via Proxy Models</title>
      <author><first>David</first><last>Anugraha</last></author>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <author><first>Chenyue</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Patrick Amadeus</first><last>Irawan</last></author>
      <author><first>En-Shiun Annie</first><last>Lee</last></author>
      <pages>1981-2011</pages>
      <abstract>Performance prediction is a method to estimate the performance of Language Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating computational costs associated with model capacity and data for fine-tuning. Our paper presents ProxyLM, a scalable task- and language-agnostic framework designed to predict the performance of LMs using proxy models. These proxy models act as surrogates, approximating the performance of the LM of interest. By leveraging these proxy models, ProxyLM significantly reduces computational overhead in task evaluations, achieving up to a 37.08x speedup over traditional methods, even with our smallest proxy models. Our results across multiple multilingual NLP tasks and various robustness tests demonstrate that ProxyLM not only adapts well to previously unseen languages in pre-trained LMs, but also generalizes effectively across different datasets, outperforming the state-of-the-art by at least 1.78x in terms of root-mean-square error (RMSE).</abstract>
      <url hash="a623191e">2025.findings-naacl.106</url>
      <bibkey>anugraha-etal-2025-proxylm</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>S</fixed-case>im<fixed-case>SM</fixed-case>o<fixed-case>E</fixed-case>: Toward Efficient Training Mixture of Experts via Solving Representational Collapse</title>
      <author><first>Giang</first><last>Do</last></author>
      <author><first>Hung</first><last>Le</last><affiliation>Deakin University</affiliation></author>
      <author><first>Truyen</first><last>Tran</last><affiliation>Deakin University</affiliation></author>
      <pages>2012-2025</pages>
      <abstract>Sparse mixture of experts (SMoE) have emerged as an effective approach for scaling large language models while keeping a constant computational cost. Regardless of several notable successes of SMoE, effective training such architecture remains elusive due to the representation collapse problem, which in turn harms model performance and causes parameter redundancy. In this work, we present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel similarity of neural network algorithm, that guarantees a solution to address the representation collapse issue between experts given a fixed FLOPs budget. We conduct extensive empirical evaluations on three large language models for both Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness, and scalability of our method. The results demonstrate that SimSMoE significantly enhances existing routing policy and outperforms other SMoE routing methods in performance for the tasks. Our implementation is publicly available at https://github.com/giangdip2410/SimSMoE.</abstract>
      <url hash="e5c47699">2025.findings-naacl.107</url>
      <bibkey>do-etal-2025-simsmoe</bibkey>
    </paper>
    <paper id="108">
      <title><fixed-case>U</fixed-case>ni<fixed-case>RAG</fixed-case>: Universal Retrieval Augmentation for Large Vision Language Models</title>
      <author><first>Sahel</first><last>Sharifymoghaddam</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Shivani</first><last>Upadhyay</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <pages>2026-2039</pages>
      <abstract>Recently, Large Vision Language Models (LVLMs) have unlocked many complex use cases that require Multi-Modal (MM) understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of LVLMs we introduce UniRAG, a plug-and-play technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT-4o and Gemini-Pro and smaller open-source models like LLaVA, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by Vision-Language (VL) retrievers like UniIR models. All the necessary code to reproduce our results is available at https://github.com/castorini/UniRAG.</abstract>
      <url hash="c070981d">2025.findings-naacl.108</url>
      <bibkey>sharifymoghaddam-etal-2025-unirag</bibkey>
    </paper>
    <paper id="109">
      <title>Evaluating the Performance of Large Language Models via Debates</title>
      <author><first>Behrad</first><last>Moniri</last></author>
      <author><first>Hamed</first><last>Hassani</last><affiliation>University of Pennsylvania, University of Pennsylvania and University of Pennsylvania</affiliation></author>
      <author><first>Edgar</first><last>Dobriban</last><affiliation>The Wharton School, University of Pennsylvania</affiliation></author>
      <pages>2040-2075</pages>
      <abstract>Large Language Models (LLMs) are rapidly evolving and impacting various fields, necessitating the development of effective methods to evaluate and compare their performance. Most current approaches for performance evaluation are either based on fixed, domain-specific questions that lack the flexibility required in many real-world applications, or rely on human input, making them unscalable. To address these issues, we propose an automated benchmarking framework based on debates between LLMs, judged by another LLM. This method assesses not only domain knowledge, but also skills such as argumentative reasoning and inconsistency recognition. We evaluate the performance of various state-of-the-art LLMs using the debate framework and achieve rankings that align closely with popular rankings based on human input, eliminating the need for costly human crowdsourcing.</abstract>
      <url hash="c50b2478">2025.findings-naacl.109</url>
      <bibkey>moniri-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="110">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>G</fixed-case>raph2<fixed-case>LLM</fixed-case>: Evaluating <fixed-case>LLM</fixed-case>s for Causal Queries</title>
      <author><first>Ivaxi</first><last>Sheth</last><affiliation>CISPA, saarland university, saarland informatics campus</affiliation></author>
      <author><first>Bahare</first><last>Fatemi</last><affiliation>Google</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>2076-2098</pages>
      <abstract>Causality is essential in scientific research, enabling researchers to interpret true relationships between variables. These causal relationships are often represented by causal graphs, which are directed acyclic graphs. With the recent advancements in Large Language Models (LLMs), there is an increasing interest in exploring their capabilities in causal reasoning and their potential use to hypothesize causal graphs. These tasks necessitate the LLMs to encode the causal graph effectively for subsequent downstream tasks. In this paper, we introduce <b>CausalGraph2LLM</b>, a comprehensive benchmark comprising over <i>700k</i> queries across diverse causal graph settings to evaluate the causal reasoning capabilities of LLMs. We categorize the causal queries into two types: graph-level and node-level queries. We benchmark both open-sourced and closed models for our study. Our findings reveal that while LLMs show promise in this domain, they are highly sensitive to the encoding used. Even capable models like GPT-4 and Gemini-1.5 exhibit sensitivity to encoding, with deviations of about 60%. We further demonstrate this sensitivity for downstream causal intervention tasks. Moreover, we observe that LLMs can often display biases when presented with contextual information about a causal graph, potentially stemming from their parametric memory.</abstract>
      <url hash="c0911ada">2025.findings-naacl.110</url>
      <bibkey>sheth-etal-2025-causalgraph2llm</bibkey>
    </paper>
    <paper id="111">
      <title><fixed-case>P</fixed-case>uzzle<fixed-case>GPT</fixed-case>: Emulating Human Puzzle-Solving Ability for Time and Location Prediction</title>
      <author><first>Hammad</first><last>Ayyubi</last></author>
      <author><first>Xuande</first><last>Feng</last></author>
      <author><first>Junzhang</first><last>Liu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Xudong</first><last>Lin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhecan</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Shih-Fu</first><last>Chang</last><affiliation>Columbia University and Columbia University</affiliation></author>
      <pages>2099-2116</pages>
      <abstract>The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can’t be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets – TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically generated reasoning pipelines like VisProg, by at least 32% and 38%, respectively. It even rivals or surpasses finetuned models.</abstract>
      <url hash="659312f6">2025.findings-naacl.111</url>
      <bibkey>ayyubi-etal-2025-puzzlegpt</bibkey>
    </paper>
    <paper id="112">
      <title><fixed-case>SAFR</fixed-case>: Neuron Redistribution for Interpretability</title>
      <author><first>Ruidi</first><last>Chang</last><affiliation>Rice University</affiliation></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Hanjie</first><last>Chen</last><affiliation>Rice University</affiliation></author>
      <pages>2117-2126</pages>
      <abstract>Superposition refers to encoding representations of multiple features within a single neuron, which is common in deep neural networks. This property allows neurons to combine and represent multiple features, enabling the model to capture intricate information and handle complex tasks. Despite promising performance, the model’s interpretability has been diminished. This paper presents a novel approach to enhance model interpretability by regularizing feature superposition. We introduce SAFR, which simply applies regularizations to the loss function to promote monosemantic representations for important tokens while encouraging polysemanticity for correlated token pairs, where important tokens and correlated token pairs are identified via VMASK and attention weights respectively. We evaluate SAFR with a transformer model on two classification tasks. Experiments demonstrate the effectiveness of SAFR in improving model interpretability without compromising prediction performance. Besides, SAFR provides explanations by visualizing the neuron allocation within the intermediate layers.</abstract>
      <url hash="ff0afb1d">2025.findings-naacl.112</url>
      <bibkey>chang-etal-2025-safr</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>GPT</fixed-case>-4<fixed-case>V</fixed-case> Cannot Generate Radiology Reports Yet</title>
      <author><first>Yuyang</first><last>Jiang</last></author>
      <author><first>Chacha</first><last>Chen</last></author>
      <author><first>Dang</first><last>Nguyen</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Benjamin M.</first><last>Mervak</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <pages>2127-2154</pages>
      <abstract>GPT-4’s purported strong multimodal abilities raise interests in using it to automate radiology report writing, but there lacks thorough evaluations. In this work, we perform a systematic evaluation of GPT-4 (4o and vision-preview) in generating radiology reports across three chest X-ray report benchmarks: MIMIC-CXR, CheXpert Plus, and IU X-Ray. We attempt to directly generate reports with different prompting strategies and find that the models fail terribly in both lexical metrics and clinical efficacy metrics. To understand the low performance, we decompose the task into two steps: 1) the **medical image reasoning** step of predicting medical condition labels from images; and 2) the **report synthesis** step of generating reports from (groundtruth) conditions. We show that GPT-4’s performance in image reasoning is consistently low across different prompts. In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully. Even when given groundtruth conditions in report synthesis, its generated reports are less correct and less natural-sounding than a finetuned Llama. Altogether, our findings cast doubt on the viability of using GPT-4 in a radiology workflow.</abstract>
      <url hash="33147b16">2025.findings-naacl.113</url>
      <bibkey>jiang-etal-2025-gpt</bibkey>
    </paper>
    <paper id="114">
      <title>Is Semantic Chunking Worth the Computational Cost?</title>
      <author><first>Renyi</first><last>Qu</last><affiliation>Vectara</affiliation></author>
      <author><first>Ruixuan</first><last>Tu</last></author>
      <author><first>Forrest Sheng</first><last>Bao</last><affiliation>Vectara, Inc.</affiliation></author>
      <pages>2155-2177</pages>
      <abstract>Recent advances in Retrieval-Augmented Generation (RAG) systems have popularized semantic chunking, which aims to improve retrieval performance by dividing documents into semantically coherent segments. Despite its growing adoption, the actual benefits over simpler fixed-size chunking, where documents are split into consecutive, fixed-size segments, remain unclear. This study systematically evaluates the effectiveness of semantic chunking using three common retrieval-related tasks: document retrieval, evidence retrieval, and retrieval-based answer generation. The results show that the computational costs associated with semantic chunking are not justified by consistent performance gains. These findings challenge the previous assumptions about semantic chunking and highlight the need for more efficient chunking strategies in RAG systems.</abstract>
      <url hash="b2f78b28">2025.findings-naacl.114</url>
      <bibkey>qu-etal-2025-semantic</bibkey>
    </paper>
    <paper id="115">
      <title>On Using <fixed-case>A</fixed-case>rabic Language Dialects in Recommendation Systems</title>
      <author><first>Abdulla</first><last>Alshabanah</last></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>2178-2186</pages>
      <abstract>While natural language processing (NLP) techniques have been applied to user reviews in recommendation systems, the potential of leveraging Arabic dialects in this context remains unexplored. Arabic is spoken by over 420 million people, with significant dialectal variation across regions. These dialects, often classified as low-resource languages, present both challenges and opportunities for machine learning applications. This paper represents the first attempt to incorporate Arabic dialects as a signal in recommendation systems. We explore both explicit and implicit approaches for integrating Arabic dialect information from user reviews, demonstrating its impact on improving recommendation performance. Our findings highlight the potential for leveraging dialectal diversity in Arabic to enhance recommendation systems and encourage further research at the intersection of NLP and recommendation systems within the Arab multicultural world.</abstract>
      <url hash="1037976f">2025.findings-naacl.115</url>
      <bibkey>alshabanah-annavaram-2025-using</bibkey>
    </paper>
    <paper id="116">
      <title>Assessing <fixed-case>LLM</fixed-case>s for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing</title>
      <author><first>Hadi</first><last>Askari</last></author>
      <author><first>Anshuman</first><last>Chhabra</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Prasant</first><last>Mohapatra</last></author>
      <pages>2187-2201</pages>
      <abstract>Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles. However, little is known about the robustness of such a process of zero-shot summarization.To bridge this gap, we propose *relevance paraphrasing*, a simple strategy that can be used to measure the robustness of LLMs as summarizers. The relevance paraphrasing approach identifies the most *relevant* sentences that contribute to generating an ideal summary, and then *paraphrases* these inputs to obtain a minimally perturbed dataset. Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM’s one aspect of robustness. We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B-v1, and Dolly-v2-7B). Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements.</abstract>
      <url hash="e4ec9b20">2025.findings-naacl.116</url>
      <bibkey>askari-etal-2025-assessing</bibkey>
    </paper>
    <paper id="117">
      <title>Beyond Silent Letters: Amplifying <fixed-case>LLM</fixed-case>s in Emotion Recognition with Vocal Nuances</title>
      <author><first>Zehui</first><last>Wu</last></author>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Lin</first><last>Ai</last></author>
      <author><first>Pengyuan</first><last>Shi</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kaan</first><last>Donbekci</last><affiliation>Columbia University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <pages>2202-2218</pages>
      <url hash="3b9a30e8">2025.findings-naacl.117</url>
      <bibkey>wu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="118">
      <title><fixed-case>D</fixed-case>omain<fixed-case>S</fixed-case>um: A Hierarchical Benchmark for Fine-Grained Domain Shift in Abstractive Text Summarization</title>
      <author><first>Haohan</first><last>Yuan</last></author>
      <author><first>Haopeng</first><last>Zhang</last><affiliation>University of Hawaii System</affiliation></author>
      <pages>2219-2231</pages>
      <abstract>Most research on abstractive summarization focuses on single-domain applications, often neglecting how domain shifts between documents affect performance and the generalization ability of summarization models. To address this issue, we introduce DomainSum, a hierarchical benchmark designed to capture fine-grained domain shifts in abstractive summarization. We categorize these shifts into three levels: genre, style, and topic, and demonstrate through comprehensive benchmark analysis that they follow a hierarchical structure. Furthermore, we evaluate the domain generalization capabilities of commonly used pre-trained language models (PLMs) and large language models (LLMs) in both in-domain and cross-domain settings. Our benchmark and source code are released at https://github.com/hpzhang94/DomainSum.</abstract>
      <url hash="2e01f31c">2025.findings-naacl.118</url>
      <bibkey>yuan-zhang-2025-domainsum</bibkey>
    </paper>
    <paper id="119">
      <title>Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations</title>
      <author><first>Wenjie Jacky</first><last>Mo</last></author>
      <author><first>Jiashu</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Jiongxiao</first><last>Wang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Jun</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>Hadi</first><last>Askari</last></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>2232-2249</pages>
      <abstract>Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, this study critically examines the use of demonstrations as a defense mechanism against backdoor attacks in black-box LLMs. With an identified task, we retrieve task-relevant demonstrations from a clean data pool and integrate them with user queries during testing. Importantly, this approach does not necessitate modifications or tuning of the model, nor does it require insight into the model’s internal architecture. The alignment properties inherent in in-context learning play a pivotal role in mitigating the impact of backdoor triggers, effectively recalibrating the behavior of compromised models. Our experimental analysis demonstrates that this method robustly defends against both instance-level and instruction-level backdoor attacks, outperforming existing defense baselines across most evaluation scenarios.</abstract>
      <url hash="ac532c8a">2025.findings-naacl.119</url>
      <bibkey>mo-etal-2025-test</bibkey>
    </paper>
    <paper id="120">
      <title>“All that Glitters”: Techniques for Evaluations with Unreliable Model and Human Annotations</title>
      <author><first>Michael</first><last>Hardy</last></author>
      <pages>2250-2278</pages>
      <abstract>“Gold” and “ground truth” human-mediated labels have error. This error can escape commonly reported metrics of label quality or obscure questions of accuracy, bias, fairness, and usefulness during model evaluation. This study demonstrates methods for answering such questions even in the context of very low reliabilities from expert humans. We analyze human labels, GPT model ratings, and transformer encoder model ratings of the quality of classroom teaching from two LLM architecture families–encoders and GPT decoders. First, we demonstrate that using standard metrics in the presence of poor labels can mask both label and model quality. The encoder family of models achieve state-of-the-art, even “super-human”, results across all classroom annotation tasks using standard metrics. However, evaluation techniques accounting for unreliable labels reveal important flaws, including spurious correlations and nonrandom racial biases across models and humans. We estimate that if models were used in a human-in-the-loop context, the variance contributed by GPT model labels would worsen ratings. These techniques also highlight tasks where encoders could offer 80% reduction in human costs while also reducing bias.</abstract>
      <url hash="e8ca1fb5">2025.findings-naacl.120</url>
      <bibkey>hardy-2025-glitters</bibkey>
    </paper>
    <paper id="121">
      <title><fixed-case>K</fixed-case>wai<fixed-case>C</fixed-case>hat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus</title>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Yiming</first><last>Lei</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Chenkai</first><last>Zhang</last></author>
      <author><first>Haitao</first><last>Leng</last><affiliation>Kuaishou- 快手科技</affiliation></author>
      <author><first>Chuan</first><last>Wang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Qingjie</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunhong</first><last>Wang</last><affiliation>Beihang University</affiliation></author>
      <pages>2279-2294</pages>
      <abstract>Video-based dialogue systems have compelling application value, such as education assistants, thereby garnering growing interest. However, the current video-based dialogue systems are limited by their reliance on a single dialogue type, which hinders their versatility in practical applications across a range of scenarios, including question-answering and emotionally dialog, etc. In this paper, we identify this challenge as how to generate video-driven multilingual mixed-type dialogues. To mitigate this challenge, we propose a novel task and create a human-to-human video-driven multilingual mixed-type dialogue corpus, termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues, across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally, we establish baseline models on KwaiChat. An extensive analysis of 7 distinct LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still cannot perform well in this situation even with the help of in-context learning and fine-tuning, which indicates that the task is not trivial and needs further research.</abstract>
      <url hash="f975301b">2025.findings-naacl.121</url>
      <bibkey>shi-etal-2025-kwaichat</bibkey>
    </paper>
    <paper id="122">
      <title><fixed-case>G</fixed-case>en<fixed-case>EOL</fixed-case>: Harnessing the Generative Power of <fixed-case>LLM</fixed-case>s for Training-Free Sentence Embeddings</title>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>2295-2308</pages>
      <abstract>Training-free embedding methods directly leverage pretrained large language models (LLMs) to embed text, bypassing the costly and complex procedure of contrastive learning. Previous training-free embedding methods have mainly focused on optimizing embedding prompts and have overlooked the benefits of utilizing the generative abilities of LLMs. We propose a novel method, GenEOL, which uses LLMs to generate diverse transformations of a sentence that preserve its meaning, and aggregates the resulting embeddings of these transformations to enhance the overall sentence embedding. GenEOL significantly outperforms the existing training-free embedding methods by an average of 2.85 points across several LLMs on the sentence semantic text similarity (STS) benchmark. GenEOL also achieves notable gains in clustering, reranking, and pair-classification tasks from the MTEB benchmark. Additionally, GenEOL stabilizes representation quality across LLM layers and remains robust to perturbations of embedding prompts.</abstract>
      <url hash="fd8706d7">2025.findings-naacl.122</url>
      <bibkey>thirukovalluru-dhingra-2025-geneol</bibkey>
    </paper>
    <paper id="123">
      <title>Attention Tracker: Detecting Prompt Injection Attacks in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kuo-Han</first><last>Hung</last></author>
      <author><first>Ching-Yun</first><last>Ko</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ambrish</first><last>Rawat</last><affiliation>International Business Machines</affiliation></author>
      <author><first>I-Hsin</first><last>Chung</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Winston H.</first><last>Hsu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <pages>2309-2322</pages>
      <abstract>Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose Attention Tracker, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities.</abstract>
      <url hash="20d12c1e">2025.findings-naacl.123</url>
      <bibkey>hung-etal-2025-attention</bibkey>
    </paper>
    <paper id="124">
      <title>Unsupervised Speech-text word-level alignment with Dynamic Programming</title>
      <author><first>Tianshu</first><last>Yu</last></author>
      <author><first>Zihan</first><last>Gong</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Minghuan</first><last>Tan</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Guhong</first><last>Chen</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>2323-2334</pages>
      <url hash="de9eb415">2025.findings-naacl.124</url>
      <bibkey>yu-etal-2025-unsupervised</bibkey>
    </paper>
    <paper id="125">
      <title><fixed-case>S</fixed-case>ci<fixed-case>A</fixed-case>ssess: Benchmarking <fixed-case>LLM</fixed-case> Proficiency in Scientific Literature Analysis</title>
      <author><first>Hengxing</first><last>Cai</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xiaochen</first><last>Cai</last><affiliation>DP Technology</affiliation></author>
      <author><first>Junhan</first><last>Chang</last></author>
      <author><first>Sihang</first><last>Li</last></author>
      <author><first>Lin</first><last>Yao</last></author>
      <author><first>Wang</first><last>Changxin</last></author>
      <author><first>Zhifeng</first><last>Gao</last><affiliation>DP Technology</affiliation></author>
      <author><first>Hongshuai</first><last>Wang</last><affiliation>DP technology</affiliation></author>
      <author><first>Li</first><last>Yongge</last></author>
      <author><first>Mujie</first><last>Lin</last></author>
      <author><first>Shuwen</first><last>Yang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Jiankun</first><last>Wang</last></author>
      <author><first>Mingjun</first><last>Xu</last></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Xi</first><last>Fang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Jiaxi</first><last>Zhuang</last></author>
      <author><first>Yuqi</first><last>Yin</last></author>
      <author><first>Yaqi</first><last>Li</last></author>
      <author><first>Changhong</first><last>Chen</last><affiliation>DP Technology</affiliation></author>
      <author><first>Zheng</first><last>Cheng</last></author>
      <author><first>Zifeng</first><last>Zhao</last><affiliation>AI for Science Institute</affiliation></author>
      <author><first>Linfeng</first><last>Zhang</last><affiliation>DP Technology</affiliation></author>
      <author><first>Guolin</first><last>Ke</last><affiliation>DP Technology</affiliation></author>
      <pages>2335-2357</pages>
      <abstract>Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data.In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis &amp; Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine.To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis.SciAssess and its resources are available at <url>https://github.com/sci-assess/SciAssess</url>.</abstract>
      <url hash="560962be">2025.findings-naacl.125</url>
      <bibkey>cai-etal-2025-sciassess</bibkey>
    </paper>
    <paper id="126">
      <title>Towards Understanding the Fragility of Multilingual <fixed-case>LLM</fixed-case>s against Fine-Tuning Attacks</title>
      <author><first>Samuele</first><last>Poppi</last><affiliation>University of Modena and Reggio Emilia and University of Pisa</affiliation></author>
      <author><first>Zheng Xin</first><last>Yong</last><affiliation>Brown University</affiliation></author>
      <author><first>Yifei</first><last>He</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Bobbie</first><last>Chern</last></author>
      <author><first>Han</first><last>Zhao</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Aobo</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Jianfeng</first><last>Chi</last><affiliation>Meta AI</affiliation></author>
      <pages>2358-2372</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have sparked widespread concerns about their safety. Recent work demonstrates that safety alignment of LLMs can be easily removed by fine-tuning with a few adversarially chosen instruction-following examples, i.e., fine-tuning attacks. We take a further step to understand fine-tuning attacks in multilingual LLMs. We first discover cross-lingual generalization of fine-tuning attacks: using a few adversarially chosen instruction-following examples in one language, multilingual LLMs can also be easily compromised (e.g., multilingual LLMs fail to refuse harmful prompts in other languages). Motivated by this finding, we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space. Through SIL, we validate this hypothesis and find that only changing 20% of weight parameters in fine-tuning attacks can break safety alignment across all languages. Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks, and we demonstrate that our attack vector can still jailbreak LLMs adapted to new languages.</abstract>
      <url hash="c444aba9">2025.findings-naacl.126</url>
      <bibkey>poppi-etal-2025-towards</bibkey>
    </paper>
    <paper id="127">
      <title><fixed-case>MASSW</fixed-case>: A New Dataset and Benchmark Tasks for <fixed-case>AI</fixed-case>-Assisted Scientific Workflows</title>
      <author><first>Xingjian</first><last>Zhang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yutong</first><last>Xie</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jin</first><last>Huang</last></author>
      <author><first>Jinge</first><last>Ma</last></author>
      <author><first>Zhaoying</first><last>Pan</last></author>
      <author><first>Qijia</first><last>Liu</last></author>
      <author><first>Ziyang</first><last>Xiong</last></author>
      <author><first>Tolga</first><last>Ergen</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Dongsub</first><last>Shim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Qiaozhu</first><last>Mei</last><affiliation>Google and University of Michigan</affiliation></author>
      <pages>2373-2394</pages>
      <abstract>Scientific innovation relies on detailed workflows, which include critical steps such as contextualizing literature, generating ideas, validating ideas, interpreting results, and planning new research. Scientific publications that document these workflows are extensive and unstructured, making it difficult to effectively navigate and explore the space of scientific innovation. To meet this challenge, we introduce **MASSW**, a comprehensive dataset of **M**ulti-**A**spect **S**ummarization of **S**cientific **W**orkflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications – *context, key idea, method, outcome*, and *projected impact* – which correspond to five key steps in a research workflow. We show that these LLM-extract summaries have a comparable quality to human annotations, and they facilitate a variety of downstream tasks, corresponding to different types of predictions and recommendations along the scientific workflow. Overall, MASSW demonstrates decent utility as a pre-computed and trustful resource for the AI4Science community to create and benchmark a wide-range of new AI methods for optimizing scientific workflows and fostering scientific innovation. Our code and datasets are made available anonymously: [link](https://osf.io/7ygrq/?view_only=3d8261a0ea09489fa67ece2c68235afa).</abstract>
      <url hash="471a8d7a">2025.findings-naacl.127</url>
      <bibkey>zhang-etal-2025-massw</bibkey>
    </paper>
    <paper id="128">
      <title>Neuro-symbolic Training for Reasoning over Spatial Language</title>
      <author><first>Tanawan</first><last>Premsri</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>2395-2414</pages>
      <abstract>Spatial reasoning based on natural language expressions is essential for everyday human tasks. This reasoning ability is also crucial for machines to interact with their environment in a human-like manner. However, recent research shows that even state-of-the-art language models struggle with spatial reasoning over text, especially when facing nesting spatial expressions. This is attributed to not achieving the right level of abstraction required for generalizability.To alleviate this issue, we propose training language models with neuro-symbolic techniques that exploit the spatial logical rules as constraints, providing additional supervision to improve spatial reasoning and question answering.Training language models to adhere to spatial reasoning rules guides them in making more effective and general abstractions for transferring spatial knowledge to various domains. We evaluate our approach on existing spatial question-answering benchmarks. Our results indicate the effectiveness of our proposed technique in improving language models in complex multi-hop spatial reasoning over text.</abstract>
      <url hash="3d77cc93">2025.findings-naacl.128</url>
      <bibkey>premsri-kordjamshidi-2025-neuro</bibkey>
    </paper>
    <paper id="129">
      <title>On Localizing and Deleting Toxic Memories in Large Language Models</title>
      <author><first>Anubrata</first><last>Das</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Manoj</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>Amazon and University of Massachusetts, Lowell</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Morteza</first><last>Ziyadi</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>2415-2423</pages>
      <abstract>Warning: This paper contains offensive language.Ensuring that large language models (LLMs) do not generate harmful text is critical for their safe deployment. A common failure mode involves producing toxic responses to otherwise innocuous prompts. While various detoxification methods have been proposed, the underlying mechanisms that drive toxic generation in LLMs are not yet fully understood. Our work aims to provide a mechanistic understanding of toxic generation against innocuous-seeming adversarial prompts through the lens of memory localization. We find evidence of localization of toxic memories in the early Multilayer Perceptron (MLP) layers of GPT-2-XL. We further investigate the effects of editing and deleting these toxic memories in MLP layers to reduce toxic generation. Editing significantly reduces toxic generation, from 62.86% to 28.61%. However, this reduction comes with a trade-off in generation quality as perplexity increases from 78.18 on GPT2-XL against the adversarial prompts to 106.06 after editing. Localization-informed deletion achieves a better toxicity-perplexity tradeoff compared to random early layer editing, which reduces toxicity but leads to greater perplexity increases.</abstract>
      <url hash="1578570f">2025.findings-naacl.129</url>
      <bibkey>das-etal-2025-localizing</bibkey>
    </paper>
    <paper id="130">
      <title><fixed-case>D</fixed-case>i<fixed-case>VIS</fixed-case>e: Direct Visual-Input Speech Synthesis Preserving Speaker Characteristics And Intelligibility</title>
      <author><first>Yifan</first><last>Liu</last></author>
      <author><first>Yu</first><last>Fang</last></author>
      <author><first>Zhouhan</first><last>Lin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2424-2439</pages>
      <abstract>Video-to-speech (V2S) synthesis, the task of generating speech directly from silent video input, is inherently more challenging than other speech synthesis tasks due to the need to accurately reconstruct both speech content and speaker characteristics from visual cues alone. Recently, audio-visual pretraining has eliminated the need for additional acoustic hints in V2S, which previous methods often relied on to ensure training convergence. However, even with pretraining, existing methods continue to face challenges in achieving a balance between acoustic intelligibility and the preservation of speaker-specific characteristics. We analyzed this limitation and were motivated to introduce DiVISe (<b>Di</b>rect <b>V</b>sual-<b>I</b>nput <b>S</b>peech Synth<b>e</b>sis), an end-to-end V2S model that predicts Mel-spectrograms directly from video frames alone. Despite not taking any acoustic hints, DiVISe effectively preserves speaker characteristics in the generated audio, and achieves superior performance on both objective and subjective metrics across the LRS2 and LRS3 datasets. Our results demonstrate that DiVISe not only outperforms existing V2S models in acoustic intelligibility but also scales more effectively with increased data and model parameters. Code and weights will be made publicly available after acceptance of this paper.</abstract>
      <url hash="358871b9">2025.findings-naacl.130</url>
      <bibkey>liu-etal-2025-divise</bibkey>
    </paper>
    <paper id="131">
      <title><fixed-case>G</fixed-case>raph<fixed-case>ICL</fixed-case>: Unlocking Graph Learning Potential in <fixed-case>LLM</fixed-case>s through Structured Prompt Design</title>
      <author><first>Yuanfu</first><last>Sun</last><affiliation>New York University</affiliation></author>
      <author><first>Zhengnan</first><last>Ma</last></author>
      <author><first>Yi</first><last>Fang</last><affiliation>New York University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Case Western Reserve University</affiliation></author>
      <author><first>Qiaoyu</first><last>Tan</last><affiliation>New York University Shanghai</affiliation></author>
      <pages>2440-2459</pages>
      <abstract>The growing importance of textual and relational systems has driven interest in enhancing large language models (LLMs) for graph-structured data, particularly Text-Attributed Graphs (TAGs), where samples are represented by textual descriptions interconnected by edges. While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning, a comprehensive benchmark for evaluating LLMs solely through prompt design remains surprisingly absent. Without such a carefully crafted evaluation benchmark, most if not all, tailored graph LLMs are compared against general LLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can potentially camouflage many advantages as well as unexpected predicaments of them. To achieve more general evaluations and unveil the true potential of LLMs for graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a comprehensive benchmark comprising novel prompt templates designed to capture graph structure and handle limited label knowledge. Our systematic evaluation shows that general-purpose LLMs equipped with our GraphICL outperform state-of-the-art specialized graph LLMs and graph neural network models in resource-constrained settings and out-of-domain tasks. These findings highlight the significant potential of prompt engineering to enhance LLM performance on graph learning tasks without training and offer a strong baseline for advancing research in graph LLMs.</abstract>
      <url hash="ca5c9b69">2025.findings-naacl.131</url>
      <bibkey>sun-etal-2025-graphicl</bibkey>
    </paper>
    <paper id="132">
      <title><fixed-case>FIDELITY</fixed-case>: Fine-grained Interpretable Distillation for Effective Language Insights and Topic Yielding</title>
      <author><first>Divyansh</first><last>Singh</last></author>
      <author><first>Brodie</first><last>Mather</last><affiliation>The Institute for Human &amp; Machine Cognition</affiliation></author>
      <author><first>Demi</first><last>Zhang</last></author>
      <author><first>Patrick</first><last>Lehman</last></author>
      <author><first>Justin</first><last>Ho</last></author>
      <author><first>Bonnie J</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>2460-2472</pages>
      <abstract>The rapid expansion of text data has increased the need for effective methods to distill meaningful information from large datasets. Traditional and state-of-the-art approaches have made significant strides in topic modeling, yet they fall short in generating contextually specific and semantically intuitive topics, particularly in dynamic environments and low-resource languages. Additionally, multi-document summarization systems often struggle with issues like redundancy, scalability, and maintaining readability. We introduce FIDELITY (Fine-grained Interpretable Distillation for Effective Language Insights and Topic Yielding), a hybrid method that combines topic modeling and text summarization to produce fine-grained, semantically rich, and contextually relevant output. FIDELITY enhances dataset accessibility and interpretability, outperforming traditional models in topic diversity, similarity, and in the ability to process new, unseen documents. Additionally, it demonstrates robust multilingual capabilities, effectively handling low-resource languages like Tagalog. This makes FIDELITY a powerful tool for distilling and understanding complex textual data, providing detailed insights while maintaining the necessary granularity for practical applications.</abstract>
      <url hash="b7e0ec4e">2025.findings-naacl.132</url>
      <bibkey>singh-etal-2025-fidelity</bibkey>
    </paper>
    <paper id="133">
      <title><fixed-case>C</fixed-case>lassic4<fixed-case>C</fixed-case>hildren: Adapting <fixed-case>C</fixed-case>hinese Literary Classics for Children with Large Language Model</title>
      <author><first>Jiali</first><last>Chen</last></author>
      <author><first>Xusen</first><last>Hei</last></author>
      <author><first>Yuqi</first><last>Xue</last></author>
      <author><first>Zihan</first><last>Wu</last></author>
      <author><first>Jiayuan</first><last>Xie</last></author>
      <author><first>Yi</first><last>Cai</last><affiliation>South China University of Technology</affiliation></author>
      <pages>2473-2488</pages>
      <abstract>Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children’s reading preferences (i.e., vivid character portrayals, concise narrative structures, and appropriate readability with simpler words and sentences), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters’ personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children’s reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves performance in automatic and human evaluation.</abstract>
      <url hash="ed756d03">2025.findings-naacl.133</url>
      <bibkey>chen-etal-2025-classic4children</bibkey>
    </paper>
    <paper id="134">
      <title>Considering Length Diversity in Retrieval-Augmented Summarization</title>
      <author><first>Juseon-Do</first><last>Juseon-Do</last></author>
      <author><first>Jaesung</first><last>Hwang</last></author>
      <author><first>Jingun</first><last>Kwon</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Institute of Science Tokyo and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>2489-2500</pages>
      <abstract>This study investigates retrieval-augmented summarization by specifically examining the impact of exemplar summary lengths because previous methods have not considered length constraints. We propose a Diverse Length-aware Maximal Marginal Relevance (DL-MMR) algorithm to better control summary lengths. This algorithm combines the query relevance with diverse target lengths in retrieval-augmented summarization. Unlike previous methods that necessitate exhaustive exemplar-exemplar relevance comparisons using MMR, DL-MMR considers the exemplar target length as well and avoids comparing exemplars to each other, thereby reducing computational cost and conserving memory during the construction of an exemplar pool. Experimental results showed the effectiveness of DL-MMR, which considers length diversity, compared to the original MMR algorithm. DL-MMR additionally showed the effectiveness in memory saving of 781,513 times and computational cost reduction of 500,092 times, while maintaining the same level of informativeness.</abstract>
      <url hash="29ff7163">2025.findings-naacl.134</url>
      <bibkey>juseon-do-etal-2025-considering</bibkey>
    </paper>
    <paper id="135">
      <title><fixed-case>LMOD</fixed-case>: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models</title>
      <author><first>Zhenyue</first><last>Qin</last><affiliation>Yale University</affiliation></author>
      <author><first>Yu</first><last>Yin</last></author>
      <author><first>Dylan</first><last>Campbell</last><affiliation>Australian National University</affiliation></author>
      <author><first>Xuansheng</first><last>Wu</last></author>
      <author><first>Ke</first><last>Zou</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ninghao</first><last>Liu</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Yih Chung</first><last>Tham</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Qingyu</first><last>Chen</last><affiliation>Yale University</affiliation></author>
      <pages>2501-2522</pages>
      <abstract>The prevalence of vision-threatening eye diseases is a significant global burden, with many cases remaining undiagnosed or diagnosed too late for effective treatment. Large vision-language models (LVLMs) have the potential to assist in understanding anatomical information, diagnosing eye diseases, and drafting interpretations and follow-up plans, thereby reducing the burden on clinicians and improving access to eye care. However, limited benchmarks are available to assess LVLMs’ performance in ophthalmology-specific applications. In this study, we introduce LMOD, a large-scale multimodal ophthalmology benchmark consisting of 21,993 instances across (1) five ophthalmic imaging modalities: optical coherence tomography, color fundus photographs, scanning laser ophthalmoscopy, lens photographs, and surgical scenes; (2) free-text, demographic, and disease biomarker information; and (3) primary ophthalmology-specific applications such as anatomical information understanding, disease diagnosis, and subgroup analysis. In addition, we benchmarked 13 state-of-the-art LVLM representatives from closed-source, open-source, and medical domains. The results demonstrate a significant performance drop for LVLMs in ophthalmology compared to other domains. Systematic error analysis further identified six major failure modes: misclassification, failure to abstain, inconsistent reasoning, hallucination, assertions without justification, and lack of domain-specific knowledge. In contrast, supervised neural networks specifically trained on these tasks as baselines demonstrated high accuracy. These findings underscore the pressing need for benchmarks in the development and validation of ophthalmology-specific LVLMs.</abstract>
      <url hash="463667e9">2025.findings-naacl.135</url>
      <bibkey>qin-etal-2025-lmod</bibkey>
    </paper>
    <paper id="136">
      <title>Syntriever: How to Train Your Retriever with Synthetic Data from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Minsang</first><last>Kim</last><affiliation>SK Telecom</affiliation></author>
      <author><first>Seung Jun</first><last>Baek</last><affiliation>Korea University</affiliation></author>
      <pages>2523-2539</pages>
      <abstract>LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@<tex-math>K</tex-math>. the source code is available in https://github.com/kmswin1/Syntriever</abstract>
      <url hash="b0a190fa">2025.findings-naacl.136</url>
      <bibkey>kim-baek-2025-syntriever</bibkey>
    </paper>
    <paper id="137">
      <title><fixed-case>D</fixed-case>yn<fixed-case>C</fixed-case>lean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Huitong</first><last>Pan</last><affiliation>Temple University</affiliation></author>
      <author><first>Zhijia</first><last>Chen</last><affiliation>Facebook</affiliation></author>
      <author><first>Longin Jan</first><last>Latecki</last><affiliation>Temple University</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Eduard</first><last>Dragut</last><affiliation>Temple University</affiliation></author>
      <pages>2540-2556</pages>
      <abstract>Distantly Supervised Named Entity Recognition (DS-NER) has attracted attention due to its scalability and ability to automatically generate labeled data. However, distant annotation introduces many mislabeled instances, limiting its performance. Most of the existing work attempt to solve this problem by developing intricate models to learn from the noisy labels. An alternative approach is to attempt to clean the labeled data, thus increasing the quality of distant labels. This approach has received little attention for NER. In this paper, we propose a training dynamics-based label cleaning approach, which leverages the behavior of a model as training progresses to characterize the distantly annotated samples. We also introduce an automatic threshold estimation strategy to locate the errors in distant labels. Extensive experimental results demonstrate that: (1) models trained on our cleaned DS-NER datasets, which were refined by directly removing identified erroneous annotations, achieve significant improvements in F1-score, ranging from 3.18% to 8.95%; and (2) our method outperforms numerous advanced DS-NER approaches across four datasets.</abstract>
      <url hash="96326178">2025.findings-naacl.137</url>
      <bibkey>zhang-etal-2025-dynclean</bibkey>
    </paper>
    <paper id="138">
      <title>An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during Multi-stage Fine-tuning</title>
      <author><first>Andrew</first><last>Bai</last><affiliation>, University of California, Los Angeles</affiliation></author>
      <author><first>Chih-Kuan</first><last>Yeh</last><affiliation>Google</affiliation></author>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <author><first>Ankur</first><last>Taly</last><affiliation>Google</affiliation></author>
      <pages>2557-2569</pages>
      <abstract>Incrementally fine-tuning foundational models on new tasks or domains is now the de facto approach in NLP. A known pitfall of this approach is the <i>catastrophic forgetting</i> of prior knowledge that happens during fine-tuning. A common approach to alleviate such forgetting is to rehearse samples from prior tasks during fine-tuning. Several existing works assume a fixed memory buffer to store prior task examples, while relying on inferences (forward passes) with the model at hand for choosing examples for rehearsal from the buffer. However, given the increasing computational cost of model inference, and decreasing cost of data storage, we focus on the setting to rehearse samples with a fixed computational budget instead of a fixed memory budget. We propose a sampling scheme, <b>mix-cd</b>, that prioritizes rehearsal of “collateral damage” samples, which are samples predicted correctly by the prior model but forgotten by the incrementally tuned one. The crux of our scheme is a procedure to efficiently estimate the density of collateral damage samples without incurring additional model inferences. Our approach is computationally efficient, easy to implement, and outperforms several leading continual learning methods in compute-constrained settings. All the code will be publicly available at https://github.com/jybai/mix-cd-rehearsal.</abstract>
      <url hash="a79ecb79">2025.findings-naacl.138</url>
      <bibkey>bai-etal-2025-efficient</bibkey>
    </paper>
    <paper id="139">
      <title><fixed-case>COAST</fixed-case>: Enhancing the Code Debugging Ability of <fixed-case>LLM</fixed-case>s through Communicative Agent Based Data Synthesis</title>
      <author><first>Weiqing</first><last>Yang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hanbin</first><last>Wang</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xinze</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Yu</first><last>Gu</last></author>
      <author><first>Minghe</first><last>Yu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>2570-2585</pages>
      <abstract>Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at https://github.com/NEUIR/COAST.</abstract>
      <url hash="aca947a5">2025.findings-naacl.139</url>
      <bibkey>yang-etal-2025-coast</bibkey>
    </paper>
    <paper id="140">
      <title>Chain-of-Probe: Examining the Necessity and Accuracy of <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> Step-by-Step</title>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Weiwen</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yufei</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Liangyou</first><last>Li</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2586-2606</pages>
      <abstract>Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in confidence during the model’s reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by the reasoning steps required. Furthermore, by analyzing patterns in confidence change, we examine the correctness of the model’s reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model’s reasoning.</abstract>
      <url hash="dbb1f8f0">2025.findings-naacl.140</url>
      <bibkey>wang-etal-2025-chain</bibkey>
    </paper>
    <paper id="141">
      <title><fixed-case>INDIC</fixed-case> <fixed-case>QA</fixed-case> <fixed-case>BENCHMARK</fixed-case>: A Multilingual Benchmark to Evaluate Question Answering capability of <fixed-case>LLM</fixed-case>s for <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Abhishek Kumar</first><last>Singh</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Vishwajeet</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Ashish</first><last>Mittal</last><affiliation>IBM Research, Indian Institute of Technology, Bombay and IBM Research</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>2607-2626</pages>
      <abstract>Large Language Models (LLMs) perform well on unseen tasks in English, but their abilities in non-English languages are less explored due to limited benchmarks and training data. To bridge this gap, we introduce the Indic-QA Benchmark, a large dataset for context-grounded question answering in 11 major Indian languages, covering both extractive and abstractive tasks. Evaluations of multilingual LLMs, including instruction fine-tuned versions, revealed weak performance in low-resource languages due to a strong English-language bias in their training data. We also investigated the Translate-Test paradigm,where inputs are translated to English for processing and the results are translated back into the source language for output. This approach outperformed multilingual LLMs, particularly in low-resource settings. By releasing Indic-QA, we aim to promote further research into LLMs’ question-answering capabilities in low-resource languages. This benchmark offers a critical resource to address existing limitations and foster multilingual understanding.</abstract>
      <url hash="b5e0c645">2025.findings-naacl.141</url>
      <bibkey>singh-etal-2025-indic</bibkey>
    </paper>
    <paper id="142">
      <title>Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data</title>
      <author><first>Juanhui</first><last>Li</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>Hui</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheikh Muhammad</first><last>Sarwar</last></author>
      <author><first>Limeng</first><last>Cui</last><affiliation>Amazon</affiliation></author>
      <author><first>Hansu</first><last>Gu</last><affiliation>Amazon</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Qi</first><last>He</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>2627-2641</pages>
      <abstract>In real-world NLP applications, Large Language Models (LLMs) offer promising solutions due to their extensive training on vast datasets. However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required. To address these limitations, smaller models are typically preferred for deployment. However, their training is hindered by the scarcity of labeled data. In contrast, unlabeled data is often readily which can be leveraged by using LLMs to generate pseudo-labels for training smaller models. This enables the smaller models (student) to acquire knowledge from LLMs (teacher) while reducing computational costs. This process introduces challenges, such as potential noisy pseudo-labels. % and the high computational expense of processing large unlabeled datasets. Selecting high-quality and informative data is therefore critical to enhance model performance while improving the efficiency of data utilization. To address this, we propose LLKD that enables Learning with Less computational resources and less data for Knowledge Distillation from LLMs. LLKD is an adaptive sample selection method that incorporates signals from both the teacher and student. Specifically, it prioritizes samples where the teacher demonstrates high confidence in its labeling, indicating reliable labels, and where the student exhibits a high information need, identifying challenging samples that require further learning. Our comprehensive experiments show that LLKD achieves superior performance across various datasets with higher data efficiency.</abstract>
      <url hash="b83b74f2">2025.findings-naacl.142</url>
      <bibkey>li-etal-2025-learning</bibkey>
    </paper>
    <paper id="143">
      <title><fixed-case>LSDC</fixed-case>: An Efficient and Effective Large-Scale Data Compression Method for Supervised Fine-tuning of Large Language Models</title>
      <author><first>Zhaoguang</first><last>Long</last></author>
      <author><first>Yuhao</first><last>Zhou</last></author>
      <author><first>Shangqing</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yupei</first><last>Ren</last></author>
      <author><first>Li</first><last>Cai</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Chenghao</first><last>Jia</last></author>
      <author><first>Zhe</first><last>Chen</last></author>
      <author><first>Zhe</first><last>Fang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yuxiang</first><last>Song</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>2642-2653</pages>
      <abstract>With the scale of Large Language Models(LLMs) and the size of the training data continuing to expand, the computational costs required for training or tuning have significantly increased as well. In this work we propose an efficient and effective Large-Scale Data Compression (LSDC) method to substantially reduce the size of training data and thus enhance the training efficiency without compromising the performance of LLMs through a bifurcated quantization strategy. Specifically, our method first segments the dataset into multiple clusters, significantly reducing the time and memory requirements for data compression. Then, during the second phase of coreset selection, the diversity of samples is ensured by maximizing the submodular gain in order to avoid performance degradation. The comparative experiments showed that the performance of LLMs fine-tuned on a 20% compressed subset of the Alpaca dataset using LSDC outperformed those on the full dataset. Moreover,on a domain-specific instruction dataset of millions of samples, the LLMs fine-tuned on a 10% compressed dataset using LSDC outperformed those on the entire dataset, which dramatically enhances the domain-adaption capabilities of LLMs. This provides a promising potential of LSDC in training bigger LLMs from scratch and supervised fine-tuning as well.</abstract>
      <url hash="b683ff24">2025.findings-naacl.143</url>
      <bibkey>long-etal-2025-lsdc</bibkey>
    </paper>
    <paper id="144">
      <title>What Is Missing in Multilingual Visual Reasoning and How to Fix It</title>
      <author><first>Yueqi</first><last>Song</last></author>
      <author><first>Simran</first><last>Khanuja</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2654-2667</pages>
      <abstract>NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on this task in a zero-shot setting, boosting open models LLaVA-v1.5-13B by 13.4%, LLaVA-v1.6-34B by 20.3%, and Qwen-VL by 16.7%, while also minorly improving GPT-4V’s performance.</abstract>
      <url hash="d527c997">2025.findings-naacl.144</url>
      <bibkey>song-etal-2025-missing</bibkey>
    </paper>
    <paper id="145">
      <title>Enhancing the Prototype Network with Local-to-Global Optimization for Few-Shot Relation Extraction</title>
      <author><first>Hui</first><last>Sun</last></author>
      <author><first>Rongxin</first><last>Chen</last><affiliation>Jimei University</affiliation></author>
      <pages>2668-2677</pages>
      <abstract>Few-Shot Relation Extraction (FSRE) aims to achieve high classification performance by training relation classification models with a small amount of labeled data. Prototypical networks serve as a straightforward and efficient method for optimizing model performance by combining similarity evaluation and contrastive learning. However, directly integrating these methods can introduce unpredictable noise, such as information redundancy, which hinders classification performance and negatively affects embedding space learning. The technique presented in this paper applies Local-To-Global optimization to enhance prototypical networks in few-shot relation extraction. Specifically, this paper develops a local optimization strategy that indirectly optimizes the prototypes by optimizing the other information contained within the prototypes. It considers relation prototypes as global anchors and incorporates the techniques introduced in this paper, such as information alignment, local contrastive learning, and a local adaptive focal loss function, to address the issues of information redundancy. This approach enables the model to learn a unified and effective embedding space. We conduct extensive experiments on the FewRel 1.0 and FewRel 2.0 datasets to validate the effectiveness of the proposed model.</abstract>
      <url hash="4c701214">2025.findings-naacl.145</url>
      <bibkey>sun-chen-2025-enhancing</bibkey>
    </paper>
    <paper id="146">
      <title><fixed-case>LLM</fixed-case>s for Mathematical Modeling: Towards Bridging the Gap between Natural and Mathematical Languages</title>
      <author><first>Xuhan</first><last>Huang</last></author>
      <author><first>Qingning</first><last>Shen</last></author>
      <author><first>Yan</first><last>Hu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>2678-2710</pages>
      <url hash="51abce3e">2025.findings-naacl.146</url>
      <bibkey>huang-etal-2025-llms</bibkey>
    </paper>
    <paper id="147">
      <title>Advancing <fixed-case>P</fixed-case>ersian <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Sara Bourbour</first><last>Hosseinbeigi</last><affiliation>Tarbiat Modares University</affiliation></author>
      <author><first>Behnam</first><last>Rohani</last><affiliation>Sharif University of Technology, Sharif University of Technology</affiliation></author>
      <author><first>Mostafa</first><last>Masoudi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mehrnoush</first><last>Shamsfard</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Zahra</first><last>Saaberi</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Mostafa Karimi</first><last>Manesh</last><affiliation>Shahid Beheshti University</affiliation></author>
      <author><first>Mohammad Amin</first><last>Abbasi</last><affiliation>Iran University of Science and Technology Tehran, University of Tehran</affiliation></author>
      <pages>2711-2727</pages>
      <abstract>Evaluation of large language models (LLMs) in low-resource languages like Persian has received less attention than in high-resource languages like English. Existing evaluation approaches for Persian LLMs generally lack comprehensive frameworks, limiting their ability to assess models’ performance over a wide range of tasks requiring considerable cultural and contextual knowledge, as well as a deeper understanding of Persian literature and style. This paper first aims to fill this gap by providing two new benchmarks, PeKA and PK-BETS, on topics such as history, literature, and cultural knowledge, as well as challenging the present state-of-the-art models’ abilities in a variety of Persian language comprehension tasks. These datasets are meant to reduce data contamination while providing an accurate assessment of Persian LLMs. The second aim of this paper is the general evaluation of LLMs across the current Persian benchmarks to provide a comprehensive performance overview. By offering a structured evaluation methodology, we hope to promote the examination of LLMs in the Persian language.</abstract>
      <url hash="381c3142">2025.findings-naacl.147</url>
      <bibkey>hosseinbeigi-etal-2025-advancing</bibkey>
    </paper>
    <paper id="148">
      <title>Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling</title>
      <author><first>Zile</first><last>Qiao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Tong</first><last>Mo</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Weiping</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2728-2740</pages>
      <abstract>Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation. Specifically, we introduce the novel concept of “supportiveness”—which represents how effectively a knowledge piece facilitates downstream tasks. Based on supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4.</abstract>
      <url hash="6d5256c0">2025.findings-naacl.148</url>
      <bibkey>qiao-etal-2025-supportiveness</bibkey>
    </paper>
    <paper id="149">
      <title>Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models</title>
      <author><first>Jiatao</first><last>Li</last></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xunjian</first><last>Yin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>2741-2775</pages>
      <abstract>The integration of documents generated by LLMs themselves (Self-Docs) alongside retrieved documents has emerged as a promising strategy for retrieval-augmented generation systems. However, previous research primarily focuses on optimizing the use of Self-Docs, with their inherent properties remaining underexplored. To bridge this gap, we first investigate the overall effectiveness of Self-Docs, identifying key factors that shape their contribution to RAG performance (RQ1). Building on these insights, we develop a taxonomy grounded in Systemic Functional Linguistics to compare the influence of various Self-Docs categories (RQ2) and explore strategies for combining them with external sources (RQ3). Our findings reveal which types of Self-Docs are most beneficial and offer practical guidelines for leveraging them to achieve significant improvements in knowledge-intensive question answering tasks.</abstract>
      <url hash="b3e17add">2025.findings-naacl.149</url>
      <bibkey>li-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="150">
      <title><fixed-case>PREMISE</fixed-case>: Matching-based Prediction for Accurate Review Recommendation</title>
      <author><first>Wei</first><last>Han</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>2776-2794</pages>
      <abstract>We present PREMISE, a new architecture for the matching-based learning in the multimodal fields for the MRHP task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.</abstract>
      <url hash="91afa145">2025.findings-naacl.150</url>
      <bibkey>han-etal-2025-premise</bibkey>
    </paper>
    <paper id="151">
      <title>Semi-supervised Fine-tuning for Large Language Models</title>
      <author><first>Junyu</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Zhiping</first><last>Xiao</last><affiliation>University of Washington</affiliation></author>
      <author><first>Wei</first><last>Ju</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>2795-2808</pages>
      <abstract>Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated.Towards this end, we introduce a **semi-supervised fine-tuning (SemiFT)** task and a framework named **SemiEvol** for LLM alignment from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios. Github Repository: [https://github.com/luo-junyu/SemiEvol](https://github.com/luo-junyu/SemiEvol).</abstract>
      <url hash="fc436725">2025.findings-naacl.151</url>
      <bibkey>luo-etal-2025-semi</bibkey>
    </paper>
    <paper id="152">
      <title><fixed-case>CALM</fixed-case>: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering</title>
      <author><first>Yumeng</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Fan</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yi R.</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>2809-2817</pages>
      <abstract>Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions across languages, we observe significant performance disparities. To address this, we explore the **C**ross-Lingual Self-**A**ligning ability of **L**anguage **M**odels (**CALM**) to align knowledge across languages. Specifically, for a given question, we sample multiple responses across different languages and select the most self-consistent response as the target, leaving the remaining responses as negative examples. We then employ direct preference optimization (DPO) to align the model’s knowledge across different languages. Evaluations on the MEDQA and X-CSQA datasets demonstrate CALM’s effectiveness in enhancing cross-lingual knowledge question answering, both in zero-shot and retrieval-augmented settings. We also found that increasing the number of languages involved in CALM training leads to higher accuracy and consistency. We offer a qualitative analysis of how cross-lingual consistency can enhance knowledge alignment and explore the method’s generalizability.</abstract>
      <url hash="b878079a">2025.findings-naacl.152</url>
      <bibkey>wang-etal-2025-calm</bibkey>
    </paper>
    <paper id="153">
      <title>Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring</title>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Taehee</first><last>Park</last></author>
      <author><first>Sangwon</first><last>Ryu</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>2818-2824</pages>
      <abstract>In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method’s generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts.</abstract>
      <url hash="38524c55">2025.findings-naacl.153</url>
      <bibkey>do-etal-2025-towards</bibkey>
    </paper>
    <paper id="154">
      <title><fixed-case>M</fixed-case>ed<fixed-case>E</fixed-case>ureka: A Medical Domain Benchmark for Multi-Granularity and Multi-Data-Type Embedding-Based Retrieval</title>
      <author><first>Yongqi</first><last>Fan</last></author>
      <author><first>Nan</first><last>Wang</last></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>2825-2851</pages>
      <abstract>Embedding-based retrieval (EBR), the mainstream approach in information retrieval (IR), aims to help users obtain relevant information and plays a crucial role in retrieval-augmented generation (RAG) techniques of large language models (LLMs). Numerous methods have been proposed to significantly improve the quality of retrieved content and many generic benchmarks are proposed to evaluate the retrieval abilities of embedding models. However, texts in the medical domain present unique contexts, structures, and language patterns, such as terminology, doctor-patient dialogue, and electronic health records (EHRs). Despite these unique features, specific benchmarks for medical context retrieval are still lacking. In this paper, we propose MedEureka, an enriched benchmark designed to evaluate medical-context retrieval capabilities of embedding models with multi-granularity and multi-data types. MedEureka includes four levels of granularity and six types of medical texts, encompassing 18 datasets, incorporating granularity and data type description to prompt instruction-fine-tuned text embedding models for embedding generation. We also provide the MedEureka Toolkit to support evaluation on the MedEureka test set. Our experiments evaluate state-of-the-art open-source and proprietary embedding models, and fine-tuned classical baselines, providing a detailed performance analysis. This underscores the challenges of using embedding models for medical domain retrieval and the need for further research. Our code and data are released in the repository: <url>https://github.com/JOHNNY-fans/MedEureka</url>.</abstract>
      <url hash="7816f9d5">2025.findings-naacl.154</url>
      <bibkey>fan-etal-2025-medeureka</bibkey>
    </paper>
    <paper id="155">
      <title>A Federated Framework for <fixed-case>LLM</fixed-case>-based Recommendation</title>
      <author><first>Jujia</first><last>Zhao</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Chen</first><last>Xu</last></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>2852-2865</pages>
      <abstract>Large Language Models (LLMs) have showcased their potential in building generative recommendation systems through fine-tuning user behavior data. However, utilizing the user behavior data may pose significant privacy risks like in the traditional recommender models, potentially leading to ethical dilemmas and violations of data protection regulations. To address the privacy concerns, Federated Learning for Recommendation (Fed4Rec) has been identified as a promising solution. However, directly applying Fed4Rec in the LLM context introduces two challenges: 1) exacerbated client performance imbalance, which ultimately impacts the system’s long-term effectiveness, and 2) substantial client resource costs, posing a high demand for clients’ both computational and storage capability to locally train and infer LLMs.To tackle these challenges, we propose a federated framework for LLM-based recommendation (shorted as FELLRec). Generally, FELLRec designs two key strategies. 1) Dynamic balance strategy, which designs dynamic parameter aggregation and learning speed for different clients during training, aiming to ensure relatively balanced performance across clients. 2) Flexible storage strategy, which selectively retains certain sensitive LLM layers on the client side, while offloading other layers to the server, aiming to preserve privacy while saving resources. Specifically, FELLRec flexibly maintains those input and output layers on the client side to ensure the protection of all sensitive information. Experiment results show that FELLRec can achieve a more balanced client performance and improved overall performance in a computational and storage-efficient way while safeguarding user privacy well.</abstract>
      <url hash="99f2ea32">2025.findings-naacl.155</url>
      <bibkey>zhao-etal-2025-federated</bibkey>
    </paper>
    <paper id="156">
      <title><fixed-case>W</fixed-case>ater<fixed-case>S</fixed-case>eeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents</title>
      <author><first>Leyi</first><last>Pan</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yijian</first><last>Lu</last></author>
      <author><first>Zitian</first><last>Gao</last><affiliation>Ubiquant</affiliation></author>
      <author><first>Yichen</first><last>Di</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Irwin</first><last>King</last></author>
      <author><first>Philip S.</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>2866-2882</pages>
      <abstract>Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, its localization capability lays the foundation for building interpretable AI detection systems. Our code is available at https://github.com/THU-BPM/WaterSeeker.</abstract>
      <url hash="40b052d2">2025.findings-naacl.156</url>
      <bibkey>pan-etal-2025-waterseeker</bibkey>
    </paper>
    <paper id="157">
      <title><fixed-case>MIRAGE</fixed-case>: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation</title>
      <author><first>Chanhee</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>2883-2900</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings.</abstract>
      <url hash="4c966747">2025.findings-naacl.157</url>
      <bibkey>park-etal-2025-mirage</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>FIRE</fixed-case>: Fact-checking with Iterative Retrieval and Verification</title>
      <author><first>Zhuohan</first><last>Xie</last></author>
      <author><first>Rui</first><last>Xing</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Melbourne</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Hasan</first><last>Iqbal</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Dhruv</first><last>Sahnan</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>2901-2914</pages>
      <abstract>Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model’s internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose FIRE, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, FIRE employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare FIRE with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that FIRE holds promise for application in large-scale fact-checking operations.</abstract>
      <url hash="c365b339">2025.findings-naacl.158</url>
      <bibkey>xie-etal-2025-fire</bibkey>
    </paper>
    <paper id="159">
      <title>Lessons from a User Experience Evaluation of <fixed-case>NLP</fixed-case> Interfaces</title>
      <author><first>Eduardo</first><last>Calò</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Lydia</first><last>Penkert</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Saad</first><last>Mahamood</last><affiliation>trivago N.V.</affiliation></author>
      <pages>2915-2929</pages>
      <abstract>Human evaluations lay at the heart of evaluations within the field of Natural Language Processing (NLP). Seen as the “golden standard” of evaluations, questions are being asked on whether these evaluations are both reproducible and repeatable. One overlooked aspect is the design choices made by researchers when designing user interfaces (UIs). In this paper, four UIs used in past NLP human evaluations are assessed by UX experts, based on standardized human-centered interaction principles. Building on these insights, we derive several recommendations that the NLP community should apply when designing UIs, to enable more consistent human evaluation responses.</abstract>
      <url hash="71e8a802">2025.findings-naacl.159</url>
      <bibkey>calo-etal-2025-lessons</bibkey>
    </paper>
    <paper id="160">
      <title><fixed-case>T</fixed-case>rend<fixed-case>S</fixed-case>im: Simulating Trending Topics in Social Media Under Poisoning Attacks with <fixed-case>LLM</fixed-case>-based Multi-agent System</title>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Jianxun</first><last>Lian</last></author>
      <author><first>Chen</first><last>Ma</last></author>
      <author><first>Yaning</first><last>Qu</last></author>
      <author><first>Ye</first><last>Luo</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Xu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Le</first><last>Wu</last><affiliation>Hefei University of Technology</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2930-2949</pages>
      <abstract>Trending topics have become a significant part of modern social media, attracting users to participate in discussions of breaking events. However, they also bring in a new channel for poisoning attacks, resulting in negative impacts on society. Therefore, it is urgent to study this critical problem and develop effective strategies for defense. In this paper, we propose TrendSim, an LLM-based multi-agent system to simulate trending topics in social media under poisoning attacks. Specifically, we create a simulation environment for trending topics that incorporates a time-aware interaction mechanism, centralized message dissemination, and an interactive system. Moreover, we develop LLM-based humanoid agents to simulate users in social media, and propose prototype-based attackers to replicate poisoning attacks. Besides, we evaluate TrendSim from multiple aspects to validate its effectiveness. Based on TrendSim, we conduct simulation experiments to study four critical problems about poisoning attacks on trending topics.</abstract>
      <url hash="651eba0f">2025.findings-naacl.160</url>
      <bibkey>zhang-etal-2025-trendsim</bibkey>
    </paper>
    <paper id="161">
      <title><fixed-case>ASR</fixed-case>ank: Zero-Shot Re-Ranking with Answer Scent for Document Retrieval</title>
      <author><first>Abdelrahman</first><last>Abdallah</last></author>
      <author><first>Jamshid</first><last>Mozafari</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Bhawna</first><last>Piryani</last><affiliation>Universität Innsbruck</affiliation></author>
      <author><first>Adam</first><last>Jatowt</last><affiliation>Universität Innsbruck</affiliation></author>
      <pages>2950-2970</pages>
      <abstract>Retrieval-Augmented Generation (RAG) models have drawn considerable attention in modern open-domain question answering. The effectiveness of RAG depends on the quality of the top retrieved documents. However, conventional retrieval methods sometimes fail to rank the most relevant documents at the top. In this paper, we introduce ASRANK, a new re-ranking method based on scoring retrieved documents using zero-shot answer scent which relies on a pre-trained large language model to compute the likelihood of the document-derived answers aligning with the answer scent. Our approach demonstrates marked improvements across several datasets, including NQ, TriviaQA, WebQA, ArchivalQA, HotpotQA, and Entity Questions. Notably, ASRANK increases Top-1 retrieval accuracy on NQ from 19.2% to 46.5% for MSS and 22.1% to 47.3% for BM25. It also shows strong retrieval performance on several datasets compared to state-of-the-art methods (47.3 Top-1 by ASRANK vs 35.4 by UPR by BM25).</abstract>
      <url hash="270f9aee">2025.findings-naacl.161</url>
      <bibkey>abdallah-etal-2025-asrank</bibkey>
    </paper>
    <paper id="162">
      <title><fixed-case>DSQG</fixed-case>-Syn: Synthesizing High-quality Data for Text-to-<fixed-case>SQL</fixed-case> Parsing by Domain Specific Question Generation</title>
      <author><first>Shaoming</first><last>Duan</last></author>
      <author><first>Youxuan</first><last>Wu</last></author>
      <author><first>Chuanyi</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Han</last><affiliation>Harbin Institute of Technology(ShenZhen)</affiliation></author>
      <author><first>Shengyuan</first><last>Yu</last></author>
      <author><first>Liang</first><last>Yan</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Yingwei</first><last>Liang</last><affiliation>Guangdong Power Grid Co., Ltd</affiliation></author>
      <pages>2971-2989</pages>
      <abstract>Synthetic data has recently proven effective in enhancing the accuracy of Text-to-SQL parsers. However, existing methods generate SQL queries first by randomly sampling tables and columns based on probability and then synthesize natural language questions (NLQs). This approach often produces a large number of NLQ-SQL pairs that are irrelevant to the target domain and inconsistent in query intent, significantly diminishing the fine-tuning effectiveness of LLMs. In this paper, we introduce DSQG-Syn, a novel text-to-SQL data synthesis framework that based on domain-specific question generation. Specifically, we design a question generation method that creates domain-relevant questions based on predefined question types, ensuring coverage of major SQL operations. Guided by these questions, we synthesize NLQ-SQL pairs that are both domain-relevant and intent-consistent. To further enhance data quality, we filter out noisy samples from the generated pairs. When popular open-source LLMs are fine-tuned on our high-quality synthesized dataset, they achieve significant accuracy improvements, surpassing the performance of closed-source LLM-based approaches. Moreover, we demonstrate that our method outperforms existing state-of-the-art (SOTA) data synthesis techniques.</abstract>
      <url hash="8f0f1389">2025.findings-naacl.162</url>
      <bibkey>duan-etal-2025-dsqg</bibkey>
    </paper>
    <paper id="163">
      <title><fixed-case>E</fixed-case>go<fixed-case>S</fixed-case>peak: Learning When to Speak for Egocentric Conversational Agents in the Wild</title>
      <author><first>Junhyeok</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Min Soo</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Jungbin</first><last>Cho</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jisoo</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Sungwoong</first><last>Kim</last></author>
      <author><first>Gyeongbo</first><last>Sim</last><affiliation>NCSOFT</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>2990-3005</pages>
      <abstract>Predicting when to initiate speech in real-world environments remains a fundamental challenge for conversational agents. We introduce , a novel framework for real-time speech initiation prediction in egocentric streaming video. By modeling the conversation from the speaker’s first-person viewpoint, is tailored for human-like interactions in which a conversational agent must continuously observe its environment and dynamically decide when to talk.Our approach bridges the gap between simplified experimental setups and complex natural conversations by integrating four key capabilities: (1) first-person perspective, (2) RGB processing, (3) online processing, and (4) untrimmed video processing. We also present YT-Conversation, a diverse collection of in-the-wild conversational videos from YouTube, as a resource for large-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that outperforms random and silence-based baselines in real time. Our results also highlight the importance of multimodal input and context length in effectively deciding when to speak. Code and data are available at website.</abstract>
      <url hash="6c6da0af">2025.findings-naacl.163</url>
      <bibkey>kim-etal-2025-egospeak</bibkey>
    </paper>
    <paper id="164">
      <title><fixed-case>P</fixed-case>lot2<fixed-case>C</fixed-case>ode: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots</title>
      <author><first>Chengyue</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zhixuan</first><last>Liang</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Yixiao</first><last>Ge</last></author>
      <author><first>Qiushan</first><last>Guo</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zeyu</first><last>Lu</last></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Ying</first><last>Shan</last><affiliation>Tencent AI Lab Center of Visual Computing and Tencent PCG ARC Lab</affiliation></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>3006-3028</pages>
      <abstract>Multi-modal Large Language Models have shown remarkable progress in visual contexts, yet their ability to convert visual figures into executable code remains underexplored. To address this, we introduce Plot2Code, a comprehensive benchmark designed to assess MLLMs’ visual coding capabilities. Plot2Code includes 132 high-quality matplotlib plots across six plot types, as well as an additional 150 and 86 plots from Python’s and R’s plotly libraries respectively, totaling 368 plots. Each plot is paired with its source code and a descriptive instruction generated by GPT-4, enabling thorough evaluation across diverse inputs. Furthermore, we propose three automatic evaluation metrics—code pass rate, text-match ratio, and GPT-4V rating judgement—to assess the quality of generated code and rendered images. Notably, the GPT-4V rating demonstrates strong reliability, as it correlates well with human evaluations, particularly for datasets of a certain size. Cross-validation across MLLMs (GPT-4V, Gemini-1.5-Pro, and Claude-3-Opus) also shows high consistency in ratings, which likely stems from the fact that ratings are based on rendered images rather than direct MLLM outputs, indicating minimal bias for this metric. Our evaluation of 14 MLLMs, including both proprietary and open-source models, highlights significant challenges in visual coding, particularly for text-dense plots, where MLLMs heavily rely on textual instructions. We believe these findings will advance future development of MLLMs.</abstract>
      <url hash="6d910c3c">2025.findings-naacl.164</url>
      <bibkey>wu-etal-2025-plot2code</bibkey>
    </paper>
    <paper id="165">
      <title><fixed-case>F</fixed-case>unnel<fixed-case>RAG</fixed-case>: A Coarse-to-Fine Progressive Retrieval Paradigm for <fixed-case>RAG</fixed-case></title>
      <author><first>Xinping</first><last>Zhao</last></author>
      <author><first>Yan</first><last>Zhong</last></author>
      <author><first>Zetian</first><last>Sun</last></author>
      <author><first>Xinshuo</first><last>Hu</last></author>
      <author><first>Zhenyu</first><last>Liu</last></author>
      <author><first>Dongfang</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>3029-3046</pages>
      <abstract>Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It mainly consists of retrieval and generation. The retrieval modules (a.k.a. retrievers) aim to find useful information used to facilitate the generation modules (a.k.a. generators). As such, generators’ performance largely depends on the effectiveness and efficiency of retrievers. However, the widely used retrieval paradigm remains flat. It treats retrieval procedures as a one-off deal with constant granularity. Despite effectiveness, we argue that they suffer from two limitations: (1) flat retrieval exerts a significant burden on one retriever; (2) constant granularity limits the ceiling of retrieval performance. In this work, we propose a progressive retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small quantity, and low-to-high capacity, which can relieve the burden on one retriever and also promote the ceiling of retrieval performance. Extensive experiments manifest that FunnelRAG achieves comparable retrieval performance while the time overhead is reduced by nearly 40 percent.</abstract>
      <url hash="81493860">2025.findings-naacl.165</url>
      <bibkey>zhao-etal-2025-funnelrag</bibkey>
    </paper>
    <paper id="166">
      <title>The Power of Bullet Lists: A Simple Yet Effective Prompting Approach to Enhancing Spatial Reasoning in Large Language Models</title>
      <author><first>Ikhyun</first><last>Cho</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Changyeon</first><last>Park</last><affiliation>Mirae Asset Securities</affiliation></author>
      <author><first>Julia</first><last>Hockenmaier</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>3047-3057</pages>
      <abstract>While large language models (LLMs) are dominating the field of natural language processing, it remains an open question how well these models can perform spatial reasoning. Contrary to recent studies suggesting that LLMs struggle with spatial reasoning tasks, we demonstrate in this paper that a novel prompting technique, termed Patient Visualization of Thought (Patient-VoT), can boost LLMs’ spatial reasoning abilities. The core idea behind Patient-VoT is to explicitly integrate *bullet lists, coordinates, and visualizations* into the reasoning process. By applying Patient-VoT, we achieve a significant boost in spatial reasoning performance compared to prior prompting techniques. We also show that integrating bullet lists into reasoning is effective in planning tasks, highlighting its general effectiveness across different applications.</abstract>
      <url hash="5afb6659">2025.findings-naacl.166</url>
      <bibkey>cho-etal-2025-power</bibkey>
    </paper>
    <paper id="167">
      <title>Overcoming both Domain Shift and Label Shift for Referring Video Segmentation</title>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Sashuai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yan</first><last>Xia</last></author>
      <pages>3058-3069</pages>
      <abstract>Open-set domain generalization (OSDG) aims to enhance the robustness of the model when facing both domain shift and label shift, highlighting a wide range of potential in real-world applications. However, previous OSDG methods can only recognize seen objects and mark all unseen objects as “unknown” categories during inference, which is far from satisfactory. In this paper, we explore the scenario of referring video segmentation to study how to make the model maintain good segmentation ability for unknown objects under OSDG setting. To bridge the huge gap caused by label shift, we propose CLIP-based Reasoning Prompt (CRPrompt), which can combine text and visual prompts together to improve text-object matching ability of CLIP, transferring the segmentation ability to unseen classes based on the knowledge learned from seen classes and large-scale text-image pairs, i.e., color, shape, spatial relationships. Meanwhile, to improve the robustness of CRPrompt, we propose Retrieval-augmented Instance Normalization (RaIN), which can effectively enhance the robustness of the model by retrieving visual objects with similar semantic concepts through input query and performing Instance Norm among them. Extensive experiments on open-set and zero-shot domain generalization tasks demonstrate the effectiveness of our approach.</abstract>
      <url hash="7a3c0762">2025.findings-naacl.167</url>
      <bibkey>huang-etal-2025-overcoming</bibkey>
    </paper>
    <paper id="168">
      <title>Language Modeling with Editable External Knowledge</title>
      <author><first>Belinda Z.</first><last>Li</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Emmy</first><last>Liu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Alexis</first><last>Ross</last><affiliation>Massachusetts Institute of Technology and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Abbas</first><last>Zeitoun</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>3070-3090</pages>
      <abstract>When the world changes, so does the text that people write about it. How do we build language models that can be easily updated to reflect these changes? One popular approach is retrieval-augmented generation (RAG), in which new documents are inserted into a knowledge base and retrieved during prediction for downstream tasks. Most prior work on RAG has focused on improving model behavior during *prediction* through better retrieval or reasoning. This paper introduces ERASE, which instead improves model behavior **when new documents are acquired**, by incrementally deleting or rewriting other entries in the knowledge base each time a document is added. In two new datasets evaluating models’ ability to answer questions about a stream of news articles or conversations, ERASE improves accuracy relative to conventional retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B) absolute. This improvement is complementary to improved retrieval or reasoning for RAG: we demonstrate an 11% improvement by applying ERASE to SelfRAG.</abstract>
      <url hash="a17bba86">2025.findings-naacl.168</url>
      <bibkey>li-etal-2025-language</bibkey>
    </paper>
    <paper id="169">
      <title>Beyond Excess and Deficiency: Adaptive Length Bias Mitigation in Reward Models for <fixed-case>RLHF</fixed-case></title>
      <author><first>Yuyan</first><last>Bu</last></author>
      <author><first>Liangyu</first><last>Huo</last></author>
      <author><first>Yi</first><last>Jing</last><affiliation>duxiaoman</affiliation></author>
      <author><first>Qing</first><last>Yang</last></author>
      <pages>3091-3098</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models (LLMs) with human values. However, it has been noted that reward models in RLHF often exhibit unintended biases, such as an overemphasis on response length based on the erroneous assumption that longer responses are universally preferred. This “length bias” can lead to excessively verbose responses that compromise the quality of LLMs alignment. Previous efforts to mitigate length bias in reward models have inadvertently decreased their accuracy by neglecting the legitimate influence of response length on human preferences. In this work, we argue that response length is a context-specific factor in human evaluations, with different queries naturally eliciting varying preferences for response length. We propose an adaptive approach to modeling length preference that dynamically adjusts the influence of response length in reward evaluations according to the context of the query. Experimental results demonstrate that our adaptive approach effectively balances the mitigation of undesired length hacking and alignment accuracy, reducing unnecessary verbosity while improving overall response quality.</abstract>
      <url hash="5a9f523e">2025.findings-naacl.169</url>
      <bibkey>bu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="170">
      <title>Neuroplasticity and Corruption in Model Mechanisms: A Case Study Of Indirect Object Identification</title>
      <author><first>Vishnu Kabir</first><last>Chhabra</last></author>
      <author><first>Ding</first><last>Zhu</last></author>
      <author><first>Mohammad Mahdi</first><last>Khalili</last><affiliation>Ohio State University, Columbus and Yahoo! Research</affiliation></author>
      <pages>3099-3122</pages>
      <abstract>Previous research has shown that fine-tuning language models on general tasks enhance their underlying mechanisms. However, the impact of fine-tuning on poisoned data and the resulting changes in these mechanisms are poorly understood. This study investigates the changes in a model’s mechanisms during toxic fine-tuning and identifies the primary corruption mechanisms. We also analyze the changes after retraining a corrupted model on the original dataset and observe neuroplasticity behaviors, where the model relearns original mechanisms after fine-tuning the corrupted model. Our findings indicate that; (i) Underlying mechanisms are amplified across task-specific fine-tuning which can be generalized to longer epochs, (ii) Model corruption via toxic fine-tuning is localized to specific circuit components, (iii) Models exhibit neuroplasticity when retraining corrupted models on clean dataset, reforming the original model mechanisms.</abstract>
      <url hash="04193f10">2025.findings-naacl.170</url>
      <bibkey>chhabra-etal-2025-neuroplasticity</bibkey>
    </paper>
    <paper id="171">
      <title><fixed-case>VANE</fixed-case>-Bench: Video Anomaly Evaluation Benchmark for Conversational <fixed-case>LMM</fixed-case>s</title>
      <author><first>Hanan</first><last>Gani</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Rohit</first><last>Bharadwaj</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Muzammal</first><last>Naseer</last><affiliation>Khalifa University of Science, Technology and Research</affiliation></author>
      <author><first>Fahad Shahbaz</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Linköping University</affiliation></author>
      <author><first>Salman</first><last>Khan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Australian National University</affiliation></author>
      <pages>3123-3140</pages>
      <abstract>The recent advancements in Large Language Models (LLMs) have greatly influenced the development of Large Multi-modal Video Models (Video-LMMs), significantly enhancing our ability to interpret and analyze video data. Despite their impressive capabilities, current Video-LMMs have not been evaluated for anomaly detection tasks, which is critical to their deployment in practical scenarios e.g., towards identifying deepfakes, manipulated video content, traffic accidents and crimes. In this paper, we introduce VANE-Bench, a benchmark designed to assess the proficiency of Video-LMMs in detecting and localizing anomalies and inconsistencies in videos. Our dataset comprises an array of videos synthetically generated using existing state-of-the-art text-to-video generation models, encompassing a variety of subtle anomalies and inconsistencies grouped into five categories: unnatural transformations, unnatural appearance, pass-through, disappearance and sudden appearance. Additionally, our benchmark features real-world samples from existing anomaly detection datasets, focusing on crime-related irregularities, atypical pedestrian behavior, and unusual events. The task is structured as a visual question-answering challenge to gauge the models’ ability to accurately detect and localize the anomalies within the videos. We evaluate nine existing Video-LMMs, both open and closed sources, on this benchmarking task and find that most of the models encounter difficulties in effectively identifying the subtle anomalies. In conclusion, our research offers significant insights into the current capabilities of Video-LMMs in the realm of anomaly detection, highlighting the importance of our work in evaluating and improving these models for real-world applications. Our code and data is publicly available at https://github.com/rohit901/VANE-Bench.</abstract>
      <url hash="e52719c5">2025.findings-naacl.171</url>
      <bibkey>gani-etal-2025-vane</bibkey>
    </paper>
    <paper id="172">
      <title>Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models</title>
      <author><first>Jiachen</first><last>Ma</last></author>
      <author><first>Yijiang</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Xiao</last><affiliation>Yale University and Zhejiang University</affiliation></author>
      <author><first>Anda</first><last>Cao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Chao</first><last>Ye</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3141-3157</pages>
      <abstract>Text-to-image (T2I) models can be maliciously used to generate harmful content such as sexually explicit, unfaithful, and misleading or Not-Safe-for-Work (NSFW) images. Previous attacks largely depend on the availability of the diffusion model or involve a lengthy optimization process. In this work, we investigate a more practical and universal attack that does not require the presence of a target model and demonstrate that the high-dimensional text embedding space inherently contains NSFW concepts that can be exploited to generate harmful images. We present the Jailbreaking Prompt Attack (JPA). JPA first searches for the target malicious concepts in the text embedding space using a group of antonyms generated by ChatGPT. Subsequently, a prefix prompt is optimized in the discrete vocabulary space to align malicious concepts semantically in the text embedding space.We further introduce a soft assignment with gradient masking technique that allows us to perform gradient ascent in the discrete vocabulary space.We perform extensive experiments with open-sourced T2I models, e.g. stable-diffusion-v1-4 and closed-sourced online services, e.g. DALL·E 2 and Midjourney with black-box safety checkers. Results show that (1) JPA bypasses both text and image safety checkers, (2) while preserving high semantic alignment with the target prompt. (3) JPA demonstrates a much faster speed than previous methods and can be executed in a fully automated manner. These merits render it a valuable tool for robustness evaluation in future text-to-image generation research.</abstract>
      <url hash="4276ca93">2025.findings-naacl.172</url>
      <bibkey>ma-etal-2025-jailbreaking</bibkey>
    </paper>
    <paper id="173">
      <title><fixed-case>E</fixed-case>mo3<fixed-case>D</fixed-case>: Metric and Benchmarking Dataset for 3<fixed-case>D</fixed-case> Facial Expression Generation from Emotion Description</title>
      <author><first>Mahshid</first><last>Dehghani</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Amirahmad</first><last>Shafiee</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Ali</first><last>Shafiei</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Neda</first><last>Fallah</last></author>
      <author><first>Farahmand</first><last>Alizadeh</last></author>
      <author><first>Mohammad Mehdi</first><last>Gholinejad</last></author>
      <author><first>Hamid</first><last>Behroozi</last></author>
      <author><first>Jafar</first><last>Habibi</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Ehsaneddin</first><last>Asgari</last><affiliation>Qatar Computing Research Institute and University of California, Berkeley</affiliation></author>
      <pages>3158-3172</pages>
      <abstract>3D facial emotion modeling has important applications in areas such as animation design, virtual reality, and emotional human-computer interaction (HCI). However, existing models are constrained by limited emotion classes and insufficient datasets. To address this, we introduce Emo3D, an extensive “Text-Image-Expression dataset” that spans a wide spectrum of human emotions, each paired with images and 3D blendshapes. Leveraging Large Language Models (LLMs), we generate a diverse array of textual descriptions, enabling the capture of a broad range of emotional expressions. Using this unique dataset, we perform a comprehensive evaluation of fine-tuned language-based models and vision-language models, such as Contrastive Language-Image Pretraining (CLIP), for 3D facial expression synthesis. To better assess conveyed emotions, we introduce Emo3D metric, a new evaluation metric that aligns more closely with human perception than traditional Mean Squared Error (MSE). Unlike MSE, which focuses on numerical differences, Emo3D captures emotional nuances in visual-text alignment and semantic richness. Emo3D dataset and metric hold great potential for advancing applications in animation and virtual reality.</abstract>
      <url hash="6b20eaf0">2025.findings-naacl.173</url>
      <bibkey>dehghani-etal-2025-emo3d</bibkey>
    </paper>
    <paper id="174">
      <title>Task-wrapped Continual Learning in Task-Oriented Dialogue Systems</title>
      <author><first>Min</first><last>Zeng</last></author>
      <author><first>Haiqin</first><last>Yang</last><affiliation>International Digital Economy Academy (IDEA)</affiliation></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Yike</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology and Imperial College London</affiliation></author>
      <pages>3173-3183</pages>
      <abstract>Continual learning is vital for task-oriented dialogue systems (ToDs), and AdapterCL, equipped with residual adapters, has proven effectiveness in this domain. However, its performance is limited by training separate adapters for each task, preventing global knowledge sharing. To address this, we propose **Task-wrapped Continual Learning (TCL)**, a novel framework that employs **Task-Wrapped Adapters (TWAs)**, to simultaneously learn both global and task-specific information through parameter sharing. TCL leverages task-conditioned hypernetworks to transfer global knowledge across tasks, enabling TWAs to start from more informed initialization, efficiently learning task-specific details while reducing model parameters. Additionally, the simple, linear structure of both hypernetworks and TWAs ensure stable training, with task-free inference supported through effective loss utilization. Across 37 ToD domains, TCL consistently outperforms AdapterCL, significantly reducing forgetting. Remarkably, by setting the task embedding dimension to 1, TCL achieves a 4.76% improvement over AdapterCL while using only 46% of the parameters. These findings position TWA as a lightweight, powerful alternative to traditional adapters, offering a promising solution for continual learning in ToDs. The code is availableat https://github.com/cloversjtu/TCL.</abstract>
      <url hash="30e97a46">2025.findings-naacl.174</url>
      <bibkey>zeng-etal-2025-task</bibkey>
    </paper>
    <paper id="175">
      <title>Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains</title>
      <author><first>Katerina</first><last>Korre</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Federico</first><last>Ruggeri</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last><affiliation>Università di Bologna</affiliation></author>
      <pages>3184-3198</pages>
      <abstract>Hate speech relies heavily on cultural influences, leading to varying individual interpretations. For that reason, we propose a Semantic Componential Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate speech definitions. We create the first dataset of hate speech definitions encompassing 493 definitions from more than 100 cultures, drawn from five key domains: online dictionaries, academic research, Wikipedia, legal texts, and online platforms. By decomposing these definitions into semantic components,our analysis reveals significant variation across definitions, yet many domains borrow definitions from one another without taking into account the target culture. We conduct zero-shot model experiments using our proposed dataset, employing three popular open-sourced LLMs to understand the impact of different definitions on hate speech detection. Our findings indicate that LLMs are sensitive to definitions: responses for hate speech detection change according to the complexity of definitions used in the prompt.</abstract>
      <url hash="b3d13510">2025.findings-naacl.175</url>
      <bibkey>korre-etal-2025-untangling</bibkey>
    </paper>
    <paper id="176">
      <title><fixed-case>C</fixed-case>ode<fixed-case>RAG</fixed-case>-Bench: Can Retrieval Augment Code Generation?</title>
      <author><first>Zora Zhiruo</first><last>Wang</last></author>
      <author><first>Akari</first><last>Asai</last><affiliation>Paul G. Allen School of Computer Science &amp; Engineering, University of Washington</affiliation></author>
      <author><first>Xinyan Velocity</first><last>Yu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Frank F.</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yiqing</first><last>Xie</last></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Meta AI and Carnegie Mellon University</affiliation></author>
      <pages>3199-3214</pages>
      <abstract>While language models (LMs) excel at generating code, many programs are difficult to generate using only parametric knowledge. Despite the success of retrieval-augmented generation (RAG) in text-centric tasks, its potential for code generation remains under-explored. This work introduces CodeRAG-bench, a holistic retrieval-augmented code generation benchmark covering tasks like basic programming, open-domain, and repository-level problems and provides reproducible evaluations on both retrieval and end-to-end code generation performance. We further create a diverse, open datastore for code retrieval, aggregating sources such as competition solutions, tutorials, library documentation, StackOverflow posts, and GitHub repositories. Based on CodeRAG-bench, we conduct large-scale evaluations of 10 retrievers and 10 LMs and systematically analyze when retrieval can benefit code generation models and identify remaining challenges. We find that while retrieving high-quality contexts improves code generation, retrievers often struggle to fetch useful contexts, and generators face limitations in using those contexts effectively. We hope CodeRAG-bench encourages further development in code-oriented RAG methods.</abstract>
      <url hash="1c333a0c">2025.findings-naacl.176</url>
      <bibkey>wang-etal-2025-coderag</bibkey>
    </paper>
    <paper id="177">
      <title>Multi-Condition Guided Diffusion Network for Multimodal Emotion Recognition in Conversation</title>
      <author><first>Wenjin</first><last>Tian</last></author>
      <author><first>Xianying</first><last>Huang</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>Shihao</first><last>Zou</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>3215-3227</pages>
      <abstract>Emotion recognition in conversation (ERC) involves identifying emotional labels associated with utterances within a conversation, a task that is essential for developing empathetic robots. Current research emphasizes contextual factors, the speaker’s influence, and extracting complementary information across different modalities. However, it often overlooks the cross-modal noise at the semantic level and the redundant information brought by the features themselves. This study introduces a diffusion-based approach designed to effectively address the challenges posed by redundant information and unexpected noise while robustly capturing shared semantics, thus facilitating the learning of compact and representative features from multimodal data. Specifically, we present the Multi-Condition Guided Diffusion Network (McDiff). McDiff employs a modal prior knowledge extraction strategy to derive the prior distribution for each modality, thereby enhancing the regional attention of each modality and applying the generated prior distribution at each diffusion step. Furthermore, we propose a method to learn the mutual information of each modality through a specific objective constraints approach prior to the forward process, which aims to improve inter-modal interaction and mitigate the effects of noise and redundancy. Comprehensive experiments conducted on two multimodal datasets, IEMOCAP and MELD, demonstrate that McDiff significantly surpasses existing state-of-the-art methodologies, thereby affirming the generalizability and efficacy of the proposed model.</abstract>
      <url hash="09c9afba">2025.findings-naacl.177</url>
      <bibkey>tian-etal-2025-multi</bibkey>
    </paper>
    <paper id="178">
      <title>Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Senses</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Ruochen</first><last>Zhang</last><affiliation>Brown University</affiliation></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Elisa</first><last>Gilbert</last><affiliation>Universität Leipzig</affiliation></author>
      <author><first>Hiroki</first><last>Nomoto</last><affiliation>Tokyo University of Foreign Studies</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>3228-3250</pages>
      <abstract>Multilingual large language models (LLMs) have gained prominence, but concerns arise regarding their reliability beyond English. This study addresses the gap in cross-lingual semantic evaluation by introducing a novel benchmark for cross-lingual sense disambiguation, StingrayBench. In this paper, we demonstrate using false friends—words that are orthographically similar but have completely different meanings in two languages— as a possible approach to pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We collect false friends in four language pairs, namely Indonesian-Malay, Indonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to distinguish the use of them in context. In our analysis of various models, we observe they tend to be biased toward higher-resource languages. We also propose new metrics for quantifying the cross-lingual sense bias and comprehension based on our benchmark. Our work contributes to developing more diverse and inclusive language modeling, promoting fairer access for the wider multilingual community.</abstract>
      <url hash="f8de59d1">2025.findings-naacl.178</url>
      <bibkey>cahyawijaya-etal-2025-thank</bibkey>
    </paper>
    <paper id="179">
      <title>Atoxia: Red-teaming Large Language Models with Target Toxic Answers</title>
      <author><first>Yuhao</first><last>Du</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Zhuo</first><last>Li</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Pengyu</first><last>Cheng</last><affiliation>Tencent</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>3251-3266</pages>
      <abstract>Despite the substantial advancements in artificial intelligence, large language models (LLMs) remain being challenged by generation safety. With adversarial jailbreaking prompts, one can effortlessly induce LLMs to output harmful content, causing unexpected negative social impacts. This vulnerability highlights the necessity for robust LLM red-teaming strategies to identify and mitigate such risks before large-scale application. To detect specific types of risks, we propose a novel red-teaming method that **A**ttacks LLMs with **T**arget **Toxi**c **A**nswers (**Atoxia**). Given a particular harmful answer, Atoxia generates a corresponding user query and a misleading answer opening to examine the internal defects of a given LLM. The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward. We verify the effectiveness of our method on various red-teaming benchmarks, such as AdvBench and HH-Harmless. The empirical results demonstrate that Atoxia can successfully detect safety risks in not only open-source models but also state-of-the-art black-box models such as GPT-4o.</abstract>
      <url hash="7764c4c1">2025.findings-naacl.179</url>
      <bibkey>du-etal-2025-atoxia</bibkey>
    </paper>
    <paper id="180">
      <title>A Practical Method for Generating String Counterfactuals</title>
      <author><first>Matan</first><last>Avitan</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar-Ilan University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Shauli</first><last>Ravfogel</last><affiliation>New York University</affiliation></author>
      <pages>3267-3286</pages>
      <abstract>Interventions targeting the representation space of language models (LMs) have emerged as an effective means to influence model behavior. Such methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model’s representations and, in so doing, create a counterfactual representation. However, because the intervention operates within the representation space, understanding precisely what aspects of the text it modifies poses a challenge. In this paper, we give a method to convert representation counterfactuals into string counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation space intervention and to interpret the features utilized to encode a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification through data augmentation.</abstract>
      <url hash="1a722468">2025.findings-naacl.180</url>
      <bibkey>avitan-etal-2025-practical</bibkey>
    </paper>
    <paper id="181">
      <title>Probing-<fixed-case>RAG</fixed-case>: Self-Probing to Guide Language Models in Selective Document Retrieval</title>
      <author><first>Ingeol</first><last>Baek</last></author>
      <author><first>Hwan</first><last>Chang</last></author>
      <author><first>ByeongJeong</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Jimin</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>3287-3304</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and-generate processes may not be optimized for real-world scenarios, where queries might require multiple retrieval steps or none at all. In this paper, we propose a Probing-RAG, which utilizes the hidden state representations from the intermediate layers of language models to adaptively determine the necessity of additional retrievals for a given query. By employing a pre-trained prober, Probing-RAG effectively captures the model’s internal cognition, enabling reliable decision-making about retrieving external documents. Experimental results across five open-domain QA datasets demonstrate that Probing-RAG outperforms previous methods while reducing the number of redundant retrieval steps.</abstract>
      <url hash="705b45c4">2025.findings-naacl.181</url>
      <bibkey>baek-etal-2025-probing</bibkey>
    </paper>
    <paper id="182">
      <title>Extracting Military Event Temporal Relations via Relative Event Time Prediction and Virtual Adversarial Training</title>
      <author><first>Jie</first><last>Gong</last></author>
      <author><first>Qiwang</first><last>Hu</last></author>
      <pages>3305-3317</pages>
      <abstract>Extracting temporal relationships between events in the text is crucial for understanding how events unfold over time, especially in the information-dense and precision-demanding military field. Existing models for extracting event temporal relations typically compare the relative times of events directly, neglecting the contextual information between event pairs. This can lead to difficulties in handling uncertain temporal boundaries expressed in text. In this paper, we propose an event temporal relationship extraction model for the military field, based on relative event time prediction and virtual adversarial training, MFRV. The relative event time prediction as an auxiliary task enhances the model’s ability to capture and infer temporal relationships. Virtual adversarial training increases the model’s generalization by generating adversarial samples. Additionally, we adopt the MoCo (Multi-objective gradient correction) method to balance the losses from relative event time prediction and virtual adversarial training, effectively resolving the gradient bias issue in multi-objective optimization. Furthermore, we have constructed a new dataset, TRMF, specifically for event temporal relationship extraction in the military field. Experiments conducted on TRMF, as well as widely used public datasets MATRES and TCR, demonstrate the effectiveness of MFRV.</abstract>
      <url hash="d786588b">2025.findings-naacl.182</url>
      <bibkey>gong-hu-2025-extracting</bibkey>
    </paper>
    <paper id="183">
      <title>Unlocking the Planning Capabilities of Large Language Models with Maximum Diversity Fine-tuning</title>
      <author><first>Wenjun</first><last>Li</last></author>
      <author><first>Changyu</first><last>Chen</last></author>
      <author><first>Pradeep</first><last>Varakantham</last></author>
      <pages>3318-3340</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive task-solving capabilities through prompting techniques and system designs, including solving planning tasks (e.g., math proofs, basic travel planning) when sufficient data is available online and used during pre-training. However, for planning tasks with limited prior data (e.g., blocks world, advanced travel planning), the performance of LLMs, including proprietary models like GPT and Gemini, is poor. This paper investigates the impact of fine-tuning on the planning capabilities of LLMs, revealing that LLMs can achieve strong performance in planning through substantial (tens of thousands of specific examples) fine-tuning. Yet, this process incurs high economic, time, and computational costs for each planning problem variation. To address this, we propose Clustering-Based Maximum Diversity Sampling (CMDS), which selects diverse and representative data to enhance sample efficiency and the model’s generalization capability. Extensive evaluations demonstrate that CMDS-<tex-math>l</tex-math>, a baseline method combining CMDS with language embeddings, outperforms random sampling. Furthermore, we introduce a novel algorithm, CMDS-<tex-math>g</tex-math>, which encodes planning task instances with their graph representations into the embedding space. Empirical results show that CMDS-<tex-math>g</tex-math> consistently outperforms baseline methods across various scales and multiple benchmark domains.</abstract>
      <url hash="e880791b">2025.findings-naacl.183</url>
      <bibkey>li-etal-2025-unlocking</bibkey>
    </paper>
    <paper id="184">
      <title>Continuous Speech Tokenizer in Text To Speech</title>
      <author><first>Yixing</first><last>Li</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xingwu</first><last>Sun</last><affiliation>Tencent AI Platform</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhanhui</first><last>Kang</last></author>
      <pages>3341-3347</pages>
      <abstract>The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer.</abstract>
      <url hash="f439dcd4">2025.findings-naacl.184</url>
      <bibkey>li-etal-2025-continuous</bibkey>
    </paper>
    <paper id="185">
      <title>Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media</title>
      <author><first>Owen</first><last>Cook</last></author>
      <author><first>Charlie</first><last>Grimshaw</last></author>
      <author><first>Ben Peng</first><last>Wu</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Sophie</first><last>Dillon</last></author>
      <author><first>Jack</first><last>Hicks</last></author>
      <author><first>Luke</first><last>Jones</last></author>
      <author><first>Thomas</first><last>Smith</last></author>
      <author><first>Matyas</first><last>Szert</last></author>
      <author><first>Xingyi</first><last>Song</last><affiliation>University of Sheffield</affiliation></author>
      <pages>3348-3358</pages>
      <abstract>Misinformation spreads rapidly on social media, confusing the truth and targeting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X’s community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.</abstract>
      <url hash="0e279bf1">2025.findings-naacl.185</url>
      <bibkey>cook-etal-2025-efficient</bibkey>
    </paper>
    <paper id="186">
      <title>Challenges in Trustworthy Human Evaluation of Chatbots</title>
      <author><first>Wenting</first><last>Zhao</last><affiliation>Cornell University</affiliation></author>
      <author><first>Alexander M</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>Tanya</first><last>Goyal</last><affiliation>Cornell University</affiliation></author>
      <pages>3359-3365</pages>
      <abstract>Recently, open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained reputation as trustworthy publicly available benchmarks for LLM performance. While gold standard, it is often tricky to implement the required guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that different source of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high quality human annotations.</abstract>
      <url hash="b957d8bf">2025.findings-naacl.186</url>
      <bibkey>zhao-etal-2025-challenges</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>RATSD</fixed-case>: Retrieval Augmented Truthfulness Stance Detection from Social Media Posts Toward Factual Claims</title>
      <author><first>Zhengyuan</first><last>Zhu</last></author>
      <author><first>Zeyu</first><last>Zhang</last></author>
      <author><first>Haiqi</first><last>Zhang</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>3366-3381</pages>
      <abstract>Social media provides a valuable lens for assessing public perceptions and opinions. This paper focuses on the concept of truthfulness stance, which evaluates whether a textual utterance affirms, disputes, or remains neutral or indifferent toward a factual claim. Our systematic analysis fills a gap in the existing literature by offering the first in-depth conceptual framework encompassing various definitions of stance. We introduce RATSD (Retrieval Augmented Truthfulness Stance Detection), a novel method that leverages large language models (LLMs) with retrieval-augmented generation (RAG) to enhance the contextual understanding of tweets in relation to claims. RATSD is evaluated on TSD-CT, our newly developed dataset containing 3,105 claim-tweet pairs, along with existing benchmark datasets. Our experiment results demonstrate that RATSD outperforms state-of-the-art methods, achieving a significant increase in Macro-F1 score on TSD-CT. Our contributions establish a foundation for advancing research in misinformation analysis and provide valuable tools for understanding public perceptions in digital discourse.</abstract>
      <url hash="c0f1b7ef">2025.findings-naacl.187</url>
      <bibkey>zhu-etal-2025-ratsd</bibkey>
    </paper>
    <paper id="188">
      <title><fixed-case>FACT</fixed-case>: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</title>
      <author><first>Jinlin</first><last>Wang</last></author>
      <author><first>Suyuchen</first><last>Wang</last><affiliation>Université de Montréal and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Ziwen</first><last>Xia</last><affiliation>Deepwisdom</affiliation></author>
      <author><first>Sirui</first><last>Hong</last><affiliation>DeepWisdom</affiliation></author>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Chenglin</first><last>Wu</last><affiliation>DeepWisdom</affiliation></author>
      <pages>3382-3392</pages>
      <abstract>Large Language Models (LLMs) are proficient at retrieving single facts from extended contexts, yet they struggle with tasks requiring the simultaneous retrieval of multiple facts, especially during generation. This paper identifies a novel “lost-in-the-middle” phenomenon, where LLMs progressively lose track of critical information throughout the generation process, resulting in incomplete or inaccurate retrieval. To address this challenge, we introduce Find All Crucial Texts (FACT), an iterative retrieval method that refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval. Experiments demonstrate that FACT substantially enhances multi-fact retrieval performance across various tasks, though improvements are less notable in general-purpose QA scenarios. Our findings shed light on the limitations of LLMs in multi-fact retrieval and underscore the need for more resilient long-context retrieval strategies.</abstract>
      <url hash="77c578d7">2025.findings-naacl.188</url>
      <bibkey>wang-etal-2025-fact</bibkey>
    </paper>
    <paper id="189">
      <title>Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding</title>
      <author><first>Xingjian</first><last>Diao</last></author>
      <author><first>Chunhui</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Weiyi</first><last>Wu</last></author>
      <author><first>Zhongyu</first><last>Ouyang</last></author>
      <author><first>Peijun</first><last>Qing</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Ming</first><last>Cheng</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Jiang</first><last>Gui</last><affiliation>Dartmouth College</affiliation></author>
      <pages>3393-3409</pages>
      <abstract>Multimodal foundation models (MFMs) have demonstrated significant success in tasks such as visual captioning, question answering, and image-text retrieval. However, these models face inherent limitations due to their finite internal capacity, which restricts their ability to process extended temporal sequences—an essential requirement for comprehensive video and audio analysis. To overcome these challenges, we introduce a specialized cognitive module, temporal working memory (TWM), which aims to enhance the temporal modeling capabilities of MFMs. It selectively retains task-relevant information across temporal dimensions, ensuring that critical details are preserved throughout the processing of video and audio content. The TWM uses a query-guided attention approach to focus on the most informative multimodal segments within temporal sequences. By retaining only the most relevant content, TWM optimizes the use of the model’s limited capacity, enhancing its temporal modeling ability. This plug-and-play module can be easily integrated into existing MFMs. With our TWM, nine state-of-the-art models exhibit significant performance improvements across tasks such as video captioning, question answering, and video-text retrieval. By enhancing temporal modeling, TWM extends the capability of MFMs to handle complex, time-sensitive data effectively. Our code is available at https://github.com/xid32/NAACL_2025_TWM.</abstract>
      <url hash="843d8c72">2025.findings-naacl.189</url>
      <bibkey>diao-etal-2025-temporal</bibkey>
    </paper>
    <paper id="190">
      <title>Investigating the Transferability of Code Repair for Low-Resource Programming Languages</title>
      <author><first>Kyle</first><last>Wong</last></author>
      <author><first>Alfonso</first><last>Amayuelas</last></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>3410-3432</pages>
      <abstract>Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent use case is iterative code repair, where an LLM fixes an incorrect program by rationalizing about errors and generating new code. Recent works augment the code repair process by integrating modern techniques such as chain-of-thought reasoning or distillation, but only study their benefits on high-resource languages like Python, and ignore low-resource languages like Perl. To address this gap of knowledge, we investigate the benefits of distilling code repair for both high and low resource languages to determine if the techniques that are effective in a high resource setting are also applicable in a low resource setting. Our evaluation shows that distilling the ability to repair code has language dependent benefits. To explain this behavior, we perform a further analysis and find that contrary to preexisting beliefs, the correlation between reasoning ability and code correction ability is weak. We hypothesize this weak correlation is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair.</abstract>
      <url hash="ca88990f">2025.findings-naacl.190</url>
      <bibkey>wong-etal-2025-investigating</bibkey>
    </paper>
    <paper id="191">
      <title>Multilingual Blending: Large Language Model Safety Alignment Evaluation with Language Mixture</title>
      <author><first>Jiayang</first><last>Song</last></author>
      <author><first>Yuheng</first><last>Huang</last></author>
      <author><first>Zhehua</first><last>Zhou</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <pages>3433-3449</pages>
      <abstract>As safety remains a crucial concern throughout the development lifecycle of Large Language Models (LLMs), researchers and industrial practitioners have increasingly focused on safeguarding and aligning LLM behaviors with human preferences and ethical standards. LLMs, trained on extensive multilingual corpora, exhibit powerful generalization abilities across diverse languages and domains. However, current safety alignment practices predominantly focus on single-language scenarios, which leaves their effectiveness in complex multilingual contexts, especially for those complex mixed-language formats, largely unexplored. In this study, we introduce Multilingual Blending, a mixed-language query-response scheme designed to evaluate the safety alignment of various state-of-the-art LLMs (e.g., GPT-4o, GPT 3.5, Llama3) under sophisticated, multilingual conditions. We further investigate language patterns such as language availability, morphology, and language family that could impact the effectiveness of Multilingual Blending in compromising the safeguards of LLMs. Our experimental results show that, without meticulously crafted prompt templates, Multilingual Blending significantly amplifies the detriment of malicious queries, leading to dramatically increased bypass rates in LLM safety alignment (67.23% on GPT-3.5 and 40.34% on GPT-4o), far exceeding those of single-language baselines. Moreover, the performance of Multilingual Blending varies notably based on intrinsic linguistic properties, with languages of different morphology and from diverse families being more prone to evading safety alignments. These findings underscore the necessity of evaluating LLMs and developing corresponding safety alignment strategies in a complex, multilingual context to align with their superior cross-language generalization capabilities.</abstract>
      <url hash="fdb3ee28">2025.findings-naacl.191</url>
      <bibkey>song-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="192">
      <title>Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting</title>
      <author><first>Jiarui</first><last>Wu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Zhuo</first><last>Liu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <pages>3450-3468</pages>
      <abstract>Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations.</abstract>
      <url hash="f0d2ad22">2025.findings-naacl.192</url>
      <bibkey>wu-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="193">
      <title>Concise and Organized Perception Facilitates Reasoning in Large Language Models</title>
      <author><first>Junjie</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Shaotian</first><last>Yan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Shen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhengdong</first><last>Xiao</last></author>
      <author><first>Liang</first><last>Xie</last></author>
      <author><first>Wenxiao</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jieping</first><last>Ye</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3469-3498</pages>
      <abstract>Exploiting large language models (LLMs) to tackle reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex logical problems, characterized by plenty of premises within the context and requiring multi-hop reasoning. In particular, the reasoning capabilities of LLMs are brittle to disorder and distractibility. In this work, we first examine the mechanism from the perspective of information flow and reveal that LLMs confront difficulties akin to human-like cognitive biases when dealing with disordered and irrelevant content in reasoning tasks. However, in contrast to LLMs, disordered and irrelevant content does not significantly decrease human performance, as humans have a propensity to distill the most relevant information and systematically organize their thoughts, aiding them in responding to questions.Stem from that, we further propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to identify the most pertinent information while eliminating redundancy efficiently. It then prompts the LLMs in a more organized form that adapts to the model’s inference process. By perceiving concise and organized context, the reasoning abilities of LLMs can be better elicited. Extensive experimental results on several popular logical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark (DI-GSM) show that COP significantly outperforms previous state-of-the-art methods.</abstract>
      <url hash="8e4a9d9c">2025.findings-naacl.193</url>
      <bibkey>liu-etal-2025-concise</bibkey>
    </paper>
    <paper id="194">
      <title>Verifiable Format Control for Large Language Model Generations</title>
      <author><first>Zhaoyang</first><last>Wang</last></author>
      <author><first>Jinqi</first><last>Jiang</last></author>
      <author><first>Huichi</first><last>Zhou</last></author>
      <author><first>Wenhao</first><last>Zheng</last></author>
      <author><first>Xuchao</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chetan</first><last>Bansal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>3499-3513</pages>
      <abstract>Recent Large Language Models (LLMs) have demonstrated satisfying general instruction following ability. However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications. Most existing methods focus on benchmarking general instruction following while overlook how to improve the specific format following ability for small LLMs. Besides, these methods often rely on evaluations based on advanced LLMs (e.g., GPT-4), which can introduce the intrinsic bias of LLMs and be costly due to the API calls. In this paper, we first curate a fully verifiable format following dataset VFF. In contrast to existing works often adopting external LLMs for instruction-following validations, every sample of VFF can be easily validated with a Python function. Further, we propose to leverage this verifiable feature to synthesize massive data for progressively training small LLMs, in order to improve their format following abilities. Experimental results highlight the prevalent limitations in the format following capabilities of 7B level open-source LLMs and demonstrate the effectiveness of our method in enhancing this essential ability.</abstract>
      <url hash="92af4c89">2025.findings-naacl.194</url>
      <bibkey>wang-etal-2025-verifiable</bibkey>
    </paper>
    <paper id="195">
      <title>Taxonomy and Analysis of Sensitive User Queries in Generative <fixed-case>AI</fixed-case> Search System</title>
      <author><first>Hwiyeol</first><last>Jo</last><affiliation>Search US, NAVER</affiliation></author>
      <author><first>Taiwoo</first><last>Park</last><affiliation>NAVER Search US</affiliation></author>
      <author><first>Hyunwoo</first><last>Lee</last><affiliation>NAVER</affiliation></author>
      <author><first>Nayoung</first><last>Choi</last><affiliation>Emory University</affiliation></author>
      <author><first>Changbong</first><last>Kim</last></author>
      <author><first>Ohjoon</first><last>Kwon</last><affiliation>NAVER</affiliation></author>
      <author><first>Donghyeon</first><last>Jeon</last><affiliation>NAVER</affiliation></author>
      <author><first>Eui Hyeon</first><last>Lee</last></author>
      <author><first>Kyoungho</first><last>Shin</last><affiliation>NAVER</affiliation></author>
      <author><first>Lim Sun</first><last>Suk</last></author>
      <author><first>Kyungmi</first><last>Kim</last><affiliation>NAVER</affiliation></author>
      <author><first>Lee</first><last>Jihye</last></author>
      <author><first>Sun</first><last>Kim</last><affiliation>Naver</affiliation></author>
      <pages>3514-3529</pages>
      <abstract>Although there has been a growing interest among industries in integrating generative LLMs into their services, limited experience and scarcity of resources act as a barrier in launching and servicing large-scale LLM-based services. In this paper, we share our experiences in developing and operating generative AI models within a national-scale search engine, with a specific focus on the sensitiveness of user queries. We propose a taxonomy for sensitive search queries, outline our approaches, and present a comprehensive analysis report on sensitive queries from actual users. We believe that our experiences in launching generative AI search systems can contribute to reducing the barrier in building generative LLM-based services.</abstract>
      <url hash="0f389f18">2025.findings-naacl.195</url>
      <bibkey>jo-etal-2025-taxonomy</bibkey>
    </paper>
    <paper id="196">
      <title><fixed-case>S</fixed-case>yn<fixed-case>G</fixed-case>host: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer</title>
      <author><first>Pengzhou</first><last>Cheng</last></author>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Zongru</first><last>Wu</last></author>
      <author><first>Fengwei</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Libo</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3530-3546</pages>
      <url hash="c35643e3">2025.findings-naacl.196</url>
      <bibkey>cheng-etal-2025-synghost</bibkey>
    </paper>
    <paper id="197">
      <title><fixed-case>TESTEVAL</fixed-case>: Benchmarking Large Language Models for Test Case Generation</title>
      <author><first>Wenhan</first><last>Wang</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Chenyuan</first><last>Yang</last></author>
      <author><first>Zhijie</first><last>Wang</last></author>
      <author><first>Yuheng</first><last>Huang</last></author>
      <author><first>Zhaoyang</first><last>Chu</last></author>
      <author><first>Da</first><last>Song</last></author>
      <author><first>Lingming</first><last>Zhang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>An Ran</first><last>Chen</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <pages>3547-3562</pages>
      <abstract>For program languages, testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform software testing, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in software testing. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities.In this paper, we propose TestEval, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate 17 popular LLMs, including both commercial and open-source ones, on TestEval. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths.</abstract>
      <url hash="2e9a414a">2025.findings-naacl.197</url>
      <bibkey>wang-etal-2025-testeval</bibkey>
    </paper>
    <paper id="198">
      <title>Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models</title>
      <author><first>Siyin</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xingsong</first><last>Ye</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Junwen</first><last>Duan</last></author>
      <author><first>Shimin</first><last>Li</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>3563-3605</pages>
      <abstract>As Artificial General Intelligence (AGI) becomes increasingly integrated into various facets of human life, ensuring the safety and ethical alignment of such systems is paramount. Previous studies primarily focus on single-modality threats, which may not suffice given the integrated and complex nature of cross-modality interactions. We introduce a novel safety alignment challenge called Safe Inputs but Unsafe Output (*SIUO*) to evaluate cross-modality safety alignment. Specifically, it considers cases where single modalities are safe independently but could potentially lead to unsafe or unethical outputs when combined. To empirically investigate this problem, we developed the *SIUO*, a cross-modality benchmark encompassing 9 critical safety domains, such as self-harm, illegal activities, and privacy violations. Our findings reveal substantial safety vulnerabilities in both closed- and open-source LVLMs, such as GPT-4V and LLaVA, underscoring the inadequacy of current models to reliably interpret and respond to complex, real-world scenarios.</abstract>
      <url hash="d24530a3">2025.findings-naacl.198</url>
      <bibkey>wang-etal-2025-safe</bibkey>
    </paper>
    <paper id="199">
      <title><fixed-case>FLEX</fixed-case>: A Benchmark for Evaluating Robustness of Fairness in Large Language Models</title>
      <author><first>Dahyun</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>3606-3620</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have significantly enhanced interactions between users and models. These advancements concurrently underscore the need for rigorous safety evaluations due to the manifestation of social biases, which can lead to harmful societal impacts. Despite these concerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs, which can generate biased responses even with simple adversarial instructions. To address this critical gap, we introduce a new benchmark, Fairness Benchmark in LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias. To thoroughly evaluate the robustness of LLMs, we integrate prompts that amplify potential biases into the fairness assessment. Comparative experiments between FLEX and existing benchmarks demonstrate that traditional evaluations may underestimate the inherent risks in models. This highlights the need for more stringent LLM evaluation benchmarks to guarantee safety and fairness.</abstract>
      <url hash="d8d61ad5">2025.findings-naacl.199</url>
      <bibkey>jung-etal-2025-flex</bibkey>
    </paper>
    <paper id="200">
      <title>When and How to Augment Your Input: Question Routing Helps Balance the Accuracy and Efficiency of Large Language Models</title>
      <author><first>Shufan</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>He</first><last>Zheng</last></author>
      <author><first>Lei</first><last>Cui</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>3621-3634</pages>
      <abstract>Although large language models rely on parametric knowledge to achieve exceptional performance across various question-answering tasks, they still face challenges when addressing knowledge-based long-tail questions. Augmented generation techniques, such as chain-of-thought prompting and retrieval augmentation, can effectively enhance the ability of these models to answer long-tail questions. However, improving accuracy through augmented generation often results in significant latency within question-answering systems. This paper addresses the issue of “when and how to augment the input” by proposing an adaptive question routing framework. This framework employs a query router to select the most appropriate augmentation path at the right time, thereby enhancing both the accuracy and efficiency of question-answering systems. Extensive comparative experiments on benchmarks such as AmbigNQ, HotpotQA, MMLU-STEM, and PopQA demonstrate that our method surpasses existing approaches in both accuracy and efficiency. Furthermore, this paper introduces two metrics for evaluating adaptive question augmentation methods and presents a new benchmark for adaptive question augmentation, aiming to advance the field.</abstract>
      <url hash="232a6407">2025.findings-naacl.200</url>
      <bibkey>chen-etal-2025-augment</bibkey>
    </paper>
    <paper id="201">
      <title><fixed-case>G</fixed-case>ra<fixed-case>PPI</fixed-case>: A Retrieve-Divide-Solve <fixed-case>G</fixed-case>raph<fixed-case>RAG</fixed-case> Framework for Large-scale Protein-protein Interaction Exploration</title>
      <author><first>Ziwen</first><last>Li</last></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Youngseung</first><last>Jeon</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>3635-3648</pages>
      <abstract>Drug discovery (DD) has tremendously contributed to maintaining and improving public health. Hypothesizing that inhibiting protein misfolding can slow disease progression, researchers focus on target identification (Target ID) to find protein structures for drug binding. While Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug discovery, integrating models into cohesive workflows remains challenging. We conducted a user study with drug discovery researchers to identify the applicability of LLMs and RAGs in Target ID. We identified two main findings: 1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on an initial protein and protein candidates that have a therapeutic impact; 2) the model must provide the PPI and relevant explanations for better understanding. Based on these observations, we identified three limitations on previous approaches for Target ID: 1) semantic ambiguity, 2) lack of explainability, and 3) short retrieval units. To address these issues, we propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve agent pipeline RAG framework to support large-scale PPI signaling pathway exploration in understanding therapeutic impacts by decomposing the analysis of entire PPI pathways into sub-tasks focused on the analysis of PPI edges.</abstract>
      <url hash="cd2a0f6e">2025.findings-naacl.201</url>
      <bibkey>li-etal-2025-grappi</bibkey>
    </paper>
    <paper id="202">
      <title>From Curiosity to Clarity : Exploring the Impact of Consecutive Why-Questions</title>
      <author><first>Geonyeong</first><last>Son</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Jaeyoung</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Misuk</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <pages>3649-3664</pages>
      <abstract>Humans attempt to understand the real world by asking the fundamental question ”Why?” when faced with incomprehensible situations in everyday life. Such why-questions provide essential knowledge that can help in understanding these situations. In this study, we conducted an end-to-end process to verify the utility of consecutive why-questions, from constructing a large language model (LLM)-based dataset to performing quantitative evaluation and analysis. Firstly, we created a WHY-Chain dataset, consisting of answers generated by an LLM in response to chain-of-why-questions, including a validity check. We also incorporated objectives that effectively capture the ”consecutive” characteristic of the data. Using the WHY-Chain dataset and two types of self-supervised objectives, we trained the pre-trained model. As a result, the refined model demonstrated improved performance on downstream tasks that require commonsense reasoning. Additionally, we conducted various ablation studies to assess the impact of different factors, confirming the scalability of the proposed approach. Lastly, we confirmed the consistency of the logical information by reasoning chain analysis of the answers generated from consecutive why-questions.</abstract>
      <url hash="011d64b9">2025.findings-naacl.202</url>
      <bibkey>son-etal-2025-curiosity</bibkey>
    </paper>
    <paper id="203">
      <title><fixed-case>C</fixed-case>ollab<fixed-case>S</fixed-case>tory: Multi-<fixed-case>LLM</fixed-case> Collaborative Story Generation and Authorship Analysis</title>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Amazon</affiliation></author>
      <author><first>Nafis Irtiza</first><last>Tripto</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>3665-3679</pages>
      <abstract>The rise of unifying frameworks that enable seamless interoperability of Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended tasks a possibility. Despite this, there have not been efforts to explore such collaborative writing. We take the next step beyond human-LLM collaboration to explore this multi-LLM scenario by generating the first exclusively LLM-generated collaborative stories dataset called CollabStory. We focus on single-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs co-author stories. We generate over 32k stories using open-source instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that have set the standard for human-human multi-author writing tasks and analysis. We extend their authorship-related tasks for multi-LLM settings and present baselines for LLM-LLM collaboration. We find that current baselines are not able to handle this emerging scenario. Thus, CollabStory is a resource that could help propel an understanding as well as the development of new techniques to discern the use of multiple LLMs. This is crucial to study in the context of writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing challenges related to plagiarism detection, credit assignment, maintaining academic integrity in educational settings, and addressing copyright infringement concerns. We make our dataset and code available at https://github.com/saranya-venkatraman/CollabStory.</abstract>
      <url hash="60217790">2025.findings-naacl.203</url>
      <bibkey>venkatraman-etal-2025-collabstory</bibkey>
    </paper>
    <paper id="204">
      <title><fixed-case>NTSEBENCH</fixed-case>: Cognitive Reasoning Benchmark for Vision Language Models</title>
      <author><first>Pranshu</first><last>Pandya</last></author>
      <author><first>Vatsal</first><last>Gupta</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <author><first>Agney S</first><last>Talwarr</last></author>
      <author><first>Tushar</first><last>Kataria</last><affiliation>University of Utah</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>3680-3708</pages>
      <abstract>Cognitive textual and visual reasoning tasks, including puzzles, series, and analogies, demand the ability to quickly reason, decipher, and evaluate patterns both textually and spatially. Due to extensive training on vast amounts of human-curated data, large language models (LLMs) and vision language models (VLMs) excel in common-sense reasoning tasks, but still struggle with more complex reasoning that demands deeper cognitive understanding. We introduce NTSEBENCH, a new dataset designed to evaluate cognitive multimodal reasoning and problem-solving skills of large models. The dataset contains 2,728 multiple-choice questions, accompanied by a total of 4,642 images, spanning 26 categories. These questions are drawn from the nationwide NTSE examination in India and feature a mix of visual and textual general aptitude challenges, designed to assess intelligence and critical thinking skills beyond mere rote learning. We establish baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison between open-source and propriety models, we propose four distinct modeling strategies to handle different modalities—text and images—in the dataset instances.</abstract>
      <url hash="ecc48f09">2025.findings-naacl.204</url>
      <bibkey>pandya-etal-2025-ntsebench</bibkey>
    </paper>
    <paper id="205">
      <title><fixed-case>K</fixed-case>now<fixed-case>A</fixed-case>gent: Knowledge-Augmented Planning for <fixed-case>LLM</fixed-case>-Based Agents</title>
      <author><first>Yuqi</first><last>Zhu</last></author>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Yixin</first><last>Ou</last></author>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Shiwei</first><last>Lyu</last></author>
      <author><first>Yue</first><last>Shen</last><affiliation>antgroup</affiliation></author>
      <author><first>Lei</first><last>Liang</last></author>
      <author><first>Jinjie</first><last>Gu</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3709-3732</pages>
      <abstract>Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.</abstract>
      <url hash="8da6a5a9">2025.findings-naacl.205</url>
      <bibkey>zhu-etal-2025-knowagent</bibkey>
    </paper>
    <paper id="206">
      <title><fixed-case>SWITCH</fixed-case>: Studying with Teacher for Knowledge Distillation of Large Language Models</title>
      <author><first>Jahyun</first><last>Koo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yerin</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yongil</first><last>Kim</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Taegwan</first><last>Kang</last></author>
      <author><first>Hyunkyung</first><last>Bae</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>3733-3746</pages>
      <abstract>Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model compression, with the use of student-generated outputs (SGOs) as training data being particularly notable for reducing the mismatch between training and inference. However, SGOs often produce noisy and biased sequences, which can lead to misguidance from the teacher model, especially in long sequences. To mitigate these challenges, we propose SWITCH (Studying With Teacher for Knowledge Distillation), a novel approach that strategically incorporates the teacher model during the student’s sequence generation. SWITCH identifies discrepancies between the token probabilities of the teacher and student models, allowing the teacher to intervene selectively, particularly in long sequences that are more prone to teacher misguidance. Extensive experimental results across three model families and five instruction-following datasets show that SWITCH surpasses traditional KD methods, particularly excelling in the generation of long sequential data.</abstract>
      <url hash="8e51feb0">2025.findings-naacl.206</url>
      <bibkey>koo-etal-2025-switch</bibkey>
    </paper>
    <paper id="207">
      <title><fixed-case>T</fixed-case>hought2<fixed-case>T</fixed-case>ext: Text Generation from <fixed-case>EEG</fixed-case> Signal using Large Language Models (<fixed-case>LLM</fixed-case>s)</title>
      <author><first>Abhijit</first><last>Mishra</last><affiliation>University of Texas at Austin and Apple</affiliation></author>
      <author><first>Shreya</first><last>Shukla</last></author>
      <author><first>Jose</first><last>Torres</last></author>
      <author><first>Jacek</first><last>Gwizdka</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Shounak</first><last>Roychowdhury</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>3747-3759</pages>
      <abstract>Decoding and expressing brain activity in a comprehensible form is a challenging frontier in AI. This paper presents *Thought2Text*, which uses instruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to achieve this goal. The approach involves three stages: (1) training an EEG encoder for visual feature extraction, (2) fine-tuning LLMs on image and text data, enabling multimodal description generation, and (3) further fine-tuning on EEG embeddings to generate text directly from EEG during inference. Experiments on a public EEG dataset collected for six subjects with image stimuli and text captions demonstrate the efficacy of multimodal LLMs (*LLaMA-v3*, *Mistral-v0.3*, *Qwen2.5*), validated using traditional language generation evaluation metrics, as well as *fluency* and *adequacy* measures. This approach marks a significant advancement towards portable, low-cost “thoughts-to-text” technology with potential applications in both neuroscience and natural language processing.</abstract>
      <url hash="75c8bcb0">2025.findings-naacl.207</url>
      <bibkey>mishra-etal-2025-thought2text</bibkey>
    </paper>
    <paper id="208">
      <title>A Comprehensive Survey of Contemporary <fixed-case>A</fixed-case>rabic Sentiment Analysis: Methods, Challenges, and Future Directions</title>
      <author><first>Zhiqiang</first><last>Shi</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Ruchit</first><last>Agrawal</last><affiliation>University of Birmingham</affiliation></author>
      <pages>3760-3772</pages>
      <abstract>Sentiment Analysis, a popular subtask of Natural Language Processing, employs computational methods to extract sentiment, opinions, and other subjective aspects from linguistic data. Given its crucial role in understanding human sentiment, research in sentiment analysis has witnessed significant growth in the recent years. However, the majority of approaches are aimed at the English language, and research towards Arabic sentiment analysis remains relatively unexplored. This paper presents a comprehensive and contemporary survey of Arabic Sentiment Analysis, identifies the challenges and limitations of existing literature in this field and presents avenues for future research. We present a systematic review of Arabic sentiment analysis methods, focusing specifically on research utilizing deep learning. We then situate Arabic Sentiment Analysis within the broader context, highlighting research gaps in Arabic sentiment analysis as compared to general sentiment analysis. Finally, we outline the main challenges and promising future directions for research in Arabic sentiment analysis.</abstract>
      <url hash="f5598cef">2025.findings-naacl.208</url>
      <bibkey>shi-agrawal-2025-comprehensive</bibkey>
    </paper>
    <paper id="209">
      <title>Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models</title>
      <author><first>Shintaro</first><last>Ozaki</last></author>
      <author><first>Kazuki</first><last>Hayashi</last></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>3773-3809</pages>
      <abstract>As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data.</abstract>
      <url hash="b5da3260">2025.findings-naacl.209</url>
      <bibkey>ozaki-etal-2025-towards</bibkey>
    </paper>
    <paper id="210">
      <title>Large Language Models are Easily Confused: A Quantitative Metric, Security Implications and Typological Analysis</title>
      <author><first>Yiyi</first><last>Chen</last></author>
      <author><first>Qiongxiu</first><last>Li</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Russa</first><last>Biswas</last><affiliation>Aalborg University, Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <pages>3810-3827</pages>
      <abstract>Language Confusion is a phenomenon where Large Language Models (LLMs) generate text that is neither in the desired language, nor in a contextually appropriate language. This phenomenon presents a critical challenge in text generation by LLMs, often appearing as erratic and unpredictable behavior. We hypothesize that there are linguistic regularities to this inherent vulnerability in LLMs and shed light on patterns of language confusion across LLMs. We introduce a novel metric, Language Confusion Entropy, designed to directly measure and quantify this confusion, based on language distributions informed by linguistic typology and lexical variation. Comprehensive comparisons with the Language Confusion Benchmark (Marchisio et al., 2024) confirm the effectiveness of our metric, revealing patterns of language confusion across LLMs. We further link language confusion to LLM security, and find patterns in the case of multilingual embedding inversion attacks. Our analysis demonstrates that linguistic typology offers theoretically grounded interpretation, and valuable insights into leveraging language similarities as a prior for LLM alignment and security.</abstract>
      <url hash="a7ec0267">2025.findings-naacl.210</url>
      <bibkey>chen-etal-2025-large</bibkey>
    </paper>
    <paper id="211">
      <title>Huatuo-26<fixed-case>M</fixed-case>, a Large-scale <fixed-case>C</fixed-case>hinese Medical <fixed-case>QA</fixed-case> Dataset</title>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Shunian</first><last>Chen</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Yuxuan</first><last>Zhu</last></author>
      <author><first>Xiangbo</first><last>Wu</last><affiliation>SRIBD</affiliation></author>
      <author><first>Zhiyi</first><last>Zhang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xiaolong</first><last>Xu</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>3828-3848</pages>
      <abstract>Large Language Models infuse newfound vigor into the advancement of the medical domain, yet the scarcity of data poses a significant bottleneck hindering community progress. In this paper, we release the largest ever medical Question Answering (QA) dataset with 26 Million QA pairs named Huatuo-26M. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. We also experimentally show the benefit of the proposed dataset in many aspects: (i) it serves as a fine-tuning data for training medical Large Language Models (LLMs); (ii) it works as an external knowledge source for retrieval-augmented generation (RAG); (iii) it demonstrates transferability by enhancing zero-shot performance on other QA datasets; and (iv) it aids in training biomedical model as a pre-training corpus. Our empirical findings substantiate the dataset’s utility in these domains, thereby confirming its significance as a resource in the medical QA landscape.</abstract>
      <url hash="3fcb3f8a">2025.findings-naacl.211</url>
      <bibkey>wang-etal-2025-huatuo</bibkey>
    </paper>
    <paper id="212">
      <title><fixed-case>SEP</fixed-case>-<fixed-case>MLDC</fixed-case>: A Simple and Effective Paradigm for Multi-Label Document Classification</title>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Shuqin</first><last>Li</last></author>
      <author><first>Xiaotong</first><last>Zhang</last></author>
      <author><first>Yuanyuan</first><last>Wang</last></author>
      <author><first>Feng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Hongyang</first><last>Chen</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>3849-3859</pages>
      <abstract>Multi-label document classification (MLDC) aims to allocate more than one label to each document and attracts increasing attention in many practical applications. However, previous studies have failed to pay sufficient attention to the lack of semantic information on labels and the long-tail problem prevalent in the datasets. Additionally, most existing methods focus on optimizing document features, overlooking the potential of high-quality label features to enhance classification performance. In this paper, we propose a simple and effective paradigm for MLDC. Regarding the problem of insufficient label information and imbalance in the sample size of categories, we utilize large language models (LLMs) to semantically expand the label content and generate pseudo-samples for the tail categories. To optimize the features of both documents and labels, we design the contrastive learning boosted feature optimization module facilitated by the similarity matrices. Finally, we construct a label-guided feature selection module to incorporate the optimized label features into the input features to provide richer semantic information for the classifier. Extensive experiments have demonstrated that our proposed method significantly outperforms state-of-the-art baselines.</abstract>
      <url hash="0a28a900">2025.findings-naacl.212</url>
      <bibkey>liu-etal-2025-sep</bibkey>
    </paper>
    <paper id="213">
      <title>Improving Pre-trained Language Models with Knowledge Enhancement and Filtering Framework</title>
      <author><first>Qi</first><last>Zhao</last></author>
      <author><first>Qi</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Tian</first><last>Xie</last></author>
      <author><first>Haiyue</first><last>Zhang</last></author>
      <author><first>Hongyu</first><last>Yang</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <pages>3860-3871</pages>
      <abstract>Pre-trained language models (PLMs) are widely used in NLP but struggle with capturing entity knowledge. To address this, knowledge enhancement techniques have been proposed. However, existing methods rely heavily on external knowledge bases embedding and often introduce noisy entity representations. In this work, we propose a novel **K**nowledge **E**nhancement **F**iltering **F**ramework named KEFF, which contains both knowledge enhancement and knowledge enhancement filtering modules for PLM. We find that there are certain redundant bits in the embedding space of PLMs. Building on this insight, we implement knowledge-enhanced mapping of redundant bit values in entity span tokens. In order to solve the knowledge enhancement problem of existing methods that introduce noisy entity representation knowledge, we further propose a novel knowledge enhancement filter based on our knowledge enhancement method. Finally, experiments on four knowledge-driven NLP tasks show that our method effectively improves the ability of PLMs on downstream tasks. Compared to state-of-the-art approachs, our method achieves the highest F1-score and accuracy, while reducing the computational cost by 1.7-2.5x.</abstract>
      <url hash="584ea5c9">2025.findings-naacl.213</url>
      <bibkey>zhao-etal-2025-improving</bibkey>
    </paper>
    <paper id="214">
      <title>Using Review Combination and Pseudo-Tokens for Aspect Sentiment Quad Prediction</title>
      <author><first>Jiazhou</first><last>Chen</last></author>
      <author><first>Xu</first><last>Jia</last><affiliation>Hebei Normal University</affiliation></author>
      <author><first>RuiQiang</first><last>Guo</last></author>
      <pages>3872-3883</pages>
      <abstract>Aspect Sentiment Quad Prediction (ASQP) aims to identify quadruples consisting of an aspect term, aspect category, opinion term, and sentiment polarity from a given sentence, which is the most representative and challenging task in aspect-based sentiment analysis. A major challenge arises when implicit sentiment is present, as existing models often confuse implicit and explicit sentiment, making it difficult to extract the quadruples effectively. To tackle this issue, we propose a framework that leverages distinct labeled features from diverse reviews and incorporates pseudo-token prompts to harness the semantic knowledge of pre-trained models, effectively capturing both implicit and explicit sentiment expressions. Our approach begins by categorizing reviews based on the presence of implicit sentiment elements. We then build new samples that combine those with implicit sentiment and those with explicit sentiment. Next, we employ prompts with pseudo-tokens to guide the model in distinguishing between implicit and explicit sentiment expressions. Extensive experimental results show that our proposed method enhances the model’s ability across four public datasets, averaging 1.99% F1 improvement, particularly in instances involving implicit sentiment. We release our code at https://github.com/chienarmor/absa-implicit.</abstract>
      <url hash="e74fc37e">2025.findings-naacl.214</url>
      <bibkey>chen-etal-2025-using</bibkey>
    </paper>
    <paper id="215">
      <title><fixed-case>DDGIP</fixed-case>: Radiology Report Generation Through Disease Description Graph and Informed Prompting</title>
      <author><first>Chentao</first><last>Huang</last></author>
      <author><first>Guangli</first><last>Li</last><affiliation>East China Jiao Tong University</affiliation></author>
      <author><first>Xinjiong</first><last>Zhou</last></author>
      <author><first>Yafeng</first><last>Ren</last></author>
      <author><first>Hongbin</first><last>Zhang</last><affiliation>East China Jiao Tong University</affiliation></author>
      <pages>3884-3894</pages>
      <abstract>Automatic radiology report generation has attracted considerable attention with the rise of computer-aided diagnostic systems. Due to the inherent biases in medical imaging data, generating reports with precise clinical details is challenging yet crucial for accurate diagnosis. To this end, we design a disease description graph that encapsulates comprehensive and pertinent disease information. By aligning visual features with the graph, our model enhances the quality of the generated reports. Furthermore, we introduce a novel informed prompting method which increases the accuracy of short-gram predictions, acting as an implicit bag-of-words planning for surface realization. Notably, this informed prompt succeeds with a three-layer decoder, reducing the reliance on conventional prompting methods that require extensive model parameters. Extensive experiments on two widely-used datasets, IU-Xray and MIMIC-CXR, demonstrate that our method outperforms previous state-of-the-art models.</abstract>
      <url hash="d6eb2864">2025.findings-naacl.215</url>
      <bibkey>huang-etal-2025-ddgip</bibkey>
    </paper>
    <paper id="216">
      <title>Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding</title>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Sangjin</first><last>Choi</last></author>
      <author><first>Taeho</first><last>Hwang</last></author>
      <author><first>Jeongyeon</first><last>Seo</last></author>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Huije</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Youngjin</first><last>Kwon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>3895-3911</pages>
      <abstract>Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.</abstract>
      <url hash="990f660f">2025.findings-naacl.216</url>
      <bibkey>cho-etal-2025-lossless</bibkey>
    </paper>
    <paper id="217">
      <title>Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large Language Models</title>
      <author><first>Jialiang</first><last>Wu</last></author>
      <author><first>Yi</first><last>Shen</last><affiliation>China Mobile Communications Group Co.,Ltd</affiliation></author>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Yi</first><last>Tang</last></author>
      <author><first>Sen</first><last>Song</last></author>
      <author><first>Xiaoyi</first><last>Wang</last><affiliation>Beijing Wispirit Technology</affiliation></author>
      <author><first>Longjun</first><last>Cai</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3912-3921</pages>
      <abstract>Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the correlation between hidden-state prediction changes and output factuality into a deeper, token-wise level. Based on the insights , we propose cross-layer Entropy eNhanced Decoding (END), a decoding method that mitigates hallucinations without requiring extra training. END leverages inner probability changes across layers to individually quantify the factual knowledge required for each candidate token, and adjusts the final predicting distribution to prioritize tokens with higher factuality. Experiments on both hallucination and QA benchmarks demonstrate that END significantly enhances the truthfulness and informativeness of generation while maintaining robust QA accuracy. Moreover, our work provides a deeper perspective of understanding the correlations between inherent knowledge and output factuality.</abstract>
      <url hash="b00a9005">2025.findings-naacl.217</url>
      <bibkey>wu-etal-2025-improve-decoding</bibkey>
    </paper>
    <paper id="218">
      <title><fixed-case>TE</fixed-case>a<fixed-case>R</fixed-case>: Improving <fixed-case>LLM</fixed-case>-based Machine Translation with Systematic Self-Refinement</title>
      <author><first>Zhaopeng</first><last>Feng</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Bei</first><last>Wu</last></author>
      <author><first>Jiayu</first><last>Liao</last><affiliation>Tencent NLP Speech</affiliation></author>
      <author><first>Wenqiang</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Jun</first><last>Lang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3922-3938</pages>
      <abstract>Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, human evaluations reveal that LLM-generated translations still contain various errors. Notably, feeding the error information back into the LLMs can facilitate self-refinement, leading to enhanced translation quality. Motivated by these findings, we introduce TEaR (Translate, Estimate, and Refine), a systematic LLM-based self-refinement framework aimed at bootstrapping translation performance. Our key results show that: 1) TEaR framework enables LLMs to improve their translation quality relying solely on self-feedback, measured by both automatic metrics and Multidimensional Quality Metrics (MQM) scores; 2) TEaR autonomously selects improvements, ensuring a robust translation quality baseline while outperforming both internal refinement and external feedback methods. Error analysis and iterative refinement experiments show its ability to continuously reduce translation errors and enhance overall translation quality. Our code and data are publicly available at https://github.com/fzp0424/self_correct_mt.</abstract>
      <url hash="88a9a00b">2025.findings-naacl.218</url>
      <bibkey>feng-etal-2025-tear</bibkey>
    </paper>
    <paper id="219">
      <title>Vulnerability of Large Language Models to Output Prefix Jailbreaks: Impact of Positions on Safety</title>
      <author><first>Yiwei</first><last>Wang</last><affiliation>University of California, Merced</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>3939-3952</pages>
      <abstract>Previous research on jailbreak attacks has mainly focused on optimizing the adversarial snippet content injected into input prompts to expose LLM security vulnerabilities. A significant portion of this research focuses on developing more complex, less readable adversarial snippets that can achieve higher attack success rates. In contrast to this trend, our research investigates the impact of the adversarial snippet’s position on the effectiveness of jailbreak attacks. We find that placing a simple and readable adversarial snippet at the beginning of the output effectively exposes LLM safety vulnerabilities, leading to much higher attack success rates than the input suffix attack or prompt-based output jailbreaks. Precisely speaking, we discover that directly enforcing the user’s target embedded output prefix is an effective method to expose LLMs’ safety vulnerabilities.</abstract>
      <url hash="19bc7d7f">2025.findings-naacl.219</url>
      <bibkey>wang-etal-2025-vulnerability</bibkey>
    </paper>
    <paper id="220">
      <title><fixed-case>I</fixed-case>ma<fixed-case>RA</fixed-case>: An Imaginative Frame Augmented Method for Low-Resource Multimodal Metaphor Detection and Explanation</title>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Minzheng</first><last>Wang</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>3953-3967</pages>
      <abstract>Multimodal metaphor detection is an important and challenging task in multimedia computing, which aims to distinguish between metaphorical and literal multimodal expressions. Existing studies mainly utilize typical multimodal computing approaches for detection, neglecting the unique cross-domain and cross-modality characteristics underlying multimodal metaphor understanding. According to Conceptual Metaphor Theory (CMT), the inconsistency between source and target domains and their attribute similarity are essential to infer the intricate meanings implied in metaphors. In practice, the scarcity of the annotated multimodal metaphorical contents in the real world brings additional difficulty to the detection task and further complicates the understanding of multimodal metaphors. To address the above challenges, in this paper, we propose a novel Imaginative FRame Augmented (ImaRA) method for low-resource multimodal metaphor detection and explanation inspired by CMT. Specifically, we first identify imaginative frame as an associative structure to stimulate the imaginative thinking of multimodal metaphor detection and understanding. We then construct a cross-modal imagination dataset rich in multimodal metaphors and corresponding imaginative frames, and retrieve an augmented instance from this imagination dataset using imaginative frames mined from the input. This augmented instance serves as the demonstration exemplar to boost the metaphor reasoning ability of the multimodal large language model (MLLM) in low-resource multimodal scenarios. Experiments on two publicly available datasets show that our method consistently achieves robust results compared to MLLM-based methods for both multimodal metaphor detection and explanation in low-resource scenarios and meanwhile surpasses existing multimodal metaphor detection methods with full training data.</abstract>
      <url hash="3b8875fd">2025.findings-naacl.220</url>
      <bibkey>tian-etal-2025-imara</bibkey>
    </paper>
    <paper id="221">
      <title><fixed-case>XAMPLER</fixed-case>: Learning to Retrieve Cross-Lingual In-Context Examples</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>3968-3977</pages>
      <abstract>Recent studies indicate that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving relevant in-context examples tailored to the input query, enhances few-shot in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, poses challenges due to the scarcity of cross-lingual retrievers and annotated data. Thus, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever based on Glot500, a multilingual small language model, using positive and negative English examples constructed from the predictions of a multilingual large language model, i.e., MaLA500. Leveraging the cross-lingual capacity of the retriever, it can directly retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on two multilingual text classification benchmarks, namely SIB200 with 176 languages and MasakhaNEWS with 16 languages, demonstrate that XAMPLER substantially improves the in-context learning performance across languages.</abstract>
      <url hash="3d4d7436">2025.findings-naacl.221</url>
      <bibkey>lin-etal-2025-xampler</bibkey>
    </paper>
    <paper id="222">
      <title>Evaluating Cultural and Social Awareness of <fixed-case>LLM</fixed-case> Web Agents</title>
      <author><first>Haoyi</first><last>Qiu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Divyansh</first><last>Agarwal</last><affiliation>Salesforce.com</affiliation></author>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Sarah</first><last>Tan</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>3978-4005</pages>
      <abstract>As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents’ sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents’ ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages – fine-tuning on culture-specific datasets significantly enhances the agents’ ability to generalize across different regions, while prompting boosts the agents’ ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents’ cultural and social awareness during the development cycle.</abstract>
      <url hash="84538ddf">2025.findings-naacl.222</url>
      <bibkey>qiu-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="223">
      <title><fixed-case>GRAIT</fixed-case>: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation</title>
      <author><first>Runchuan</first><last>Zhu</last></author>
      <author><first>Xinke</first><last>Jiang</last></author>
      <author><first>Jiang</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhipeng</first><last>Ma</last></author>
      <author><first>Jiahe</first><last>Song</last></author>
      <author><first>Fengshuo</first><last>Bai</last></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Conghui</first><last>He</last><affiliation>Shanghai AI Lab</affiliation></author>
      <pages>4006-4021</pages>
      <abstract>Refusal-Aware Instruction Tuning (RAIT) aims to enhance Large Language Models (LLMs) by improving their ability to refuse responses to questions beyond their knowledge, thereby reducing hallucinations and improving reliability. Effective RAIT must address two key challenges: firstly, effectively reject unknown questions to minimize hallucinations; secondly, avoid over-refusal to ensure questions that can be correctly answered are not rejected, thereby maintain the helpfulness of LLM outputs. In this paper, we address the two challenges by deriving insightful observations from the gradient-based perspective, and proposing the Gradient-driven Refusal Aware Instruction Tuning Framework GRAIT: (1) employs gradient-driven sample selection to effectively minimize hallucinations and (2) introduces an adaptive weighting mechanism during fine-tuning to reduce the risk of over-refusal, achieving the balance between accurate refusals and maintaining useful responses. Experimental evaluations on open-ended and multiple-choice question answering tasks demonstrate that GRAIT significantly outperforms existing RAIT methods in the overall performance. The source code and data will be available at https://github.com/opendatalab/GRAIT .</abstract>
      <url hash="734399c1">2025.findings-naacl.223</url>
      <bibkey>zhu-etal-2025-grait</bibkey>
    </paper>
    <paper id="224">
      <title>Entity Pair-guided Relation Summarization and Retrieval in <fixed-case>LLM</fixed-case>s for Document-level Relation Extraction</title>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hongsen</first><last>Yu</last></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Huangming</first><last>Xu</last></author>
      <pages>4022-4037</pages>
      <abstract>Document-level relation extraction (DocRE) aims to extract relations between entities in a document. While previous research has primarily focused on traditional small models, recent studies have extended the scope to large language models (LLMs). Current LLM-based methods typically focus on filtering all potential relations (candidate relations) within a document at one time and then performing triplet fact extraction. However, most approaches for candidate relation filtering are based on the document level, which results in insufficient correlation between candidate relations and entity pairs. In addition, the data imbalance problem caused by a large amount of no-relation data (NA problem) is another important reason for the suboptimal performance of LLM-based methods. To address these issues, we propose an entity pair-guided relation summarization and retrieval model (EP-RSR) for DocRE, which introduces an innovative LLM-based document-level relation extraction paradigm, EPRF (Entity Pair-Relation-Fact), along with an entity pair-level candidate relation filtering method. Our approach first selects entity pairs that potentially contain relations and uses them to guide relation summarization and retrieval for extracting relation facts. This enhances the relevance between candidate relations and entity pairs while alleviating the issue of imbalanced NA data. Benchmark testing on three datasets demonstrates that our approach achieves state-of-the-art (SOTA) performance for LLM-based models. Our code is available at https://github.com/LookingYu/EP-RSR.</abstract>
      <url hash="0fbe2769">2025.findings-naacl.224</url>
      <bibkey>zhang-etal-2025-entity</bibkey>
    </paper>
    <paper id="225">
      <title>A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>4038-4050</pages>
      <abstract>Recent studies have highlighted the potential of exploiting parallel corpora to enhance multilingual large language models, improving performance in both bilingual tasks, e.g., machine translation, and general-purpose tasks, e.g., text classification. Building upon these findings, our comprehensive study aims to identify the most effective strategies for leveraging parallel corpora. We investigate the impact of parallel corpora quality and quantity, training objectives, and model size on the performance of multilingual large language models enhanced with parallel corpora across diverse languages and tasks. Our analysis reveals several key insights: (i) filtering noisy translations is essential for effectively exploiting parallel corpora, while language identification and short sentence filtering have little effect; (ii) even a corpus with just 10K parallel sentences can yield results comparable to those obtained from much larger datasets; (iii) employing only the machine translation objective yields the best results among various training objectives and their combinations; (iv) larger multilingual language models benefit more from parallel corpora than smaller models. Our study offers valuable insights into the optimal utilization of parallel corpora to enhance multilingual large language models, extending the generalizability of previous findings from limited languages and tasks to a broader range of scenarios.</abstract>
      <url hash="20ecb65b">2025.findings-naacl.225</url>
      <bibkey>lin-etal-2025-recipe</bibkey>
    </paper>
    <paper id="226">
      <title>Omni-Chart-600<fixed-case>K</fixed-case>: A Comprehensive Dataset of Chart Types for Chart Understanding</title>
      <author><first>Shulei</first><last>Wang</last></author>
      <author><first>Shuai</first><last>Yang</last></author>
      <author><first>Wang</first><last>Lin</last></author>
      <author><first>Zirun</first><last>Guo</last></author>
      <author><first>Sihang</first><last>Cai</last><affiliation>College of Computer Science and Technology, Zhejiang University</affiliation></author>
      <author><first>Hai</first><last>Huang</last></author>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Jingyuan</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tao</first><last>Jin</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4051-4069</pages>
      <abstract>To address the deficiencies in chart types and the limited scope of chart tasks in existing datasets, we conducted a comprehensive review of current data collection methodologies. By integrating manual annotation with data generation leveraging GPT-4, we developed a dataset that includes 21 diverse chart types and a broad spectrum of tasks, such as data retrieval and mathematical reasoning. Our analysis of existing models revealed that capabilities in information extraction, mathematical reasoning, and understanding of multiple chart types are essential for performing a variety of chart tasks. To overcome the limitations in these areas, we devised a two-stage training strategy and a method for jointly training the vision encoder tailored for multi-type charts. In the first stage, we designed several tasks to enhance the model’s general understanding of charts, aligning multimodal large models pre-trained on natural images to chart tasks. To further improve the model’s capability to understand various chart tasks and enhance its reasoning abilities, we employed Chain-of-Thought data for training in the second stage. Through two-stage training on our proposed dataset, the pre-trained multimodal large language model achieved state-of-the-art performance across multiple chart understanding tasks, demonstrating the superiority of our data and methods.</abstract>
      <url hash="02a25420">2025.findings-naacl.226</url>
      <bibkey>wang-etal-2025-omni</bibkey>
    </paper>
    <paper id="227">
      <title>Comprehensive Layer-wise Analysis of <fixed-case>SSL</fixed-case> Models for Audio Deepfake Detection</title>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author><first>Younes</first><last>Samih</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Suraj</first><last>Maharjan</last><affiliation>Amazon</affiliation></author>
      <author><first>Tim</first><last>Polzehl</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>4070-4082</pages>
      <abstract>This paper conducts a comprehensive layer-wise analysis of self-supervised learning (SSL) models for audio deepfake detection across diverse contexts, including multilingual datasets (English, Chinese, Spanish), partial, song, and scene-based deepfake scenarios. By systematically evaluating the contributions of different transformer layers, we uncover critical insights into model behavior and performance. Our findings reveal that lower layers consistently provide the most discriminative features, while higher layers capture less relevant information. Notably, all models achieve competitive equal error rate (EER) scores even when employing a reduced number of layers. This indicates that we can reduce computational costs and increase the inference speed of detecting deepfakes by utilizing only a few lower layers. This work enhances our understanding of SSL models in deepfake detection, offering valuable insights applicable across varied linguistic and contextual settings. Our models and code are publicly available at https://github.com/Yaselley/SSL_Layerwise_Deepfake.</abstract>
      <url hash="d24d955b">2025.findings-naacl.227</url>
      <bibkey>el-kheir-etal-2025-comprehensive</bibkey>
    </paper>
    <paper id="228">
      <title>Attention on Multiword Expressions: A Multilingual Study of <fixed-case>BERT</fixed-case>-based Models with Regard to Idiomaticity and Microsyntax</title>
      <author><first>Iuliia</first><last>Zaitova</last></author>
      <author><first>Vitalii</first><last>Hirak</last></author>
      <author><first>Badr M.</first><last>Abdullah</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <author><first>Bernd</first><last>Möbius</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Tania</first><last>Avgustinova</last></author>
      <pages>4083-4092</pages>
      <abstract>This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages — English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.</abstract>
      <url hash="4571c784">2025.findings-naacl.228</url>
      <bibkey>zaitova-etal-2025-attention</bibkey>
    </paper>
    <paper id="229">
      <title>Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios</title>
      <author><first>Jiwei</first><last>Tang</last></author>
      <author><first>Jin</first><last>Xu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Tingwei</first><last>Lu</last></author>
      <author><first>Zhicheng</first><last>Zhang</last></author>
      <author><first>YimingZhao</first><last>YimingZhao</last></author>
      <author><first>LinHai</first><last>LinHai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hai-Tao</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>4093-4108</pages>
      <abstract>Large language models (LLMs) demonstrate exceptional capabilities in various scenarios. However, they suffer from much redundant information and are sensitive to the position of key information in long context scenarios. To address these challenges, we present Perception Compressor, a training-free prompt compression framework. It includes a perception retriever that leverages guiding questions and instruction to retrieve the most relevant demonstrations, a dual-slope ratio allocator to dynamically allocate compression ratios and open-book ratios, and a semi-guided iterative compression that retains key information at the token level while removing tokens that distract the LLM. We conduct extensive experiments on long context benchmarks, i.e., NaturalQuestions, LongBench, and MuSiQue. Experiment results show that Perception Compressor outperforms existing methods by a large margin, achieving state-of-the-art performance.</abstract>
      <url hash="cb4a6115">2025.findings-naacl.229</url>
      <bibkey>tang-etal-2025-perception</bibkey>
    </paper>
    <paper id="230">
      <title><fixed-case>M</fixed-case>ojo<fixed-case>B</fixed-case>ench: Language Modeling and Benchmarks for Mojo</title>
      <author><first>Md Nishat</first><last>Raihan</last></author>
      <author><first>Joanna C. S.</first><last>Santos</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>4109-4128</pages>
      <abstract>The recently introduced Mojo programming language (PL) by Modular, has received significant attention in the scientific community due to its claimed significant speed boost over Python. Despite advancements in code Large Language Models (LLMs) across various PLs, Mojo remains unexplored in this context. To address this gap, we introduce MojoBench, the first framework for Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM pretrained and finetuned for Mojo code generation, which supports instructions in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a 30-35% performance improvement over leading models like GPT-4o and Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with underrepresented and unseen PLs, offering potential strategies for enhancing model adaptability. MojoBench contributes to our understanding of LLM capabilities and limitations in emerging programming paradigms fostering more robust code generation systems.</abstract>
      <url hash="f2b35935">2025.findings-naacl.230</url>
      <bibkey>raihan-etal-2025-mojobench</bibkey>
    </paper>
    <paper id="231">
      <title><fixed-case>VL</fixed-case>ind-Bench: Measuring Language Priors in Large Vision-Language Models</title>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Minsung</first><last>Kim</last></author>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyukhun</first><last>Koh</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>4129-4144</pages>
      <abstract>Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks. However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information. Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution. Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied. Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors. To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs. It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as commonsense knowledge, visual perception, and commonsense biases. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors, presenting a strong challenge in the field.</abstract>
      <url hash="c82adec3">2025.findings-naacl.231</url>
      <bibkey>lee-etal-2025-vlind</bibkey>
    </paper>
    <paper id="232">
      <title><fixed-case>GRAG</fixed-case>: Graph Retrieval-Augmented Generation</title>
      <author><first>Yuntong</first><last>Hu</last><affiliation>Emory University</affiliation></author>
      <author><first>Zhihan</first><last>Lei</last><affiliation>Emory University</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Pan</last></author>
      <author><first>Chen</first><last>Ling</last></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>Emory University</affiliation></author>
      <pages>4145-4157</pages>
      <abstract>Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views—the text view and the graph view—enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods. Our datasets as well as codes of GRAG are available at https://github.com/HuieL/GRAG.</abstract>
      <url hash="5ffdcb23">2025.findings-naacl.232</url>
      <bibkey>hu-etal-2025-grag</bibkey>
    </paper>
    <paper id="233">
      <title>Sequence-level Large Language Model Training with Contrastive Preference Optimization</title>
      <author><first>Zhili</first><last>Feng</last></author>
      <author><first>Dhananjay</first><last>Ram</last><affiliation>Amazon</affiliation></author>
      <author><first>Cole</first><last>Hawkins</last><affiliation>Amazon</affiliation></author>
      <author><first>Aditya</first><last>Rawal</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinman</first><last>Zhao</last><affiliation>Morph Technologies</affiliation></author>
      <author><first>Sheng</first><last>Zha</last><affiliation>Amazon</affiliation></author>
      <pages>4158-4164</pages>
      <abstract>The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.</abstract>
      <url hash="c0baf723">2025.findings-naacl.233</url>
      <bibkey>feng-etal-2025-sequence</bibkey>
    </paper>
    <paper id="234">
      <title>Scaling Up Membership Inference: When and How Attacks Succeed on Large Language Models</title>
      <author><first>Haritz</first><last>Puerto</last><affiliation>TU Darmstadt</affiliation></author>
      <author><first>Martin</first><last>Gubri</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Seong Joon</first><last>Oh</last><affiliation>Parameter Lab and Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>4165-4182</pages>
      <abstract>Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable “cheating.” In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable document- and dataset-level MIA. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.</abstract>
      <url hash="1674f156">2025.findings-naacl.234</url>
      <bibkey>puerto-etal-2025-scaling</bibkey>
    </paper>
    <paper id="235">
      <title>Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding</title>
      <author><first>Kyungmin</first><last>Min</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Minbeom</first><last>Kim</last></author>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dongryeol</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>4183-4198</pages>
      <abstract>Large Vision-Language Models (LVLMs) demonstrate impressive capabilities in generating detailed and coherent responses from visual inputs.However, they are prone to generate hallucinations due to an over-reliance on language priors. To address this issue, we investigate the language priors in LVLMs and make two key observations: (1) Even when predicting the tokens associated with image-related part-of-speech (POS), models increasingly rely on linguistic priors as the token sequences grow, thereby amplifying hallucinations. (2) Methods that directly calibrate LVLM’s output distribution to mitigate language priors can lead to a degradation in text quality or even exacerbate hallucinations.Based on these findings, we propose a novel method, <b>Sum</b>mary-<b>G</b>uided <b>D</b>ecoding <b>(SumGD)</b>. This method naturally encourages the model to focus more on image information by reducing the text context through summaries, while controlling only the image-related POS tokens to maintain text quality.Through experiments, we demonstrate that SumGD achieves state-of-the-art performance on object hallucination benchmarks. Furthermore, in terms of the trade-off between precision and recall, SumGD achieves Pareto optimality among the existing methods.Lastly, we observe that although existing methods struggle to balance the reduction of object hallucinations with maintaining text quality, SumGD demonstrates robustness in handling this challenge.</abstract>
      <url hash="c421dbb2">2025.findings-naacl.235</url>
      <bibkey>min-etal-2025-mitigating</bibkey>
    </paper>
    <paper id="236">
      <title>Exploring Hybrid Sampling Inference for Aspect-based Sentiment Analysis</title>
      <author><first>Xiaoyi</first><last>Bao</last></author>
      <author><first>Minjie</first><last>Qiang</last></author>
      <author><first>Jinghang</first><last>Gu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>4199-4210</pages>
      <abstract>As the training of large language models (LLMs) will encounter high computational costs, massive works are now focusing on inference. Their methods can be generally summarised as re-sampling the target multiple times and performing a vote upon the outputs. Despite bringing significant performance improvements, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple yet efficient inference strategies named __Hybrid Sampling__ that combining both multiple and single sampling to greatly reduce the cost of multiple sampling without sacrificing performance. __Hybrid Sampling__ could dynamically choose the essential part of generated sequence for multiple sampling and proceed the rest with single sampling, achieving a performance-cost balance. Extensive experiments in several benchmarks underscore the robustness and effectiveness of our proposed Hybrid Sampling and more importantly, it is much faster.</abstract>
      <url hash="93cad0c5">2025.findings-naacl.236</url>
      <bibkey>bao-etal-2025-exploring</bibkey>
    </paper>
    <paper id="237">
      <title><fixed-case>F</fixed-case>e<fixed-case>RG</fixed-case>-<fixed-case>LLM</fixed-case> : Feature Engineering by Reason Generation Large Language Models</title>
      <author><first>Jeonghyun</first><last>Ko</last></author>
      <author><first>Gyeongyun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Donghoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Kyunam</first><last>Lee</last><affiliation>SK Telecom</affiliation></author>
      <pages>4211-4228</pages>
      <url hash="be03c9c0">2025.findings-naacl.237</url>
      <bibkey>ko-etal-2025-ferg</bibkey>
    </paper>
    <paper id="238">
      <title>Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Abdellah</first><last>El Mekki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4229-4256</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive performance on a wide range of natural language processing (NLP) tasks, primarily through in-context learning (ICL). In ICL, the LLM is provided with examples that represent a given task such that it learns to generate answers for test inputs. However, access to these in-context examples is not guaranteed especially for low-resource or massively multilingual tasks. In this work, we propose an unsupervised approach to mine in-context examples for machine translation (MT), enabling unsupervised MT (UMT) across different languages. Our approach begins with word-level mining to acquire word translations that are then used to perform sentence-level mining. As the quality of mined parallel pairs may not be optimal due to noise or mistakes, we introduce a filtering criterion to select the optimal in-context examples from a pool of unsupervised parallel sentences. We evaluate our approach using two multilingual LLMs on 288 directions from the FLORES-200 dataset (CITATION) and analyze the impact of various linguistic features on performance. Our findings demonstrate the effectiveness of our unsupervised approach in mining in-context examples for MT, leading to better or comparable translation performance as translation with regular in-context samples (extracted from human-annotated data), while also outperforming the other state-of-the-art UMT methods by an average of 7 BLEU points.</abstract>
      <url hash="544b0cda">2025.findings-naacl.238</url>
      <bibkey>el-mekki-abdul-mageed-2025-effective</bibkey>
    </paper>
    <paper id="239">
      <title><fixed-case>GPT</fixed-case>-<fixed-case>NER</fixed-case>: Named Entity Recognition via Large Language Models</title>
      <author><first>Shuhe</first><last>Wang</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Xiaoya</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Rongbin</first><last>Ouyang</last><affiliation>Peking University, Peking Univserity</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Tianwei</first><last>Zhang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jiwei</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chen</first><last>Guo</last></author>
      <pages>4257-4275</pages>
      <abstract>Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text “Columbus is a city” is transformed to generate the text sequence "@@Columbus## is a city”, where special tokens @@## marks the entity to extract. To efficiently address the <i>hallucination</i> issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag.We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.</abstract>
      <url hash="d778201a">2025.findings-naacl.239</url>
      <bibkey>wang-etal-2025-gpt</bibkey>
    </paper>
    <paper id="240">
      <title><fixed-case>QP</fixed-case>runer: Probabilistic Decision Quantization for Structured Pruning in Large Language Models</title>
      <author><first>Changhai</first><last>Zhou</last></author>
      <author><first>Yuhua</first><last>Zhou</last></author>
      <author><first>Yibin</first><last>Wang</last></author>
      <author><first>Shijie</first><last>Han</last></author>
      <author><first>Qian</first><last>Qiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Hongguang</first><last>Li</last><affiliation>JF SmartInvest Holdings</affiliation></author>
      <pages>4276-4286</pages>
      <abstract>The rise of large language models (LLMs) has significantly advanced various natural language processing (NLP) tasks. However, the resource demands of these models pose substantial challenges. Structured pruning is an effective approach to reducing model size, but it often results in significant accuracy degradation, necessitating parameter updates to adapt. Unfortunately, such fine-tuning requires substantial memory, which limits its applicability. To address these challenges, we introduce quantization into the structured pruning framework to reduce memory consumption during both fine-tuning and inference. However, the combined errors from pruning and quantization increase the difficulty of fine-tuning, requiring a more refined quantization scheme. To this end, we propose QPruner, a novel framework that employs structured pruning to reduce model size, followed by a layer-wise mixed-precision quantization scheme. Quantization precisions are assigned to each layer based on their importance to the target task, and Bayesian optimization is employed to refine precision allocation strategies, ensuring a balance between model accuracy and memory efficiency. Extensive experiments on benchmark datasets demonstrate that QPruner significantly outperforms existing methods in memory savings while maintaining or improving model performance.</abstract>
      <url hash="95b78558">2025.findings-naacl.240</url>
      <bibkey>zhou-etal-2025-qpruner</bibkey>
    </paper>
    <paper id="241">
      <title><fixed-case>MES</fixed-case>-<fixed-case>RAG</fixed-case>: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to <fixed-case>RAG</fixed-case></title>
      <author><first>Pingyu</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Daiheng</first><last>Gao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jing</first><last>Tang</last></author>
      <author><first>Huimin</first><last>Chen</last><affiliation>Independent researcher</affiliation></author>
      <author><first>Wenbo</first><last>Zhou</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weiming</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Nenghai</first><last>Yu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4287-4298</pages>
      <abstract>Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by using external knowledge, but it struggles with precise entity information retrieval. Our proposed **MES-RAG** framework enhances entity-specific query handling and provides accurate, secure, and consistent responses. MES-RAG introduces proactive security measures that ensure system integrity by applying protections prior to data access. Additionally, the system supports real-time multi-modal outputs, including text, images, audio, and video, seamlessly integrating into existing RAG architectures. Experimental results demonstrate that MES-RAG significantly improves both accuracy and recall, highlighting its effectiveness in advancing the security and utility of question-answering, increasing accuracy to **0.83 (+0.25)** on targeted task. Our code and data are available at https://github.com/wpydcr/MES-RAG.</abstract>
      <url hash="4fc93f28">2025.findings-naacl.241</url>
      <bibkey>wu-etal-2025-mes</bibkey>
    </paper>
    <paper id="242">
      <title><fixed-case>LVP</fixed-case>runing: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models</title>
      <author><first>Yizheng</first><last>Sun</last></author>
      <author><first>Yanze</first><last>Xin</last></author>
      <author><first>Hao</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jingyuan</first><last>Sun</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <pages>4299-4308</pages>
      <abstract>Multi-modal Large Language Models (MLLMs) have achieved remarkable success by integrating visual and textual modalities. However, they incur significant computational overhead due to the large number of vision tokens processed, limiting their practicality in resource-constrained environments. We introduce Language-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet simple method that significantly reduces the computational burden while preserving model performance. LVPruning employs cross-attention modules to compute the importance of vision tokens based on their interaction with language tokens, determining which to prune. Importantly, LVPruning can be integrated without modifying the original MLLM parameters, which makes LVPruning simple to apply or remove. Our experiments show that LVPruning can effectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5, resulting in a 62.1% decrease in inference Tera Floating-Point Operations Per Second (TFLOPs), with an average performance loss of just 0.45% across nine multi-modal benchmarks.</abstract>
      <url hash="46cfabf6">2025.findings-naacl.242</url>
      <bibkey>sun-etal-2025-lvpruning</bibkey>
    </paper>
    <paper id="243">
      <title>How Much Knowledge Can You Pack into a <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Adapter without Harming <fixed-case>LLM</fixed-case>?</title>
      <author><first>Sergey</first><last>Pletenev</last></author>
      <author><first>Maria</first><last>Marina</last></author>
      <author><first>Daniil</first><last>Moskovskiy</last></author>
      <author><first>Vasily</first><last>Konovalov</last><affiliation>AIRI</affiliation></author>
      <author><first>Pavel</first><last>Braslavski</last><affiliation>Nazarbayev University</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Mikhail</first><last>Salnikov</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <pages>4309-4322</pages>
      <abstract>The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model’s parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model’s performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.</abstract>
      <url hash="eec5180b">2025.findings-naacl.243</url>
      <bibkey>pletenev-etal-2025-much</bibkey>
    </paper>
    <paper id="244">
      <title><fixed-case>TART</fixed-case>: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning</title>
      <author><first>Xinyuan</first><last>Lu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Liangming</first><last>Pan</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4323-4339</pages>
      <abstract>Current Large Language Models (LLMs) exhibit limited ability to understand table structures and to apply precise numerical reasoning, which is crucial for tasks such as table question answering and table-based fact verification. To address these challenges, we introduce our Tool-Augmented Reasoning framework for Tables (TART), which integrates LLMs with specialized tools. TART contains three key components: a table formatter to ensure accurate data representation, a tool maker to develop specific computational tools, and an explanation generator to maintain explainability. We also present the TOOLTAB dataset, a new benchmark designed specifically for training LLMs in table–tool integration. Our experiments indicate that TART achieves substantial improvements over existing methods (e.g., Chain-of-Thought) by improving both the precision of data processing and the clarity of the reasoning process. Notably, TART paired with CodeLlama achieves 90.0% of the accuracy of the closed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse real-world scenarios. Both code and data are openly available at https://github.com/XinyuanLu00/TART.</abstract>
      <url hash="8d649c61">2025.findings-naacl.244</url>
      <bibkey>lu-etal-2025-tart</bibkey>
    </paper>
    <paper id="245">
      <title>Enhancing Text-to-<fixed-case>SQL</fixed-case> with Question Classification and Multi-Agent Collaboration</title>
      <author><first>Zhihui</first><last>Shao</last></author>
      <author><first>Shubin</first><last>Cai</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Rongsheng</first><last>Lin</last></author>
      <author><first>Zhong</first><last>Ming</last><affiliation>Shenzhen University</affiliation></author>
      <pages>4340-4349</pages>
      <abstract>Large Language Models (LLMs) have recently demonstrated remarkable performance in Text-to-SQL tasks. However, existing research primarily focuses on the optimization of prompts and improvements in workflow, with few studies delving into the exploration of the questions. In this paper, we propose a Text-to-SQL framework based on question classification and multi-agent collaboration (QCMA-SQL). Specifically, we first employ multiple cross-attention mechanisms to train a schema selector to classify questions and select the most suitable database schema. Subsequently, we employ the appropriate agents based on the varying difficulty levels of the questions to generate preliminary SQL queries. Moreover, we implement syntax validation and execution optimization steps to generate final SQL queries. Experimental results on the Spider dataset show that the QCMA-SQL framework achieves an execution accuracy of 87.4%, outperforming state-of-the-art methods. Through ablation studies, we find that classifying the questions ultimately leads to a 2.8% increase in execution accuracy.</abstract>
      <url hash="f79438ba">2025.findings-naacl.245</url>
      <bibkey>shao-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="246">
      <title>Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks</title>
      <author><first>Wataru</first><last>Hashimoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>4350-4366</pages>
      <abstract>Trustworthiness in model predictions is crucial for safety-critical applications in the real world. However, deep neural networks often suffer from the issues of uncertainty estimation, such as miscalibration. In this study, we propose <tex-math>k</tex-math>-Nearest Neighbor Uncertainty Estimation (<tex-math>k</tex-math>NN-UE), which is a new uncertainty estimation method that uses not only the distances from the neighbors, but also the ratio of labels in the neighbors. Experiments on sentiment analysis, natural language inference, and named entity recognition show that our proposed method outperforms the baselines and recent density-based methods in several calibration and uncertainty metrics. Moreover, our analyses indicate that approximate nearest neighbor search techniques reduce the inference overhead without significantly degrading the uncertainty estimation performance when they are appropriately combined.</abstract>
      <url hash="09c7b569">2025.findings-naacl.246</url>
      <bibkey>hashimoto-etal-2025-efficient</bibkey>
    </paper>
    <paper id="247">
      <title><fixed-case>B</fixed-case>it<fixed-case>A</fixed-case>buse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks</title>
      <author><first>Hanyong</first><last>Lee</last></author>
      <author><first>Chaelyn</first><last>Lee</last></author>
      <author><first>Yongjae</first><last>Lee</last><affiliation>Retrvr Inc.</affiliation></author>
      <author><first>Jaesung</first><last>Lee</last><affiliation>Chung-Ang University and Chung-Ang University</affiliation></author>
      <pages>4367-4384</pages>
      <abstract>Phishing often targets victims through visually perturbed texts to bypass security systems. The noise contained in these texts functions as an adversarial attack, designed to deceive language models and hinder their ability to accurately interpret the content. However, since it is difficult to obtain sufficient phishing cases, previous studies have used synthetic datasets that do not contain real-world cases. In this study, we propose the BitAbuse dataset, which includes real-world phishing cases, to address the limitations of previous research. Our dataset comprises a total of 325,580 visually perturbed texts. The dataset inputs are drawn from the raw corpus, consisting of visually perturbed sentences and sentences generated through an artificial perturbation process. Each input sentence is labeled with its corresponding ground truth, representing the restored, non-perturbed version. Language models trained on our proposed dataset demonstrated significantly better performance compared to previous methods, achieving an accuracy of approximately 96%. Our analysis revealed a significant gap between real-world and synthetic examples, underscoring the value of our dataset for building reliable pre-trained models for restoration tasks. We release the BitAbuse dataset, which includes real-world phishing cases annotated with visual perturbations, to support future research in adversarial attack defense.</abstract>
      <url hash="4c5cbace">2025.findings-naacl.247</url>
      <bibkey>lee-etal-2025-bitabuse</bibkey>
    </paper>
    <paper id="248">
      <title>Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization</title>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Shen</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tongyi Lab</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group US</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>4385-4398</pages>
      <abstract>In the fast-changing realm of information, the capacity to construct coherent timelines from extensive event-related content has become increasingly significant and challenging. The complexity arises in aggregating related documents to build a meaningful event graph around a central topic. This paper proposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline SummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective on the integration of Large Language Models (LLMs) to tackle the task of Timeline Summarization (TLS). By iteratively reflecting on how events are linked and posing new questions regarding a specific news topic to gather information online or from an offline knowledge base, LLMs produce and refresh chronological summaries based on documents retrieved in each round. Furthermore, we curate Open-TLS, a novel dataset of timelines on recent news topics authored by professional journalists to evaluate open-domain TLS where information overload makes it impossible to find comprehensive relevant documents from the web. Our experiments indicate that CHRONOS is not only adept at open-domain timeline summarization but also rivals the performance of existing state-of-the-art systems designed for closed-domain applications, where a related news corpus is provided for summarization.</abstract>
      <url hash="5f318fcf">2025.findings-naacl.248</url>
      <bibkey>wu-etal-2025-unfolding</bibkey>
    </paper>
    <paper id="249">
      <title><fixed-case>R</fixed-case>etriever<fixed-case>G</fixed-case>uard: Empowering Information Retrieval to Combat <fixed-case>LLM</fixed-case>-Generated Misinformation</title>
      <author><first>Chuwen</first><last>Chen</last></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>4399-4411</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in generating human-like text and have been shown to store factual knowledge within their extensive parameters. However, models like ChatGPT can still actively or passively generate false or misleading information, increasing the challenge of distinguishing between human-created and machine-generated content. This poses significant risks to the authenticity and reliability of digital communication. This work aims to enhance retrieval models’ ability to identify the authenticity of texts generated by large language models, with the goal of improving the truthfulness of retrieved texts and reducing the harm of false information in the era of large models. Our contributions include: (1) we construct a diverse dataset of authentic human-authored texts and highly deceptive AI-generated texts from various domains; (2) we propose a self-supervised training method, RetrieverGuard, that enables the model to capture textual rules and styles of false information from the corpus without human-labelled data, achieving higher accuracy and robustness in identifying misleading and highly deceptive AI-generated content.</abstract>
      <url hash="c10926be">2025.findings-naacl.249</url>
      <bibkey>chen-zhang-2025-retrieverguard</bibkey>
    </paper>
    <paper id="250">
      <title>Unified Automated Essay Scoring and Grammatical Error Correction</title>
      <author><first>SeungWoo</first><last>Song</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>Junghun</first><last>Yuk</last></author>
      <author><first>ChangSu</first><last>Choi</last></author>
      <author><first>HanGyeol</first><last>Yoo</last></author>
      <author><first>HyeonSeok</first><last>Lim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>KyungTae</first><last>Lim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jungyeul</first><last>Park</last><affiliation>The University of British Columbia</affiliation></author>
      <pages>4412-4426</pages>
      <abstract>This study explores the integration of automated writing evaluation (AWE) and grammatical error correction (GEC) through multitask learning, demonstrating how combining these distinct tasks can enhance performance in both areas. By leveraging a shared learning framework, we show that models trained jointly on AWE and GEC outperform those trained on each task individually. To support this effort, we introduce a dataset specifically designed for multitask learning using AWE and GEC. Our experiments reveal significant synergies between tasks, leading to improvements in both writing assessment accuracy and error correction precision. This research represents a novel approach for optimizing language learning tools by unifying writing evaluation and correction tasks, offering insights into the potential of multitask learning in educational applications.</abstract>
      <url hash="81c1d50d">2025.findings-naacl.250</url>
      <bibkey>song-etal-2025-unified</bibkey>
    </paper>
    <paper id="251">
      <title>A Closer Look into Mixture-of-Experts in Large Language Models</title>
      <author><first>Ka Man</first><last>Lo</last></author>
      <author><first>Zeyu</first><last>Huang</last></author>
      <author><first>Zihan</first><last>Qiu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>4427-4447</pages>
      <abstract>Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models. Concretely, we comprehensively study the parametric and behavioral features of four popular MoE-based models and reveal some intriguing observations, including 1) Neurons act like fine-grained experts; 2) The router of MoE usually selects experts with larger output norms; 3) The expert diversity increases as the layer increases, while the last layer is an outlier, which is further validated by an initial experiment. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.</abstract>
      <url hash="c5634b6c">2025.findings-naacl.251</url>
      <bibkey>lo-etal-2025-closer</bibkey>
    </paper>
    <paper id="252">
      <title><fixed-case>CDB</fixed-case>: A Unified Framework for Hope Speech Detection Through Counterfactual, Desire and Belief</title>
      <author><first>Tulio Ferreira Leite Da</first><last>Silva</last><affiliation>Universidade de São Paulo</affiliation></author>
      <author><first>Gonzalo Freijedo</first><last>Aduna</last><affiliation>Ecole Normale Supérieure – PSL</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>Institut de recherche en informatique de toulouse</affiliation></author>
      <author><first>Alda</first><last>Mari</last><affiliation>CNRS</affiliation></author>
      <author><first>Zongmin</first><last>Li</last></author>
      <author><first>Li</first><last>Yue</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Jian</first><last>Su</last><affiliation>A*STAR</affiliation></author>
      <pages>4448-4463</pages>
      <abstract>Computational modeling of user-generated desires on social media can significantly aid decision-makers across various fields. Initially explored through wish speech,this task has evolved into a nuanced examination of hope speech. To enhance understanding and detection, we propose a novel scheme rooted in formal semantics approaches to modality, capturing both future-oriented hopes through desires and beliefs and the counterfactuality of past unfulfilled wishes and regrets. We manually re-annotated existing hope speech datasets and built a new one which constitutes a new benchmark in the field. We also explore the capabilities of LLMs in automatically detecting hope speech, relying on several prompting strategies. To the best of our knowledge, this is the first attempt towards a language-driven decomposition of the notional category hope and its automatic detection in a unified setting.</abstract>
      <url hash="bbac821e">2025.findings-naacl.252</url>
      <bibkey>silva-etal-2025-cdb</bibkey>
    </paper>
    <paper id="253">
      <title>How Well Do <fixed-case>LLM</fixed-case>s Handle <fixed-case>C</fixed-case>antonese? Benchmarking <fixed-case>C</fixed-case>antonese Capabilities of Large Language Models</title>
      <author><first>Jiyue</first><last>Jiang</last></author>
      <author><first>Pengan</first><last>Chen</last><affiliation>Shanghai Artificial Intelligence Laboratory, University of Hong Kong and University of Hong Kong</affiliation></author>
      <author><first>Liheng</first><last>Chen</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Qinghang</first><last>Bao</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chuan</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>4464-4505</pages>
      <abstract>The rapid evolution of large language models (LLMs) has transformed the competitive landscape in natural language processing (NLP), particularly for English and other data-rich languages. However, underrepresented languages like Cantonese, spoken by over 85 million people, face significant development gaps, which is particularly concerning given the economic significance of the Guangdong-Hong Kong-Macau Greater Bay Area, and in substantial Cantonese-speaking populations in places like Singapore and North America. Despite its wide use, Cantonese has scant representation in NLP research, especially compared to other languages from similarly developed regions. To bridge these gaps, we outline current Cantonese NLP methods and introduce new benchmarks designed to evaluate LLM performance in factual generation, mathematical logic, complex reasoning, and general knowledge in Cantonese, which aim to advance open-source Cantonese LLM technology. We also propose future research directions and recommended models to enhance Cantonese LLM development.</abstract>
      <url hash="2bb0e895">2025.findings-naacl.253</url>
      <bibkey>jiang-etal-2025-well</bibkey>
    </paper>
    <paper id="254">
      <title>Improving Reward Models with Synthetic Critiques</title>
      <author><first>Zihuiwen</first><last>Ye</last></author>
      <author><first>Fraser David</first><last>Greenlee</last><affiliation>Cohere</affiliation></author>
      <author><first>Max</first><last>Bartolo</last><affiliation>Cohere and University College London</affiliation></author>
      <author><first>Phil</first><last>Blunsom</last><affiliation>Google, Department of Computer Science, University of Oxford and DeepMind</affiliation></author>
      <author><first>Jon Ander</first><last>Campos</last><affiliation>Cohere</affiliation></author>
      <author><first>Matthias</first><last>Gallé</last><affiliation>Cohere</affiliation></author>
      <pages>4506-4520</pages>
      <abstract>Reward models (RMs) play a critical role in aligning language models through the process of reinforcement learning from human feedback. RMs are trained to predict a score reflecting human preference, which requires significant time and cost for human annotation. Additionally, RMs tend to quickly overfit on superficial features in the training set, hindering their generalization performance on unseen distributions. We propose a novel approach using synthetic natural language critiques generated by large language models to provide additional feedback, evaluating aspects such as instruction following, correctness, and style. This offers richer signals and more robust features for RMs to assess and score on. We demonstrate that high-quality critiques improve the performance and data efficiency of RMs initialized from different pretrained models, reducing the reliance on costly human annotations. Furthermore, incorporating critiques improves both the interpretability and robustness of RM training.</abstract>
      <url hash="ab12ae3b">2025.findings-naacl.254</url>
      <bibkey>ye-etal-2025-improving</bibkey>
    </paper>
    <paper id="255">
      <title>Rethinking Smoothness for Fast and Adaptable Entity Alignment Decoding</title>
      <author><first>Yuanyi</first><last>Wang</last></author>
      <author><first>Han</first><last>Li</last></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>China Unicom Network Communications Co., Ltd.</affiliation></author>
      <author><first>Bo</first><last>He</last></author>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Tianhao</first><last>Yan</last></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jingyu</first><last>Wang</last></author>
      <pages>4521-4535</pages>
      <abstract>Entity alignment (EA) is crucial for integrating multi-source knowledge graphs (KGs), aiming to identify equivalent entities across different graphs. However, most existing EA decoding methods rely on both entity and relation embeddings, limiting their generalizability and efficiency, especially in GNN-based models. To address these challenges, we propose Triple Feature Propagation (TFP), an adaptable and fast EA decoding framework that only utilizes entity embeddings. TFP reconstructs KG representation by maximizing the smoothness of entity embeddings. The discretized smoothness-maximization process yields the explicit Euler solution of TFP. We also generalize multi-view matrices: entity-to-entity, entity-to-relation, relation-to-entity, and relation-to-triple, to capture structural diversity. Extensive experiments on public datasets demonstrate that TFP is fast and adaptable to various encoders, achieving comparable results to state-of-the-art methods in under 6 seconds, and surpassing them in many cases.</abstract>
      <url hash="5e0c7ca4">2025.findings-naacl.255</url>
      <bibkey>wang-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="256">
      <title>Lost in the Distance: Large Language Models Struggle to Capture Long-Distance Relational Knowledge</title>
      <author><first>Meiyun</first><last>Wang</last></author>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>4536-4544</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive capabilities in handling long contexts, but challenges remain in capturing relational knowledge spread far apart within text. Connecting long-distance knowledge is important for solving tasks as the context length increases: imagine reading a lengthy detective novel where seemingly trivial information introduced early on often becomes essential during the climactic reveal of the culprit. In this study, we expose the ”Lost in the Distance” phenomenon, where LLM performance of capturing the relational knowledge degrades significantly when the relational knowledge is separated by noise, i.e., unrelated sentences to solve a task. Specifically, we design an experiment in which we insert artificial noise between two related elements and observe model performance as the distance between them increases. Our findings show that while LLMs can handle edge noise with little impact, their ability to reason about distant relationships declines sharply as the intervening noise grows. These findings are consistent in both forward-looking prediction and backward-looking prediction settings. We validate this across various models (GPT-4, Gemini-1.5-pro, GPT-4o-mini, Gemini-1.5-flash, Claude-3.5-Sonnet) and tasks (causal reasoning and knowledge extraction). These results reveal a significant limitation in how LLMs process relational knowledge over long contexts. We release our code and data to support further research.</abstract>
      <url hash="a08ab6ee">2025.findings-naacl.256</url>
      <bibkey>wang-etal-2025-lost</bibkey>
    </paper>
    <paper id="257">
      <title><fixed-case>F</fixed-case>in<fixed-case>NLI</fixed-case>: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</title>
      <author><first>Jabez</first><last>Magomere</last></author>
      <author><first>Elena</first><last>Kochkina</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Samuel</first><last>Mensah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Simerjot</first><last>Kaur</last><affiliation>JPMorgan Chase and Co</affiliation></author>
      <author><first>Charese</first><last>Smiley</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <pages>4545-4568</pages>
      <abstract>We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset’s difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.</abstract>
      <url hash="aaee4cda">2025.findings-naacl.257</url>
      <bibkey>magomere-etal-2025-finnli</bibkey>
    </paper>
    <paper id="258">
      <title>Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models</title>
      <author><first>Atharva</first><last>Mehta</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shivam</first><last>Chauhan</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Amirbek</first><last>Djanibekov</last></author>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Gus</first><last>Xia</last><affiliation>New York University</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>4569-4585</pages>
      <abstract>The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres.We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models – MusicGen and Mustango, for two underrepresented non-Western music traditions – Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.</abstract>
      <url hash="70b293d6">2025.findings-naacl.258</url>
      <bibkey>mehta-etal-2025-music</bibkey>
    </paper>
    <paper id="259">
      <title><fixed-case>SFMSS</fixed-case>: Service Flow aware Medical Scenario Simulation for Conversational Data Generation</title>
      <author><first>Zhijie</first><last>Bao</last></author>
      <author><first>Qingyun</first><last>Liu</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>4586-4604</pages>
      <abstract>Medical-specific Large Language Models (LLMs) have demonstrated impressive performance on medical-related exams and tasks. Despite their success in single-turn question and answering, instruction-tuned LLMs often falter in real-world healthcare applications, highlighting a disconnect between existing instruction datasets and practical contexts. To address this issue, we propose Service Flow aware Medical Scenario Simulation (SFMSS), a simulation framework designed for medical conversational data generation. SFMSS employs three key strategies to ensure the quality of the data generation. the use of Authentic Seed Data ensures alignment of real-world distributions. Diverse Patient Simulation enables simulated patients to exhibit distinct communication styles and complex behavioral logic. Service Flow Control ensures that conversations progress in alignment with medical objectives. We construct a dataset targeting on outpatient reception through SFMSS, named SFMSS-CD. Building on this dataset, we develop a model called SFMSS-Nurse. We conduct both automatic and human evaluations, involving 15 users and 15 clinical experts, to assess the effectiveness of SFMSS. The results demonstrate that SFMSS-Nurse outperforms all baselines, including the current state-of-the-art model GPT-4o, and aligns with human preferences and clinical demands.</abstract>
      <url hash="f73d7304">2025.findings-naacl.259</url>
      <bibkey>bao-etal-2025-sfmss</bibkey>
    </paper>
    <paper id="260">
      <title>Re-evaluating Automatic <fixed-case>LLM</fixed-case> System Ranking for Alignment with Human Preference</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Xinyu</first><last>Hu</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <author><first>Jonathan</first><last>Bragg</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4605-4629</pages>
      <abstract>Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models’ performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.</abstract>
      <url hash="c9b654d2">2025.findings-naacl.260</url>
      <bibkey>gao-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="261">
      <title><fixed-case>G</fixed-case>uide<fixed-case>Q</fixed-case>: Framework for Guided Questioning for progressive informational collection and classification</title>
      <author><first>Priya</first><last>Mishra</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Suraj</first><last>Racha</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Kaustubh</first><last>Ponkshe</last></author>
      <author><first>Adit</first><last>Akarsh</last></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>4630-4644</pages>
      <abstract>The veracity of a factoid is largely independent of the language it is written in. However, language models are inconsistent in their ability to answer the same factual question across languages. This raises questions about how LLMs represent a given fact across languages. We explore multilingual factual knowledge through two aspects: the model’s ability to answer a query consistently across languages, and the ability to ”store” answers in a shared representation for several languages. We propose a methodology to measure the extent of representation sharing across languages by repurposing knowledge editing methods. We examine LLMs with various multilingual configurations using a new multilingual dataset. We reveal that high consistency does not necessarily imply shared representation, particularly for languages with different scripts. Moreover, we find that script similarity is a dominant factor in representation sharing. Finally, we observe that if LLMs could fully share knowledge across languages, their accuracy in their best-performing language could benefit an increase of up to 150% on average. These findings highlight the need for improved multilingual knowledge representation in LLMs and suggest a path for the development of more robust and consistent multilingual LLMs.</abstract>
      <url hash="d82341a2">2025.findings-naacl.261</url>
      <bibkey>mishra-etal-2025-guideq</bibkey>
    </paper>
    <paper id="262">
      <title>Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations</title>
      <author><first>Kirti</first><last>Bhagat</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Kinshuk</first><last>Vasisht</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <pages>4645-4653</pages>
      <abstract>While a large body of work inspects language models for biases concerning gender, race, occupation and religion, biases of geographical nature are relatively less explored. Some recent studies benchmark the degree to which large language models encode geospatial knowledge. However, the impact of the encoded geographical knowledge (or lack thereof) on real-world applications has not been documented. In this work, we examine large language models for two common scenarios that require geographical knowledge: (a) travel recommendations and (b) geo-anchored story generation. Specifically, we study five popular language models, and across about 100K travel requests, and 200K story generations, we observe that travel recommendations corresponding to poorer countries are less unique with fewer location references, and stories from these regions more often convey emotions of hardship and sadness compared to those from wealthier nations.</abstract>
      <url hash="878a85cf">2025.findings-naacl.262</url>
      <bibkey>bhagat-etal-2025-richer</bibkey>
    </paper>
    <paper id="263">
      <title>Swan and <fixed-case>A</fixed-case>rabic<fixed-case>MTEB</fixed-case>: Dialect-Aware, <fixed-case>A</fixed-case>rabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Abdellah</first><last>El Mekki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Fakhraddin</first><last>Alwajih</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4654-4670</pages>
      <abstract>In this paper, we introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan models are dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmarks will be made publicly accessible for research.</abstract>
      <url hash="5eee28ad">2025.findings-naacl.263</url>
      <bibkey>bhatia-etal-2025-swan</bibkey>
    </paper>
    <paper id="264">
      <title><fixed-case>TAGCOS</fixed-case>: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data</title>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Yaxuan</first><last>Qin</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Weizhong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Rui</first><last>Pan</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>4671-4686</pages>
      <abstract>Instruction tuning has achieved unprecedented success in NLP, turning large language models into versatile chatbots. However, the increasing variety and volume of instruction datasets demand significant computational resources. To address this, it is essential to extract a small and highly informative subset (i.e., Coreset) that achieves comparable performance to the full dataset. Achieving this goal poses non-trivial challenges: 1) data selection requires accurate data representations that reflect the training samples’ quality, 2) considering the diverse nature of instruction datasets, and 3) ensuring the efficiency of the coreset selection algorithm for large models. To address these challenges, we propose Task-Agnostic Gradient Clustered COreset Selection (TAGCOS). Specifically, we leverage sample gradients as the data representations, perform clustering to group similar data, and apply an efficient greedy algorithm for coreset selection. Experimental results show that our algorithm, selecting only 5% of the data, surpasses other unsupervised methods and achieves performance close to that of the full dataset.</abstract>
      <url hash="6ea216aa">2025.findings-naacl.264</url>
      <bibkey>zhang-etal-2025-tagcos</bibkey>
    </paper>
    <paper id="265">
      <title>From Text to Emoji: How <fixed-case>PEFT</fixed-case>-Driven Personality Manipulation Unleashes the Emoji Potential in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Navya</first><last>Jain</last></author>
      <author><first>Zekun</first><last>Wu</last><affiliation>Department of Computer Science, University College London, University of London and Holistic AI</affiliation></author>
      <author><first>Cristian Enrique Munoz</first><last>Villalobos</last></author>
      <author><first>Airlie</first><last>Hilliard</last></author>
      <author><first>Xin</first><last>Guan</last><affiliation>Holistic AI</affiliation></author>
      <author><first>Adriano</first><last>Koshiyama</last></author>
      <author><first>Emre</first><last>Kazim</last></author>
      <author><first>Philip Colin</first><last>Treleaven</last><affiliation>University College London, University of London</affiliation></author>
      <pages>4687-4723</pages>
      <abstract>The manipulation of the personality traits of large language models (LLMs) has emerged as a key area of research. Methods like prompt-based In-Context Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have been explored but show irregularity and variability; IKE depends on the prompt, leading to variability and sensitivity, while MEND yields inconsistent and gibberish outputs. To address this, we employed Opinion QA Based Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank Adaptation (QLoRA), to manipulate the Big Five personality traits: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT, models such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent behaviour by generating emojis for certain traits, despite no emojis being present in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in 99.5% of extraversion-related test instances, while Mistral-7B-Instruct did so in 92.5% of openness-related test instances. ICL Explainability analysis indicated that the LLMs used emojis intentionally to express these traits. Mechanistic Interpretability analysis showed that this latent behaviour of LLMs could be traced to specific neurons that became activated or amplified after PEFT. This paper provides a number of novel contributions. First, introducing an Opinion QA dataset for PEFT-driven personality manipulation; second, developing metric models to benchmark LLM personality traits; third, demonstrating PEFT’s superiority over IKE in personality manipulation; and finally, analysing and validating emoji usage through explainability methods such as Mechanistic Interpretability and In-context learning Explainability methods.</abstract>
      <url hash="aa4cbfb6">2025.findings-naacl.265</url>
      <bibkey>jain-etal-2025-text</bibkey>
    </paper>
    <paper id="266">
      <title>Decoding Fatphobia: Examining Anti-Fat and Pro-Thin Bias in <fixed-case>AI</fixed-case>-Generated Images</title>
      <author><first>Jane</first><last>Warren</last></author>
      <author><first>Gary M.</first><last>Weiss</last><affiliation>Fordham University</affiliation></author>
      <author><first>Fernando</first><last>Martinez</last></author>
      <author><first>Annika</first><last>Guo</last></author>
      <author><first>Yijun</first><last>Zhao</last><affiliation>Fordham University</affiliation></author>
      <pages>4724-4736</pages>
      <abstract>Existing studies have shown that AI-generated images tend to reinforce social biases, including those related to race and gender. However, no studies have investigated weight bias, or fatphobia, in AI-generated images. This study utilizes DALL-E 3 to determine the extent to which anti-fat and pro-thin biases are present in AI-generated images, and examines stereotypical associations between moral character and body weight. Four-thousand images are generated using twenty pairs of positive and negative textual prompts. These images are then manually labeled with weight information and analyzed to determine the extent to which they reflect fatphobia. The findings and their impact are discussed and related to existing research on weight bias.</abstract>
      <url hash="a8d2d69d">2025.findings-naacl.266</url>
      <bibkey>warren-etal-2025-decoding</bibkey>
    </paper>
    <paper id="267">
      <title><fixed-case>MMAU</fixed-case>: A Holistic Benchmark of Agent Capabilities Across Diverse Domains</title>
      <author><first>Guoli</first><last>Yin</last><affiliation>Apple</affiliation></author>
      <author><first>Haoping</first><last>Bai</last><affiliation>Apple</affiliation></author>
      <author><first>Shuang</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Yanchao</first><last>Sun</last><affiliation>Apple AI/ML</affiliation></author>
      <author><first>Zhaoyang</first><last>Xu</last><affiliation>Apple</affiliation></author>
      <author><first>Shen</first><last>Ma</last><affiliation>Apple</affiliation></author>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Xiang</first><last>Kong</last><affiliation>Apple</affiliation></author>
      <author><first>Aonan</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Dian Ang</first><last>Yap</last><affiliation>Apple</affiliation></author>
      <author><first>Yizhe</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Karsten</first><last>Ahnert</last><affiliation>Apple</affiliation></author>
      <author><first>Vik</first><last>Kamath</last><affiliation>Apple</affiliation></author>
      <author><first>Mathias</first><last>Berglund</last><affiliation>Apple</affiliation></author>
      <author><first>Dominic</first><last>Walsh</last><affiliation>Apple</affiliation></author>
      <author><first>Tobias</first><last>Gindele</last><affiliation>Apple</affiliation></author>
      <author><first>Juergen</first><last>Wiest</last><affiliation>Apple</affiliation></author>
      <author><first>Zhengfeng</first><last>Lai</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoming Simon</first><last>Wang</last><affiliation>Didi Research US</affiliation></author>
      <author><first>Jiulong</first><last>Shan</last><affiliation>Apple</affiliation></author>
      <author><first>Meng</first><last>Cao</last><affiliation>Apple</affiliation></author>
      <author><first>Ruoming</first><last>Pang</last><affiliation>Apple</affiliation></author>
      <author><first>Zirui</first><last>Wang</last></author>
      <pages>4737-4765</pages>
      <abstract>Recent advances in large language models (LLMs) have increased the demand for comprehensive benchmarks to evaluate their capabilities as human-like agents. Existing benchmarks, while useful, often focus on specific application scenarios, emphasizing task completion but failing to dissect the underlying skills that drive these outcomes. This lack of granularity makes it difficult to deeply discern where failures stem from. Additionally, setting up these environments requires considerable effort, and issues of unreliability and reproducibility sometimes arise, especially in interactive tasks. To address these limitations, we introduce the Massive Multitask Agent Understanding (MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need for complex environment setups. It evaluate models across five domains, including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine Learning coding, Contest-level programming and Mathematics, and covering five essential capabilities: Understanding, Reasoning, Planning, Problem-solving, and Self-correction. With a total of 20 meticulously designed tasks encompassing over 3K distinct prompts, MMAU provides a comprehensive framework for evaluating the strengths and limitations of LLM agents. By testing 20 representative models on MMAU, we provide deep and insightful analyses. Ultimately, MMAU not only sheds light on the capabilities and limitations of LLM agents but also enhances the interpretability of their performance.</abstract>
      <url hash="9c64afe6">2025.findings-naacl.267</url>
      <bibkey>yin-etal-2025-mmau</bibkey>
    </paper>
    <paper id="268">
      <title>Improving Consistency in <fixed-case>LLM</fixed-case> Inference using Probabilistic Tokenization</title>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>4766-4778</pages>
      <abstract>Prior research has demonstrated noticeable performance gains through the use of probabilistic tokenizations, an approach that involves employing multiple tokenizations of the same input string during the training phase of a language model. Despite these promising findings, modern large language models (LLMs) have yet to be trained using probabilistic tokenizations. Interestingly, while the tokenizers of these contemporary LLMs have the capability to generate multiple tokenizations, this property remains underutilized.In this work, we propose a novel method to leverage the multiple tokenization capabilities of modern LLM tokenizers, aiming to enhance the self-consistency of LLMs in reasoning tasks. Our experiments indicate that when utilizing probabilistic tokenizations, LLMs generate logically diverse reasoning paths, moving beyond mere surface-level linguistic diversity. We carefully study probabilistic tokenization and offer insights to explain the self consistency improvements it brings through extensive experimentation on 5 LLM families and 4 reasoning benchmarks.</abstract>
      <url hash="8f3672ad">2025.findings-naacl.268</url>
      <bibkey>sathe-etal-2025-improving</bibkey>
    </paper>
    <paper id="269">
      <title><fixed-case>W</fixed-case>ord<fixed-case>G</fixed-case>ame: Efficient &amp; Effective <fixed-case>LLM</fixed-case> Jailbreak via Simultaneous Obfuscation in Query and Response</title>
      <author><first>Tianrong</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Yuanpu</first><last>Cao</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lu</first><last>Lin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4779-4807</pages>
      <abstract>The recent breakthrough in large language models (LLMs) such as ChatGPT has revolutionized every industry at an unprecedented pace. Alongside this progress also comes mounting concerns about LLMs’ susceptibility to jailbreaking attacks, which leads to the generation of harmful or unsafe content. While safety alignment measures have been implemented in LLMs to mitigate existing jailbreak attempts and force them to become increasingly complicated, it is still far from perfect. In this paper, we analyze the common pattern of the current safety alignment and show that it is possible to exploit such patterns for jailbreaking attacks by simultaneous obfuscation in queries and responses. Specifically, we propose WordGame attack, which replaces malicious words with word games to break down the adversarial intent of a query and encourage benign content regarding the games to precede the anticipated harmful content in the response, creating a context that is hardly covered by any corpus used for safety alignment. Extensive experiments demonstrate that WordGame attack can break the guardrails of the current leading proprietary and open-source LLMs, including the latest Claude 3, GPT 4, and Llama 3 models more effectively than existing attacks efficiently. The attack also remains powerful when external defenses are adopted. Further ablation studies on such simultaneous obfuscation in query and response provide evidence of the merits of the attack strategy beyond an individual attack.</abstract>
      <url hash="d6a00a9a">2025.findings-naacl.269</url>
      <bibkey>zhang-etal-2025-wordgame</bibkey>
    </paper>
    <paper id="270">
      <title>Human and <fixed-case>LLM</fixed-case>-Based Resume Matching: An Observational Study</title>
      <author><first>Swanand</first><last>Vaishampayan</last></author>
      <author><first>Hunter</first><last>Leary</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Yoseph Berhanu</first><last>Alebachew</last></author>
      <author><first>Louis</first><last>Hickman</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Brent A.</first><last>Stevenor</last><affiliation>NREMT</affiliation></author>
      <author><first>Weston</first><last>Beck</last><affiliation>Discovered</affiliation></author>
      <author><first>Chris</first><last>Brown</last><affiliation>Virginia Tech</affiliation></author>
      <pages>4808-4823</pages>
      <abstract>Resume matching assesses the extent to which candidates qualify for jobs based on the content of resumes. This process increasingly uses natural language processing (NLP) techniques to automate parsing and rating tasks—saving time and effort. Large language models (LLMs) are increasingly used for this purpose—thus, we explore their capabilities for resume matching in an observational study. We compare zero-shot GPT-4 and human ratings for 736 resumes submitted to job openings from diverse fields using real-world evaluation criteria. We also study the effects of prompt engineering techniques on GPT-4 ratings and compare differences in GPT-4 and human ratings across racial and gender groups. Our results show: LLM scores correlate minorly with humans, suggesting they are not interchangeable; prompt engineering such as CoT improves the quality of LLM ratings; and LLM scores do not show larger group differences (i.e., bias) than humans. Our findings provide implications for LLM-based resume rating to promote more fair and NLP-based resume matching in a multicultural world.</abstract>
      <url hash="dfa858ca">2025.findings-naacl.270</url>
      <bibkey>vaishampayan-etal-2025-human</bibkey>
    </paper>
    <paper id="271">
      <title>A Practical Examination of <fixed-case>AI</fixed-case>-Generated Text Detectors for Large Language Models</title>
      <author><first>Brian</first><last>Tufts</last></author>
      <author><first>Xuandong</first><last>Zhao</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>4824-4841</pages>
      <abstract>The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank, Binoculars) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate practical adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.</abstract>
      <url hash="6ae6fd80">2025.findings-naacl.271</url>
      <bibkey>tufts-etal-2025-practical</bibkey>
    </paper>
    <paper id="272">
      <title>Robust Bias Detection in <fixed-case>MLM</fixed-case>s and its Application to Human Trait Ratings</title>
      <author><first>Ingroj</first><last>Shrestha</last></author>
      <author><first>Louis</first><last>Tay</last><affiliation>Purdue University</affiliation></author>
      <author><first>Padmini</first><last>Srinivasan</last><affiliation>University of Iowa</affiliation></author>
      <pages>4842-4858</pages>
      <abstract>There has been significant prior work using templates to study bias against demographic attributes in MLMs. However, these have limitations: they overlook random variability of templates and target concepts analyzed, assume equality amongst templates, and overlook bias quantification. Addressing these, we propose a systematic statistical approach to assess bias in MLMs, using mixed models to account for random effects, pseudo-perplexity weights for sentences derived from templates and quantify bias using statistical effect sizes. Replicating prior studies, we match on bias scores in magnitude and direction with small to medium effect sizes.Next, we explore the novel problem of gender bias in the context of *personality* and *character* traits, across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased for binary gender but the most biased for non-binary *neo*, while RoBERTa-large is the most biased for binary gender but shows small to no bias for *neo*. There is some alignment of MLM bias and findings in psychology (human perspective) - in *agreeableness* with RoBERTa-large and *emotional stability* with BERT-large. There is general agreement for the remaining 3 personality dimensions: both sides observe at most small differences across gender. For character traits, human studies on gender bias are limited thus comparisons are not feasible.</abstract>
      <url hash="06bc7807">2025.findings-naacl.272</url>
      <bibkey>shrestha-etal-2025-robust</bibkey>
    </paper>
    <paper id="273">
      <title>How Inclusively do <fixed-case>LM</fixed-case>s Perceive Social and Moral Norms?</title>
      <author><first>Michael</first><last>Galarnyk</last></author>
      <author><first>Agam</first><last>Shah</last></author>
      <author><first>Dipanwita</first><last>Guhathakurta</last></author>
      <author><first>Poojitha</first><last>Nandigam</last></author>
      <author><first>Sudheer</first><last>Chava</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>4859-4869</pages>
      <abstract>**This paper discusses and contains offensive content.** Language models (LMs) are used in decision-making systems and as interactive assistants. However, how well do these models making judgements align with the diversity of human values, particularly regarding social and moral norms? In this work, we investigate how inclusively LMs perceive norms across demographic groups (e.g., gender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare their outputs with the existing responses of 100 human annotators. We introduce the Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on ordinal questions. We find notable disparities in LM responses, with younger, higher-income groups showing closer alignment, raising concerns about the representation of marginalized perspectives. Our findings highlight the importance of further efforts to make LMs more inclusive of diverse human values. The code and prompts are available on GitHub under the CC BY-NC 4.0 license.</abstract>
      <url hash="2cf7ea83">2025.findings-naacl.273</url>
      <bibkey>galarnyk-etal-2025-inclusively</bibkey>
    </paper>
    <paper id="274">
      <title>Jailbreaking with Universal Multi-Prompts</title>
      <author><first>Yu-Ling</first><last>Hsu</last></author>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Shang-Tse</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>4870-4891</pages>
      <abstract>Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.</abstract>
      <url hash="fceb088f">2025.findings-naacl.274</url>
      <bibkey>hsu-etal-2025-jailbreaking</bibkey>
    </paper>
    <paper id="275">
      <title>Echoes of Discord: Forecasting Hater Reactions to Counterspeech</title>
      <author><first>Xiaoying</first><last>Song</last></author>
      <author><first>Sharon Lisseth</first><last>Perez</last></author>
      <author><first>Xinchen</first><last>Yu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Lingzi</first><last>Hong</last><affiliation>University of North Texas</affiliation></author>
      <pages>4892-4905</pages>
      <abstract>Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division. Counterspeech has been recognized as a way to mitigate the harmful consequences. While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters’ reactions toward counterspeech, despite the immediate alteration of haters’ attitudes being an important aspect of counterspeech. This study fills the gap by analyzing the impact of counterspeech from the hater’s perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful. We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters’ reactions, to assess the impact of counterspeech. To predict haters’ behaviors, we employ two strategies: a two-stage reaction predictor and a three-way classifier. The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters’ reactions. Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type. We conclude the study with an assessment showing the most common errors identified by the best-performing model.</abstract>
      <url hash="003ea5a7">2025.findings-naacl.275</url>
      <bibkey>song-etal-2025-echoes</bibkey>
    </paper>
    <paper id="276">
      <title>Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy</title>
      <author><first>Athiya</first><last>Deviyani</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Fernando</first><last>Diaz</last><affiliation>Carnegie Mellon University and Google</affiliation></author>
      <pages>4906-4925</pages>
      <abstract>Meta-evaluation of automatic evaluation metrics—assessing evaluation metrics themselves—is crucial for accurately benchmarking natural language processing systems and has implications for scientific inquiry, production model development, and policy enforcement. While existing approaches to metric meta-evaluation focus on general statements about the absolute and relative quality of metrics across arbitrary system outputs, in practice, metrics are applied in highly contextual settings, often measuring the performance for a highly constrained set of system outputs. For example, we may only be interested in evaluating a specific model or class of models. We introduce a method for contextual metric meta-evaluation by comparing the local metric accuracy of evaluation metrics. Across translation, speech recognition, and ranking tasks, we demonstrate that the local metric accuracies vary both in absolute value and relative effectiveness as we shift across evaluation contexts. This observed variation highlights the importance of adopting context-specific metric evaluations over global ones.</abstract>
      <url hash="2d4424e6">2025.findings-naacl.276</url>
      <bibkey>deviyani-diaz-2025-contextual</bibkey>
    </paper>
    <paper id="277">
      <title>Advocating Character Error Rate for Multilingual <fixed-case>ASR</fixed-case> Evaluation</title>
      <author><first>Thennal D</first><last>K</last></author>
      <author><first>Jesin</first><last>James</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Deepa Padmini</first><last>Gopinath</last></author>
      <author><first>Muhammed Ashraf</first><last>K</last></author>
      <pages>4926-4935</pages>
      <abstract>Automatic speech recognition (ASR) systems have traditionally been evaluated using English datasets, with the word error rate (WER) serving as the predominant metric. WER’s simplicity and ease of interpretation have contributed to its widespread adoption, particularly for English. However, as ASR systems expand to multilingual contexts, WER fails in various ways, particularly with morphologically complex languages or those without clear word boundaries. Our work documents the limitations of WER as an evaluation metric and advocates for the character error rate (CER) as the primary metric in multilingual ASR evaluation. We show that CER avoids many of the challenges WER faces and exhibits greater consistency across writing systems. We support our proposition by conducting human evaluations of ASR transcriptions in three languages—Malayalam, English, and Arabic—which exhibit distinct morphological characteristics. We show that CER correlates more closely with human judgments than WER, even for English. To facilitate further research, we release our human evaluation dataset for future benchmarking of ASR metrics. Our findings suggest that CER should be prioritized, or at least supplemented, in multilingual ASR evaluations to account for the varying linguistic characteristics of different languages.</abstract>
      <url hash="743fb276">2025.findings-naacl.277</url>
      <bibkey>k-etal-2025-advocating</bibkey>
    </paper>
    <paper id="278">
      <title>Enhancing Temporal Understanding in <fixed-case>LLM</fixed-case>s for Semi-structured Tables</title>
      <author><first>Irwin</first><last>Deng</last></author>
      <author><first>Kushagra</first><last>Dixit</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <pages>4936-4955</pages>
      <abstract>Temporal reasoning over tabular data presents substantial challenges for large language models (LLMs), as evidenced by recent research. In this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific limitations of LLMs. Our investigation leads to enhancements in TempTabQA, a benchmark specifically designed for tabular temporal question answering. We provide critical insights for enhancing LLM performance in temporal reasoning tasks with tabular data. Furthermore, we introduce a novel approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings demonstrate that our method improves evidence-based reasoning across various models. Additionally, our experimental results reveal that indirect supervision with auxiliary unstructured data (TRAM) substantially boosts model performance in these tasks. This work contributes to a deeper understanding of LLMs’ temporal reasoning abilities over tabular data and promotes advancements in their application across diverse fields.</abstract>
      <url hash="11c475ea">2025.findings-naacl.278</url>
      <bibkey>deng-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="279">
      <title><fixed-case>B</fixed-case>n<fixed-case>TTS</fixed-case>: Few-Shot Speaker Adaptation in Low-Resource Setting</title>
      <author><first>Mohammad Jahid Ibna</first><last>Basher</last><affiliation>Chittagong University of Engineering and Technology</affiliation></author>
      <author><first>Md</first><last>Kowsher</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Rabindra Nath</first><last>Nandi</last><affiliation>Hishab Singapure Pte. Ltd</affiliation></author>
      <author><first>Nusrat Jahan</first><last>Prottasha</last></author>
      <author><first>Mehadi Hasan</first><last>Menon</last></author>
      <author><first>Tareq Al</first><last>Muntasir</last></author>
      <author><first>Shammur Absar</first><last>Chowdhury</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Niloofar</first><last>Yousefi</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Ozlem</first><last>Garibay</last><affiliation>University of Central Florida</affiliation></author>
      <pages>4956-4968</pages>
      <abstract>This paper introduces BnTTS (Bangla Text-To-Speech), the first framework for Bangla speaker adaptation-based TTS, designed to bridge the gap in Bangla speech synthesis using minimal training data. Building upon the XTTS architecture, our approach integrates Bangla into a multilingual TTS pipeline, with modifications to account for the phonetic and linguistic characteristics of the language. We pretrain BnTTS on 3.85k hours of Bangla speech dataset with corresponding text labels and evaluate performance in both zero-shot and few-shot settings on our proposed test dataset. Empirical evaluations in few-shot settings show that BnTTS significantly improves the naturalness, intelligibility, and speaker fidelity of synthesized Bangla speech. Compared to state-of-the-art Bangla TTS systems, BnTTS exhibits superior performance in Subjective Mean Opinion Score (SMOS), Naturalness, and Clarity metrics.</abstract>
      <url hash="c69ac0c5">2025.findings-naacl.279</url>
      <bibkey>basher-etal-2025-bntts</bibkey>
    </paper>
    <paper id="280">
      <title>Playing with Voices: Tabletop Role-Playing Game Recordings as a Diarization Challenge</title>
      <author><first>Lian</first><last>Remme</last></author>
      <author><first>Kevin</first><last>Tang</last><affiliation>Heinrich Heine University Düsseldorf and University of Florida</affiliation></author>
      <pages>4969-4983</pages>
      <abstract>This paper provides a proof of concept that audio of tabletop role-playing games (TTRPG) could serve as a challenge for diarization systems. TTRPGs are carried out mostly by conversation. Participants often alter their voices to indicate that they are talking as a fictional character. Audio processing systems are susceptible to voice conversion with or without technological assistance. TTRPG present a conversational phenomenon in which voice conversion is an inherent characteristic for an immersive gaming experience. This could make it more challenging for diarizers to pick the real speaker and determine that impersonating is just that. We present the creation of a small TTRPG audio dataset and compare it against the AMI and the ICSI corpus. The performance of two diarizers, pyannote.audio and wespeaker, were evaluated. We observed that TTRPGs’ properties result in a higher confusion rate for both diarizers.Additionally, wespeaker strongly underestimates the number of speakers in the TTRPG audio files.We propose TTRPG audio as a promising challenge for diarization systems.</abstract>
      <url hash="4bd05e57">2025.findings-naacl.280</url>
      <bibkey>remme-tang-2025-playing</bibkey>
    </paper>
    <paper id="281">
      <title>Causally Testing Gender Bias in <fixed-case>LLM</fixed-case>s: A Case Study on Occupational Bias</title>
      <author><first>Yuen</first><last>Chen</last></author>
      <author><first>Vethavikashini Chithrra</first><last>Raghuram</last><affiliation>CCC Intelligent Solutions</affiliation></author>
      <author><first>Justus</first><last>Mattern</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Zhijing</first><last>Jin</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>4984-5004</pages>
      <abstract>Generated texts from large language models (LLMs) have been shown to exhibit a variety of harmful, human-like biases against various demographics. These findings motivate research efforts aiming to understand and measure such effects. This paper introduces a causal formulation for bias measurement in generative language models. Based on this theoretical foundation, we outline a list of desiderata for designing robust bias benchmarks. We then propose a benchmark called OccuGender, with a bias-measuring procedure to investigate occupational gender bias. We test several state-of-the-art open-source LLMs on OccuGender, including Llama, Mistral, and their instruction-tuned versions. The results show that these models exhibit substantial occupational gender bias. Lastly, we discuss prompting strategies for bias mitigation and an extension of our causal formulation to illustrate the generalizability of our framework.</abstract>
      <url hash="9610e537">2025.findings-naacl.281</url>
      <bibkey>chen-etal-2025-causally</bibkey>
    </paper>
    <paper id="282">
      <title><fixed-case>OLMES</fixed-case>: A Standard for Language Model Evaluations</title>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Dany</first><last>Haddad</last></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>5005-5033</pages>
      <abstract>Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models can be particularly challenging, as choices of how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural “cloze” formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered, documented recommendations guided by results from existing literature as well as new experiments resolving open questions.</abstract>
      <url hash="22909d22">2025.findings-naacl.282</url>
      <bibkey>gu-etal-2025-olmes</bibkey>
    </paper>
    <paper id="283">
      <title>Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning</title>
      <author><first>Joy</first><last>Crosbie</last></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>5034-5096</pages>
      <abstract>Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model’s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that the induction mechanism plays in ICL.</abstract>
      <url hash="2bc1e3bf">2025.findings-naacl.283</url>
      <bibkey>crosbie-shutova-2025-induction</bibkey>
    </paper>
    <paper id="284">
      <title><fixed-case>M</fixed-case>o<fixed-case>LA</fixed-case>: <fixed-case>M</fixed-case>o<fixed-case>E</fixed-case> <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> with Layer-wise Expert Allocation</title>
      <author><first>Chongyang</first><last>Gao</last></author>
      <author><first>Kezhen</first><last>Chen</last><affiliation>Together AI</affiliation></author>
      <author><first>Jinmeng</first><last>Rao</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Baochen</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Yawen</first><last>Zhang</last><affiliation>Google X, Mineral.ai</affiliation></author>
      <author><first>Daiyi</first><last>Peng</last></author>
      <author><first>Xiaoyuan</first><last>Guo</last><affiliation>Mineral.ai</affiliation></author>
      <author><first>Vs</first><last>Subrahmanian</last><affiliation>Northwestern University</affiliation></author>
      <pages>5097-5112</pages>
      <abstract>Recent efforts to integrate low-rank adaptation (LoRA) with the Mixture-of-Experts (MoE) have managed to achieve performance comparable to full-parameter fine-tuning by tuning much fewer parameters. Despite promising results, research on improving the efficiency and expert analysis of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, <i>
          <b>M</b>oE-L<b>o</b>RA with <b>L</b>ayer-wise Expert <b>A</b>llocation (MoLA)</i> for Transformer-based models, where each model layer uses a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines on top of both LLAMA-2, Mistral, and Gemma. We find that allocating more LoRA experts to middle layers further enhances the effectiveness of models with a certain number of experts in total. The redundancy of the experts is more obvious in the lower layers. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code has been made available at <url>https://github.com/GCYZSL/MoLA</url>.</abstract>
      <url hash="ac32bc2b">2025.findings-naacl.284</url>
      <bibkey>gao-etal-2025-mola</bibkey>
    </paper>
    <paper id="285">
      <title><fixed-case>C</fixed-case>ode<fixed-case>S</fixed-case>im: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging</title>
      <author><first>Md. Ashraful</first><last>Islam</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Mohammed Eunus</first><last>Ali</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>5113-5139</pages>
      <url hash="74ff4b22">2025.findings-naacl.285</url>
      <bibkey>islam-etal-2025-codesim</bibkey>
    </paper>
    <paper id="286">
      <title>On the Feasibility of In-Context Probing for Data Attribution</title>
      <author><first>Cathy</first><last>Jiao</last></author>
      <author><first>Weizhen</first><last>Gao</last></author>
      <author><first>Aditi</first><last>Raghunathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>5140-5155</pages>
      <abstract>Data attribution methods are used to measure the contribution of training data towards model outputs, and have several important applications in areas such as dataset curation and model interpretability. However, many standard data attribution methods, such as influence functions, utilize model gradients and are computationally expensive. In our paper, we show in-context probing (ICP) – prompting a LLM – can serve as a fast proxy for gradient-based data attribution for data selection under conditions contingent on data similarity. We study this connection empirically on standard NLP tasks, and show that ICP and gradient-based data attribution are well-correlated in identifying influential training data for tasks that share similar task type and content as the training data. Additionally, fine-tuning models on influential data selected by both methods achieves comparable downstream performance, further emphasizing their similarities. We then examine the connection between ICP and gradient-based data attribution using synthetic data on linear regression tasks. Our synthetic data experiments show similar results with those from NLP tasks, suggesting that this connection can be isolated in simpler settings, which offers a pathway to bridging their differences.</abstract>
      <url hash="bd20ff71">2025.findings-naacl.286</url>
      <bibkey>jiao-etal-2025-feasibility</bibkey>
    </paper>
    <paper id="287">
      <title>Evaluation of Multilingual Image Captioning: How far can we get with <fixed-case>CLIP</fixed-case> models?</title>
      <author><first>Goncalo Emanuel Cavaco</first><last>Gomes</last></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <pages>5156-5175</pages>
      <abstract>The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.</abstract>
      <url hash="1b6c2762">2025.findings-naacl.287</url>
      <bibkey>gomes-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="288">
      <title>Avoiding Copyright Infringement via Large Language Model Unlearning</title>
      <author><first>Guangyao</first><last>Dou</last></author>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Qing</first><last>Lyu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Eric</first><last>Wong</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>5176-5200</pages>
      <abstract>Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. In real-world scenarios, model owners need to continuously address copyright infringement as new requests for content removal emerge at different time points. This leads to the need for sequential unlearning, where copyrighted content is removed sequentially as new requests arise. Despite its practical relevance, sequential unlearning in the context of copyright infringement has not been rigorously explored in existing literature. To address this gap, we propose Stable Sequential Unlearning (SSU), a novel framework designed to unlearn copyrighted content from LLMs over multiple time steps. Our approach works by identifying and removing specific weight updates in the model’s parameters that correspond to copyrighted content. We improve unlearning efficacy by introducing random labeling loss and ensuring the model retains its general-purpose knowledge by adjusting targeted parameters. Experimental results show that SSU achieves an effective trade-off between unlearning efficacy and general-purpose language abilities, outperforming existing baselines.</abstract>
      <url hash="10796bf0">2025.findings-naacl.288</url>
      <bibkey>dou-etal-2025-avoiding</bibkey>
    </paper>
    <paper id="289">
      <title>A Context-Aware Contrastive Learning Framework for Hateful Meme Detection and Segmentation</title>
      <author><first>Xuanyu</first><last>Su</last></author>
      <author><first>Yansong</first><last>Li</last></author>
      <author><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Nathalie</first><last>Japkowicz</last><affiliation>American University</affiliation></author>
      <pages>5201-5215</pages>
      <abstract>Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within “Confounder Memes”. To address this, we introduce HateSieve, a new framework designed to enhance the detection and segmentation of hateful elements in memes. HateSieve features a novel Contrastive Meme Generator that creates semantically correlated memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments show that HateSieve not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. Caution: Contains academic discussions of hate speech; viewer discretion advised.</abstract>
      <url hash="9563ec96">2025.findings-naacl.289</url>
      <bibkey>su-etal-2025-context</bibkey>
    </paper>
    <paper id="290">
      <title><fixed-case>LLM</fixed-case>-Generated Passphrases That Are Secure and Easy to Remember</title>
      <author><first>Jie S.</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jonas</first><last>Geiping</last><affiliation>ELLIS Institute Tübingen and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <author><first>Micah</first><last>Goldblum</last><affiliation>Columbia University</affiliation></author>
      <author><first>Aniruddha</first><last>Saha</last></author>
      <author><first>Tom</first><last>Goldstein</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>5216-5234</pages>
      <abstract>Automatically generated passwords and passphrases are a cornerstone of IT security. Yet, these passphrases are often hard to remember and see only limited adoption. In this work, we use large language models to generate passphrases with rigorous security guarantees via the computation of the entropy of the output as a metric of the security of the passphrase. We then present a range of practical methods to generate language model outputs with sufficient entropy: raising entropy through in-context examples and generation through a new top-q truncation method. We further verify the influence of prompt construction in steering the output topic and grammatical structure. Finally, we conduct user studies to determine the adoption rates for these LLM-generated passphrases in practice.</abstract>
      <url hash="5be85106">2025.findings-naacl.290</url>
      <bibkey>li-etal-2025-llm</bibkey>
    </paper>
    <paper id="291">
      <title>Does Data Contamination Detection Work (Well) for <fixed-case>LLM</fixed-case>s? A Survey and Evaluation on Detection Assumptions</title>
      <author><first>Yujuan</first><last>Fu</last></author>
      <author><first>Ozlem</first><last>Uzuner</last><affiliation>George Mason University</affiliation></author>
      <author><first>Meliha</first><last>Yetisgen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>5235-5256</pages>
      <abstract>Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.</abstract>
      <url hash="1995f97b">2025.findings-naacl.291</url>
      <bibkey>fu-etal-2025-data</bibkey>
    </paper>
    <paper id="292">
      <title>Representation-to-Creativity (<fixed-case>R</fixed-case>2<fixed-case>C</fixed-case>): Automated Holistic Scoring Model for Essay Creativity</title>
      <author><first>Deokgi</first><last>Kim</last></author>
      <author><first>Joonyoung</first><last>Jo</last></author>
      <author><first>Byung-Won</first><last>On</last><affiliation>Kunsan National University</affiliation></author>
      <author><first>Ingyu</first><last>Lee</last><affiliation>Yeungnam University</affiliation></author>
      <pages>5257-5275</pages>
      <abstract>Despite active research on Automated Essay Scoring (AES), there is a noticeable scarcity of studies focusing on predicting creativity scores for essays. In this study, we develop a new essay rubric specifically designed for assessing creativity in essays. Leveraging this rubric, we construct ground truth data consisting of 5,048 essays. Furthermore, we propose a novel self-supervised learning model that recognizes cluster patterns within the essay embedding space and leverages them for creativity scoring. This approach aims to automatically generate a high-quality training set, thereby facilitating the training of diverse language models. Our experimental findings indicated a substantial enhancement in the assessment of essay creativity, demonstrating an increase in F1-score up to 58% compared to the primary state-of-the-art models across the ASAP and AIHUB datasets.</abstract>
      <url hash="75bd02aa">2025.findings-naacl.292</url>
      <bibkey>kim-etal-2025-representation</bibkey>
    </paper>
    <paper id="293">
      <title>From Single to Multi: How <fixed-case>LLM</fixed-case>s Hallucinate in Multi-Document Summarization</title>
      <author><first>Catarina G</first><last>Belém</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Hayate</first><last>Iso</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Seiji</first><last>Maekawa</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last><affiliation>Megagon Labs, Inc</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs, Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>5276-5309</pages>
      <abstract>Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from a set of documents. Since no benchmarks exist for investigating hallucinations in MDS, we leverage existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, GPT-3.5-turbo and GPT-4o still generate summaries about 79.45% and 44% of the time, raising concerns about their tendency to fabricate content. To better understand the characteristics of these hallucinations, we conduct a human evaluation of 700+ insights and discover that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches that systematically mitigate hallucinations in MDS.</abstract>
      <url hash="a8c47e2a">2025.findings-naacl.293</url>
      <bibkey>belem-etal-2025-single</bibkey>
    </paper>
    <paper id="294">
      <title>Aligning to Constraints for Data-Efficient Language Model Customization</title>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chao</first><last>Shang</last><affiliation>Amazon AWS AI</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sarthak</first><last>Jain</last><affiliation>Profluent Bio</affiliation></author>
      <author><first>Qiang</first><last>Ning</last><affiliation>Jump Trading</affiliation></author>
      <author><first>Bonan</first><last>Min</last><affiliation>Amazon and Tufts University</affiliation></author>
      <author><first>Vittorio</first><last>Castelli</last><affiliation>Amazon</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5310-5325</pages>
      <abstract>General-purpose language models (LMs) are aligned to diverse user intents, but fall short when it comes to specific applications. While finetuning is the default method for customized alignment, human annotations are often unavailable in various customization scenarios. Based on the observation that one of the main issues of LM customization is constraint adherence, we investigate the feasibility of using constraints as a bridge from general LMs to customized ones. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs’ capability to adhere to different classes of constraints, thereby improving task performance comparable to or approaching that of finetuning with labeled data.</abstract>
      <url hash="74ba2c5c">2025.findings-naacl.294</url>
      <bibkey>wang-etal-2025-aligning</bibkey>
    </paper>
    <paper id="295">
      <title>Where is this coming from? Making groundedness count in the evaluation of Document <fixed-case>VQA</fixed-case> models</title>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>School of Computer Science, Carnegie Mellon University and J.P. Morgan Chase</affiliation></author>
      <author><first>Siddharth</first><last>Parekh</last></author>
      <author><first>Pranav</first><last>Shetty</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Zhao</first><last>Jin</last></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>5326-5346</pages>
      <abstract>Document Visual Question Answering (VQA) models have evolved at an impressive rate over the past few years, coming close to or matching human performance on some benchmarks. We argue that common evaluation metrics used by popular benchmarks do not account for the semantic and multimodal groundedness of a model’s outputs. As a result, hallucinations and major semantic errors are treated the same way as well-grounded outputs, and the evaluation scores do not reflect the reasoning capabilities of the model. In response, we propose a new evaluation methodology that accounts for the groundedness of predictions with regard to the semantic characteristics of the output as well as the multimodal placement of the output within the input document. Our proposed methodology is parameterized in such a way that users can configure the score according to their preferences. We validate our scoring methodology using human judgment and show its potential impact on existing popular leaderboards. Through extensive analyses, we demonstrate that our proposed method produces scores that are a better indicator of a model’s robustness and tends to give higher rewards to better-calibrated answers.</abstract>
      <url hash="f6d41bf5">2025.findings-naacl.295</url>
      <bibkey>nourbakhsh-etal-2025-coming</bibkey>
    </paper>
    <paper id="296">
      <title>Transformer-based Causal Language Models Perform Clustering</title>
      <author><first>Xinbo</first><last>Wu</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Lav R.</first><last>Varshney</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>5347-5372</pages>
      <abstract>Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still an area of active development. Recent works (Ouyang et al., 2022; Rafailov et al., 2023; Zhang et al., 2023) have shown great improvements in instruction-following capability through additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances, and validate our results in a more realistic setting. We further present applications in pre-training and alignment, inspired by clustering.</abstract>
      <url hash="8de7f6ce">2025.findings-naacl.296</url>
      <bibkey>wu-varshney-2025-transformer</bibkey>
    </paper>
    <paper id="297">
      <title>Towards Better Multi-task Learning: A Framework for Optimizing Dataset Combinations in Large Language Models</title>
      <author><first>Zaifu</first><last>Zhan</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <pages>5373-5386</pages>
      <abstract>To efficiently select optimal dataset combinations for enhancing multi-task learning (MTL) performance in large language models, we proposed a novel framework that leverages a neural network to predict the best dataset combinations. The framework iteratively refines the selection, greatly improving efficiency, while being model-, dataset-, and domain-independent. Through experiments on 12 biomedical datasets across four tasks—named entity recognition, relation extraction, event extraction, and text classification—we demonstrate that our approach effectively identifies better combinations, even for tasks that may seem unpromising from a human perspective. This verifies that our framework provides a promising solution for maximizing MTL potential.</abstract>
      <url hash="0f9eb0a5">2025.findings-naacl.297</url>
      <bibkey>zhan-zhang-2025-towards</bibkey>
    </paper>
    <paper id="298">
      <title>Gender Bias in Instruction-Guided Speech Synthesis Models</title>
      <author><first>Chun-Yi</first><last>Kuan</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>5387-5413</pages>
      <abstract>Recent advancements in controllable expressive speech synthesis, especially in text-to-speech (TTS) models, have allowed for the generation of speech with specific styles guided by textual descriptions, known as style prompts. While this development enhances the flexibility and naturalness of synthesized speech, there remains a significant gap in understanding how these models handle vague or abstract style prompts. This study investigates the potential gender bias in how models interpret occupation-related prompts, specifically examining their responses to instructions like “Act like a nurse”. We explore whether these models exhibit tendencies to amplify gender stereotypes when interpreting such prompts. Our experimental results reveal the model’s tendency to exhibit gender bias for certain occupations. Moreover, models of different sizes show varying degrees of this bias across these occupations.</abstract>
      <url hash="ce2949f1">2025.findings-naacl.298</url>
      <bibkey>kuan-lee-2025-gender</bibkey>
    </paper>
    <paper id="299">
      <title><fixed-case>R</fixed-case>eso<fixed-case>F</fixed-case>ilter: Fine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis</title>
      <author><first>Zeao</first><last>Tu</last><affiliation>Tomorrow Advancing Life</affiliation></author>
      <author><first>Xiangdi</first><last>Meng</last></author>
      <author><first>Yu</first><last>He</last></author>
      <author><first>Zihan</first><last>Yao</last></author>
      <author><first>Tianyu</first><last>Qi</last></author>
      <author><first>Jun</first><last>Liu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Ming</first><last>Li</last></author>
      <pages>5414-5428</pages>
      <abstract>Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.</abstract>
      <url hash="cc64745c">2025.findings-naacl.299</url>
      <bibkey>tu-etal-2025-resofilter</bibkey>
    </paper>
    <paper id="300">
      <title><fixed-case>UCFE</fixed-case>: A User-Centric Financial Expertise Benchmark for Large Language Models</title>
      <author><first>Yuzhe</first><last>Yang</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Nanjing university and The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Yan</first><last>Hu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yilin</first><last>Guo</last></author>
      <author><first>Ruoli</first><last>Gan</last></author>
      <author><first>Yueru</first><last>He</last></author>
      <author><first>Mingcong</first><last>Lei</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Qianqian</first><last>Xie</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Honghai</first><last>Yu</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>5429-5448</pages>
      <abstract>This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 11 LLMs services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial domain but also provides a robust framework for assessing their performance and user satisfaction.</abstract>
      <url hash="2befe48b">2025.findings-naacl.300</url>
      <bibkey>yang-etal-2025-ucfe</bibkey>
    </paper>
    <paper id="301">
      <title><fixed-case>BRIEF</fixed-case>: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression</title>
      <author><first>Yuankai</first><last>Li</last></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5449-5470</pages>
      <abstract>Retrieval-augmented generation (RAG) can supplement large language models (LLMs) by integrating external knowledge. However, as the number of retrieved documents increases, the input length to LLMs grows linearly, causing a dramatic increase in latency and a degradation in long-context understanding. This is particularly serious for multi-hop questions that require a chain of reasoning across documents. To accelerate inference, reduce costs, and minimize distractions, this paper presents BRIEF (Bridging Retrieval and Inference through Evidence Fusion), a lightweight approach that performs query-aware multi-hop reasoning by compressing retrieved documents into highly dense textual summaries to integrate into in-context RAG. To enable learning compression for multi-hop reasoning, we curate synthetic data by extracting atomic propositions that encapsulate distinct factoids from the source documents to compose synthetic summaries. Based on our synthetic data built entirely by open-source models, BRIEF generates more concise summaries and enables a range of LLMs to achieve exceptional open-domain question answering (QA) performance. For example, on HotpotQA, BRIEF improves the compression rate by 2 times compared to the state-of-the-art baseline, while outperforming it by 3.00% EM and 4.16% F1 with Flan-UL2 as the reader model. It also generates more concise summaries than proprietary GPT-3.5, while demonstrating nearly identical QA performance.</abstract>
      <url hash="534fa76d">2025.findings-naacl.301</url>
      <bibkey>li-etal-2025-brief</bibkey>
    </paper>
    <paper id="302">
      <title>An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via <fixed-case>LLM</fixed-case> as Optimizer</title>
      <author><first>Weipeng</first><last>Jiang</last></author>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Juan</first><last>Zhai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zhengyu</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>5471-5483</pages>
      <abstract>Despite prior safety alignment efforts, LLMs can still generate harmful and unethical content when subjected to jailbreaking attacks. Existing jailbreaking methods fall into two main categories: template-based and optimization-based methods. The former requires significant manual effort and domain knowledge, while the latter, exemplified by GCG, which seeks to maximize the likelihood of harmful LLM outputs through token-level optimization, also encounters several limitations: requiring white-box access, necessitating pre-constructed affirmative phrase, and suffering from low efficiency. This paper introduces ECLIPSE, a novel and efficient black-box jailbreaking method with optimizable suffixes. We employ task prompts to translate jailbreaking objectives into natural language instructions, guiding LLMs to generate adversarial suffixes for malicious queries. A harmfulness scorer provides continuous feedback, enabling LLM self-reflection and iterative optimization to autonomously produce effective suffixes. Experimental results demonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92 across three open-source LLMs and GPT-3.5-Turbo, significantly outperforming GCG by 2.4 times. Moreover, ECLIPSE matches template-based methods in ASR while substantially reducing average attack overhead by 83%, offering superior attack efficiency.</abstract>
      <url hash="3a4dfacc">2025.findings-naacl.302</url>
      <bibkey>jiang-etal-2025-optimizable</bibkey>
    </paper>
    <paper id="303">
      <title>Multi-Stage <fixed-case>LLM</fixed-case> Fine-Tuning with a Continual Learning Setting</title>
      <author><first>Changhao</first><last>Guan</last></author>
      <author><first>Chao</first><last>Huang</last></author>
      <author><first>Hongliang</first><last>Li</last></author>
      <author><first>You</first><last>Li</last></author>
      <author><first>Ning</first><last>Cheng</last></author>
      <author><first>Zihe</first><last>Liu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Liu</last><affiliation>University of Science and Technology Beijing</affiliation></author>
      <pages>5484-5498</pages>
      <abstract>In recent years, large language models (LLMs) have made significant progress in knowledge-intensive applications. However, when adapting them to specific domains, we may encounter a multi-stage continuous learning scenario, especially in cases where domain knowledge evolves rapidly.This issue severely limits traditional fine-tuning approaches for LLMs.To overcome this limitation, we propose a new learning paradigm designed specifically for multi-stage continuous learning. This paradigm includes a preference-based learning bias to identify potential knowledge conflicts, as well as a self-distillation-based data augmentation strategy to expand and enrich the training corpus, thereby improving the integration of knowledge-compatible information.In the experiments, we show that our proposed method achieves a significant improvement in accuracy after 7 stages of fine-tuning compared to previous methods, while also demonstrating excellent performance in preserving general knowledge.We have released our code and dataset at Multi-Stage-Learning.</abstract>
      <url hash="b94cd24c">2025.findings-naacl.303</url>
      <bibkey>guan-etal-2025-multi</bibkey>
    </paper>
    <paper id="304">
      <title>Constraining Sequential Model Editing with Editing Anchor Compression</title>
      <author><first>Hao-Xiang</first><last>Xu</last></author>
      <author><first>Jun-Yu</first><last>Ma</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhen-Hua</first><last>Ling</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5499-5515</pages>
      <abstract>Large language models (LLMs) struggle with hallucinations due to false or outdated knowledge. Given the high resource demands of retraining these models, there is an increasing focus on developing model editing. However, the general abilities of LLMs across downstream tasks are prone to significant degradation during sequential editing. This paper statistically observes that the parameter matrix after editing exhibits a significant deviation compared to its previous state as the number of edits increases. This serious deviation affects the original knowledge associations within LLMs and leads to the degradation of their general abilities. To this end, a framework termed Editing Anchor Compression (EAC) is proposed to constrain the deviation of the parameter matrix during sequential editing. It compresses the editing information by selecting editing anchors that are important in encoding new relations without deviating too much from the original matrix, thereby preserving the general abilities. Experiments of applying EAC to two popular editing methods on three LLMs across four tasks are conducted. Evaluation results show that EAC effectively minimizes unreasonable deviations caused by model editing, preserving over 70% of the general abilities while better retaining the editing knowledge compared to the original counterpart methods.</abstract>
      <url hash="39a9407d">2025.findings-naacl.304</url>
      <bibkey>xu-etal-2025-constraining</bibkey>
    </paper>
    <paper id="305">
      <title><fixed-case>MLKV</fixed-case>: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding</title>
      <author><first>Zayd Muhammad Kawakibi</first><last>Zuhri</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Muhammad Farid</first><last>Adilazuarda</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>5516-5525</pages>
      <abstract>Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV’s potential for efficient deployment of transformer models at scale.</abstract>
      <url hash="d43397bf">2025.findings-naacl.305</url>
      <bibkey>zuhri-etal-2025-mlkv</bibkey>
    </paper>
    <paper id="306">
      <title>Clarify When Necessary: Resolving Ambiguity Through Interaction with <fixed-case>LM</fixed-case>s</title>
      <author><first>Michael JQ</first><last>Zhang</last><affiliation>New York University</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>New York University</affiliation></author>
      <pages>5526-5543</pages>
      <abstract>In this work, we explore the challenges of developing interactive assistants that resolve ambiguity by asking their users clarifying questions. Specifically, we develop a task-agnostic framework for evaluating a system’s ability to determine when to ask for clarification. Determining when to ask for clarification is a challenging task that requires systems to consider the demands of the individual user (i.e., how much they prioritize speed and usability versus carefulness) and the distribution of interpretations for a given request (i.e., whether an ambiguous request has one dominant, inferable interpretation). Using this framework, we evaluate systems for determining when to clarify across three NLP applications: QA, MT, and NLI. Finally, we introduce present a novel uncertainty estimation approach, IntentSim, that determines the utility of asking a clarifying question by estimating the entropy over user intents. Our method consistently outperforms existing uncertainty estimation approaches at identifying predictions that will benefit from clarification. Furthermore, we find that IntentSim is robust, demonstrating improvements across a wide range of NLP tasks and LMs. Together, our work lays foundation for further studies on clarifying interactions with LM assistants.</abstract>
      <url hash="5f08715d">2025.findings-naacl.306</url>
      <bibkey>zhang-choi-2025-clarify</bibkey>
    </paper>
    <paper id="307">
      <title><fixed-case>DOLFIN</fixed-case> - Document-Level Financial Test-Set for Machine Translation</title>
      <author><first>Mariam</first><last>Nakhle</last></author>
      <author><first>Marco</first><last>Dinarelli</last><affiliation>CNRS</affiliation></author>
      <author><first>Raheel</first><last>Qader</last><affiliation>Lingua Custodia</affiliation></author>
      <author><first>Emmanuelle</first><last>Esperança-Rodier</last><affiliation>University of Grenoble-Alpes</affiliation></author>
      <author><first>Hervé</first><last>Blanchon</last><affiliation>Université Grenoble Alpes</affiliation></author>
      <pages>5544-5556</pages>
      <abstract>Despite the strong research interest in document-level Machine Translation (MT), the test-sets dedicated to this task are still scarce. The existing test-sets mainly cover topics from the general domain and fall short on specialised domains, such as legal and financial. Also, despite their document-level aspect, they still follow a sentence-level logic that doesn’t allow for including certain linguistic phenomena such as information reorganisation. In this work, we aim to fill this gap by proposing a novel test-set : DOLFIN. The dataset is built from specialised financial documents and it makes a step towards true document-level MT by abandoning the paradigm of perfectly aligned sentences, presenting data in units of sections rather than sentences. The test-set consists of an average of 1950 aligned sections for five language pairs. We present the detailed data collection pipeline that can serve as inspiration for aligning new document-level datasets. We demonstrate the usefulness and the quality of this test-set with the evaluation of a series of models. Our results show that the test-set is able to discriminate between context-sensitive and context-agnostic models and shows the weaknesses when models fail to accurately translate financial texts. The test-set will be made public for the community.</abstract>
      <url hash="96178cc0">2025.findings-naacl.307</url>
      <bibkey>nakhle-etal-2025-dolfin</bibkey>
    </paper>
    <paper id="308">
      <title>Are Large Language Models Effective in Clinical Trial Design? A Study on Baseline Feature Generation</title>
      <author><first>Nafis</first><last>Neehal</last></author>
      <author><first>Bowen</first><last>Wang</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Shayom</first><last>Debopadhaya</last><affiliation>Albany Medical College</affiliation></author>
      <author><first>Corey</first><last>Curran</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vibha</first><last>Anand</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Kristin</first><last>Bennett</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>5557-5570</pages>
      <url hash="ca4c7243">2025.findings-naacl.308</url>
      <bibkey>neehal-etal-2025-large</bibkey>
    </paper>
    <paper id="309">
      <title>Lightweight Contenders: Navigating Semi-Supervised Text Mining through Peer Collaboration and Self Transcendence</title>
      <author><first>Qianren</first><last>Mao</last><affiliation>Zhongguancun Laboratory, Beijing, P.R.China. and Beihang University</affiliation></author>
      <author><first>Weifeng</first><last>Jiang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Junnan</first><last>Liu</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Qian</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xianqing</first><last>Wen</last></author>
      <author><first>Jianxin</first><last>Li</last><affiliation>Beihang University</affiliation></author>
      <author><first>Jinhu</first><last>Lu</last></author>
      <pages>5571-5585</pages>
      <abstract>The semi-supervised learning (SSL) strategy in lightweight models requires reducing annotated samples and facilitating cost-effective inference. However, the constraint on model parameters, imposed by the scarcity of training labels, limits the SSL performance. In this paper, we introduce PS-NET, a novel framework tailored for semi-supervised text mining with lightweight models. PS-NET incorporates online distillation to train lightweight student models by imitating the Teacher model. It also integrates an ensemble of student peers that collaboratively instruct each other. Additionally, PS-NET implements a constant adversarial perturbation schema to further self-augmentation by progressive generalizing. Our PS-NET, equipped with a 2-layer distilled BERT, exhibits notable performance enhancements over SOTA lightweight SSL frameworks of FLiText and Disco in SSL text classification with extremely rare labelled data.</abstract>
      <url hash="e883e950">2025.findings-naacl.309</url>
      <bibkey>mao-etal-2025-lightweight</bibkey>
    </paper>
    <paper id="310">
      <title>Language-based Valence and Arousal Expressions between the <fixed-case>U</fixed-case>nited <fixed-case>S</fixed-case>tates and <fixed-case>C</fixed-case>hina: a Cross-Cultural Examination</title>
      <author><first>Young Min</first><last>Cho</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Dandan</first><last>Pang</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <author><first>Stuti</first><last>Thapa</last><affiliation>University of Tulsa</affiliation></author>
      <author><first>Garrick</first><last>Sherman</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <author><first>Louis</first><last>Tay</last><affiliation>Purdue University</affiliation></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>5586-5600</pages>
      <abstract>While affective expressions on social media have been extensively studied, most research has focused on the Western context. This paper explores cultural differences in affective expressions by comparing valence and arousal on Twitter/X (geolocated to the US) and Sina Weibo (in Mainland China). Using the NRC-VAD lexicon to measure valence and arousal, we identify distinct patterns of emotional expression across both platforms. Our analysis reveals a functional representation between valence and arousal, showing a negative offset in contrast to traditional lab-based findings which suggest a positive offset. Furthermore, we uncover significant cross-cultural differences in arousal, with US users displaying higher emotional intensity than Chinese users, regardless of the valence of the content. Finally, we conduct a comprehensive language analysis correlating n-grams and LDA topics with affective dimensions to deepen our understanding of how language and culture shape emotional expression. These findings contribute to a more nuanced understanding of affective communication across cultural and linguistic contexts on social media.</abstract>
      <url hash="79791c93">2025.findings-naacl.310</url>
      <bibkey>cho-etal-2025-language</bibkey>
    </paper>
    <paper id="311">
      <title>Chain-of-Rank: Enhancing Large Language Models for Domain-Specific <fixed-case>RAG</fixed-case> in Edge Device</title>
      <author><first>Juntae</first><last>Lee</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Jihwan</first><last>Bang</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Kyuhong</first><last>Shim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Seunghan</first><last>Yang</last><affiliation>Qualcomm AI Research</affiliation></author>
      <author><first>Simyung</first><last>Chang</last><affiliation>QualComm AI Research</affiliation></author>
      <pages>5601-5608</pages>
      <abstract>Retrieval-augmented generation (RAG) with large language models (LLMs) is especially valuable in specialized domains, where precision is critical. To more specialize the LLMs into a target domain, domain-specific RAG has recently been developed by allowing the LLM to access the target domain early via finetuning. The domain-specific RAG makes more sense in resource-constrained environments like edge devices, as they should perform a specific task (e.g. personalization) reliably using only small-scale LLMs. While the domain-specific RAG is well-aligned with edge devices in this respect, it often relies on widely-used reasoning techniques like chain-of-thought (CoT). The reasoning step is useful to understand the given external knowledge, and yet it is computationally expensive and difficult for small-scale LLMs to learn it. Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from intricate lengthy reasoning to simple ranking of the reliability of input external documents. Then, CoR reduces computational complexity while maintaining high accuracy, making it particularly suited for resource-constrained environments. We attain the state-of-the-art (SOTA) results in benchmarks, and analyze its efficacy.</abstract>
      <url hash="d7610faa">2025.findings-naacl.311</url>
      <bibkey>lee-etal-2025-chain-rank</bibkey>
    </paper>
    <paper id="312">
      <title><fixed-case>MAL</fixed-case>o<fixed-case>RA</fixed-case>: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning</title>
      <author><first>Xujia</first><last>Wang</last></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Hanqing</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>5609-5626</pages>
      <abstract>Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA have significantly improved the adaptation of LLMs to downstream tasksin a resource-efficient manner. However, in multi-task scenarios, challenges such as training imbalance and the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which combines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by promoting task-specific learning among experts. Despite this, MoLoRA remains inefficient in terms of training speed, parameter utilization, and overall multi-task performance. In this paper, we propose Mixture of Asymmetric Low-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages asymmetric optimization among LoRA experts. MALoRA reduces the number of trainable parameters by 30% to 48%, increases training speed by 1.2x, and matches the computational efficiency of single-task LoRA models. Additionally, MALoRA addresses overfitting issues commonly seen in high-rank configurations, enhancing performance stability. Extensive experiments across diverse multi-task learning scenarios demonstrate that MALoRA consistently outperforms all baseline methods in both inter-domain and intra-domain tasks.</abstract>
      <url hash="1b5c50e3">2025.findings-naacl.312</url>
      <bibkey>wang-etal-2025-malora</bibkey>
    </paper>
    <paper id="313">
      <title><fixed-case>L</fixed-case>lama<fixed-case>L</fixed-case>ens: Specialized Multilingual <fixed-case>LLM</fixed-case> for Analyzing News and Social Media Content</title>
      <author><first>Mohamed Bayan</first><last>Kmainasi</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Ali Ezzat</first><last>Shahroor</last></author>
      <author><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Sahinur Rahman</first><last>Laskar</last><affiliation>UPES</affiliation></author>
      <author><first>Naeemul</first><last>Hassan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>5627-5649</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable success as general-purpose task solvers across various fields. However, their capabilities remain limited when addressing domain-specific problems, particularly in downstream NLP tasks. Research has shown that models fine-tuned on instruction-based downstream NLP datasets outperform those that are not fine-tuned. While most efforts in this area have primarily focused on resource-rich languages like English and broad domains, little attention has been given to multilingual settings and specific domains. To address this gap, this study focuses on developing a specialized LLM, LlamaLens, for analyzing news and social media content in a multilingual context. To the best of our knowledge, this is the first attempt to tackle both domain specificity and multilinguality, with a particular focus on news and social media. Our experimental setup includes 18 tasks, represented by 52 datasets covering Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the current state-of-the-art (SOTA) on 23 testing sets, and achieves comparable performance on 8 sets. We make the models and resources publicly available for the research community (https://huggingface.co/QCRI).</abstract>
      <url hash="750e5ce6">2025.findings-naacl.313</url>
      <bibkey>kmainasi-etal-2025-llamalens</bibkey>
    </paper>
    <paper id="314">
      <title><fixed-case>LLM</fixed-case>s are Biased Teachers: Evaluating <fixed-case>LLM</fixed-case> Bias in Personalized Education</title>
      <author><first>Iain</first><last>Weissburg</last></author>
      <author><first>Sathvika</first><last>Anand</last></author>
      <author><first>Sharon</first><last>Levy</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Haewon</first><last>Jeong</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <pages>5650-5698</pages>
      <abstract>With the increasing adoption of large language models (LLMs) in education, concerns about inherent biases in these models have gained prominence. We evaluate LLMs for bias in the personalized educational setting, specifically focusing on the models’ roles as “teachers.” We reveal significant biases in how models generate and select educational content tailored to different demographic groups, including race, ethnicity, sex, gender, disability status, income, and national origin. We introduce and apply two bias score metrics—Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB)—to analyze 9 open and closed state-of-the-art LLMs. Our experiments, which utilize over 17,000 educational explanations across multiple difficulty levels and topics, uncover that models potentially harm student learning by both perpetuating harmful stereotypes and reversing them. We find that bias is similar for all frontier models, with the highest MAB along income levels while MDB is highest relative to both income and disability status. For both metrics, we find the lowest bias exists for sex/gender and race/ethnicity.</abstract>
      <url hash="c6452370">2025.findings-naacl.314</url>
      <bibkey>weissburg-etal-2025-llms</bibkey>
    </paper>
    <paper id="315">
      <title>Preserving Zero-shot Capability in Supervised Fine-tuning for Multi-label Text Classification</title>
      <author><first>Si-An</first><last>Chen</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chih-Jen</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <pages>5699-5712</pages>
      <abstract>Zero-shot multi-label text classification (ZMTC) requires models to predict multiple labels for a document, including labels unseen during training. Previous work assumes that models leveraging label descriptions ensures zero-shot capability. However, we find that supervised methods, despite achieving strong overall performance, lose their zero-shot capability during training, revealing a trade-off between overall and zero-shot performance. To address the issue, we propose OF-DE and OF-LAN, which preserve the zero-shot capabilities of powerful dual encoder and label-wise attention network architectures by freezing the label encoder. Additionally, we introduce a self-supervised auxiliary loss to further improve zero-shot performance. Experiments demonstrate that our approach significantly improves zero-shot performance of supervised methods while maintaining strong overall accuracy.</abstract>
      <url hash="20b01462">2025.findings-naacl.315</url>
      <bibkey>chen-etal-2025-preserving</bibkey>
    </paper>
    <paper id="316">
      <title>Data-centric <fixed-case>NLP</fixed-case> Backdoor Defense from the Lens of Memorization</title>
      <author><first>Zhenting</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Zhizhi</first><last>Wang</last></author>
      <author><first>Mingyu</first><last>Jin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Juan</first><last>Zhai</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Shiqing</first><last>Ma</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <pages>5713-5731</pages>
      <abstract>Backdoor attack is a severe threat to the trustworthiness of DNN-based language models. In this paper, we first extend the definition of memorization of language models from sample-wise to more fine-grained sentence element-wise (e.g., word, phrase, structure, and style), and then point out that language model backdoors are a type of element-wise memorization. Through further analysis, we find that the strength of such memorization is positively correlated to the frequency of duplicated elements in the training dataset. In conclusion, duplicated sentence elements are necessary for successful backdoor attacks. Based on this, we propose a data-centric defense. We first detect trigger candidates in training data by finding memorizable elements, i.e., duplicated elements, and then confirm real triggers by testing if the candidates can activate backdoor behaviors (i.e., malicious elements). Results show that our method outperforms state-of-the-art defenses in defending against different types of NLP backdoors.</abstract>
      <url hash="f62ca2c6">2025.findings-naacl.316</url>
      <bibkey>wang-etal-2025-data-centric</bibkey>
    </paper>
    <paper id="317">
      <title>Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs</title>
      <author><first>Sen</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5732-5744</pages>
      <abstract>Two lines of approaches are adopted for complex reasoning with LLMs. One line of work prompts LLMs with various reasoning structures, while the structural outputs can be naturally regarded as intermediate reasoning steps. Another line of work adopt LLM-free declarative solvers to do the reasoning task, rendering higher reasoning accuracy but lacking interpretability due to the black-box nature of the solvers. Aiming to resolve the trade-off between answer accuracy and interpretability, we present a simple extension to the latter line of work. Specifically, we showcase that the intermediate search logs generated by Prolog interpreters can be accessed and interpreted into human-readable reasoning proofs. As long as LLMs correctly translate problem descriptions into Prolog representations, the corresponding reasoning proofs are ensured to be causal and reliable. On two logical reasoning and one arithmetic reasoning datasets, our framework obtains significant improvements in terms of both answer accuracy and reasoning proof accuracy. We released our code at <url>https://github.com/DAMO-NLP-SG/CaRing</url> for future research regarding better reasoning proofs using LLMs.</abstract>
      <url hash="060edadb">2025.findings-naacl.317</url>
      <bibkey>yang-etal-2025-neuro</bibkey>
    </paper>
    <paper id="318">
      <title>Infogent: An Agent-Based Framework for Web Information Aggregation</title>
      <author><first>Revanth</first><last>Gangi Reddy</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Dilek</first><last>Hakkani-Tür</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>5745-5758</pages>
      <abstract>Despite seemingly performant web agents on the task-completion benchmarks, most existing methods evaluate the agents based on a presupposition: the web navigation task consists of a linear sequence of actions with an end state that marks task completion. In contrast, our work focuses on web navigation for information aggregation, wherein the agent must explore different websites to gather information for a complex query. We consider web information aggregation from two different perspectives: i) Direct API-driven Access relies on a text-only view of the Web, leveraging external tools such as Google Search API to navigate the Web and a scraper to extract website contents. (ii) Interactive Visual Access uses screenshots of the webpages and requires interaction with the browser to navigate and access information. Motivated by these diverse information access settings, we introduce Infogent, a novel modular framework for web information aggregation involving three distinct components: Navigator, Extractor, and Aggregator. Experiments on different information access settings demonstrate that Infogent beats an existing SOTA multi-agent search framework by 7% under Direct API-Driven Access on FRAMES and improves over an existing information-seeking web agent by 4.3% under Interactive Visual Access on AssistantBench.</abstract>
      <url hash="43f1d9e0">2025.findings-naacl.318</url>
      <bibkey>gangi-reddy-etal-2025-infogent</bibkey>
    </paper>
    <paper id="319">
      <title>On the Role of Key Phrases in Argument Mining</title>
      <author><first>Nilmadhab</first><last>Das</last></author>
      <author><first>Vijaya V</first><last>Saradhi</last></author>
      <author><first>Ashish</first><last>Anand</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <pages>5759-5772</pages>
      <abstract>Argument mining (AM) focuses on analyzing argumentative structures such as Argument Components (ACs) and Argumentative Relations (ARs). Modeling dependencies between ACs and ARs is challenging due to the complex interactions between ACs. Existing approaches often overlook crucial conceptual links, such as key phrases that connect two related ACs, and tend to rely on cartesian product methods to model these dependencies, which can result in class imbalances. To extract key phrases from the AM benchmarks, we employ a prompt-based strategy utilizing an open-source Large Language Model (LLM). Building on this, we propose a unified text-to-text generation framework that leverages Augmented Natural Language (ANL) formatting and integrates the extracted key phrases inside the ANL itself to efficiently solve multiple AM tasks in a joint formulation. Our method sets new State-of-the-Art (SoTA) on three structurally distinct standard AM benchmarks, surpassing baselines by up to 9.5% F1 score, demonstrating its strong potential.</abstract>
      <url hash="655259f6">2025.findings-naacl.319</url>
      <bibkey>das-etal-2025-role</bibkey>
    </paper>
    <paper id="320">
      <title><fixed-case>T</fixed-case>ab<fixed-case>C</fixed-case>omp: A Dataset for Visual Table Reading Comprehension</title>
      <author><first>Somraj</first><last>Gautam</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author><first>Abhishek</first><last>Bhandari</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <author><first>Gaurav</first><last>Harit</last></author>
      <pages>5773-5780</pages>
      <abstract>Reaching a human-level understanding of real-world documents necessitates effective machine reading comprehension, yet recent developments in this area often struggle with table images. In response, we introduce the Visual Table Reading Comprehension (TabComp) dataset, which includes table images, questions, and generative answers designed to evaluate OCR-free models. Unlike general Visual Question Answering (VQA) datasets, TabComp uniquely focuses on table images, fostering the development of systems which obviate the use of optical character recognition (OCR) technology, which often struggles with complex table layouts. Our findings reveal that current OCR-free models perform poorly on TabComp, highlighting the need for robust, specialized models for accurate table reading comprehension. We propose TabComp as a benchmark for evaluating OCR-free models in table reading comprehension and encourage the research community to collaborate on developing more effective solutions. The code and data are available at - https://github.com/dialabiitj/TabComp/</abstract>
      <url hash="6c51e94a">2025.findings-naacl.320</url>
      <bibkey>gautam-etal-2025-tabcomp</bibkey>
    </paper>
    <paper id="321">
      <title><fixed-case>R</fixed-case>ank<fixed-case>A</fixed-case>daptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned <fixed-case>LLM</fixed-case>s via Performance Model</title>
      <author><first>Changhai</first><last>Zhou</last></author>
      <author><first>Shijie</first><last>Han</last></author>
      <author><first>Lining</first><last>Yang</last></author>
      <author><first>Yuhua</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Cheng</last></author>
      <author><first>Yibin</first><last>Wang</last></author>
      <author><first>Hongguang</first><last>Li</last><affiliation>JF SmartInvest Holdings</affiliation></author>
      <pages>5781-5795</pages>
      <abstract>The efficient compression of large language models (LLMs) has become increasingly popular. However, recovering the performance of compressed LLMs remains a major challenge. The current practice in LLM compression entails the implementation of structural pruning, complemented by a recovery phase that leverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning’s uneven modification of model architecture, coupled with standard LoRA’s fixed configuration allocation across layers in an online pipeline, leads to suboptimal performance in various downstream tasks for pruned models. To address this challenge, we introduce RankAdaptor, a hierarchical rank allocation method that enables efficient fine-tuning of pruned LLMs according to layerwise specific recovery requirements. We employ a performance model that conducts offline meta-learning and online incremental learning to explore optimal rank values for each layer. Comprehensive experiments on popular benchmarks show that RankAdaptor consistently outperforms state-of-the-art methods across a variety of pruning settings and LLM architectures, with improvements ranging from 0.7% to 5.5%.</abstract>
      <url hash="95b51e12">2025.findings-naacl.321</url>
      <bibkey>zhou-etal-2025-rankadaptor</bibkey>
    </paper>
    <paper id="322">
      <title>Rationale Behind Essay Scores: Enhancing <fixed-case>S</fixed-case>-<fixed-case>LLM</fixed-case>’s Multi-Trait Essay Scoring with Rationale Generated by <fixed-case>LLM</fixed-case>s</title>
      <author><first>SeongYeub</first><last>Chu</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong Woo</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Bryan</first><last>Wong</last></author>
      <author><first>Mun Yong</first><last>Yi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>5796-5814</pages>
      <abstract>Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays. The code is available at <b>
          <url>https://github.com/BBeeChu/RMTS.git</url></b>.</abstract>
      <url hash="be5b4dca">2025.findings-naacl.322</url>
      <bibkey>chu-etal-2025-rationale</bibkey>
    </paper>
    <paper id="323">
      <title><fixed-case>MTPC</fixed-case>hat: A Multimodal Time-Aware Persona Dataset for Conversational Agents</title>
      <author><first>Wanqi</first><last>Yang</last></author>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>5815-5826</pages>
      <abstract>Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model’s ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.</abstract>
      <url hash="36028ecc">2025.findings-naacl.323</url>
      <bibkey>yang-etal-2025-mtpchat</bibkey>
    </paper>
    <paper id="324">
      <title><fixed-case>M</fixed-case>eta<fixed-case>A</fixed-case>lign: Align Large Language Models with Diverse Preferences during Inference Time</title>
      <author><first>Mozhi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Chenkun</first><last>Tan</last></author>
      <author><first>Mianqiu</first><last>Huang</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Yaqian</first><last>Zhou</last><affiliation>Fudan University, Tsinghua University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>5827-5845</pages>
      <abstract>Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is essential. Existing alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed predefined preferences directly within the model’s parameters. These methods, however, often result in a static alignment that can not account for the diversity of human preferences in practical applications.In response to this challenge, we propose an effective method, <b>MetaAlign</b>, which aims to help LLMs dynamically align with various explicit or implicit preferences specified at inference time. Experimental results show that LLMs optimized on our meticulously constructed MetaAlign Dataset can effectively align with any preferences specified at the inference stage, validating the feasibility of MetaAlign. We hope that our work can provide some insights into the alignment of language models.</abstract>
      <url hash="5444c003">2025.findings-naacl.324</url>
      <bibkey>zhang-etal-2025-metaalign</bibkey>
    </paper>
    <paper id="325">
      <title><fixed-case>MAQA</fixed-case>: Evaluating Uncertainty Quantification in <fixed-case>LLM</fixed-case>s Regarding Data Uncertainty</title>
      <author><first>Yongjin</first><last>Yang</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Haneul</first><last>Yoo</last><affiliation>KAIST</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>Sogang University</affiliation></author>
      <pages>5846-5863</pages>
      <abstract>Despite the massive advancements in large language models (LLMs), they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on single-labeled questions, which removes data uncertainty—the irreducible randomness often present in user queries, which can arise from factors like multiple possible answers. This limitation may cause uncertainty quantification results to be unreliable in practical settings. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, **MAQA**, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that previous methods relatively struggle compared to single-answer settings, though this varies depending on the task. Moreover, we observe that entropy- and consistency-based methods effectively estimate model uncertainty, even in the presence of data uncertainty.</abstract>
      <url hash="24f5b509">2025.findings-naacl.325</url>
      <bibkey>yang-etal-2025-maqa</bibkey>
    </paper>
    <paper id="326">
      <title>Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning</title>
      <author><first>Hyundong Justin</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Karishma</first><last>Sharma</last></author>
      <author><first>Nicolaas Paul</first><last>Jedema</last><affiliation>Amazon</affiliation></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon AGI</affiliation></author>
      <pages>5864-5885</pages>
      <abstract>Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users’ styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user’s style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5% against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, presents a novel yet simple approach for personalized alignment.</abstract>
      <url hash="76beb0b3">2025.findings-naacl.326</url>
      <bibkey>cho-etal-2025-tuning</bibkey>
    </paper>
    <paper id="327">
      <title>Causal Inference with Large Language Model: A Survey</title>
      <author><first>Jing</first><last>Ma</last><affiliation>Case Western Reserve University</affiliation></author>
      <pages>5886-5898</pages>
      <abstract>Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.</abstract>
      <url hash="4a6f88b0">2025.findings-naacl.327</url>
      <bibkey>ma-2025-causal</bibkey>
    </paper>
    <paper id="328">
      <title>Ask Optimal Questions: Aligning Large Language Models with Retriever’s Preference in Conversation</title>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <author><first>Byeongguk</first><last>Jeon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sungdong</first><last>Kim</last><affiliation>KAIST AI</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>5899-5921</pages>
      <abstract>Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever’s Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers’ preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers’ Feedback on over 410K query rewrites across 12K conversations. Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers’ preferences as feedback. The resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches, including GPT-3.5.</abstract>
      <url hash="7f243ef1">2025.findings-naacl.328</url>
      <bibkey>yoon-etal-2025-ask</bibkey>
    </paper>
    <paper id="329">
      <title>Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific <fixed-case>RAG</fixed-case></title>
      <author><first>Kushagra</first><last>Bhushan</last></author>
      <author><first>Yatin</first><last>Nandwani</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Khandelwal</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sonam</first><last>Gupta</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Gaurav</first><last>Pandey</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Raghu</last><affiliation>IBM Research - New Delhi</affiliation></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>5922-5943</pages>
      <abstract>Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways – context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we finetune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10% relative gain in token-level recall while preserving the LLM’s generalization capabilities.</abstract>
      <url hash="e7572056">2025.findings-naacl.329</url>
      <bibkey>bhushan-etal-2025-systematic</bibkey>
    </paper>
    <paper id="330">
      <title>Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models</title>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>5944-5964</pages>
      <abstract>Through numerous endeavors, large language models (LLMs) have witnessed significant advancements in their instruction-following capability. However, we discern that LLMs are prone to generate responses to instruction-formatted statements in an instinctive manner, rather than comprehending the underlying user intention reside within the given instructions. We also recognize that the significance of instruction understanding capability is largely overlooked in most of LLM evaluation benchmarks. To ensure more comprehensive evaluation on the instruction understanding capability of LLM, we propose Intention of Instruction (IntInst) benchmark, which primary objective is to distinguish the appropriate instruction that accurately instruct to generate a given context. IntInst presents four instruction candidates and requires LLMs to select one among them. Through extensive experiments with several instruction-tuned LLMs, we reveal that most LLMs struggle to grasp the actual intention concealed in the instruction and thoroughly analyze the factors influencing instruction understanding.</abstract>
      <url hash="723b7c23">2025.findings-naacl.330</url>
      <bibkey>moon-etal-2025-find</bibkey>
    </paper>
    <paper id="331">
      <title>Long-Tail Crisis in Nearest Neighbor Language Models</title>
      <author><first>Yuto</first><last>Nishida</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>Future Corporation and Tohoku University</affiliation></author>
      <author><first>Hiroyuki</first><last>Deguchi</last><affiliation>NTT Communications</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>5965-5978</pages>
      <abstract>The <tex-math>k</tex-math>-nearest-neighbor language model (<tex-math>k</tex-math>NN-LM), one of the retrieval-augmented language models, improves the perplexity for given text by directly accessing a large datastore built from any text data during inference.A widely held hypothesis for the success of <tex-math>k</tex-math>NN-LM is that its explicit memory, i.e., the datastore, enhances predictions for long-tail phenomena.However, prior works have primarily shown its ability to retrieve long-tail contexts, leaving the model’s performance remain underexplored in estimating the probabilities of long-tail target tokens during inference.In this paper, we investigate the behavior of <tex-math>k</tex-math>NN-LM on low-frequency tokens, examining prediction probability, retrieval accuracy, and token distribution in the datastore.Our experimental results reveal that <tex-math>k</tex-math>NN-LM does not improve prediction performance for low-frequency tokens but mainly benefits high-frequency tokens regardless of long-tail contexts in the datastore.</abstract>
      <url hash="999353cb">2025.findings-naacl.331</url>
      <bibkey>nishida-etal-2025-long</bibkey>
    </paper>
    <paper id="332">
      <title>Keep Guessing? When Considering Inference Scaling, Mind the Baselines</title>
      <author><first>Gal</first><last>Yona</last><affiliation>Research, Google</affiliation></author>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Omer</first><last>Levy</last><affiliation>Facebook</affiliation></author>
      <author><first>Roee</first><last>Aharoni</last><affiliation>Google</affiliation></author>
      <pages>5979-5991</pages>
      <abstract>Scaling inference compute in large language models (LLMs) through repeated sampling consistently increases the coverage (fraction of problems solved) as the number of samples increases. We conjecture that this observed improvement is partially due to the answer distribution of standard evaluation benchmarks, which is skewed towards a relatively small set of common answers. To test this conjecture, we define a baseline that enumerates answers according to their prevalence in the training set. Experiments spanning two domains – mathematical reasoning and factual knowledge – reveal that this baseline outperforms repeated model sampling for some LLMs, while the coverage for others is on par with that of a mixture strategy that obtains <tex-math>k</tex-math> answers by using only 10 model samples and similarly guessing the remaining <tex-math>k-10</tex-math> attempts via enumeration. Our baseline enables a more accurate measurement of how much repeated sampling improves coverage in such settings beyond prompt-agnostic guessing.</abstract>
      <url hash="8ac1001f">2025.findings-naacl.332</url>
      <bibkey>yona-etal-2025-keep</bibkey>
    </paper>
    <paper id="333">
      <title>Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey</title>
      <author><first>Ruiyao</first><last>Xu</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University</affiliation></author>
      <pages>5992-6012</pages>
      <abstract>Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers: https://github.com/rux001/Awesome-LLM-Anomaly-OOD-Detection.</abstract>
      <url hash="fec52e7e">2025.findings-naacl.333</url>
      <bibkey>xu-ding-2025-large</bibkey>
    </paper>
    <paper id="334">
      <title>Time-aware <fixed-case>R</fixed-case>e<fixed-case>A</fixed-case>ct Agent for Temporal Knowledge Graph Question Answering</title>
      <author><first>QianyiHu</first><last>QianyiHu</last></author>
      <author><first>Xinhui</first><last>Tu</last><affiliation>Central China Normal University</affiliation></author>
      <author><first>Guo</first><last>Cong</last></author>
      <author><first>Shunping</first><last>Zhang</last></author>
      <pages>6013-6024</pages>
      <abstract>Temporal knowledge graph question answering (TKGQA) addresses time-sensitive queries using knowledge bases. Although large language models (LLMs) and LLM-based agents such as ReAct have shown potential for TKGQA, they often lack sufficient temporal constraints in the retrieval process. To tackle this challenge, we propose TempAgent, a novel autonomous agent framework built on LLMs that enhances their ability to conduct temporal reasoning and comprehension. By integrating temporal constraints into information retrieval, TempAgent effectively discards irrelevant material and concentrates on extracting pertinent temporal and factual information. We evaluate our framework on the MultiTQ dataset, a real-world multi-granularity TKGQA benchmark, using a fully automated setup. Our experimental results reveal the remarkable effectiveness of our approach: TempAgent achieves a 41.3% improvement over the baseline model and a 32.2% gain compared to the Abstract Reasoning Induction (ARI) method. Moreover, our method attains an accuracy of 70.2% on the @hit1 metric, underscoring its substantial advantage in addressing time-aware TKGQA tasks.</abstract>
      <url hash="755d037d">2025.findings-naacl.334</url>
      <bibkey>qianyihu-etal-2025-time</bibkey>
    </paper>
    <paper id="335">
      <title><fixed-case>SG</fixed-case>-<fixed-case>FSM</fixed-case>: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine</title>
      <author><first>Xiaochen</first><last>Wang</last></author>
      <author><first>Junqing</first><last>He</last><affiliation>International Digital Econemy Academy</affiliation></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University, Monash University and Monash University</affiliation></author>
      <author><first>Yiru</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiangdi</first><last>Meng</last></author>
      <author><first>Kunhao</first><last>Pan</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>6025-6037</pages>
      <abstract>Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs’ performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.</abstract>
      <url hash="0b1932a7">2025.findings-naacl.335</url>
      <bibkey>wang-etal-2025-sg</bibkey>
    </paper>
    <paper id="336">
      <title>Dynamic Strategy Planning for Efficient Question Answering with Large Language Models</title>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>Pradyot</first><last>Prakash</last><affiliation>Meta</affiliation></author>
      <author><first>Alexander</first><last>Radovic</last><affiliation>Facebook</affiliation></author>
      <author><first>Akshay</first><last>Shekher</last></author>
      <author><first>Denis</first><last>Savenkov</last><affiliation>Meta</affiliation></author>
      <pages>6038-6059</pages>
      <abstract>Research has shown an effectiveness of reasoning (e.g. Chain-of-Thought), planning (e.g. SelfAsk) and retrieval augmented generation strategies to improve performance of Large Language Models (LLMs) on various tasks, such as question answering. However, using a single fixed strategy for answering all different kinds of questions is sub-optimal in performance and inefficient in terms of generated tokens and retrievals. In our work, we propose a novel technique, DyPlan, to induce a dynamic strategy selection process in LLMs for cost-effective question-answering. DyPlan incorporates an initial decision step to select the most suitable strategy conditioned on the input question and guides the LLM’s response generation accordingly. We extend DyPlan to DyPlan-verify, adding an internal verification and correction process to further enrich the generated answer. Experimentation on three prominent multi-hop question answering (MHQA) datasets reveals how DyPlan can improve model performance by 7-13% while reducing the cost by 11-32% relative to the best baseline model.</abstract>
      <url hash="1f02d0c7">2025.findings-naacl.336</url>
      <bibkey>parekh-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="337">
      <title>Can <fixed-case>I</fixed-case> Introduce My Boyfriend to My Grandmother? Evaluating Large Language Models Capabilities on <fixed-case>I</fixed-case>ranian Social Norm Classification</title>
      <author><first>Hamidreza</first><last>Saffari</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Mohammadamin</first><last>Shafiei</last><affiliation>University of Milan</affiliation></author>
      <author><first>Donya</first><last>Rooein</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Francesco</first><last>Pierri</last><affiliation>Politecnico di Milano</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>6060-6074</pages>
      <abstract>Creating globally inclusive AI systems demands datasets reflecting diverse social norms. Iran, with its unique cultural blend, offers an ideal case study, with Farsi adding linguistic complexity. In this work, we introduce the Iranian Social Norms (ISN) dataset, a novel collection of 1,699 Iranian social norms, including environments, demographic features, and scope annotation, alongside English translations. Our evaluation of 6 Large Language Models (LLMs) in classifying Iranian social norms, using a variety of prompts, uncovered critical insights into the impact of geographic and linguistic context. Results revealed a substantial performance gap in LLMs’ comprehension of Iranian norms. Notably, while the geographic context in English prompts enhanced the performance, this effect was absent in Farsi, pointing to nuanced linguistic challenges. Particularly, performance was significantly worse for Iran-specific norms, emphasizing the importance of culturally tailored datasets. As the first Farsi dataset for social norm classification, ISN will facilitate crucial cross-cultural analyses, shedding light on how values differ across contexts and cultures.</abstract>
      <url hash="c1648c2b">2025.findings-naacl.337</url>
      <bibkey>saffari-etal-2025-introduce</bibkey>
    </paper>
    <paper id="338">
      <title><fixed-case>PLD</fixed-case>+: Accelerating <fixed-case>LLM</fixed-case> Inference by Leveraging Language Model Artifacts</title>
      <author><first>Shwetha</first><last>Somasundaram</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Anirudh</first><last>Phukan</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Apoorv</first><last>Saxena</last><affiliation>Adobe Systems</affiliation></author>
      <pages>6075-6089</pages>
      <abstract>To reduce the latency associated with autoretrogressive LLM inference, speculative decoding has emerged as a novel decoding paradigm, where future tokens are drafted and verified in parallel. However, the practical deployment of speculative decoding is hindered by its requirements for additional computational resources and fine-tuning, which limits its out-of-the-box usability. To address these challenges, we present PLD+, a suite of novel algorithms developed to accelerate the inference process of LLMs, particularly for input-guided tasks. These tasks, which include code editing, text editing, summarization, etc., often feature outputs with substantial overlap with their inputs—an attribute PLD+ is designed to exploit. PLD+ also leverages the artifacts (attention and hidden states) generated during inference to accelerate inference speed. We test our approach on five input-guided tasks and through extensive experiments we find that PLD+ outperforms all tuning-free approaches. In the greedy setting, it even outperforms the state-of-the-art tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31 in terms of avg. speedup). Our approach is tuning free, does not require any additional compute and can easily be used for accelerating inference of any LLM.</abstract>
      <url hash="7be951a7">2025.findings-naacl.338</url>
      <bibkey>somasundaram-etal-2025-pld</bibkey>
    </paper>
    <paper id="339">
      <title>Adapting <fixed-case>LLM</fixed-case> Agents with Universal Communication Feedback</title>
      <author><first>Kuan</first><last>Wang</last></author>
      <author><first>Yadong</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Santacroce</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <pages>6090-6107</pages>
      <abstract>Recent advances in large language models (LLMs) have demonstrated potential for LLM agents. To facilitate the training for these agents with both linguistic feedback and non-linguistic reward signals, we introduce Learning through Communication (LTC). We design a universal buffer to store all the feedback, and an iterative pipeline to enable an LLM agent to explore and update its policy in an given environment. To optimize agent interactions for task-specific learning with our universal buffer and pipeline, we introduce diverse communication patterns tailored for both single-agent and multi-agent environments. We evaluate the efficacy of our LTC approach on four diverse datasets: ALFWorld (single-agent), HotpotQA (multi-agent collaboration), Chameleon (multi-agent competition), and GSM8k (multi-agent teacher-student). On these data sets, LTC outperforms the supervised instruction fine-tuning baselines by 3.6% to 12%. These results highlight the versatility and efficiency of LTC in facilitating online adaptation for LLM agents.</abstract>
      <url hash="d5ccd885">2025.findings-naacl.339</url>
      <bibkey>wang-etal-2025-adapting</bibkey>
    </paper>
    <paper id="340">
      <title>Ignore the <fixed-case>KL</fixed-case> Penalty! Boosting Exploration on Critical Tokens to Enhance <fixed-case>RL</fixed-case> Fine-Tuning</title>
      <author><first>Jean</first><last>Vassoyan</last></author>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Roman</first><last>Plaud</last></author>
      <pages>6108-6118</pages>
      <abstract>The ability to achieve long-term goals is a key challenge in the current development of large language models (LLMs). To address this, pre-trained LLMs can be fine-tuned with reinforcement learning (RL) to explore solutions that optimize a given goal. However, exploration with LLMs is difficult, as a balance has to be struck between discovering new solutions and staying close enough to the pre-trained model, so as not to degrade basic capabilities. This is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we investigate the exploration dynamics of a small language model on a simple arithmetic task. We show how varying degrees of pre-training influence exploration and demonstrate the importance of “critical tokens” which have a dramatic impact on the final outcome. Consequently, we introduce a simple modification to the KL penalty that favors exploration on critical tokens, increasing the efficiency of the RL fine-tuning stage.</abstract>
      <url hash="30ea21be">2025.findings-naacl.340</url>
      <bibkey>vassoyan-etal-2025-ignore</bibkey>
    </paper>
    <paper id="341">
      <title><fixed-case>S</fixed-case>ea<fixed-case>E</fixed-case>xam and <fixed-case>S</fixed-case>ea<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>LLM</fixed-case>s with Local Multilingual Questions in <fixed-case>S</fixed-case>outheast <fixed-case>A</fixed-case>sia</title>
      <author><first>Chaoqun</first><last>Liu</last></author>
      <author><first>Wenxuan</first><last>Zhang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Mahani</first><last>Aljunied</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Shanda Group and Alibaba Group</affiliation></author>
      <pages>6119-6136</pages>
      <abstract>This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.</abstract>
      <url hash="ff27cc52">2025.findings-naacl.341</url>
      <bibkey>liu-etal-2025-seaexam</bibkey>
    </paper>
    <paper id="342">
      <title>Learning to Search Effective Example Sequences for In-Context Learning</title>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <author><first>Ankita</first><last>Sinha</last><affiliation>intuit</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <pages>6137-6146</pages>
      <abstract>Large language models (LLMs) demonstrate impressive few-shot learning capabilities, but their performance varies widely based on the sequence of in-context examples. Key factors influencing this include the sequence’s length, composition, and arrangement, as well as its relation to the specific query. Existing methods often tackle these factors in isolation, overlooking their interdependencies. Moreover, the extensive search space for selecting optimal sequences complicates the development of a holistic approach. In this work, we introduce Beam Search-based Example Sequence Constructor (BESC), a novel method for learning to construct optimal example sequences. addresses all key factors involved in sequence selection by considering them jointly during inference, while incrementally building the sequence. This design enables the use of beam search to significantly reduce the complexity of the search space. Experiments across various datasets and language models show notable improvements in performance.</abstract>
      <url hash="b1d78cf5">2025.findings-naacl.342</url>
      <bibkey>gao-etal-2025-learning</bibkey>
    </paper>
    <paper id="343">
      <title>From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models</title>
      <author><first>Harsh Nishant</first><last>Lalai</last></author>
      <author><first>Aashish</first><last>Anantha Ramakrishnan</last><affiliation>Pennsylvania State University, Pennsylvania State University</affiliation></author>
      <author><first>Raj Sanjay</first><last>Shah</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>6147-6160</pages>
      <abstract>With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques through a comprehensive survey of the research literature. Our work has two key advantages: (1) We analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, and watermarking addition and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research protecting text authorship. This extensive coverage and detailed analysis sets our work apart, outlining the evolving landscape of text watermarking in Language Models.</abstract>
      <url hash="e595c9d3">2025.findings-naacl.343</url>
      <bibkey>lalai-etal-2025-intentions</bibkey>
    </paper>
    <paper id="344">
      <title><fixed-case>M</fixed-case>-<fixed-case>IFE</fixed-case>val: Multilingual Instruction-Following Evaluation</title>
      <author><first>Antoine</first><last>Dussolle</last></author>
      <author><first>A.</first><last>Cardeña</last></author>
      <author><first>Shota</first><last>Sato</last><affiliation>Lightblue</affiliation></author>
      <author><first>Peter</first><last>Devine</last></author>
      <pages>6161-6176</pages>
      <abstract>Instruction following is a core capability of modern Large language models (LLMs), making evaluating this capability essential to understanding these models. The Instruction Following Evaluation (IFEval) benchmark from the literature does this using objective criteria, offering a measure of LLM performance without subjective AI or human judgement. However, it only includes English instructions, limiting its ability to assess LLMs in other languages.We propose the Multilingual Instruction Following Evaluation (M-IFEval) benchmark, expanding the evaluation to French, Japanese, and Spanish, with both general and language-specific instructions. Applying this benchmark to 8 state-of-the-art LLMs, we find that benchmark performance across languages and instruction types can vary widely, underscoring the importance of a multilingual benchmark for evaluating LLMs in a diverse cultural context.</abstract>
      <url hash="790ff7c7">2025.findings-naacl.344</url>
      <bibkey>dussolle-etal-2025-ifeval</bibkey>
    </paper>
    <paper id="345">
      <title>Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language</title>
      <author><first>Zhiqiang</first><last>Zhong</last></author>
      <author><first>Simon Sataa-Yu</first><last>Larsen</last></author>
      <author><first>Haoyu</first><last>Guo</last></author>
      <author><first>Tao</first><last>Tang</last></author>
      <author><first>Kuangyu</first><last>Zhou</last><affiliation>Microsoft</affiliation></author>
      <author><first>Davide</first><last>Mottin</last><affiliation>Aarhus University</affiliation></author>
      <pages>6177-6194</pages>
      <abstract>Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA<tex-math>^3</tex-math>, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA<tex-math>^3</tex-math> by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.Experimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA<tex-math>^3</tex-math> leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA<tex-math>^3</tex-math> notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility.</abstract>
      <url hash="b4e2e79e">2025.findings-naacl.345</url>
      <bibkey>zhong-etal-2025-automatic</bibkey>
    </paper>
    <paper id="346">
      <title>Let Modalities Teach Each Other: Modal-Collaborative Knowledge Extraction and Fusion for Multimodal Knowledge Graph Completion</title>
      <author><first>Guoliang</first><last>Zhu</last></author>
      <author><first>Tao</first><last>Ren</last></author>
      <author><first>Dandan</first><last>Wang</last><affiliation>Institute of Software Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Hu</last><affiliation>Institute of Software, CAS</affiliation></author>
      <pages>6195-6207</pages>
      <abstract>Multimodal knowledge graph completion (MKGC) aims to predict missing triples in MKGs using multimodal information. Recent research typically either extracts information from each modality separately to predict, then ensembles the predictions at the decision stage, or projects multiple modalities into a unified feature space to learn multimodal representations for prediction. However, these methods usually overlook the intrinsic correlation between modalities in MKGs which should be leveraged in both unimodal knowledge extraction and multimodal knowledge fusion. Motivated by this, we propose a noval Modal-collaborative knowledge learning (Moodle) framework for MKGC, the key idea of which is to foster mutual guidance and collaboration during unimodal knowledge extraction, to let each modality acquire distinct and complementary knowledge that subsequently enhances the multimodal knowledge fusion. Specifically, Moodle preserves the representations of different modalities to learn unimodal knowledge while modeling the mutual guidance through multi-task learning. Furthermore, Moodle performs multimodal knowledge fusion and prediction guided by unimodal knowledge, capturing their synergistic relationships and acquire fine-grained semantic knowledge through contrastive learning. Extensive experiments on three real-world datasets demonstrate the advantages of Moodle over state-of-the-art methods.</abstract>
      <url hash="0b585c44">2025.findings-naacl.346</url>
      <bibkey>zhu-etal-2025-modalities</bibkey>
    </paper>
    <paper id="347">
      <title>Modeling the Differential Prevalence of Online Supportive Interactions in Private Instant Messages of Adolescents</title>
      <author><first>Ondrej</first><last>Sotolar</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Michał</first><last>Tkaczyk</last></author>
      <author><first>Jaromír</first><last>Plhák</last><affiliation>Masaryk University</affiliation></author>
      <author><first>David</first><last>Smahel</last></author>
      <pages>6208-6226</pages>
      <abstract>This paper focuses on modeling gender-based and pair-or-group disparities in online supportive interactions among adolescents. To address the limitations of conventional social science methods in handling large datasets, this research employs language models to detect supportive interactions based on the Social Support Behavioral Code and to model their distribution. The study conceptualizes detection as a classification task, constructs a new dataset, and trains predictive models. The novel dataset comprises 196,772 utterances from 2165 users collected from Instant Messenger apps. The results show that the predictions of language models can be used to effectively model the distribution of supportive interactions in private online dialogues. As a result, this study provides new computational evidence that supports the theory that supportive interactions are more prevalent in online female-to-female conversations. The findings advance our understanding of supportive interactions in adolescent communication and present methods to automate the analysis of large datasets, opening new research avenues in computational social science.</abstract>
      <url hash="2e70ef8a">2025.findings-naacl.347</url>
      <bibkey>sotolar-etal-2025-modeling</bibkey>
    </paper>
    <paper id="348">
      <title>Dynamic Feature Fusion for Sign Language Translation Using <fixed-case>H</fixed-case>yper<fixed-case>N</fixed-case>etworks</title>
      <author><first>Ruiquan</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Zhao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Zhicong</first><last>Wu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Haoqi</first><last>Zhang</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>6227-6239</pages>
      <abstract>This paper presents an efficient dual-stream early fusion method for sign language translation. Inspired by the brain’s ability to process color, shape, and motion simultaneously, the method explores complex dependencies between RGB and keypoint streams, improving speed and efficiency. A key challenge is extracting complementary features from both streams while ensuring global semantic consistency to avoid conflicts and improve generalization. To address this issue, we propose a hypernetwork-based fusion strategy that effectively extracts salient features from RGB and keypoint streams, alongside a partial shortcut connection training method to strengthen the complementary information between the dual streams. Additionally, we introduce self-distillation and SST contrastive learning to maintain feature advantages while aligning the global semantic space. Experiments show that our method achieves state-of-the-art performance on two public sign language datasets, reducing model parameters by about two-thirds.</abstract>
      <url hash="3b928a91">2025.findings-naacl.348</url>
      <bibkey>zhang-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="349">
      <title>Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models</title>
      <author><first>Sonam</first><last>Gupta</last><affiliation>Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Yatin</first><last>Nandwani</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asaf</first><last>Yehudai</last></author>
      <author><first>Dinesh</first><last>Khandelwal</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Raghu</last><affiliation>IBM Research - New Delhi</affiliation></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>6240-6249</pages>
      <abstract>Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization.S3FT leverages the existence of multiple valid responses to a query.By utilizing the model’s correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples.The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to 4.4 on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. 2.5, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.</abstract>
      <url hash="e601902b">2025.findings-naacl.349</url>
      <bibkey>gupta-etal-2025-selective</bibkey>
    </paper>
    <paper id="350">
      <title><fixed-case>P</fixed-case>roverb<fixed-case>E</fixed-case>val: Exploring <fixed-case>LLM</fixed-case> Evaluation Challenges for Low-resource Language Understanding</title>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Atnafu Lambebo</first><last>Tonja</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tadesse Destaw</first><last>Belay</last></author>
      <author><first>Yonas</first><last>Chanie</last></author>
      <author><first>Bontu Fufa</first><last>Balcha</last></author>
      <author><first>Negasi Haile</first><last>Abadi</last><affiliation>Lesan AI</affiliation></author>
      <author><first>Henok Biadglign</first><last>Ademtew</last></author>
      <author><first>Mulubrhan Abebe</first><last>Nerea</last></author>
      <author><first>Debela Desalegn</first><last>Yadeta</last></author>
      <author><first>Derartu Dagne</first><last>Geremew</last><affiliation>Adama Science and Technology University and Gebeya Inc.</affiliation></author>
      <author><first>Assefa Atsbiha</first><last>Tesfu</last></author>
      <author><first>Philipp</first><last>Slusallek</last><affiliation>German Research Center for Artificial Intelligence (DFKI) and Saarland University</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>6250-6266</pages>
      <url hash="8e6da195">2025.findings-naacl.350</url>
      <bibkey>azime-etal-2025-proverbeval</bibkey>
    </paper>
    <paper id="351">
      <title><fixed-case>MRE</fixed-case>-<fixed-case>MI</fixed-case>: A Multi-image Dataset for Multimodal Relation Extraction in Social Media Posts</title>
      <author><first>Shizhou</first><last>Huang</last></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Donghua University, Shanghai</affiliation></author>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Yang</first><last>Yu</last></author>
      <author><first>Xin Alex</first><last>Lin</last></author>
      <pages>6267-6277</pages>
      <abstract>Despite recent advances in Multimodal Relation Extraction (MRE), existing datasets and approaches primarily focus on single-image scenarios, overlooking the prevalent real-world cases where relationships are expressed through multiple images alongside text. To address this limitation, we present MRE-MI, a novel human-annotated dataset that includes both multi-image and single-image instances for relation extraction. Beyond dataset creation, we establish comprehensive baselines and propose a simple model named Global and Local Relevance-Modulated Attention Model (GLRA) to address the new challenges in multi-image scenarios. Our extensive experiments reveal that incorporating multiple images substantially improves relation extraction in multi-image scenarios. Furthermore, GLRA achieves state-of-the-art results on MRE-MI, demonstrating its effectiveness. The datasets and source code can be found at https://github.com/JinFish/MRE-MI.</abstract>
      <url hash="5a8f5c7e">2025.findings-naacl.351</url>
      <bibkey>huang-etal-2025-mre</bibkey>
    </paper>
    <paper id="352">
      <title>Discrete Diffusion Language Model for Efficient Text Summarization</title>
      <author><first>Do Huu</first><last>Dat</last></author>
      <author><first>Duc Anh</first><last>Do</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <pages>6278-6290</pages>
      <abstract>While diffusion models excel at conditionally generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. This work addresses the limitations of prior discrete diffusion models for conditional long-text generation, particularly in the long abstractive summarization task. Despite faster decoding speeds compared to autoregressive methods, previous discrete diffusion models failed on the abstractive summarization task due to the incompatibility between the backbone architectures and the random noising process. To overcome these challenges, we introduce a novel semantic-aware noising process that enables Transformer backbones to handle long sequences effectively. Additionally, we propose CrossMamba, an adaptation of the Mamba model to the encoder-decoder paradigm, which integrates seamlessly with the random absorbing noising process. Our approaches outperform existing discrete diffusion models on three benchmark summarization datasets: Gigaword, CNN/DailyMail, and Arxiv, while also achieving much faster inference speed compared to autoregressive models.</abstract>
      <url hash="35057b6c">2025.findings-naacl.352</url>
      <bibkey>dat-etal-2025-discrete</bibkey>
    </paper>
    <paper id="353">
      <title><fixed-case>CAPE</fixed-case>: A <fixed-case>C</fixed-case>hinese Dataset for Appraisal-based Emotional Generation in Large Language Models</title>
      <author><first>June M.</first><last>Liu</last></author>
      <author><first>He</first><last>Cao</last></author>
      <author><first>Renliang</first><last>Sun</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Jiaxing</first><last>Zhang</last><affiliation>IDEA</affiliation></author>
      <pages>6291-6309</pages>
      <abstract>Generating emotionally appropriate responses in conversations with large language models presents a significant challenge due to the complexities of human emotions and cognitive processes, which remain largely underexplored in their critical role in social interactions. In this study, we introduce a two-stage automatic data generation framework to create CAPE, a Chinese dataset named Cognitive Appraisal theory-based Emotional corpus. This corpus facilitates the generation of dialogues with contextually appropriate emotional responses by accounting for diverse personal and situational factors. We propose two tasks utilizing this dataset: emotion prediction and next utterance prediction. Both automated and human evaluations demonstrate that agents trained on our dataset can deliver responses that are more aligned with human emotional expressions. Our study shows the potential for advancing emotional expression in conversational agents, paving the way for more nuanced and meaningful human-computer interactions.</abstract>
      <url hash="ea34ab16">2025.findings-naacl.353</url>
      <bibkey>liu-etal-2025-cape</bibkey>
    </paper>
    <paper id="354">
      <title>Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models</title>
      <author><first>Hongbang</first><last>Yuan</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>6310-6323</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success but still tend to generate factually erroneous responses, a phenomenon known as hallucination. A recent trend is to use preference learning to fine-tune models to align with factuality. However, existing work primarily evaluates fine-tuned models on in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets remains underexplored. In this paper, we conduct a comprehensive evaluation of the factuality of different models tuned by various preference learning algorithms and demonstrate that their performance on OOD datasets either increases minimally or decreases. Subsequently, we reveal that the main cause of model’s failure to uphold factuality under a distribution shift is <b>under-alignment</b>, rather than <b>over-alignment</b>, by analyzing the token distribution shift of the models before and after tuning. Finally, we propose <b>APEFT</b> (<b>A</b>tomic <b>P</b>reference <b>E</b>nhanced <b>F</b>actuality <b>T</b>uning), a framework that enhances model’s awareness of factuality at the granularity of individual facts. Extensive experiments demonstrate that APEFT improves model performance by an average of on both ID and OOD datasets, which is highly effective.</abstract>
      <url hash="4167fffa">2025.findings-naacl.354</url>
      <bibkey>yuan-etal-2025-beyond</bibkey>
    </paper>
    <paper id="355">
      <title>Weight-based Analysis of Detokenization in Language Models: Understanding the First Stage of Inference Without Inference</title>
      <author><first>Go</first><last>Kamoda</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Tatsuro</first><last>Inaba</last></author>
      <author><first>Keito</first><last>Kudo</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>MBZUAI, RIKEN and Tohoku University</affiliation></author>
      <pages>6324-6343</pages>
      <abstract>According to the stages-of-inference hypothesis, early layers of language models map their subword-tokenized input, which does not necessarily correspond to a linguistically meaningful segmentation, to more meaningful representations that form the model’s “inner vocabulary”.Prior analysis of this *detokenization* stage has predominantly relied on probing and interventions such as path patching, which involve selecting particular inputs, choosing a subset of components that will be patched, and then observing changes in model behavior.Here, we show that several important aspects of the detokenization stage can be understood purely by analyzing model weights, without performing any model inference steps.Specifically, we introduce an analytical decomposition of first-layer attention in GPT-2.Our decomposition yields interpretable terms that quantify the relative contributions of position-related, token-related, and mixed effects.By focusing on terms in this decomposition, we discover weight-based explanations of attention bias toward close tokens and attention for detokenization.</abstract>
      <url hash="4a122ff3">2025.findings-naacl.355</url>
      <bibkey>kamoda-etal-2025-weight</bibkey>
    </paper>
    <paper id="356">
      <title><fixed-case>D</fixed-case>i<fixed-case>PT</fixed-case>: Enhancing <fixed-case>LLM</fixed-case> Reasoning through Diversified Perspective-Taking</title>
      <author><first>Hoang Anh</first><last>Just</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Mahavir</first><last>Dabas</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Ming</first><last>Jin</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Ruoxi</first><last>Jia</last><affiliation>Virginia Tech</affiliation></author>
      <pages>6344-6374</pages>
      <abstract>Existing work on improving language model reasoning typically explores a single solution path, which can be prone to errors. Inspired by perspective-taking in social studies, this paper introduces DiPT, a novel approach that complements current reasoning methods by explicitly incorporating diversified viewpoints. This approach allows the model to gain a deeper understanding of the problem’s context and identify the most effective solution path during the inference stage. Additionally, it provides a general data-centric AI recipe for augmenting existing data to improve their quality for fine-tuning. Our empirical results demonstrate that DiPT can be flexibly integrated into existing methods that focus on a single reasoning approach, enhancing their reasoning performance and stability when presented with paraphrased problems. Furthermore, we illustrate improved context understanding by maintaining the model’s safe outputs against “jailbreaking” prompts intentionally designed to bypass safeguards built into deployed models. Lastly, we show that fine-tuning with data enriched with diverse perspectives can boost the reasoning capabilities of the model compared to fine-tuning with raw data alone.</abstract>
      <url hash="390f7999">2025.findings-naacl.356</url>
      <bibkey>just-etal-2025-dipt</bibkey>
    </paper>
    <paper id="357">
      <title><fixed-case>SOLID</fixed-case>: Self-seeding and Multi-intent Self-instructing <fixed-case>LLM</fixed-case>s for Generating Intent-aware Information-Seeking Dialogs</title>
      <author><first>Arian</first><last>Askari</last></author>
      <author><first>Roxana</first><last>Petcu</last><affiliation>University of Amsterdam and University of Amsterdam</affiliation></author>
      <author><first>Chuan</first><last>Meng</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Amin</first><last>Abolghasemi</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <pages>6375-6395</pages>
      <abstract>Intent prediction in information-seeking dialogs is challenging and requires a substantial amount of data with human-labeled intents for effective model training. While Large Language Models (LLMs) have demonstrated effectiveness in generating synthetic data, existing methods typically rely on human feedback and are tailored to structured, task-oriented intents. In this paper, we leverage LLMs for zero-shot generation of large-scale, open-domain, intent-aware information-seeking dialogs to serve as training data for intent prediction models. We introduce SOLID, a method that generates dialogs turn by turn using novel self-seeding and multi-intent self-instructing strategies. Additionally, we propose SOLID-RL, a finetuned version that generates an entire dialog in one step using data created with SOLID. SOLID and SOLID-RL are each used to generate over 300k intent-aware dialogs, significantly surpassing the size of existing datasets. Experiments show that intent prediction models trained on sampled dialogs generated by SOLID and SOLID-RL outperform those trained solely on human-generated dialogs. Our findings demonstrate the potential of LLMs to expand training datasets, as they provide valuable resources for conversational agents across multiple tasks. Our self-seeding and self-instructing approaches are adaptable to various conversational data types and languages with minimal modifications.</abstract>
      <url hash="742da042">2025.findings-naacl.357</url>
      <bibkey>askari-etal-2025-solid</bibkey>
    </paper>
    <paper id="358">
      <title><fixed-case>C</fixed-case>ollage<fixed-case>P</fixed-case>rompt: A Benchmark for Budget-Friendly Visual Recognition with <fixed-case>GPT</fixed-case>-4<fixed-case>V</fixed-case></title>
      <author><first>Siyu</first><last>Xu</last></author>
      <author><first>Yunke</first><last>Wang</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Daochang</first><last>Liu</last><affiliation>University of Western Australia</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chang</first><last>Xu</last><affiliation>University of Sydney</affiliation></author>
      <pages>6396-6418</pages>
      <url hash="bf2e8500">2025.findings-naacl.358</url>
      <bibkey>xu-etal-2025-collageprompt</bibkey>
    </paper>
    <paper id="359">
      <title><fixed-case>ARISE</fixed-case>: Iterative Rule Induction and Synthetic Data Generation for Text Classification</title>
      <author><first>Yaswanth</first><last>M</last></author>
      <author><first>Vaibhav</first><last>Singh</last></author>
      <author><first>Ayush</first><last>Maheshwari</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Amrith</first><last>Krishna</last><affiliation>Learno</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>6419-6434</pages>
      <abstract>We propose ARISE, a framework that iteratively induces rules and generates synthetic data for text classification. We combine synthetic data generation and automatic rule induction, via bootstrapping, to iteratively filter the generated rules and data. We induce rules via inductive generalisation of syntactic-ngrams, enabling us to capture a complementary source of supervision. These rules alone lead to performance gains in both, in-context learning (ICL) and fine-tuning (FT) settings. Similarly, use of augmented data from ARISE alone improves the performance for a model, outperforming configurations that rely on complex methods like contrastive learning. Further, our extensive experiments on various datasets covering three full-shot, eight few-shot and seven multilingual variant settings demonstrate that the rules and data we generate lead to performance improvements across these diverse domains and languages.</abstract>
      <url hash="87b02ca5">2025.findings-naacl.359</url>
      <bibkey>m-etal-2025-arise</bibkey>
    </paper>
    <paper id="360">
      <title>Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context</title>
      <author><first>Sangwon</first><last>Yu</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Ik-hwan</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jongyoon</first><last>Song</last><affiliation>Samsung Research</affiliation></author>
      <author><first>Saehyung</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Junsung</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>6435-6455</pages>
      <abstract>Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs’ performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the <b>misordered context</b> problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called <b>co</b>ntext <b>re</b>petition (<b>CoRe</b>), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model’s reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known “lost-in-the-middle” problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.</abstract>
      <url hash="774dd6cf">2025.findings-naacl.360</url>
      <bibkey>yu-etal-2025-unleashing</bibkey>
    </paper>
    <paper id="361">
      <title>Text Annotation via Inductive Coding: Comparing Human Experts to <fixed-case>LLM</fixed-case>s in Qualitative Data Analysis</title>
      <author><first>Angelina</first><last>Parfenova</last></author>
      <author><first>Andreas</first><last>Marfurt</last><affiliation>HSLU - Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Alexander</first><last>Denzler</last><affiliation>HSLU - Lucerne University of Applied Sciences and Arts</affiliation></author>
      <pages>6456-6469</pages>
      <abstract>This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM-generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.</abstract>
      <url hash="d186460a">2025.findings-naacl.361</url>
      <bibkey>parfenova-etal-2025-text</bibkey>
    </paper>
    <paper id="362">
      <title>Investigating the Zone of Proximal Development of Language Models for In-Context Learning</title>
      <author><first>Peng</first><last>Cui</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>6470-6483</pages>
      <abstract>In this paper, we introduce a learning analytics framework to analyze the in-context learning (ICL) behavior of large language models (LLMs) through the lens of the Zone of Proximal Development (ZPD), an established theory in educational psychology. ZPD delineates the range of tasks a learner can accomplish with appropriate guidance but not yet independently. We adapt this concept to ICL, measuring the ZPD of LLMs based on model performance on individual examples in different settings. Furthermore, we propose an item response theory (IRT) model to predict the distribution of zones for LLMs. Our findings reveal a series of intricate and multifaceted behaviors of ICL, providing new insights into understanding and leveraging this technique. Finally, we demonstrate how our framework can enhance LLM in both inference and fine-tuning scenarios: (1) By predicting a model’s zone distribution, we selectively apply ICL to queries that are most likely to benefit from demonstrations, achieving a better balance between inference cost and performance; (2) We propose a human-like curriculum for fine-tuning, which prioritizes examples within the model’s ZPD. The curriculum results in improved performance, and we explain its effectiveness through an analysis of the training dynamics of LLMs.</abstract>
      <url hash="8ae508ad">2025.findings-naacl.362</url>
      <bibkey>cui-sachan-2025-investigating</bibkey>
    </paper>
    <paper id="363">
      <title>Breaking <fixed-case>R</fixed-case>e<fixed-case>A</fixed-case>ct Agents: Foot-in-the-Door Attack Will Get You In</title>
      <author><first>Itay</first><last>Nakash</last><affiliation>International Business Machines</affiliation></author>
      <author><first>George</first><last>Kour</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Guy</first><last>Uziel</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>International Business Machines</affiliation></author>
      <pages>6484-6509</pages>
      <abstract>Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become prevalent.As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack.Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions.Our results show that once a ReAct agent’s thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a ‘foot-in-the-door’, allowing an attacker to embed malicious instructions into the agent’s thought process, making it more susceptible to harmful directives.To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.</abstract>
      <url hash="8a14dfff">2025.findings-naacl.363</url>
      <bibkey>nakash-etal-2025-breaking</bibkey>
    </paper>
    <paper id="364">
      <title>As easy as <fixed-case>PIE</fixed-case>: understanding when pruning causes language models to disagree</title>
      <author><first>Pietro</first><last>Tropeano</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Maria</first><last>Maistro</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Tuukka</first><last>Ruotsalo</last><affiliation>Lappeenranta University of Technology, University of Copenhagen and University of Helsinki</affiliation></author>
      <author><first>Christina</first><last>Lioma</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>6510-6536</pages>
      <abstract>Language Model (LM) pruning compresses the model by removing weights, nodes, or other parts of its architecture. Typically, pruning focuses on the resulting efficiency gains at the cost of effectiveness.However, when looking at how individual data pointsare affected by pruning, it turns out that a particular subset of data points always bears most of the brunt (in terms of reduced accuracy) when pruning,but this effect goes unnoticed when reporting the mean accuracy of all data points. These data points are called PIEs and have been studied in image processing, but not in NLP.In a study of various NLP datasets, pruning methods, and levels of compression, we find that PIEs impact inference quality considerably, regardless of class frequency, andthat BERT is more prone to this than BiLSTM. We also find that PIEs contain a high amount of data points that have the largest influence on how well the model generalises to unseen data. This means that when pruning, with seemingly moderate loss to accuracy across all data points, we in fact hurt tremendously those data points that matter the most. We trace what makes PIEs both hard and impactful to inference to their overall longer and more semantically complex text. These findings are novel and contribute to understanding how LMs are affected by pruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE</abstract>
      <url hash="6887a908">2025.findings-naacl.364</url>
      <bibkey>tropeano-etal-2025-easy</bibkey>
    </paper>
    <paper id="365">
      <title>Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction</title>
      <author><first>ShengbinYue</first><last>ShengbinYue</last></author>
      <author><first>Ting</first><last>Huang</last></author>
      <author><first>Zheng</first><last>Jia</last></author>
      <author><first>Siyuan</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Shujun</first><last>Liu</last></author>
      <author><first>Yun</first><last>Song</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>6537-6570</pages>
      <abstract>Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants’ characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs’ performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.</abstract>
      <url hash="2340311f">2025.findings-naacl.365</url>
      <bibkey>shengbinyue-etal-2025-multi</bibkey>
    </paper>
    <paper id="366">
      <title>Exploring Backward Reasoning in Large Language Models</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <pages>6571-6586</pages>
      <abstract>Multi-step reasoning through in-context learning strategies have been extensively explored, highlighting the abilities of Large Language Models (LLMs) to generate answers derived from step-by-step reasoning. These studies focus the attention on LLMs’ forward reasoning abilities epitomised in a series of general premises leading to a final solution. In this paper, by taking the reverse perspective, we study the backward reasoning abilities of LLMs, namely the inference that leads to the causal hypothesis. Behind formalising the backward problems, we analyse whether the LLMs are able to reason about the conclusion and reconstruct the original question that led to the delivery of the final answer. Operating with question-answering tasks involving symbolic reasoning, understanding, and commonsense abilities, we observe that the proposed models reveal robust comprehension capabilities managing different kinds of input; however, they are not always able to reason in the backward direction. Finally, to challenge this limitation, we demonstrate that instructing LLMs to generate the answer by reconsidering the structure of the problem allows for improved backward reasoning direction.</abstract>
      <url hash="0254cc5d">2025.findings-naacl.366</url>
      <bibkey>ranaldi-pucci-2025-exploring</bibkey>
    </paper>
    <paper id="367">
      <title><fixed-case>MMLF</fixed-case>: Multi-query Multi-passage Late Fusion Retrieval</title>
      <author><first>Yuan-Ching</first><last>Kuo</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Yi</first><last>Yu</last></author>
      <author><first>Chih-Ming</first><last>Chen</last></author>
      <author><first>Chuan-Ju</first><last>Wang</last><affiliation>Academia Sinica</affiliation></author>
      <pages>6587-6598</pages>
      <abstract>Leveraging large language models (LLMs) for query expansion has proven highly effective across diverse tasks and languages. Yet, challenges remain in optimizing query formatting and prompting, often with less focus on handling retrieval results. In this paper, we introduce Multi-query Multi-passage Late Fusion (MMLF), a straightforward yet potent pipeline that generates sub-queries, expands them into pseudo-documents, retrieves them individually, and aggregates results using reciprocal rank fusion. Our experiments demonstrate that MMLF exhibits superior performance across five BEIR benchmark datasets, achieving an average improvement of 4% and a maximum gain of up to 8% in both Recall@1k and nDCG@10 compared to state of the art across BEIR information retrieval datasets.</abstract>
      <url hash="74dd908f">2025.findings-naacl.367</url>
      <bibkey>kuo-etal-2025-mmlf</bibkey>
    </paper>
    <paper id="368">
      <title>Dynamic Guided and Domain Applicable Safeguards for Enhanced Security in Large Language Models</title>
      <author><first>Weidi</first><last>Luo</last></author>
      <author><first>He</first><last>Cao</last></author>
      <author><first>Zijing</first><last>Liu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Yu</first><last>Wang</last></author>
      <author><first>Aidan</first><last>Wong</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Bin</first><last>Feng</last><affiliation>International Digital Economy Academy, International Digital Economy Academy</affiliation></author>
      <author><first>Yuan</first><last>Yao</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yu</first><last>Li</last><affiliation>International Digital Economy Academy</affiliation></author>
      <pages>6599-6620</pages>
      <abstract>With the extensive deployment of Large Language Models (LLMs), ensuring their safety has become increasingly critical. However, existing defense methods often struggle with two key issues: (i) inadequate defense capabilities, particularly in domain-specific scenarios like chemistry, where a lack of specialized knowledge can lead to the generation of harmful responses to malicious queries. (ii) over-defensiveness, which compromises the general utility and responsiveness of LLMs. To mitigate these issues, we introduce a multi-agents-based defense framework, Guide for Defense (G4D), which leverages accurate external information to provide an unbiased summary of user intentions and analytically grounded safety response guidance. Extensive experiments on popular jailbreak attacks and benign datasets show that our G4D can enhance LLM’s robustness against jailbreak attacks on general and domain-specific scenarios without compromising the model’s general functionality.</abstract>
      <url hash="234ea196">2025.findings-naacl.368</url>
      <bibkey>luo-etal-2025-dynamic</bibkey>
    </paper>
    <paper id="369">
      <title>k<fixed-case>NN</fixed-case> For Whisper And Its Effect On Bias And Speaker Adaptation</title>
      <author><first>Maya K.</first><last>Nachesa</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Vlad</first><last>Niculae</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>6621-6627</pages>
      <abstract>Speech recognition performance varies by language, domain, and speaker characteristics such as accent, but fine-tuning a model on any of these categories may lead to catastrophic forgetting. Token-level <tex-math>k</tex-math> nearest neighbor search (<tex-math>k</tex-math>NN), first proposed for neural sequence decoders for natural language generation (NLG) and machine translation (MT), is a non-parametric method that instead adapts using inference-time search in an external datastore, without training the underlying model. We show that Whisper, a transformer end-to-end speech model, benefits from <tex-math>k</tex-math>NN. We investigate the differences between the speech and text setups. We discuss implications for speaker adaptation, and analyze improvements by gender, accent, and age.</abstract>
      <url hash="18f7af86">2025.findings-naacl.369</url>
      <bibkey>nachesa-niculae-2025-knn</bibkey>
    </paper>
    <paper id="370">
      <title><fixed-case>V</fixed-case>isual<fixed-case>C</fixed-case>oder: Guiding Large Language Models in Code Execution with Fine-grained Multimodal Chain-of-Thought Reasoning</title>
      <author><first>Cuong Le</first><last>Chi</last></author>
      <author><first>Chau Truong Vinh</first><last>Hoang</last><affiliation>FPT</affiliation></author>
      <author><first>Phan Nhật</first><last>Huy</last></author>
      <author><first>Dung D.</first><last>Le</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Tien N</first><last>Nguyen</last><affiliation>university of texas at dallas</affiliation></author>
      <author><first>Nghi D. Q.</first><last>Bui</last></author>
      <pages>6628-6645</pages>
      <abstract>Predicting program behavior and reasoning about code execution remain significant challenges in software engineering, particularly for large language models (LLMs) designed for code analysis. While these models excel at understanding static syntax, they often struggle with dynamic reasoning tasks. We introduce VisualCoder, a simple yet effective approach that enhances code reasoning by integrating multimodal Chain-of-Thought (CoT) reasoning with a visual Control Flow Graph (CFG). By aligning code snippets with their corresponding CFGs, VisualCoder provides deeper insights into execution flows. We address challenges in multimodal CoT integration through a reference mechanism, ensuring consistency between code and its execution path, thereby improving performance in program behavior prediction, error detection, and output generation.</abstract>
      <url hash="02e7f9d3">2025.findings-naacl.370</url>
      <bibkey>chi-etal-2025-visualcoder</bibkey>
    </paper>
    <paper id="371">
      <title>Optimizing <fixed-case>LLM</fixed-case>s for <fixed-case>I</fixed-case>talian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation</title>
      <author><first>Luca</first><last>Moroni</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Giovanni</first><last>Puccetti</last><affiliation>CNR</affiliation></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last><affiliation>Facebook</affiliation></author>
      <author><first>Andrei Stefan</first><last>Bejgu</last></author>
      <author><first>Alessio</first><last>Miaschi</last><affiliation>Institute for Computational Linguistics “A. Zampolli” (CNR-ILC), Pisa</affiliation></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <author><first>Andrea</first><last>Esuli</last><affiliation>CNR</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>6646-6660</pages>
      <abstract>The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token “fertility”) and slower inference speed.In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7B-v0.1, reducing token fertility by 25%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.</abstract>
      <url hash="bf628580">2025.findings-naacl.371</url>
      <bibkey>moroni-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="372">
      <title>Beyond the Mode: Sequence-Level Distillation of Multilingual Translation Models for Low-Resource Language Pairs</title>
      <author><first>Aarón</first><last>Galiano-Jiménez</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last><affiliation>University of Alicante</affiliation></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last><affiliation>Universidad de Alicante</affiliation></author>
      <pages>6661-6676</pages>
      <abstract>This paper delves into sequence-level knowledge distillation (KD) of multilingual pre-trained translation models. We posit that, beyond the approximated mode obtained via beam search, the whole output distribution of the teacher contains valuable insights for students. We explore the potential of n-best lists from beam search to guide student’s learning and then investigate alternative decoding methods to address observed issues like low variability and under-representation of infrequent tokens. Our research in data-limited scenarios reveals that although sampling methods can slightly compromise the translation quality of the teacher output compared to beam search based methods, they enrich the generated corpora with increased variability and lexical richness, ultimately enhancing student model performance and reducing the gender bias amplification commonly associated with KD.</abstract>
      <url hash="df7cc2fc">2025.findings-naacl.372</url>
      <bibkey>galiano-jimenez-etal-2025-beyond</bibkey>
    </paper>
    <paper id="373">
      <title><fixed-case>LLM</fixed-case>s for Extremely Low-Resource <fixed-case>F</fixed-case>inno-<fixed-case>U</fixed-case>gric Languages</title>
      <author><first>Taido</first><last>Purason</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Hele-Andra</first><last>Kuulmets</last></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>6677-6697</pages>
      <abstract>The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on Võro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP.</abstract>
      <url hash="97f36fd5">2025.findings-naacl.373</url>
      <bibkey>purason-etal-2025-llms</bibkey>
    </paper>
    <paper id="374">
      <title><fixed-case>LOFT</fixed-case>: Scalable and More Realistic Long-Context Evaluation</title>
      <author><first>Jinhyuk</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Anthony</first><last>Chen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zhuyun</first><last>Dai</last><affiliation>Google</affiliation></author>
      <author><first>Dheeru</first><last>Dua</last><affiliation>Google</affiliation></author>
      <author><first>Devendra Singh</first><last>Sachan</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Michael</first><last>Boratko</last><affiliation>Google</affiliation></author>
      <author><first>Yi</first><last>Luan</last><affiliation>Google</affiliation></author>
      <author><first>Séb</first><last>Arnold</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Siddharth</first><last>Dalmia</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Hexiang</first><last>Hu</last><affiliation>xAI</affiliation></author>
      <author><first>Xudong</first><last>Lin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Panupong</first><last>Pasupat</last><affiliation>Google</affiliation></author>
      <author><first>Aida</first><last>Amini</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Jeremy R.</first><last>Cole</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Google and University College London</affiliation></author>
      <author><first>Iftekhar</first><last>Naim</last><affiliation>Google</affiliation></author>
      <author><first>Ming-Wei</first><last>Chang</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>Kelvin</first><last>Guu</last><affiliation>Google</affiliation></author>
      <pages>6698-6723</pages>
      <abstract>Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs’ ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs’ performance on in-context retrieval and reasoning. Our findings reveal LCLMs’ surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks. However, LCLMs still face challenges in areas like compositional reasoning that are required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their capabilities to tackle existing paradigms.</abstract>
      <url hash="ae323f32">2025.findings-naacl.374</url>
      <bibkey>lee-etal-2025-loft</bibkey>
    </paper>
    <paper id="375">
      <title>On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>6724-6736</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.</abstract>
      <url hash="5c9fa2f5">2025.findings-naacl.375</url>
      <bibkey>vladika-matthes-2025-influence</bibkey>
    </paper>
    <paper id="376">
      <title>Aligning Black-box Language Models with Human Judgments</title>
      <author><first>Gerrit J.j.</first><last>Van Den Burg</last><affiliation>Amazon</affiliation></author>
      <author><first>Gen</first><last>Suzuki</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Murat</first><last>Sensoy</last></author>
      <pages>6737-6749</pages>
      <abstract>Large language models (LLMs) are increasingly used as automated judges to evaluate recommendation systems, search engines, and other subjective tasks, where relying on human evaluators can be costly, time-consuming, and unscalable. LLMs offer an efficient solution for continuous, automated evaluation. However, since the systems that are built and improved with these judgments are ultimately designed for human use, it is crucial that LLM judgments align closely with human evaluators to ensure such systems remain human-centered. On the other hand, aligning LLM judgments with human evaluators is challenging due to individual variability and biases in human judgments. We propose a simple yet effective framework to align LLM judgments with individual human evaluators or their aggregated judgments, without retraining or fine-tuning the LLM. Our approach learns a linear mapping between the LLM’s outputs and human judgments, achieving over 142% average improvement in agreement across 29 tasks with only a small number of calibration examples used for training. Notably, our method works in zero-shot and few-shot settings, exceeds inter-human agreement on four out of six tasks, and enables smaller LLMs to achieve performance comparable to that of larger models.</abstract>
      <url hash="ba1eca59">2025.findings-naacl.376</url>
      <bibkey>van-den-burg-etal-2025-aligning</bibkey>
    </paper>
    <paper id="377">
      <title>Guideline Compliance in Task-Oriented Dialogue: The Chained Prior Approach</title>
      <author><first>Xiangyu</first><last>Wen</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jianyuan</first><last>Zhong</last></author>
      <author><first>Zhijian</first><last>Xu</last></author>
      <author><first>Qiang</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6750-6776</pages>
      <abstract>Task-oriented dialogue (TOD) systems are widely used across various domains, including customer service, appointment scheduling, and technical support. In real-world scenarios, such systems must adhere to given operational guidelines. However, existing solutions based on large language models often cannot achieve strict guideline compliance, even when fine-tuned with domain knowledge. To address this issue, we introduce a novel TOD system named GuidedTOD, which explicitly considers domain-specific guidelines by integrating a policy module. This module employs a Markov Chain, termed Chained Prior, to efficiently encode and dynamically update guideline knowledge. During inference, the Chained Prior re-ranks outputs from the domain-expert language model using beam search, ensuring guideline adherence. Experimental results show that GuidedTOD significantly improves guideline compliance, achieving approximately 20% better action prediction accuracy than state-of-the-art solutions. Code is available here: https://github.com/cure-lab/GuidedTOD.</abstract>
      <url hash="427addb2">2025.findings-naacl.377</url>
      <bibkey>wen-etal-2025-guideline</bibkey>
    </paper>
    <paper id="378">
      <title><fixed-case>A</fixed-case>uto<fixed-case>B</fixed-case>reach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization via Multi-<fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Xiao</first><last>Yang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhengwei</first><last>Fang</last></author>
      <author><first>Yu</first><last>Tian</last></author>
      <author><first>Yinpeng</first><last>Dong</last></author>
      <author><first>Zhaoxia</first><last>Yin</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Tsinghua University</affiliation></author>
      <pages>6777-6798</pages>
      <abstract>Recent studies show that large language models (LLMs) are vulnerable to jailbreak attacks, which can bypass their defense mechanisms. However, existing jailbreak research often exhibits limitations in universality, validity, and efficiency. Therefore, we rethink jailbreaking LLMs and define three key properties to guide the design of effective jailbreak methods. We introduce AutoBreach, a novel black-box approach that uses wordplay-guided mapping rule sampling to create universal adversarial prompts. By leveraging LLMs’ summarization and reasoning abilities, AutoBreach minimizes manual effort. To boost jailbreak success rates, we further suggest sentence compression and chain-of-thought-based mapping rules to correct errors and wordplay misinterpretations in target LLMs. Also, we propose a two-stage mapping rule optimization that initially optimizes mapping rules before querying target LLMs to enhance efficiency. Experimental results indicate AutoBreach efficiently identifies security vulnerabilities across various LLMs (Claude-3, GPT-4, etc.), achieving an average success rate of over 80% with fewer than 10 queries. Notably, the adversarial prompts generated by AutoBreach for GPT-4 can directly bypass the defenses of the advanced commercial LLM GPT o1-preview, demonstrating strong transferability and universality.</abstract>
      <url hash="a999a794">2025.findings-naacl.378</url>
      <bibkey>chen-etal-2025-autobreach</bibkey>
    </paper>
    <paper id="379">
      <title><tex-math>\mathcal{S}^2</tex-math><fixed-case>IT</fixed-case>: Stepwise Syntax Integration Tuning for Large Language Models in Aspect Sentiment Quad Prediction</title>
      <author><first>Bingfeng</first><last>Chen</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Chenjie</first><last>Qiu</last></author>
      <author><first>Yifeng</first><last>Xie</last></author>
      <author><first>Boyan</first><last>Xu</last></author>
      <author><first>Ruichu</first><last>Cai</last><affiliation>Guangdong University of Technology</affiliation></author>
      <author><first>Zhifeng</first><last>Hao</last><affiliation>Shantou University</affiliation></author>
      <pages>6799-6806</pages>
      <url hash="1c58c7b4">2025.findings-naacl.379</url>
      <bibkey>chen-etal-2025-s2it</bibkey>
    </paper>
    <paper id="380">
      <title><fixed-case>B</fixed-case>an<fixed-case>NERD</fixed-case>: A Benchmark Dataset and Context-Driven Approach for <fixed-case>B</fixed-case>angla Named Entity Recognition</title>
      <author><first>Md. Motahar</first><last>Mahtab</last><affiliation>GIGATECH, BEXIMCO</affiliation></author>
      <author><first>Faisal Ahamed</first><last>Khan</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Md. Ekramul</first><last>Islam</last><affiliation>Giga Tech Limited.</affiliation></author>
      <author><first>Md. Shahad Mahmud</first><last>Chowdhury</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Labib Imam</first><last>Chowdhury</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Sadia</first><last>Afrin</last><affiliation>Heinrich-Heine Universität Düsseldorf and Giga Tech Limited</affiliation></author>
      <author><first>Hazrat</first><last>Ali</last><affiliation>Giga Tech Limited</affiliation></author>
      <author><first>Mohammad Mamun Or</first><last>Rashid</last></author>
      <author><first>Nabeel</first><last>Mohammed</last><affiliation>North South University</affiliation></author>
      <author><first>Mohammad Ruhul</first><last>Amin</last><affiliation>Fordham University</affiliation></author>
      <pages>6807-6828</pages>
      <abstract>In this study, we introduce **BanNERD**, the most extensive human-annotated and validated **B**angla **N**amed **E**ntity **R**ecognition **D**ataset to date, comprising over 85,000 sentences. **BanNERD** is curated from a diverse array of sources, spanning over 29 domains, thereby offering a comprehensive range of generalized contexts. To ensure the dataset’s quality, expert linguists developed a detailed annotation guideline tailored to the Bangla language. All annotations underwent rigorous validation by a team of validators, with final labels being determined via majority voting, thereby ensuring the highest annotation quality and a high IAA score of 0.88. In a cross-dataset evaluation, models trained on BanNERD consistently outperformed those trained on four existing Bangla NER datasets. Additionally, we propose a method named **BanNERCEM** (Bangla NER context-ensemble Method) which outperforms existing approaches on Bangla NER datasets and performs competitively on English datasets using lightweight Bangla pretrained LLMs. Our approach passes each context separately to the model instead of previous concatenation-based approaches achieving the highest average macro F1 score of 81.85% across 10 NER classes, outperforming previous approaches and ensuring better context utilization. We are making the code and datasetspublicly available at https://github.com/eblict-gigatech/BanNERD in order to contribute to the further advancement of Bangla NLP.</abstract>
      <url hash="6b5dc741">2025.findings-naacl.380</url>
      <bibkey>mahtab-etal-2025-bannerd</bibkey>
    </paper>
    <paper id="381">
      <title>Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias</title>
      <author><first>Andres</first><last>Algaba</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Carmen</first><last>Mazijn</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Vincent</first><last>Holst</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Floriano</first><last>Tori</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <author><first>Sylvia</first><last>Wenmackers</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Vincent</first><last>Ginis</last><affiliation>Vrije Universiteit Brussel</affiliation></author>
      <pages>6829-6864</pages>
      <abstract>Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4’s knowledge cut-off date. In our experiment, LLMs are tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias, which persists even after controlling for publication year, title length, number of authors, and venue. The results hold for both GPT-4, and the more capable models GPT-4o and Claude 3.5 where the papers are part of the training data. Additionally, we observe a large consistency between the characteristics of LLM’s existing and non-existent generated references, indicating the model’s internalization of citation patterns. By analyzing citation graphs, we show that the references recommended are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases, such as the Matthew effect, and introduce new ones, potentially skewing scientific knowledge dissemination.</abstract>
      <url hash="5f9d1552">2025.findings-naacl.381</url>
      <bibkey>algaba-etal-2025-large</bibkey>
    </paper>
    <paper id="382">
      <title>What can Large Language Models Capture about Code Functional Equivalence?</title>
      <author><first>Nickil</first><last>Maveli</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Antonio</first><last>Vergari</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Shay B</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>6865-6903</pages>
      <abstract>Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code)-LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.</abstract>
      <url hash="3e409ff4">2025.findings-naacl.382</url>
      <bibkey>maveli-etal-2025-large</bibkey>
    </paper>
    <paper id="383">
      <title>Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning</title>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Shaoxiong</first><last>Feng</last><affiliation>RedNote</affiliation></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Yueqi</first><last>Zhang</last></author>
      <author><first>Chuyi</first><last>Tan</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>6904-6917</pages>
      <abstract>Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information of batch queries from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the overall cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.</abstract>
      <url hash="0a794c00">2025.findings-naacl.383</url>
      <bibkey>wang-etal-2025-make</bibkey>
    </paper>
    <paper id="384">
      <title>Large Language Models Are Better Logical Fallacy Reasoners with Counterargument, Explanation, and Goal-Aware Prompt Formulation</title>
      <author><first>Jiwon</first><last>Jeong</last></author>
      <author><first>Hyeju</first><last>Jang</last><affiliation>Indiana University</affiliation></author>
      <author><first>Hogun</first><last>Park</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>6918-6937</pages>
      <abstract>The advancement of Large Language Models (LLMs) has greatly improved our ability to process complex language. However, accurately detecting logical fallacies remains a significant challenge. This study presents a novel and effective prompt formulation approach for logical fallacy detection, applicable in both supervised (fine-tuned) and unsupervised (zero-shot) settings. Our method enriches input text by incorporating implicit contextual information—counterarguments, explanations, and goals—which we query for validity within the argument’s context. We then rank these queries based on confidence scores to inform classification. We evaluate our approach across multiple datasets from 5 domains, covering 29 distinct fallacy types, using models from GPT and LLaMA series. The results show substantial improvements over state-of-the-art models: up to a 0.57 increase in F1-score in zero-shot settings and up to 0.45 in fine-tuned models. Extensive analyses further illustrate why and how our method excels.</abstract>
      <url hash="4374ffc8">2025.findings-naacl.384</url>
      <bibkey>jeong-etal-2025-large</bibkey>
    </paper>
    <paper id="385">
      <title><fixed-case>M</fixed-case>orph<fixed-case>NLI</fixed-case>: A Stepwise Approach to Natural Language Inference Using Text Morphing</title>
      <author><first>Vlad Andrei</first><last>Negru</last><affiliation>Technical University of Cluj-Napoca</affiliation></author>
      <author><first>Robert</first><last>Vacareanu</last><affiliation>Scale AI</affiliation></author>
      <author><first>Camelia</first><last>Lemnaru</last><affiliation>Technical University of Cluj-Napoca</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Rodica</first><last>Potolea</last></author>
      <pages>6938-6953</pages>
      <abstract>We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into entailment, contradiction, neutral, we use a language model to generate the necessary edits to incrementally transform (i.e., morph) the premise into the hypothesis. Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output. We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines with improvements up to 12.6% (relative). Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.</abstract>
      <url hash="4da48529">2025.findings-naacl.385</url>
      <bibkey>negru-etal-2025-morphnli</bibkey>
    </paper>
    <paper id="386">
      <title>Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-<fixed-case>SQL</fixed-case> Systems</title>
      <author><first>Đorđe</first><last>Klisura</last></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>6954-6976</pages>
      <abstract>Text-to-SQL systems empower users to interact with databases using natural language, automatically translating queries into executable SQL code. However, their reliance on database schema information for SQL generation exposes them to significant security vulnerabilities, particularly schema inference attacks that can lead to unauthorized data access or manipulation. In this paper, we introduce a novel zero-knowledge framework for reconstructing the underlying database schema of text-to-SQL models without any prior knowledge of the database. Our approach systematically probes text-to-SQL models with specially crafted questions and leverages a surrogate GPT-4 model to interpret the outputs, effectively uncovering hidden schema elements—including tables, columns, and data types. We demonstrate that our method achieves high accuracy in reconstructing table names, with F1 scores of up to .99 for generative models and .78 for fine-tuned models, underscoring the severity of schema leakage risks. We also show that our attack can steal prompt information in non-text-to-SQL models. Furthermore, we propose a simple protection mechanism for generative models and empirically show its limitations in mitigating these attacks.</abstract>
      <url hash="dd0b84b8">2025.findings-naacl.386</url>
      <bibkey>klisura-rios-2025-unmasking</bibkey>
    </paper>
    <paper id="387">
      <title>Media of Langue: Exploring Word Translation Network</title>
      <author><first>Goki</first><last>Muramoto</last></author>
      <author><first>Atsuki</first><last>Sato</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Takayoshi</first><last>Koyama</last></author>
      <pages>6977-6994</pages>
      <abstract>In the human activity of word translation, two languages face each other, mutually searching their own language system for the semantic place of words in the other language. We discover the huge network formed by the chain of these mutual translations as *Word Translation Network*, a network where words are nodes, and translation volume is represented as edges, and propose *Word Translation Map*, a novel interface for exploring this network. *Word Translation Map* points to the semantic configurations of many words in multiple languages at once, containing the information of existing dictionaries such as bilingual and synonym dictionaries. We have also implemented and published this interface as a web application, focusing on seven language pairs. This paper first defines the *Word Translation Network* and describes how to actually construct the network from bilingual corpora, followed by an analysis of the properties of the network. Next, we explain how to design a *Word Translation Map* using this network, and finally, we analyze the features of the *Word Translation Map* as a dictionary. The web application is publicly accessible at www.media-of-langue.org.</abstract>
      <url hash="5743e170">2025.findings-naacl.387</url>
      <bibkey>muramoto-etal-2025-media</bibkey>
    </paper>
    <paper id="388">
      <title>Tackling Social Bias against the Poor: a Dataset and a Taxonomy on Aporophobia</title>
      <author><first>Georgina</first><last>Curto</last><affiliation>United Nations University Institute in Macau</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Muhammad Hammad Fahim</first><last>Siddiqui</last></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada and University of Ottawa</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>6995-7016</pages>
      <abstract>Eradicating poverty is the first goal in the U.N. Sustainable Development Goals. However, aporophobia – the societal bias against people living in poverty – constitutes a major obstacle to designing, approving and implementing poverty-mitigation policies. This work presents an initial step towards operationalizing the concept of aporophobia to identify and track harmful beliefs and discriminative actions against poor people on social media. In close collaboration with non-profits and governmental organizations, we conduct data collection and exploration. Then we manually annotate a corpus of English tweets from five world regions for the presence of (1) direct expressions of aporophobia, and (2) statements referring to or criticizing aporophobic views or actions of others, to comprehensively characterize the social media discourse related to bias and discrimination against the poor. Based on the annotated data, we devise a taxonomy of categories of aporophobic attitudes and actions expressed through speech on social media. Finally, we train several classifiers and identify the main challenges for automatic detection of aporophobia in social networks. This work paves the way towards identifying, tracking, and mitigating aporophobic views on social media at scale.</abstract>
      <url hash="d88107e0">2025.findings-naacl.388</url>
      <bibkey>curto-etal-2025-tackling</bibkey>
    </paper>
    <paper id="389">
      <title>The <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Knowledge Graph: Infusing <fixed-case>ASL</fixed-case> Models with Linguistic Knowledge</title>
      <author><first>Lee</first><last>Kezar</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nidhi</first><last>Munikote</last></author>
      <author><first>Zian</first><last>Zeng</last><affiliation>University of Hawaii System</affiliation></author>
      <author><first>Zed</first><last>Sehyr</last><affiliation>Chapman University</affiliation></author>
      <author><first>Naomi</first><last>Caselli</last><affiliation>Boston University, Boston University</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <pages>7017-7029</pages>
      <abstract>Sign language models could make modern language technologies more accessible to those who sign, but the supply of accurately labeled data struggles to meet the demand associated with training large, end-to-end neural models. As an alternative to this approach, we explore how knowledge about the linguistic structure of signs may be used as inductive priors for learning sign recognition and comprehension tasks. We first construct the American Sign Language Knowledge Graph (ASLKG) from 11 sources of linguistic knowledge, with emphasis on features related to signs’ phonological and lexical-semantic properties. Then, we use the ASLKG to train neuro-symbolic models on ASL video input tasks, achieving accuracies of 91% for isolated sign recognition, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.</abstract>
      <url hash="f581bd04">2025.findings-naacl.389</url>
      <bibkey>kezar-etal-2025-american</bibkey>
    </paper>
    <paper id="390">
      <title>Reinforcement Learning for Aligning Large Language Models Agents with Interactive Environments: Quantifying and Mitigating Prompt Overfitting</title>
      <author><first>Mohamed Salim</first><last>Aissi</last></author>
      <author><first>Clément</first><last>Romac</last><affiliation>Inria and Hugging Face</affiliation></author>
      <author><first>Thomas</first><last>Carta</last></author>
      <author><first>Sylvain</first><last>Lamprier</last><affiliation>Université d’Angers</affiliation></author>
      <author><first>Pierre-Yves</first><last>Oudeyer</last><affiliation>Inria</affiliation></author>
      <author><first>Olivier</first><last>Sigaud</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Laure</first><last>Soulier</last><affiliation>Sorbonne Université, CNRS, ISIR</affiliation></author>
      <author><first>Nicolas</first><last>Thome</last><affiliation>sorbonne université</affiliation></author>
      <pages>7030-7046</pages>
      <abstract>Reinforcement learning (RL) is a promising approach for aligning large language models (LLMs) knowledge with sequential decision-making tasks. However, few studies have thoroughly investigated the impact on LLM agents capabilities of fine-tuning them with RL in a specific environment. In this paper, we propose a novel framework to analyze the sensitivity of LLMs to prompt formulations following RL training in a textual environment. Our findings reveal that the performance of LLMs degrades when faced with prompt formulations different from those used during the RL training phase. Besides, we analyze the source of this sensitivity by examining the model’s internal representations and salient tokens. Finally, we propose to use a contrastive loss to mitigate this sensitivity and improve the robustness and generalization capabilities of LLMs.</abstract>
      <url hash="4711d0e0">2025.findings-naacl.390</url>
      <bibkey>aissi-etal-2025-reinforcement</bibkey>
    </paper>
    <paper id="391">
      <title>An empirical study of validating synthetic data for formula generation</title>
      <author><first>Usneek</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>José</first><last>Cambronero</last></author>
      <author><first>Sumit</first><last>Gulwani</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Aditya</first><last>Kanade</last><affiliation>Microsoft</affiliation></author>
      <author><first>Anirudh</first><last>Khatry</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mukul</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Gust</first><last>Verbruggen</last><affiliation>Microsoft</affiliation></author>
      <pages>7047-7054</pages>
      <abstract>Large language models (LLMs) can be leveraged to help write formulas in spreadsheets, but formula data resources are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use another model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the natural language (NL) generated by the LLM is accurate for it to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.</abstract>
      <url hash="a3a85af2">2025.findings-naacl.391</url>
      <bibkey>singh-etal-2025-empirical</bibkey>
    </paper>
    <paper id="392">
      <title><fixed-case>T</fixed-case>e<fixed-case>C</fixed-case>o<fixed-case>F</fixed-case>e<fixed-case>S</fixed-case>: Text Column Featurization using Semantic Analysis</title>
      <author><first>Ananya</first><last>Singha</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Mukul</first><last>Singh</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ashish</first><last>Tiwari</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sumit</first><last>Gulwani</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chris</first><last>Parnin</last><affiliation>North Carolina State University</affiliation></author>
      <pages>7055-7061</pages>
      <abstract>Extracting insights from text columns can bechallenging and time-intensive. Existing methods for topic modeling and feature extractionare based on syntactic features and often overlook the semantics. We introduce the semantictext column featurization problem, and presenta scalable approach for automatically solvingit. We extract a small sample smartly, use alarge language model (LLM) to label only thesample, and then lift the labeling to the wholecolumn using text embeddings. We evaluateour approach by turning existing text classification benchmarks into semantic categorization benchmarks. Our approach performs better than baselines and naive use of LLMs.</abstract>
      <url hash="a5dca2c8">2025.findings-naacl.392</url>
      <bibkey>singha-etal-2025-tecofes</bibkey>
    </paper>
    <paper id="393">
      <title><fixed-case>CA</fixed-case>*: Addressing Evaluation Pitfalls in Computation-Aware Latency for Simultaneous Speech Translation</title>
      <author><first>Xi</first><last>Xu</last></author>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Siqi</first><last>Ouyang</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>7062-7067</pages>
      <abstract>Simultaneous speech translation (SimulST) systems must balance translation quality with response time, making latency measurement crucial for evaluating their real-world performance. However, there has been a longstanding belief that current metrics yield unrealistically high latency measurements in unsegmented streaming settings. In this paper, we investigate this phenomenon, revealing its root cause in a fundamental misconception underlying existing latency evaluation approaches. We demonstrate that this issue affects not only streaming but also segment-level latency evaluation across different metrics. Furthermore, we propose a modification to correctly measure computation-aware latency for SimulST systems, addressing the limitations present in existing metrics.</abstract>
      <url hash="14a7f1e4">2025.findings-naacl.393</url>
      <bibkey>xu-etal-2025-ca</bibkey>
    </paper>
    <paper id="394">
      <title>Augmented Adversarial Trigger Learning</title>
      <author><first>Zhe</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yanjun</first><last>Qi</last><affiliation>Amazon and University of Virginia</affiliation></author>
      <pages>7068-7100</pages>
      <abstract>Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs.</abstract>
      <url hash="27d7b04c">2025.findings-naacl.394</url>
      <bibkey>wang-qi-2025-augmented</bibkey>
    </paper>
    <paper id="395">
      <title>Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Qiusi</first><last>Zhan</last></author>
      <author><first>Richard</first><last>Fang</last></author>
      <author><first>Henil Shalin</first><last>Panchal</last></author>
      <author><first>Daniel</first><last>Kang</last></author>
      <pages>7101-7117</pages>
      <abstract>Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments. However, integrating external tools introduces security risks, such as indirect prompt injection (IPI) attacks. Despite defenses designed for IPI attacks, their robustness remains questionable due to insufficient testing against adaptive attacks.In this paper, we evaluate eight different defenses and bypass all of them using adaptive attacks, consistently achieving an attack success rate of over 50%.This reveals critical vulnerabilities in current defenses. Our research underscores the need for adaptive attack evaluation when designing defenses to ensure robustness and reliability.The code is available at https://github.com/uiuc-kang-lab/AdaptiveAttackAgent.</abstract>
      <url hash="d92ec17c">2025.findings-naacl.395</url>
      <bibkey>zhan-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="396">
      <title>Flaming-hot Initiation with Regular Execution Sampling for Large Language Models</title>
      <author><first>Weizhe</first><last>Chen</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Zhicheng</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Guanlin</first><last>Liu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Renjie</first><last>Zheng</last><affiliation>ByteDance</affiliation></author>
      <author><first>Wenlei</first><last>Shi</last></author>
      <author><first>Chen</first><last>Dun</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zheng</first><last>Wu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Xing</first><last>Jin</last></author>
      <author><first>Lin</first><last>Yan</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>7118-7127</pages>
      <abstract>Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This becomes especially critical in reasoning-related tasks with sandbox checkers, such as math or code, where the goal is to generate correct solutions to specific problems with higher probability. In this work, we introduce Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet highly effective method to efficiently find good responses. Our empirical findings show that FIRE sampling enhances inference-time generation quality and also benefits training in the alignment stage. Furthermore, we explore how FIRE sampling improves performance by promoting diversity and analyze the impact of employing FIRE at different positions within a response.</abstract>
      <url hash="019c9cea">2025.findings-naacl.396</url>
      <bibkey>chen-etal-2025-flaming</bibkey>
    </paper>
    <paper id="397">
      <title><fixed-case>HEISIR</fixed-case>: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sangyeop</first><last>Kim</last><affiliation>Coxwave and Seoul National University</affiliation></author>
      <author><first>Hangyeul</first><last>Lee</last></author>
      <author><first>Yohan</first><last>Lee</last><affiliation>Coxwave</affiliation></author>
      <pages>7128-7144</pages>
      <abstract>The growth of conversational AI services has increased demand for effective information retrieval from dialogue data. However, existing methods often face challenges in capturing semantic intent or require extensive labeling and fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted Semantic Indexing for Retrieval), a novel framework that enhances semantic understanding in conversational data retrieval through optimized data ingestion, eliminating the need for resource-intensive labeling or model adaptation.HEISIR implements a two-step process: (1) Hierarchical Triplets Formulation and (2) Adjunct Augmentation, creating semantic indices consisting of Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured representation effectively captures the underlying semantic information from dialogue content. HEISIR achieves high retrieval performance while maintaining low latency during the actual retrieval process. Our experimental results demonstrate that HEISIR outperforms fine-tuned models across various embedding types and language models. Beyond improving retrieval capabilities, HEISIR also offers opportunities for intent and topic analysis in conversational data, providing a versatile solution for dialogue systems.</abstract>
      <url hash="3d38f8d4">2025.findings-naacl.397</url>
      <bibkey>kim-etal-2025-heisir</bibkey>
    </paper>
    <paper id="398">
      <title>“Women do not have heart attacks!” Gender Biases in Automatically Generated Clinical Cases in <fixed-case>F</fixed-case>rench</title>
      <author><first>Fanny</first><last>Ducel</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Nicolas</first><last>Hiebel</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Olivier</first><last>Ferret</last><affiliation>CEA</affiliation></author>
      <author><first>Karën</first><last>Fort</last><affiliation>University of Lorraine</affiliation></author>
      <author><first>Aurélie</first><last>Névéol</last><affiliation>LISN-CNRS / Université Paris Saclay</affiliation></author>
      <pages>7145-7159</pages>
      <abstract>Healthcare professionals are increasingly including Language Models (LMs) in clinical practice. However, LMs have been shown to exhibit and amplify stereotypical biases that can cause life-threatening harm in a medical context. This study aims to evaluate gender biases in automatically generated clinical cases in French, on ten disorders. Using seven LMs fine-tuned for clinical case generation and an automatic linguistic gender detection tool, we measure the associations between disorders and gender. We unveil that LMs over-generate cases describing male patients, creating synthetic corpora that are not consistent with documented prevalence for these disorders. For instance, when prompts do not specify a gender, LMs generate eight times more clinical cases describing male (vs. female patients) for heart attack. We discuss the ideal synthetic clinical case corpus and establish that explicitly mentioning demographic information in generation instructions appears to be the fairest strategy. In conclusion, we argue that the presence of gender biases in synthetic text raises concerns about LM-induced harm, especially for women and transgender people.</abstract>
      <url hash="641d230e">2025.findings-naacl.398</url>
      <bibkey>ducel-etal-2025-women</bibkey>
    </paper>
    <paper id="399">
      <title><fixed-case>NOTA</fixed-case>: Multimodal Music Notation Understanding for Visual Large Language Model</title>
      <author><first>Mingni</first><last>Tang</last></author>
      <author><first>Jiajia</first><last>Li</last></author>
      <author><first>Lu</first><last>Yang</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last></author>
      <author><first>Jinhao</first><last>Tian</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Lefei</first><last>Zhang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <pages>7160-7173</pages>
      <abstract>Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music score notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline.</abstract>
      <url hash="afff9bc1">2025.findings-naacl.399</url>
      <bibkey>tang-etal-2025-nota</bibkey>
    </paper>
    <paper id="400">
      <title>Exploring Large Language Models for Hate Speech Detection in <fixed-case>R</fixed-case>ioplatense <fixed-case>S</fixed-case>panish</title>
      <author><first>Juan Manuel</first><last>Pérez</last><affiliation>Universidad de San Andres and Universidad de Buenos Aires</affiliation></author>
      <author><first>Paula</first><last>Miguel</last><affiliation>Universidad de Buenos Aires and Universidad de Buenos Aires</affiliation></author>
      <author><first>Viviana</first><last>Cotik</last><affiliation>Computer Science Department, University of Buenos Aires</affiliation></author>
      <pages>7174-7187</pages>
      <abstract>Hate speech detection deals with many language variants, slang, slurs, expression modalities, and cultural nuances. This outlines the importance of working with specific corpora, when addressing hate speech within the scope of Natural Language Processing, recently revolutionized by the irruption of Large Language Models. This work presents a brief analysis of the performance of large language models in the detection of Hate Speech for Rioplatense Spanish. We performed classification experiments leveraging chain-of-thought reasoning with ChatGPT 3.5, Mixtral, and Aya, comparing their results with those of a state-of-the-art BERT classifier. These experiments outline that, even if large language models show a lower precision compared to the fine-tuned BERT classifier and, in some cases, they find hard-to-get slurs or colloquialisms, they still are sensitive to highly nuanced cases (particularly, homophobic/transphobic hate speech). We make our code and models publicly available for future research.</abstract>
      <url hash="d5c52b64">2025.findings-naacl.400</url>
      <bibkey>perez-etal-2025-exploring</bibkey>
    </paper>
    <paper id="401">
      <title>An Annotated Dataset of Errors in Premodern <fixed-case>G</fixed-case>reek and Baselines for Detecting Them</title>
      <author><first>Creston</first><last>Brooks</last></author>
      <author><first>Johannes</first><last>Haubold</last><affiliation>Princeton University</affiliation></author>
      <author><first>Charlie</first><last>Cowen-Breen</last></author>
      <author><first>Jay</first><last>White</last></author>
      <author><first>Desmond</first><last>DeVaul</last><affiliation>Princeton University</affiliation></author>
      <author><first>Frederick</first><last>Riemenschneider</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Karthik R</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <author><first>Barbara</first><last>Graziosi</last><affiliation>Princeton University</affiliation></author>
      <pages>7188-7202</pages>
      <abstract>As premodern texts are passed down over centuries, errors inevitably accrue. These errors can be challenging to identify, as some have survived undetected for so long precisely because they are so elusive. While prior work has evaluated error detection methods on artificially-generated errors, we introduce the first dataset of real errors in premodern Greek, enabling the evaluation of error detection methods on errors that genuinely accumulated at some stage in the centuries-long copying process. To create this dataset, we use metrics derived from BERT conditionals to sample 1,000 words more likely to contain errors, which are then annotated and labeled by a domain expert as errors or not. We then propose and evaluate new error detection methods and find that our discriminator-based detector outperforms all other methods, improving the true positive rate for classifying real errors by 5%. We additionally observe that scribal errors are more difficult to detect than print or digitization errors. Our dataset enables the evaluation of error detection methods on real errors in premodern texts for the first time, providing a benchmark for developing more effective error detection algorithms to assist scholars in restoring premodern works.</abstract>
      <url hash="781390b1">2025.findings-naacl.401</url>
      <bibkey>brooks-etal-2025-annotated</bibkey>
    </paper>
    <paper id="402">
      <title><fixed-case>W</fixed-case>orld<fixed-case>M</fixed-case>ed<fixed-case>QA</fixed-case>-<fixed-case>V</fixed-case>: a multilingual, multimodal medical examination dataset for multimodal language models evaluation</title>
      <author><first>João</first><last>Matos</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Shan</first><last>Chen</last></author>
      <author><first>Siena Kathleen V.</first><last>Placino</last></author>
      <author><first>Yingya</first><last>Li</last><affiliation>Harvard University</affiliation></author>
      <author><first>Juan Carlos Climent</first><last>Pardo</last></author>
      <author><first>Daphna</first><last>Idan</last></author>
      <author><first>Takeshi</first><last>Tohyama</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>David</first><last>Restrepo</last><affiliation>Centrale Supélec and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Luis Filipe</first><last>Nakayama</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>José María Millet</first><last>Pascual-Leone</last></author>
      <author><first>Guergana K</first><last>Savova</last><affiliation>Harvard University</affiliation></author>
      <author><first>Hugo</first><last>Aerts</last><affiliation>Harvard University</affiliation></author>
      <author><first>Leo Anthony</first><last>Celi</last><affiliation>Massachusetts Institute of Technology and Beth Israel Deaconess Medical Center</affiliation></author>
      <author><first>An-Kwok Ian</first><last>Wong</last><affiliation>Duke University</affiliation></author>
      <author><first>Danielle</first><last>Bitterman</last><affiliation>Harvard University</affiliation></author>
      <author><first>Jack</first><last>Gallifant</last></author>
      <pages>7203-7216</pages>
      <abstract>Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.</abstract>
      <url hash="4288a55c">2025.findings-naacl.402</url>
      <bibkey>matos-etal-2025-worldmedqa</bibkey>
    </paper>
    <paper id="403">
      <title><fixed-case>B</fixed-case>an<fixed-case>TH</fixed-case>: A Multi-label Hate Speech Detection Dataset for Transliterated <fixed-case>B</fixed-case>angla</title>
      <author><first>Fabiha</first><last>Haider</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Fariha Tanjim</first><last>Shifat</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Md Farhan</first><last>Ishmam</last><affiliation>Islamic University of Technology</affiliation></author>
      <author><first>Md Sakib Ul Rahman</first><last>Sourove</last></author>
      <author><first>Deeparghya Dutta</first><last>Barua</last><affiliation>Penta Global Limited</affiliation></author>
      <author><first>Md</first><last>Fahim</last><affiliation>Independent University, Bangladesh</affiliation></author>
      <author><first>Md Farhad Alam</first><last>Bhuiyan</last></author>
      <pages>7217-7236</pages>
      <abstract>The proliferation of transliterated texts in digital spaces has emphasized the need for detecting and classifying hate speech in languages beyond English, particularly in low-resource languages. As online discourse can perpetuate discrimination based on target groups, e.g. gender, religion, and origin, multi-label classification of hateful content can help in understanding hate motivation and enhance content moderation. While previous efforts have focused on monolingual or binary hate classification tasks, no work has yet addressed the challenge of multi-label hate speech classification in transliterated Bangla. We introduce BanTH, the first multi-label transliterated Bangla hate speech dataset. The samples are sourced from YouTube comments, where each instance is labeled with one or more target groups, reflecting the regional demographic. We propose a novel translation-based LLM prompting strategy that translates or transliterates under-resourced text to higher-resourced text before classifying the hate group(s). Experiments reveal further pre-trained encoders achieving state-of-the-art performance on the BanTH dataset while translation-based prompting outperforms other strategies in the zero-shot setting. We address a critical gap in Bangla hate speech and set the stage for further exploration into code-mixed and multi-label classification in underrepresented languages.</abstract>
      <url hash="bf62f5ec">2025.findings-naacl.403</url>
      <bibkey>haider-etal-2025-banth</bibkey>
    </paper>
    <paper id="404">
      <title>Mutual Reinforcement of <fixed-case>LLM</fixed-case> Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization</title>
      <author><first>Yen-Ju</first><last>Lu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ting-Yao</first><last>Hu</last><affiliation>Apple</affiliation></author>
      <author><first>Hema Swetha</first><last>Koppula</last><affiliation>Apple</affiliation></author>
      <author><first>Hadi</first><last>Pouransari</last><affiliation>Apple</affiliation></author>
      <author><first>Jen-Hao Rick</first><last>Chang</last><affiliation>Apple</affiliation></author>
      <author><first>Yin</first><last>Xia</last><affiliation>Apple and JD.com Technology America</affiliation></author>
      <author><first>Xiang</first><last>Kong</last><affiliation>Apple</affiliation></author>
      <author><first>Qi</first><last>Zhu</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoming Simon</first><last>Wang</last><affiliation>Didi Research US</affiliation></author>
      <author><first>Oncel</first><last>Tuzel</last><affiliation>Apple</affiliation></author>
      <author><first>Raviteja</first><last>Vemulapalli</last><affiliation>Apple</affiliation></author>
      <pages>7237-7256</pages>
      <abstract>In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM’s dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.</abstract>
      <url hash="76a8dd86">2025.findings-naacl.404</url>
      <bibkey>lu-etal-2025-mutual</bibkey>
    </paper>
    <paper id="405">
      <title><fixed-case>UNLEARN</fixed-case> Efficient Removal of Knowledge in Large Language Models</title>
      <author><first>Tyler</first><last>Lizzo</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Larry</first><last>Heck</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7257-7268</pages>
      <abstract>Large Language Models (LLMs) excel in many Natural Language Processing tasks but are outperformed by specialized tools for certain tasks. This raises the question: Can we reduce redundant LLM parameters when using these tools? Given the size and high training costs of LLMs, it is essential to efficiently forget specific knowledge without retraining. This paper introduces UNLEARN, a novel method that uses subspace techniques to selectively remove knowledge without access to the original training data, without retraining, and with minimal impact to other tasks. Our results show that UNLEARN significantly outperforms previous methods for forgetting targeted (unwanted) knowledge while also preserving related (wanted) knowledge. We also propose LEARN, a complementary approach for targeted knowledge addition, which achieves fine-tuning accuracy comparable to Low-Rank Adaptation (LoRA) without degrading related task performance.</abstract>
      <url hash="050c2cac">2025.findings-naacl.405</url>
      <bibkey>lizzo-heck-2025-unlearn</bibkey>
    </paper>
    <paper id="406">
      <title>Adaptive Parameter Compression for Language Models</title>
      <author><first>Jeremias</first><last>Bohn</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Frederic</first><last>Mrozinski</last></author>
      <author><first>Georg</first><last>Groh</last><affiliation>Technical University Munich</affiliation></author>
      <pages>7269-7286</pages>
      <url hash="57cbaede">2025.findings-naacl.406</url>
      <bibkey>bohn-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="407">
      <title>Personalize Your <fixed-case>LLM</fixed-case>: Fake it then Align it</title>
      <author><first>Yijing</first><last>Zhang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dyah</first><last>Adila</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Changho</first><last>Shin</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Frederic</first><last>Sala</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <pages>7287-7301</pages>
      <abstract>Personalizing large language models (LLMs) is essential for delivering tailored interactions that improve user experience. Many existing personalization methods require fine-tuning LLMs for each user, rendering them prohibitively expensive for widespread adoption. Although retrieval-based approaches offer a more compute-efficient alternative, they still depend on large, high-quality datasets that are not consistently available for all users. To address this challenge, we propose Chameleon, a scalable and efficient personalization approach that uses (1) self-generated personal preference data and (2) representation editing to enable quick and cost-effective personalization. Our experiments on various tasks, including those from the LaMP personalization benchmark, show that Chameleon efficiently adapts models to personal preferences, improving instruction-tuned models and outperforms two personalization baselines by an average of 40% across two model architectures.</abstract>
      <url hash="d4cbd989">2025.findings-naacl.407</url>
      <bibkey>zhang-etal-2025-personalize</bibkey>
    </paper>
    <paper id="408">
      <title>A Survey to Recent Progress Towards Understanding In-Context Learning</title>
      <author><first>Haitao</first><last>Mao</last></author>
      <author><first>Guangliang</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yao</first><last>Ma</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Rongrong</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>7302-7323</pages>
      <abstract>In-Context Learning (ICL) empowers Large Language Models (LLMs) with the ability to learn from a few examples provided in the prompt, enabling downstream generalization without the requirement for gradient updates. Despite encouragingly empirical success, the underlying mechanism of ICL remains unclear. Existing research remains ambiguous with various viewpoints, utilizing intuition-driven and ad-hoc technical solutions to interpret ICL. In this paper, we leverage a data generation perspective to reinterpret recent efforts from a systematic angle, demonstrating the potential broader usage of these popular technical solutions. For a conceptual definition, we rigorously adopt the terms of skill recognition and skill learning. Skill recognition selects one learned data generation function previously seen during pre-training while skill learning can learn new data generation functions from in-context data. Furthermore, we provide insights into the strengths and weaknesses of both abilities, emphasizing their commonalities through the perspective of data generation. This analysis suggests potential directions for future research. The corresponding paper list can be found here.</abstract>
      <url hash="bb6325aa">2025.findings-naacl.408</url>
      <bibkey>mao-etal-2025-survey</bibkey>
    </paper>
    <paper id="409">
      <title>Inference Scaling for Bridging Retrieval and Augmented Generation</title>
      <author><first>Youngwon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Daniel F</first><last>Campos</last><affiliation>Snowflake</affiliation></author>
      <author><first>Filip</first><last>Graliński</last><affiliation>Snowflake and Adam Mickiewicz University</affiliation></author>
      <author><first>Zhewei</first><last>Yao</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yuxiong</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>7324-7339</pages>
      <abstract>Retrieval-augmented generation (RAG) has emerged as a popular approach to steering the output of a large language model (LLM) by incorporating retrieved contexts as inputs. However, existing work observed the generator bias, such that improving the retrieval results may negatively affect the outcome. In this work, we show such bias can be mitigated, from inference scaling, aggregating inference calls from the permuted order of retrieved contexts. The proposed Mixture-of-Intervention (MoI) explicitly models the debiased utility of each passage with multiple forward passes to construct a new ranking. We also show that MoI can leverage the retriever’s prior knowledge to reduce the computational cost by minimizing the number of permutations considered and lowering the cost per LLM call. We showcase the effectiveness of MoI on diverse RAG tasks, improving ROUGE-L on MS MARCO and EM on HotpotQA benchmarks by ~7 points.</abstract>
      <url hash="9b7faf63">2025.findings-naacl.409</url>
      <bibkey>lee-etal-2025-inference</bibkey>
    </paper>
    <paper id="410">
      <title><fixed-case>G</fixed-case>eo<fixed-case>C</fixed-case>oder: Solving Geometry Problems by Generating Modular Code through Vision-Language Models</title>
      <author><first>Aditya</first><last>Sharma</last><affiliation>École Polytechnique de Montréal, Université de Montréal</affiliation></author>
      <author><first>Aman</first><last>Dalmia</last></author>
      <author><first>Mehran</first><last>Kazemi</last><affiliation>Google</affiliation></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <author><first>Christopher</first><last>Pal</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>7340-7356</pages>
      <abstract>Geometry problem-solving demands advanced reasoning abilities to process multimodal inputs and employ mathematical knowledge effectively. Vision-language models (VLMs) have made significant progress in various multimodal tasks. Yet, they still struggle with geometry problems and are significantly limited by their inability to perform mathematical operations not seen during pre-training, such as calculating the cosine of an arbitrary angle, and by difficulties in correctly applying relevant geometry formulas. To overcome these challenges, we present GeoCoder, which leverages modular code-finetuning to generate and execute code using a predefined geometry function library. By executing the code, we achieve accurate and deterministic calculations, contrasting the stochastic nature of autoregressive token prediction, while the function library minimizes errors in formula usage. We also propose a multimodal retrieval-augmented variant of GeoCoder, named RAG-GeoCoder, which incorporates a non-parametric memory module for retrieving functions from the geometry library, thereby reducing reliance on parametric memory. Our modular code-finetuning approach enhances the geometric reasoning capabilities of VLMs, yielding an average improvement of over 16% across various question complexities on the GeomVerse dataset compared to other fine-tuning methods.</abstract>
      <url hash="5f2fda72">2025.findings-naacl.410</url>
      <bibkey>sharma-etal-2025-geocoder</bibkey>
    </paper>
    <paper id="411">
      <title><fixed-case>SEE</fixed-case>val: Advancing <fixed-case>LLM</fixed-case> Text Evaluation Efficiency and Accuracy through Self-Explanation Prompting</title>
      <author><first>Meng-Chen</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Md Mosharaf</first><last>Hossain</last><affiliation>Amazon</affiliation></author>
      <author><first>Tess</first><last>Wood</last><affiliation>Amazon</affiliation></author>
      <author><first>Shayan Ali</first><last>Akbar</last><affiliation>Amazon</affiliation></author>
      <author><first>Si-Chi</first><last>Chin</last><affiliation>Amazon</affiliation></author>
      <author><first>Erwin</first><last>Cornejo</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <pages>7357-7368</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success in various natural language generation (NLG) tasks, but their performance in automatic text evaluation is not yet ready as human replacements. In this paper, we propose SEEval (Self-Explanation in Evaluation), a novel prompt-based text evaluator. Inspired by educational psychology, SEEval incorporates self-explanation, a metacognitive strategy, to enhance automatic text evaluation. Our experimental results show that SEEval, without probability normalization, is able to achieve competitive and often superior performance compared to the two state-of-the-art baselines – G-Eval and Analyze-Rate – across all evaluation dimensions and is 20 times more efficient in terms of run-time. The SEEval method is also generalizable as its results are consistent across three other selected LLMs – Claude 3.5 Sonnet, Command R+, and Mistral-Large 2.</abstract>
      <url hash="8ebcc22d">2025.findings-naacl.411</url>
      <bibkey>wu-etal-2025-seeval</bibkey>
    </paper>
    <paper id="412">
      <title>When natural language is not enough: The limits of in-context learning demonstrations in multilingual reasoning</title>
      <author><first>Leonardo</first><last>Ranaldi</last></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7369-7396</pages>
      <abstract>Previous studies have demonstrated the effectiveness of reasoning methods in eliciting multi-step reasoned answers from Large Language Models (LLMs) by leveraging in-context demonstrations. These methods, exemplified by Chain-of-Thought (CoT) and Program-Aided Language Models (PAL), have been shown to perform well in monolingual contexts, primarily in English. There has, however, been limited exploration of their abilities in other languages.To gain a deeper understanding of the role of reasoning methods for in-context demonstrations, we investigate how well CoT and PAL perform across languages for arithmetic and symbolic reasoning tasks. Our findings indicate that the effectiveness of reasoning methods varies significantly across different languages and models. Specifically, CoT, which relies on natural language demonstrations, tends to be more accurate in high-resource than in low-resource languages. Conversely, the structured nature of PAL demonstrations facilitates multilingual comprehension, enabling LLMs to generate programmatic answers in both high- and low-resource languages and leading to significant performance improvements over CoT concerning the accuracy of the generated responses.</abstract>
      <url hash="6e1e5d73">2025.findings-naacl.412</url>
      <bibkey>ranaldi-etal-2025-natural</bibkey>
    </paper>
    <paper id="413">
      <title>Uncovering Latent Arguments in Social Media Messaging by Employing <fixed-case>LLM</fixed-case>s-in-the-Loop Strategy</title>
      <author><first>Tunazzina</first><last>Islam</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <pages>7397-7429</pages>
      <abstract>The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic **LLMs-in-the-Loop** strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Additionally, we design a downstream task as stance prediction by leveraging talking points in climate debates. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events.</abstract>
      <url hash="b419cd89">2025.findings-naacl.413</url>
      <bibkey>islam-goldwasser-2025-uncovering</bibkey>
    </paper>
    <paper id="414">
      <title><fixed-case>A</fixed-case>crostic<fixed-case>S</fixed-case>leuth: Probabilistic Identification and Ranking of Acrostics in Multilingual Corpora</title>
      <author><first>Aleksandr</first><last>Fedchin</last></author>
      <author><first>Isabel</first><last>Cooperman</last></author>
      <author><first>Pramit</first><last>Chaudhuri</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>7430-7437</pages>
      <abstract>For centuries, writers have hidden messages as acrostics, in which initial letters of consecutive lines or paragraphs form meaningful words or phrases. Scholars searching for acrostics manually can only focus on a few authors at a time and often favor qualitative arguments about whether a given acrostic is accidental or intentional. Here we describe AcrosticSleuth, a first-of-its-kind approach to identify acrostics automatically and rank them by the probability that the corresponding sequence of characters does not occur by chance. Since acrostics are rare, we formalize the problem as a binary classification task in the presence of extreme class imbalance. To evaluate AcrosticSleuth, we present the Acrostic Identification Dataset (AcrostID), a collection of acrostics from the WikiSource online database. Despite the class imbalance, AcrosticSleuth achieves F1 scores of 0.39, 0.59, and 0.66 on the French, English, and Russian subdomains of WikiSource, respectively. We further demonstrate that AcrosticSleuth can identify previously unknown instances of wordplay in high-profile literary contexts, including the English philosopher Thomas Hobbes’ signature in the opening paragraphs of The Elements of Law.</abstract>
      <url hash="736fbb11">2025.findings-naacl.414</url>
      <bibkey>fedchin-etal-2025-acrosticsleuth</bibkey>
    </paper>
    <paper id="415">
      <title><fixed-case>M</fixed-case>ed<fixed-case>T</fixed-case>hink: A Rationale-Guided Framework for Explaining Medical Visual Question Answering</title>
      <author><first>Xiaotang</first><last>Gai</last></author>
      <author><first>Chenyi</first><last>Zhou</last></author>
      <author><first>Jiaxiang</first><last>Liu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jian</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zuozhu</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>7438-7450</pages>
      <abstract>Medical Visual Question Answering (Med-VQA), which offers language responses to image-based medical inquiries, represents a challenging task and significant advancement in healthcare. It assists medical experts to swiftly interpret medical images, thereby enabling faster and more accurate diagnoses. However, the model interpretability and transparency of existing Med-VQA solutions are often limited, posing challenges in understanding their decision-making processes. To address this issue, we devise a semi-automated annotation process to streamline data preparation and build new benchmark Med-VQA datasets R-RAD, R-SLAKE and R-Path. These datasets provide intermediate medical decision-making rationales generated by multimodal large language models and human annotations for question-answering pairs in existing Med-VQA datasets, i.e., VQA-RAD, SLAKE and PathVQA. Moreover, we design a novel framework, MedThink, which finetunes lightweight pretrained generative models by incorporating medical decision-making rationales. MedThink includes three distinct strategies to generate decision outcomes and corresponding rationales, clearly showcasing the medical decision-making process during reasoning. Our comprehensive experiments show that our method achieves an accuracy of 83.5% on R-RAD, 86.3% on R-SLAKE and 87.2% on R-Path. These results significantly exceed those of existing state-of-the-art models with comparable parameters. Datasets and code are available at https://github.com/Tang-xiaoxiao/Medthink.</abstract>
      <url hash="6536a779">2025.findings-naacl.415</url>
      <bibkey>gai-etal-2025-medthink</bibkey>
    </paper>
    <paper id="416">
      <title>How to Learn in a Noisy World? Self-Correcting the Real-World Data Noise in Machine Translation</title>
      <author><first>Yan</first><last>Meng</last></author>
      <author><first>Di</first><last>Wu</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>7451-7467</pages>
      <abstract>The massive amounts of web-mined parallel data often contain large amounts of noise. Semantic misalignment, as the primary source of the noise, poses a challenge for training machine translation systems. In this paper, we first introduce a process for simulating misalignment controlled by semantic similarity, which closely resembles misaligned sentences in real-world web-crawled corpora. Under our simulated misalignment noise settings, we quantitatively analyze its impact on machine translation and demonstrate the limited effectiveness of widely used pre-filters for noise detection. This underscores the necessity of more fine-grained ways to handle hard-to-detect misalignment noise. By analyzing the reliability of the model’s self-knowledge for distinguishing misaligned and clean data at the token level, we propose self-correction—an approach that gradually increases trust in the model’s self-knowledge to correct the supervision signal during training. Comprehensive experiments show that our method significantly improves translation performance both in the presence of simulated misalignment noise and when applied to real-world, noisy web-mined datasets, across a range of translation tasks.</abstract>
      <url hash="d9408dab">2025.findings-naacl.416</url>
      <bibkey>meng-etal-2025-learn</bibkey>
    </paper>
    <paper id="417">
      <title>Rejected Dialects: Biases Against <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican Language in Reward Models</title>
      <author><first>Joel</first><last>Mire</last></author>
      <author><first>Zubin Trivadi</first><last>Aysola</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Chechelnitsky</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Nicholas</first><last>Deas</last><affiliation>Columbia University</affiliation></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Maarten</first><last>Sap</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>7468-7487</pages>
      <abstract>Preference alignment via reward models helps build safe, helpful, and reliable large language models (LLMs). However, subjectivity in preference judgments and the lack of representative sampling in preference data collection can introduce new biases, hindering reward models’ fairness and equity. In this work, we introduce a framework for evaluating dialect biases in reward models and conduct a case study on biases against African American Language (AAL) through several experiments comparing reward model preferences and behavior on paired White Mainstream English (WME) and both machine-translated and human-written AAL corpora. We show that reward models are less aligned with human preferences when processing AAL texts vs. WME ones (-4% accuracy on average), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and steer conversations toward WME, even when prompted with AAL texts. Our findings provide a targeted analysis of anti-AAL biases at a relatively understudied stage in LLM development, highlighting representational harms and ethical questions about the desired behavior of LLMs concerning AAL.</abstract>
      <url hash="a95cbb71">2025.findings-naacl.417</url>
      <bibkey>mire-etal-2025-rejected</bibkey>
    </paper>
    <paper id="418">
      <title>Do Large Language Models Align with Core Mental Health Counseling Competencies?</title>
      <author><first>Viet Cuong</first><last>Nguyen</last></author>
      <author><first>Mohammad</first><last>Taher</last></author>
      <author><first>Dongwan</first><last>Hong</last></author>
      <author><first>Vinicius Konkolics</first><last>Possobom</last></author>
      <author><first>Vibha Thirunellayi</first><last>Gopalakrishnan</last></author>
      <author><first>Ekta</first><last>Raj</last></author>
      <author><first>Zihang</first><last>Li</last></author>
      <author><first>Heather J.</first><last>Soled</last><affiliation>Northwell Health</affiliation></author>
      <author><first>Michael L.</first><last>Birnbaum</last></author>
      <author><first>Srijan</first><last>Kumar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Munmun</first><last>De Choudhury</last></author>
      <pages>7488-7511</pages>
      <abstract>The rapid evolution of Large Language Models (LLMs) presents a promising solution to the global shortage of mental health professionals. However, their alignment with essential counseling competencies remains underexplored. We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22 general-purpose and medical-finetuned LLMs across five key competencies. While frontier models surpass minimum aptitude thresholds, they fall short of expert-level performance, excelling in Intake, Assessment &amp; Diagnosis but struggling with Core Counseling Attributes and Professional Practice &amp; Ethics. Surprisingly, medical LLMs do not outperform generalist models in accuracy, though they provide slightly better justifications while making more context-related errors. These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning. Our results underscore the need for specialized, fine-tuned models aligned with core mental health counseling competencies and supported by human oversight before real-world deployment. Code and data associated with this manuscript can be found at: https://github.com/cuongnguyenx/CounselingBench</abstract>
      <url hash="9115f733">2025.findings-naacl.418</url>
      <bibkey>nguyen-etal-2025-large</bibkey>
    </paper>
    <paper id="419">
      <title>Uncertainty Quantification for Clinical Outcome Predictions with (Large) Language Models</title>
      <author><first>Zizhang</first><last>Chen</last></author>
      <author><first>Peizhao</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Xiaomeng</first><last>Dong</last></author>
      <author><first>Pengyu</first><last>Hong</last><affiliation>Brandeis University</affiliation></author>
      <pages>7512-7523</pages>
      <abstract>To facilitate healthcare delivery, language models (LMs) have significant potential for clinical prediction tasks using electronic health records (EHRs). However, in these high-stakes applications, unreliable decisions can result in significant costs due to compromised patient safety and ethical concerns, thus increasing the need for good uncertainty modelling of automated clinical predictions. To address this, we consider uncertainty quantification of LMs for EHR tasks in both white-box and black-box settings. We first quantify uncertainty in white-box models, where we have access to model parameters and output logits. We show that an effective reduction of model uncertainty can be achieved by using the proposed multi-tasking and ensemble methods in EHRs. Continuing with this idea, we extend our approach to black-box settings, including popular proprietary LMs such as GPT-4. We validate our framework using longitudinal clinical data from over 6,000 patients across ten clinical prediction tasks. Results show that ensembling methods and multi-task prediction prompts reduce uncertainty across different scenarios. These findings increase model transparency in white-box and black-box settings, thereby advancing reliable AI healthcare.</abstract>
      <url hash="49d648c3">2025.findings-naacl.419</url>
      <bibkey>chen-etal-2025-uncertainty</bibkey>
    </paper>
    <paper id="420">
      <title>Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Shrinidhi</first><last>Kumbhar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Venkatesh</first><last>Mishra</last></author>
      <author><first>Kevin</first><last>Coutinho</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Divij</first><last>Handa</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ashif</first><last>Iquebal</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>7524-7555</pages>
      <abstract>Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.</abstract>
      <url hash="f0795211">2025.findings-naacl.420</url>
      <bibkey>kumbhar-etal-2025-hypothesis</bibkey>
    </paper>
    <paper id="421">
      <title>Aligning to What? Limits to <fixed-case>RLHF</fixed-case> Based Alignment</title>
      <author><first>Logan</first><last>Barnhart</last></author>
      <author><first>Reza</first><last>Akbarian Bafghi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Stephen</first><last>Becker</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Maziar</first><last>Raissi</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>7556-7591</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) is increasingly used to align large language models (LLMs) with human preferences. However, the effectiveness of RLHF in addressing underlying biases remains unclear. This study investigates the relationship between RLHF and both covert and overt biases in LLMs, particularly focusing on biases against African Americans. We applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and evaluated the covert and overt biases of the resulting models using matched-guise probing and explicit bias testing. We performed additional tests with DPO on different base models and datasets; among several implications, we found that SFT before RLHF calcifies model biases. Additionally, we extend the tools for measuring biases to multi-modal models. Through our experiments we collect evidence that indicates that current alignment techniques are inadequate for nebulous tasks such as mitigating covert biases, highlighting the need for capable datasets, data curating techniques, or alignment tools.</abstract>
      <url hash="6dbe4f2b">2025.findings-naacl.421</url>
      <bibkey>barnhart-etal-2025-aligning</bibkey>
    </paper>
    <paper id="422">
      <title>Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models</title>
      <author><first>Srishti</first><last>Yadav</last></author>
      <author><first>Zhi</first><last>Zhang</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>7592-7608</pages>
      <abstract>Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.</abstract>
      <url hash="830d59bf">2025.findings-naacl.422</url>
      <bibkey>yadav-etal-2025-beyond</bibkey>
    </paper>
    <paper id="423">
      <title>Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning</title>
      <author><first>Jeffrey</first><last>Olmo</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Jared</first><last>Wilson</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Max</first><last>Forsey</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Bryce</first><last>Hepner</last></author>
      <author><first>Thomas Vincent</first><last>Howe</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>David</first><last>Wingate</last><affiliation>Brigham Young University</affiliation></author>
      <pages>7609-7619</pages>
      <abstract>Sparse Autoencoders (SAEs) are a promising approach for extracting neural network representations by learning a sparse and overcomplete decomposition of the network’s internal activations. However, SAEs are traditionally trained considering only activation values and not the effect those activations have on downstream computations. This limits the information available to learn features, and biases the autoencoder towards neglecting features which are represented with small activation values but strongly influence model outputs.To address this, we introduce Gradient SAEs (g-SAEs), which modify the <tex-math>k</tex-math>-sparse autoencoder architecture by augmenting the TopK activation function to rely on the gradients of the input activation when selecting the <tex-math>k</tex-math> elements. For a given sparsity level, g-SAEs produce reconstructions that are more faithful to original network performance when propagated through the network.Additionally, we find evidence that g-SAEs learn latents that are on average more effective at steering models in arbitrary contexts.By considering the downstream effects of activations, our approach leverages the dual nature of neural network features as both representations, retrospectively, and actions, prospectively. While previous methods have approached the problem of feature discovery primarily focused on the former aspect, g-SAEs represent a step towards accounting for the latter as well.</abstract>
      <url hash="371f02fc">2025.findings-naacl.423</url>
      <bibkey>olmo-etal-2025-features</bibkey>
    </paper>
    <paper id="424">
      <title>Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving</title>
      <author><first>Botao</first><last>Yu</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Frazier N.</first><last>Baker</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Ziru</first><last>Chen</last></author>
      <author><first>Garrett</first><last>Herb</last></author>
      <author><first>Boyu</first><last>Gou</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Daniel</first><last>Adu-Ampratwum</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Xia</first><last>Ning</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <pages>7620-7640</pages>
      <abstract>To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents’ ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.</abstract>
      <url hash="31fb4835">2025.findings-naacl.424</url>
      <bibkey>yu-etal-2025-tooling</bibkey>
    </paper>
    <paper id="425">
      <title><fixed-case>R</fixed-case>us<fixed-case>C</fixed-case>ode: <fixed-case>R</fixed-case>ussian Cultural Code Benchmark for Text-to-Image Generation</title>
      <author><first>Viacheslav</first><last>Vasilev</last></author>
      <author><first>Julia</first><last>Agafonova</last></author>
      <author><first>Nikolai</first><last>Gerasimenko</last></author>
      <author><first>Alexander</first><last>Kapitanov</last><affiliation>SberDevices</affiliation></author>
      <author><first>Polina</first><last>Mikhailova</last><affiliation>salute devices</affiliation></author>
      <author><first>Evelina</first><last>Mironova</last></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <pages>7641-7657</pages>
      <abstract>Text-to-image generation models have gained popularity among users around the world. However, many of these models exhibit a strong bias toward English-speaking cultures, ignoring or misrepresenting the unique characteristics of other language groups, countries, and nationalities. The lack of cultural awareness can reduce the generation quality and lead to undesirable consequences such as unintentional insult, and the spread of prejudice. In contrast to the field of natural language processing, cultural awareness in computer vision has not been explored as extensively. In this paper, we strive to reduce this gap. We propose a RusCode benchmark for evaluating the quality of text-to-image generation containing elements of the Russian cultural code. To do this, we form a list of 19 categories that best represent the features of Russian visual culture. Our final dataset consists of 1250 text prompts in Russian and their translations into English. The prompts cover a wide range of topics, including complex concepts from art, popular culture, folk traditions, famous people’s names, natural objects, scientific achievements, etc. We present the results of a human evaluation of the side-by-side comparison of Russian visual concepts representations using popular generative models.</abstract>
      <url hash="0bcea155">2025.findings-naacl.425</url>
      <bibkey>vasilev-etal-2025-ruscode</bibkey>
    </paper>
    <paper id="426">
      <title>Evaluation of <fixed-case>LLM</fixed-case>s-based Hidden States as Author Representations for Psychological Human-Centered <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>Pranav</first><last>Chitale</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Khushboo</first><last>Singh</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>7658-7667</pages>
      <abstract>Like most of NLP, models for human-centered NLP tasks—tasks attempting to assess author-level information—predominantly use rep-resentations derived from hidden states of Transformer-based LLMs. However, what component of the LM is used for the representation varies widely. Moreover, there is a need for Human Language Models (HuLMs) that implicitly model the author and provide a user-level hidden state. Here, we systematically evaluate different ways of representing documents and users using different LM and HuLM architectures to predict task outcomes as both dynamically changing states and averaged trait-like user-level attributes of valence, arousal, empathy, and distress. We find that representing documents as an average of the token hidden states performs the best generally. Further, while a user-level hidden state itself is rarely the best representation, we find its inclusion in the model strengthens token or document embeddings used to derive document- and user-level representations resulting in best performances.</abstract>
      <url hash="df014c04">2025.findings-naacl.426</url>
      <bibkey>soni-etal-2025-evaluation</bibkey>
    </paper>
    <paper id="427">
      <title>Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey</title>
      <author><first>Xiaoyu</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Paiheng</first><last>Xu</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Junda</first><last>Wu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jiaxin</first><last>Yuan</last></author>
      <author><first>Yifan</first><last>Yang</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Tianrui</first><last>Guan</last></author>
      <author><first>Haoliang</first><last>Wang</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Furong</first><last>Huang</last><affiliation>University of Maryland</affiliation></author>
      <pages>7668-7684</pages>
      <abstract>Causal inference has demonstrated significant potential to enhance Natural Language Processing (NLP) models in areas such as predictive accuracy, fairness, robustness, and explainability by capturing causal relationships among variables. The rise of generative Large Language Models (LLMs) has greatly impacted various language processing tasks. This survey focuses on research that evaluates or improves LLMs from a causal view in the following areas: reasoning capacity, fairness and safety issues, explainability, and handling multimodality. Meanwhile, LLMs can assist in causal inference tasks, such as causal relationship discovery and causal effect estimation, by leveraging their generation ability and knowledge learned during pre-training. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and robust artificial intelligence systems.</abstract>
      <url hash="75f478c3">2025.findings-naacl.427</url>
      <bibkey>liu-etal-2025-large-language</bibkey>
    </paper>
    <paper id="428">
      <title><fixed-case>T</fixed-case>hought<fixed-case>S</fixed-case>culpt: Reasoning with Intermediate Revision and Search</title>
      <author><first>Yizhou</first><last>Chi</last></author>
      <author><first>Kevin</first><last>Yang</last><affiliation>Scaled Cognition</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>7685-7711</pages>
      <abstract>We present THOUGHTSCULPT, a general reasoning and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an LLM evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art reasoning methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).</abstract>
      <url hash="845bed86">2025.findings-naacl.428</url>
      <bibkey>chi-etal-2025-thoughtsculpt</bibkey>
    </paper>
    <paper id="429">
      <title>Optimizing Hidden <fixed-case>M</fixed-case>arkov Language Models: An Empirical Study of Reparameterization and Initialization Techniques</title>
      <author><first>Ivan</first><last>Lee</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>7712-7723</pages>
      <abstract>Hidden Markov models (HMMs) are valuable for their ability to provide exact and tractable inference. However, learning an HMM in an unsupervised manner involves a non-convex optimization problem that is plagued by poor local optima. Recent work on scaling-up HMMs to perform competitively as language models has indicated that this challenge only increases with larger hidden state sizes. Several techniques to address this problem have been proposed, but have not be evaluated comprehensively. This study provides a comprehensive empirical analysis of two recent strategies that use neural networks to enhance HMM optimization: neural reparameterization and neural initialization. We find that (1) these techniques work effectively for scaled HMM language modeling, (2) linear reparameterizations can be as effective as non-linear ones, and (3) the strategies are complementary.</abstract>
      <url hash="585c06fe">2025.findings-naacl.429</url>
      <bibkey>lee-berg-kirkpatrick-2025-optimizing</bibkey>
    </paper>
    <paper id="430">
      <title>Using Linguistic Entrainment to Evaluate Large Language Models for Use in Cognitive Behavioral Therapy</title>
      <author><first>Mina</first><last>Kian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Kaleen</first><last>Shrestha</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Katrin</first><last>Fischer</last></author>
      <author><first>Xiaoyuan</first><last>Zhu</last></author>
      <author><first>Jonathan</first><last>Ong</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Aryan</first><last>Trehan</last></author>
      <author><first>Jessica</first><last>Wang</last></author>
      <author><first>Gloria</first><last>Chang</last></author>
      <author><first>Séb</first><last>Arnold</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Maja</first><last>Mataric</last><affiliation>University of Southern California</affiliation></author>
      <pages>7724-7743</pages>
      <abstract>Entrainment, the responsive communication between interacting individuals, is a crucial process in building a strong relationship between a mental health therapist and their client, leading to positive therapeutic outcomes. However, so far entrainment has not been investigated as a measure of efficacy of large language models (LLMs) delivering mental health therapy. In this work, we evaluate the linguistic entrainment of an LLM (ChatGPT 3.5-turbo) in a mental health dialog setting. We first validate computational measures of linguistic entrainment with two measures of the quality of client self-disclosures: intimacy and engagement (<tex-math>p &lt; 0.05</tex-math>). We then compare the linguistic entrainment of the LLM to trained therapists and non-expert online peer supporters in a cognitive behavioral therapy (CBT) setting. We show that the LLM is outperformed by humans with respect to linguistic entrainment (<tex-math>p &lt; 0.001</tex-math>). These results support the need to be cautious in using LLMs out-of-the-box for mental health applications.</abstract>
      <url hash="324a0614">2025.findings-naacl.430</url>
      <bibkey>kian-etal-2025-using</bibkey>
    </paper>
    <paper id="431">
      <title>Analysis of <fixed-case>LLM</fixed-case> as a grammatical feature tagger for <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican <fixed-case>E</fixed-case>nglish</title>
      <author><first>Rahul</first><last>Porwal</last></author>
      <author><first>Alice</first><last>Rozet</last><affiliation>University of Florida</affiliation></author>
      <author><first>Jotsna</first><last>Gowda</last></author>
      <author><first>Pryce</first><last>Houck</last></author>
      <author><first>Kevin</first><last>Tang</last><affiliation>Heinrich Heine University Düsseldorf and University of Florida</affiliation></author>
      <author><first>Sarah</first><last>Moeller</last><affiliation>University of Florida</affiliation></author>
      <pages>7744-7756</pages>
      <abstract>African American English (AAE) presents unique challenges in natural language processing (NLP) This research systematically compares the performance of available NLP models—rule-based, transformer-based, and large language models (LLMs)—capable of identifying key grammatical features of AAE, namely Habitual Be and Multiple Negation. These features were selected for their distinct grammatical complexity and frequency of occurrence. The evaluation involved sentence-level binary classification tasks, using both zero-shot and few-shot strategies. The analysis reveals that while LLMs show promise compared to the baseline, they are influenced by biases such as recency and unrelated features in the text such as formality. This study highlights the necessity for improved model training and architectural adjustments to better accommodate AAE’s unique linguistic characteristics. Data and code are available.</abstract>
      <url hash="32ec130e">2025.findings-naacl.431</url>
      <bibkey>porwal-etal-2025-analysis</bibkey>
    </paper>
    <paper id="432">
      <title><fixed-case>LLM</fixed-case>-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers</title>
      <author><first>Anton</first><last>Razzhigaev</last></author>
      <author><first>Matvey</first><last>Mikhalchuk</last><affiliation>Artificial Intelligence Research Institute (AIRI)</affiliation></author>
      <author><first>Temurbek</first><last>Rahmatullaev</last></author>
      <author><first>Elizaveta</first><last>Goncharova</last><affiliation>Artificial Intelligence Research Institure and Higher School of Economics</affiliation></author>
      <author><first>Polina</first><last>Druzhinina</last><affiliation>Artificial Intelligence Research Institute</affiliation></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology and Institute of Numerical Mathematics</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>AIRI, Sber and Samara National Research University</affiliation></author>
      <pages>7757-7764</pages>
      <abstract>We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens — especially stopwords, articles, and commas — consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer’s embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of “filler” tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.</abstract>
      <url hash="7532f42c">2025.findings-naacl.432</url>
      <bibkey>razzhigaev-etal-2025-llm</bibkey>
    </paper>
    <paper id="433">
      <title>On A Scale From 1 to 5: Quantifying Hallucination in Faithfulness Evaluation</title>
      <author><first>Xiaonan</first><last>Jing</last><affiliation>Expedia Group</affiliation></author>
      <author><first>Srinivas</first><last>Billa</last></author>
      <author><first>Danny</first><last>Godbout</last></author>
      <pages>7765-7780</pages>
      <abstract>Hallucination has been a popular topic in natural language generation (NLG). In real-world applications, unfaithful content can result in poor data quality or loss of trust from end users. Thus, it is crucial to fact-check before adopting NLG for production usage, which can be expensive if done manually. In this paper, we investigate automated faithfulness evaluation in guided NLG. We developed a rubric template and used large language models (LLMs) to score the generation on quantifiable scales. We compared popular LLMs as well as widely adopted natural language inference (NLI) models in scoring quality and sensitivity. In addition, we developed methods for the generation of synthetic unfaithful data, as well as heuristics to quantify the percentage of hallucination. Our results on 4 travel-domain industry dataset show that GPT-4 can provide accurate judgement and explanation of whether a source and a generation are factually consistent. Furthermore, we found that tuning NLI models on synthetic data can improve performance. Lastly, we present insights on the latency and cost of deploying such a system.</abstract>
      <url hash="6d8f01c0">2025.findings-naacl.433</url>
      <bibkey>jing-etal-2025-scale</bibkey>
    </paper>
    <paper id="434">
      <title><fixed-case>LITERA</fixed-case>: An <fixed-case>LLM</fixed-case> Based Approach to <fixed-case>L</fixed-case>atin-to-<fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Paul</first><last>Rosu</last></author>
      <pages>7781-7794</pages>
      <abstract>This paper introduces an LLM-based Latin-to-English translation platform designed to address the challenges of translating Latin texts. We named the model LITERA, which stands for Latin Interpretation and Translations into English for Research Assistance. Through a multi-layered translation process utilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an unprecedented level of accuracy, showcased by greatly improved BLEU scores, particularly in classical Latin, along with improved BLEURT scores. The development of LITERA involved close collaboration with Duke University’s Classical Studies Department, which was instrumental in creating a small, high-quality parallel Latin-English dataset. This paper details the architecture, fine-tuning methodology, and prompting strategies used in LITERA, emphasizing its ability to produce literal translations.</abstract>
      <url hash="359942f8">2025.findings-naacl.434</url>
      <bibkey>rosu-2025-litera</bibkey>
    </paper>
    <paper id="435">
      <title>Investigating the Shortcomings of <fixed-case>LLM</fixed-case>s in Step-by-Step Legal Reasoning</title>
      <author><first>Venkatesh</first><last>Mishra</last></author>
      <author><first>Bimsara</first><last>Pathiraja</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Sat</first><last>Chidananda</last></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author><first>Gaowen</first><last>Liu</last></author>
      <author><first>Ali</first><last>Payani</last><affiliation>Cisco</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>7795-7826</pages>
      <abstract>Reasoning abilities of LLMs have been a key focus in recent years. One challenging reasoning domain with interesting nuances is legal reasoning, which requires careful application of rules, and precedents while balancing deductive and analogical reasoning, and conflicts between rules. Although there have been a few works on using LLMs for legal reasoning, their focus has been on overall accuracy. In this paper, we dig deeper to do a step-by-step analysis and figure out where they commit errors. We use the college-level Multiple Choice Question-Answering (MCQA) task from the <i>Civil Procedure</i> dataset and propose a new error taxonomy derived from initial manual analysis of reasoning chains with respect to several LLMs, including two objective measures: soundness and correctness scores. We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs. The computation of soundness and correctness on the dataset using the auto-evaluator framework reveals several interesting insights. Furthermore, we show that incorporating the error taxonomy as feedback in popular prompting techniques marginally increases LLM performance. Our work will also serve as an evaluation framework that can be used in detailed error analysis of reasoning chains for logic-intensive complex tasks.</abstract>
      <url hash="566b91c8">2025.findings-naacl.435</url>
      <bibkey>mishra-etal-2025-investigating</bibkey>
    </paper>
    <paper id="436">
      <title>Towards Long Context Hallucination Detection</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Kishaloy</first><last>Halder</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Qi</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Phu Mon</first><last>Htut</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Neha</first><last>Anna John</last></author>
      <author><first>Yassine</first><last>Benajiba</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>7827-7835</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference. We publicly release our dataset and code to promote research along the same line.</abstract>
      <url hash="8bcf1d68">2025.findings-naacl.436</url>
      <bibkey>liu-etal-2025-towards</bibkey>
    </paper>
    <paper id="437">
      <title>How to Talk to Language Models: Serialization Strategies for Structured Entity Matching</title>
      <author><first>Haoteng</first><last>Yin</last></author>
      <author><first>Jinha</first><last>Kim</last><affiliation>Amazon</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>Amazon</affiliation></author>
      <author><first>Krishanu</first><last>Sarker</last><affiliation>Amazon</affiliation></author>
      <author><first>Vidit</first><last>Bansal</last><affiliation>Amazon</affiliation></author>
      <pages>7836-7850</pages>
      <abstract>Entity matching (EM), which identifies whether two data records refer to the same real-world entity, is crucial for knowledge base construction and enhancing data-driven AI systems. Recent advances in language models (LMs) have shown great potential in resolving entities with rich textual attributes. However, their performance heavily depends on how structured entities are “talked” through serialized text. The impact of this serialization process remains underexplored, particularly for entities with complex relations in knowledge graphs (KGs). In this work, we systematically study entity serialization by benchmarking the effect of common schemes with LMs of different sizes on diverse tabular matching datasets. We apply our findings to propose a novel serialization scheme for KG entities based on random walks and utilize LLMs to encode sampled semantic walks for matching. Using this lightweight approach with open-source LLMs, we achieve a leading performance on EM in canonical and highly heterogeneous KGs, demonstrating significant throughput increases and superior robustness compared to GPT-4-based methods. Our study on serialization provides valuable insights for the deployment of LMs in real-world EM tasks.</abstract>
      <url hash="cb879736">2025.findings-naacl.437</url>
      <bibkey>yin-etal-2025-talk</bibkey>
    </paper>
    <paper id="438">
      <title>Accounting for Sycophancy in Language Model Uncertainty Estimation</title>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Mert</first><last>Inan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <pages>7851-7866</pages>
      <abstract>Effective human-machine collaboration requires machine learning models to externalize uncertainty, so users can reflect and intervene when necessary. For language models, these representations of uncertainty may be impacted by sycophancy bias: proclivity to agree with users, even if they are wrong. For instance, models may be over-confident in (incorrect) problem solutions suggested by a user. We study the relationship between sycophancy and uncertainty estimation for the first time. We propose a generalization of the definition of sycophancy bias to measure downstream impacts on uncertainty estimation, and also propose a new algorithm (SyRoUP) to account for sycophancy in the uncertainty estimation process. Unlike previous works, we study a broad array of user behaviors, varying both correctness and confidence of user suggestions to see how model answers (and their certainty) change. Our experiments across conversation forecasting and question-answering tasks show that user confidence plays a critical role in modulating the effects of sycophancy, and that SyRoUP can better predict these effects. From these results, we argue that externalizing both model <i>and</i> user uncertainty can help to mitigate the impacts of sycophancy bias.</abstract>
      <url hash="d9ee80c9">2025.findings-naacl.438</url>
      <bibkey>sicilia-etal-2025-accounting</bibkey>
    </paper>
    <paper id="439">
      <title>Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-sample Aggregation on Large Language Models</title>
      <author><first>Jishnu</first><last>Ray Chowdhury</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Jayanth</first><last>Mohan</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Tomas</first><last>Malik</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>7867-7884</pages>
      <abstract>Keyphrases are the essential topical phrases that summarize a document. Keyphrase generation is a long-standing NLP task for automatically generating keyphrases for a given document. While the task has been comprehensively explored in the past via various models, only a few works perform some preliminary analysis of Large Language Models (LLMs) for the task. Given the impact of LLMs in the field of NLP, it is important to conduct a more thorough examination of their potential for keyphrase generation. In this paper, we attempt to meet this demand with our research agenda. Specifically, we focus on the zero-shot capabilities of open-source instruction-tuned LLMs (Phi-3, Llama-3) and the closed-source GPT-4o for this task. We systematically investigate the effect of providing task-relevant specialized instructions in the prompt. Moreover, we design task-specific counterparts to self-consistency-style strategies for LLMs and show significant benefits from our proposals over the baselines.</abstract>
      <url hash="29cc32ef">2025.findings-naacl.439</url>
      <bibkey>ray-chowdhury-etal-2025-zero</bibkey>
    </paper>
    <paper id="440">
      <title>Meta-Reasoning Improves Tool Use in Large Language Models</title>
      <author><first>Lisa</first><last>Alazraki</last></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <pages>7885-7897</pages>
      <abstract>External tools help large language models succeed at tasks where they would otherwise typically fail. In existing frameworks, choosing tools at test time relies on naive greedy decoding, regardless of whether the model has been fine-tuned on tool-annotated data or prompted with in-context examples. In contrast, we find that gathering and choosing among a suitable set of candidate tools has greater potential to lead to an optimal selection. We present Tool selECTion via meta-reasONing (TECTON), a two-phase system that first *reasons* over a task and outputs candidate tools using a custom fine-tuned language modelling head. Then, with the custom head disabled, it *meta-reasons* (i.e., it reasons over the previous reasoning process) to make a final choice. We show that TECTON results in substantial gains—both in-distribution and out-of-distribution—on a range of math reasoning datasets.</abstract>
      <url hash="dd6a649a">2025.findings-naacl.440</url>
      <bibkey>alazraki-rei-2025-meta</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>CLERC</fixed-case>: A Dataset for <fixed-case>U</fixed-case>. <fixed-case>S</fixed-case>. Legal Case Retrieval and Retrieval-Augmented Analysis Generation</title>
      <author><first>Abe Bohan</first><last>Hou</last></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Eugene</first><last>Yang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Nils</first><last>Holzenberger</last><affiliation>Télécom ParisTech</affiliation></author>
      <author><first>Andrew</first><last>Blair-Stanek</last><affiliation>Johns Hopkins University and University of Maryland School of Law</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <pages>7898-7913</pages>
      <abstract>Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligence systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to create a colossal dataset. supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset **CLERC** (Case Law Evaluation and Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3% recall@1000.</abstract>
      <url hash="b37f64c7">2025.findings-naacl.441</url>
      <bibkey>hou-etal-2025-clerc</bibkey>
    </paper>
    <paper id="442">
      <title><fixed-case>GAI</fixed-case>f<fixed-case>E</fixed-case>: Using <fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case> to Improve Literacy in Low-resourced Settings</title>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Nouhoum</first><last>Coulibaly</last></author>
      <author><first>Seydou</first><last>Diallo</last></author>
      <author><first>Sebastien</first><last>Diarra</last></author>
      <author><first>Christopher M</first><last>Homan</last></author>
      <author><first>Mamadou K.</first><last>Keita</last></author>
      <author><first>Michael</first><last>Leventhal</last><affiliation>RobotsMali</affiliation></author>
      <pages>7914-7929</pages>
      <abstract>Illiteracy is a predictor of many negative social and personal outcomes. Illiteracy rates are particularly high in countries with underresourced languages, where few books exist that are suitable for children to learn to read from. We present GAIfE (Generative AI for Education), a toolchain and workflow developed through empirical methods, that demonstrates how existing tools can be adapted to address low literacy for an underresourced language. We used GAIfE (a play on the Bambara word for “book”) to construct materials for developing children’s reading competence in Bambara, the vehicular language of Mali. Our approach to the generation and post-generation editing of content skewed by the Global-North-centric bias of available LLMs, enabled us to rapidly multiply the content in Bambara available online by 10 times while maintaining high standards of attractiveness of the material to maintain high engagement, accurate representation of the Malian culture and physical and social environment and language quality. Using our materials, pilot reading programs achieved a 67% reduction in the number of children unable to read Bambara. Our approach demonstrated the power of bias-aware application of generative AI to the problem domain as well as the potential impact the application of this technology could have on reducing illiteracy and improving learning outcomes through native language education.</abstract>
      <url hash="d711c00a">2025.findings-naacl.442</url>
      <bibkey>tapo-etal-2025-gaife</bibkey>
    </paper>
    <paper id="443">
      <title>Hard Emotion Test Evaluation Sets for Language Models</title>
      <author><first>Tiberiu</first><last>Sosea</last><affiliation>Google</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>7930-7944</pages>
      <abstract>Language models perform well on emotion datasets but it remains unclear whether these models indeed understand emotions expressed in text or simply exploit supperficial lexical cues (e.g., emotion words). In this paper, we present two novel test evaluation sets sourced from two existing datasets that allow us to evaluate whether language models make real inferential decisions for emotion detection or not. Our human-annotated test sets are created by iteratively rephrasing input texts to gradually remove explicit emotion cues (while preserving the semantic similarity and the emotions) until a strong baseline BERT model yields incorrect predictions. Using our new test sets, we carry out a comprehensive analysis into the capabilities of small and large language models to predict emotions. Our analysis reveals that all models struggle to correctly predict emotions when emotion lexical cues become scarcer and scarcer, but large language models perform better than small pre-trained language models and push the performance by 14% over the 5% BERT baseline. We make our evaluation test sets and code publicly available.</abstract>
      <url hash="1cfbf7bb">2025.findings-naacl.443</url>
      <bibkey>sosea-caragea-2025-hard</bibkey>
    </paper>
    <paper id="444">
      <title><fixed-case>UCL</fixed-case>-Bench: A <fixed-case>C</fixed-case>hinese User-Centric Legal Benchmark for Large Language Models</title>
      <author><first>Ruoli</first><last>Gan</last></author>
      <author><first>Duanyu</first><last>Feng</last></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhihang</first><last>Lin</last><affiliation>Westlake Scietrain</affiliation></author>
      <author><first>Haochen</first><last>Jia</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Zhenyang</first><last>Cai</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Qianqian</first><last>Xie</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jimin</first><last>Huang</last><affiliation>The Fin AI</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>7945-7988</pages>
      <abstract>Existing legal benchmarks focusing on knowledge and logic effectively evaluate LLMs on various tasks in legal domain. However, few have explored the practical application of LLMs by actual users. To further assess whether LLMs meet the specific needs of legal practitioners in real-world scenarios, we introduce UCL-Bench, a Chinese User-Centric Legal Benchmark, comprising 22 tasks across 5 distinct legal scenarios.To build the UCL-Bench, we conduct a user survey targeting legal professionals to understand their needs and challenges. Based on the survey results, we craft tasks, verified by legal professionals, and categorized them according to Bloom’s taxonomy. Each task in UCL-Bench mirrors real-world legal scenarios, and instead of relying on pre-defined answers, legal experts provide detailed answer guidance for each task, incorporating both “information” and “needs” elements to mimic the complexities of legal practice. With the guidance, we use GPT-4 as the user simulator and evaluator, enabling multi-turn dialogues as a answer guidance based evaluation framework. Our findings reveal that many recent open-source general models achieve the highest performance, suggesting that they are well-suited to address the needs of legal practitioners. However, these legal LLMs do not outperform ChatGPT, indicating a need for training strategies aligned with users’ needs. Furthermore, we find that the most effective models are able to address legal issues within fewer dialogue turns, highlighting the importance of concise and accurate responses in achieving high performance. The code and dataset are available at https://github.com/wittenberg11/UCL-bench.</abstract>
      <url hash="d871dfd5">2025.findings-naacl.444</url>
      <bibkey>gan-etal-2025-ucl</bibkey>
    </paper>
    <paper id="445">
      <title><fixed-case>MIDAS</fixed-case>: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn <fixed-case>NLU</fixed-case></title>
      <author><first>Yan</first><last>Li</last></author>
      <author><first>So-Eon</first><last>Kim</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Seong-Bae</first><last>Park</last></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <pages>7989-8012</pages>
      <abstract>Although Large Language Models (LLMs) can generate coherent text, they often struggle to recognise user intent behind queries. In contrast, Natural Language Understanding (NLU) models interpret the purpose and key information of user input for responsive interactions. Existing NLU models typically map utterances to a dual-level semantic frame, involving sentence-level intent (SI) and word-level slot (WS) labels. However, real-life conversations primarily consist of multi-turn dialogues, requiring the interpretation of complex and extended exchanges. Researchers encounter challenges in addressing all facets of multi-turn dialogue using a unified NLU model. This paper introduces MIDAS, a novel approach leveraging multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. We construct distinct teachers for SI detection, WS filling, and conversation-level domain (CD) classification, each fine-tuned for specific knowledge. A multi-teacher loss is proposed to facilitate the integration of these teachers, guiding a student model in multi-turn dialogue tasks. Results demonstrate the efficacy of our model in improving multi-turn conversation understanding, showcasing the potential for advancements in NLU through multi-level dialogue knowledge distillation. Our implementation is open-sourced on GitHub (https://github.com/adlnlp/Midas).</abstract>
      <url hash="6451c451">2025.findings-naacl.445</url>
      <bibkey>li-etal-2025-midas</bibkey>
    </paper>
    <paper id="446">
      <title>A Practical Analysis of Human Alignment with *<fixed-case>PO</fixed-case></title>
      <author><first>Kian</first><last>Ahrabian</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xihui</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alon</first><last>Benhaim</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xia</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <pages>8013-8021</pages>
      <abstract>At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.</abstract>
      <url hash="f7173f65">2025.findings-naacl.446</url>
      <bibkey>ahrabian-etal-2025-practical</bibkey>
    </paper>
    <paper id="447">
      <title>Understanding Reference Policies in Direct Preference Optimization</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>8022-8037</pages>
      <abstract>Direct Preference Optimization (DPO) has become a widely used training method for the instruction fine-tuning of large language models (LLMs). In this work, we explore an under-investigated aspect of DPO – its dependency on the reference model or policy. Such reference policies, typically instantiated as the model to be further fine-tuned, are important since they can impose an upper limit on DPO’s effectiveness. Therefore, we address three related research questions in this work. First, we explore the optimal strength of the KL divergence constraint in DPO, which penalizes deviations from the reference policy, and find that DPO is sensitive to this strength. Next, we examine the necessity of the KL-constraint from the reference policies in DPO by providing both theoretical and empirical comparisons between DPO and related learning objectives, demonstrating DPO’s superiority in this controlled setting. Additionally, we investigate whether DPO benefits from stronger reference policies, finding that a stronger reference policy can lead to improved performance, but only when it is similar to the model being fine-tuned. Our findings highlight the confounding role of reference policies in DPO and offer insights for best practices, while also identifying open research questions for future studies.</abstract>
      <url hash="be7b5487">2025.findings-naacl.447</url>
      <bibkey>liu-etal-2025-understanding</bibkey>
    </paper>
    <paper id="448">
      <title><fixed-case>LLM</fixed-case>-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</title>
      <author><first>Saaket</first><last>Agashe</last></author>
      <author><first>Yue</first><last>Fan</last></author>
      <author><first>Anthony</first><last>Reyna</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Xin Eric</first><last>Wang</last><affiliation>Simular and University of California, Santa Cruz</affiliation></author>
      <pages>8038-8057</pages>
      <abstract>Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners’ beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs’ Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement.</abstract>
      <url hash="94ce435d">2025.findings-naacl.448</url>
      <bibkey>agashe-etal-2025-llm</bibkey>
    </paper>
    <paper id="449">
      <title><fixed-case>A</fixed-case>ssertion<fixed-case>B</fixed-case>ench: A Benchmark to Evaluate Large-Language Models for Assertion Generation</title>
      <author><first>Vaishnavi</first><last>Pulavarthi</last></author>
      <author><first>Deeksha</first><last>Nandal</last></author>
      <author><first>Soham</first><last>Dan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Debjit</first><last>Pal</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>8058-8065</pages>
      <abstract>Assertions have been the de facto collateral for hardware for over a decade. The verification quality, i.e., detection and diagnosis of corner-case design bugs, is critically dependent on the assertion quality. There has been a considerable amount of research to generate high-quality assertions from hardware design source code and design execution trace data. With recent advent of generative AI techniques such as Large-Language Models (LLMs), there has been a renewed interest in deploying LLMs for assertion generation. However, there is little effort to quantitatively establish the effectiveness and suitability of various LLMs for assertion generation. In this paper, we present AssertionBench, a novel benchmark to evaluate LLMs’ effectiveness for assertion generation quantitatively. AssertionBench contains 100 curated Verilog hardware designs from OpenCores and formally verified assertions for each design, generated from GoldMine and HARM. We use AssertionBench to compare state-of-the-art LLMs, e.g., GPT-3.5, GPT-4o, CodeLLaMa-2, and LLaMa3-70B, to assess their effectiveness in inferring functionally correct assertions for hardware designs. Our experiments comprehensively demonstrate how LLMs perform relative to each other, the benefits of using more in-context exemplars in generating a higher fraction of functionally correct assertions, and the significant room for improvement for LLM-based assertion generators.</abstract>
      <url hash="f8507bb8">2025.findings-naacl.449</url>
      <bibkey>pulavarthi-etal-2025-assertionbench</bibkey>
    </paper>
    <paper id="450">
      <title>On Reference (In-)Determinacy in Natural Language Inference</title>
      <author><first>Sihao</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chaitanya</first><last>Malaviya</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Alex</first><last>Fabrikant</last><affiliation>Google Research</affiliation></author>
      <author><first>Hagai</first><last>Taitelbaum</last><affiliation>Research, Google</affiliation></author>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google DeepMind and Google</affiliation></author>
      <author><first>Senaka</first><last>Buthpitiya</last><affiliation>Google</affiliation></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>8066-8078</pages>
      <abstract>We revisit the reference determinacy (RD) assumption in the task of natural language inference (NLI), i.e., the premise and hypothesis are assumed to refer to the same context when human raters annotate a label. While RD is a practical assumption for constructing a new NLI dataset, we observe that current NLI models—which are typically trained solely on hypothesis-premise pairs created with the RD assumption—fail in downstream applications such as fact verification, where the input premise and hypothesis may refer to different contexts. To highlight the impact of this phenomenon in real-world use cases, we introduce RefNLI, a diagnostic benchmark for identifying reference ambiguity in NLI examples. In RefNLI, the premise is retrieved from a knowledge source (i.e. Wikipedia) and does not necessarily refer to the same context as the hypothesis. With RefNLI, we demonstrate that finetuned NLI models and few-shot prompted LLMs both fail to recognize context mismatch, leading to &gt;80% false contradiction and &gt;50% entailment predictions. We discover that the existence of reference ambiguity in NLI examples can in part explain the inherent human disagreements in NLI, and provide insight into how the RD assumption impacts NLI dataset creation process.</abstract>
      <url hash="919fe26b">2025.findings-naacl.450</url>
      <bibkey>chen-etal-2025-reference</bibkey>
    </paper>
    <paper id="451">
      <title><fixed-case>DHP</fixed-case> Benchmark: Are <fixed-case>LLM</fixed-case>s Good <fixed-case>NLG</fixed-case> Evaluators?</title>
      <author><first>Yicheng</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiayi</first><last>Yuan</last></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Zhuoer</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yingchi</first><last>Liu</last><affiliation>Axon</affiliation></author>
      <author><first>Mark</first><last>Cusick</last><affiliation>Axon Enterprise, Inc.</affiliation></author>
      <author><first>Param</first><last>Kulkarni</last><affiliation>Axon</affiliation></author>
      <author><first>Zhengping</first><last>Ji</last><affiliation>Axon</affiliation></author>
      <author><first>Yasser</first><last>Ibrahim</last><affiliation>Axon</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>8079-8094</pages>
      <abstract>Large Language Models (LLMs) are increasingly serving as evaluators in Natural Language Generation (NLG) tasks; this is often referred to as “LLM-as-a-judge” paradigm. However, the capabilities of LLMs in evaluating NLG quality remain underexplored. Current studies depend on human assessments and simple metrics that fail to capture the discernment of LLMs across diverse NLG tasks. To address this gap, we propose the Discernment of Hierarchical Perturbation (DHP) benchmarking framework, which provides quantitative discernment scores for LLMs. This framework leverages hierarchically perturbed text data and statistical tests to systematically measure the NLG evaluation capabilities of LLMs. We re-established six evaluation datasets for this benchmark, covering four NLG tasks: Summarization, Story Completion, Question Answering, and Translation. Our comprehensive benchmarking of five major LLM families provides critical insight into their strengths and limitations as NLG evaluators. Our dataset is available at https://huggingface.co/datasets/YCWANGVINCE/DHP_Benchmark.</abstract>
      <url hash="afa4546a">2025.findings-naacl.451</url>
      <bibkey>wang-etal-2025-dhp</bibkey>
    </paper>
    <paper id="452">
      <title><fixed-case>G</fixed-case>raph<fixed-case>E</fixed-case>val36<fixed-case>K</fixed-case>: Benchmarking Coding and Reasoning Capabilities of Large Language Models on Graph Datasets</title>
      <author><first>Qiming</first><last>Wu</last></author>
      <author><first>Zichen</first><last>Chen</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Will</first><last>Corcoran</last></author>
      <author><first>Misha</first><last>Sra</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Ambuj</first><last>Singh</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>8095-8117</pages>
      <abstract>Large language models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating significant capabilities in processing and understanding text data. However, recent studies have identified limitations in LLMs’ ability to manipulate, program, and reason about structured data, especially graphs. We introduce GraphEval36K, the first comprehensive graph dataset, comprising 40 graph coding problems and 36,900 test cases to evaluate the ability of LLMs on graph problem-solving. Our dataset is categorized into eight primary and four sub-categories to ensure a thorough evaluation across different types of graphs. We benchmark eight LLMs, finding that private models outperform open-source ones, though the gap is narrowing. We also analyze the performance of LLMs across directed vs undirected graphs, different kinds of graph concepts, and network models. Furthermore, to improve the usability of our evaluation framework, we propose Structured Symbolic Decomposition (SSD), an instruction-based method designed to enhance LLM performance on complex graph tasks. Results show that SSD improves the average passing rate of GPT-4, GPT-4o, Gemini-Pro and Claude-3-Sonnet by 8.38%, 6.78%, 29.28% and 25.28%, respectively.</abstract>
      <url hash="39a0cc74">2025.findings-naacl.452</url>
      <bibkey>wu-etal-2025-grapheval36k</bibkey>
    </paper>
    <paper id="453">
      <title><fixed-case>S</fixed-case>imul<fixed-case>B</fixed-case>ench: Evaluating Language Models with Creative Simulation Tasks</title>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tuney</first><last>Zheng</last></author>
      <author><first>Jie</first><last>Huang</last><affiliation>xAI</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last><affiliation>xAI and University of Washington</affiliation></author>
      <pages>8118-8131</pages>
      <abstract>We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation tasks, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM’s general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on SimulBench, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these creative simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55% more cases.</abstract>
      <url hash="474724f7">2025.findings-naacl.453</url>
      <bibkey>jia-etal-2025-simulbench</bibkey>
    </paper>
    <paper id="454">
      <title><fixed-case>R</fixed-case>easoning<fixed-case>R</fixed-case>ec: Bridging Personalized Recommendations and Human-Interpretable Explanations through <fixed-case>LLM</fixed-case> Reasoning</title>
      <author><first>Millennium</first><last>Bismay</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiangjue</first><last>Dong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>James</first><last>Caverlee</last><affiliation>Google and Texas A&amp;M University - College Station</affiliation></author>
      <pages>8132-8148</pages>
      <abstract>This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user preferences, subsequently used to fine-tune a smaller LLM for enhanced recommendation accuracy and human-interpretable explanation. Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM’s capacity to generate plausible explanations. Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art methods by up to 12.5% in recommendation prediction while concurrently providing human-intelligible explanations.</abstract>
      <url hash="34ac9a98">2025.findings-naacl.454</url>
      <bibkey>bismay-etal-2025-reasoningrec</bibkey>
    </paper>
    <paper id="455">
      <title>2<fixed-case>D</fixed-case>-<fixed-case>DPO</fixed-case>: Scaling Direct Preference Optimization with 2-Dimensional Supervision</title>
      <author><first>Shilong</first><last>Li</last></author>
      <author><first>Yancheng</first><last>He</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hangyu</first><last>Guo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weixun</first><last>Wang</last></author>
      <author><first>Jihao</first><last>Gu</last></author>
      <author><first>Wenbo</first><last>Su</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>8149-8173</pages>
      <abstract>Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.</abstract>
      <url hash="5d5dce95">2025.findings-naacl.455</url>
      <bibkey>li-etal-2025-2d</bibkey>
    </paper>
    <paper id="456">
      <title>Demystifying the Power of Large Language Models in Graph Generation</title>
      <author><first>Yu</first><last>Wang</last><affiliation>University of Oregon and Vanderbilt University</affiliation></author>
      <author><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Namyong</first><last>Park</last><affiliation>Meta AI</affiliation></author>
      <author><first>Nesreen K.</first><last>Ahmed</last><affiliation>Intel AI Research</affiliation></author>
      <author><first>Danai</first><last>Koutra</last><affiliation>Amazon and University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tyler</first><last>Derr</last><affiliation>Vanderbilt University</affiliation></author>
      <pages>8174-8189</pages>
      <abstract>Despite the unprecedented success of applying Large Language Models (LLMs) to graph discriminative tasks such as node classification and link prediction, its potential for graph structure generation remains largely unexplored. To fill this crucial gap, this paper presents a systematic investigation into the capability of LLMs for graph structure generation. Specifically, we design prompts triggering LLMs to generate codes that optimize network properties by injecting domain expertise from network science. Since graphs in different domains exhibit unique structural properties captured by various metrics (e.g., clustering coefficient capturing triangles in social networks while squares reflecting road segments in transportation networks), we first evaluate the capability of LLMs to generate graphs satisfying each structural property in different domains. After that, we select the optimal property configurations and benchmark the graph structure generation performance of LLMs against established graph generative models across multiple domains. Our findings shed light on generating graph structures from an LLM perspective. Our code is publically available https://github.com/yuwvandy/LLM-GraphGen.</abstract>
      <url hash="b71f2349">2025.findings-naacl.456</url>
      <bibkey>wang-etal-2025-demystifying</bibkey>
    </paper>
    <paper id="457">
      <title><fixed-case>COIG</fixed-case>-<fixed-case>CQIA</fixed-case>: Quality is All You Need for <fixed-case>C</fixed-case>hinese Instruction Fine-tuning</title>
      <author><first>Yuelin</first><last>Bai</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xeron</first><last>Du</last></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Leo</first><last>Jin</last></author>
      <author><first>Junting</first><last>Zhou</last></author>
      <author><first>Ziqiang</first><last>Liu</last></author>
      <author><first>Feiteng</first><last>Fang</last></author>
      <author><first>Mingshan</first><last>Chang</last></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Xincheng</first><last>Zhang</last></author>
      <author><first>Nuo</first><last>Ma</last><affiliation>01.ai</affiliation></author>
      <author><first>Zekun Moore</first><last>Wang</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Haihong</first><last>Wu</last></author>
      <author><first>Hongquan</first><last>Lin</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ge</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <pages>8190-8205</pages>
      <abstract>Remarkable progress on large language models (LLMs), particularly in English, has facilitated impressive capabilities in following human instructions. However, there remains a noticeable gap in instruction fine-tuning for Chinese, where the complex linguistic features pose significant challenges. Existing datasets, generally distilled from English-centric LLMs, are not well-aligned with Chinese users’ interaction patterns. To bridge this gap, we introduce COIG-CQIA, a new Chinese instruction tuning dataset derived from various real-world data resources and undergoing comprehensive human verification. We conduct extensive experiments on COIG-CQIA, and compare them with strong baseline models and datasets. The experimental results show that models trained on COIG-CQIA achieve highly competitive performance in diverse benchmarks. Additionally, our findings offer several insights for designing effective Chinese instruction-tuning datasets and data mixing strategies. Our dataset are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA.</abstract>
      <url hash="324a0bef">2025.findings-naacl.457</url>
      <bibkey>bai-etal-2025-coig</bibkey>
    </paper>
    <paper id="458">
      <title>Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation</title>
      <author><first>Yu</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>Intuit AI Research</affiliation></author>
      <author><first>Xiang</first><last>Gao</last><affiliation>Intuit</affiliation></author>
      <author><first>Wendi</first><last>Cui</last><affiliation>Intuit</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Kamalika</first><last>Das</last><affiliation>Intuit</affiliation></author>
      <pages>8206-8217</pages>
      <abstract>In tasks such as summarization and open-book question answering (QA), Large Language Models (LLMs) frequently experience “contextual hallucination”, where they generate irrelevant or incorrect responses despite having access to accurate information in the input. This issue often stems from the models’ propensity to prioritize self-generated content over input context, leading to a disregard for pertinent details. To address this challenge, we introduce, Guided Attention Map Editing (GAME), an innovative approach that dynamically adjusts attention maps to enhance contextual relevance. During inference, GAME employs a trained classifier to identify attention maps likely to induce hallucinations and implements targeted interventions. These interventions, guided by gradient-informed “edit directions”, strategically redistribute attention weights across various heads to efficiently mitigate hallucination. Extensive evaluations on challenging summarization and open-book QA tasks demonstrate that GAME consistently and significantly reduces hallucinations across diverse open-source models, thereby improving the reliability and applicability of LLMs.</abstract>
      <url hash="87e13d50">2025.findings-naacl.458</url>
      <bibkey>wang-etal-2025-gradient</bibkey>
    </paper>
    <paper id="459">
      <title>Alleviating Hallucinations of Large Language Models through Induced Hallucinations</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>V.</first><last>W.</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>8218-8232</pages>
      <abstract>Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as hallucination. In this work, we propose a simple Induce-then-Contrast Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and FActScore, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various task formats, model sizes, and model families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively, without compromising their generalization capabilities on other tasks.</abstract>
      <url hash="3d7fe18f">2025.findings-naacl.459</url>
      <bibkey>zhang-etal-2025-alleviating</bibkey>
    </paper>
    <paper id="460">
      <title><fixed-case>M</fixed-case>o<fixed-case>DE</fixed-case>: Effective Multi-task Parameter Efficient Fine-Tuning with a Mixture of Dyadic Experts</title>
      <author><first>Lin</first><last>Ning</last><affiliation>Google</affiliation></author>
      <author><first>Harsh</first><last>Lara</last><affiliation>Research, Google</affiliation></author>
      <author><first>Meiqi</first><last>Guo</last><affiliation>Google</affiliation></author>
      <author><first>Abhinav</first><last>Rastogi</last><affiliation>Google</affiliation></author>
      <pages>8233-8246</pages>
      <abstract>Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA) have revolutionized the adaptation of large language models (LLMs) to diverse tasks. Recent efforts have explored mixtures of LoRA modules for multi-task settings. However, our analysis reveals redundancy in the down-projection matrices of these architectures. This observation motivates our proposed method, Mixture of Dyadic Experts (MoDE), which introduces a novel design for efficient multi-task adaptation. This is done by sharing the down-projection matrix across tasks and employing atomic rank-one adapters, coupled with routers that allow more sophisticated task-level specialization. Our design allows for more fine-grained mixing, thereby increasing the model’s ability to jointly handle multiple tasks. We evaluate MoDE on the Supernatural Instructions (SNI) benchmark consisting of a diverse set of 700+ tasks and demonstrate that it outperforms state-of-the-art multi-task parameter-efficient fine-tuning (PEFT) methods, without introducing additional parameters. Our findings contribute to a deeper understanding of parameter efficiency in multi-task LLM adaptation and provide a practical solution for deploying high-performing, lightweight models.</abstract>
      <url hash="bb0acea4">2025.findings-naacl.460</url>
      <bibkey>ning-etal-2025-mode</bibkey>
    </paper>
    <paper id="461">
      <title>Unsupervised Sentence Representation Learning with Syntactically Aligned Negative Samples</title>
      <author><first>Zhilan</first><last>Wang</last></author>
      <author><first>Zekai</first><last>Zhi</last></author>
      <author><first>Rize</first><last>Jin</last><affiliation>Tiangong University</affiliation></author>
      <author><first>Kehui</first><last>Song</last><affiliation>Tiangong University</affiliation></author>
      <author><first>He</first><last>Wang</last><affiliation>tiangong university</affiliation></author>
      <author><first>Da-Jung</first><last>Cho</last><affiliation>Ajou University</affiliation></author>
      <pages>8247-8259</pages>
      <abstract>Sentence representation learning benefits from data augmentation strategies to improve model performance and generalization, yet existing approaches often encounter issues such as semantic inconsistencies and feature suppression. To address these limitations, we propose a method for generating Syntactically Aligned Negative (SAN) samples through a semantic importance-aware Masked Language Model (MLM) approach. Our method quantifies semantic contributions of individual words to produce negative samples that have substantial textual overlap with the original sentences while conveying different meanings. We further introduce Hierarchical-InfoNCE (HiNCE), a novel contrastive learning objective employing differential temperature weighting to optimize the utilization of both in-batch and syntactically aligned negative samples. Extensive evaluations across seven semantic textual similarity benchmarks demonstrate consistent improvements over state-of-the-art models.</abstract>
      <url hash="2516d657">2025.findings-naacl.461</url>
      <bibkey>wang-etal-2025-unsupervised</bibkey>
    </paper>
    <paper id="462">
      <title>Hierarchical Speculative Decoding with Dynamic Window</title>
      <author><first>Shensian</first><last>Syu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>8260-8273</pages>
      <abstract>Speculative Decoding (SD) utilizes an efficient draft model to generate multiple tokens, which are subsequently verified in parallel by a target model. This approach has shown significant potential for accelerating inference in large language models (LLMs), with performance heavily reliant on the hyperparameter <tex-math>K</tex-math>—the window size. However, previous methods often depend on simple heuristics to select <tex-math>K</tex-math> or dynamically adjust the window size, which may necessitate additional training or careful resource management to avoid competition.To address these challenges, we propose <b>H</b>ierarchical <b>S</b>peculative <b>D</b>ecoding with <b>D</b>ynamic <b>W</b>indow (HSDDW), a straightforward framework that eliminates the need for additional training. Specifically, we introduce a <i>self-verify</i> mechanism that enables the draft model to autonomously decide when to stop generating tokens. Additionally, by integrating a hierarchical structure that leverages the capabilities of models of different sizes, we significantly enhance the overall speed of the system.HSDDW demonstrates competitive performance across four datasets, achieving notable speedups of <tex-math>2.91\times</tex-math> on MT-Bench and <tex-math>2.99\times</tex-math> on Alpaca, outperforming existing state-of-the-art methods.</abstract>
      <url hash="b3d5d0c8">2025.findings-naacl.462</url>
      <bibkey>syu-lee-2025-hierarchical</bibkey>
    </paper>
    <paper id="463">
      <title><fixed-case>Q</fixed-case>-<fixed-case>FAKER</fixed-case>: Query-free Hard Black-box Attack via Controlled Generation</title>
      <author><first>CheolWon</first><last>Na</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>YunSeok</first><last>Choi</last><affiliation>SungKyunKwan University</affiliation></author>
      <author><first>Jee-Hyong</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>8274-8289</pages>
      <abstract>Many adversarial attack approaches are proposed to verify the vulnerability of language models. However, they require numerous queries and the information on the target model. Even black-box attack methods also require the target model’s output information. They are not applicable in real-world scenarios, as in hard black-box settings where the target model is closed and inaccessible. Even the recently proposed hard black-box attacks still require many queries and demand extremely high costs for training adversarial generators. To address these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a novel and efficient method that generates adversarial examples without accessing the target model. To avoid accessing the target model, we use a surrogate model instead. The surrogate model generates adversarial sentences for a target-agnostic attack. During this process, we leverage controlled generation techniques. We evaluate our proposed method on eight datasets. Experimental results demonstrate our method’s effectiveness including high transferability and the high quality of the generated adversarial examples, and prove its practical in hard black-box settings.</abstract>
      <url hash="48df05f8">2025.findings-naacl.463</url>
      <bibkey>na-etal-2025-q</bibkey>
    </paper>
    <paper id="464">
      <title><fixed-case>PRD</fixed-case>etect: Perturbation-Robust <fixed-case>LLM</fixed-case>-generated Text Detection Based on Syntax Tree</title>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Zhiyi</first><last>Yin</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hexiang</first><last>Tan</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaoling</first><last>Jing</last></author>
      <author><first>Du</first><last>Su</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>8290-8301</pages>
      <abstract>As LLM-generated text becomes increasingly prevalent on the internet, often containing hallucinations or biases, detecting such content has emerged as a critical area of research.Recent methods have demonstrated impressive performance in detecting text generated entirely by LLMs.However, in real-world scenarios, users often introduce perturbations to the LLM-generated text, and the robustness of existing detection methods against these perturbations has not been sufficiently explored.This paper empirically investigates this challenge and finds that even minor perturbations can severely degrade the performance of current detection methods. To address this issue, we find that the syntactic tree is minimally affected by disturbances and exhibits distinct differences between human-written and LLM-generated text.Therefore, we propose a detection method based on syntactic trees, which can capture features invariant to perturbations.It demonstrates significantly improved robustness against perturbation on the HC3 and GPT-3.5-mixed datasets.Moreover, it also has the shortest time expenditure.We provide the code and data at https://github.com/thulx18/PRDetect.</abstract>
      <url hash="d9b7be06">2025.findings-naacl.464</url>
      <bibkey>li-etal-2025-prdetect</bibkey>
    </paper>
    <paper id="465">
      <title>Enabling Natural Zero-Shot Prompting on Encoder Models via Statement-Tuning</title>
      <author><first>Ahmed</first><last>Elshabrawy</last></author>
      <author><first>Yongxin</first><last>Huang</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Institute for Computer Science, Artificial Intelligence and Technology, Mohamed bin Zayed University of Artificial Intelligence and Technische Universität Darmstadt</affiliation></author>
      <author><first>Alham Fikri</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>8302-8321</pages>
      <abstract>While Large Language Models (LLMs) exhibit remarkable capabilities in zero-shot and few-shot scenarios, they often require computationally prohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT and RoBERTa achieve state-of-the-art results through fine-tuning but struggle with extending to few-shot and zero-shot settings due to their architectural constraints. Hence, we propose Statement-Tuning, a technique that models discriminative tasks as a set of finite statements and trains an encoder model to discriminate between the potential statements to determine the label. We do Statement-Tuning on multiple tasks to enable cross-task generalization. Experimental results demonstrate that Statement-Tuning achieves competitive performance compared to state-of-the-art LLMs with significantly fewer parameters. Furthermore, we compare with previous encoder-based methodology and show that our method is more accurate and more robust to spurious patterns. Moreover, the study investigates the impact of several design choices on few-shot and zero-shot generalization, revealing that Statement-Tuning can achieve strong performance with modest training data and benefits from task and statement diversity for unseen task generalizability. We release all the code used to generate statement data, train and evaluate our Statement-Tuned models.</abstract>
      <url hash="476d7fb3">2025.findings-naacl.465</url>
      <bibkey>elshabrawy-etal-2025-enabling</bibkey>
    </paper>
    <paper id="466">
      <title>Faster Machine Translation Ensembling with Reinforcement Learning and Competitive Correction</title>
      <author><first>Kritarth</first><last>Prasad</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Mohammadi</first><last>Zaki</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Pratik Rakesh</first><last>Singh</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Pankaj</first><last>Wasnik</last><affiliation>Sony Research India</affiliation></author>
      <pages>8322-8335</pages>
      <abstract>Ensembling neural machine translation (NMT) models to produce higher-quality translations than the <tex-math>L</tex-math> individual models has been extensively studied. Recent methods typically employ a candidate selection block (CSB) and an encoder-decoder fusion block (FB), requiring inference across <i>all</i> candidate models, leading to significant computational overhead, generally <tex-math>\Omega(L)</tex-math>. This paper introduces <b>SmartGen</b>, a reinforcement learning (RL)-based strategy that improves the CSB by selecting a small, fixed number of candidates and identifying optimal groups to pass to the fusion block for each input sentence. Furthermore, previously, the CSB and FB were trained independently, leading to suboptimal NMT performance. Our DQN-based <b>SmartGen</b> addresses this by using feedback from the FB block as a reward during training. We also resolve a key issue in earlier methods, where candidates were passed to the FB without modification, by introducing a Competitive Correction Block (CCB). Finally, we validate our approach with extensive experiments on English-Hindi translation tasks in both directions as well as English to Chinese and English to German.</abstract>
      <url hash="3e724685">2025.findings-naacl.466</url>
      <bibkey>prasad-etal-2025-faster</bibkey>
    </paper>
    <paper id="467">
      <title>Evaluating Numeracy of Language Models as a Natural Language Inference Task</title>
      <author><first>Rahmad</first><last>Mahendra</last><affiliation>Royal Melbourne Institute of Technology and Universitas Indonesia</affiliation></author>
      <author><first>Damiano</first><last>Spina</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Lawrence</first><last>Cavedon</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Karin</first><last>Verspoor</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>8336-8361</pages>
      <abstract>While recent advancements in large language models (LLMs) have enhanced their capabilities to solve mathematical problems, other aspects of numeracy remain underexplored. In this paper, we propose a benchmark to evaluate the ability of language models to perform basic numeracy tasks. We frame numeracy as a Natural Language Inference (NLI) task to assess the models’ ability to understand both numbers and language contexts. We evaluate 49 language models (LMs), including fine-tuned LMs on NLI datasets, instruction-tuned LLMs, and specialized math-LLMs. Our findings reveal three main insights: (1) LLMs only clearly outperform smaller LMs in arithmetic tasks, indicating that mathematical reasoning cannot be generalized to other numeracy skills such as number comparison and normalization; (2) while most language models achieve fair to good accuracy for NLI entailment cases, they still struggle to predict contradiction and neutral cases; and (3) the robustness of language models’ numeracy capabilities needs improvement, particularly in understanding the semantics and pragmatics of numbers in linguistic contexts.</abstract>
      <url hash="0feefebb">2025.findings-naacl.467</url>
      <bibkey>mahendra-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="468">
      <title>Are Language Models Agnostic to Linguistically Grounded Perturbations? A Case Study of <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Poulami</first><last>Ghosh</last><affiliation>Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>8362-8396</pages>
      <abstract>Pre-trained language models (PLMs) are known to be susceptible to perturbations to the input text, but existing works do not explicitly focus on linguistically grounded attacks, which are subtle and more prevalent in nature. In this paper, we study whether PLMs are agnostic to linguistically grounded attacks or not. To this end, we offer the first study addressing this, investigating different Indic languages and various downstream tasks. Our findings reveal that although PLMs are susceptible to linguistic perturbations, when compared to non-linguistic attacks, PLMs exhibit a slightly lower susceptibility to linguistic attacks. This highlights that even constrained attacks are effective. Moreover, we investigate the implications of these outcomes across a range of languages, encompassing diverse language families and different scripts.</abstract>
      <url hash="0dc09779">2025.findings-naacl.468</url>
      <bibkey>ghosh-etal-2025-language</bibkey>
    </paper>
    <paper id="469">
      <title>Do <fixed-case>LLM</fixed-case>s Have Distinct and Consistent Personality? <fixed-case>TRAIT</fixed-case>: Personality Testset designed for <fixed-case>LLM</fixed-case>s with Psychometrics</title>
      <author><first>Seungbeen</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungwon</first><last>Lim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Computer Science Department, Stanford University and NVIDIA</affiliation></author>
      <author><first>Giyeong</first><last>Oh</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hyungjoo</first><last>Chae</last></author>
      <author><first>Jiwan</first><last>Chung</last></author>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Beong-woo</first><last>Kwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yeonsoo</first><last>Lee</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <pages>8397-8437</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have led to their adaptation in various domains as conversational agents. We wonder: can personality tests be applied to these agents to analyze their behavior, similar to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice questions designed to assess the personality of LLMs. TRAIT is built on two psychometrically validated small human questionnaires, Big Five Inventory (BFI) and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a variety of real-world scenarios. TRAIT also outperforms existing personality tests for LLMs in terms of reliability and validity, achieving the highest scores across four key metrics: Content Validity, Internal Validity, Refusal Rate, and Reliability. Using TRAIT, we reveal two notable insights into personalities of LLMs: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (e.g., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.</abstract>
      <url hash="17ab50f9">2025.findings-naacl.469</url>
      <bibkey>lee-etal-2025-llms</bibkey>
    </paper>
    <paper id="470">
      <title>Tell Me What You Know About Sexism: Expert-<fixed-case>LLM</fixed-case> Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection</title>
      <author><first>Myrthe</first><last>Reuver</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Indira</first><last>Sen</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Matteo</first><last>Melis</last></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>GESIS – Leibniz Institute for the Social Sciences and Heinrich-Heine University Düsseldorf</affiliation></author>
      <pages>8438-8467</pages>
      <abstract>This paper investigates hybrid intelligence and collaboration between researchers of sexism and Large Language Models (LLMs), with afour-component pipeline. First, nine sexism researchers answer questions about their knowledge of sexism and of LLMs. They then participate in two interactive experiments involving an LLM (GPT3.5). The first experiment has experts assessing the model’s knowledgeabout sexism and suitability for use in research. The second experiment tasks them with creating three different definitions of sexism: anexpert-written definition, an LLM-written one, and a co-created definition. Lastly, zero-shot classification experiments use the three definitions from each expert in a prompt template for sexism detection, evaluating GPT4o on 2.500 texts sampled from five sexism benchmarks. We then analyze the resulting 67.500 classification decisions. The LLM interactions lead to longer and more complex definitions of sexism. Expert-written definitions on average perform poorly compared to LLM-generated definitions. However, some experts do improve classification performance with their co-created definitions of sexism, also experts who are inexperienced in using LLMs.</abstract>
      <url hash="117f5692">2025.findings-naacl.470</url>
      <bibkey>reuver-etal-2025-tell</bibkey>
    </paper>
    <paper id="471">
      <title>The Role of Prosody in Spoken Question Answering</title>
      <author><first>Jie</first><last>Chi</last></author>
      <author><first>Maureen</first><last>de Seyssel</last><affiliation>Apple</affiliation></author>
      <author><first>Natalie</first><last>Schluter</last><affiliation>Technical University of Denmark, Apple and IT University</affiliation></author>
      <pages>8468-8479</pages>
      <abstract>Spoken language understanding research to date has generally carried a heavy text perspective. Most datasets are derived from text, which is then subsequently synthesized into speech, and most models typically rely on automatic transcriptions of speech. This is to the detriment of prosody–additional information carried by the speech signal beyond the phonetics of the words themselves and difficult to recover from text alone. In this work, we investigate the role of prosody in Spoken Question Answering. By isolating prosodic and lexical information on the SLUE-SQA-5 dataset, which consists of natural speech, we demonstrate that models trained on prosodic information alone can perform reasonably well by utilizing prosodic cues. However, we find that when lexical information is available, models tend to predominantly rely on it. Our findings suggest that while prosodic cues provide valuable supplementary information, more effective integration methods are required to ensure prosody contributes more significantly alongside lexical features.</abstract>
      <url hash="8b6f9646">2025.findings-naacl.471</url>
      <bibkey>chi-etal-2025-role</bibkey>
    </paper>
    <paper id="472">
      <title>Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation Generation</title>
      <author><first>Palaash</first><last>Goel</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Dushyant Singh</first><last>Chauhan</last></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>8480-8493</pages>
      <abstract>Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a <b>T</b>arget-a<b>U</b>gmented sha<b>R</b>ed fusion-<b>B</b>ased sarcasm explanati<b>O</b>n model, aka. . We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed model on the dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of by an average margin of <tex-math>+3.3\%</tex-math>. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on ‘s generated explanations and find them out to be comparatively better than other systems.</abstract>
      <url hash="735769c9">2025.findings-naacl.472</url>
      <bibkey>goel-etal-2025-target</bibkey>
    </paper>
    <paper id="473">
      <title>Seeds of Discourse: A Multilingual Corpus of Direct Quotations from <fixed-case>A</fixed-case>frican Media on Agricultural Biotechnologies</title>
      <author><first>Patricia</first><last>Chiril</last></author>
      <author><first>Trevor</first><last>Spreadbury</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Joeva Sean</first><last>Rock</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Brian</first><last>Dowd-Uribe</last></author>
      <author><first>David</first><last>Uminsky</last><affiliation>University of Chicago</affiliation></author>
      <pages>8494-8500</pages>
      <abstract>Direct quotations play a crucial role in journalism by substantiating claims and enhancing persuasive communication. This makes news articles a rich resource for opinion mining, providing valuable insights into the topics they cover. This paper presents the first multilingual corpora (English and French) featuring both manually annotated (1,657) and automatically extracted (102,483) direct quotations related to agricultural biotechnologies from a curated list of Africa-based news sources. In addition, we provide 665 instances annotated for Aspect-Based Sentiment Analysis, enabling a fine-grained examination of sentiment toward key aspects of agricultural biotechnologies. These corpora are freely available to the research community for future work on media discourse surrounding agricultural biotechnologies.</abstract>
      <url hash="284c13ea">2025.findings-naacl.473</url>
      <bibkey>chiril-etal-2025-seeds</bibkey>
    </paper>
    <paper id="474">
      <title>Position Really Matters: Towards a Holistic Approach for Prompt Tuning</title>
      <author><first>Xianjun</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Wenchao</first><last>Yu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Linda Ruth</first><last>Petzold</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <pages>8501-8523</pages>
      <abstract>Prompt tuning is highly effective in efficiently extracting knowledge from foundation models, encompassing both language, vision, and vision-language models. However, the efficacy of employing fixed soft prompts with a <i>predetermined position</i> for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. We first provide a theoretical analysis, revealing that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Then, we present a holistic parametric prompt tuning strategy that dynamically determines different factors of prompts based on specific tasks or instances. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP, vision recognition, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask settings.</abstract>
      <url hash="fa05121e">2025.findings-naacl.474</url>
      <bibkey>yang-etal-2025-position</bibkey>
    </paper>
  </volume>
</collection>
