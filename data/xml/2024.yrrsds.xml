<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.yrrsds">
  <volume id="1" ingest-date="2024-10-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 20th Workshop of Young Researchers' Roundtable on Spoken Dialogue Systems</booktitle>
      <editor><first>Koji</first><last>Inoue</last></editor>
      <editor><first>Yahui</first><last>Fu</last></editor>
      <editor><first>Agnes</first><last>Axelsson</last></editor>
      <editor><first>Atsumoto</first><last>Ohashi</last></editor>
      <editor><first>Brielen</first><last>Madureira</last></editor>
      <editor><first>Yuki</first><last>Zenimoto</last></editor>
      <editor><first>Biswesh</first><last>Mohapatra</last></editor>
      <editor><first>Armand</first><last>Stricker</last></editor>
      <editor><first>Sopan</first><last>Khosla</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyoto, Japan</address>
      <month>September</month>
      <year>2024</year>
      <url hash="489eeb33">2024.yrrsds-1</url>
      <venue>yrrsds</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="3ac94a4b">2024.yrrsds-1.0</url>
      <bibkey>yrrsds-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Conversational <fixed-case>XAI</fixed-case> and Explanation Dialogues</title>
      <author><first>Nils</first><last>Feldhus</last></author>
      <pages>1–4</pages>
      <abstract>My main research interest is human-centric explainability, i.e., making language models more interpretable by building applications that lower the barrier of entry to explanations. I am enthusiastic about interactive systems that pique the interest of more people beyond just the experts to learn about the inner workings of language models. My hypothesis is that users of language model applications and dialogue systems are more satisfied and trustworthy if they can look behind the curtain and get easy access to explanations of their behavior.</abstract>
      <url hash="fb22b602">2024.yrrsds-1.1</url>
      <bibkey>feldhus-2024-conversational</bibkey>
    </paper>
    <paper id="2">
      <title>Enhancing Emotion Recognition in Spoken Dialogue Systems through Multimodal Integration and Personalization</title>
      <author><first>Takumasa</first><last>Kaneko</last></author>
      <pages>5–7</pages>
      <abstract>My research interests focus on multimodal emotion recognition and personalization in emotion recognition tasks. In multimodal emotion recognition, existing studies demonstrate that integrating various data types like speech, text, and video enhances accuracy. However, real-time constraints and high dataset costs limit their practical application. I propose constructing a multimodal emotion recognition model by combining available unimodal datasets. In terms of personalization, traditional discrete emotion labels often fail to capture the complexity of human emotions. Although recent methods embed speaker characteristics to boost prediction accuracy, they require extensive retraining. I introduce continuous prompt tuning, which updates only the speaker prompts while keeping the speech encoder weights fixed, enabling the addition of new speaker data without additional retraining. This paper discusses these existing research gaps and presents novel approaches to address them, aiming to significantly improve emotion recognition in spoken dialogue systems.</abstract>
      <url hash="ff0b30da">2024.yrrsds-1.2</url>
      <bibkey>kaneko-2024-enhancing</bibkey>
    </paper>
    <paper id="3">
      <title>Towards Personalisation of User Support Systems.</title>
      <author><first>Tomoya</first><last>Higuchi</last></author>
      <pages>8–10</pages>
      <abstract>My research interests lie on the development of advanced user support systems, emphasizing the enhancement of user engagement and system effectiveness. The field of user support systems aims to help users accomplish complex tasks efficiently while ensuring a pleasant and intuitive interaction experience. I explore how to incorporate engaging and context-appropriate assistance into these systems to make the task completion process more effective and enjoyable for users.</abstract>
      <url hash="ad4d35ed">2024.yrrsds-1.3</url>
      <bibkey>higuchi-2024-towards</bibkey>
    </paper>
    <paper id="4">
      <title>Social Agents for Positively Influencing Human Psychological States</title>
      <author><first>Muhammad Yeza</first><last>Baihaqi</last></author>
      <pages>11–13</pages>
      <abstract>My research interest lies in the realm of social interactive agents, specifically in the development of social agents for positively influencing human psychological states. This interdisciplinary field merges elements of artificial intelligence, psychology, and human-computer interaction. My work integrates psychological theories with dialogue system technologies, including rule-based systems and large language models (LLMs). The core aim of my work is to leverage these systems to promote mental well-being and enhance user experiences in various contexts. At YRRSDS 2024, I plan to discuss several intriguing topics in spoken dialogue systems (SDS), including implementing psychological theories into SDS, assessing human-agent rapport without direct human evaluation, and conducting SDS evaluations using other SDSs. These topics promise to stimulate insightful and engaging discussions during the roundtable at YRRSDS.</abstract>
      <url hash="6213cdcd">2024.yrrsds-1.4</url>
      <bibkey>baihaqi-2024-social</bibkey>
    </paper>
    <paper id="5">
      <title>Personalized Topic Transition for Dialogue System</title>
      <author><first>Kai</first><last>Yoshida</last></author>
      <pages>14–15</pages>
      <abstract>In our research, we aim to achieve SDS capable of generating responses considering user preferences. While users have individual topic preferences, existing SDSs do not adequately consider such information. With the development of LLMs, SDSs are expected to be implemented in various tasks, including coexisting with humans in robotic applications. To become better partners with humans, systems are anticipated to memorize user preferences and utilize them in their response generation. Our future reserarch aim to realize SDSs that can remember and complement user information through dialogue, enabling personalized interactions. In YRRSDS, The author would like to propose the following topics for discussion. 1. What is the necessity of SDSs aimed specifically at dialogue rather than being just user interfaces? What do general users need from SDSs through conversation? 2. The relationship between SDSs and users: Should SDSs act just as agents, or should they aim to become like friends or family? 3. Privacy in conversational content. Nowadays, many SDS applications operate online via APIs, but is this preferable from a privacy perspective? If it is not preferable, how can this issue be resolved?</abstract>
      <url hash="c759372b">2024.yrrsds-1.5</url>
      <bibkey>yoshida-2024-personalized</bibkey>
    </paper>
    <paper id="6">
      <title>Elucidation of Psychotherapy and Development of New Treatment Methods Using <fixed-case>AI</fixed-case></title>
      <author><first>Shio</first><last>Maeda</last></author>
      <pages>16–17</pages>
      <abstract>My research theme is to develop an optimal analytical model for various information generated during therapy using multimodal data in psychotherapy, to elucidate the process of psychotherapy, and to create an AI therapist to develop a new psychotherapy. In this context, I would like to participate in the Young Researchers’ Roundtable on Spoken Dialogue Systems because I believe that I can broaden my research horizons by participating and discussing with various young researchers.</abstract>
      <url hash="cb848a79">2024.yrrsds-1.6</url>
      <bibkey>maeda-2024-elucidation</bibkey>
    </paper>
    <paper id="7">
      <title>Assessing Interactional Competence with Multimodal Dialog Systems</title>
      <author><first>Mao</first><last>Saeki</last></author>
      <pages>18–20</pages>
      <abstract>My research interests lie in multimodal dialog systems, especially in turn-taking and the understanding and generation of non-verbal cues. I am also interested in bringing dialog system research into industry, and making virtual agents practical in real world setting. I have been working on the Intelligent Language Learning Assistant (InteLLA) system, a virtual agent designed to provide fully automated English proficiency assessments through oral conversations. This project is driven by the practical need to address the lack of opportunities for second-language learners to assess and practice their conversation skills.</abstract>
      <url hash="b76e1985">2024.yrrsds-1.7</url>
      <bibkey>saeki-2024-assessing</bibkey>
    </paper>
    <paper id="8">
      <title>Faithfulness of Natural Language Generation</title>
      <author><first>Patricia</first><last>Schmidtova</last></author>
      <pages>21–24</pages>
      <abstract>In this position paper, I present my research interest in the faithfulness of natural language generation, i.e. the adherence to the data provided by a user or the dialog state. I motivate the task and present my progress and plans on the topic. I propose my position on the future of research dialog systems and share topics I would like to discuss during the roundtables.</abstract>
      <url hash="c3c4b3c7">2024.yrrsds-1.8</url>
      <bibkey>schmidtova-2024-faithfulness</bibkey>
    </paper>
    <paper id="9">
      <title>Knowledge-Grounded Dialogue Systems for Generating Interesting and Engaging Responses</title>
      <author><first>Hiroki</first><last>Onozeki</last></author>
      <pages>25–27</pages>
      <abstract>My research interests lie in the area of building a dialogue system to generate interesting and entertaining responses, with a particular focus on knowledge-grounded dialogue systems. Study of open-domain dialogue systems seeks to maximize user engagement by enhancing specific dialogue skills. To achieve this goal, much research has focused on the generation of empathetic responses, personality-based responses, and knowledge-grounded responses. In addition, interesting and entertaining responses from the open-domain dialogue systems can increase user satisfaction and engagement due to their diversity and ability to attract the user’s interest. It has also been observed in task-oriented dialogue, user engagement can be increased by incorporating interesting responses into the dialogue. For example, methods have been proposed to incorporate interesting responses into spoken dialogue systems (SDSs) that support the execution of complex tasks and provide a pleasant and enjoyable experience for the user. However, even in the case of interesting responses, if the dialogue is incoherent, user engagement is likely to be significantly reduced. To create a dialogue system that is consistent and interesting in a dialogue context, I am working on using knowledge-grounded response generation methods to select interesting knowledge that is relevant to the dialogue context and to make responses that are based on that knowledge.</abstract>
      <url hash="86f52549">2024.yrrsds-1.9</url>
      <bibkey>onozeki-2024-knowledge</bibkey>
    </paper>
    <paper id="10">
      <title>Towards a Dialogue System That Can Take Interlocutors’ Values into Account</title>
      <author><first>Yuki</first><last>Zenimoto</last></author>
      <pages>28–29</pages>
      <abstract>In this position paper, I present my research interests regarding the dialogue systems that can reflect the interlocutor’s values, such as their way of thinking and perceiving things. My work focuses on two main aspects: dialogue systems for eliciting the interlocutor’s values and methods for understanding the interlocutor’s values from narratives. Additionally, I discuss the abilities required for Spoken Dialogue Systems (SDSs) that can converse with the same user multiple times. Finally, I suggest topics for discussion regarding an SDS as a personal assistant for everyday use.</abstract>
      <url hash="1bba8e63">2024.yrrsds-1.10</url>
      <bibkey>zenimoto-2024-towards</bibkey>
    </paper>
    <paper id="11">
      <title>Multimodal Spoken Dialogue System with Biosignals</title>
      <author><first>Shun</first><last>Katada</last></author>
      <pages>30–31</pages>
      <abstract>The dominance of large language models has forced the transformation of research directions in many domains. The growth speed of large-scale models and the knowledge acquired have reached incredible levels. Thus, researchers must have the ability and foresight to adapt to a rapidly changing environment. In this position paper, the author introduces research interests and discusses their relationships from the perspective of spoken dialogue systems. In particular, the fields of multimodal processing and affective computing are introduced. Additionally, the effects of large language models on spoken dialogue systems research and topics for discussion are presented.</abstract>
      <url hash="db31f885">2024.yrrsds-1.11</url>
      <bibkey>katada-2024-multimodal</bibkey>
    </paper>
    <paper id="12">
      <title>Timing Sensitive Turn-Taking in Spoken Dialogue Systems Based on User Satisfaction</title>
      <author><first>Sadahiro</first><last>Yoshikawa</last></author>
      <pages>32–34</pages>
      <abstract/>
      <url hash="b24d5a49">2024.yrrsds-1.12</url>
      <bibkey>yoshikawa-2024-timing</bibkey>
    </paper>
    <paper id="13">
      <title>Towards Robust and Multilingual Task-Oriented Dialogue Systems</title>
      <author><first>Atsumoto</first><last>Ohashi</last></author>
      <pages>35–36</pages>
      <abstract>In this position paper, I present my research interests regarding the field of task-oriented dialogue systems. My work focuses on two main aspects: optimizing the task completion ability of dialogue systems using reinforcement learning, and developing language resources and exploring multilinguality to support the advancement of dialogue systems across different languages. I discuss the limitations of current approaches in achieving robust task completion performance and propose a novel optimization approach called Post-Processing Networks. Furthermore, I highlight the importance of multilingual dialogue datasets and describe our work on constructing JMultiWOZ, the first large-scale Japanese task-oriented dialogue dataset.</abstract>
      <url hash="011f2d99">2024.yrrsds-1.13</url>
      <bibkey>ohashi-2024-towards</bibkey>
    </paper>
    <paper id="14">
      <title>Toward Faithful Dialogs: Evaluating and Improving the Faithfulness of Dialog Systems</title>
      <author><first>Sicong</first><last>Huang</last></author>
      <pages>37–39</pages>
      <abstract>My primary research interests lie in evaluating and improving the faithfulness of language model-based text generation systems. Recent advances in large language models (LLMs) such as GPT-4 and Llama have enabled the wide adoption of LLMs in various aspects of natural language processing (NLP). Despite their widespread use, LLMs still suffer from the problem of hallucination, limiting the practicality of deploying such systems in use cases where being factual and faithful is of critical importance. My research specifically aims to evaluate and improve the faithfulness, i.e. the factual alignment between the generated text and a given context, of text generation systems. By developing techniques to reliably evaluate, label, and improve generation faithfulness, we can enable wider adoption of dialog systems that need to converse with human users using accurate information.</abstract>
      <url hash="6d8b08f7">2024.yrrsds-1.14</url>
      <bibkey>huang-2024-toward</bibkey>
    </paper>
    <paper id="15">
      <title>Cognitive Model of Listener Response Generation and Its Application to Dialogue Systems</title>
      <author><first>Taiga</first><last>Mori</last></author>
      <pages>40–42</pages>
      <abstract>In this position paper, we introduce our efforts in modeling listener response generation and its application to dialogue systems. We propose that the cognitive process of generating listener responses involves four levels: attention level, word level, propositional information level, and activity level, with different types of responses used depending on the level. Attention level responses indicate that the listener is listening to and paying attention to the speaker’s speech. Word-level responses demonstrate the listener’s knowledge or understanding of a single representation. Propositional information level responses indicate the listener’s understanding, empathy, and emotions towards a single propositional information. Activity level responses are oriented towards activities. Additionally, we briefly report on our current initiative in generating propositional information level responses using a knowledge graph and LLMs.</abstract>
      <url hash="3776c22e">2024.yrrsds-1.15</url>
      <bibkey>mori-2024-cognitive</bibkey>
    </paper>
    <paper id="16">
      <title>Topological Deep Learning for Term Extraction</title>
      <author><first>Benjamin Matthias</first><last>Ruppik</last></author>
      <pages>43–45</pages>
      <abstract>Ben is a postdoctoral researcher in the Dialog Systems and Machine Learning research group led by Milica Gašić at the Heinrich-Heine-Universität Düsseldorf, which he joined in 2022. In collaboration with the Topology and Geometry group in the Mathematics Department, under the supervision of Marcus Zibrowius, Ben is developing applications of Topological Data Analysis in Natural Language Processing, focusing on dialogue systems. Before transitioning to machine learning research, Ben was a pure mathematician at the Max-Planck-Institute for Mathematics in Bonn, where he specialized in knotted surfaces in 4-dimensional manifolds. He graduated from the University of Bonn in 2022.</abstract>
      <url hash="dcd48077">2024.yrrsds-1.16</url>
      <bibkey>ruppik-2024-topological</bibkey>
    </paper>
    <paper id="17">
      <title>Dialogue Management with Graph-structured Knowledge</title>
      <author><first>Nicholas Thomas</first><last>Walker</last></author>
      <pages>46–47</pages>
      <abstract>I am a postdoctoral researcher at Otto-Friedrich University of Bamberg, and my research interests include the knowledge-grounded dialogue systems, logical rule-based reasoning for dialogue management, and human-robot interaction.</abstract>
      <url hash="e62e5a0c">2024.yrrsds-1.17</url>
      <bibkey>walker-2024-dialogue</bibkey>
    </paper>
    <paper id="18">
      <title>Towards a Co-creation Dialogue System</title>
      <author><first>Xulin</first><last>Zhou</last></author>
      <pages>48–49</pages>
      <abstract>In this position paper, I present my research interests in dialogue systems, where the user and the system collaboratively work on tasks through conversation. My work involves analyzing dialogues in which two parties collaborate through conversation, focusing on tasks that yield outcomes with no single correct answer. To support this research, I have created a tagline co-writing dialogue corpus, which I have analyzed from various perspectives. Additionally, I developed a prototype for a tagline co-writing dialogue system.</abstract>
      <url hash="6eb0134e">2024.yrrsds-1.18</url>
      <bibkey>zhou-2024-towards</bibkey>
    </paper>
    <paper id="19">
      <title>Enhancing Decision-Making with <fixed-case>AI</fixed-case> Assistance</title>
      <author><first>Yoshiki</first><last>Tanaka</last></author>
      <pages>50–52</pages>
      <abstract>My research interests broadly lie in the influence of artificial intelligence (AI) agents on human decision-making. Specifically, I aim to develop applications for conversational agents in decision-making support. During my master’s program, I developed a system that uses an interview dialogue system to support user review writing. In this approach, the conversational agent gathers product information such as users’ impressions and opinions during the interview, to create reviews, facilitating the review writing process. Additionally, I conducted a comprehensive evaluation from the perspectives of system users and review readers. Although experimental results have shown that the system is capable of generating helpful reviews, the quality of the reviews still depends on how effectively the agent elicits the information from users. Therefore, I believe that personalizing the agent’s interview strategy to users’ preferences regarding the review writing process can further enhance both the user experience and the helpfulness of the review.</abstract>
      <url hash="430f44b5">2024.yrrsds-1.19</url>
      <bibkey>tanaka-2024-enhancing</bibkey>
    </paper>
    <paper id="20">
      <title>Ontology Construction for Task-oriented Dialogue</title>
      <author><first>Renato</first><last>Vukovic</last></author>
      <pages>53–56</pages>
      <abstract>My research interests lie generally in dialogue ontology construction, that uses techniques from information extraction to extract relevant terms from task-oriented dialogue data and order them by finding hierarchical relations between them.</abstract>
      <url hash="ec329515">2024.yrrsds-1.20</url>
      <bibkey>vukovic-2024-ontology</bibkey>
    </paper>
    <paper id="21">
      <title>Generalized Visual-Language Grounding with Complex Language Context</title>
      <author><first>Bhathiya</first><last>Hemanthage</last></author>
      <pages>57–59</pages>
      <abstract>My research focus on <b>Visual Dialogues</b> and <b>Generalized Visual-Language Grounding with Complex Language Context</b>. Specifically, my research aim to utilize Large Language Models (LLMs) to build <i>conversational agents capable of comprehending and responding to visual cues</i>. Visual-Language Pre-trained (VLP) models, primarily utilizing transformer-based encoder-decoder architectures, are extensively employed across a range of visual-language tasks, such as visual question answering (VQA) and referring expression comprehension (REC). The effectiveness of these models stems from their robust visual-language integration capabilities. However, their performance is constrained in more complex applications like multimodal conversational agents, where intricate and extensive language contexts pose significant challenges. These tasks demands language-only reasoning before engaging in multimodal fusion. In response, my research investigates the application of Large Language Models (LLMs) with advance comprehension and generation capabilities to enhance performance in complex multimodal tasks, particularly multimodal dialogues. In brief, my work in visual dialogues revolves around two major research questions. i) How to redefine visually grounded conversational agent architectures to benefit from LLMs ii) How to transfer the large body of knowledge encoded in LLMs to conversational systems.</abstract>
      <url hash="2a681220">2024.yrrsds-1.21</url>
      <bibkey>hemanthage-2024-generalized</bibkey>
    </paper>
    <paper id="22">
      <title>Towards a Real-Time Multimodal Emotion Estimation Model for Dialogue Systems</title>
      <author><first>Jingjing</first><last>Jiang</last></author>
      <pages>60–61</pages>
      <abstract>This position paper presents my research interest in establishing human-like chat-oriented dialogue systems. To this end, my work focuses on two main areas: the construction and utilization of multimodal datasets and real-time multimodal affective computing. I discuss the limitations of current multimodal dialogue corpora and multimodal affective computing models. As a solution, I have constructed a human-human dialogue dataset containing various synchronized multimodal information, and I have conducted preliminary analyses on it. In future work, I will further analyze the collected data and build a real-time multimodal emotion estimation model for dialogue systems.</abstract>
      <url hash="8fb60aa1">2024.yrrsds-1.22</url>
      <bibkey>jiang-2024-towards</bibkey>
    </paper>
    <paper id="23">
      <title>Exploring Explainability and Interpretability in Generative <fixed-case>AI</fixed-case></title>
      <author><first>Shiyuan</first><last>Huang</last></author>
      <pages>62–63</pages>
      <abstract/>
      <url hash="4165f77c">2024.yrrsds-1.23</url>
      <bibkey>huang-2024-exploring</bibkey>
    </paper>
    <paper id="24">
      <title>Innovative Approaches to Enhancing Safety and Ethical <fixed-case>AI</fixed-case> Interactions in Digital Environments</title>
      <author><first>Zachary</first><last>Yang</last></author>
      <pages>64–67</pages>
      <abstract>Ensuring safe online environments is a formidable challenge, but nonetheless an important one as people are now chronically online. The increasing online presence of people paired with the prevalence of harmful content such as toxicity, hate speech, misinformation and disinformation across various social media platforms and within different video calls for stronger detection and prevention methods. My research interests primarily lie in applied natural language processing for social good. Previously, I focused on measuring partisan polarization on social media during the COVID-19 pandemic and its societal impacts. Currently, at Ubisoft La Forge, I am dedicated to enhancing player safety within in-game chat systems by developing methods to detect toxicity, evaluating the biases in these detection systems, and assessing the current ecological state of online interactions. Additionally, I am engaged in simulating social media environments using LLMs to ethically test detection methods, evaluate the effectiveness of current mitigation strategies, and potentially introduce new, successful strategies. My suggested topics for discussion: 1. Understanding and mitigating social harms through high fidelity simulated social media environments 2. Enhancing safety in online environments such as within in-game chats (text and speech) 3. Personification of LLM agents 4. Ethically simulating social media sandbox environments at scale with LLM agents 5. Re-balancing the playing field between good and bad actors: Strategies for countering societal-scale manipulation.</abstract>
      <url hash="78fe35ed">2024.yrrsds-1.24</url>
      <bibkey>yang-2024-innovative</bibkey>
    </paper>
    <paper id="25">
      <title>Leveraging Linguistic Structural Information for Improving the Model’s Semantic Understanding Ability</title>
      <author><first>Sangmyeong</first><last>Lee</last></author>
      <pages>68–69</pages>
      <abstract>This position paper describes research interests of the author (semantic structure comprehension in multimodal dialogue environments), his point of view on Spoken Dialogue System research that a new wave is to be carried out for coexistence with LLMs, and discussion topic proposals. Those three topics are as follows: 1) How to keep up with, or manipulate LLM for academia research, 2) How representational languages for semantic structural information could be used in this new era, and 3) how to deal with disambiguating the user’s language during the actual dialogue scenario.</abstract>
      <url hash="e9dfc8d7">2024.yrrsds-1.25</url>
      <bibkey>lee-2024-leveraging</bibkey>
    </paper>
    <paper id="26">
      <title>Multi-User Dialogue Systems and Controllable Language Generation</title>
      <author><first>Nicolas</first><last>Wagner</last></author>
      <pages>70–72</pages>
      <abstract>My research interests include multi-user dialogue systems with a focus on user modelling and the development of moderation strategies. Contemporary Spoken Dialogue Systems (SDSs) frequently lack the ability to deal with more than one user simultaneously. Moreover, I am interested in researching on the Controllability of Language Generation using Large Language Models (LLMs). Our hypothesis is that an integration of explicit dialogue control signals improves the Controllability and Reliability of generated sequences independently of the underlying LLM.</abstract>
      <url hash="a9fa911a">2024.yrrsds-1.26</url>
      <bibkey>wagner-2024-multi</bibkey>
    </paper>
    <paper id="27">
      <title>Enhancing Role-Playing Capabilities in Persona Dialogue Systems through Corpus Construction and Evaluation Methods</title>
      <author><first>Ryuichi</first><last>Uehara</last></author>
      <pages>73–75</pages>
      <abstract>My research interest involves persona dialogue systems, which use the profile information of a character or real person, called a persona, and responds accordingly. Persona dialogue systems can improve the consistency of the system’s responses, users’ trust, and user enjoyment. My current research focuses on persona dialogue systems, especially dialogue agents that role-play as fictional characters. The first task involves obtaining the dialogue and personas of novel characters and building a dialogue corpus. The second task involves evaluating whether the dialogue agent’s responses are character-like relative to the context. The goal of these studies is to allow dialogue agents to generate responses that are more character-like.</abstract>
      <url hash="253c5e1a">2024.yrrsds-1.27</url>
      <bibkey>uehara-2024-enhancing</bibkey>
    </paper>
    <paper id="28">
      <title>Character Expression and User Adaptation for Spoken Dialogue Systems</title>
      <author><first>Kenta</first><last>Yamamoto</last></author>
      <pages>76–77</pages>
      <abstract>The author is interested in building dialogue systems with character and user adaptation. The goal is to create a dialogue system capable of establishing deeper relationships with users. To build a trustful relationship with users, it is important for the system to express its character. The author particularly aims to convey the system’s character through multimodal behavior. Users currently try to speak clearly to avoid speech recognition errors when interacting with SDSs. However, it is necessary to develop SDSs that allow users to converse naturally, as if they were speaking with a human. The author focused on user adaptation by considering user personality. In particular, the author proposes a system that adjusts its manner of speaking according to the user’s personality. Furthermore, the author is interested not only in adjusting the system’s speaking style to match the user but also in making the system’s listening style more conducive to natural conversation.</abstract>
      <url hash="00f851e1">2024.yrrsds-1.28</url>
      <bibkey>yamamoto-2024-character</bibkey>
    </paper>
    <paper id="29">
      <title>Interactive Explanations through Dialogue Systems</title>
      <author><first>Isabel</first><last>Feustel</last></author>
      <pages>78–80</pages>
      <abstract>The growing need for transparency in AI systems has led to the increased popularity of explainable AI (XAI), with dialogue systems emerging as a promising approach to provide dynamic and interactive explanations. To overcome the limitations of non-conversational XAI methods, we proposed and implemented a generic dialogue architecture that integrates domain-specific knowledge, enhancing user comprehension and interaction. By incorporating computational argumentation and argumentative tree structures into our prototype, we found a positive impact on the dialogue’s effectiveness. In future research, we plan to improve Natural Language Understanding (NLU) to reduce error rates and better interpret user queries, and to advance Natural Language Generation (NLG) techniques for generating more fluid and contextually appropriate responses using large language models. Additionally, we will refine argument annotation to enable better selection and presentation of information, ensuring the system provides the most relevant and coherent explanations based on user needs. Over the next 5 to 10 years, we anticipate significant advancements in dialogue systems’ flexibility, personalization, and cultural adaptability, driven by large language models and open domain dialogues. These developments will enhance global communication, user satisfaction, and the effectiveness of virtual assistants across various applications while addressing ethical and social implications.</abstract>
      <url hash="37512989">2024.yrrsds-1.29</url>
      <bibkey>feustel-2024-interactive</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Emotion-aware Task-oriented Dialogue Systems in the Era of Large Language Models</title>
      <author><first>Shutong</first><last>Feng</last></author>
      <pages>81–83</pages>
      <abstract>My research interests lie in the area of <b>modelling affective behaviours of interlocutors in conversations</b>. In particular, I look at emotion perception, expression, and management in information-retrieval task-oriented dialogue (ToD) systems. Traditionally, ToD systems focus primarily on fulfilling the user’s goal by requesting and providing appropriate information. Yet, in real life, the user’s emotional experience also contributes to the overall satisfaction. This requires the system’s ability to recognise, manage, and express emotions. To this end, I incorporated emotion in the entire ToD system pipeline (Feng et al., 2024, to appear in SIGDIAL 2024). In addition, in the era of large language models (LLMs), emotion recognition and generation have been made easy even under a zero-shot set-up (Feng et al., 2023; Stricker and Paroubek, 2024). Therefore, I am also interested in building ToD systems with LLMs and examining various types of affect in other ToD set-ups such as depression detection in clinical consultations.</abstract>
      <url hash="a056d2a5">2024.yrrsds-1.30</url>
      <bibkey>feng-2024-towards</bibkey>
    </paper>
    <paper id="31">
      <title>Utilizing Large Language Models for Customized Dialogue Data Augmentation and Psychological Counseling</title>
      <author><first>Zhiyang</first><last>Qi</last></author>
      <pages>84–86</pages>
      <abstract>Large language models (LLMs), such as GPT-4, have driven significant technological advances in spoken dialogue systems (SDSs). In the era of LLMs, my research focuses on: (1) employing these models for customized dialogue data augmentation to improve SDS adaptability to various speaking styles, and (2) utilizing LLMs to support counselors with psychological counseling dialogues. In the future, I aim to integrate these themes, applying user adaptability to psychological counseling dialogues to facilitate smoother conversations.</abstract>
      <url hash="eca631b3">2024.yrrsds-1.31</url>
      <bibkey>qi-2024-utilizing</bibkey>
    </paper>
    <paper id="32">
      <title>Toward More Human-like <fixed-case>SDS</fixed-case>s: Advancing Emotional and Social Engagement in Embodied Conversational Agents</title>
      <author><first>Zi Haur</first><last>Pang</last></author>
      <pages>87–89</pages>
      <abstract>The author’s research advances human-AI interaction across two innovative domains to enhance the depth and authenticity of communication. Through Emotional Validation, which leverages psychotherapeutic techniques, the research enriches SDSs with advanced capabilities for understanding and responding to human emotions. On the other hand, while utilizing Embodied Conversational Agents (ECAs), the author focuses on developing agents that simulate sophisticated human social behaviors, enhancing their ability to engage in context-sensitive and personalized dialogue. Together, these initiatives aim to transform SDSs and ECAs into empathetic, embodied companions, pushing the boundaries of conversational AI.</abstract>
      <url hash="e4619241">2024.yrrsds-1.32</url>
      <bibkey>pang-2024-toward</bibkey>
    </paper>
  </volume>
</collection>
