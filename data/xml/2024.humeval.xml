<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.humeval">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024</booktitle>
      <editor><first>Simone</first><last>Balloccu</last></editor>
      <editor id="anja-belz"><first>Anya</first><last>Belz</last></editor>
      <editor><first>Rudali</first><last>Huidrom</last></editor>
      <editor><first>Ehud</first><last>Reiter</last></editor>
      <editor><first>Joao</first><last>Sedoc</last></editor>
      <editor><first>Craig</first><last>Thomson</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="761fe296">2024.humeval-1</url>
      <venue>humeval</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="8d7fc377">2024.humeval-1.0</url>
      <bibkey>humeval-2024-human</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Quality and Quantity of Machine Translation References for Automatic Metrics</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author id="ondrej-bojar"><first>Ondřej</first><last>Bojar</last></author>
      <pages>1–11</pages>
      <abstract>Automatic machine translation metrics typically rely on human translations to determine the quality of system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average (or maximum) helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.</abstract>
      <url hash="63f93dc2">2024.humeval-1.1</url>
      <bibkey>zouhar-bojar-2024-quality</bibkey>
    </paper>
    <paper id="2">
      <title>Exploratory Study on the Impact of <fixed-case>E</fixed-case>nglish Bias of Generative Large Language Models in <fixed-case>D</fixed-case>utch and <fixed-case>F</fixed-case>rench</title>
      <author><first>Ayla</first><last>Rigouts Terryn</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <pages>12–27</pages>
      <abstract>The most widely used LLMs like GPT4 and Llama 2 are trained on large amounts of data, mostly in English but are still able to deal with non-English languages. This English bias leads to lower performance in other languages, especially low-resource ones. This paper studies the linguistic quality of LLMs in two non-English high-resource languages: Dutch and French, with a focus on the influence of English. We first construct a comparable corpus of text generated by humans versus LLMs (GPT-4, Zephyr, and GEITje) in the news domain. We proceed to annotate linguistic issues in the LLM-generated texts, obtaining high inter-annotator agreement, and analyse these annotated issues. We find a substantial influence of English for all models under all conditions: on average, 16% of all annotations of linguistic errors or peculiarities had a clear link to English. Fine-tuning a LLM to a target language (GEITje is fine-tuned on Dutch) reduces the number of linguistic issues and probably also the influence of English. We further find that using a more elaborate prompt leads to linguistically better results than a concise prompt. Finally, increasing the temperature for one of the models leads to lower linguistic quality but does not alter the influence of English.</abstract>
      <url hash="4d07a6b7">2024.humeval-1.2</url>
      <bibkey>rigouts-terryn-de-lhoneux-2024-exploratory</bibkey>
    </paper>
    <paper id="3">
      <title>Adding Argumentation into Human Evaluation of Long Document Abstractive Summarization: A Case Study on Legal Opinions</title>
      <author><first>Mohamed</first><last>Elaraby</last></author>
      <author><first>Huihui</first><last>Xu</last></author>
      <author><first>Morgan</first><last>Gray</last></author>
      <author id="kevin-d-ashley"><first>Kevin</first><last>Ashley</last></author>
      <author id="diane-litman"><first>Diane</first><last>Litman</last></author>
      <pages>28–35</pages>
      <abstract>Human evaluation remains the gold standard for assessing abstractive summarization. However, current practices often prioritize constructing evaluation guidelines for fluency, coherence, and factual accuracy, overlooking other critical dimensions. In this paper, we investigate argument coverage in abstractive summarization by focusing on long legal opinions, where summaries must effectively encapsulate the document’s argumentative nature. We introduce a set of human-evaluation guidelines to evaluate generated summaries based on argumentative coverage. These guidelines enable us to assess three distinct summarization models, studying the influence of including argument roles in summarization. Furthermore, we utilize these evaluation scores to benchmark automatic summarization metrics against argument coverage, providing insights into the effectiveness of automated evaluation methods.</abstract>
      <url hash="4066d1a1">2024.humeval-1.3</url>
      <bibkey>elaraby-etal-2024-adding</bibkey>
    </paper>
    <paper id="4">
      <title>A Gold Standard with Silver Linings: Scaling Up Annotation for Distinguishing <fixed-case>B</fixed-case>osnian, <fixed-case>C</fixed-case>roatian, <fixed-case>M</fixed-case>ontenegrin and <fixed-case>S</fixed-case>erbian</title>
      <author><first>Aleksandra</first><last>Miletić</last></author>
      <author><first>Filip</first><last>Miletić</last></author>
      <pages>36–46</pages>
      <abstract>Bosnian, Croatian, Montenegrin and Serbian are the official standard linguistic varieties in Bosnia and Herzegovina, Croatia, Montenegro, and Serbia, respectively. When these four countries were part of the former Yugoslavia, the varieties were considered to share a single linguistic standard. After the individual countries were established, the national standards emerged. Today, a central question about these varieties remains the following: How different are they from each other? How hard is it to distinguish them? While this has been addressed in NLP as part of the task on Distinguishing Between Similar Languages (DSL), little is known about human performance, making it difficult to contextualize system results. We tackle this question by reannotating the existing BCMS dataset for DSL with annotators from all target regions. We release a new gold standard, replacing the original single-annotator, single-label annotation by a multi-annotator, multi-label one, thus improving annotation reliability and explicitly coding the existence of ambiguous instances. We reassess a previously proposed DSL system on the new gold standard and establish the human upper bound on the task. Finally, we identify sources of annotation difficulties and provide linguistic insights into the BCMS dialect continuum, with multiple indicators highlighting an intermediate position of Bosnian and Montenegrin.</abstract>
      <url hash="1ff9fbe6">2024.humeval-1.4</url>
      <bibkey>miletic-miletic-2024-gold</bibkey>
    </paper>
    <paper id="5">
      <title>Insights of a Usability Study for <fixed-case>KBQA</fixed-case> Interactive Semantic Parsing: Generation Yields Benefits over Templates but External Validity Remains Challenging</title>
      <author><first>Ashley</first><last>Lewis</last></author>
      <author><first>Lingbo</first><last>Mo</last></author>
      <author id="marie-catherine-de-marneffe"><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <author id="michael-white"><first>Michael</first><last>White</last></author>
      <pages>47–62</pages>
      <abstract>We present our findings from a usability study of an interactive semantic parsing system for knowledge based question answering (KBQA). The system is designed to help users access information within a knowledge base without having to know its query language. The system translates the user’s question into the query language, retrieves an answer, then presents an English explanation of the process so that the user can make corrections if necessary. To our knowledge, our work is the most thorough usability study conducted for such a system and the only one that uses crowdworkers as participants to verify that the system is usable for average users. Our crowdworkers participate in KBQA dialogues using 4 versions of a system based on the framework by Mo et al. (2022) and answer surveys about their experiences. Some key takeaways from this work are: 1) we provide evidence for the benefits of interactivity in semantic parsing with human users and using generated questions in lieu of templated representations, 2) we identify limitations of simulations and provide contrasting evidence from actual system use, and 3) we provide an examination of crowdsourcing methodology, in particular the trade-offs of using crowdworkers vs. a specially trained group of evaluators.</abstract>
      <url hash="c8cb6e3c">2024.humeval-1.5</url>
      <bibkey>lewis-etal-2024-insights</bibkey>
    </paper>
    <paper id="6">
      <title>Extrinsic evaluation of question generation methods with user journey logs</title>
      <author><first>Elie</first><last>Antoine</last></author>
      <author><first>Eléonore</first><last>Besnehard</last></author>
      <author id="frederic-bechet"><first>Frederic</first><last>Bechet</last></author>
      <author id="geraldine-damnati"><first>Geraldine</first><last>Damnati</last></author>
      <author><first>Eric</first><last>Kergosien</last></author>
      <author><first>Arnaud</first><last>Laborderie</last></author>
      <pages>63–70</pages>
      <abstract>There is often a significant disparity between the performance of Natural Language Processing (NLP) tools as evaluated on benchmark datasets using metrics like ROUGE or BLEU, and the actual user experience encountered when employing these tools in real-world scenarios. This highlights the critical necessity for user-oriented studies aimed at evaluating user experience concerning the effectiveness of developed methodologies. A primary challenge in such “ecological” user studies is their assessment of specific configurations of NLP tools, making replication under identical conditions impractical. Consequently, their utility is limited for the automated evaluation and comparison of different configurations of the same tool. The objective of this study is to conduct an “ecological” evaluation of a question generation within the context of an external task involving document linking. To do this we conducted an "<i>ecological</i>" evaluation of a document linking tool in the context of the exploration of a Social Science archives and from this evaluation, we aim to derive a form of a “reference corpus” that can be used offline for the automated comparison of models and quantitative tool assessment. This corpus is available on the following link: https://gitlab.lis-lab.fr/archival-public/autogestion-qa-linking</abstract>
      <url hash="93bee20c">2024.humeval-1.6</url>
      <bibkey>antoine-etal-2024-extrinsic</bibkey>
    </paper>
    <paper id="7">
      <title>Towards Holistic Human Evaluation of Automatic Text Simplification</title>
      <author><first>Luisa</first><last>Carrer</last></author>
      <author><first>Andreas</first><last>Säuberli</last></author>
      <author><first>Martin</first><last>Kappus</last></author>
      <author><first>Sarah</first><last>Ebling</last></author>
      <pages>71–80</pages>
      <abstract>Text simplification refers to the process of rewording within a single language, moving from a standard form into an easy-to-understand one. Easy Language and Plain Language are two examples of simplified varieties aimed at improving readability and understanding for a wide-ranging audience. Human evaluation of automatic text simplification is usually done by employing experts or crowdworkers to rate the generated texts. However, this approach does not include the target readers of simplified texts and does not reflect actual comprehensibility. In this paper, we explore different ways of measuring the quality of automatically simplified texts. We conducted a multi-faceted evaluation study involving end users, post-editors, and Easy Language experts and applied a variety of qualitative and quantitative methods. We found differences in the perception and actual comprehension of the texts by different user groups. In addition, qualitative surveys and behavioral observations proved to be essential in interpreting the results.</abstract>
      <url hash="e2b2a61c">2024.humeval-1.7</url>
      <bibkey>carrer-etal-2024-towards</bibkey>
    </paper>
    <paper id="8">
      <title>Decoding the Metrics Maze: Navigating the Landscape of Conversational Question Answering System Evaluation in Procedural Tasks</title>
      <author><first>Alexander</first><last>Frummet</last></author>
      <author><first>David</first><last>Elsweiler</last></author>
      <pages>81–90</pages>
      <abstract>Conversational systems are widely used for various tasks, from answering general questions to domain-specific procedural tasks, such as cooking. While the effectiveness of metrics for evaluating general question answering (QA) tasks has been extensively studied, the evaluation of procedural QA remains a challenge as we do not know what answer types users prefer in such tasks. Existing studies on metrics evaluation often focus on general QA tasks and typically limit assessments to one answer type, such as short, SQuAD-like responses or longer passages. This research aims to achieve two objectives. Firstly, it seeks to identify the desired traits of conversational QA systems in procedural tasks, particularly in the context of cooking (RQ1). Second, it assesses how commonly used conversational QA metrics align with these traits and perform across various categories of correct and incorrect answers (RQ2). Our findings reveal that users generally favour concise conversational responses, except in time-sensitive scenarios where brief, clear answers hold more value (e.g. when heating in oil). While metrics effectively identify inaccuracies in short responses, several commonly employed metrics tend to assign higher scores to incorrect conversational answers when compared to correct ones. We provide a selection of metrics that reliably detect correct and incorrect information in short and conversational answers.</abstract>
      <url hash="bbdc3c78">2024.humeval-1.8</url>
      <bibkey>frummet-elsweiler-2024-decoding</bibkey>
    </paper>
    <paper id="9">
      <title>The 2024 <fixed-case>R</fixed-case>epro<fixed-case>NLP</fixed-case> Shared Task on Reproducibility of Evaluations in <fixed-case>NLP</fixed-case>: Overview and Results</title>
      <author><first>Anya</first><last>Belz</last></author>
      <author><first>Craig</first><last>Thomson</last></author>
      <pages>91–105</pages>
      <abstract>This paper presents an overview of, and the results from, the 2024 Shared Task on Reproducibility of Evaluations in NLP (ReproNLP’24), following on from three previous shared tasks on reproducibility of evaluations in NLP, ReproNLP’23, ReproGen’22 and ReproGen’21. This shared task series forms part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP and machine learning, against a backdrop of increasing recognition of the importance of reproducibility across the two fields. We describe the ReproNLP’24 shared task, summarise results from the reproduction studies submitted, and provide additional comparative analysis of their results.</abstract>
      <url hash="5d207e5d">2024.humeval-1.9</url>
      <bibkey>belz-thomson-2024-2024</bibkey>
    </paper>
    <paper id="10">
      <title>Once Upon a Replication: It is Humans’ Turn to Evaluate <fixed-case>AI</fixed-case>’s Understanding of Children’s Stories for <fixed-case>QA</fixed-case> Generation</title>
      <author><first>Andra-Maria</first><last>Florescu</last></author>
      <author><first>Marius</first><last>Micluta-Campeanu</last></author>
      <author id="liviu-p-dinu"><first>Liviu P.</first><last>Dinu</last></author>
      <pages>106–113</pages>
      <abstract>The following paper presents the outcomes of a collaborative experiment on human evaluation from the ReproNLP 2024 shared task, track B, part of the ReproHum project. For this paper, we evaluated a QAG (question-answer generation) system centered on English children’s storybooks that was presented in a previous research, by using human evaluators for the study. The system generated relevant QA (Question-Answer) pairs based on a dataset with storybooks for early education (kindergarten up to middle school) called FairytaleQA. In the framework of the ReproHum project, we first outline the previous paper and the reproduction strategy that has been decided upon. The complete setup of the first human evaluation is then described, along with the modifications required to replicate it. We also add other relevant related works on this subject. In conclusion, we juxtapose the replication outcomes with those documented in the cited publication. Additionally, we explore the general features of this endeavor as well as its shortcomings.</abstract>
      <url hash="654632fa">2024.humeval-1.10</url>
      <attachment type="OptionalSupplementaryMaterial" hash="7f752ec1">2024.humeval-1.10.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>florescu-etal-2024-upon</bibkey>
    </paper>
    <paper id="11">
      <title>Exploring Reproducibility of Human-Labelled Data for Code-Mixed Sentiment Analysis</title>
      <author><first>Sachin</first><last>Sasidharan Nair</last></author>
      <author><first>Tanvi</first><last>Dinkar</last></author>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <pages>114–124</pages>
      <abstract>Growing awareness of a ‘Reproducibility Crisis’ in natural language processing (NLP) has focused on human evaluations of generative systems. While labelling for supervised classification tasks makes up a large part of human input to systems, the reproduction of such efforts has thus far not been been explored. In this paper, we re-implement a human data collection study for sentiment analysis of code-mixed Malayalam movie reviews, as well as automated classification experiments. We find that missing and under-specified information makes reproduction challenging, and we observe potentially consequential differences between the original labels and those we collect. Classification results indicate that the reliability of the labels is important for stable performance.</abstract>
      <url hash="800c5fff">2024.humeval-1.11</url>
      <bibkey>sasidharan-nair-etal-2024-exploring</bibkey>
    </paper>
    <paper id="12">
      <title>Reproducing the Metric-Based Evaluation of a Set of Controllable Text Generation Techniques</title>
      <author><first>Michela</first><last>Lorandi</last></author>
      <author><first>Anya</first><last>Belz</last></author>
      <pages>125–131</pages>
      <abstract>Rerunning a metric-based evaluation should be more straightforward and results should be closer than in a human-based evaluation, especially where code and model checkpoints are made available by the original authors. As this brief report of our efforts to rerun a metric-based evaluation of a set of multi-aspect controllable text generation (CTG) techniques shows however, such reruns of evaluations do not always produce results that are the same as the original results, and can reveal errors in the orginal work.</abstract>
      <url hash="cae237d0">2024.humeval-1.12</url>
      <bibkey>lorandi-belz-2024-reproducing</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um: #0033-03: How Reproducible Are Fluency Ratings of Generated Text? A Reproduction of <fixed-case>A</fixed-case>ugust et al. 2022</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Anouck</first><last>Braggaar</last></author>
      <author><first>Nadine</first><last>Braun</last></author>
      <author><first>Martijn</first><last>Goudbeek</last></author>
      <author id="emiel-krahmer"><first>Emiel</first><last>Krahmer</last></author>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Steffen</first><last>Pauws</last></author>
      <author><first>Frédéric</first><last>Tomas</last></author>
      <pages>132–144</pages>
      <abstract>In earlier work, August et al. (2022) evaluated three different Natural Language Generation systems on their ability to generate fluent, relevant, and factual scientific definitions. As part of the ReproHum project (Belz et al., 2023), we carried out a partial reproduction study of their human evaluation procedure, focusing on human fluency ratings. Following the standardised ReproHum procedure, our reproduction study follows the original study as closely as possible, with two raters providing 300 ratings each. In addition to this, we carried out a second study where we collected ratings from eight additional raters and analysed the variability of the ratings. We successfully reproduced the inferential statistics from the original study (i.e. the same hypotheses were supported), albeit with a lower inter-annotator agreement. The remainder of our paper shows significant variation between different raters, raising questions about what it really means to reproduce human evaluation studies.</abstract>
      <url hash="b0a4e9d4">2024.humeval-1.13</url>
      <bibkey>van-miltenburg-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0927-03: <fixed-case>DE</fixed-case>xpert Evaluation? Reproducing Human Judgements of the Fluency of Generated Text</title>
      <author><first>Tanvi</first><last>Dinkar</last></author>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>145–152</pages>
      <abstract>ReproHum is a large multi-institution project designed to examine the reproducibility of human evaluations of natural language processing. As part of the second phase of the project, we attempt to reproduce an evaluation of the fluency of continuations generated by a pre-trained language model compared to a range of baselines. Working within the constraints of the project, with limited information about the original study, and without access to their participant pool, or the responses of individual participants, we find that we are not able to reproduce the original results. Our participants display a greater tendency to prefer one of the system responses, avoiding a judgement of ‘equal fluency’ more than in the original study. We also conduct further evaluations: we elicit ratings from (1) a broader range of participants; (2) from the same participants at different times; and (3) with an altered definition of fluency. Results of these experiments suggest that the original evaluation collected too few ratings, and that the task formulation may be quite ambiguous. Overall, although we were able to conduct a re-evaluation study, we conclude that the original evaluation was not comprehensive enough to make truly meaningful comparisons</abstract>
      <url hash="3b6e7cfe">2024.humeval-1.14</url>
      <bibkey>dinkar-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0927-3: Reproducing The Human Evaluation Of The <fixed-case>DE</fixed-case>xperts Controlled Text Generation Method</title>
      <author><first>Javier</first><last>González Corbelle</last></author>
      <author><first>Ainhoa</first><last>Vivel Couso</last></author>
      <author><first>Jose Maria</first><last>Alonso-Moral</last></author>
      <author><first>Alberto</first><last>Bugarín-Diz</last></author>
      <pages>153–162</pages>
      <abstract>This paper presents a reproduction study aimed at reproducing and validating a human NLP evaluation performed for the DExperts text generation method. The original study introduces DExperts, a controlled text generation method, evaluated using non-toxic prompts from the RealToxicityPrompts dataset. Our reproduction study aims to reproduce the human evaluation of the continuations generated by DExperts in comparison with four baseline methods, in terms of toxicity, topicality, and fluency. We first describe the agreed approach for reproduction within the ReproHum project and detail the configuration of the original evaluation, including necessary adaptations for reproduction. Then, we make a comparison of our reproduction results with those reported in the reproduced paper. Interestingly, we observe how the human evaluators in our experiment appreciate higher quality in the texts generated by DExperts in terms of less toxicity and better fluency. All in all, new scores are higher, also for the baseline methods. This study contributes to ongoing efforts in ensuring the reproducibility and reliability of findings in NLP evaluation and emphasizes the critical role of robust methodologies in advancing the field.</abstract>
      <url hash="fc4a070f">2024.humeval-1.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="89a4534c">2024.humeval-1.15.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>gonzalez-corbelle-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #1018-09: Reproducing Human Evaluations of Redundancy Errors in Data-To-Text Systems</title>
      <author><first>Filip</first><last>Klubička</last></author>
      <author id="john-kelleher"><first>John D.</first><last>Kelleher</last></author>
      <pages>163–198</pages>
      <abstract>This paper describes a reproduction of a human evaluation study evaluating redundancies generated in automatically generated text from a data-to-text system. While the scope of the original study is broader, a human evaluation—a manual error analysis—is included as part of the system evaluation. We attempt a reproduction of this human evaluation, however while the authors annotate multiple properties of the generated text, we focus exclusively on a single quality criterion, that of redundancy. In focusing our study on a single minimal reproducible experimental unit, with the experiment being fairly straightforward and all data made available by the authors, we encountered no challenges with our reproduction and were able to reproduce the trend found in the original experiment. However, while still confirming the general trend, we found that both our annotators identified twice as many errors in the dataset than the original authors.</abstract>
      <url hash="0a99acef">2024.humeval-1.16</url>
      <bibkey>klubicka-kelleher-2024-reprohum</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um#0043: Human Evaluation Reproducing Language Model as an Annotator: Exploring Dialogue Summarization on <fixed-case>AMI</fixed-case> Dataset</title>
      <author><first>Vivian</first><last>Fresen</last></author>
      <author><first>Mei-Shin</first><last>Wu-Urbanek</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>199–209</pages>
      <abstract>This study, conducted as part of the ReproHum project, aimed to replicate and evaluate the experiment presented in “Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization” by Feng et al. (2021). By employing DialoGPT, BART, and PGN models, the study assessed dialogue summarization’s informativeness. Based on the ReproHum project’s baselines, we conducted a human evaluation for the AIMI dataset, aiming to compare the results of the original study with our own experiments. Our objective is to contribute to the research on human evaluation and the reproducibility of the original study’s findings in the field of Natural Language Processing (NLP). Through this endeavor, we seek to enhance understanding and establish reliable benchmarks in human evaluation methodologies within the NLP domain.</abstract>
      <url hash="953b0db4">2024.humeval-1.17</url>
      <bibkey>fresen-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0712-01: Human Evaluation Reproduction Report for “Hierarchical Sketch Induction for Paraphrase Generation”</title>
      <author><first>Mohammad</first><last>Arvan</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>210–220</pages>
      <abstract>Human evaluations are indispensable in the development of NLP systems because they provide direct insights into how effectively these systems meet real-world needs and expectations. Ensuring the reproducibility of these evaluations is vital for maintaining credibility in natural language processing research. This paper presents our reproduction of the human evaluation experiments conducted by Hosking et al. (2022) for their paraphrase generation approach. Through careful replication we found that our results closely align with those in the original study, indicating a high degree of reproducibility.</abstract>
      <url hash="4800a84c">2024.humeval-1.18</url>
      <bibkey>arvan-parde-2024-reprohum</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0712-01: Reproducing Human Evaluation of Meaning Preservation in Paraphrase Generation</title>
      <author><first>Lewis N.</first><last>Watson</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <pages>221–228</pages>
      <abstract>Reproducibility is a cornerstone of scientific research, ensuring the reliability and generalisability of findings. The ReproNLP Shared Task on Reproducibility of Evaluations in NLP aims to assess the reproducibility of human evaluation studies. This paper presents a reproduction study of the human evaluation experiment presented in “Hierarchical Sketch Induction for Paraphrase Generation” by Hosking et al. (2022). The original study employed a human evaluation on Amazon Mechanical Turk, assessing the quality of paraphrases generated by their proposed model using three criteria: meaning preservation, fluency, and dissimilarity. In our reproduction study, we focus on the meaning preservation criterion and utilise the Prolific platform for participant recruitment, following the ReproNLP challenge’s common approach to reproduction. We discuss the methodology, results, and implications of our reproduction study, comparing them to the original findings. Our findings contribute to the understanding of reproducibility in NLP research and highlights the potential impact of platform changes and evaluation criteria on the reproducibility of human evaluation studies.</abstract>
      <url hash="fba708e7">2024.humeval-1.19</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fd77e3e6">2024.humeval-1.19.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>watson-gkatzia-2024-reprohum</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0043-4: Evaluating Summarization Models: investigating the impact of education and language proficiency on reproducibility</title>
      <author><first>Mateusz</first><last>Lango</last></author>
      <author><first>Patricia</first><last>Schmidtova</last></author>
      <author><first>Simone</first><last>Balloccu</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <pages>229–237</pages>
      <abstract>In this paper, we describe several reproductions of a human evaluation experiment measuring the quality of automatic dialogue summarization (Feng et al., 2021). We investigate the impact of the annotators’ highest level of education, field of study, and native language on the evaluation of the informativeness of the summary. We find that the evaluation is relatively consistent regardless of these factors, but the biggest impact seems to be a prior specific background in natural language processing (as opposed to, e.g. a background in computer sci- ence). We also find that the experiment setup (asking for single vs. multiple criteria) may have an impact on the results.</abstract>
      <url hash="20085545">2024.humeval-1.20</url>
      <attachment type="OptionalSupplementaryMaterial" hash="06a7a7c5">2024.humeval-1.20.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>lango-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0033-3: Comparable Relative Results with Lower Absolute Values in a Reproduction Study</title>
      <author><first>Yiru</first><last>Li</last></author>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>238–249</pages>
      <abstract>In the context of the ReproHum project aimed at assessing the reliability of human evaluation, we replicated the human evaluation conducted in “Generating Scientific Definitions with Controllable Complexity” by August et al. (2022). Specifically, humans were asked to assess the fluency of automatically generated scientific definitions by three different models, with output complexity varying according to target audience. Evaluation conditions were kept as close as possible to the original study, except of necessary and minor adjustments. Our results, despite yielding lower absolute performance, show that relative performance across the three tested systems remains comparable to what was observed in the original paper. On the basis of lower inter-annotator agreement and feedback received from annotators in our experiment, we also observe that the ambiguity of the concept being evaluated may play a substantial role in human assessment.</abstract>
      <url hash="b3a9654c">2024.humeval-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="309c1870">2024.humeval-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>li-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0124-03: Reproducing Human Evaluations of end-to-end approaches for Referring Expression Generation</title>
      <author><first>Saad</first><last>Mahamood</last></author>
      <pages>250–254</pages>
      <abstract>In this paper we describe our attempt to reproduce a single human evaluation quality criterion of the human evaluation that was in conducted in the paper “NeuralREG: An end-to-end approach to referring expression generation”. In particular, this paper describes the approach and challenges involved in reproducing the human evaluation as done by the original authors of the paper, the results obtained, and what insights we have gained from attempting this particular reproduction. Insights that we hope will enable refinements to both how human evaluations are documented by author(s) and enable better reproductions of NLP experiments in the future.</abstract>
      <url hash="1707e102">2024.humeval-1.22</url>
      <bibkey>mahamood-2024-reprohum</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0087-01: Human Evaluation Reproduction Report for Generating Fact Checking Explanations</title>
      <author><first>Tyler</first><last>Loakman</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <pages>255–260</pages>
      <abstract>This paper describes a partial reproduction of the work titled “Generating Fact Checking Explanations” by Atanasova et al. (2020) as part of the ReproHum element within the ReproNLP shared task, aimed at reproducing findings in NLP research related to human evaluation. The task investigates whether NLP research is becoming more or less reproducible over time. Following instructions from the task organizers and the original authors, we gathered relative rankings for three fact-checking explanations (including a gold standard and outputs from two models) for 40 inputs based on the criterion of Coverage. Our reproduction and reanalysis of the original study’s raw results support the initial findings, showing similar patterns between the original work and our reproduction. Though we observed slight variations from the original results, our findings align with the main conclusions drawn by the original authors regarding the effectiveness of their proposed models.</abstract>
      <url hash="7f2042a3">2024.humeval-1.23</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b17223cc">2024.humeval-1.23.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>loakman-lin-2024-reprohum</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0892-01: The painful route to consistent results: A reproduction study of human evaluation in <fixed-case>NLG</fixed-case></title>
      <author><first>Irene</first><last>Mondella</last></author>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>261–268</pages>
      <abstract>In spite of the core role human judgement plays in evaluating the performance of NLP systems, the way human assessments are elicited in NLP experiments, and to some extent the nature of human judgement itself, pose challenges to the reliability and validity of human evaluation. In the context of the larger ReproHum project, aimed at running large scale multi-lab reproductions of human judgement, we replicated the understandability assessment by humans on several generated outputs of simplified text described in the paper “Neural Text Simplification of Clinical Letters with a Domain Specific Phrase Table” by Shardlow and Nawaz, appeared in the Proceedings of ACL 2019. Although we had to implement a series of modifications compared to the original study, which were necessary to run our human evaluation on exactly the same data, we managed to collect assessments and compare results with the original study. We obtained results consistent with those of the reference study, confirming their findings. The paper is complete with as much information as possible to foster and facilitate future reproduction.</abstract>
      <url hash="99a2cd49">2024.humeval-1.24</url>
      <bibkey>mondella-etal-2024-reprohum</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0087-01: A Reproduction Study of the Human Evaluation of the Coverage of Fact Checking Explanations</title>
      <author><first>Mingqi</first><last>Gao</last></author>
      <author><first>Jie</first><last>Ruan</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>269–273</pages>
      <abstract>We present a reproduction study of the human evaluation of the coverage of fact checking explanations conducted by Atanasova et al. (2020), as a team in Track B of ReproNLP 2024. The setup of our reproduction study is almost the same as the original study, with some necessary modifications to the evaluation guideline and annotation interface. Our reproduction achieves a higher IAA of 0.20 compared to the original study’s 0.12, but discovers a mismatch between the IAA calculated by us with the raw annotation in the original study and the IAA reported in the original paper. Additionally, our reproduction results on the ranks of three types of explanations are drastically different from the original experiment, rendering that one important conclusion in the original paper cannot be confirmed at all. The case study illustrates that the annotators in the reproduction study may understand the quality criterion differently from the annotators in the original study.</abstract>
      <url hash="a7c4ee78">2024.humeval-1.25</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2c4b1d3d">2024.humeval-1.25.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>gao-etal-2024-reprohum</bibkey>
      <revision id="1" href="2024.humeval-1.25v1" hash="9495d902"/>
      <revision id="2" href="2024.humeval-1.25v2" hash="a7c4ee78" date="2024-06-19">This version corrects all content caused by the problematic datasets.</revision>
    </paper>
    <paper id="26">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0866-04: Another Evaluation of Readers’ Reactions to News Headlines</title>
      <author><first>Zola</first><last>Mahlaza</last></author>
      <author><first>Toky Hajatiana</first><last>Raboanary</last></author>
      <author><first>Kyle</first><last>Seakgwa</last></author>
      <author><first>C. Maria</first><last>Keet</last></author>
      <pages>274–280</pages>
      <abstract>The reproduction of Natural Language Processing (NLP) studies is important in establishing their reliability. Nonetheless, many papers in NLP have never been reproduced. This paper presents a reproduction of Gabriel et al. (2022)’s work to establish the extent to which their findings, pertaining to the utility of large language models (T5 and GPT2) to automatically generate writer’s intents when given headlines to curb misinformation, can be confirmed. Our results show no evidence to support two of their four findings and they partially support the rest of the original findings. Specifically, while we confirmed that all the models are judged to be capable of influencing readers’ trust or distrust, there was a difference in T5’s capability to reduce trust. Our results show that its generations are more likely to have greater influence in reducing trust while Gabriel et al. (2022) found more cases where they had no impact at all. In addition, most of the model generations are considered socially acceptable only if we relax the criteria for determining a majority to mean more than chance rather than the apparent &gt; 70% of the original study. Overall, while they found that “machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation”, we found that they are more likely to decrease trust in both cases vs. having no impact at all.</abstract>
      <url hash="2c057d4c">2024.humeval-1.26</url>
      <attachment type="OptionalSupplementaryMaterial" hash="c1182088">2024.humeval-1.26.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>mahlaza-etal-2024-reprohum</bibkey>
    </paper>
  </volume>
</collection>
