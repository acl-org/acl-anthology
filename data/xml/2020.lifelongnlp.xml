<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.lifelongnlp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</booktitle>
      <editor><first>William M.</first><last>Campbell</last></editor>
      <editor><first>Alex</first><last>Waibel</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Timothy J.</first><last>Hazen</last></editor>
      <editor><first>Kevin</first><last>Kilgour</last></editor>
      <editor><first>Eunah</first><last>Cho</last></editor>
      <editor><first>Varun</first><last>Kumar</last></editor>
      <editor><first>Hadrien</first><last>Glaude</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
      <venue>lifelongnlp</venue>
    </meta>
    <frontmatter>
      <url hash="407cd6f4">2020.lifelongnlp-1.0</url>
      <bibkey>lifelongnlp-2020-life</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Deep Active Learning for Sequence Labeling Based on Diversity and Uncertainty in Gradient</title>
      <author><first>Yekyung</first><last>Kim</last></author>
      <pages>1–8</pages>
      <abstract>Recently, several studies have investigated active learning (AL) for natural language processing tasks to alleviate data dependency. However, for query selection, most of these studies mainly rely on uncertainty-based sampling, which generally does not exploit the structural information of the unlabeled data. This leads to a sampling bias in the batch active learning setting, which selects several samples at once. In this work, we demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task. We examined the effects of our sequence-based approach by selecting weighted diverse in the gradient embedding approach across multiple tasks, datasets, models, and consistently outperform classic uncertainty-based sampling and diversity-based sampling.</abstract>
      <url hash="a0229731">2020.lifelongnlp-1.1</url>
      <bibkey>kim-2020-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="2">
      <title>Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting</title>
      <author><first>Christian</first><last>Huber</last></author>
      <author><first>Juan</first><last>Hussain</last></author>
      <author><first>Tuan-Nam</first><last>Nguyen</last></author>
      <author><first>Kaihang</first><last>Song</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>9–17</pages>
      <abstract>When training speech recognition systems, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to-end speech recognition systems that only accept transcribed speech as training data, which is harder and more expensive to obtain than text data. In this paper we present experiments in adapting end-to-end speech recognition systems by a method which is called batch-weighting and which we contrast against regular fine-tuning, i.e., to continue to train existing neural speech recognition models on adaptation data. We perform experiments using theses techniques in adapting to topic, accent and vocabulary, showing that batch-weighting consistently outperforms fine-tuning. In order to show the generalization capabilities of batch-weighting we perform experiments in several languages, i.e., Arabic, English and German. Due to its relatively small computational requirements batch-weighting is a suitable technique for supervised life-long learning during the life-time of a speech recognition system, e.g., from user corrections.</abstract>
      <url hash="e8e4ade1">2020.lifelongnlp-1.2</url>
      <bibkey>huber-etal-2020-supervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="3">
      <title>Data Augmentation using Pre-trained Transformer Models</title>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>Ashutosh</first><last>Choudhary</last></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <pages>18–26</pages>
      <abstract>Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.</abstract>
      <url hash="8aee8bed">2020.lifelongnlp-1.3</url>
      <bibkey>kumar-etal-2020-data</bibkey>
      <pwccode url="https://github.com/varinf/TransformersDataAugmentation" additional="true">varinf/TransformersDataAugmentation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
  </volume>
</collection>
