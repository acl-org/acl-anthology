<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.louhi">
  <volume id="1" ingest-date="2021-04-19" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis</booktitle>
      <editor><first>Eben</first><last>Holderness</last></editor>
      <editor><first>Antonio</first><last>Jimeno Yepes</last></editor>
      <editor><first>Alberto</first><last>Lavelli</last></editor>
      <editor><first>Anne-Lyse</first><last>Minard</last></editor>
      <editor><first>James</first><last>Pustejovsky</last></editor>
      <editor><first>Fabio</first><last>Rinaldi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>online</address>
      <month>April</month>
      <year>2021</year>
      <venue>louhi</venue>
    </meta>
    <frontmatter>
      <url hash="082c1a45">2021.louhi-1.0</url>
      <bibkey>louhi-2021-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>r<fixed-case>C</fixed-case>orona: Analyzing <fixed-case>A</fixed-case>rabic Tweets in the Early Days of Coronavirus (<fixed-case>COVID</fixed-case>-19) Pandemic</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <pages>1–6</pages>
      <abstract>Over the past few months, there were huge numbers of circulating tweets and discussions about Coronavirus (COVID-19) in the Arab region. It is important for policy makers and many people to identify types of shared tweets to better understand public behavior, topics of interest, requests from governments, sources of tweets, etc. It is also crucial to prevent spreading of rumors and misinformation about the virus or bad cures. To this end, we present the largest manually annotated dataset of Arabic tweets related to COVID-19. We describe annotation guidelines, analyze our dataset and build effective machine learning and transformer based models for classification.</abstract>
      <url hash="64f4f767">2021.louhi-1.1</url>
      <bibkey>mubarak-hassan-2021-arcorona</bibkey>
    </paper>
    <paper id="2">
      <title>Multilingual Negation Scope Resolution for Clinical Text</title>
      <author><first>Mareike</first><last>Hartmann</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>7–18</pages>
      <abstract>Negation scope resolution is key to high-quality information extraction from clinical texts, but so far, efforts to make encoders used for information extraction negation-aware have been limited to English. We present a universal approach to multilingual negation scope resolution, that overcomes the lack of training data by relying on disparate resources in different languages and domains. We evaluate two approaches to learn from these resources, training on combined data and training in a multi-task learning setup. Our experiments show that zero-shot scope resolution in clinical text is possible, and that combining available resources improves performance in most cases.</abstract>
      <url hash="6ab0db1a">2021.louhi-1.2</url>
      <bibkey>hartmann-sogaard-2021-multilingual</bibkey>
    </paper>
    <paper id="3">
      <title>Understanding Social Support Expressed in a <fixed-case>COVID</fixed-case>-19 Online Forum</title>
      <author><first>Anietie</first><last>Andy</last></author>
      <author><first>Brian</first><last>Chu</last></author>
      <author><first>Ramie</first><last>Fathy</last></author>
      <author><first>Barrington</first><last>Bennett</last></author>
      <author><first>Daniel</first><last>Stokes</last></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last></author>
      <pages>19–27</pages>
      <abstract>In online forums focused on health and wellbeing, individuals tend to seek and give the following social support: emotional and informational support. Understanding the expressions of these social supports in an online COVID- 19 forum is important for: (a) the forum and its members to provide the right type of support to individuals and (b) determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum: (a) emotional support given (b) emotional support sought (c) informational support given, and (d) informational support sought. Using these models, we aim to: (i) determine if there is a correlation between the different social supports expressed in posts e.g. when members of the forum give emotional support in posts, do they also tend to give or seek informational support in the same post? (ii) determine how these social supports sought and given changes over time in published posts. We find that (i) there is a positive correlation between the informational support given in posts and the emotional support given and emotional support sought, respectively, in these posts and (ii) over time, users tended to seek more emotional support and give less emotional support.</abstract>
      <url hash="61bb3c59">2021.louhi-1.3</url>
      <bibkey>andy-etal-2021-understanding</bibkey>
    </paper>
    <paper id="4">
      <title>Fast and Effective Biomedical Entity Linking Using a Dual Encoder</title>
      <author><first>Rajarshi</first><last>Bhowmik</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>28–37</pages>
      <abstract>Biomedical entity linking is the task of identifying mentions of biomedical concepts in text documents and mapping them to canonical entities in a target thesaurus. Recent advancements in entity linking using BERT-based models follow a <i>retrieve and rerank</i> paradigm, where the candidate entities are first selected using a retriever model, and then the retrieved candidates are ranked by a reranker model. While this paradigm produces state-of-the-art results, they are slow both at training and test time as they can process only one mention at a time. To mitigate these issues, we propose a BERT-based dual encoder model that resolves multiple mentions in a document in one shot. We show that our proposed model is multiple times faster than existing BERT-based models while being competitive in accuracy for biomedical entity linking. Additionally, we modify our dual encoder model for end-to-end biomedical entity linking that performs both mention span detection and entity disambiguation and out-performs two recently proposed models.</abstract>
      <url hash="d4e60d3c">2021.louhi-1.4</url>
      <bibkey>bhowmik-etal-2021-fast</bibkey>
      <pwccode url="https://github.com/kingsaint/BioMedical-EL" additional="false">kingsaint/BioMedical-EL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="5">
      <title>Leveraging knowledge sources for detecting self-reports of particular health issues on social media</title>
      <author><first>Parsa</first><last>Bagherzadeh</last></author>
      <author><first>Sabine</first><last>Bergler</last></author>
      <pages>38–48</pages>
      <abstract>This paper investigates incorporating quality knowledge sources developed by experts for the medical domain as well as syntactic information for classification of tweets into four different health oriented categories. We claim that resources such as the MeSH hierarchy and currently available parse information are effective extensions of moderately sized training datasets for various fine-grained tweet classification tasks of self-reported health issues.</abstract>
      <url hash="6d266274">2021.louhi-1.5</url>
      <bibkey>bagherzadeh-bergler-2021-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="6">
      <title>Integrating Higher-Level Semantics into Robust Biomedical Name Representations</title>
      <author><first>Pieter</first><last>Fivez</last></author>
      <author><first>Simon</first><last>Suster</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>49–58</pages>
      <abstract>Neural encoders of biomedical names are typically considered robust if representations can be effectively exploited for various downstream NLP tasks. To achieve this, encoders need to model domain-specific biomedical semantics while rivaling the universal applicability of pretrained self-supervised representations. Previous work on robust representations has focused on learning low-level distinctions between names of fine-grained biomedical concepts. These fine-grained concepts can also be clustered together to reflect higher-level, more general semantic distinctions, such as grouping the names nettle sting and tick-borne fever together under the description puncture wound of skin. It has not yet been empirically confirmed that training biomedical name encoders on fine-grained distinctions automatically leads to bottom-up encoding of such higher-level semantics. In this paper, we show that this bottom-up effect exists, but that it is still relatively limited. As a solution, we propose a scalable multi-task training regime for biomedical name encoders which can also learn robust representations using only higher-level semantic classes. These representations can generalise both bottom-up as well as top-down among various semantic hierarchies. Moreover, we show how they can be used out-of-the-box for improved unsupervised detection of hypernyms, while retaining robust performance on various semantic relatedness benchmarks.</abstract>
      <url hash="729dc25a">2021.louhi-1.6</url>
      <bibkey>fivez-etal-2021-integrating</bibkey>
      <pwccode url="https://github.com/clips/higherlevelsemantics" additional="false">clips/higherlevelsemantics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/is-a">IS-A</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery">SemEval-2018 Task-9</pwcdataset>
    </paper>
    <paper id="7">
      <title>Classification of mental illnesses on social media using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Ankit</first><last>Murarka</last></author>
      <author><first>Balaji</first><last>Radhakrishnan</last></author>
      <author><first>Sushma</first><last>Ravichandran</last></author>
      <pages>59–68</pages>
      <abstract>Given the current social distancing regulations across the world, social media has become the primary mode of communication for most people. This has isolated millions suffering from mental illnesses who are unable to receive assistance in person. They have increasingly turned to online platforms to express themselves and to look for guidance in dealing with their illnesses. Keeping this in mind, we propose a solution to classify mental illness posts on social media thereby enabling users to seek appropriate help. In this work, we classify five prominent kinds of mental illnesses- depression, anxiety, bipolar disorder, ADHD and PTSD by analyzing unstructured user data on Reddit. In addition, we share a new high-quality dataset1 to drive research on this topic. The dataset consists of the title and post texts from 17159 posts and 13 subreddits each associated with one of the five mental illnesses listed above or a None class indicating the absence of any mental illness. Our model is trained on Reddit data but is easily extensible to other social media platforms as well as demonstrated in our results. We believe that our work is the first multi-class model that uses a Transformer based architecture such as RoBERTa to analyze people’s emotions and psychology. We also demonstrate how we stress test our model using behavioral testing. Our dataset is publicly available and we encourage researchers to utilize this to advance research in this arena. We hope that this work contributes to the public health system by automating some of the detection process and alerting relevant authorities about users that need immediate help.</abstract>
      <url hash="b26fed39">2021.louhi-1.7</url>
      <bibkey>murarka-etal-2021-classification</bibkey>
    </paper>
    <paper id="8">
      <title>Topic Modeling for Maternal Health Using <fixed-case>R</fixed-case>eddit</title>
      <author><first>Shuang</first><last>Gao</last></author>
      <author><first>Shivani</first><last>Pandya</last></author>
      <author><first>Smisha</first><last>Agarwal</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>69–76</pages>
      <abstract>This paper applies topic modeling to understand maternal health topics, concerns, and questions expressed in online communities on social networking sites. We examine Latent Dirichlet Analysis (LDA) and two state-of-the-art methods: neural topic model with knowledge distillation (KD) and Embedded Topic Model (ETM) on maternal health texts collected from Reddit. The models are evaluated on topic quality and topic inference, using both auto-evaluation metrics and human assessment. We analyze a disconnect between automatic metrics and human evaluations. While LDA performs the best overall with the auto-evaluation metrics NPMI and Coherence, Neural Topic Model with Knowledge Distillation is favorable by expert evaluation. We also create a new partially expert annotated gold-standard maternal health topic</abstract>
      <url hash="69a6628c">2021.louhi-1.8</url>
      <bibkey>gao-etal-2021-topic</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>F</fixed-case>uzzy<fixed-case>BIO</fixed-case>: A Proposal for Fuzzy Representation of Discontinuous Entities</title>
      <author><first>Anne</first><last>Dirkson</last></author>
      <author><first>Suzan</first><last>Verberne</last></author>
      <author><first>Wessel</first><last>Kraaij</last></author>
      <pages>77–82</pages>
      <abstract>Discontinuous entities pose a challenge to named entity recognition (NER). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the BIO representation scheme that can handle these entity types are commonly used (i.e. BIOHD). However, the extra tag types make the NER task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous BIO scheme (FuzzyBIO). We focus on the task of Adverse Drug Response extraction and normalization to compare FuzzyBIO to BIOHD. We find that FuzzyBIO improves recall of NER for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using FuzzyBIO also improves end-to-end performance for continuous and composite entities in two of three data sets. Since FuzzyBIO improves performance for some data sets and the conversion from BIOHD to FuzzyBIO is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities.</abstract>
      <url hash="044aa9b1">2021.louhi-1.9</url>
      <bibkey>dirkson-etal-2021-fuzzybio</bibkey>
      <pwccode url="https://github.com/AnneDirkson/FuzzyBIO" additional="false">AnneDirkson/FuzzyBIO</pwccode>
    </paper>
    <paper id="10">
      <title>Cluster Analysis of Online Mental Health Discourse using Topic-Infused Deep Contextualized Representations</title>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Pradnya</first><last>Kulkarni</last></author>
      <author><first>Manisha</first><last>Marathe</last></author>
      <pages>83–93</pages>
      <abstract>With mental health as a problem domain in NLP, the bulk of contemporary literature revolves around building better mental illness prediction models. The research focusing on the identification of discussion clusters in online mental health communities has been relatively limited. Moreover, as the underlying methodologies used in these studies mainly conform to the traditional machine learning models and statistical methods, the scope for introducing contextualized word representations for topic and theme extraction from online mental health communities remains open. Thus, in this research, we propose topic-infused deep contextualized representations, a novel data representation technique that uses autoencoders to combine deep contextual embeddings with topical information, generating robust representations for text clustering. Investigating the Reddit discourse on Post-Traumatic Stress Disorder (PTSD) and Complex Post-Traumatic Stress Disorder (C-PTSD), we elicit the thematic clusters representing the latent topics and themes discussed in the r/ptsd and r/CPTSD subreddits. Furthermore, we also present a qualitative analysis and characterization of each cluster, unraveling the prevalent discourse themes.</abstract>
      <url hash="cd3133c8">2021.louhi-1.10</url>
      <bibkey>kulkarni-etal-2021-cluster</bibkey>
    </paper>
    <paper id="11">
      <title>Scientific Claim Verification with <fixed-case>V</fixed-case>er<fixed-case>T</fixed-case>5erini</title>
      <author><first>Ronak</first><last>Pradeep</last></author>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>94–103</pages>
      <abstract>This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini’s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.</abstract>
      <url hash="daec55f2">2021.louhi-1.11</url>
      <bibkey>pradeep-etal-2021-scientific</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
  </volume>
</collection>
