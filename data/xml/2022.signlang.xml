<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.signlang">
  <volume id="1" ingest-date="2022-09-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the LREC2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources</booktitle>
      <editor><first>Eleni</first><last>Efthimiou</last></editor>
      <editor><first>Stavroula-Evita</first><last>Fotinea</last></editor>
      <editor><first>Thomas</first><last>Hanke</last></editor>
      <editor><first>Julie A.</first><last>Hochgesang</last></editor>
      <editor><first>Jette</first><last>Kristoffersen</last></editor>
      <editor><first>Johanna</first><last>Mesch</last></editor>
      <editor><first>Marc</first><last>Schulder</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="f505b7bf">2022.signlang-1</url>
      <venue>signlang</venue>
    </meta>
    <frontmatter>
      <url hash="fd34fc3a">2022.signlang-1.0</url>
      <bibkey>signlang-2022-lrec2022</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>eru<fixed-case>SIL</fixed-case>: A Framework to Build a Continuous <fixed-case>P</fixed-case>eruvian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Interpretation Dataset</title>
      <author><first>Gissella</first><last>Bejarano</last></author>
      <author><first>Joe</first><last>Huamani-Malca</last></author>
      <author><first>Francisco</first><last>Cerna-Herrera</last></author>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Pablo</first><last>Rivas</last></author>
      <pages>1–8</pages>
      <abstract>Video-based datasets for Continuous Sign Language are scarce due to the challenging task of recording videos from native signers and the reduced number of people who can annotate sign language. COVID-19 has evidenced the key role of sign language interpreters in delivering nationwide health messages to deaf communities. In this paper, we present a framework for creating a multi-modal sign language interpretation dataset based on videos and we use it to create the first dataset for Peruvian Sign Language (LSP) interpretation annotated by hearing volunteers who have intermediate knowledge of PSL guided by the video audio. We rely on hearing people to produce a first version of the annotations, which should be reviewed by native signers in the future. Our contributions: i) we design a framework to annotate a sign Language dataset; ii) we release the first annotated LSP multi-modal interpretation dataset (AEC); iii) we evaluate the annotation done by hearing people by training a sign language recognition model. Our model reaches up to 80.3% of accuracy among a minimum of five classes (signs) AEC dataset, and 52.4% in a second dataset. Nevertheless, analysis by subject in the second dataset show variations worth to discuss.</abstract>
      <url hash="16ee0aee">2022.signlang-1.1</url>
      <bibkey>bejarano-etal-2022-perusil</bibkey>
    </paper>
    <paper id="2">
      <title>Introducing Sign Languages to a Multilingual <fixed-case>W</fixed-case>ordnet: Bootstrapping Corpora and Lexical Resources of <fixed-case>G</fixed-case>reek <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage and <fixed-case>G</fixed-case>erman <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Sam</first><last>Bigeard</last></author>
      <author><first>Marc</first><last>Schulder</last></author>
      <author><first>Maria</first><last>Kopf</last></author>
      <author><first>Thomas</first><last>Hanke</last></author>
      <author><first>Kyriaki</first><last>Vasilaki</last></author>
      <author><first>Anna</first><last>Vacalopoulou</last></author>
      <author><first>Theodore</first><last>Goulas</last></author>
      <author><first>Athanasia-Lida</first><last>Dimou</last></author>
      <author><first>Stavroula-Evita</first><last>Fotinea</last></author>
      <author><first>Eleni</first><last>Efthimiou</last></author>
      <pages>9–15</pages>
      <abstract>Wordnets have been a popular lexical resource type for many years. Their sense-based representation of lexical items and numerous relation structures have been used for a variety of computational and linguistic applications. The inclusion of different wordnets into multilingual wordnet networks has further extended their use into the realm of cross-lingual research. Wordnets have been released for many spoken languages. Research has also been carried out into the creation of wordnets for several sign languages, but none have yet resulted in publicly available datasets. This article presents our own efforts towards an inclusion of sign languages in a multilingual wordnet, starting with Greek Sign Language (GSL) and German Sign Language (DGS). Based on differences in available language resources between GSL and DGS, we trial two workflows with different coverage priorities. We also explore how synergies between both workflows can be leveraged and how future work on additional sign languages could profit from building on existing sign language wordnet data. The results of our work are made publicly available.</abstract>
      <url hash="7d7c12d0">2022.signlang-1.2</url>
      <bibkey>bigeard-etal-2022-introducing</bibkey>
    </paper>
    <paper id="3">
      <title>Introducing the signgloss<fixed-case>R</fixed-case> Package</title>
      <author><first>Carl</first><last>Börstell</last></author>
      <pages>16–23</pages>
      <abstract>The signglossR package is a library written in the programming language R, intended as an easy-to-use resource for those who work with signed language data and are familiar with R. The package contains a variety of functions designed specifically towards signed language research, facilitating a single-pipeline workflow with R when accessing public language resources remotely (online) or a user’s own files and data. The package specifically targets processing of image and video files, but also features some interaction with software commonly used by researchers working on signed language and gesture, such as ELAN and OpenPose. The signglossR package combines features and functionality from many other libraries and tools in order to simplify and collect existing resources in one place, as well as adding some new functionality, and adapt everything to the needs of researchers working with visual language data. In this paper, the main features of this package are introduced.</abstract>
      <url hash="df93b3e3">2022.signlang-1.3</url>
      <bibkey>borstell-2022-introducing</bibkey>
      <pwccode url="https://github.com/borstell/signglossr" additional="false">borstell/signglossr</pwccode>
    </paper>
    <paper id="4">
      <title>Moving towards a Functional Approach in the <fixed-case>F</fixed-case>lemish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Dictionary Making Process</title>
      <author><first>Caro</first><last>Brosens</last></author>
      <author><first>Margot</first><last>Janssens</last></author>
      <author><first>Sam</first><last>Verstraete</last></author>
      <author><first>Thijs</first><last>Vandamme</last></author>
      <author><first>Hannes</first><last>De Durpel</last></author>
      <pages>24–28</pages>
      <abstract>This presentation will outline the dictionary making process of the new online Flemish Sign Language dictionary launched in 2019. First some necessary background information is provided, consisting of a brief history of Flemish Sign Language (VGT) lexicography. Then three phases in the development of the renewed dictionary of VGT will be explored: (i) user research, (ii) data-cleaning and modeling, and (iii) innovations. More than wanting to project a report of lexicographic research on a website, the goal was to make the new dictionary a practical, user-friendly reference tool that meets the needs, expectations, and skills of the dictionary users. To gain a better understanding of who the users were, several sources were consulted: the user research by Joni Oyserman (2013), the quantitative data from Google Analytics and VGTC’s own user profiles. Since 2017, VGTC has been using Signbank, an electronic database specifically developed to compile and manage lexicographic data for sign languages. Bringing together all this raw data inadvertently led to inconsistencies and small mistakes, therefore the data had to be manually revised and complemented. The VGT dictionary was mainly formally modernized, but there are also several substantive differences regarding the previous dictionary: for instance, search options were expanded, and semantic categories were added as well as a new feedback feature. In addition, the new website is also structurally different, it is now responsive to all screen sizes. Lastly, possible future innovations will briefly be discussed. VGTC aims to continuously improve both the user-based interface and the content of the current dictionary. Future goals include, but are not limited to, adding definitions and sample sentences (preferably extracted from the corpus), as well as information on the etymology and common use of signs.</abstract>
      <url hash="3a4c814c">2022.signlang-1.4</url>
      <bibkey>brosens-etal-2022-moving</bibkey>
    </paper>
    <paper id="5">
      <title>Phonetics of Negative Headshake in <fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage: A Small-Scale Corpus Study</title>
      <author><first>Anastasia</first><last>Chizhikova</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <pages>29–36</pages>
      <abstract>We analyzed negative headshake found in the online corpus of Russian Sign Language. We found that negative headshake can co-occur with negative manual signs, although most of these signs are not accompanied by it. We applied OpenFace, a Computer Vision toolkit, to extract head rotation measurements from video recordings, and analyzed the headshake in terms of the number of peaks (turns), the amplitude of the turns, and their frequency. We find that such basic phonetic measurements of headshake can be extracted using a combination of manual annotation and Computer Vision, and can be further used in comparative research across constructions and sign languages.</abstract>
      <url hash="4645c4aa">2022.signlang-1.5</url>
      <bibkey>chizhikova-kimmelman-2022-phonetics</bibkey>
      <pwccode url="https://github.com/nastyachizhikova/negative_headshake_phonetics_rsl" additional="false">nastyachizhikova/negative_headshake_phonetics_rsl</pwccode>
    </paper>
    <paper id="6">
      <title>Documenting the Use of <fixed-case>I</fixed-case>ranian Sign Language (<fixed-case>ZEI</fixed-case>) in Kermanshah</title>
      <author><first>Yassaman</first><last>Choubsaz</last></author>
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Sara</first><last>Siyavoshi</last></author>
      <author><first>Farzaneh</first><last>Soleimanbeigi</last></author>
      <pages>37–41</pages>
      <abstract>We describe a sign language documentation project funded by the Endangered Languages Documentation Project (ELDP) in the province of Kermanshah, a city in west of Iran. The deposit at ELDP archive (elararchive.org) includes recording of 38 native signers of Zaban Eshareh Irani living in Kermanshah. The recordings start with an elicitation of the signs of the Farsi alphabet along with fingerspelling of some words as well as vocabulary elicitation of some basic concepts. Subsequently, the participants are asked to watch short movies and then they are asked to retell the story. Later, the participants have natural conversations in pairs guided by a deaf moderator. Initial annotations of ID-glosses and translations to Persian and English were also archived. ID-glosses are stored as a dataset in Global Signbank, along with a citation form of signs and their phonological description. The resulting datasets and one-hour annotation of the conversations are available to other researchers in ELDP archive.</abstract>
      <url hash="fda30e39">2022.signlang-1.6</url>
      <bibkey>choubsaz-etal-2022-documenting</bibkey>
    </paper>
    <paper id="7">
      <title>Applying the Transcription System Typannot to Mouth Gestures</title>
      <author><first>Claire</first><last>Danet</last></author>
      <author><first>Chloé</first><last>Thomas</last></author>
      <author><first>Adrien</first><last>Contesse</last></author>
      <author><first>Morgane</first><last>Rébulard</last></author>
      <author><first>Claudia S.</first><last>Bianchini</last></author>
      <author><first>Léa</first><last>Chevrefils</last></author>
      <author><first>Patrick</first><last>Doan</last></author>
      <pages>42–47</pages>
      <abstract>Research on sign languages (SLs) requires dedicated, efficient and comprehensive transcription systems to analyze and compare the sign parameters; at present, many transcription systems focus on manual parameters, relegating the non-manual component to a lesser role. This article presents Typannot, a formal transcription system, and in particular its application to mouth gestures: 1) first, exposing its kinesiological approach, i.e. an intrinsic articulatory description anchored in the body; 2) then, showing its conception to integrate linguistic, graphic and technical aspects within a typeface; 3) finally, presenting its application to a corpus in French Sign Language (LSF) recorded with motion capture.</abstract>
      <url hash="f2dc2703">2022.signlang-1.7</url>
      <bibkey>danet-etal-2022-applying</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>L</fixed-case>ibras Portal: A Way of Documentation, a Way of Sharing</title>
      <author><first>Ronice</first><last>de Quadros</last></author>
      <author><first>Renata</first><last>Krusser</last></author>
      <author><first>Daniela</first><last>Saito</last></author>
      <pages>48–52</pages>
      <abstract>Libras Portal is an interface that makes available in one single site a series of elements and tools related to the Brazilian Sign Language (Libras) and comprises Libras documentation which may be employed for research and for educational aims. Libras Portal was developed to codify tools that prop an education network and practice community, making possible the sharing of knowledge, data, and interaction in Libras and Portuguese. It involves accessibility and usability of the web, especially videos in Libras. The latter are access-friendly to available hyperlinks and tools related to communication with the target practice community. The layout also employs visual and textual resources for deaf users. The portal makes available resources for research and the teaching of language, namely Libras Grammar, Libras corpus, Sign Bank, and Literary Anthology of Libras. It is also a store for the sharing of literary, academic, and didactic materials, courses, glossaries, anthologies, lesson models, and grammar analyses. Consequently, tools were developed for the accessibility of deaf people, for easy web browsing, index information, video upload, research, and development of products for communities of deaf people. The current paper will describe the development of research and resources for accessibility.</abstract>
      <url hash="52bf104f">2022.signlang-1.8</url>
      <bibkey>de-quadros-etal-2022-libras</bibkey>
    </paper>
    <paper id="9">
      <title>Representation and Synthesis of Geometric Relocations</title>
      <author><first>Michael</first><last>Filhol</last></author>
      <author><first>John</first><last>McDonald</last></author>
      <pages>53–58</pages>
      <abstract>One of the key features of signed discourse is the geometric placements of gestural units in signing space. Signers use the geometry of signing space to describe the placements and forms of objects and also use it to contrast participants or locales in a story. Depending on the specific functions of the placement in the discourse, features such as geometric precision, gaze redirection and timing will all differ. A signing avatar must capture these differences to sign such discourse naturally. This paper builds on prior work that animated geometric depictions to enable a signing avatar to more naturally use signing space for opposing participants and concepts in discourse. Building from a structured linguistic description of a signed newscast, they system automatically synthesizes animation that correctly utilizes signing space to lay out the opposing locales in the report. The efficacy of the approach is demonstrated through comparisons of the avatar’s motion with the source signing.</abstract>
      <url hash="fd6cc252">2022.signlang-1.9</url>
      <bibkey>filhol-mcdonald-2022-representation</bibkey>
    </paper>
    <paper id="10">
      <title>Sign Language Phonetic Annotator-Analyzer: Open-Source Software for Form-Based Analysis of Sign Languages</title>
      <author><first>Kathleen Currie</first><last>Hall</last></author>
      <author><first>Yurika</first><last>Aonuki</last></author>
      <author><first>Kaili</first><last>Vesik</last></author>
      <author><first>April</first><last>Poy</last></author>
      <author><first>Nico</first><last>Tolmie</last></author>
      <pages>59–66</pages>
      <abstract>This paper provides an introduction to the Sign Language Phonetic Annotator-Analyzer (SLP-AA) software, a free and open-source tool currently under development, for facilitating detailed form-based transcription of signs. The software is designed to have a user-friendly interface that allows coders to transcribe a great deal of phonetic detail without being constrained to a particular phonetic annotation system or phonological framework. Here, we focus on the ‘annotator’ component of the software, outlining the functionality for transcribing movement, location, hand configuration, orientation, and contact, as well as the timing relations between them.</abstract>
      <url hash="eb0c2d3c">2022.signlang-1.10</url>
      <bibkey>hall-etal-2022-sign</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>ASL</fixed-case>-Homework-<fixed-case>RGBD</fixed-case> Dataset: An Annotated Dataset of 45 Fluent and Non-fluent Signers Performing <fixed-case>A</fixed-case>merican <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Homeworks</title>
      <author><first>Saad</first><last>Hassan</last></author>
      <author><first>Matthew</first><last>Seita</last></author>
      <author><first>Larwan</first><last>Berke</last></author>
      <author><first>Yingli</first><last>Tian</last></author>
      <author><first>Elaine</first><last>Gale</last></author>
      <author><first>Sooyeon</first><last>Lee</last></author>
      <author><first>Matt</first><last>Huenerfauth</last></author>
      <pages>67–72</pages>
      <abstract>We are releasing a dataset containing videos of both fluent and non-fluent signers using American Sign Language (ASL), which were collected using a Kinect v2 sensor. This dataset was collected as a part of a project to develop and evaluate computer vision algorithms to support new technologies for automatic detection of ASL fluency attributes. A total of 45 fluent and non-fluent participants were asked to perform signing homework assignments that are similar to the assignments used in introductory or intermediate level ASL courses. The data is annotated to identify several aspects of signing including grammatical features and non-manual markers. Sign language recognition is currently very data-driven and this dataset can support the design of recognition technologies, especially technologies that can benefit ASL learners. This dataset might also be interesting to ASL education researchers who want to contrast fluent and non-fluent signing.</abstract>
      <url hash="fb8493f2">2022.signlang-1.11</url>
      <bibkey>hassan-etal-2022-asl</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-asl">MS-ASL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wlasl">WLASL</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>MY</fixed-case> <fixed-case>DGS</fixed-case> – <fixed-case>ANNIS</fixed-case>: <fixed-case>ANNIS</fixed-case> and the <fixed-case>P</fixed-case>ublic <fixed-case>DGS</fixed-case> <fixed-case>C</fixed-case>orpus</title>
      <author><first>Amy</first><last>Isard</last></author>
      <author><first>Reiner</first><last>Konrad</last></author>
      <pages>73–79</pages>
      <abstract>In 2018 the DGS-Korpus project published the first full release of the Public DGS Corpus. The data have already been published in two different ways to fulfil the needs of different user groups, and we have now published the third portal MY DGS – ANNIS using the ANNIS browser-based corpus software. ANNIS is a corpus query tool for visualization and querying of multi-layer corpus data. It has its own query language, AQL, and is accessed from a web browser without requiring a login. It allows more complex queries and visualizations than those provided by the existing research portal. We introduce ANNIS and its query language AQL, describe the structure of MY DGS – ANNIS, and give some example queries. The use cases with queries over multiple annotation tiers and metadata illustrate the research potential of this powerful tool and show how students and researchers can explore the Public DGS Corpus.</abstract>
      <url hash="b645e947">2022.signlang-1.12</url>
      <bibkey>isard-konrad-2022-dgs</bibkey>
    </paper>
    <paper id="13">
      <title>Outreach and Science Communication in the <fixed-case>DGS</fixed-case>-Korpus Project: Accessibility of Data and the Benefit of Interactive Exchange between Communities</title>
      <author><first>Elena</first><last>Jahn</last></author>
      <author><first>Calvin</first><last>Khan</last></author>
      <author><first>Annika</first><last>Herrmann</last></author>
      <pages>80–87</pages>
      <abstract>In this paper, we tackle the issues of science communication and dissemination within a sign language corpus project with a focus on spreading accessible information and involving the D/deaf community on various levels. We will discuss successful examples, challenges, and limitations to public relations in such a project and particularly elaborate on use cases. The focus group is presented as a best-practice example of a what we think is a necessary perspective: taking external knowledge seriously and let community experts interact with and provide feedback on a par with academic personnel. Showing both social media and on-site events, we present some exemplary approaches from our team involved in public relations. Keywords: public relations, science communication, sign language community, DGS-Korpus project</abstract>
      <url hash="9d92d8e5">2022.signlang-1.13</url>
      <bibkey>jahn-etal-2022-outreach</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>MC</fixed-case>-<fixed-case>TRISLAN</fixed-case>: A Large 3<fixed-case>D</fixed-case> Motion Capture Sign Language Data-set</title>
      <author><first>Pavel</first><last>Jedlička</last></author>
      <author><first>Zdeněk</first><last>Krňoul</last></author>
      <author><first>Milos</first><last>Zelezny</last></author>
      <author><first>Ludek</first><last>Muller</last></author>
      <pages>88–93</pages>
      <abstract>The new 3D motion capture data corpus expands the portfolio of existing language resources by a corpus of 18 hours of Czech sign language. This helps to alleviate the current problem, which is a critical lack of high quality data necessary for research and subsequent deployment of machine learning techniques in this area. We currently provide the largest collection of annotated sign language recordings acquired by state-of-the-art 3D human body recording technology for the successful future deployment in communication technologies, especially machine translation and sign language synthesis.</abstract>
      <url hash="f601756c">2022.signlang-1.14</url>
      <bibkey>jedlicka-etal-2022-mc</bibkey>
    </paper>
    <paper id="15">
      <title>A Machine Learning-based Segmentation Approach for Measuring Similarity between Sign Languages</title>
      <author><first>Tonni Das</first><last>Jui</last></author>
      <author><first>Gissella</first><last>Bejarano</last></author>
      <author><first>Pablo</first><last>Rivas</last></author>
      <pages>94–101</pages>
      <abstract>Due to the lack of more variate, native and continuous datasets, sign languages are low-resources languages that can benefit from multilingualism in machine translation. In order to analyze the benefits of approaches like multilingualism, finding the similarity between sign languages can guide better matches and contributions between languages. However, calculating the similarity between sign languages again implies a laborious work to measure how close or distant signs are and their respective contexts. For that reason, we propose to support the similarity measurement between sign languages through a video-segmentation-based machine learning model that will quantify this match among signs of different countries’ sign languages. Using a machine learning approach the similarity measurement process can run more smoothly, compared to a more manual approach. We use a pre-trained temporal segmentation model for British Sign Language (BSL). We test it on three datasets, an American Sign Language (ASL) dataset, an Indian Sign Language (ISL), and an Australian Sign Language (AUSLAN) dataset. We hypothesize that the percentage of segmented and recognized signs by this machine learning model can represent the percentage of overlap or similarity between British and the other three sign languages. In our ongoing work, we evaluate three metrics considering Swadesh’s and Woodward’s list and their synonyms. We found that our intermediate-strict metric coincides with a more classical analysis of the similarity between British and American Sign Language, as well as with the classical low measurement between Indian and British sign languages. On the other hand, our similarity measurement between British and Australian Sign language just holds for part of the Australian Sign Language and not the whole data sample.</abstract>
      <url hash="c6767fd9">2022.signlang-1.15</url>
      <bibkey>jui-etal-2022-machine</bibkey>
    </paper>
    <paper id="16">
      <title>The Sign Language Dataset Compendium: Creating an Overview of Digital Linguistic Resources</title>
      <author><first>Maria</first><last>Kopf</last></author>
      <author><first>Marc</first><last>Schulder</last></author>
      <author><first>Thomas</first><last>Hanke</last></author>
      <pages>102–109</pages>
      <abstract>One of the challenges that sign language researchers face is the identification of suitable language datasets, particularly for cross-lingual studies. There is no single source of information on what sign language corpora and lexical resources exist or how they compare. Instead, they have to be found through extensive literature review or word-of-mouth. The amount of information available on individual datasets can also vary widely and may be distributed across different publications, data repositories and (potentially defunct) project websites. This article introduces the Sign Language Dataset Compendium, an extensive overview of linguistic resources for sign languages. It covers existing corpora and lexical resources, as well as commonly used data collection tasks. Special attention is paid to covering resources for many different languages from around the globe. All information is provided in a standardised format to make entries comparable, but kept flexible enough to allow for differences in content. The compendium is intended as a growing resource that will be updated regularly.</abstract>
      <url hash="f7d0fd66">2022.signlang-1.16</url>
      <bibkey>kopf-etal-2022-sign</bibkey>
    </paper>
    <paper id="17">
      <title>Making Sign Language Corpora Comparable: A Study of Palm-Up and Throw-Away in <fixed-case>P</fixed-case>olish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage, <fixed-case>G</fixed-case>erman <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage, and <fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Anna</first><last>Kuder</last></author>
      <pages>110–117</pages>
      <abstract>This paper is primarily devoted to describing the preparation phase of a large-scale comparative study based on naturalistic linguistic data drawn from multiple sign language corpora. To provide an example, I am using my current project on manual gestural elements in Polish Sign Language, German Sign Language, and Russian Sign Language. The paper starts with a description of the reasons behind undertaking this project. Then, I describe the scope of my study, which is focused on two manual elements present in all three mentioned sign languages: palm-up and throw-away; and the three corpora which are my data sources. This is followed by a presentation of the steps taken in the initial stages of the project in order to make the data comparable. Those steps are: choosing the adequate data samples from all three corpora, gathering all data within the chosen software, and creating an annotation schema that builds on the annotations already present in all three corpora. Even though the project is still underway, and the annotation process is ongoing, preliminary discussions about the nature of the analysed manual activities are presented based on the initial annotations for the sake of evaluating the created annotation schema. I conclude the paper with some remarks about the performance of the employed methodology.</abstract>
      <url hash="35203796">2022.signlang-1.17</url>
      <bibkey>kuder-2022-making</bibkey>
    </paper>
    <paper id="18">
      <title>Open Repository of the <fixed-case>P</fixed-case>olish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Corpus: Publication Project of the <fixed-case>P</fixed-case>olish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Corpus</title>
      <author><first>Anna</first><last>Kuder</last></author>
      <author><first>Joanna</first><last>Wójcicka</last></author>
      <author><first>Piotr</first><last>Mostowski</last></author>
      <author><first>Paweł</first><last>Rutkowski</last></author>
      <pages>118–123</pages>
      <abstract>Between 2010 and 2020, the research team of the Section for Sign Linguistics collected, annotated, and translated a large corpus of Polish Sign Language (polski język migowy, PJM). After this task was finished, a substantial part of the gathered materials was published online as the Open Repository of the Polish Sign Language Corpus. The current paper gives an overview of the process of converting the material from the Corpus into the Repository. If presents and explains the decisions made along the way and describes the process of data preparation and publication. There are two levels of access to the Repository, which are meant to fulfil the needs of a wide range of public users, from members of the Deaf community, through hearing students of PJM, sign language teachers and interpreters, to users with academic background. We describe how corpus material available in open access was prepared to be searchable by text type and elicitation tasks, by sociolinguistic metadata, and by translation into written Polish. We go on to explain how access for research purposes differs from open access. We present possible ways in which data gathered in the Repository may be used by members of the signing community in Poland and abroad.</abstract>
      <url hash="fc3ba9fd">2022.signlang-1.18</url>
      <bibkey>kuder-etal-2022-open</bibkey>
    </paper>
    <paper id="19">
      <title>Functional Data Analysis of Non-manual Marking of Questions in <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Anna</first><last>Kuznetsova</last></author>
      <author><first>Alfarabi</first><last>Imashev</last></author>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <pages>124–131</pages>
      <abstract>This paper is a continuation of Kuznetsova et al. (2021), which described non-manual markers of polar and wh-questions in comparison with statements in an NLP dataset of Kazakh-Russian Sign Language (KRSL) using Computer Vision. One of the limitations of the previous work was the distortion of the 3D face landmarks when the head was rotated. The proposed solution was to train a simple linear regression model to predict the distortion and then subtract it from the original output. We improve this technique with a multilayer perceptron. Another limitation that we intend to address in this paper is the discrete analysis of the continuous movement of non-manuals. In Kuznetsova et al. (2021) we averaged the value of the non-manual over its scope for statistical analysis. To preserve information on the shape of the movement, in this study we use a statistical tool that is often used in speech research, Functional Data Analysis, specifically Functional PCA.</abstract>
      <url hash="c988c226">2022.signlang-1.19</url>
      <bibkey>kuznetsova-etal-2022-functional</bibkey>
      <pwccode url="https://github.com/kuzanna2016/non-manuals-2021" additional="false">kuzanna2016/non-manuals-2021</pwccode>
    </paper>
    <paper id="20">
      <title>Two New <fixed-case>AZ</fixed-case>ee Production Rules Refining Multiplicity in <fixed-case>F</fixed-case>rench <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Emmanuella</first><last>Martinod</last></author>
      <author><first>Claire</first><last>Danet</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>132–138</pages>
      <abstract>This paper is a contribution to sign language (SL) modeling. We focus on the hitherto imprecise notion of “Multiplicity”, assumed to express plurality in French Sign Language (LSF), using AZee approach. AZee is a linguistic and formal approach to modeling LSF. It takes into account the linguistic properties and specificities of LSF while respecting constraints linked to a modeling process. We present the methodology to extract AZee production rules. Based on the analysis of strong form-meaning associations in SL data (elicited image descriptions and short news), we identified two production rules structuring the expression of multiplicity in LSF. We explain how these newly extracted production rules are different from existing ones. Our goal is to refine the AZee approach to allow the coverage of a growing part of LSF. This work could lead to an improvement in SL synthesis and SL automatic translation.</abstract>
      <url hash="3015d9a7">2022.signlang-1.20</url>
      <bibkey>martinod-etal-2022-two</bibkey>
    </paper>
    <paper id="21">
      <title>Language Planning in Action: Depiction as a Driver of New Terminology in <fixed-case>I</fixed-case>rish <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Rachel</first><last>Moiselle</last></author>
      <author><first>Lorraine</first><last>Leeson</last></author>
      <pages>139–143</pages>
      <abstract>In this paper, we examine the linguistic phenomenon known as ‘depiction’, which relates to the ability to visually represent semantic components (Dudis, 2004). While some elements of this have been described for Irish Sign Language, with particular attention to the ‘productive lexicon’ (Leeson &amp; Grehan, 2004; Leeson &amp; Saeed, 2012; Matthews, 1996; O’Baoill &amp; Matthews, 2000), here, we take the analysis further, drawing on what we have learned from cognitive linguistics over the past decade. Drawing on several recently developed domain-specific glossaries (e.g., STEM1, Covid-192, political domain, Sexual, Domestic and Gender Based Violence (SDGBV)-related vocabulary) we present ongoing analysis indicating that a deliberate focus on iconicity, in particular, elements of depiction, appears to be a primary driver. We also consider the potential implications of the insights we intend to gain from Deaf-led glossary glossary development work in the context of Machine Translation goals, for example, for work in progress on the Horizon 2020 funded SignON project.</abstract>
      <url hash="03c2d4a7">2022.signlang-1.21</url>
      <bibkey>moiselle-leeson-2022-language</bibkey>
    </paper>
    <paper id="22">
      <title>Facilitating the Spread of New Sign Language Technologies across <fixed-case>E</fixed-case>urope</title>
      <author><first>Hope</first><last>Morgan</last></author>
      <author><first>Onno</first><last>Crasborn</last></author>
      <author><first>Maria</first><last>Kopf</last></author>
      <author><first>Marc</first><last>Schulder</last></author>
      <author><first>Thomas</first><last>Hanke</last></author>
      <pages>144–147</pages>
      <abstract>For developing sign language technologies like automatic translation, huge amounts of training data are required. Even the larger corpora available for some sign languages are tiny compared to the amounts of data used for corresponding spoken language technologies. The overarching goal of the European project EASIER is to develop a framework for bidirectional automatic translation between sign and spoken languages and between sign languages. One part of this multi-dimensional project is that it will pool available language resources from European sign languages into a larger dataset to address the data scarcity problem. This approach promises to open the floor for lower-resourced sign languages in Europe. This article focusses on efforts in the EASIER project to allow for new languages to make use of such technologies in the future. What are the characteristics of sign language resources needed to train recognition, translation, and synthesis algorithms, and how can other countries including those without any sign resources follow along with these developments? The efforts undertaken in EASIER include creating workflow documents and organizing training sessions in online workshops. They reflect the current state of the art, and will likely need to be updated in the coming decade.</abstract>
      <url hash="8a09e910">2022.signlang-1.22</url>
      <bibkey>morgan-etal-2022-facilitating</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>ISL</fixed-case>-<fixed-case>LEX</fixed-case> v.1: An Online Lexical Resource of <fixed-case>I</fixed-case>sraeli <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Hope</first><last>Morgan</last></author>
      <author><first>Wendy</first><last>Sandler</last></author>
      <author><first>Rose</first><last>Stamp</last></author>
      <author><first>Rama</first><last>Novogrodsky</last></author>
      <pages>148–153</pages>
      <abstract>This paper describes a new online lexical resource and interactive tool for Israeli Sign Language, ISL-LEX v.1. The dataset contains 961 non-compound ISL signs with the following information: subjective frequency ratings from native signers, iconicity ratings from native and non-native signers (presented separately), and phonological properties in six domains. The selection of signs was also designed to reflect a broad distinction between those signs acquired early in childhood and those acquired later. ISL-LEX is an online interface built using the SIGN-LEX visualization (Caselli et al. 2022), and is intended for use by researchers, educators, and students. It is therefore offered in two text-based versions, English and Hebrew, with video instructions in ISL.</abstract>
      <url hash="fab379af">2022.signlang-1.23</url>
      <bibkey>morgan-etal-2022-isl</bibkey>
    </paper>
    <paper id="24">
      <title>Towards Large Vocabulary <fixed-case>K</fixed-case>azakh-<fixed-case>R</fixed-case>ussian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Dataset: <fixed-case>KRSL</fixed-case>-<fixed-case>O</fixed-case>nline<fixed-case>S</fixed-case>chool</title>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Aigerim</first><last>Kydyrbekova</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>154–158</pages>
      <abstract>This paper presents a new dataset for Kazakh-Russian Sign Language (KRSL) created for the purposes of Sign Language Processing. In 2020, Kazakhstan’s schools were quickly switched to online mode due to the COVID-19 pandemic. Every working day, the El-arna TV channel was broadcasting video lessons for grades from 1 to 11 with sign language translation. This opportunity allowed us to record a corpus with a large vocabulary and spontaneous SL interpretation. To this end, this corpus contains video recordings of Kazakhstan’s online school translated to Kazakh-Russian sign language by 7 interpreters. At the moment we collected and cleaned 890 hours of video material. A custom annotation tool was created to make the process of data annotation simple and easy-to-use by the Deaf community. To date, around 325 hours of videos have been annotated with glosses and 4,009 lessons out of 4,547 were transcribed with automatic speech-to-text software. The KRSL-OnlineSchool dataset will be made publicly available at <url>https://krslproject.github.io/online-school/</url></abstract>
      <url hash="bf3020d9">2022.signlang-1.24</url>
      <bibkey>mukushev-etal-2022-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
    </paper>
    <paper id="25">
      <title>Towards Semi-automatic Sign Language Annotation Tool: <fixed-case>SLAN</fixed-case>-tool</title>
      <author><first>Medet</first><last>Mukushev</last></author>
      <author><first>Arman</first><last>Sabyrov</last></author>
      <author><first>Madina</first><last>Sultanova</last></author>
      <author><first>Vadim</first><last>Kimmelman</last></author>
      <author><first>Anara</first><last>Sandygulova</last></author>
      <pages>159–164</pages>
      <abstract>This paper presents a semi-automatic annotation tool for sign languages namely SLAN-tool. The SLAN-tool provides a web-based service for the annotation of sign language videos. Researchers can use the SLAN-tool web service to annotate new and existing sign language datasets with different types of annotations, such as gloss, handshape configurations, and signing regions. This is allowed using a custom tier adding functionality. A unique feature of the tool is its automatic annotation functionality which uses several neural network models in order to recognize signing segments from videos and classify handshapes according to HamNoSys handshape inventory. Furthermore, SLAN-tool users can export annotations and import them into ELAN. The SLAN-tool is publicly available at <url>https://slan-tool.com</url>.</abstract>
      <url hash="0dcc9c3c">2022.signlang-1.25</url>
      <bibkey>mukushev-etal-2022-towards-semi</bibkey>
    </paper>
    <paper id="26">
      <title>Resources for Computer-Based Sign Recognition from Video, and the Criticality of Consistency of Gloss Labeling across Multiple Large <fixed-case>ASL</fixed-case> Video Corpora</title>
      <author><first>Carol</first><last>Neidle</last></author>
      <author><first>Augustine</first><last>Opoku</last></author>
      <author><first>Carey</first><last>Ballard</last></author>
      <author><first>Konstantinos M.</first><last>Dafnis</last></author>
      <author><first>Evgenia</first><last>Chroni</last></author>
      <author><first>Dimitri</first><last>Metaxas</last></author>
      <pages>165–172</pages>
      <abstract>The WLASL purports to be “the largest video dataset for Word-Level American Sign Language (ASL) recognition.” It brings together various publicly shared video collections that could be quite valuable for sign recognition research, and it has been used extensively for such research. However, a critical problem with the accompanying annotations has heretofore not been recognized by the authors, nor by those who have exploited these data: There is no 1-1 correspondence between sign productions and gloss labels. Here we describe a large (and recently expanded and enhanced), linguistically annotated, downloadable, video corpus of citation-form ASL signs shared by the American Sign Language Linguistic Research Project (ASLLRP)—with 23,452 sign tokens and an online Sign Bank—in which such correspondences are enforced. We furthermore provide annotations for 19,672 of the WLASL video examples consistent with ASLLRP glossing conventions. For those wishing to use WLASL videos, this provides a set of annotations that makes it possible: (1) to use those data reliably for computational research; and/or (2) to combine the WLASL and ASLLRP datasets, creating a combined resource that is larger and richer than either of those datasets individually, with consistent gloss labeling for all signs. We also offer a summary of our own sign recognition research to date that exploits these data resources.</abstract>
      <url hash="1b7078e4">2022.signlang-1.26</url>
      <bibkey>neidle-etal-2022-resources</bibkey>
    </paper>
    <paper id="27">
      <title>Signed Language Transcription and the Creation of a Cross-linguistic Comparative Database</title>
      <author><first>Justin</first><last>Power</last></author>
      <author><first>David</first><last>Quinto-Pozos</last></author>
      <author><first>Danny</first><last>Law</last></author>
      <pages>173–180</pages>
      <abstract>As the availability of signed language data has rapidly increased, sign scholars have been confronted with the challenge of creating a common framework for the cross-linguistic comparison of the phonological forms of signs. While transcription techniques have played a fundamental role in the creation of cross-linguistic comparative databases for spoken languages, transcription has featured much less prominently in sign research and lexicography. Here we report the experiences of the Sign Change project in using the signed language transcription system HamNoSys to create a comparative database of basic vocabulary for thirteen signed languages. We report the results of a small-scale study, in which we measured (i) the average time required for two trained transcribers to complete a transcription and (ii) the similarity of their independently produced transcriptions. We find that, across the two transcribers, the transcription of one sign required, on average, one minute and a half. We also find that the similarity of transcriptions differed across phonological parameters. We consider the implications of our findings about transcription time and transcription similarity for other projects that plan to incorporate transcription techniques.</abstract>
      <url hash="46ee2924">2022.signlang-1.27</url>
      <bibkey>power-etal-2022-signed</bibkey>
    </paper>
    <paper id="28">
      <title>Integrating <fixed-case>A</fixed-case>uslan Resources into the Language Data Commons of <fixed-case>A</fixed-case>ustralia</title>
      <author><first>River Tae</first><last>Smith</last></author>
      <author><first>Louisa</first><last>Willoughby</last></author>
      <author><first>Trevor</first><last>Johnston</last></author>
      <pages>181–186</pages>
      <abstract>This paper describes a project to secure Auslan (Australian Sign Language) resources within a national language data network called the Language Data Commons of Australia (LDaCA). The resources are Auslan Signbank, a web-based multi-media dictionary, and the Auslan Corpus, a collection of video recordings of the language being used in various contexts with time-aligned ELAN annotation files. We aim to make these resources accessible to the language community, encourage community participation in the curation of the data, and facilitate and extend their uses in language teaching and linguistic research. The software platforms of both resources will be made compatible with other LDaCA resources; and the two will also be aggregated and linked so that (i) users of the dictionary can view attested corpus examples for an entry; and (ii) users of the corpus can instantly view the dictionary entry for an already glossed sign to check phonological, lexical and grammatical information about it, and/or to ensure that the correct annotation gloss (aka ‘ID-gloss’) for a sign token has been chosen. This will enhance additions to annotations in the Auslan Corpus, entries in Auslan Signbank and the integrity of research based on both.</abstract>
      <url hash="89de0462">2022.signlang-1.28</url>
      <bibkey>smith-etal-2022-integrating</bibkey>
    </paper>
    <paper id="29">
      <title>Capturing Distalization</title>
      <author><first>Rose</first><last>Stamp</last></author>
      <author><first>Lilyana</first><last>Khatib</last></author>
      <author><first>Hagit</first><last>Hel-Or</last></author>
      <pages>187–191</pages>
      <abstract>Coding and analyzing large amounts of video data is a challenge for sign language researchers, who traditionally code 2D video data manually. In recent years, the implementation of 3D motion capture technology as a means of automatically tracking movement in sign language data has been an important step forward. Several studies show that motion capture technologies can measure sign language movement parameters – such as volume, speed, variance – with high accuracy and objectivity. In this paper, using motion capture technology and machine learning, we attempt to automatically measure a more complex feature in sign language known as distalization. In general, distalized signs use the joints further from the torso (such as the wrist), however, the measure is relative and therefore distalization is not straightforward to measure. The development of a reliable and automatic measure of distalization using motion tracking technology is of special interest in many fields of sign language research.</abstract>
      <url hash="8b4f10d8">2022.signlang-1.29</url>
      <bibkey>stamp-etal-2022-capturing</bibkey>
    </paper>
    <paper id="30">
      <title>The Corpus of <fixed-case>I</fixed-case>sraeli <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage</title>
      <author><first>Rose</first><last>Stamp</last></author>
      <author><first>Ora</first><last>Ohanin</last></author>
      <author><first>Sara</first><last>Lanesman</last></author>
      <pages>192–197</pages>
      <abstract>The Corpus of Israeli Sign Language is a four-year project (2020-2024) which aims to create a digital open-access corpus of spontaneous and elicited data from a representative sample of the Israeli deaf community. In this paper, the methodology for building the Corpus of Israeli Sign Language is described. Israeli Sign Language (ISL) is the main sign language used across Israel by around 10,000 people. As part of the corpus, data will be collected from 120 deaf ISL signers across four sites in Israel: Tel Aviv and the Centre, Haifa and the North, Be’er Sheva and the South and Jerusalem and the surrounding area. Participants will engage in a variety of tasks, eliciting a range of signing styles from free conversation to lexical elicitation. The dataset will consist of recordings of over 360 hours of video data which will be used to conduct sociolinguistic investigations of language contact, variation, and change in the near term, and other linguistic analyses in the future.</abstract>
      <url hash="ebfc90e1">2022.signlang-1.30</url>
      <bibkey>stamp-etal-2022-corpus</bibkey>
    </paper>
    <paper id="31">
      <title>Segmentation of Signs for Research Purposes: Comparing Humans and Machines</title>
      <author><first>Bencie</first><last>Woll</last></author>
      <author><first>Neil</first><last>Fox</last></author>
      <author><first>Kearsy</first><last>Cormier</last></author>
      <pages>198–201</pages>
      <abstract>Sign languages such as British Sign Language (BSL) are visual languages which lack standard writing systems. Annotation of sign language data, especially for the purposes of machine readability, is therefore extremely slow. Tools to help automate and thus speed up the annotation process are very much needed. Here we test the development of one such tool (VIA-SLA), which uses temporal convolutional networks (Renz et al., 2021a, b) for the purpose of segmenting continuous signing in any sign language, and is designed to integrate smoothly with ELAN, the widely used annotation software for analysis of videos of sign language. We compare automatic segmentation by machine with segmentation done by a human, both in terms of time needed and accuracy of segmentation, using samples taken from the BSL Corpus (Schembri et al., 2014). A small sample of four short video files is tested (mean duration 25 seconds). We find that mean accuracy in terms of number and location of segmentations is relatively high, at around 78%. This preliminary test suggests that VIA-SLA promises to be very useful for sign linguists.</abstract>
      <url hash="0ca63f6c">2022.signlang-1.31</url>
      <bibkey>woll-etal-2022-segmentation</bibkey>
    </paper>
    <paper id="32">
      <title>Sign Language Video Anonymization</title>
      <author><first>Zhaoyang</first><last>Xia</last></author>
      <author><first>Yuxiao</first><last>Chen</last></author>
      <author><first>Qilong</first><last>Zhangli</last></author>
      <author><first>Matt</first><last>Huenerfauth</last></author>
      <author><first>Carol</first><last>Neidle</last></author>
      <author><first>Dimitri</first><last>Metaxas</last></author>
      <pages>202–211</pages>
      <abstract>Deaf signers who wish to communicate in their native language frequently share videos on the Web. However, videos cannot preserve privacy—as is often desirable for discussion of sensitive topics—since both hands and face convey critical linguistic information and therefore cannot be obscured without degrading communication. Deaf signers have expressed interest in video anonymization that would preserve linguistic content. However, attempts to develop such technology have thus far shown limited success. We are developing a new method for such anonymization, with input from ASL signers. We modify a motion-based image animation model to generate high-resolution videos with the signer identity changed, but with the preservation of linguistically significant motions and facial expressions. An asymmetric encoder-decoder structured image generator is used to generate the high-resolution target frame from the low-resolution source frame based on the optical flow and confidence map. We explicitly guide the model to attain a clear generation of hands and faces by using bounding boxes to improve the loss computation. FID and KID scores are used for the evaluation of the realism of the generated frames. This technology shows great potential for practical applications to benefit deaf signers.</abstract>
      <url hash="bbd5527c">2022.signlang-1.32</url>
      <bibkey>xia-etal-2022-sign</bibkey>
    </paper>
  </volume>
</collection>
