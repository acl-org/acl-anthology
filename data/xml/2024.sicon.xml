<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.sicon">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Social Influence in Conversations (SICon 2024)</booktitle>
      <editor><first>James</first><last>Hale</last></editor>
      <editor><first>Kushal</first><last>Chawla</last></editor>
      <editor><first>Muskan</first><last>Garg</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, Florida, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="a8143e6b">2024.sicon-1</url>
      <venue>sicon</venue>
    </meta>
    <frontmatter>
      <url hash="28d42c57">2024.sicon-1.0</url>
      <bibkey>sicon-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Observing the <fixed-case>S</fixed-case>outhern <fixed-case>US</fixed-case> Culture of Honor Using Large-Scale Social Media Analysis</title>
      <author><first>Juho</first><last>Kim</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Michael</first><last>Guerzhoy</last><affiliation>University of Toronto</affiliation></author>
      <pages>1-8</pages>
      <abstract>A culture of honor refers to a social system where individuals’ status, reputation, and esteem play a central role in governing interpersonal relations. Past works have associated this concept with the United States (US) South and related with it various traits such as higher sensitivity to insult, a higher value on reputation, and a tendency to react violently to insults. In this paper, we hypothesize and confirm that internet users from the US South, where a culture of honor is more prevalent, are more likely to display a trait predicted by their belonging to a culture of honor. Specifically, we test the hypothesis that US Southerners are more likely to retaliate to personal attacks by personally attacking back. We leverage OpenAI’s GPT-3.5 API to both geolocate internet users and to automatically detect whether users are insulting each other. We validate the use of GPT-3.5 by measuring its performance on manually-labeled subsets of the data. Our work demonstrates the potential of formulating a hypothesis based on a conceptual framework, operationalizing it in a way that is amenable to large-scale LLM-aided analysis, manually validating the use of the LLM, and drawing a conclusion.</abstract>
      <url hash="db8a9919">2024.sicon-1.1</url>
      <bibkey>kim-guerzhoy-2024-observing</bibkey>
    </paper>
    <paper id="2">
      <title>Should We Respect <fixed-case>LLM</fixed-case>s? A Cross-Lingual Study on the Influence of Prompt Politeness on <fixed-case>LLM</fixed-case> Performance</title>
      <author><first>Ziqi</first><last>Yin</last><affiliation>Waseda University</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Waseda University</affiliation></author>
      <author><first>Kaito</first><last>Horio</last><affiliation>Waseda University</affiliation></author>
      <author><first>Daisuike</first><last>Kawahara</last><affiliation>Waseda University</affiliation></author>
      <author><first>Satoshi</first><last>Sekine</last><affiliation>RIKEN AIP, NII LLMC</affiliation></author>
      <pages>9-35</pages>
      <abstract>We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.</abstract>
      <url hash="5deb24e2">2024.sicon-1.2</url>
      <bibkey>yin-etal-2024-respect</bibkey>
    </paper>
    <paper id="3">
      <title>Personality Differences Drive Conversational Dynamics: A High-Dimensional <fixed-case>NLP</fixed-case> Approach</title>
      <author><first>Julia R.</first><last>Fisher</last><affiliation>Stanford University</affiliation></author>
      <author><first>Nilam</first><last>Ram</last><affiliation>Stanford University</affiliation></author>
      <pages>36-45</pages>
      <abstract>This paper investigates how the topical flow of dyadic conversations emerges over time and how differences in interlocutors’ personality traits contribute to this topical flow. Leveraging text embeddings, we map the trajectories of conversations between strangers into a high-dimensional space. Using nonlinear projections and clustering, we then identify when each interlocutor enters and exits various topics. Differences in conversational flow are quantified via , a summary measure of the “spread” of topics covered during a conversation, and , a time-varying measure of the cosine similarity between interlocutors’ embeddings. Our findings suggest that interlocutors with a larger difference in the personality dimension of openness influence each other to spend more time discussing a wider range of topics and that interlocutors with a larger difference in extraversion experience a larger decrease in linguistic alignment throughout their conversation. We also examine how participants’ affect (emotion) changes from before to after a conversation, finding that a larger difference in extraversion predicts a larger difference in affect change and that a greater topic entropy predicts a larger affect increase. This work demonstrates how communication research can be advanced through the use of high-dimensional NLP methods and identifies personality difference as an important driver of social influence.</abstract>
      <url hash="e63faa80">2024.sicon-1.3</url>
      <bibkey>fisher-ram-2024-personality</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>R</fixed-case>ecom<fixed-case>M</fixed-case>ind: Movie Recommendation Dialogue with Seeker’s Internal State</title>
      <author><first>Takashi</first><last>Kodama</last><affiliation>Research and Development Center for LLMs, National Institute of Informatics</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>Research and Development Center for LLMs, National Institute of Informatics</affiliation></author>
      <author><first>Yin Jou</first><last>Huang</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Research and Development Center for LLMs, National Institute of Informatics, Kyoto University</affiliation></author>
      <pages>46-63</pages>
      <abstract>Humans pay careful attention to the interlocutor’s internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker’s internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis and experiment, we constructed RecomMind, a movie recommendation dialogue dataset with annotations of the seeker’s internal state at the entity level. Each entity has a first-person label annotated by the seeker and a second-person label annotated by the recommender. Our analysis based on RecomMind reveals that the success of recommendations is enhanced when recommenders mention entities that seekers do not know but are interested in. We also propose a response generation framework that explicitly considers the seeker’s internal state, utilizing the chain-of-thought prompting. The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations.</abstract>
      <url hash="4faa7e5f">2024.sicon-1.4</url>
      <bibkey>kodama-etal-2024-recommind</bibkey>
    </paper>
    <paper id="5">
      <title>Redefining Proactivity for Information Seeking Dialogue</title>
      <author><first>Jing Yang</first><last>Lee</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Seokhwan</first><last>Kim</last><affiliation>Google Cloud AI</affiliation></author>
      <author><first>Kartik</first><last>Mehta</last><affiliation>Amazon AGI</affiliation></author>
      <author><first>Jiun-Yu</first><last>Kao</last><affiliation>Amazon AGI</affiliation></author>
      <author><first>Yu-Hsiang</first><last>Lin</last><affiliation>Amazon AGI, Meta</affiliation></author>
      <author><first>Arpit</first><last>Gupta</last><affiliation>Amazon AGI</affiliation></author>
      <pages>64-84</pages>
      <abstract>Humans pay careful attention to the interlocutor’s internal state in dialogues. For example, in recommendation dialogues, we make recommendations while estimating the seeker’s internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis and experiment, we constructed RecomMind, a movie recommendation dialogue dataset with annotations of the seeker’s internal state at the entity level. Each entity has a first-person label annotated by the seeker and a second-person label annotated by the recommender. Our analysis based on RecomMind reveals that the success of recommendations is enhanced when recommenders mention entities that seekers do not know but are interested in. We also propose a response generation framework that explicitly considers the seeker’s internal state, utilizing the chain-of-thought prompting. The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of recommendations.</abstract>
      <url hash="528a1116">2024.sicon-1.5</url>
      <bibkey>lee-etal-2024-redefining</bibkey>
    </paper>
    <paper id="6">
      <title>Leveraging Large Language Models for Code-Mixed Data Augmentation in Sentiment Analysis</title>
      <author><first>Linda</first><last>Zeng</last><affiliation>The Harker School</affiliation></author>
      <pages>85-101</pages>
      <abstract>Code-mixing (CM), where speakers blend languages within a single expression, is prevalent in multilingual societies but poses challenges for natural language processing due to its complexity and limited data. We propose using a large language model to generate synthetic CM data, which is then used to enhance the performance of task-specific models for CM sentiment analysis. Our results show that in Spanish-English, synthetic data improved the F1 score by 9.32%, outperforming previous augmentation techniques. However, in Malayalam-English, synthetic data only helped when the baseline was low; with strong natural data, additional synthetic data offered little benefit. Human evaluation confirmed that this approach is a simple, cost-effective way to generate natural-sounding CM sentences, particularly beneficial for low baselines. Our findings suggest that few-shot prompting of large language models is a promising method for CM data augmentation and has significant impact on improving sentiment analysis, an important element in the development of social influence systems.</abstract>
      <url hash="d86534d0">2024.sicon-1.6</url>
      <bibkey>zeng-2024-leveraging</bibkey>
    </paper>
    <paper id="7">
      <title>Balancing Transparency and Accuracy: A Comparative Analysis of Rule-Based and Deep Learning Models in Political Bias Classification</title>
      <author><first>Manuel Nunez</first><last>Martinez</last><affiliation>University of Florida</affiliation></author>
      <author><first>Sonja</first><last>Schmer-Galunder</last><affiliation>University of Florida</affiliation></author>
      <author><first>Zoey</first><last>Liu</last><affiliation>University of Florida</affiliation></author>
      <author><first>Sangpil</first><last>Youm</last><affiliation>University of Florida</affiliation></author>
      <author><first>Chathuri</first><last>Jayaweera</last><affiliation>University of Florida</affiliation></author>
      <author><first>Bonnie J.</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>102-115</pages>
      <abstract>The unchecked spread of digital information, combined with increasing political polarization and the tendency of individuals to isolate themselves from opposing political viewpoints opposing views, has driven researchers to develop systems for automatically detecting political bias in media. This trend has been further fueled by discussions on social media. We explore methods for categorizing bias in US news articles, comparing rule-based and deep learning approaches. The study highlights the sensitivity of modern self-learning systems to unconstrained data ingestion, while reconsidering the strengths of traditional rule-based systems. Applying both models to left-leaning (CNN) and right-leaning (FOX) News articles, we assess their effectiveness on data beyond the original training and test sets. This analysis highlights each model’s accuracy, offers a framework for exploring deep-learning explainability, and sheds light on political bias in US news media. We contrast the opaque architecture of a deep learning model with the transparency of a linguistically informed rule-based model, showing that the rule-based model performs consistently across different data conditions and offers greater transparency, whereas the deep learning model is dependent on the training set and struggles with unseen data.</abstract>
      <url hash="46566c92">2024.sicon-1.7</url>
      <bibkey>martinez-etal-2024-balancing</bibkey>
    </paper>
    <paper id="8">
      <title>”So, are you a different person today?” Analyzing Bias in Questions during Parole Hearings</title>
      <author><first>Wassiliki</first><last>Siskou</last><affiliation>Cluster of Excellence ”The Politics of Inequality”, University of Konstanz, University of Passau</affiliation></author>
      <author><first>Ingrid</first><last>Espinoza</last><affiliation>Cluster of Excellence ”The Politics of Inequality”, University of Konstanz</affiliation></author>
      <pages>116-128</pages>
      <abstract>During Parole Suitability Hearings commissioners need to evaluate whether an inmate’s risk of reoffending has decreased sufficiently to justify their release from prison before completing their full sentence. The conversation between the commissioners and the inmate is the key element of such hearings and is largely driven by question-and-answer patterns which can be influenced by the commissioner’s questioning behavior. To our knowledge, no previous study has investigated the relationship between the types of questions asked during parole hearings and potentially biased outcomes. We address this gap by analysing commissioner’s questioning behavior during Californian parole hearings. We test ChatGPT-4o’s capability of annotating questions automatically and achieve a high F1-score of 0.91 without prior training. By analysing all questions posed directly by commissioners to inmates, we tested for potential biases in question types across multiple demographic variables. The results show minimal bias in questioning behavior toward inmates asking for parole.</abstract>
      <url hash="4a61d936">2024.sicon-1.8</url>
      <bibkey>siskou-espinoza-2024-different</bibkey>
    </paper>
    <paper id="9">
      <title>Principles for <fixed-case>AI</fixed-case>-Assisted Social Influence and Their Application to Social Mediation</title>
      <author><first>Ian</first><last>Perera</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Alex</first><last>Memory</last><affiliation>Johns Hopkins University Applied Physics Laboratory</affiliation></author>
      <author><first>Vera A.</first><last>Kazakova</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Bonnie J.</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <author><first>Brodie</first><last>Mather</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Ritwik</first><last>Bose</last><affiliation>Johns Hopkins University Applied Physics Laboratory</affiliation></author>
      <author><first>Arash</first><last>Mahyari</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Corey</first><last>Lofdahl</last><affiliation>Leidos, Inc.</affiliation></author>
      <author><first>Mack S.</first><last>Blackburn</last><affiliation>Leidos, Inc.</affiliation></author>
      <author><first>Archna</first><last>Bhatia</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Brandon</first><last>Patterson</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <author><first>Peter</first><last>Pirolli</last><affiliation>Florida Institute for Human and Machine Cognition</affiliation></author>
      <pages>129-140</pages>
      <abstract>Successful social influence, whether at individual or community levels, requires expertise and care in several dimensions of communication: understanding of emotions, beliefs, and values; transparency; and context-aware behavior shaping. Based on our experience in identifying mediation needs in social media and engaging with moderators and users, we developed a set of principles that we believe social influence systems should adhere to to ensure ethical operation, effectiveness, widespread adoption, and trust by users on both sides of the engagement of influence. We demonstrate these principles in D-ESC: Dialogue Assistant for Engaging in Social-Cybermediation, in the context of AI-assisted social media mediation, a newer paradigm of automatic moderation that responds to unique and changing communities while engendering and maintaining trust in users, moderators, and platform-holders. Through this case study, we identify opportunities for our principles to guide future systems towards greater opportunities for positive social change.</abstract>
      <url hash="3380e150">2024.sicon-1.9</url>
      <bibkey>perera-etal-2024-principles</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>EHDC</fixed-case>hat: A Knowledge-Grounded, Empathy-Enhanced Language Model for Healthcare Interactions</title>
      <author><first>Shenghan</first><last>Wu</last><affiliation>Institute of Data Science, National University of Singapore</affiliation></author>
      <author><first>Wynne</first><last>Hsu</last><affiliation>Institute of Data Science, National University of Singapore</affiliation></author>
      <author><first>Mong Li</first><last>Lee</last><affiliation>Institute of Data Science, National University of Singapore</affiliation></author>
      <pages>141-151</pages>
      <abstract>Large Language Models (LLMs) excel at a range of tasks but often struggle with issues like hallucination and inadequate empathy support. To address hallucinations, we ground our dialogues in medical knowledge sourced from external repositories such as Disease Ontology and DrugBank. To improve empathy support, we develop the Empathetic Healthcare Dialogues dataset, which utilizes multiple dialogue strategies in each response. This dataset is then used to fine-tune an LLM, and we introduce a lightweight, adaptable method called Strategy Combination Guidance to enhance the emotional support capabilities of the fine-tuned model, named EHDChat. Our evaluations show that EHDChat significantly outperforms existing models in providing emotional support and medical accuracy, demonstrating the effectiveness of our approach in enhancing empathetic and informed AI interactions in healthcare.</abstract>
      <url hash="3e5cb928">2024.sicon-1.10</url>
      <bibkey>wu-etal-2024-ehdchat</bibkey>
    </paper>
    <paper id="11">
      <title>Domain-Expanded <fixed-case>ASTE</fixed-case>: Rethinking Generalization in Aspect Sentiment Triplet Extraction</title>
      <author><first>Yew Ken</first><last>Chia</last><affiliation>Singapore University of Technology and Design, DAMO Academy, Alibaba Group, Singapore</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Guizhen</first><last>Chen</last><affiliation>DAMO Academy, Alibaba Group, Nanyang Technological University</affiliation></author>
      <author><first>Wei</first><last>Han</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Sharifah Mahani</first><last>Aljunied</last><affiliation>DAMO Academy, Alibaba Group, Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>DAMO Academy, Alibaba Group, Singapore</affiliation></author>
      <pages>152-165</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is a challenging task in sentiment analysis, aiming to provide fine-grained insights into human sentiments. However, existing benchmarks are limited to two domains and do not evaluate model performance on unseen domains, raising concerns about the generalization of proposed methods. Furthermore, it remains unclear if large language models (LLMs) can effectively handle complex sentiment tasks like ASTE. In this work, we address the issue of generalization in ASTE from both a benchmarking and modeling perspective. We introduce a domain-expanded benchmark by annotating samples from diverse domains, enabling evaluation of models in both in-domain and out-of-domain settings. Additionally, we propose CASE, a simple and effective decoding strategy that enhances trustworthiness and performance of LLMs in ASTE. Through comprehensive experiments involving multiple tasks, settings, and models, we demonstrate that CASE can serve as a general decoding strategy for complex sentiment tasks. By expanding the scope of evaluation and providing a more reliable decoding strategy, we aim to inspire the research community to reevaluate the generalizability of benchmarks and models for ASTE. Our code, data, and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste.</abstract>
      <url hash="c763be83">2024.sicon-1.11</url>
      <bibkey>chia-etal-2024-domain</bibkey>
    </paper>
  </volume>
</collection>
