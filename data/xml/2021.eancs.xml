<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.eancs">
  <volume id="1" ingest-date="2021-10-28" type="proceedings">
    <meta>
      <booktitle>The First Workshop on Evaluations and Assessments of Neural Conversation Systems</booktitle>
      <editor><first>Wei</first><last>Wei</last></editor>
      <editor><first>Bo</first><last>Dai</last></editor>
      <editor><first>Tuo</first><last>Zhao</last></editor>
      <editor><first>Lihong</first><last>Li</last></editor>
      <editor><first>Diyi</first><last>Yang</last></editor>
      <editor><first>Yun-Nung</first><last>Chen</last></editor>
      <editor><first>Y-Lan</first><last>Boureau</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Alborz</first><last>Geramifard</last></editor>
      <editor><first>Aman</first><last>Ahuja</last></editor>
      <editor><first>Haoming</first><last>Jiang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
      <venue>eancs</venue>
    </meta>
    <frontmatter>
      <url hash="d5425705">2021.eancs-1.0</url>
      <bibkey>eancs-2021-evaluations</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Counterfactual Matters: Intrinsic Probing For Dialogue State Tracking</title>
      <author><first>Yi</first><last>Huang</last></author>
      <author><first>Junlan</first><last>Feng</last></author>
      <author><first>Xiaoting</first><last>Wu</last></author>
      <author><first>Xiaoyu</first><last>Du</last></author>
      <pages>1–6</pages>
      <abstract>A Dialogue State Tracker (DST) is a core component of modular task-oriented dialogue systems. Tremendous research progress has been made in past ten years to improve performance of DSTs especially on benchmark datasets. However, their generalization to novel and realistic scenarios beyond the held-out conversations is limited. In this paper, we design experimental studies to answer: 1) How does the distribution of dialogue data affect the performance of DSTs? 2) What are effective ways to probe counterfactual matter for DSTs? Our findings are: the performance variance of generative DSTs is not only due to the model structure itself, but can be attributed to the distribution of cross-domain values. Evaluating iconic generative DST models on MultiWOZ dataset with counterfactuals results in a significant performance drop of up to 34.64% (from 50.91% to 16.27%) in absolute joint goal accuracy. It is believed that our experimental results can guide the future work to better understand the intrinsic core of DST and rethink the suitable way for specific tasks given the application property.</abstract>
      <url hash="c8ad8443">2021.eancs-1.1</url>
      <bibkey>huang-etal-2021-counterfactual</bibkey>
      <doi>10.18653/v1/2021.eancs-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>GCDF</fixed-case>1: A Goal- and Context- Driven <fixed-case>F</fixed-case>-Score for Evaluating User Models</title>
      <author><first>Alexandru</first><last>Coca</last></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>7–14</pages>
      <abstract>The evaluation of dialogue systems in interaction with simulated users has been proposed to improve turn-level, corpus-based metrics which can only evaluate test cases encountered in a corpus and cannot measure system’s ability to sustain multi-turn interactions. Recently, little emphasis was put on automatically assessing the quality of the user model itself, so unless correlations with human studies are measured, the reliability of user model based evaluation is unknown. We propose GCDF1, a simple but effective measure of the quality of semantic-level conversations between a goal-driven user agent and a system agent. In contrast with previous approaches we measure the F-score at dialogue level and consider user and system behaviours to improve recall and precision estimation. We facilitate scores interpretation by providing a rich hierarchical structure with information about conversational patterns present in the test data and tools to efficiently query the conversations generated. We apply our framework to assess the performance and weaknesses of a Convlab2 user model.</abstract>
      <url hash="d8c83537">2021.eancs-1.2</url>
      <bibkey>coca-etal-2021-gcdf1</bibkey>
      <doi>10.18653/v1/2021.eancs-1.2</doi>
      <pwccode url="https://github.com/alexcoca/gcdf1" additional="false">alexcoca/gcdf1</pwccode>
    </paper>
    <paper id="3">
      <title>A Comprehensive Assessment of Dialog Evaluation Metrics</title>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <author><first>Shikib</first><last>Mehri</last></author>
      <pages>15–33</pages>
      <abstract>Automatic evaluation metrics are a crucial component of dialog systems research. Standard language evaluation metrics are known to be ineffective for evaluating dialog. As such, recent research has proposed a number of novel, dialog-specific metrics that correlate better with human judgements. Due to the fast pace of research, many of these metrics have been assessed on different datasets and there has as yet been no time for a systematic comparison between them. To this end, this paper provides a comprehensive assessment of recently proposed dialog evaluation metrics on a number of datasets. In this paper, 23 different automatic evaluation metrics are evaluated on 10 different datasets. Furthermore, the metrics are assessed in different settings, to better qualify their respective strengths and weaknesses. This comprehensive assessment offers several takeaways pertaining to dialog evaluation metrics in general. It also suggests how to best assess evaluation metrics and indicates promising directions for future work.</abstract>
      <url hash="7a150afa">2021.eancs-1.3</url>
      <bibkey>yeh-etal-2021-comprehensive</bibkey>
      <doi>10.18653/v1/2021.eancs-1.3</doi>
      <pwccode url="https://github.com/exe1023/DialEvalMetrics" additional="false">exe1023/DialEvalMetrics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog-1">DailyDialog++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fed">FED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-personachat">USR-PersonaChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-topicalchat">USR-TopicalChat</pwcdataset>
    </paper>
  </volume>
</collection>
