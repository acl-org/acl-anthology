<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.wassa">
  <volume id="1" ingest-date="2021-04-19" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
      <editor><first>Orphee</first><last>De Clercq</last></editor>
      <editor><first>Alexandra</first><last>Balahur</last></editor>
      <editor><first>Joao</first><last>Sedoc</last></editor>
      <editor><first>Valentin</first><last>Barriere</last></editor>
      <editor><first>Shabnam</first><last>Tafreshi</last></editor>
      <editor><first>Sven</first><last>Buechel</last></editor>
      <editor><first>Veronique</first><last>Hoste</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <venue>wassa</venue>
    </meta>
    <frontmatter>
      <url hash="142fa187">2021.wassa-1.0</url>
      <bibkey>wassa-2021-approaches</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ox<fixed-case>CCI</fixed-case>n: Toxic Content Classification with Interpretability</title>
      <author><first>Tong</first><last>Xiang</last></author>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Eugene</first><last>Yang</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <pages>1–12</pages>
      <abstract>Despite the recent successes of transformer-based models in terms of effectiveness on a variety of tasks, their decisions often remain opaque to humans. Explanations are particularly important for tasks like offensive language or toxicity detection on social media because a manual appeal process is often in place to dispute automatically flagged content. In this work, we propose a technique to improve the interpretability of these models, based on a simple and powerful assumption: a post is at least as toxic as its most toxic span. We incorporate this assumption into transformer models by scoring a post based on the maximum toxicity of its spans and augmenting the training process to identify correct spans. We find this approach effective and can produce explanations that exceed the quality of those provided by Logistic Regression analysis (often regarded as a highly-interpretable model), according to a human study.</abstract>
      <url hash="a448ab4a">2021.wassa-1.1</url>
      <bibkey>xiang-etal-2021-toxccin</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="2">
      <title>Language that Captivates the Audience: Predicting Affective Ratings of <fixed-case>TED</fixed-case> Talks in a Multi-Label Classification Task</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <pages>13–24</pages>
      <abstract>The aim of the paper is twofold: (1) to automatically predict the ratings assigned by viewers to 14 categories available for TED talks in a multi-label classification task and (2) to determine what types of features drive classification accuracy for each of the categories. The focus is on features of language usage from five groups pertaining to syntactic complexity, lexical richness, register-based n-gram measures, information-theoretic measures and LIWC-style measures. We show that a Recurrent Neural Network classifier trained exclusively on within-text distributions of such features can reach relatively high levels of overall accuracy (69%) across the 14 categories. We find that features from two groups are strong predictors of the affective ratings across all categories and that there are distinct patterns of language usage for each rating category.</abstract>
      <url hash="a76b7aa8">2021.wassa-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="fc99377f">2021.wassa-1.2.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>kerz-etal-2021-language</bibkey>
    </paper>
    <paper id="3">
      <title>Partisanship and Fear are Associated with Resistance to <fixed-case>COVID</fixed-case>-19 Directives</title>
      <author><first>Mike</first><last>Lindow</last></author>
      <author><first>David</first><last>DeFranza</last></author>
      <author><first>Arul</first><last>Mishra</last></author>
      <author><first>Himanshu</first><last>Mishra</last></author>
      <pages>25–33</pages>
      <abstract>Ideological differences have had a large impact on individual and community response to the COVID-19 pandemic in the United States. Early behavioral research during the pandemic showed that conservatives were less likely to adhere to health directives, which contradicts a body of work suggesting that conservative ideology emphasizes a rule abiding, loss aversion, and prevention focus. We reconcile this contradiction by analyzing semantic content of local press releases, federal press releases, and localized tweets during the first month of the government response to COVID-19 in the United States. Controlling for factors such as COVID-19 confirmed cases and deaths, local economic indicators, and more, we find that online expressions of fear in conservative areas lead to an increase in adherence to public health recommendations concerning COVID-19, and that expressions of fear in government press releases are a significant predictor of expressed fear on Twitter.</abstract>
      <url hash="a00d1ca5">2021.wassa-1.3</url>
      <bibkey>lindow-etal-2021-partisanship</bibkey>
    </paper>
    <paper id="4">
      <title>Explainable Detection of Sarcasm in Social Media</title>
      <author><first>Ramya</first><last>Akula</last></author>
      <author><first>Ivan</first><last>Garibay</last></author>
      <pages>34–39</pages>
      <abstract>Sarcasm is a linguistic expression often used to communicate the opposite of what is said, usually something that is very unpleasant with an intention to insult or ridicule. Inherent ambiguity in sarcastic expressions makes sarcasm detection very difficult. In this work, we focus on detecting sarcasm in textual conversations, written in English, from various social networking platforms and online media. To this end, we develop an interpretable deep learning model using multi-head self-attention and gated recurrent units. We show the effectiveness and interpretability of our approach by achieving state-of-the-art results on datasets from social networking platforms, online discussion forums, and political dialogues.</abstract>
      <url hash="bf205924">2021.wassa-1.4</url>
      <bibkey>akula-garibay-2021-explainable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sarcasm-corpus-v2">Sarcasm Corpus V2</pwcdataset>
    </paper>
    <paper id="5">
      <title>Emotion Ratings: How Intensity, Annotation Confidence and Agreements are Entangled</title>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>40–49</pages>
      <abstract>When humans judge the affective content of texts, they also implicitly assess the correctness of such judgment, that is, their confidence. We hypothesize that people’s (in)confidence that they performed well in an annotation task leads to (dis)agreements among each other. If this is true, confidence may serve as a diagnostic tool for systematic differences in annotations. To probe our assumption, we conduct a study on a subset of the Corpus of Contemporary American English, in which we ask raters to distinguish neutral sentences from emotion-bearing ones, while scoring the confidence of their answers. Confidence turns out to approximate inter-annotator disagreements. Further, we find that confidence is correlated to emotion intensity: perceiving stronger affect in text prompts annotators to more certain classification performances. This insight is relevant for modelling studies of intensity, as it opens the question wether automatic regressors or classifiers actually predict intensity, or rather human’s self-perceived confidence.</abstract>
      <url hash="69836997">2021.wassa-1.5</url>
      <bibkey>troiano-etal-2021-emotion</bibkey>
    </paper>
    <paper id="6">
      <title>Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing</title>
      <author><first>Erenay</first><last>Dayanik</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>50–61</pages>
      <abstract>Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up “wrong” features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes.</abstract>
      <url hash="e5197f67">2021.wassa-1.6</url>
      <bibkey>dayanik-pado-2021-disentangling</bibkey>
    </paper>
    <paper id="7">
      <title>Universal Joy A Data Set and Results for Classifying Emotions Across Languages</title>
      <author><first>Sotiris</first><last>Lamprinidis</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Daniel</first><last>Hardt</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>62–75</pages>
      <abstract>While emotions are universal aspects of human psychology, they are expressed differently across different languages and cultures. We introduce a new data set of over 530k anonymized public Facebook posts across 18 languages, labeled with five different emotions. Using multilingual BERT embeddings, we show that emotions can be reliably inferred both within and across languages. Zero-shot learning produces promising results for low-resource languages. Following established theories of basic emotions, we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. We find that structural and typological similarity between languages facilitates cross-lingual learning, as well as linguistic diversity of training data. Our results suggest that there are commonalities underlying the expression of emotion in different languages. We publicly release the anonymized data for future research.</abstract>
      <url hash="7bbb2112">2021.wassa-1.7</url>
      <bibkey>lamprinidis-etal-2021-universal</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>FEEL</fixed-case>-<fixed-case>IT</fixed-case>: Emotion and Sentiment Classification for the <fixed-case>I</fixed-case>talian Language</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>76–83</pages>
      <abstract>While sentiment analysis is a popular task to understand people’s reactions online, we often need more nuanced information: is the post negative because the user is angry or sad? An abundance of approaches have been introduced for tackling these tasks, also for Italian, but they all treat only one of the tasks. We introduce FEEL-IT, a novel benchmark corpus of Italian Twitter posts annotated with four basic emotions: <i>anger</i>, <i>fear</i>, <i>joy</i>, <i>sadness</i>. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification, obtaining competitive results. We release an open-source Python library, so researchers can use a model trained on FEEL-IT for inferring both sentiments and emotions from Italian text.</abstract>
      <url hash="033a14cf">2021.wassa-1.8</url>
      <bibkey>bianchi-etal-2021-feel</bibkey>
      <pwccode url="https://github.com/milanlproc/feel-it" additional="false">milanlproc/feel-it</pwccode>
    </paper>
    <paper id="9">
      <title>An End-to-End Network for Emotion-Cause Pair Extraction</title>
      <author><first>Aaditya</first><last>Singh</last></author>
      <author><first>Shreeshail</first><last>Hingane</last></author>
      <author><first>Saim</first><last>Wani</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>84–91</pages>
      <abstract>The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all potential clause-pairs of emotions and their corresponding causes in a document. Unlike the more well-studied task of Emotion Cause Extraction (ECE), ECPE does not require the emotion clauses to be provided as annotations. Previous works on ECPE have either followed a multi-stage approach where emotion extraction, cause extraction, and pairing are done independently or use complex architectures to resolve its limitations. In this paper, we propose an end-to-end model for the ECPE task. Due to the unavailability of an English language ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline for the ECPE task on this dataset. On this dataset, the proposed method produces significant performance improvements (∼ 6.5% increase in F1 score) over the multi-stage approach and achieves comparable performance to the state-of-the-art methods.</abstract>
      <url hash="9c5ec302">2021.wassa-1.9</url>
      <bibkey>singh-etal-2021-end</bibkey>
      <pwccode url="https://github.com/Aaditya-Singh/E2E-ECPE" additional="false">Aaditya-Singh/E2E-ECPE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ece">ECE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecpe-xia-and-ding-2019">Xia and Ding, 2019</pwcdataset>
    </paper>
    <paper id="10">
      <title><fixed-case>WASSA</fixed-case> 2021 Shared Task: Predicting Empathy and Emotion in Reaction to News Stories</title>
      <author><first>Shabnam</first><last>Tafreshi</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Valentin</first><last>Barriere</last></author>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>Alexandra</first><last>Balahur</last></author>
      <pages>92–104</pages>
      <abstract>This paper presents the results that were obtained from the WASSA 2021 shared task on predicting empathy and emotions. The participants were given access to a dataset comprising empathic reactions to news stories where harm is done to a person, group, or other. These reactions consist of essays, Batson empathic concern, and personal distress scores, and the dataset was further extended with news articles, person-level demographic information (age, gender, ethnicity, income, education level), and personality information. Additionally, emotion labels, namely Ekman’s six basic emotions, were added to the essays at both the document and sentence level. Participation was encouraged in two tracks: predicting empathy and predicting emotion categories. In total five teams participated in the shared task. We summarize the methods and resources used by the participating teams.</abstract>
      <url hash="1cffca44">2021.wassa-1.10</url>
      <bibkey>tafreshi-etal-2021-wassa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
    </paper>
    <paper id="11">
      <title><fixed-case>PVG</fixed-case> at <fixed-case>WASSA</fixed-case> 2021: A Multi-Input, Multi-Task, Transformer-Based Architecture for Empathy and Distress Prediction</title>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Sunanda</first><last>Somwase</last></author>
      <author><first>Shivam</first><last>Rajput</last></author>
      <author><first>Manisha</first><last>Marathe</last></author>
      <pages>105–111</pages>
      <abstract>Active research pertaining to the affective phenomenon of empathy and distress is invaluable for improving human-machine interaction. Predicting intensities of such complex emotions from textual data is difficult, as these constructs are deeply rooted in the psychological theory. Consequently, for better prediction, it becomes imperative to take into account ancillary factors such as the psychological test scores, demographic features, underlying latent primitive emotions, along with the text’s undertone and its psychological complexity. This paper proffers team PVG’s solution to the WASSA 2021 Shared Task on Predicting Empathy and Emotion in Reaction to News Stories. Leveraging the textual data, demographic features, psychological test score, and the intrinsic interdependencies of primitive emotions and empathy, we propose a multi-input, multi-task framework for the task of empathy score prediction. Here, the empathy score prediction is considered the primary task, while emotion and empathy classification are considered secondary auxiliary tasks. For the distress score prediction task, the system is further boosted by the addition of lexical features. Our submission ranked 1st based on the average correlation (0.545) as well as the distress correlation (0.574), and 2nd for the empathy Pearson correlation (0.517).</abstract>
      <url hash="a7e600c1">2021.wassa-1.11</url>
      <bibkey>kulkarni-etal-2021-pvg</bibkey>
      <pwccode url="https://github.com/mr-atharva-kulkarni/EACL-WASSA-2021-Empathy-Distress" additional="false">mr-atharva-kulkarni/EACL-WASSA-2021-Empathy-Distress</pwccode>
    </paper>
    <paper id="12">
      <title><fixed-case>WASSA</fixed-case>@<fixed-case>IITK</fixed-case> at <fixed-case>WASSA</fixed-case> 2021: Multi-task Learning and Transformer Finetuning for Emotion Classification and Empathy Prediction</title>
      <author><first>Jay</first><last>Mundra</last></author>
      <author><first>Rohan</first><last>Gupta</last></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <pages>112–116</pages>
      <abstract>This paper describes our contribution to the WASSA 2021 shared task on Empathy Prediction and Emotion Classification. The broad goal of this task was to model an empathy score, a distress score and the overall level of emotion of an essay written in response to a newspaper article associated with harm to someone. We have used the ELECTRA model abundantly and also advanced deep learning approaches like multi-task learning. Additionally, we also leveraged standard machine learning techniques like ensembling. Our system achieves a Pearson Correlation Coefficient of 0.533 on sub-task I and a macro F1 score of 0.5528 on sub-task II. We ranked 1st in Emotion Classification sub-task and 3rd in Empathy Prediction sub-task.</abstract>
      <url hash="9a2eaece">2021.wassa-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="9586d646">2021.wassa-1.12.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>mundra-etal-2021-wassa</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
    </paper>
    <paper id="13">
      <title>Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes</title>
      <author><first>Anvesh</first><last>Rao Vijjini</last></author>
      <author><first>Kaveri</first><last>Anuranjana</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>117–128</pages>
      <abstract>While Curriculum Learning (CL) has recently gained traction in Natural language Processing Tasks, it is still not adequately analyzed. Previous works only show their effectiveness but fail short to explain and interpret the internal workings fully. In this paper, we analyze curriculum learning in sentiment analysis along multiple axes. Some of these axes have been proposed by earlier works that need more in-depth study. Such analysis requires understanding where curriculum learning works and where it does not. Our axes of analysis include Task difficulty on CL, comparing CL pacing techniques, and qualitative analysis by visualizing the movement of attention scores in the model as curriculum phases progress. We find that curriculum learning works best for difficult tasks and may even lead to a decrement in performance for tasks with higher performance without curriculum learning. We see that One-Pass curriculum strategies suffer from catastrophic forgetting and attention movement visualization within curriculum pacing. This shows that curriculum learning breaks down the challenging main task into easier sub-tasks solved sequentially.</abstract>
      <url hash="a7024568">2021.wassa-1.13</url>
      <bibkey>rao-vijjini-etal-2021-analyzing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-100">CIFAR-100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-5">SST-5</pwcdataset>
    </paper>
    <paper id="14">
      <title>Lightweight Models for Multimodal Sequential Data</title>
      <author><first>Soumya</first><last>Sourav</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <pages>129–137</pages>
      <abstract>Human language encompasses more than just text; it also conveys emotions through tone and gestures. We present a case study of three simple and efficient Transformer-based architectures for predicting sentiment and emotion in multimodal data. The Late Fusion model merges unimodal features to create a multimodal feature sequence, the Round Robin model iteratively combines bimodal features using cross-modal attention, and the Hybrid Fusion model combines trimodal and unimodal features together to form a final feature sequence for predicting sentiment. Our experiments show that our small models are effective and outperform the publicly released versions of much larger, state-of-the-art multimodal sentiment analysis systems.</abstract>
      <url hash="78874fcb">2021.wassa-1.14</url>
      <attachment type="OptionalSupplementaryMaterial" hash="77e7d497">2021.wassa-1.14.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sourav-ouyang-2021-lightweight</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="15">
      <title>Exploring Implicit Sentiment Evoked by Fine-grained News Events</title>
      <author><first>Cynthia</first><last>Van Hee</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>138–148</pages>
      <abstract>We investigate the feasibility of defining sentiment evoked by fine-grained news events. Our research question is based on the premise that methods for detecting implicit sentiment in news can be a key driver of content diversity, which is one way to mitigate the detrimental effects of filter bubbles that recommenders based on collaborative filtering may produce. Our experiments are based on 1,735 news articles from major Flemish newspapers that were manually annotated, with high agreement, for implicit sentiment. While lexical resources prove insufficient for sentiment analysis in this data genre, our results demonstrate that machine learning models based on SVM and BERT are able to automatically infer the implicit sentiment evoked by news events.</abstract>
      <url hash="0c0177e1">2021.wassa-1.15</url>
      <bibkey>van-hee-etal-2021-exploring</bibkey>
    </paper>
    <paper id="16">
      <title>Exploring Stylometric and Emotion-Based Features for Multilingual Cross-Domain Hate Speech Detection</title>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>149–159</pages>
      <abstract>In this paper, we describe experiments designed to evaluate the impact of stylometric and emotion-based features on hate speech detection: the task of classifying textual content into hate or non-hate speech classes. Our experiments are conducted for three languages – English, Slovene, and Dutch – both in in-domain and cross-domain setups, and aim to investigate hate speech using features that model two linguistic phenomena: the writing style of hateful social media content operationalized as function word usage on the one hand, and emotion expression in hateful messages on the other hand. The results of experiments with features that model different combinations of these phenomena support our hypothesis that stylometric and emotion-based features are robust indicators of hate speech. Their contribution remains persistent with respect to domain and language variation. We show that the combination of features that model the targeted phenomena outperforms words and character n-gram features under cross-domain conditions, and provides a significant boost to deep learning models, which currently obtain the best results, when combined with them in an ensemble.</abstract>
      <url hash="f428f44d">2021.wassa-1.16</url>
      <bibkey>markov-etal-2021-exploring</bibkey>
    </paper>
    <paper id="17">
      <title>Emotion-Aware, Emotion-Agnostic, or Automatic: Corpus Creation Strategies to Obtain Cognitive Event Appraisal Annotations</title>
      <author><first>Jan</first><last>Hofmann</last></author>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>160–170</pages>
      <abstract>Appraisal theories explain how the cognitive evaluation of an event leads to a particular emotion. In contrast to theories of basic emotions or affect (valence/arousal), this theory has not received a lot of attention in natural language processing. Yet, in psychology it has been proven powerful: Smith and Ellsworth (1985) showed that the appraisal dimensions attention, certainty, anticipated effort, pleasantness, responsibility/control and situational control discriminate between (at least) 15 emotion classes. We study different annotation strategies for these dimensions, based on the event-focused enISEAR corpus (Troiano et al., 2019). We analyze two manual annotation settings: (1) showing the text to annotate while masking the experienced emotion label; (2) revealing the emotion associated with the text. Setting 2 enables the annotators to develop a more realistic intuition of the described event, while Setting 1 is a more standard annotation procedure, purely relying on text. We evaluate these strategies in two ways: by measuring inter-annotator agreement and by fine- tuning RoBERTa to predict appraisal variables. Our results show that knowledge of the emotion increases annotators’ reliability. Further, we evaluate a purely automatic rule-based labeling strategy (inferring appraisal from annotated emotion classes). Training on automatically assigned labels leads to a competitive performance of our classifier, even when tested on manual annotations. This is an indicator that it might be possible to automatically create appraisal corpora for every domain for which emotion corpora already exist.</abstract>
      <url hash="4f37c4fa">2021.wassa-1.17</url>
      <bibkey>hofmann-etal-2021-emotion</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="18">
      <title>Hate Towards the Political Opponent: A <fixed-case>T</fixed-case>witter Corpus Study of the 2020 <fixed-case>US</fixed-case> Elections on the Basis of Offensive Speech and Stance Detection</title>
      <author><first>Lara</first><last>Grimminger</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>171–180</pages>
      <abstract>The 2020 US Elections have been, more than ever before, characterized by social media campaigns and mutual accusations. We investigate in this paper if this manifests also in online communication of the supporters of the candidates Biden and Trump, by uttering hateful and offensive communication. We formulate an annotation task, in which we join the tasks of hateful/offensive speech detection and stance detection, and annotate 3000 Tweets from the campaign period, if they express a particular stance towards a candidate. Next to the established classes of favorable and against, we add mixed and neutral stances and also annotate if a candidate is mentioned with- out an opinion expression. Further, we an- notate if the tweet is written in an offensive style. This enables us to analyze if supporters of Joe Biden and the Democratic Party communicate differently than supporters of Donald Trump and the Republican Party. A BERT baseline classifier shows that the detection if somebody is a supporter of a candidate can be performed with high quality (.89 F1 for Trump and .91 F1 for Biden), while the detection that somebody expresses to be against a candidate is more challenging (.79 F1 and .64 F1, respectively). The automatic detection of hate/offensive speech remains challenging (with .53 F1). Our corpus is publicly available and constitutes a novel resource for computational modelling of offensive language under consideration of stances.</abstract>
      <url hash="1326d5f6">2021.wassa-1.18</url>
      <bibkey>grimminger-klinger-2021-hate</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="19">
      <title>Synthetic Examples Improve Cross-Target Generalization: A Study on Stance Detection on a <fixed-case>T</fixed-case>witter corpus.</title>
      <author><first>Costanza</first><last>Conforti</last></author>
      <author><first>Jakob</first><last>Berndt</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <author><first>Chryssi</first><last>Giannitsarou</last></author>
      <author><first>Flavio</first><last>Toxvaerd</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>181–187</pages>
      <abstract>Cross-target generalization is a known problem in stance detection (SD), where systems tend to perform poorly when exposed to targets unseen during training. Given that data annotation is expensive and time-consuming, finding ways to leverage abundant unlabeled in-domain data can offer great benefits. In this paper, we apply a weakly supervised framework to enhance cross-target generalization through synthetically annotated data. We focus on Twitter SD and show experimentally that integrating synthetic data is helpful for cross-target generalization, leading to significant improvements in performance, with gains in F1 scores ranging from +3.4 to +5.1.</abstract>
      <url hash="fb63b55e">2021.wassa-1.19</url>
      <bibkey>conforti-etal-2021-synthetic</bibkey>
    </paper>
    <paper id="20">
      <title>Creating and Evaluating Resources for Sentiment Analysis in the Low-resource Language: <fixed-case>S</fixed-case>indhi</title>
      <author><first>Wazir</first><last>Ali</last></author>
      <author><first>Naveed</first><last>Ali</last></author>
      <author><first>Yong</first><last>Dai</last></author>
      <author><first>Jay</first><last>Kumar</last></author>
      <author><first>Saifullah</first><last>Tumrani</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <pages>188–194</pages>
      <abstract>In this paper, we develop Sindhi subjective lexicon using a merger of existing English resources: NRC lexicon, list of opinion words, SentiWordNet, Sindhi-English bilingual dictionary, and collection of Sindhi modifiers. The positive or negative sentiment score is assigned to each Sindhi opinion word. Afterwards, we determine the coverage of the proposed lexicon with subjectivity analysis. Moreover, we crawl multi-domain tweet corpus of news, sports, and finance. The crawled corpus is annotated by experienced annotators using the Doccano text annotation tool. The sentiment annotated corpus is evaluated by employing support vector machine (SVM), recurrent neural network (RNN) variants, and convolutional neural network (CNN).</abstract>
      <url hash="32aae211">2021.wassa-1.20</url>
      <bibkey>ali-etal-2021-creating</bibkey>
    </paper>
    <paper id="21">
      <title>Towards Emotion Recognition in <fixed-case>H</fixed-case>indi-<fixed-case>E</fixed-case>nglish Code-Mixed Data: A Transformer Based Approach</title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <author><first>Akshita</first><last>Aggarwal</last></author>
      <pages>195–202</pages>
      <abstract>In the last few years, emotion detection in social-media text has become a popular problem due to its wide ranging application in better understanding the consumers, in psychology, in aiding human interaction with computers, designing smart systems etc. Because of the availability of huge amounts of data from social-media, which is regularly used for expressing sentiments and opinions, this problem has garnered great attention. In this paper, we present a Hinglish dataset labelled for emotion detection. We highlight a deep learning based approach for detecting emotions using bilingual word embeddings derived from FastText and Word2Vec approaches in Hindi-English code mixed tweets. We experiment with various deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with and without attention), along with transformers like BERT, RoBERTa, and ALBERT. The transformer based BERT model outperforms all current state-of-the-art models giving the best performance with an accuracy of 71.43%.</abstract>
      <url hash="6337ef59">2021.wassa-1.21</url>
      <bibkey>wadhawan-aggarwal-2021-towards</bibkey>
      <pwccode url="https://github.com/anshulwadhawan/emotion_detection" additional="false">anshulwadhawan/emotion_detection</pwccode>
    </paper>
    <paper id="22">
      <title>Nearest neighbour approaches for Emotion Detection in Tweets</title>
      <author><first>Olha</first><last>Kaminska</last></author>
      <author><first>Chris</first><last>Cornelis</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>203–212</pages>
      <abstract>Emotion detection is an important task that can be applied to social media data to discover new knowledge. While the use of deep learning methods for this task has been prevalent, they are black-box models, making their decisions hard to interpret for a human operator. Therefore, in this paper, we propose an approach using weighted k Nearest Neighbours (kNN), a simple, easy to implement, and explainable machine learning model. These qualities can help to enhance results’ reliability and guide error analysis. In particular, we apply the weighted kNN model to the shared emotion detection task in tweets from SemEval-2018. Tweets are represented using different text embedding methods and emotion lexicon vocabulary scores, and classification is done by an ensemble of weighted kNN models. Our best approaches obtain results competitive with state-of-the-art solutions and open up a promising alternative path to neural network methods.</abstract>
      <url hash="09c6907b">2021.wassa-1.22</url>
      <bibkey>kaminska-etal-2021-nearest</bibkey>
      <pwccode url="https://github.com/olha-kaminska/wknn_emotion_detection" additional="false">olha-kaminska/wknn_emotion_detection</pwccode>
    </paper>
    <paper id="23">
      <title><fixed-case>L</fixed-case>3<fixed-case>C</fixed-case>ube<fixed-case>M</fixed-case>aha<fixed-case>S</fixed-case>ent: A <fixed-case>M</fixed-case>arathi Tweet-based Sentiment Analysis Dataset</title>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Meet</first><last>Mandhane</last></author>
      <author><first>Manali</first><last>Likhitkar</last></author>
      <author><first>Gayatri</first><last>Kshirsagar</last></author>
      <author><first>Raviraj</first><last>Joshi</last></author>
      <pages>213–220</pages>
      <abstract>Sentiment analysis is one of the most fundamental tasks in Natural Language Processing. Popular languages like English, Arabic, Russian, Mandarin, and also Indian languages such as Hindi, Bengali, Tamil have seen a significant amount of work in this area. However, the Marathi language which is the third most popular language in India still lags behind due to the absence of proper datasets. In this paper, we present the first major publicly available Marathi Sentiment Analysis Dataset - L3CubeMahaSent. It is curated using tweets extracted from various Maharashtrian personalities’ Twitter accounts. Our dataset consists of ~16,000 distinct tweets classified in three broad classes viz. positive, negative, and neutral. We also present the guidelines using which we annotated the tweets. Finally, we present the statistics of our dataset and baseline classification results using CNN, LSTM, ULMFiT, and BERT based models.</abstract>
      <url hash="08200ad2">2021.wassa-1.23</url>
      <bibkey>kulkarni-etal-2021-l3cubemahasent</bibkey>
      <pwccode url="https://github.com/l3cube-pune/MarathiNLP" additional="false">l3cube-pune/MarathiNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/l3cubemahasent">L3CubeMahaSent</pwcdataset>
    </paper>
    <paper id="24">
      <title>Multi-Emotion Classification for Song Lyrics</title>
      <author><first>Darren</first><last>Edmonds</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>221–235</pages>
      <abstract>Song lyrics convey a multitude of emotions to the listener and powerfully portray the emotional state of the writer or singer. This paper examines a variety of modeling approaches to the multi-emotion classification problem for songs. We introduce the Edmonds Dance dataset, a novel emotion-annotated lyrics dataset from the reader’s perspective, and annotate the dataset of Mihalcea and Strapparava (2012) at the song level. We find that models trained on relatively small song datasets achieve marginally better performance than BERT (Devlin et al., 2018) fine-tuned on large social media or dialog datasets.</abstract>
      <url hash="5197b449">2021.wassa-1.24</url>
      <bibkey>edmonds-sedoc-2021-multi</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="25">
      <title><fixed-case>ONE</fixed-case>: Toward <fixed-case>ONE</fixed-case> model, <fixed-case>ONE</fixed-case> algorithm, <fixed-case>ONE</fixed-case> corpus dedicated to sentiment analysis of <fixed-case>A</fixed-case>rabic/<fixed-case>A</fixed-case>rabizi and its dialects</title>
      <author><first>Imane</first><last>Guellil</last></author>
      <author><first>Faical</first><last>Azouaou</last></author>
      <author><first>Fodil</first><last>Benali</last></author>
      <author><first>Hachani</first><last>Ala-Eddine</last></author>
      <pages>236–249</pages>
      <abstract>Arabic is the official language of 22 countries, spoken by more than 400 million speakers. Each one of this country use at least on dialect for daily life conversation. Then, Arabic has at least 22 dialects. Each dialect can be written in Arabic or Arabizi Scripts. The most recent researches focus on constructing a language model and a training corpus for each dialect, in each script. Following this technique means constructing 46 different resources (by including the Modern Standard Arabic, MSA) for handling only one language. In this paper, we extract ONE corpus, and we propose ONE algorithm to automatically construct ONE training corpus using ONE classification model architecture for sentiment analysis MSA and different dialects. After manually reviewing the training corpus, the obtained results outperform all the research literature results for the targeted test corpora.</abstract>
      <url hash="c96de57e">2021.wassa-1.25</url>
      <bibkey>guellil-etal-2021-one</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
    </paper>
    <paper id="26">
      <title>Me, myself, and ire: Effects of automatic transcription quality on emotion, sarcasm, and personality detection</title>
      <author><first>John</first><last>Culnan</last></author>
      <author><first>Seongjin</first><last>Park</last></author>
      <author><first>Meghavarshini</first><last>Krishnaswamy</last></author>
      <author><first>Rebecca</first><last>Sharp</last></author>
      <pages>250–256</pages>
      <abstract>In deployment, systems that use speech as input must make use of automated transcriptions. Yet, typically when these systems are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets: emotion, sarcasm, and personality detection. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of audio features partially mitigates transcription errors, but that a naive usage of a multi-task setup does not.</abstract>
      <url hash="cb4743b9">2021.wassa-1.26</url>
      <bibkey>culnan-etal-2021-ire</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="27">
      <title>Emotional <fixed-case>R</fixed-case>ob<fixed-case>BERT</fixed-case> and Insensitive <fixed-case>BERT</fixed-case>je: Combining Transformers and Affect Lexica for <fixed-case>D</fixed-case>utch Emotion Detection</title>
      <author><first>Luna</first><last>De Bruyne</last></author>
      <author><first>Orphee</first><last>De Clercq</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <pages>257–263</pages>
      <abstract>In a first step towards improving Dutch emotion detection, we try to combine the Dutch transformer models BERTje and RobBERT with lexicon-based methods. We propose two architectures: one in which lexicon information is directly injected into the transformer model and a meta-learning approach where predictions from transformers are combined with lexicon features. The models are tested on 1,000 Dutch tweets and 1,000 captions from TV-shows which have been manually annotated with emotion categories and dimensions. We find that RobBERT clearly outperforms BERTje, but that directly adding lexicon information to transformers does not improve performance. In the meta-learning approach, lexicon information does have a positive effect on BERTje, but not on RobBERT. This suggests that more emotional information is already contained within this latter language model.</abstract>
      <url hash="41d89002">2021.wassa-1.27</url>
      <bibkey>de-bruyne-etal-2021-emotional</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>E</fixed-case>mp<fixed-case>N</fixed-case>a at <fixed-case>WASSA</fixed-case> 2021: A Lightweight Model for the Prediction of Empathy, Distress and Emotions from Reactions to News Stories</title>
      <author><first>Giuseppe</first><last>Vettigli</last></author>
      <author><first>Antonio</first><last>Sorgente</last></author>
      <pages>264–268</pages>
      <abstract>This paper describes our submission for the WASSA 2021 shared task regarding the prediction of empathy, distress and emotions from news stories. The solution is based on combining the frequency of words, lexicon-based information, demographics of the annotators and personality of the annotators into a linear model. The prediction of empathy and distress is performed using Linear Regression while the prediction of emotions is performed using Logistic Regression. Both tasks are performed using the same features. Our models rank 4th for the prediction of emotions and 2nd for the prediction of empathy and distress. These results are particularly interesting when considered that the computational requirements of the solution are minimal.</abstract>
      <url hash="8c2cdbc4">2021.wassa-1.28</url>
      <attachment type="OptionalSupplementaryMaterial" hash="cf823096">2021.wassa-1.28.OptionalSupplementaryMaterial.html</attachment>
      <bibkey>vettigli-sorgente-2021-empna</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>M</fixed-case>ila<fixed-case>NLP</fixed-case> @ <fixed-case>WASSA</fixed-case>: Does <fixed-case>BERT</fixed-case> Feel Sad When You Cry?</title>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>269–273</pages>
      <abstract>The paper describes the MilaNLP team’s submission (Bocconi University, Milan) in the WASSA 2021 Shared Task on Empathy Detection and Emotion Classification. We focus on Track 2 - Emotion Classification - which consists of predicting the emotion of reactions to English news stories at the essay-level. We test different models based on multi-task and multi-input frameworks. The goal was to better exploit all the correlated information given in the data set. We find, though, that empathy as an auxiliary task in multi-task learning and demographic attributes as additional input provide worse performance with respect to single-task learning. While the result is competitive in terms of the competition, our results suggest that emotion and empathy are not related tasks - at least for the purpose of prediction.</abstract>
      <url hash="e500aa08">2021.wassa-1.29</url>
      <attachment type="Software" hash="d842c16e">2021.wassa-1.29.Software.zip</attachment>
      <attachment type="Dataset" hash="4963bf87">2021.wassa-1.29.Dataset.zip</attachment>
      <bibkey>fornaciari-etal-2021-milanlp</bibkey>
    </paper>
    <paper id="30">
      <title>Team Phoenix at <fixed-case>WASSA</fixed-case> 2021: Emotion Analysis on News Stories with Pre-Trained Language Models</title>
      <author><first>Yash</first><last>Butala</last></author>
      <author><first>Kanishk</first><last>Singh</last></author>
      <author><first>Adarsh</first><last>Kumar</last></author>
      <author><first>Shrey</first><last>Shrivastava</last></author>
      <pages>274–280</pages>
      <abstract>Emotion is fundamental to humanity. The ability to perceive, understand and respond to social interactions in a human-like manner is one of the most desired capabilities in artificial agents, particularly in social-media bots. Over the past few years, computational understanding and detection of emotional aspects in language have been vital in advancing human-computer interaction. The WASSA Shared Task 2021 released a dataset of news-stories across two tracks, Track-1 for Empathy and Distress Prediction and Track-2 for Multi-Dimension Emotion prediction at the essay-level. We describe our system entry for the WASSA 2021 Shared Task (for both Track-1 and Track-2), where we leveraged the information from Pre-trained language models for Track-specific Tasks. Our proposed models achieved an Average Pearson Score of 0.417, and a Macro-F1 Score of 0.502 in Track 1 and Track 2, respectively. In the Shared Task leaderboard, we secured the fourth rank in Track 1 and the second rank in Track 2.</abstract>
      <url hash="3eede593">2021.wassa-1.30</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0dc05099">2021.wassa-1.30.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>butala-etal-2021-team</bibkey>
      <pwccode url="https://github.com/yashbutala/WASSA" additional="false">yashbutala/WASSA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emotion">CARER</pwcdataset>
    </paper>
  </volume>
</collection>
