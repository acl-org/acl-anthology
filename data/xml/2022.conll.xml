<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.conll">
  <volume id="1" ingest-date="2022-12-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)</booktitle>
      <editor><first>Antske</first><last>Fokkens</last></editor>
      <editor><first>Vivek</first><last>Srikumar</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="6c6ea788">2022.conll-1</url>
      <venue>conll</venue>
    </meta>
    <frontmatter>
      <url hash="d9b79018">2022.conll-1.0</url>
      <bibkey>conll-2022-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text Classification</title>
      <author><first>Sosuke</first><last>Nishikawa</last></author>
      <author><first>Ikuya</first><last>Yamada</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last></author>
      <author><first>Isao</first><last>Echizen</last></author>
      <pages>1-12</pages>
      <abstract>We present a multilingual bag-of-entities model that effectively boosts the performance of zero-shot cross-lingual text classification by extending a multilingual pre-trained language model (e.g., M-BERT). It leverages the multilingual nature of Wikidata: entities in multiple languages representing the same concept are defined with a unique identifier. This enables entities described in multiple languages to be represented using shared embeddings. A model trained on entity features in a resource-rich language can thus be directly applied to other languages. Our experimental results on cross-lingual topic classification (using the MLDoc and TED-CLDC datasets) and entity typing (using the SHINRA2020-ML dataset) show that the proposed model consistently outperforms state-of-the-art models.</abstract>
      <url hash="bc0f8f11">2022.conll-1.1</url>
      <bibkey>nishikawa-etal-2022-multilingual</bibkey>
      <doi>10.18653/v1/2022.conll-1.1</doi>
    </paper>
    <paper id="2">
      <title>Collateral facilitation in humans and language models</title>
      <author><first>James</first><last>Michaelov</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <pages>13-26</pages>
      <abstract>Are the predictions of humans and language models affected by similar things? Research suggests that while comprehending language, humans make predictions about upcoming words, with more predictable words being processed more easily. However, evidence also shows that humans display a similar processing advantage for highly anomalous words when these words are semantically related to the preceding context or to the most probable continuation. Using stimuli from 3 psycholinguistic experiments, we find that this is also almost always also the case for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa, XLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of this phenomenon for our understanding of both human language comprehension and the predictions made by language models.</abstract>
      <url hash="55c86b97">2022.conll-1.2</url>
      <bibkey>michaelov-bergen-2022-collateral</bibkey>
      <video href="2022.conll-1.2.mp4"/>
      <doi>10.18653/v1/2022.conll-1.2</doi>
    </paper>
    <paper id="3">
      <title>How Hate Speech Varies by Target Identity: A Computational Analysis</title>
      <author><first>Michael</first><last>Yoder</last></author>
      <author><first>Lynnette</first><last>Ng</last></author>
      <author><first>David West</first><last>Brown</last></author>
      <author><first>Kathleen</first><last>Carley</last></author>
      <pages>27-39</pages>
      <abstract>This paper investigates how hate speech varies in systematic ways according to the identities it targets. Across multiple hate speech datasets annotated for targeted identities, we find that classifiers trained on hate speech targeting specific identity groups struggle to generalize to other targeted identities. This provides empirical evidence for differences in hate speech by target identity; we then investigate which patterns structure this variation. We find that the targeted demographic category (e.g. gender/sexuality or race/ethnicity) appears to have a greater effect on the language of hate speech than does the relative social power of the targeted identity group. We also find that words associated with hate speech targeting specific identities often relate to stereotypes, histories of oppression, current social movements, and other social contexts specific to identities. These experiments suggest the importance of considering targeted identity, as well as the social contexts associated with these identities, in automated hate speech classification</abstract>
      <url hash="9eddbf11">2022.conll-1.3</url>
      <bibkey>yoder-etal-2022-hate</bibkey>
      <revision id="1" href="2022.conll-1.3v1" hash="4448b7e0"/>
      <revision id="2" href="2022.conll-1.3v2" hash="9eddbf11" date="2023-02-16">Updated wording.</revision>
      <doi>10.18653/v1/2022.conll-1.3</doi>
    </paper>
    <paper id="4">
      <title>Continual Learning for Natural Language Generations with Transformer Calibration</title>
      <author><first>Peng</first><last>Yang</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>40-49</pages>
      <abstract>Conventional natural language process (NLP) generation models are trained offline with a given dataset for a particular task, which is referred to as isolated learning. Research on sequence-to-sequence language generation aims to study continual learning model to constantly learning from sequentially encountered tasks. However, continual learning studies often suffer from catastrophic forgetting, a persistent challenge for lifelong learning. In this paper, we present a novel NLP transformer model that attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., attention calibration. We model the attention in the transformer as a calibrated unit in a general formulation, where the attention calibration could give benefits to balance the stability and plasticity of continual learning algorithms through influencing both their forward inference path and backward optimization path. Our empirical experiments, paraphrase generation and dialog response generation, demonstrate that this work outperforms state-of-the-art models by a considerable margin and effectively mitigate the forgetting.</abstract>
      <url hash="22910be9">2022.conll-1.4</url>
      <bibkey>yang-etal-2022-continual</bibkey>
      <video href="2022.conll-1.4.mp4"/>
      <doi>10.18653/v1/2022.conll-1.4</doi>
    </paper>
    <paper id="5">
      <title>That’s so cute!: The <fixed-case>CARE</fixed-case> Dataset for Affective Response Detection</title>
      <author><first>Jane</first><last>Yu</last></author>
      <author><first>Alon</first><last>Halevy</last></author>
      <pages>50-69</pages>
      <abstract>Social media plays an increasing role in our communication with friends and family, and in our consumption of entertainment and information. Hence, to design effective ranking functions for posts on social media, it would be useful to predict the affective responses of a post (e.g., whether it is likely to elicit feelings of entertainment, inspiration, or anger). Similar to work on emotion detection (which focuses on the affect of the publisher of the post), the traditional approach to recognizing affective response would involve an expensive investment in human annotation of training data. We create and publicly release CARE DB, a dataset of 230k social media post annotations according to seven affective responses using the Common Affective Response Expression (CARE) method. The CARE method is a means of leveraging the signal that is present in comments that are posted in response to a post, providing high-precision evidence about the affective response to the post without human annotation. Unlike human annotation, the annotation process we describe here can be iterated upon to expand the coverage of the method, particularly for new affective responses. We present experiments that demonstrate that the CARE annotations compare favorably with crowdsourced annotations. Finally, we use CARE DB to train competitive BERT-based models for predicting affective response as well as emotion detection, demonstrating the utility of the dataset for related tasks.</abstract>
      <url hash="fb44c15d">2022.conll-1.5</url>
      <bibkey>yu-halevy-2022-thats</bibkey>
      <doi>10.18653/v1/2022.conll-1.5</doi>
    </paper>
    <paper id="6">
      <title>A Fine-grained Interpretability Evaluation Benchmark for Neural <fixed-case>NLP</fixed-case></title>
      <author><first>Lijie</first><last>Wang</last></author>
      <author><first>Yaozong</first><last>Shen</last></author>
      <author><first>Shuyuan</first><last>Peng</last></author>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hao</first><last>Liu</last></author>
      <author><first>Hongxuan</first><last>Tang</last></author>
      <author><first>Ying</first><last>Chen</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>70-84</pages>
      <abstract>While there is increasing concern about the interpretability of neural models, the evaluation of interpretability remains an open problem, due to the lack of proper evaluation datasets and metrics. In this paper, we present a novel benchmark to evaluate the interpretability of both neural models and saliency methods. This benchmark covers three representative NLP tasks: sentiment analysis, textual similarity and reading comprehension, each provided with both English and Chinese annotated data. In order to precisely evaluate the interpretability, we provide token-level rationales that are carefully annotated to be sufficient, compact and comprehensive. We also design a new metric, i.e., the consistency between the rationales before and after perturbations, to uniformly evaluate the interpretability on different types of tasks. Based on this benchmark, we conduct experiments on three typical models with three saliency methods, and unveil their strengths and weakness in terms of interpretability. We will release this benchmark (<url>https://www.luge.ai/#/luge/task/taskDetail?taskId=15</url>) and hope it can facilitate the research in building trustworthy systems.</abstract>
      <url hash="3ab64a20">2022.conll-1.6</url>
      <bibkey>wang-etal-2022-fine</bibkey>
      <doi>10.18653/v1/2022.conll-1.6</doi>
    </paper>
    <paper id="7">
      <title>Towards More Natural Artificial Languages</title>
      <author><first>Mark</first><last>Hopkins</last></author>
      <pages>85-94</pages>
      <abstract>A number of papers have recently argued in favor of using artificially generated languages to investigate the inductive biases of linguistic models, or to develop models for low-resource languages with underrepresented typologies. But the promise of artificial languages comes with a caveat: if these artificial languages are not sufficiently reflective of natural language, then using them as a proxy may lead to inaccurate conclusions. In this paper, we take a step towards increasing the realism of artificial language by introducing a variant of indexed grammars that draw their weights from hierarchical Pitman-Yor processes. We show that this framework generates languages that emulate the statistics of natural language corpora better than the current approach of directly formulating weighted context-free grammars.</abstract>
      <url hash="ee873e09">2022.conll-1.7</url>
      <bibkey>hopkins-2022-towards</bibkey>
      <video href="2022.conll-1.7.mp4"/>
      <doi>10.18653/v1/2022.conll-1.7</doi>
    </paper>
    <paper id="8">
      <title>Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models</title>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>95-109</pages>
      <abstract>Structural probing work has found evidence for latent syntactic information in pre-trained language models. However, much of this analysis has focused on monolingual models, and analyses of multilingual models have employed correlational methods that are confounded by the choice of probing tasks. In this study, we causally probe multilingual language models (XGLM and multilingual BERT) as well as monolingual BERT-based models across various languages; we do this by performing counterfactual perturbations on neuron activations and observing the effect on models’ subject-verb agreement probabilities. We observe where in the model and to what extent syntactic agreement is encoded in each language. We find significant neuron overlap across languages in autoregressive multilingual language models, but not masked language models. We also find two distinct layer-wise effect patterns and two distinct sets of neurons used for syntactic agreement, depending on whether the subject and verb are separated by other tokens. Finally, we find that behavioral analyses of language models are likely underestimating how sensitive masked language models are to syntactic information.</abstract>
      <url hash="2291c42d">2022.conll-1.8</url>
      <bibkey>mueller-etal-2022-causal</bibkey>
      <video href="2022.conll-1.8.mp4"/>
      <doi>10.18653/v1/2022.conll-1.8</doi>
    </paper>
    <paper id="9">
      <title>Combining Noisy Semantic Signals with Orthographic Cues: Cognate Induction for the <fixed-case>I</fixed-case>ndic Dialect Continuum</title>
      <author><first>Niyati</first><last>Bafna</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <pages>110-131</pages>
      <abstract>We present a novel method for unsupervised cognate/borrowing identification from monolingual corpora designed for low and extremely low resource scenarios, based on combining noisy semantic signals from joint bilingual spaces with orthographic cues modelling sound change. We apply our method to the North Indian dialect continuum, containing several dozens of dialects and languages spoken by more than 100 million people. Many of these languages are zero-resource and therefore natural language processing for them is non-existent. We first collect monolingual data for 26 Indic languages, 16 of which were previously zero-resource, and perform exploratory character, lexical and subword cross-lingual alignment experiments for the first time at this scale on this dialect continuum. We create bilingual evaluation lexicons against Hindi for 20 of the languages. We then apply our cognate identification method on the data, and show that our method outperforms both traditional orthography baselines as well as EM-style learnt edit distance matrices. To the best of our knowledge, this is the first work to combine traditional orthographic cues with noisy bilingual embeddings to tackle unsupervised cognate detection in a (truly) low-resource setup, showing that even noisy bilingual embeddings can act as good guides for this task. We release our multilingual dialect corpus, called HinDialect, as well as our scripts for evaluation data collection and cognate induction.</abstract>
      <url hash="919e14d0">2022.conll-1.9</url>
      <bibkey>bafna-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.conll-1.9</doi>
    </paper>
    <paper id="10">
      <title>Detecting Unintended Social Bias in Toxic Language Datasets</title>
      <author><first>Nihar</first><last>Sahoo</last></author>
      <author><first>Himanshu</first><last>Gupta</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>132-143</pages>
      <abstract>With the rise of online hate speech, automatic detection of Hate Speech, Offensive texts as a natural language processing task is getting popular. However, very little research has been done to detect unintended social bias from these toxic language datasets. This paper introduces a new dataset ToxicBias curated from the existing dataset of Kaggle competition named “Jigsaw Unintended Bias in Toxicity Classification”. We aim to detect social biases, their categories, and targeted groups. The dataset contains instances annotated for five different bias categories, viz., gender, race/ethnicity, religion, political, and LGBTQ. We train transformer-based models using our curated datasets and report baseline performance for bias identification, target generation, and bias implications. Model biases and their mitigation are also discussed in detail. Our study motivates a systematic extraction of social bias data from toxic language datasets.</abstract>
      <url hash="6075330c">2022.conll-1.10</url>
      <bibkey>sahoo-etal-2022-detecting</bibkey>
      <doi>10.18653/v1/2022.conll-1.10</doi>
    </paper>
    <paper id="11">
      <title>Incremental Processing of Principle <fixed-case>B</fixed-case>: Mismatches Between Neural Models and Humans</title>
      <author><first>Forrest</first><last>Davis</last></author>
      <pages>144-156</pages>
      <abstract>Despite neural language models qualitatively capturing many human linguistic behaviors, recent work has demonstrated that they underestimate the true processing costs of ungrammatical structures. We extend these more fine-grained comparisons between humans and models by investigating the interaction between Principle B and coreference processing. While humans use Principle B to block certain structural positions from affecting their incremental processing, we find that GPT-based language models are influenced by ungrammatical positions. We conclude by relating the mismatch between neural models and humans to properties of training data and suggest that certain aspects of human processing behavior do not directly follow from linguistic data.</abstract>
      <url hash="470ff7bb">2022.conll-1.11</url>
      <bibkey>davis-2022-incremental</bibkey>
      <video href="2022.conll-1.11.mp4"/>
      <doi>10.18653/v1/2022.conll-1.11</doi>
    </paper>
    <paper id="12">
      <title>Parsing as Deduction Revisited: Using an Automatic Theorem Prover to Solve an <fixed-case>SMT</fixed-case> Model of a Minimalist Parser</title>
      <author><first>Sagar</first><last>Indurkhya</last></author>
      <pages>157-175</pages>
      <abstract>We introduce a constraint-based parser for Minimalist Grammars (MG), implemented as a working computer program, that falls within the long established “Parsing as Deduction” framework. The parser takes as input an MG lexicon and a (partially specified) pairing of sound with meaning - i.e. a word sequence paired with a semantic representation - and, using an axiomatized logic, declaratively deduces syntactic derivations (i.e. parse trees) that comport with the specified interface conditions. The parser is built on the first axiomatization of MGs to use Satisfiability Modulo Theories (SMT), encoding in a constraint-based way the principles of minimalist syntax. The parser operates via a novel solution method: it assembles an SMT model of an MG derivation, translates the inputs into SMT formulae that constrain the model, and then solves the model using the Z3 SMT-solver, a high-performance automatic theorem prover; as the SMT-model has finite size (being bounded by the inputs), it is decidable and thus solvable in finite time. The output derivation is then recovered from the model solution. To demonstrate this, we run the parser on several representative inputs and examine how the output derivations differ when the inputs are partially vs. fully specified. We conclude by discussing the parser’s extensibility and how a linguist can use it to automatically identify: (i) dependencies between input interface conditions and principles of syntax, and (ii) contradictions or redundancies between the model axioms encoding principles of syntax.</abstract>
      <url hash="357d2231">2022.conll-1.12</url>
      <bibkey>indurkhya-2022-parsing</bibkey>
      <doi>10.18653/v1/2022.conll-1.12</doi>
    </paper>
    <paper id="13">
      <title>Entailment Semantics Can Be Extracted from an Ideal Language Model</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Alex</first><last>Warstadt</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>176-193</pages>
      <abstract>Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.</abstract>
      <url hash="5c45e145">2022.conll-1.13</url>
      <bibkey>merrill-etal-2022-entailment</bibkey>
      <video href="2022.conll-1.13.mp4"/>
      <doi>10.18653/v1/2022.conll-1.13</doi>
      <revision id="1" href="2022.conll-1.13v1" hash="cb15c313"/>
      <revision id="2" href="2022.conll-1.13v2" hash="5c45e145" date="2024-02-08">Minor updates.</revision>
    </paper>
    <paper id="14">
      <title>On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation</title>
      <author><first>Gal</first><last>Patel</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>194-212</pages>
      <abstract>We present a methodology that explores how sentence structure is reflected in neural representations of machine translation systems. We demonstrate our model-agnostic approach with the Transformer English-German translation model. We analyze neuron-level correlation of activations between paraphrases while discussing the methodology challenges and the need for confound analysis to isolate the effects of shallow cues. We find that similarity between activation patterns can be mostly accounted for by similarity in word choice and sentence length. Following that, we manipulate neuron activations to control the syntactic form of the output. We show this intervention to be somewhat successful, indicating that deep models capture sentence-structure distinctions, despite finding no such indication at the neuron level. To conduct our experiments, we develop a semi-automatic method to generate meaning-preserving minimal pair paraphrases (active-passive voice and adverbial clause-noun phrase) and compile a corpus of such pairs.</abstract>
      <url hash="d1abc09a">2022.conll-1.14</url>
      <attachment type="data" hash="3d61b4f5">2022.conll-1.14.data.zip</attachment>
      <bibkey>patel-etal-2022-neurons</bibkey>
      <video href="2022.conll-1.14.mp4"/>
      <doi>10.18653/v1/2022.conll-1.14</doi>
    </paper>
    <paper id="15">
      <title>Shared knowledge in natural conversations: can entropy metrics shed light on information transfers?</title>
      <author><first>Eliot</first><last>Maës</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Leonor</first><last>Becerra</last></author>
      <pages>213-227</pages>
      <abstract>The mechanisms underlying human communication have been under investigation for decades, but the answer to how understanding between locutors emerges remains incomplete. Interaction theories suggest the development of a structural alignment between the speakers, allowing for the construction of a shared knowledge base (common ground). In this paper, we propose to apply metrics derived from information theory to quantify the amount of information exchanged between participants, the dynamics of information exchanges, to provide an objective way to measure the common ground instantiation. We focus on a corpus of free conversations augmented with prosodic segmentation and an expert annotation of thematic episodes. We show that during free conversations, the amount of information remains globally constant at the scale of the conversation, but varies depending on the thematic structuring, underlining the role of the speaker introducing the theme. We propose an original methodology applied to uncontrolled material.</abstract>
      <url hash="2bfbabb5">2022.conll-1.15</url>
      <bibkey>maes-etal-2022-shared</bibkey>
      <video href="2022.conll-1.15.mp4"/>
      <doi>10.18653/v1/2022.conll-1.15</doi>
    </paper>
    <paper id="16">
      <title>Leveraging a New <fixed-case>S</fixed-case>panish Corpus for Multilingual and Cross-lingual Metaphor Detection</title>
      <author><first>Elisa</first><last>Sanchez-Bayona</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>228-240</pages>
      <abstract>The lack of wide coverage datasets annotated with everyday metaphorical expressions for languages other than English is striking. This means that most research on supervised metaphor detection has been published only for that language. In order to address this issue, this work presents the first corpus annotated with naturally occurring metaphors in Spanish large enough to develop systems to perform metaphor detection. The presented dataset, CoMeta, includes texts from various domains, namely, news, political discourse, Wikipedia and reviews. In order to label CoMeta, we apply the MIPVU method, the guidelines most commonly used to systematically annotate metaphor on real data. We use our newly created dataset to provide competitive baselines by fine-tuning several multilingual and monolingual state-of-the-art large language models. Furthermore, by leveraging the existing VUAM English data in addition to CoMeta, we present the, to the best of our knowledge, first cross-lingual experiments on supervised metaphor detection. Finally, we perform a detailed error analysis that explores the seemingly high transfer of everyday metaphor across these two languages and datasets.</abstract>
      <url hash="4634a364">2022.conll-1.16</url>
      <attachment type="data" hash="8c90c1c5">2022.conll-1.16.data.zip</attachment>
      <bibkey>sanchez-bayona-agerri-2022-leveraging</bibkey>
      <doi>10.18653/v1/2022.conll-1.16</doi>
    </paper>
    <paper id="17">
      <title>Cognitive Simplification Operations Improve Text Simplification</title>
      <author><first>Eytan</first><last>Chamovitz</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>241-265</pages>
      <abstract>Text Simplification (TS) is the task of converting a text into a form that is easier to read while maintaining the meaning of the original text. A sub-task of TS is Cognitive Simplification (CS), converting text to a form that is readily understood by people with cognitive disabilities without rendering it childish or simplistic. This sub-task has yet to be explored with neural methods in NLP, and resources for it are scarcely available. In this paper, we present a method for incorporating knowledge from the cognitive accessibility domain into a TS model, by introducing an inductive bias regarding what simplification operations to use. We show that by adding this inductive bias to a TS-trained model, it is able to adapt better to CS without ever seeing CS data, and outperform a baseline model on a traditional TS benchmark. In addition, we provide a novel test dataset for CS, and analyze the differences between CS corpora and existing TS corpora, in terms of how simplification operations are applied.</abstract>
      <url hash="73d86732">2022.conll-1.17</url>
      <attachment type="data" hash="83b90c7e">2022.conll-1.17.data.zip</attachment>
      <bibkey>chamovitz-abend-2022-cognitive</bibkey>
      <video href="2022.conll-1.17.mp4"/>
      <doi>10.18653/v1/2022.conll-1.17</doi>
    </paper>
    <paper id="18">
      <title>On Language Spaces, Scales and Cross-Lingual Transfer of <fixed-case>UD</fixed-case> Parsers</title>
      <author><first>Tanja</first><last>Samardžić</last></author>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Olga</first><last>Pelloni</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>266-281</pages>
      <abstract>Cross-lingual transfer of parsing models has been shown to work well for several closely-related languages, but predicting the success in other cases remains hard. Our study is a comprehensive analysis of the impact of linguistic distance on the transfer of UD parsers. As an alternative to syntactic typological distances extracted from URIEL, we propose three text-based feature spaces and show that they can be more precise predictors, especially on a more local scale, when only shorter distances are taken into account. Our analyses also reveal that the good coverage in typological databases is not among the factors that explain good transfer.</abstract>
      <url hash="87468549">2022.conll-1.18</url>
      <bibkey>samardzic-etal-2022-language</bibkey>
      <video href="2022.conll-1.18.mp4"/>
      <doi>10.18653/v1/2022.conll-1.18</doi>
    </paper>
    <paper id="19">
      <title>Visual Semantic Parsing: From Images to <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Mohamed Ashraf</first><last>Abdelsalam</last></author>
      <author><first>Zhan</first><last>Shi</last></author>
      <author><first>Federico</first><last>Fancellu</last></author>
      <author><first>Kalliopi</first><last>Basioti</last></author>
      <author><first>Dhaivat</first><last>Bhatt</last></author>
      <author><first>Vladimir</first><last>Pavlovic</last></author>
      <author><first>Afsaneh</first><last>Fazly</last></author>
      <pages>282-300</pages>
      <abstract>The success of scene graphs for visual scene understanding has brought attention to the benefits of abstracting a visual input (e.g., image) into a structured representation, where entities (people and objects) are nodes connected by edges specifying their relations. Building these representations, however, requires expensive manual annotation in the form of images paired with their scene graphs or frames. These formalisms remain limited in the nature of entities and relations they can capture. In this paper, we propose to leverage a widely-used meaning representation in the field of natural language processing, the Abstract Meaning Representation (AMR), to address these shortcomings. Compared to scene graphs, which largely emphasize spatial relationships, our visual AMR graphs are more linguistically informed, with a focus on higher-level semantic concepts extrapolated from visual input. Moreover, they allow us to generate meta-AMR graphs to unify information contained in multiple image descriptions under one representation. Through extensive experimentation and analysis, we demonstrate that we can re-purpose an existing text-to-AMR parser to parse images into AMRs. Our findings point to important future research directions for improved scene understanding.</abstract>
      <url hash="566673e4">2022.conll-1.19</url>
      <bibkey>abdelsalam-etal-2022-visual</bibkey>
      <video href="2022.conll-1.19.mp4"/>
      <doi>10.18653/v1/2022.conll-1.19</doi>
    </paper>
    <paper id="20">
      <title>Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities</title>
      <author><first>Suhas</first><last>Arehalli</last></author>
      <author><first>Brian</first><last>Dillon</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>301-313</pages>
      <abstract>Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory (Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.</abstract>
      <url hash="8b31254a">2022.conll-1.20</url>
      <bibkey>arehalli-etal-2022-syntactic</bibkey>
      <video href="2022.conll-1.20.mp4"/>
      <doi>10.18653/v1/2022.conll-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>O</fixed-case>pen<fixed-case>S</fixed-case>tance: Real-world Zero-shot Stance Detection</title>
      <author><first>Hanzi</first><last>Xu</last></author>
      <author><first>Slobodan</first><last>Vucetic</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <pages>314-324</pages>
      <abstract>Prior studies of zero-shot stance detection identify the attitude of texts towards unseen topics occurring in the same document corpus. Such task formulation has three limitations: (i) Single domain/dataset. A system is optimized on a particular dataset from a single domain; therefore, the resulting system cannot work well on other datasets; (ii) the model is evaluated on a limited number of unseen topics; (iii) it is assumed that part of the topics has rich annotations, which might be impossible in real-world applications. These drawbacks will lead to an impractical stance detection system that fails to generalize to open domains and open-form topics. This work defines OpenStance: open-domain zero-shot stance detection, aiming to handle stance detection in an open world with neither domain constraints nor topic-specific annotations. The key challenge of OpenStance lies in open-domain generalization: learning a system with fully unspecific supervision but capable of generalizing to any dataset. To solve OpenStance, we propose to combine indirect supervision, from textual entailment datasets, and weak supervision, from data generated automatically by pre-trained Language Models. Our single system, without any topic-specific supervision, outperforms the supervised method on three popular datasets. To our knowledge, this is the first work that studies stance detection under the open-domain zero-shot setting. All data and code will be publicly released.</abstract>
      <url hash="f18580c8">2022.conll-1.21</url>
      <bibkey>xu-etal-2022-openstance</bibkey>
      <doi>10.18653/v1/2022.conll-1.21</doi>
    </paper>
    <paper id="22">
      <title>Optimizing text representations to capture (dis)similarity between political parties</title>
      <author><first>Tanise</first><last>Ceron</last></author>
      <author><first>Nico</first><last>Blokker</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <pages>325-338</pages>
      <abstract>Even though fine-tuned neural language models have been pivotal in enabling “deep” automatic text analysis, optimizing text representations for specific applications remains a crucial bottleneck. In this study, we look at this problem in the context of a task from computational social science, namely modeling pairwise similarities between political parties. Our research question is what level of structural information is necessary to create robust text representation, contrasting a strongly informed approach (which uses both claim span and claim category annotations) with approaches that forgo one or both types of annotation with document structure-based heuristics. Evaluating our models on the manifestos of German parties for the 2021 federal election. We find that heuristics that maximize within-party over between-party similarity along with a normalization step lead to reliable party similarity prediction, without the need for manual annotation.</abstract>
      <url hash="43ef9b56">2022.conll-1.22</url>
      <bibkey>ceron-etal-2022-optimizing</bibkey>
      <video href="2022.conll-1.22.mp4"/>
      <doi>10.18653/v1/2022.conll-1.22</doi>
    </paper>
    <paper id="23">
      <title>Computational cognitive modeling of predictive sentence processing in a second language</title>
      <author><first>Umesh</first><last>Patil</last></author>
      <author><first>Sol</first><last>Lago</last></author>
      <pages>339-349</pages>
      <abstract>We propose an ACT-R cue-based retrieval model of the real-time gender predictions displayed by second language (L2) learners. The model extends a previous model of native (L1) speakers according to two central accounts in L2 sentence processing: (i) the Interference Hypothesis, which proposes that retrieval interference is higher in L2 than L1 speakers; (ii) the Lexical Bottleneck Hypothesis, which proposes that problems with gender agreement are due to weak gender representations. We tested the predictions of these accounts using data from two visual world experiments, which found that the gender predictions elicited by German possessive pronouns were delayed and smaller in size in L2 than L1 speakers. The experiments also found a “match effect”, such that when the antecedent and possessee of the pronoun had the same gender, predictions were earlier than when the two genders differed. This match effect was smaller in L2 than L1 speakers. The model implementing the Lexical Bottleneck Hypothesis captured the effects of smaller predictions, smaller match effect and delayed predictions in one of the two conditions. By contrast, the model implementing the Interference Hypothesis captured the smaller prediction effect but it showed an earlier prediction effect and an increased match effect in L2 than L1 speakers. These results provide evidence for the Lexical Bottleneck Hypothesis, and they demonstrate a method for extending computational models of L1 to L2 processing.</abstract>
      <url hash="aba085d0">2022.conll-1.23</url>
      <bibkey>patil-lago-2022-computational</bibkey>
      <video href="2022.conll-1.23.mp4"/>
      <doi>10.18653/v1/2022.conll-1.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>PIE</fixed-case>-<fixed-case>QG</fixed-case>: Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora</title>
      <author><first>Dinesh</first><last>Nagumothu</last></author>
      <author><first>Bahadorreza</first><last>Ofoghi</last></author>
      <author><first>Guangyan</first><last>Huang</last></author>
      <author><first>Peter</first><last>Eklund</last></author>
      <pages>350-359</pages>
      <abstract>Supervised Question Answering systems (QA systems) rely on domain-specific human-labeled data for training. Unsupervised QA systems generate their own question-answer training pairs, typically using secondary knowledge sources to achieve this outcome. Our approach (called PIE-QG) uses Open Information Extraction (OpenIE) to generate synthetic training questions from paraphrased passages and uses the question-answer pairs as training data for a language model for a state-of-the-art QA system based on BERT. Triples in the form of &lt;subject, predicate, object&gt; are extracted from each passage, and questions are formed with subjects (or objects) and predicates while objects (or subjects) are considered as answers. Experimenting on five extractive QA datasets demonstrates that our technique achieves on-par performance with existing state-of-the-art QA systems with the benefit of being trained on an order of magnitude fewer documents and without any recourse to external reference data sources.</abstract>
      <url hash="c53da8d4">2022.conll-1.24</url>
      <attachment type="software" hash="aa0b92ff">2022.conll-1.24.software.zip</attachment>
      <bibkey>nagumothu-etal-2022-pie</bibkey>
      <doi>10.18653/v1/2022.conll-1.24</doi>
    </paper>
    <paper id="25">
      <title>Probing for targeted syntactic knowledge through grammatical error detection</title>
      <author><first>Christopher</first><last>Davis</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <author><first>Andrew</first><last>Caines</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <author><first>Paula</first><last>Buttery</last></author>
      <pages>360-373</pages>
      <abstract>Targeted studies testing knowledge of subject-verb agreement (SVA) indicate that pre-trained language models encode syntactic information. We assert that if models robustly encode subject-verb agreement, they should be able to identify when agreement is correct and when it is incorrect. To that end, we propose grammatical error detection as a diagnostic probe to evaluate token-level contextual representations for their knowledge of SVA. We evaluate contextual representations at each layer from five pre-trained English language models: BERT, XLNet, GPT-2, RoBERTa and ELECTRA. We leverage public annotated training data from both English second language learners and Wikipedia edits, and report results on manually crafted stimuli for subject-verb agreement. We find that masked language models linearly encode information relevant to the detection of SVA errors, while the autoregressive models perform on par with our baseline. However, we also observe a divergence in performance when probes are trained on different training sets, and when they are evaluated on different syntactic constructions, suggesting the information pertaining to SVA error detection is not robustly encoded.</abstract>
      <url hash="a5f32709">2022.conll-1.25</url>
      <bibkey>davis-etal-2022-probing</bibkey>
      <doi>10.18653/v1/2022.conll-1.25</doi>
    </paper>
    <paper id="26">
      <title>An Alignment-based Approach to Text Segmentation Similarity Scoring</title>
      <author><first>Gerardo</first><last>Ocampo Diaz</last></author>
      <author><first>Jessica</first><last>Ouyang</last></author>
      <pages>374-383</pages>
      <abstract>Text segmentation is a natural language processing task with popular applications, such as topic segmentation, element discourse extraction, and sentence tokenization. Much work has been done to develop accurate segmentation similarity metrics, but even the most advanced metrics used today, B, and WindowDiff, exhibit incorrect behavior due to their evaluation of boundaries in isolation. In this paper, we present a new segment-alignment based approach to segmentation similarity scoring and a new similarity metric A. We show that A does not exhibit the erratic behavior of $ and WindowDiff, quantify the likelihood of B and WindowDiff misbehaving through simulation, and discuss the versatility of alignment-based approaches for segmentation similarity scoring. We make our implementation of A publicly available and encourage the community to explore more sophisticated approaches to text segmentation similarity scoring.</abstract>
      <url hash="d7eb2c89">2022.conll-1.26</url>
      <bibkey>ocampo-diaz-ouyang-2022-alignment</bibkey>
      <doi>10.18653/v1/2022.conll-1.26</doi>
    </paper>
    <paper id="27">
      <title>Enhancing the Transformer Decoder with Transition-based Syntax</title>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>384-404</pages>
      <abstract>Notwithstanding recent advances, syntactic generalization remains a challenge for text decoders. While some studies showed gains from incorporating source-side symbolic syntactic and semantic structure into text generation Transformers, very little work addressed the decoding of such structure. We propose a general approach for tree decoding using a transition-based approach. Examining the challenging test case of incorporating Universal Dependencies syntax into machine translation, we present substantial improvements on test sets that focus on syntactic generalization, while presenting improved or comparable performance on standard MT benchmarks. Further qualitative analysis addresses cases where syntactic generalization in the vanilla Transformer decoder is inadequate and demonstrates the advantages afforded by integrating syntactic information.</abstract>
      <url hash="80a38c44">2022.conll-1.27</url>
      <bibkey>choshen-abend-2022-enhancing</bibkey>
      <video href="2022.conll-1.27.mp4"/>
      <doi>10.18653/v1/2022.conll-1.27</doi>
    </paper>
    <paper id="28">
      <title>Characterizing Verbatim Short-Term Memory in Neural Language Models</title>
      <author><first>Kristijan</first><last>Armeni</last></author>
      <author><first>Christopher</first><last>Honey</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <pages>405-424</pages>
      <abstract>When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers’ retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM’s retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.</abstract>
      <url hash="e05e03f0">2022.conll-1.28</url>
      <bibkey>armeni-etal-2022-characterizing</bibkey>
      <revision id="1" href="2022.conll-1.28v1" hash="e0a9be19"/>
      <revision id="2" href="2022.conll-1.28v2" hash="e05e03f0" date="2023-05-07">Fixed the tokenization problem.</revision>
      <doi>10.18653/v1/2022.conll-1.28</doi>
    </paper>
  </volume>
</collection>
