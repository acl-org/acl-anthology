<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.internlp">
  <volume id="1" ingest-date="2021-07-25">
    <meta>
      <booktitle>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</booktitle>
      <editor><first>Kianté</first><last>Brantley</last></editor>
      <editor><first>Soham</first><last>Dan</last></editor>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Ji-Ung</first><last>Lee</last></editor>
      <editor><first>Filip</first><last>Radlinski</last></editor>
      <editor><first>Hinrich</first><last>Schütze</last></editor>
      <editor><first>Edwin</first><last>Simpson</last></editor>
      <editor><first>Lili</first><last>Yu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>August</month>
      <year>2021</year>
      <url hash="245ab163">2021.internlp-1</url>
    </meta>
    <frontmatter>
      <url hash="15b91d30">2021.internlp-1.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>HILDIF</fixed-case>: <fixed-case>I</fixed-case>nteractive Debugging of <fixed-case>NLI</fixed-case> Models Using Influence Functions</title>
      <author><first>Hugo</first><last>Zylberajch</last></author>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last></author>
      <author><first>Francesca</first><last>Toni</last></author>
      <pages>1–6</pages>
      <abstract>Biases and artifacts in training data can cause unwelcome behavior in text classifiers (such as shallow pattern matching), leading to lack of generalizability. One solution to this problem is to include users in the loop and leverage their feedback to improve models. We propose a novel explanatory debugging pipeline called HILDIF, enabling humans to improve deep text classifiers using influence functions as an explanation method. We experiment on the Natural Language Inference (NLI) task, showing that HILDIF can effectively alleviate artifact problems in fine-tuned BERT models and result in increased model generalizability.</abstract>
      <url hash="02b6270f">2021.internlp-1.1</url>
    </paper>
    <paper id="2">
      <title>Apple Core-dination: Linguistic Feedback and Learning in a Speech-to-Action Shared World Game</title>
      <author><first>Susann</first><last>Boy</last></author>
      <author><first>AriaRay</first><last>Brown</last></author>
      <author><first>Morgan</first><last>Wixted</last></author>
      <pages>7–15</pages>
      <abstract>We investigate the question of how adaptive feedback from a virtual agent impacts the linguistic input of the user in a shared world game environment. To do so, we carry out an exploratory pilot study to observe how individualized linguistic feedback affects the user’s speech input. We introduce a speech-controlled game, Apple Core-dination, in which an agent learns complex tasks using a base knowledge of simple actions. The agent is equipped with a learning mechanism for mapping new commands to sequences of simple actions, as well as the ability to incorporate user input into written responses. The agent repeatedly shares its internal knowledge state by responding to what it knows and does not know about language meaning and the shared environment. Our paper focuses on the linguistic feedback loop in order to analyze the nature of user input. Feedback from the agent is provided in the form of visual movement and written linguistic responses. Particular attention is given to incorporating user input into agent responses and updating the speech-to-action mappings based on commands provided by the user. Through our pilot study, we analyze task success and compare the lexical features of user input. Results show variation in input length and lexical variety across users, suggesting a correlation between the two that can be studied further.</abstract>
      <url hash="6eeb0843">2021.internlp-1.2</url>
    </paper>
    <paper id="3">
      <title><fixed-case>SHAPELURN</fixed-case>: An Interactive Language Learning Game with Logical Inference</title>
      <author><first>Katharina</first><last>Stein</last></author>
      <author><first>Leonie</first><last>Harter</last></author>
      <author><first>Luisa</first><last>Geiger</last></author>
      <pages>16–24</pages>
      <abstract>We investigate if a model can learn natural language with minimal linguistic input through interaction. Addressing this question, we design and implement an interactive language learning game that learns logical semantic representations compositionally. Our game allows us to explore the benefits of logical inference for natural language learning. Evaluation shows that the model can accurately narrow down potential logical representations for words over the course of the game, suggesting that our model is able to learn lexical mappings from scratch successfully.</abstract>
      <url hash="afedf9b5">2021.internlp-1.3</url>
    </paper>
    <paper id="4">
      <title>A Proposal: Interactively Learning to Summarise Timelines by Reinforcement Learning</title>
      <author><first>Yuxuan</first><last>Ye</last></author>
      <author><first>Edwin</first><last>Simpson</last></author>
      <pages>25–31</pages>
      <abstract>Timeline Summarisation (TLS) aims to generate a concise, time-ordered list of events described in sources such as news articles. However, current systems do not provide an adequate way to adapt to new domains nor to focus on the aspects of interest to a particular user. Therefore, we propose a method for interactively learning abstractive TLS using Reinforcement Learning (RL). We define a compound reward function and use RL to fine-tune an abstractive Multi-document Summarisation (MDS) model, which avoids the need to train using reference summaries. One of the sub-reward functions will be learned interactively from user feedback to ensure the consistency between users’ demands and the generated timeline. The other sub-reward functions contribute to topical coherence and linguistic fluency. We plan experiments to evaluate whether our approach could generate accurate and precise timelines tailored for each user.</abstract>
      <url hash="21f50786">2021.internlp-1.4</url>
    </paper>
    <paper id="5">
      <title>Dynamic Facet Selection by Maximizing Graded Relevance</title>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Md Faisal Mahbub</first><last>Chowdhury</last></author>
      <author><first>Yu</first><last>Deng</last></author>
      <author><first>Ruchi</first><last>Mahindru</last></author>
      <author><first>Nicolas Rodolfo</first><last>Fauceglia</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <pages>32–39</pages>
      <abstract>Dynamic faceted search (DFS), an interactive query refinement technique, is a form of Human–computer information retrieval (HCIR) approach. It allows users to narrow down search results through facets, where the facets-documents mapping is determined at runtime based on the context of user query instead of pre-indexing the facets statically. In this paper, we propose a new unsupervised approach for dynamic facet generation, namely optimistic facets, which attempts to generate the best possible subset of facets, hence maximizing expected Discounted Cumulative Gain (DCG), a measure of ranking quality that uses a graded relevance scale. We also release code to generate a new evaluation dataset. Through empirical results on two datasets, we show that the proposed DFS approach considerably improves the document ranking in the search results.</abstract>
      <url hash="8c786b65">2021.internlp-1.5</url>
    </paper>
    <paper id="6">
      <title>Active Curriculum Learning</title>
      <author><first>Borna</first><last>Jafarpour</last></author>
      <author><first>Dawn</first><last>Sepehr</last></author>
      <author><first>Nick</first><last>Pogrebnyakov</last></author>
      <pages>40–45</pages>
      <abstract>This paper investigates and reveals the relationship between two closely related machine learning disciplines, namely Active Learning (AL) and Curriculum Learning (CL), from the lens of several novel curricula. This paper also introduces Active Curriculum Learning (ACL) which improves AL by combining AL with CL to benefit from the dynamic nature of the AL informativeness concept as well as the human insights used in the design of the curriculum heuristics. Comparison of the performance of ACL and AL on two public datasets for the Named Entity Recognition (NER) task shows the effectiveness of combining AL and CL using our proposed framework.</abstract>
      <url hash="f1741a5b">2021.internlp-1.6</url>
    </paper>
    <paper id="7">
      <title>Tackling Fake News Detection by Interactively Learning Representations using Graph Neural Networks</title>
      <author><first>Nikhil</first><last>Mehta</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>46–53</pages>
      <abstract>Easy access, variety of content, and fast widespread interactions are some of the reasons that have made social media increasingly popular in today’s society. However, this has also enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Detecting fake news is important to prevent misinformation and maintain a healthy society. While prior works have tackled this problem by building supervised learning systems, automatedly modeling the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem <i>interactively</i>, where human insight can be continually combined with an automated system, enabling better social media representation quality. Our experiments show performance improvements in this setting.</abstract>
      <url hash="d1fc0b98">2021.internlp-1.7</url>
    </paper>
  </volume>
</collection>
